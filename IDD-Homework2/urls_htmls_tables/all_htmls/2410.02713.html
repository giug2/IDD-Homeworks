<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Video Instruction Tuning with Synthetic Data</title>
<!--Generated on Fri Oct  4 13:32:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02713v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S1" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S2" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Video Instruction-Following Data Synthesis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS1" title="In 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Video source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS2" title="In 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Video Detail Description</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS2.SSS0.Px1" title="In 3.2 Video Detail Description ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Automated Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS3" title="In 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Video Question Answering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS3.SSS0.Px1" title="In 3.3 Video Question Answering ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Question Type definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS3.SSS0.Px2" title="In 3.3 Video Question Answering ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Automated Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS3.SSS0.Px3" title="In 3.3 Video Question Answering ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Filtering.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS4" title="In 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Dataset Statistics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS4.SSS0.Px1" title="In 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Overview.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS4.SSS0.Px2" title="In 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Dataset Comparison</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS0.SSS0.Px1" title="In 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Video Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS0.SSS0.Px2" title="In 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title">Evaluation Benchmarks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS1" title="In 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overall Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS2" title="In 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Dataset Ablation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS3" title="In 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Dataset Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S5" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A1" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Author Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A2" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Video Representations</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A2.SS1" title="In Appendix B Video Representations ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Efficient Video Representations in LMMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A2.SS2" title="In Appendix B Video Representations ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span><span class="ltx_text">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline"><semantics><msub><mi></mi><mi>𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content"><apply><ci>𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.SS1" title="In Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Video Detail Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.SS2" title="In Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Video Question Answering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.SS3" title="In Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Dataset Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A4" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Beyond Singularity: Extensive Sampling Matters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A5" title="In Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Capabilities</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\pdfcolInitStack</span>
<p class="ltx_p" id="p1.2">tcb@breakable






















































































































































































































































































































 


<span class="ltx_text" id="p1.2.1" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Video Instruction Tuning with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.1">Yuanhan Zhang<sup class="ltx_sup" id="id1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id1.1.1.1.1">†,♡</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.2">Jinming Wu<sup class="ltx_sup" id="id2.2.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id2.2.2.1.1">‡,♡</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.3">Wei Li<sup class="ltx_sup" id="id3.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id3.3.3.1.1">♮</span></sup></span>,
Bo Li<sup class="ltx_sup" id="id11.11.id1"><span class="ltx_text ltx_font_italic" id="id11.11.id1.1">†,♡</span></sup>,
<span class="ltx_text ltx_font_bold" id="id5.5.4">Zejun Ma<sup class="ltx_sup" id="id5.5.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.4.1.1">♮</span></sup></span>, 
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="id6.6.5">Ziwei Liu<sup class="ltx_sup" id="id6.6.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.5.1.1">†,¶</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id7.7.6">Chunyuan Li<sup class="ltx_sup" id="id7.7.6.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.6.1.1">♮,¶</span></sup></span>
<br class="ltx_break"/>
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id2"><span class="ltx_text ltx_font_italic" id="id12.12.id2.1">♮</span></sup>ByteDance  
<sup class="ltx_sup" id="id13.13.id3"><span class="ltx_text ltx_font_italic" id="id13.13.id3.1">†</span></sup>S-Lab, NTU  
<sup class="ltx_sup" id="id14.14.id4"><span class="ltx_text ltx_font_italic" id="id14.14.id4.1">‡</span></sup>BUPT 
<br class="ltx_break"/>
<br class="ltx_break"/> 
<span class="ltx_text ltx_font_typewriter" id="id15.15.id5" style="color:#FF0000;">
<a class="ltx_ref ltx_href" href="https://llava-vl.github.io/blog/2024-09-30-llava-video" title="">https://llava-vl.github.io/blog/llava-video</a></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.id1"><span class="ltx_text" id="id16.id1.1" lang="en">The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we consider an alternative approach, creating a high-quality synthetic dataset specifically for video instruction-following, namely <span class="ltx_text" id="id16.id1.1.1">LLaVA-Video-178K</span>. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this proposed dataset, in combination with existing visual instruction tuning data, we introduce <span class="ltx_text" id="id16.id1.1.2">LLaVA-Video</span>, a new video LMM. Our experiments demonstrate that <span class="ltx_text" id="id16.id1.1.3">LLaVA-Video</span> achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.</span></p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1" lang="en"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><math alttext="\heartsuit" class="ltx_Math" display="inline" id="footnotex1.m1.1"><semantics id="footnotex1.m1.1b"><mi id="footnotex1.m1.1.1" mathvariant="normal" xref="footnotex1.m1.1.1.cmml">♡</mi><annotation-xml encoding="MathML-Content" id="footnotex1.m1.1c"><ci id="footnotex1.m1.1.1.cmml" xref="footnotex1.m1.1.1">♡</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex1.m1.1d">\heartsuit</annotation><annotation encoding="application/x-llamapun" id="footnotex1.m1.1e">♡</annotation></semantics></math> Work collaborated with ByteDance; <math alttext="\P" class="ltx_Math" display="inline" id="footnotex1.m2.1"><semantics id="footnotex1.m2.1b"><mi id="footnotex1.m2.1.1" mathvariant="normal" xref="footnotex1.m2.1.1.cmml">¶</mi><annotation-xml encoding="MathML-Content" id="footnotex1.m2.1c"><ci id="footnotex1.m2.1.1.cmml" xref="footnotex1.m2.1.1">¶</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex1.m2.1d">\P</annotation><annotation encoding="application/x-llamapun" id="footnotex1.m2.1e">¶</annotation></semantics></math> Co-senior authors</span></span></span>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We are in an era where large-scale computing and data is crucial for multimodal learning <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib26" title="">2024d</a>)</cite>. A significant recent advancement was introduced by visual instruction tuning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib30" title="">2024a</a>)</cite>, which laid the foundation for building a general-purpose visual assistant. Notably, it proposed a data generation pipeline to create high-quality image-language instruction-following data. This pipeline has inspired subsequent researches <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib24" title="">b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib23" title="">a</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib29" title="">2024</a>)</cite> aimed at generating diverse image-language instruction data across various visual domains, accelerating the development of visual instruction tuning techniques.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Compared to the construction of image-language instruction-following data, obtaining high-quality video-language instruction-following data is challenging <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib63" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib28" title="">2024e</a>)</cite>. First, sourcing high-quality videos is difficult. We need to find videos with significant temporal changes that provide more knowledge than what image-language data can offer. However, we have found that most videos in current video-language instruction-following datasets <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib7" title="">2024a</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib67" title="">2024d</a>)</cite> are relatively static. Additionally, these videos are mostly trimmed based on scene changes, resulting in simplified plots. Such simplified video-language instruction-tuning data is inadequate for models to understand videos with complex narratives. Furthermore, current video-language instruction-following datasets often use a very sparse sampling rate for frame annotation. For instance, ShareGPT4Video <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib7" title="">2024a</a>)</cite> has an average sampling rate of 0.15, sometimes sampling only 2 frames from a 30-second video.
This sparse sampling rate is effective in describing overall scenes but fails to capture detailed movements or changes in the video, resulting in hallucination when detailed descriptions of the video are required.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.4">To overcome these shortcomings, we introduce a comprehensive video instruction-tuning dataset named <span class="ltx_text" id="S1.p3.4.1">LLaVA-Video-178K</span>, consisting of 178,510 videos ranging from 0 to 3 minutes. This dataset is enriched with detailed annotations, open-ended questions, and multiple-choice questions, developed through a combination of GPT-4o <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib39" title="">2024</a>)</cite> and human efforts. It features four favorable properties: <math alttext="(i)" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.2.2"><mo id="S1.p3.1.m1.1.2.2.1" stretchy="false">(</mo><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">i</mi><mo id="S1.p3.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">(i)</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">( italic_i )</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S1.p3.4.2"> Extensive Video Source:</span> We conduct a comprehensive survey on the video sources of exsiting video understanding datasets, and conclude 10 major video data sources, from which we start our video data collection by building a video pool.
Although there are over 40 video-language datasets, their video data are mainly sourced from 10 datasets <cite class="ltx_cite ltx_citemacro_citep">(Zhou &amp; Corso, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib70" title="">2017</a>; Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib57" title="">2022</a>; Goyal et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib12" title="">2017</a>; Caba Heilbron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib5" title="">2015</a>; Kay et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib17" title="">2017</a>; Sigurdsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib45" title="">2016</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib48" title="">2023</a>; Shang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib44" title="">2019</a>; Grauman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib13" title="">2022</a>; Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib71" title="">2023a</a>)</cite>, covering a wide range of video domains, such as activities, cooking, TV shows, and egocentric views. <math alttext="(ii)" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mrow id="S1.p3.2.m2.1.1.1" xref="S1.p3.2.m2.1.1.1.1.cmml"><mo id="S1.p3.2.m2.1.1.1.2" stretchy="false" xref="S1.p3.2.m2.1.1.1.1.cmml">(</mo><mrow id="S1.p3.2.m2.1.1.1.1" xref="S1.p3.2.m2.1.1.1.1.cmml"><mi id="S1.p3.2.m2.1.1.1.1.2" xref="S1.p3.2.m2.1.1.1.1.2.cmml">i</mi><mo id="S1.p3.2.m2.1.1.1.1.1" xref="S1.p3.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S1.p3.2.m2.1.1.1.1.3" xref="S1.p3.2.m2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S1.p3.2.m2.1.1.1.3" stretchy="false" xref="S1.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><apply id="S1.p3.2.m2.1.1.1.1.cmml" xref="S1.p3.2.m2.1.1.1"><times id="S1.p3.2.m2.1.1.1.1.1.cmml" xref="S1.p3.2.m2.1.1.1.1.1"></times><ci id="S1.p3.2.m2.1.1.1.1.2.cmml" xref="S1.p3.2.m2.1.1.1.1.2">𝑖</ci><ci id="S1.p3.2.m2.1.1.1.1.3.cmml" xref="S1.p3.2.m2.1.1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">(ii)</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">( italic_i italic_i )</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S1.p3.4.3"> Dynamic Untrimmed Video Selection:</span> From these sources, we use several filtering logic to select the most dynamic videos from the video data pool. Notably, we select original, untrimmed videos to ensure plot completeness. <math alttext="(iii)" class="ltx_Math" display="inline" id="S1.p3.3.m3.1"><semantics id="S1.p3.3.m3.1a"><mrow id="S1.p3.3.m3.1.1.1" xref="S1.p3.3.m3.1.1.1.1.cmml"><mo id="S1.p3.3.m3.1.1.1.2" stretchy="false" xref="S1.p3.3.m3.1.1.1.1.cmml">(</mo><mrow id="S1.p3.3.m3.1.1.1.1" xref="S1.p3.3.m3.1.1.1.1.cmml"><mi id="S1.p3.3.m3.1.1.1.1.2" xref="S1.p3.3.m3.1.1.1.1.2.cmml">i</mi><mo id="S1.p3.3.m3.1.1.1.1.1" xref="S1.p3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S1.p3.3.m3.1.1.1.1.3" xref="S1.p3.3.m3.1.1.1.1.3.cmml">i</mi><mo id="S1.p3.3.m3.1.1.1.1.1a" xref="S1.p3.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S1.p3.3.m3.1.1.1.1.4" xref="S1.p3.3.m3.1.1.1.1.4.cmml">i</mi></mrow><mo id="S1.p3.3.m3.1.1.1.3" stretchy="false" xref="S1.p3.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.3.m3.1b"><apply id="S1.p3.3.m3.1.1.1.1.cmml" xref="S1.p3.3.m3.1.1.1"><times id="S1.p3.3.m3.1.1.1.1.1.cmml" xref="S1.p3.3.m3.1.1.1.1.1"></times><ci id="S1.p3.3.m3.1.1.1.1.2.cmml" xref="S1.p3.3.m3.1.1.1.1.2">𝑖</ci><ci id="S1.p3.3.m3.1.1.1.1.3.cmml" xref="S1.p3.3.m3.1.1.1.1.3">𝑖</ci><ci id="S1.p3.3.m3.1.1.1.1.4.cmml" xref="S1.p3.3.m3.1.1.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.3.m3.1c">(iii)</annotation><annotation encoding="application/x-llamapun" id="S1.p3.3.m3.1d">( italic_i italic_i italic_i )</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S1.p3.4.4"> Recurrent Detailed Caption Generation Pipeline with Dense Frame Sampling:</span> We propose a detailed video caption pipeline that operates recurrently, enabling us to generate detailed captions for videos of any length. This pipeline has three levels, each level of description represents a different time-range: from 10 seconds to the entire video length. It is recurrent as the historical description from any level serves as the context for generating new descriptions at any level. Additionally, we adopted a dense sampling strategy of one frame per second to ensure the sampled frames are rich enough to represent the videos. <math alttext="(iv)" class="ltx_Math" display="inline" id="S1.p3.4.m4.1"><semantics id="S1.p3.4.m4.1a"><mrow id="S1.p3.4.m4.1.1.1" xref="S1.p3.4.m4.1.1.1.1.cmml"><mo id="S1.p3.4.m4.1.1.1.2" stretchy="false" xref="S1.p3.4.m4.1.1.1.1.cmml">(</mo><mrow id="S1.p3.4.m4.1.1.1.1" xref="S1.p3.4.m4.1.1.1.1.cmml"><mi id="S1.p3.4.m4.1.1.1.1.2" xref="S1.p3.4.m4.1.1.1.1.2.cmml">i</mi><mo id="S1.p3.4.m4.1.1.1.1.1" xref="S1.p3.4.m4.1.1.1.1.1.cmml">⁢</mo><mi id="S1.p3.4.m4.1.1.1.1.3" xref="S1.p3.4.m4.1.1.1.1.3.cmml">v</mi></mrow><mo id="S1.p3.4.m4.1.1.1.3" stretchy="false" xref="S1.p3.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.4.m4.1b"><apply id="S1.p3.4.m4.1.1.1.1.cmml" xref="S1.p3.4.m4.1.1.1"><times id="S1.p3.4.m4.1.1.1.1.1.cmml" xref="S1.p3.4.m4.1.1.1.1.1"></times><ci id="S1.p3.4.m4.1.1.1.1.2.cmml" xref="S1.p3.4.m4.1.1.1.1.2">𝑖</ci><ci id="S1.p3.4.m4.1.1.1.1.3.cmml" xref="S1.p3.4.m4.1.1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.4.m4.1c">(iv)</annotation><annotation encoding="application/x-llamapun" id="S1.p3.4.m4.1d">( italic_i italic_v )</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S1.p3.4.5"> Diverse Tasks:</span> Based on the detailed video descriptions, we can generate question-answer pairs. To ensure our questions cover a wide range of scenarios, by referring to the video question-answering dataset, we define 16 question types. We prompt GPT-4o to generate question-answer pairs by referring to these question types, covering open-ended and multi-choice questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Based upon the <span class="ltx_text" id="S1.p4.1.2">LLaVA-Video-178K</span> dataset, we developed <span class="ltx_text" id="S1.p4.1.3">LLaVA-Video</span>. Contrary to previous studies suggesting that training with single frames is sufficient for video-language understanding <cite class="ltx_cite ltx_citemacro_citep">(Lei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib22" title="">2022</a>)</cite>, our findings reveal a significant impact of frame count on <span class="ltx_text" id="S1.p4.1.4">LLaVA-Video</span>’s performance, attributable to the detailed features of <span class="ltx_text" id="S1.p4.1.5">LLaVA-Video-178K</span>. Observing this, we explored maximizing frame sampling within the constraints of limited GPU memory. We introduce <span class="ltx_text" id="S1.p4.1.1">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline" id="S1.p4.1.1.m1.1"><semantics id="S1.p4.1.1.m1.1a"><msub id="S1.p4.1.1.m1.1.1" xref="S1.p4.1.1.m1.1.1.cmml"><mi id="S1.p4.1.1.m1.1.1a" xref="S1.p4.1.1.m1.1.1.cmml"></mi><mi id="S1.p4.1.1.m1.1.1.1" xref="S1.p4.1.1.m1.1.1.1.cmml">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p4.1.1.m1.1b"><apply id="S1.p4.1.1.m1.1.1.cmml" xref="S1.p4.1.1.m1.1.1"><ci id="S1.p4.1.1.m1.1.1.1.cmml" xref="S1.p4.1.1.m1.1.1.1">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.1.m1.1c">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.1.m1.1d">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span>, a video representation technique that optimally distributes visual tokens across different frames. This approach allows for incorporating up to three times more frames than traditional methods, which allocate an equal number of visual tokens to each frame.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">Video-language Instruction-Following Data</span>: We present a high-quality dataset <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.2">LLaVA-Video-178K</span> tailored for video instruction-following. It consists of 178K video with 1.3M instruction samples, including detailed captions, free-form and multiple-choice question answering.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">Video Large Multimodal Models</span>: We develop <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.2">LLaVA-Video</span>, a series of advanced large video-language models that expand the capabilities of open models in understanding video content.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">Open-Source</span>: In an effort to support the development of general-purpose visual assistants, we release our multimodal instruction data, codebase, model checkpoints, and a visual chat demo to the public.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this work, our goal is to create a high-quality video-language dataset that goes beyond simple video captions. We aim to improve the ability to follow instructions, which includes detailed video descriptions, open-ended video question-answering, and multiple-choice video question-answering data. We discuss related datasets in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.T1" title="Table 1 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>. Previous video-language datasets <cite class="ltx_cite ltx_citemacro_citep">(Miech et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib37" title="">2019</a>)</cite> include manually annotated data for various tasks, such as video captions <cite class="ltx_cite ltx_citemacro_citep">(Chen &amp; Dolan, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib6" title="">2011</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib53" title="">2016</a>; Rohrbach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib43" title="">2015</a>; Anne Hendricks et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib2" title="">2017a</a>; Caba Heilbron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib5" title="">2015</a>; Zhou &amp; Corso, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib70" title="">2017</a>)</cite>, and video question-answering <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib59" title="">2019</a>; Zadeh et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib60" title="">2019</a>; Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>)</cite>. However, manual annotation is expensive and limits the size of such datasets. To address the shortage of data, studies like <cite class="ltx_cite ltx_citemacro_citep">(Miech et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib37" title="">2019</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib19" title="">2021</a>; Zellers et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib61" title="">2021</a>; Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib57" title="">2022</a>)</cite> suggest automatically annotating data using subtitles created by ASR. While this method greatly expands the dataset size to 100 million samples, the subtitles often fail to accurately describe the main video content. Additionally, other studies <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib52" title="">2017</a>; Grunde-McLaughlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib14" title="">2021</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib49" title="">2024a</a>)</cite> use language models <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib52" title="">2017</a>)</cite> or question templates <cite class="ltx_cite ltx_citemacro_citep">(Grunde-McLaughlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib14" title="">2021</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib49" title="">2024a</a>)</cite> to generate question-answer pairs. Although this approach can generate a large number of questions and answers, it often produces poor-quality questions that do not reflect real-world user inquiries. More recent research <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib8" title="">2024b</a>)</cite> has prompted video-language models such as BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib27" title="">2023</a>)</cite>, VideoChat <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib28" title="">2024e</a>)</cite>, Video-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib63" title="">2023</a>)</cite>, and MiniGPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib72" title="">2023b</a>)</cite> to generate video captions. However, these models are limited in their ability to provide detailed descriptions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The most related works to ours are the recent AI-generated synthetic video instruction tuning data, LLaVA-Hound <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib67" title="">2024d</a>)</cite> and ShareGPT4Video <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib7" title="">2024a</a>)</cite>, where they have used GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib38" title="">2023</a>)</cite> to generate video captions and open-ended video question-answering. Although the quality of the captions and question-answer pairs has significantly improved, the video sources they use are too static to produce high-quality data for instruction-following scenarios. They also only use very sparse frames for prompting GPT-4V, which results in annotations that fail to capture nuanced actions and continuous plots in the videos. Additionally, Shot2Story <cite class="ltx_cite ltx_citemacro_citep">(Han et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib15" title="">2023</a>)</cite> and Vript <cite class="ltx_cite ltx_citemacro_citep">(Han et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib15" title="">2023</a>)</cite> also employ GPT-4V <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib38" title="">2023</a>)</cite> for video captioning. Their outputs, however, include audio details, which are outside the scope of this study.</p>
</div>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Video Instruction-Following Data Synthesis</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S3.F1.g1" src="x1.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S3.F1.2.1">Video sources in the proposed <span class="ltx_text ltx_font_italic" id="S3.F1.2.1.1">LLaVA-Video-178K</span></span>. (Left) The relationship between 10 video sources we have utilized and other existing video-language datasets. (Right) Filtering logic for video sources. The detail of filtering logic: ① Sorted by Views, ② Number of scenes greater than 2, ③ Video duration between 5 seconds and 180 seconds, ④ Ratio of scenes to video duration less than or equal to 0.5, ⑤ Resolution greater than 480p, ⑥ 50 samples for each category.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">A high-quality dataset for video instruction-tuning is crucial for developing effective video-language models. We identify a key factor in building such datasets: ensuring richness and diversity in both video content and its language annotations. We perform comprehensive survey on the existing video benchmarks, covering across various public video captioning and question-answering datasets, then identify ten unique video sources that contribute to over 40 video-language benchmarks. From each source, we select videos that exhibit significant temporal dynamics. To maintain diversity in the annotations, we establish a pipeline capable of generating detailed captions for videos of any length. Additionally, we define 16 types of questions that guide GPT-4o in creating question-answer pairs to assess the perceptual and reasoning skills of the video-language models.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Video source</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">One important starting point in building a high-quality video instruction-following dataset is to find a sufficiently diverse pool of video data. From this pool, we can select the qualified videos. In our study of public video-language datasets—including video captioning, video question answering, video summarization, and moment-wise captioning—we noticed that although different datasets focus on various video understanding tasks (<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">e.g.</span>, AGQA <cite class="ltx_cite ltx_citemacro_citep">(Grunde-McLaughlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib14" title="">2021</a>)</cite> for spatial-temporal relations and STAR <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib49" title="">2024a</a>)</cite> for situational reasoning), most are sourced from ten main video sources. For instance, both AGQA and STAR use data from Charades <cite class="ltx_cite ltx_citemacro_citep">(Sigurdsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib45" title="">2016</a>)</cite>. Specifically, these ten sources are HD-VILA-100M <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib57" title="">2022</a>)</cite>, InternVid-10M <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib48" title="">2023</a>)</cite>, VidOR <cite class="ltx_cite ltx_citemacro_citep">(Shang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib44" title="">2019</a>)</cite>, VIDAL (YouTube Shorts)<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib71" title="">2023a</a>)</cite>, YouCook2<cite class="ltx_cite ltx_citemacro_citep">(Zhou &amp; Corso, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib70" title="">2017</a>)</cite>, Charades <cite class="ltx_cite ltx_citemacro_citep">(Sigurdsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib45" title="">2016</a>)</cite>, ActivityNet <cite class="ltx_cite ltx_citemacro_citep">(Caba Heilbron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib5" title="">2015</a>)</cite>, Kinetics-700 <cite class="ltx_cite ltx_citemacro_citep">(Kay et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib17" title="">2017</a>)</cite>, Something-Something v2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib12" title="">2017</a>)</cite>, and Ego4d <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib13" title="">2022</a>)</cite>. These sources offer a wide range of video data from different websites, viewpoints, and domains. The relationship between these ten selected video datasets and others is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F1" title="Figure 1 ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>. The videos from this ten datsets build the video pool for the further video selection. Notably, we use untrimmed videos from each source except for YouCook2 and Kinetics-700. We believe that cutting videos into clips can break the plot continuity, which is essential for understanding the videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Based on the video pool, we aim to select dynamic videos. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F1" title="Figure 1 ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>, we outline our criteria for selecting high-quality data. Our main method for identifying dynamic content involves using PySceneDetect, which calculates the number of scenes in a video
We found that the number of scenes is a good indicator of video dynamism. Additionally, we have designed a specific approach ④ to exclude videos that mainly contain “slides."</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S3.F2.g1" src="x2.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.14.1">The video detail description creation pipeline</span>. A three-level creation pipeline is considered, with each level developed via a recurrent approach.
Note that <math alttext="t" class="ltx_Math" display="inline" id="S3.F2.7.m1.1"><semantics id="S3.F2.7.m1.1b"><mi id="S3.F2.7.m1.1.1" xref="S3.F2.7.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F2.7.m1.1c"><ci id="S3.F2.7.m1.1.1.cmml" xref="S3.F2.7.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="S3.F2.7.m1.1e">italic_t</annotation></semantics></math> is the index of time internal at its own level, and <math alttext="T" class="ltx_Math" display="inline" id="S3.F2.8.m2.1"><semantics id="S3.F2.8.m2.1b"><mi id="S3.F2.8.m2.1.1" xref="S3.F2.8.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.m2.1c"><ci id="S3.F2.8.m2.1.1.cmml" xref="S3.F2.8.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m2.1d">T</annotation><annotation encoding="application/x-llamapun" id="S3.F2.8.m2.1e">italic_T</annotation></semantics></math> is the last time internal index.
(a) To generate the caption for time internal <math alttext="t" class="ltx_Math" display="inline" id="S3.F2.9.m3.1"><semantics id="S3.F2.9.m3.1b"><mi id="S3.F2.9.m3.1.1" xref="S3.F2.9.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F2.9.m3.1c"><ci id="S3.F2.9.m3.1.1.cmml" xref="S3.F2.9.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.m3.1d">t</annotation><annotation encoding="application/x-llamapun" id="S3.F2.9.m3.1e">italic_t</annotation></semantics></math> at level-1, we condition on the current frames in this internal, the caption for time internal <math alttext="t-1" class="ltx_Math" display="inline" id="S3.F2.10.m4.1"><semantics id="S3.F2.10.m4.1b"><mrow id="S3.F2.10.m4.1.1" xref="S3.F2.10.m4.1.1.cmml"><mi id="S3.F2.10.m4.1.1.2" xref="S3.F2.10.m4.1.1.2.cmml">t</mi><mo id="S3.F2.10.m4.1.1.1" xref="S3.F2.10.m4.1.1.1.cmml">−</mo><mn id="S3.F2.10.m4.1.1.3" xref="S3.F2.10.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.10.m4.1c"><apply id="S3.F2.10.m4.1.1.cmml" xref="S3.F2.10.m4.1.1"><minus id="S3.F2.10.m4.1.1.1.cmml" xref="S3.F2.10.m4.1.1.1"></minus><ci id="S3.F2.10.m4.1.1.2.cmml" xref="S3.F2.10.m4.1.1.2">𝑡</ci><cn id="S3.F2.10.m4.1.1.3.cmml" type="integer" xref="S3.F2.10.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.m4.1d">t-1</annotation><annotation encoding="application/x-llamapun" id="S3.F2.10.m4.1e">italic_t - 1</annotation></semantics></math>, and the most recent description summary at level-2 if applicable.
(b) To generate caption for time internal <math alttext="t" class="ltx_Math" display="inline" id="S3.F2.11.m5.1"><semantics id="S3.F2.11.m5.1b"><mi id="S3.F2.11.m5.1.1" xref="S3.F2.11.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F2.11.m5.1c"><ci id="S3.F2.11.m5.1.1.cmml" xref="S3.F2.11.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.m5.1d">t</annotation><annotation encoding="application/x-llamapun" id="S3.F2.11.m5.1e">italic_t</annotation></semantics></math> at level-2, we condtion on the previous caption at level-2, and captions from three most recent time internals at level-1.
(c) To generate the overall caption at the last time internal <math alttext="T" class="ltx_Math" display="inline" id="S3.F2.12.m6.1"><semantics id="S3.F2.12.m6.1b"><mi id="S3.F2.12.m6.1.1" xref="S3.F2.12.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F2.12.m6.1c"><ci id="S3.F2.12.m6.1.1.cmml" xref="S3.F2.12.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.m6.1d">T</annotation><annotation encoding="application/x-llamapun" id="S3.F2.12.m6.1e">italic_T</annotation></semantics></math> at level-3, we condtion on the the most recent caption at level-2 and the current caption from level-1.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Video Detail Description</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Automated Generation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">For selected videos, we use GPT-4o <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib39" title="">2024</a>)</cite> to systematically describe their content. We start by sampling video frames at one frame per second (fps). However, due to the input size constraints of GPT-4o, we cannot use all sampled frames. Instead, we describe the videos sequentially, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F2" title="Figure 2 ‣ 3.1 Video source ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>. We create descriptions at three distinct levels, detailed below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Level-1 Description</span>: Every 10 seconds, we provide a level-1 description that outlines the events in that segment. This description considers: frames from the current clip and historical context, which includes all recent level-1 descriptions not yet summarized into a level-2 description and the latest level-2 description.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">Level-2 Description</span>: Every 30 seconds, we creat a level-2 summary of the entire video plot up to that point. This is based on the last three level-1 descriptions, covering the most recent 30 seconds; and the latest level-2 description.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.1">Level-3 Description</span>: At the video’s end, we generate a level-3 description to encapsulate the entire video. The inputs for this description are the recent level-1 descriptions not yet summarized, covering the last moments of the plot after the recent summary; and the latest level-2 description.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="S3.F3.g1" src="x3.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Question types for video question answering in data creation. For each type, we provide its name and an example question.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Video Question Answering</h3>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Question Type definition</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">In addition to detailed video descriptions, our dataset includes a variety of question-answer pairs designed for complex interactions. This setup improves the video understanding model’s ability to handle real-life queries. We refer to public video question-answering benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>; Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib59" title="">2019</a>; khattak et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib18" title="">2024</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib31" title="">2024b</a>)</cite> to organize these questions into 16 specific categories, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F3" title="Figure 3 ‣ Automated Generation ‣ 3.2 Video Detail Description ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Automated Generation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">Given a detailed video description, we use GPT-4o to generate at most one question-answer pair for each type of question. The prompts include: (1) The task definition for the current question type. (2) In-context examples for this type, which include three video descriptions and their three question-answer pairs of this specific type. (3) The detailed video description for the current video.
We instruct GPT-4o to return <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.1">None</span> if it cannot generate question-answer pairs for a specific question type.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Filtering.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">To filter out the generated question-answer pairs, we apply the following strategy: (1) remove duplicates using the sentence-transformer <cite class="ltx_cite ltx_citemacro_citep">(Reimers &amp; Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib42" title="">2020</a>)</cite>, (2) discard answers that begin with phrases like “does not specify,” “does not mention,” “does not specifically,” “does not depict,” or “does not show.”</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="348" id="S3.F4.g1" src="x4.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>One example to illustrate the video instruction-following data.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Dataset Statistics</h3>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Overview.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">We carefully select from our collected data sources to form a balanced and comprehensive collection, resulting in a total of 178K videos and 1.3M instruction-following samples. This includes 178K captions, 960K open-ended QAs, and 196K multiple-choice QAs.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" id="S3.F5.1" style="width:377.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="286" id="S3.F5.1.g1" src="x5.png" width="761"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" id="S3.F5.31" style="width:377.6pt;">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.F5.31.30">
<tr class="ltx_tr" id="S3.F5.31.30.31">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.F5.31.30.31.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.31.30.31.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.2.1">#Caption</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.31.30.31.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.3.1">#Open-Ended</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.F5.31.30.31.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.4.1">#Multi-Choice</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.F5.31.30.31.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.5.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.31.30.31.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.6.1">#Caption</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.31.30.31.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.7.1">#Open-Ended</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.31.30.31.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.F5.31.30.31.8.1">#Multi-Choice</span></td>
</tr>
<tr class="ltx_tr" id="S3.F5.7.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.F5.7.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.7.6.6.7.1"></span><span class="ltx_text" id="S3.F5.7.6.6.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.7.6.6.7.2.1">
<span class="ltx_tr" id="S3.F5.7.6.6.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.7.6.6.7.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">VidOR</span></span>
</span></span><span class="ltx_text" id="S3.F5.7.6.6.7.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.2.1.1.1.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#E5F0DB" stroke="#E5F0DB" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 4,018</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.3.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.3.2.2.2.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#F7E6D8" stroke="#F7E6D8" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 19,875</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.F5.4.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.4.3.3.3.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#DCE3F2" stroke="#DCE3F2" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 4,773</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.F5.7.6.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.7.6.6.8.1"></span><span class="ltx_text" id="S3.F5.7.6.6.8.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.7.6.6.8.2.1">
<span class="ltx_tr" id="S3.F5.7.6.6.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.7.6.6.8.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Sthsth2</span></span>
</span></span><span class="ltx_text" id="S3.F5.7.6.6.8.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.5.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.5.4.4.4.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#B9D7D2" stroke="#B9D7D2" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 8,700</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.6.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.6.5.5.5.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#C8C2BE" stroke="#C8C2BE" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.F5.7.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.7.6.6.6.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#D8A6A6" stroke="#D8A6A6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 0</td>
</tr>
<tr class="ltx_tr" id="S3.F5.13.12.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.F5.13.12.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.13.12.12.7.1"></span><span class="ltx_text" id="S3.F5.13.12.12.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.13.12.12.7.2.1">
<span class="ltx_tr" id="S3.F5.13.12.12.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.13.12.12.7.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">YouCook2</span></span>
</span></span><span class="ltx_text" id="S3.F5.13.12.12.7.3"></span></td>
<td class="ltx_td ltx_align_center" id="S3.F5.8.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.8.7.7.1.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#A9C1D9" stroke="#A9C1D9" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 7,411</td>
<td class="ltx_td ltx_align_center" id="S3.F5.9.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.9.8.8.2.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#B6B8D6" stroke="#B6B8D6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 32,143</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.F5.10.9.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.10.9.9.3.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#B9D7D2" stroke="#B9D7D2" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 5,776</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.F5.13.12.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.13.12.12.8.1"></span><span class="ltx_text" id="S3.F5.13.12.12.8.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.13.12.12.8.2.1">
<span class="ltx_tr" id="S3.F5.13.12.12.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.13.12.12.8.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Ego4D</span></span>
</span></span><span class="ltx_text" id="S3.F5.13.12.12.8.3"></span></td>
<td class="ltx_td ltx_align_center" id="S3.F5.11.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.11.10.10.4.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#E2C2C6" stroke="#E2C2C6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 1,065</td>
<td class="ltx_td ltx_align_center" id="S3.F5.12.11.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.12.11.11.5.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#DECDB4" stroke="#DECDB4" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 5,912</td>
<td class="ltx_td ltx_align_center" id="S3.F5.13.12.12.6" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.13.12.12.6.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#E5F0DB" stroke="#E5F0DB" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 520</td>
</tr>
<tr class="ltx_tr" id="S3.F5.19.18.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.F5.19.18.18.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.19.18.18.7.1"></span><span class="ltx_text" id="S3.F5.19.18.18.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.19.18.18.7.2.1">
<span class="ltx_tr" id="S3.F5.19.18.18.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.19.18.18.7.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Charades</span></span>
</span></span><span class="ltx_text" id="S3.F5.19.18.18.7.3"></span></td>
<td class="ltx_td ltx_align_center" id="S3.F5.14.13.13.1" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.14.13.13.1.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#C8C2BE" stroke="#C8C2BE" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 9,803</td>
<td class="ltx_td ltx_align_center" id="S3.F5.15.14.14.2" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.15.14.14.2.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#D8A6A6" stroke="#D8A6A6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 48,187</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.F5.16.15.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.16.15.15.3.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#E2C2C6" stroke="#E2C2C6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 13,401</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.F5.19.18.18.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.19.18.18.8.1"></span><span class="ltx_text" id="S3.F5.19.18.18.8.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.19.18.18.8.2.1">
<span class="ltx_tr" id="S3.F5.19.18.18.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.19.18.18.8.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">InternVid-10M</span></span>
</span></span><span class="ltx_text" id="S3.F5.19.18.18.8.3"></span></td>
<td class="ltx_td ltx_align_center" id="S3.F5.17.16.16.4" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.17.16.16.4.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#F7E6D8" stroke="#F7E6D8" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 45,000</td>
<td class="ltx_td ltx_align_center" id="S3.F5.18.17.17.5" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.18.17.17.5.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#DCE3F2" stroke="#DCE3F2" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 245,840</td>
<td class="ltx_td ltx_align_center" id="S3.F5.19.18.18.6" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.19.18.18.6.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#A9C1D9" stroke="#A9C1D9" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 48,246</td>
</tr>
<tr class="ltx_tr" id="S3.F5.25.24.24">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.F5.25.24.24.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.25.24.24.7.1"></span><span class="ltx_text" id="S3.F5.25.24.24.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.25.24.24.7.2.1">
<span class="ltx_tr" id="S3.F5.25.24.24.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.25.24.24.7.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">ActivityNet</span></span>
</span></span><span class="ltx_text" id="S3.F5.25.24.24.7.3"></span></td>
<td class="ltx_td ltx_align_center" id="S3.F5.20.19.19.1" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.20.19.19.1.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#DECDB4" stroke="#DECDB4" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 7,953</td>
<td class="ltx_td ltx_align_center" id="S3.F5.21.20.20.2" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.21.20.20.2.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#E5F0DB" stroke="#E5F0DB" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 44,100</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.F5.22.21.21.3" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.22.21.21.3.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#F7E6D8" stroke="#F7E6D8" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 12,771</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.F5.25.24.24.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.25.24.24.8.1"></span><span class="ltx_text" id="S3.F5.25.24.24.8.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.25.24.24.8.2.1">
<span class="ltx_tr" id="S3.F5.25.24.24.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.25.24.24.8.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">HD-VILA-100M</span></span>
</span></span><span class="ltx_text" id="S3.F5.25.24.24.8.3"></span></td>
<td class="ltx_td ltx_align_center" id="S3.F5.23.22.22.4" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.23.22.22.4.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#B6B8D6" stroke="#B6B8D6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 48,260</td>
<td class="ltx_td ltx_align_center" id="S3.F5.24.23.23.5" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.24.23.23.5.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#B9D7D2" stroke="#B9D7D2" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 263,652</td>
<td class="ltx_td ltx_align_center" id="S3.F5.25.24.24.6" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.25.24.24.6.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#C8C2BE" stroke="#C8C2BE" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 51,743</td>
</tr>
<tr class="ltx_tr" id="S3.F5.31.30.30">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.F5.31.30.30.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.31.30.30.7.1"></span><span class="ltx_text" id="S3.F5.31.30.30.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.31.30.30.7.2.1">
<span class="ltx_tr" id="S3.F5.31.30.30.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.31.30.30.7.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-700</span></span>
</span></span><span class="ltx_text" id="S3.F5.31.30.30.7.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.F5.26.25.25.1" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.26.25.25.1.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#DCE3F2" stroke="#DCE3F2" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 34,998</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.F5.27.26.26.2" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.27.26.26.2.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#A9C1D9" stroke="#A9C1D9" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S3.F5.28.27.27.3" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.28.27.27.3.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#B6B8D6" stroke="#B6B8D6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 0</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.F5.31.30.30.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.F5.31.30.30.8.1"></span><span class="ltx_text" id="S3.F5.31.30.30.8.2">
<span class="ltx_tabular ltx_align_middle" id="S3.F5.31.30.30.8.2.1">
<span class="ltx_tr" id="S3.F5.31.30.30.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.F5.31.30.30.8.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">VIDAL</span></span>
</span></span><span class="ltx_text" id="S3.F5.31.30.30.8.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.F5.29.28.28.4" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.29.28.28.4.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#D8A6A6" stroke="#D8A6A6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 55,000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.F5.30.29.29.5" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.30.29.29.5.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#E2C2C6" stroke="#E2C2C6" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 300,472</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.F5.31.30.30.6" style="padding-left:2.0pt;padding-right:2.0pt;"><svg class="ltx_picture" height="10.38" id="S3.F5.31.30.30.6.pic1" overflow="visible" version="1.1" width="10.38"><g fill="#DECDB4" stroke="#DECDB4" stroke-width="0.4pt" transform="translate(0,10.38) matrix(1 0 0 -1 0 0)"><path d="M 0 0 M 0 0 L 0 10.38 L 10.38 10.38 L 10.38 0 Z M 10.38 10.38" style="stroke:none"></path></g></svg> 58,968</td>
</tr>
</table>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution of data across different datasets and question types (Caption, Open-ended, and Multi-Choice).</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S3.F6.g1" src="x6.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> (Left) Visualization of the video duration. (Middle) Visualization of the number of words in the video caption. (Right) Visualization of caption length versus video duration.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="427" id="S3.F7.g1" src="x7.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>(Left) Display of YouTube Shorts across four video categories. (Right) Distribution of 5 uniformly chosen video categories. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p2.1">We present the distribution in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F6" title="Figure 6 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>. Our dataset shows a balanced mix across different video sources, providing a varied content selection. For each task type (caption, open-ended question, multiple-choice question), VIDAL (YouTube Shorts) has the highest share at 24.8%, 31.1%, and 30.1% respectively. It is followed by HD-VILA-100M (21.7%, 27.5%, 26.4%) and InternVid-10M (20.3%, 25.6%, 24.6%).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F6" title="Figure 6 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a> (Left) illustrates the distrubtion of the video duration. Video lengths range from 0s to 180s, with each length category containing at least 600 videos. Videos shorter than 50 seconds are numerous, mainly because all videos from VIDAL (24.8% of the dataset), which contains YouTube Shorts with lengths under 45 seconds.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F6" title="Figure 6 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a> (Middle) illustrates the distribution on the number of words for the synthetic captions.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F6" title="Figure 6 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a> (Right) shows how video length correlates with the length of captions. Generally, longer videos feature longer captions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS0.Px1.p4">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p4.1">For each video in <span class="ltx_text" id="S3.SS4.SSS0.Px1.p4.1.1">LLaVA-Video-178K</span>, referencing InsTag <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib33" title="">2023</a>)</cite>, we employ an in-house tagging model to categorize the video content. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F7" title="Figure 7 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a> displays the distribution of ten uniformly sampled video categories, showcasing examples from four of these categories. Among all videos, “comedy” predominates, primarily because YouTube Shorts is one of the most common sources in our dataset. Comedy is a typical genre that tends to attract high view counts—videos with large viewerships are more likely to be collected, as indicated in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F1" title="Figure 1 ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>. Additionally, our dataset includes some domains less represented in current video-language datasets, such as computer games.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S3.T1.2.1">Comparison of <span class="ltx_text" id="S3.T1.2.1.1">LLaVA-Video-178K</span> and other video-language datasets</span>. Average FPS represents the average number of frames per second that are used to prompt GPT-4o/GPT-4V for annotation. ★ VIDAL, WebVid, ActivityNet. ◼ Panda-70M, Pexels, Pixabay, Mixkit, BDD100K, Ego4d. ✪ HD-VILA-100M, Kinetics-700M, Ego4D, VidOR, InternVid, YouCook2, ActivityNet, Sth-sthv2, VIDAL, Charades.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.3">
<tr class="ltx_tr" id="S3.T1.3.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S3.T1.3.1.1" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.2" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.1.2.1">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">Video</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.4" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.1.4.1">#Video</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">Total Video</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">Average</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.7" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.1.7.1">#Caption</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">#OE</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">#MC</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T1.3.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">Source</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">Length</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">FPS</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">QA</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">QA</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.3.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">LLaVA-Hound</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4V</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">★</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">900K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">3Khr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">0.008</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">900K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.8" style="padding-left:5.0pt;padding-right:5.0pt;">900K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.9" style="padding-left:5.0pt;padding-right:5.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">ShareGPT4Video</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4V</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">◼</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">40K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">0.2Khr</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">0.15</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">40K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5" style="background-color:#F5FFFA;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.3.5.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.1.1" style="background-color:#F5FFFA;">LLaVA-Video-178K</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.2.1" style="background-color:#F5FFFA;">GPT-4o</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.3.1" style="background-color:#F5FFFA;">✪</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.4.1" style="background-color:#F5FFFA;">178K</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.5.1" style="background-color:#F5FFFA;">2Khr</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.6.1" style="background-color:#F5FFFA;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.7.1" style="background-color:#F5FFFA;">178K</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.8.1" style="background-color:#F5FFFA;">960K</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.5.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T1.3.5.9.1" style="background-color:#F5FFFA;">196K</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Dataset Comparison</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.3">We provide a comparison of high-quality instruction following video-language datasets, with a focus on synthetic data created with strong AI models, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.T1" title="Table 1 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>.
<math alttext="(i)" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS4.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS4.SSS0.Px2.p1.1.m1.1.2.2"><mo id="S3.SS4.SSS0.Px2.p1.1.m1.1.2.2.1" stretchy="false">(</mo><mi id="S3.SS4.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml">i</mi><mo id="S3.SS4.SSS0.Px2.p1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p1.1.m1.1c">(i)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.p1.1.m1.1d">( italic_i )</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS0.Px2.p1.3.1">A broad collection of dynamic videos.</span> In terms of video sources, although LLaVA-Hound <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib67" title="">2024d</a>)</cite> contains the largest number of videos, 44% of its video data are sourced from WebVid <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib4" title="">2021</a>)</cite>, where most videos are static. ShareGPT4Video <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib7" title="">2024a</a>)</cite> includes 30% of its videos from Pexels, Pixabay, and Mixkit, which are aesthetically good but also mostly static. Additionally, the majority of its videos come from Panda-70M, which are short clips from longer videos—suggesting simpler plots. In contrast, we carefully select video sources that offer dynamic, untrimmed videos with complex plots, which are crucial for developing a powerful video understanding model.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Example videos: <a class="ltx_ref ltx_href" href="https://ak.picdn.net/shutterstock/videos/21179416/preview/stock-footage-aerial-shot-winter-forest.mp4" title="">WebVid</a>,<a class="ltx_ref ltx_href" href="https://pixabay.com/videos/plane-modelling-miniature-lockheed-134519/" title="">Pixabay</a>,<a class="ltx_ref ltx_href" href="https://www.pexels.com/video/a-bird-is-standing-on-the-beach-27916646/" title="">Pexels</a>,<a class="ltx_ref ltx_href" href="https://mixkit.co/free-stock-video/a-young-woman-clad-in-snugly-black-sportswear-doing-lunges-52112/" title="">Mixkit</a>.</span></span></span>
<math alttext="(ii)" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS4.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.cmml"><mo id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.2" stretchy="false" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.2" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml">i</mi><mo id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.1" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.3" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.3" stretchy="false" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1"><times id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.2">𝑖</ci><ci id="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.p1.2.m2.1.1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p1.2.m2.1c">(ii)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.p1.2.m2.1d">( italic_i italic_i )</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS0.Px2.p1.3.2">High frames per second</span>. Regarding frame sampling in language annotations, the proposed datasest considers 1 FPS, while other datasets consider much lower FPS. LLaVA-Hound uniformly samples 10 frames from videos of any length. The average FPS is 0.008, which may miss some fine details. ShareGPT4Video picks key frames using CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib41" title="">2021</a>)</cite> based on frame uniqueness. This method might also miss subtle changes in the video because CLIP embeddings do not capture fine-grained dynamics well. Our method samples FPS=1 without using key frame selection algorithms, ensuring the detailed temproal information can be expressed in annotations and high coverage.
<math alttext="(iii)" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS4.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.cmml"><mo id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.2" stretchy="false" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.cmml"><mi id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.2" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.2.cmml">i</mi><mo id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.1" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.3" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.3.cmml">i</mi><mo id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.1a" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.4" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.4.cmml">i</mi></mrow><mo id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.3" stretchy="false" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1"><times id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.1"></times><ci id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.2">𝑖</ci><ci id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.3">𝑖</ci><ci id="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.4.cmml" xref="S3.SS4.SSS0.Px2.p1.3.m3.1.1.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p1.3.m3.1c">(iii)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.p1.3.m3.1d">( italic_i italic_i italic_i )</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS0.Px2.p1.3.3">Diverse tasks.</span> The proposed dataset considers three common task types, including caption, free-form and closed-form QA, while existing datasets only consider a subset. Meanwhile, the quality and numbers of samples in our dataset is higher.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We conducted evaluations for the LLaVA-Video models across all benchmarks using LMMs-Eval <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib64" title="">2024a</a>)</cite> to ensure standardization and reproducibility. To fairly compare with other leading video LMMs, we primarily used results from original papers. When results were not available, we integrated the models into LMMs-Eval and assessed them under consistent settings. Following LLaVA-OneVision <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>)</cite>, we employed SigLIP <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib62" title="">2023</a>)</cite> as our vision encoder, and Qwen2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib58" title="">2024</a>)</cite> as the LLM. The LLaVA-Video model builds on the single-image (SI) stage checkpoint from the LLaVA-OneVision model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>)</cite>, which was trained using only image data.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Video Representations</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.11">Following the classic SlowFast idea in video representations <cite class="ltx_cite ltx_citemacro_citep">(Feichtenhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib10" title="">2019</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib55" title="">2024b</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib16" title="">2024</a>)</cite>, we develop <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p1.1.1">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.1.1.m1.1"><semantics id="S4.SS0.SSS0.Px1.p1.1.1.m1.1a"><msub id="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1a" xref="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml"></mi><mi id="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1.cmml">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.1.m1.1b"><apply id="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1"><ci id="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.1.m1.1c">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.1.1.m1.1d">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span> to optimize the balance between the number of frames and the count of visual tokens, within the budget of the limited context window in LLM and GPU memory for video representation. Please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A2" title="Appendix B Video Representations ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">B</span></a> for detailed information. Specifically, we represent each video as a sequence with maximum <math alttext="T" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.2.m1.1"><semantics id="S4.SS0.SSS0.Px1.p1.2.m1.1a"><mi id="S4.SS0.SSS0.Px1.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.2.m1.1b"><ci id="S4.SS0.SSS0.Px1.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.2.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.2.m1.1d">italic_T</annotation></semantics></math> frames. Each frame is represented in <math alttext="M" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.3.m2.1"><semantics id="S4.SS0.SSS0.Px1.p1.3.m2.1a"><mi id="S4.SS0.SSS0.Px1.p1.3.m2.1.1" xref="S4.SS0.SSS0.Px1.p1.3.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.3.m2.1b"><ci id="S4.SS0.SSS0.Px1.p1.3.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.3.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.3.m2.1d">italic_M</annotation></semantics></math> tokens. we categorize the frames into two groups, based on the a strike rate <math alttext="s" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.4.m3.1"><semantics id="S4.SS0.SSS0.Px1.p1.4.m3.1a"><mi id="S4.SS0.SSS0.Px1.p1.4.m3.1.1" xref="S4.SS0.SSS0.Px1.p1.4.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.4.m3.1b"><ci id="S4.SS0.SSS0.Px1.p1.4.m3.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.4.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.4.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.4.m3.1d">italic_s</annotation></semantics></math>, where the every <math alttext="s" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.5.m4.1"><semantics id="S4.SS0.SSS0.Px1.p1.5.m4.1a"><mi id="S4.SS0.SSS0.Px1.p1.5.m4.1.1" xref="S4.SS0.SSS0.Px1.p1.5.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.5.m4.1b"><ci id="S4.SS0.SSS0.Px1.p1.5.m4.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.5.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.5.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.5.m4.1d">italic_s</annotation></semantics></math> frames are uniformly selected to form the <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p1.11.2">slow</span> frame group, and the rest of the frames are consdiered as the <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p1.11.3">fast</span> frame group. Note that a special case <math alttext="s=1" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.6.m5.1"><semantics id="S4.SS0.SSS0.Px1.p1.6.m5.1a"><mrow id="S4.SS0.SSS0.Px1.p1.6.m5.1.1" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.2" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.2.cmml">s</mi><mo id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.1" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.1.cmml">=</mo><mn id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.3" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.6.m5.1b"><apply id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1"><eq id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.1"></eq><ci id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.2">𝑠</ci><cn id="S4.SS0.SSS0.Px1.p1.6.m5.1.1.3.cmml" type="integer" xref="S4.SS0.SSS0.Px1.p1.6.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.6.m5.1c">s=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.6.m5.1d">italic_s = 1</annotation></semantics></math> leads to only one group, reducing the SlowFast representation to the original simple representation. For each group, we apply different pooling rate using Pytorch function pooling <math alttext="\mathtt{avg\_pool2d}()" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.7.m6.1"><semantics id="S4.SS0.SSS0.Px1.p1.7.m6.1a"><mrow id="S4.SS0.SSS0.Px1.p1.7.m6.1.1" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.2" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.2.cmml">𝚊𝚟𝚐</mi><mo id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.3" mathvariant="normal" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.3.cmml">_</mi><mo id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1a" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.4" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.4.cmml">𝚙𝚘𝚘𝚕𝟸𝚍</mi><mo id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1b" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1.cmml">⁢</mo><mrow id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.2" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.cmml"><mo id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.2.1" stretchy="false" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.1.cmml">(</mo><mo id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.2.2" stretchy="false" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.7.m6.1b"><apply id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1"><times id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.1"></times><ci id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.2">𝚊𝚟𝚐</ci><ci id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.3">_</ci><ci id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.4.cmml" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.4">𝚙𝚘𝚘𝚕𝟸𝚍</ci><list id="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.1.cmml" xref="S4.SS0.SSS0.Px1.p1.7.m6.1.1.5.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.7.m6.1c">\mathtt{avg\_pool2d}()</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.7.m6.1d">typewriter_avg _ typewriter_pool2d ( )</annotation></semantics></math>. <math alttext="p\times p" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.8.m7.1"><semantics id="S4.SS0.SSS0.Px1.p1.8.m7.1a"><mrow id="S4.SS0.SSS0.Px1.p1.8.m7.1.1" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.2" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.2.cmml">p</mi><mo id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.1.cmml">×</mo><mi id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.3" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.8.m7.1b"><apply id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1"><times id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.1"></times><ci id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.2">𝑝</ci><ci id="S4.SS0.SSS0.Px1.p1.8.m7.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.8.m7.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.8.m7.1c">p\times p</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.8.m7.1d">italic_p × italic_p</annotation></semantics></math> pooling and <math alttext="2p\times 2p" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.9.m8.1"><semantics id="S4.SS0.SSS0.Px1.p1.9.m8.1a"><mrow id="S4.SS0.SSS0.Px1.p1.9.m8.1.1" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.cmml"><mrow id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.cmml"><mrow id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.cmml"><mn id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.2" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.2.cmml">2</mn><mo id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.1" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.3" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.3.cmml">p</mi></mrow><mo id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.1.cmml">×</mo><mn id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.3" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.3.cmml">2</mn></mrow><mo id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.1" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.3" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.9.m8.1b"><apply id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1"><times id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.1"></times><apply id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2"><times id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.1"></times><apply id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2"><times id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.1"></times><cn id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.2.cmml" type="integer" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.2">2</cn><ci id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.3.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.2.3">𝑝</ci></apply><cn id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.3.cmml" type="integer" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.2.3">2</cn></apply><ci id="S4.SS0.SSS0.Px1.p1.9.m8.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.9.m8.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.9.m8.1c">2p\times 2p</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.9.m8.1d">2 italic_p × 2 italic_p</annotation></semantics></math> pooling for slow and fast frames, respectively. To summarize, we paramterize the video representation configuration as <math alttext="\mathcal{V}=(T,M,s,p)" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.10.m9.4"><semantics id="S4.SS0.SSS0.Px1.p1.10.m9.4a"><mrow id="S4.SS0.SSS0.Px1.p1.10.m9.4.5" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.2" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.2.cmml">𝒱</mi><mo id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.1" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.1.cmml">=</mo><mrow id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml"><mo id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2.1" stretchy="false" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml">(</mo><mi id="S4.SS0.SSS0.Px1.p1.10.m9.1.1" xref="S4.SS0.SSS0.Px1.p1.10.m9.1.1.cmml">T</mi><mo id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2.2" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml">,</mo><mi id="S4.SS0.SSS0.Px1.p1.10.m9.2.2" xref="S4.SS0.SSS0.Px1.p1.10.m9.2.2.cmml">M</mi><mo id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2.3" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml">,</mo><mi id="S4.SS0.SSS0.Px1.p1.10.m9.3.3" xref="S4.SS0.SSS0.Px1.p1.10.m9.3.3.cmml">s</mi><mo id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2.4" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml">,</mo><mi id="S4.SS0.SSS0.Px1.p1.10.m9.4.4" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.4.cmml">p</mi><mo id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2.5" stretchy="false" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.10.m9.4b"><apply id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5"><eq id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.1.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.1"></eq><ci id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.2.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.2">𝒱</ci><vector id="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.1.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.5.3.2"><ci id="S4.SS0.SSS0.Px1.p1.10.m9.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.1.1">𝑇</ci><ci id="S4.SS0.SSS0.Px1.p1.10.m9.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.2.2">𝑀</ci><ci id="S4.SS0.SSS0.Px1.p1.10.m9.3.3.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.3.3">𝑠</ci><ci id="S4.SS0.SSS0.Px1.p1.10.m9.4.4.cmml" xref="S4.SS0.SSS0.Px1.p1.10.m9.4.4">𝑝</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.10.m9.4c">\mathcal{V}=(T,M,s,p)</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.10.m9.4d">caligraphic_V = ( italic_T , italic_M , italic_s , italic_p )</annotation></semantics></math>. The total number of tokens is <math alttext="\#tokens=\left\lfloor T/s\right\rfloor\times\left\lfloor M/p^{2}\right\rfloor+%
\left(T-\left\lfloor T/s\right\rfloor\right)\times\left\lfloor M/p^{2}\right\rfloor" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.11.m10.4"><semantics id="S4.SS0.SSS0.Px1.p1.11.m10.4a"><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.4.4" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.cmml"><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.2" mathvariant="normal" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.2.cmml">#</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.3.cmml">t</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1a" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.4" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.4.cmml">o</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1b" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.5" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.5.cmml">k</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1c" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.6" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.6.cmml">e</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1d" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.7" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.7.cmml">n</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1e" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.8" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.8.cmml">s</mi></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.5" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.5.cmml">=</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.cmml"><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.cmml"><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.2.cmml"><mo id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.2.1.cmml">⌊</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.2.cmml">T</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.3.cmml">s</mi></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.3" rspace="0.055em" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.2.1.cmml">⌋</mo></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.3" rspace="0.222em" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.3.cmml">×</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.2.cmml"><mo id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.2.1.cmml">⌊</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.2.cmml">M</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.1.cmml">/</mo><msup id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.2.cmml">p</mi><mn id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.2.1.cmml">⌋</mo></mrow></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.5" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.5.cmml">+</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.cmml"><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.cmml"><mo id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.cmml">(</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.3.cmml">T</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.2.cmml">−</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.2.cmml"><mo id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.2.1.cmml">⌊</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.2.cmml">T</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.1.cmml">/</mo><mi id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.3.cmml">s</mi></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.2.1.cmml">⌋</mo></mrow></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.3" rspace="0.055em" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.cmml">)</mo></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.3" rspace="0.222em" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.3.cmml">×</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.2.cmml"><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.2.1.cmml">⌊</mo><mrow id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.2.cmml">M</mi><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.1" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.1.cmml">/</mo><msup id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.2" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.2.cmml">p</mi><mn id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.3" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.2.1.cmml">⌋</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.11.m10.4b"><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4"><eq id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.5.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.5"></eq><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6"><times id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.1"></times><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.2">#</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.3">𝑡</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.4.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.4">𝑜</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.5.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.5">𝑘</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.6.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.6">𝑒</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.7.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.7">𝑛</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.8.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.6.8">𝑠</ci></apply><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4"><plus id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.5.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.5"></plus><apply id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2"><times id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.3"></times><apply id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1"><floor id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.2"></floor><apply id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1"><divide id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.1"></divide><ci id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.2">𝑇</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.1.1.1.1.1.1.1.3">𝑠</ci></apply></apply><apply id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1"><floor id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.2"></floor><apply id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1"><divide id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.1"></divide><ci id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.2">𝑀</ci><apply id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.2">𝑝</ci><cn id="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.3.cmml" type="integer" xref="S4.SS0.SSS0.Px1.p1.11.m10.2.2.2.2.2.1.1.3.3">2</cn></apply></apply></apply></apply><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4"><times id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.3"></times><apply id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1"><minus id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.2"></minus><ci id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.3">𝑇</ci><apply id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1"><floor id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.2"></floor><apply id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1"><divide id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.1"></divide><ci id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.2">𝑇</ci><ci id="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.3.3.3.3.1.1.1.1.1.1.3">𝑠</ci></apply></apply></apply><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1"><floor id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.2"></floor><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1"><divide id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.1"></divide><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.2">𝑀</ci><apply id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.2">𝑝</ci><cn id="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.3.cmml" type="integer" xref="S4.SS0.SSS0.Px1.p1.11.m10.4.4.4.4.2.1.1.3.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.11.m10.4c">\#tokens=\left\lfloor T/s\right\rfloor\times\left\lfloor M/p^{2}\right\rfloor+%
\left(T-\left\lfloor T/s\right\rfloor\right)\times\left\lfloor M/p^{2}\right\rfloor</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.11.m10.4d"># italic_t italic_o italic_k italic_e italic_n italic_s = ⌊ italic_T / italic_s ⌋ × ⌊ italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⌋ + ( italic_T - ⌊ italic_T / italic_s ⌋ ) × ⌊ italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⌋</annotation></semantics></math></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Benchmarks.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">For full evaluation, we consdier 11 video benchmarks. conducted tests across various video captioning , video open-ended question-answering and video multiple-choice question-answering benchmarks, including ActivityNet-QA <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib59" title="">2019</a>)</cite>, which features human-annotated action-related QA pairs from the ActivityNet dataset. We also utilized LongVideoBench <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib50" title="">2024b</a>)</cite>, EgoSchema <cite class="ltx_cite ltx_citemacro_citep">(Mangalam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib36" title="">2024</a>)</cite>, and MLVU <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib69" title="">2024</a>)</cite> for long video understanding, PerceptionTest <cite class="ltx_cite ltx_citemacro_citep">(Pătrăucean et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib40" title="">2023</a>)</cite> for assessing fine-grained perception skills, and VideoMME <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib11" title="">2024</a>)</cite> and NExT-QA <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>)</cite> for diverse video domains and durations. Additional tests included VideoDetailCaption <cite class="ltx_cite ltx_citemacro_citep">(LMMs-Lab, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib32" title="">2024</a>)</cite> Dream-1K <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib47" title="">2024</a>)</cite> for detailed video descriptions and Video-ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Maaz et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib35" title="">2024</a>)</cite> for visual chat.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p2.1">For ablation studies in . <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS2" title="4.2 Dataset Ablation ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2</span></a> and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.SS3" title="4.3 Dataset Comparison ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we conduct evaluation across 4 datasets. NExT-QA <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>)</cite> and PerceptionTest <cite class="ltx_cite ltx_citemacro_citep">(Pătrăucean et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib40" title="">2023</a>)</cite>, which use training data from the <span class="ltx_text" id="S4.SS0.SSS0.Px2.p2.1.1">LLaVA-Video-178K</span>, are treated as in-domain datasets. Conversely, VideoMME <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib11" title="">2024</a>)</cite> and EgoSchema <cite class="ltx_cite ltx_citemacro_citep">(Mangalam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib36" title="">2024</a>)</cite> are considegreen zero-shot datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overall Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">We fine-tune LLaVA-OneVision (SI) on the joint dataset of video and image data. Specifically, we added video data from the <span class="ltx_text" id="S4.SS1.p1.2.1">LLaVA-Video-178K</span> dataset and four public datasets:
ActivityNet-QA <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib59" title="">2019</a>)</cite>, NExT-QA <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>)</cite>, PerceptionTest <cite class="ltx_cite ltx_citemacro_citep">(Pătrăucean et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib40" title="">2023</a>)</cite>, and LLaVA-Hound-255K <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib67" title="">2024d</a>)</cite>, focusing on videos shorter than three minutes. These datasets were selected to improve our model’s performance, contributing to a total of 1.6 million video-language samples, which include 193,510 video descriptions, 1,241,412 open-ended questions, and 215,625 multiple-choice questions. Remarkably, 92.2% of the video descriptions, 77.4% of the open-ended questions, and 90.9% of the multiple-choice questions were newly annotated. Additionally, we used 1.1 million image-language pairs from the LLaVA-OneVision model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>)</cite>.
We consider the same video representation configurations for the training and inference stages. On 128 NVIDIA H100 GPUs, the video representations for LLaVA-Video-7B and LLaVA-Video-72B are <math alttext="\mathcal{V}=(64,679,1,2)" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.4"><semantics id="S4.SS1.p1.1.m1.4a"><mrow id="S4.SS1.p1.1.m1.4.5" xref="S4.SS1.p1.1.m1.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.1.m1.4.5.2" xref="S4.SS1.p1.1.m1.4.5.2.cmml">𝒱</mi><mo id="S4.SS1.p1.1.m1.4.5.1" xref="S4.SS1.p1.1.m1.4.5.1.cmml">=</mo><mrow id="S4.SS1.p1.1.m1.4.5.3.2" xref="S4.SS1.p1.1.m1.4.5.3.1.cmml"><mo id="S4.SS1.p1.1.m1.4.5.3.2.1" stretchy="false" xref="S4.SS1.p1.1.m1.4.5.3.1.cmml">(</mo><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">64</mn><mo id="S4.SS1.p1.1.m1.4.5.3.2.2" xref="S4.SS1.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">679</mn><mo id="S4.SS1.p1.1.m1.4.5.3.2.3" xref="S4.SS1.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS1.p1.1.m1.3.3" xref="S4.SS1.p1.1.m1.3.3.cmml">1</mn><mo id="S4.SS1.p1.1.m1.4.5.3.2.4" xref="S4.SS1.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS1.p1.1.m1.4.4" xref="S4.SS1.p1.1.m1.4.4.cmml">2</mn><mo id="S4.SS1.p1.1.m1.4.5.3.2.5" stretchy="false" xref="S4.SS1.p1.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.4b"><apply id="S4.SS1.p1.1.m1.4.5.cmml" xref="S4.SS1.p1.1.m1.4.5"><eq id="S4.SS1.p1.1.m1.4.5.1.cmml" xref="S4.SS1.p1.1.m1.4.5.1"></eq><ci id="S4.SS1.p1.1.m1.4.5.2.cmml" xref="S4.SS1.p1.1.m1.4.5.2">𝒱</ci><vector id="S4.SS1.p1.1.m1.4.5.3.1.cmml" xref="S4.SS1.p1.1.m1.4.5.3.2"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1">64</cn><cn id="S4.SS1.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS1.p1.1.m1.2.2">679</cn><cn id="S4.SS1.p1.1.m1.3.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.3.3">1</cn><cn id="S4.SS1.p1.1.m1.4.4.cmml" type="integer" xref="S4.SS1.p1.1.m1.4.4">2</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.4c">\mathcal{V}=(64,679,1,2)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.4d">caligraphic_V = ( 64 , 679 , 1 , 2 )</annotation></semantics></math> and <math alttext="\mathcal{V}=(64,679,3,2)" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.4"><semantics id="S4.SS1.p1.2.m2.4a"><mrow id="S4.SS1.p1.2.m2.4.5" xref="S4.SS1.p1.2.m2.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.2.m2.4.5.2" xref="S4.SS1.p1.2.m2.4.5.2.cmml">𝒱</mi><mo id="S4.SS1.p1.2.m2.4.5.1" xref="S4.SS1.p1.2.m2.4.5.1.cmml">=</mo><mrow id="S4.SS1.p1.2.m2.4.5.3.2" xref="S4.SS1.p1.2.m2.4.5.3.1.cmml"><mo id="S4.SS1.p1.2.m2.4.5.3.2.1" stretchy="false" xref="S4.SS1.p1.2.m2.4.5.3.1.cmml">(</mo><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">64</mn><mo id="S4.SS1.p1.2.m2.4.5.3.2.2" xref="S4.SS1.p1.2.m2.4.5.3.1.cmml">,</mo><mn id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">679</mn><mo id="S4.SS1.p1.2.m2.4.5.3.2.3" xref="S4.SS1.p1.2.m2.4.5.3.1.cmml">,</mo><mn id="S4.SS1.p1.2.m2.3.3" xref="S4.SS1.p1.2.m2.3.3.cmml">3</mn><mo id="S4.SS1.p1.2.m2.4.5.3.2.4" xref="S4.SS1.p1.2.m2.4.5.3.1.cmml">,</mo><mn id="S4.SS1.p1.2.m2.4.4" xref="S4.SS1.p1.2.m2.4.4.cmml">2</mn><mo id="S4.SS1.p1.2.m2.4.5.3.2.5" stretchy="false" xref="S4.SS1.p1.2.m2.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.4b"><apply id="S4.SS1.p1.2.m2.4.5.cmml" xref="S4.SS1.p1.2.m2.4.5"><eq id="S4.SS1.p1.2.m2.4.5.1.cmml" xref="S4.SS1.p1.2.m2.4.5.1"></eq><ci id="S4.SS1.p1.2.m2.4.5.2.cmml" xref="S4.SS1.p1.2.m2.4.5.2">𝒱</ci><vector id="S4.SS1.p1.2.m2.4.5.3.1.cmml" xref="S4.SS1.p1.2.m2.4.5.3.2"><cn id="S4.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p1.2.m2.1.1">64</cn><cn id="S4.SS1.p1.2.m2.2.2.cmml" type="integer" xref="S4.SS1.p1.2.m2.2.2">679</cn><cn id="S4.SS1.p1.2.m2.3.3.cmml" type="integer" xref="S4.SS1.p1.2.m2.3.3">3</cn><cn id="S4.SS1.p1.2.m2.4.4.cmml" type="integer" xref="S4.SS1.p1.2.m2.4.4">2</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.4c">\mathcal{V}=(64,679,3,2)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.4d">caligraphic_V = ( 64 , 679 , 3 , 2 )</annotation></semantics></math>, respectively.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text" id="S4.T2.2.1">LLaVA-Video</span> performance on video benchmarks. We report the score out of 5 for VideoDC, VideoChatGPT while other results are reported in accuracy. All results are reported as 0-shot accuracy. *indicates that the training set has been observed in our data mixture.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.3">
<tr class="ltx_tr" id="S4.T2.3.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" id="S4.T2.3.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.3.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">Caption</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.3.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">Open-Ended Q&amp;A</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="7" id="S4.T2.3.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">Multi-Choice Q&amp;A</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.2.1" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.1.1">Model</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.2.1" style="width:6.9pt;height:40.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.1pt;transform:translate(-16.6pt,-16.6pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.1.1.1">VideoDC</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.3.1" style="width:6.8pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.4pt;transform:translate(-19.31pt,-19.31pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.3.1.1.1">Dream-1K</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.4.1" style="width:8.8pt;height:50.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:50.3pt;transform:translate(-20.75pt,-19.78pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.4.1.1.1">ActNet-QA</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.2.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.5.1" style="width:6.9pt;height:68.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:68.8pt;transform:translate(-30.94pt,-30.94pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.5.1.1.1">VideoChatGPT</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.6.1" style="width:8.9pt;height:49.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:49.9pt;transform:translate(-20.49pt,-19.51pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.6.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.6.1.1.1">EgoSchema</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.7.1" style="width:6.8pt;height:29.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:29.3pt;transform:translate(-11.24pt,-11.24pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.7.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.7.1.1.1">MLVU</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.8.1" style="width:6.9pt;height:43.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:43.5pt;transform:translate(-18.26pt,-18.26pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.8.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.8.1.1.1">MVBench</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.9.1" style="width:8.8pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.4pt;transform:translate(-18.32pt,-17.35pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.9.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.9.1.1.1">NExT-QA</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.10.1" style="width:8.8pt;height:65.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.2pt;transform:translate(-28.22pt,-27.25pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.10.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.10.1.1.1">PerceptionTest</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.11" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.11.1" style="width:8.9pt;height:73.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:73.9pt;transform:translate(-32.5pt,-31.53pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.11.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.11.1.1.1">LongVideoBench</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.2.12" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.2.12.1" style="width:6.9pt;height:50.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:50.4pt;transform:translate(-21.74pt,-21.74pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T2.3.2.12.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.12.1.1.1">VideoMME</span></p>
</span></div>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.4" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.5" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.6" style="padding-left:1.0pt;padding-right:1.0pt;">m-avg</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.7" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.8" style="padding-left:1.0pt;padding-right:1.0pt;">mc</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.9" style="padding-left:1.0pt;padding-right:1.0pt;">val</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.10" style="padding-left:1.0pt;padding-right:1.0pt;">val</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.11" style="padding-left:1.0pt;padding-right:1.0pt;">wo/w-subs</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="11" id="S4.T2.3.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.4.1.1">Proprietary models</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.3.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5" style="background-color:#EDEDED;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.5.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.1.1" style="background-color:#EDEDED;">GPT-4V <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib38" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.1" style="background-color:#EDEDED;">4.00</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.5.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.3.1" style="background-color:#EDEDED;">34.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.4.1" style="background-color:#EDEDED;">57.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.5.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.5.1" style="background-color:#EDEDED;">4.06</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.6.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.7.1" style="background-color:#EDEDED;">49.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.8.1" style="background-color:#EDEDED;">43.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.9.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.10.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.11" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.11.1" style="background-color:#EDEDED;">61.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.5.12" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.5.12.1" style="background-color:#EDEDED;">59.9/63.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6" style="background-color:#EDEDED;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.6.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.1.1" style="background-color:#EDEDED;">GPT-4o <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib39" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.2.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.6.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.1" style="background-color:#EDEDED;">39.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.4.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.6.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.5.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.6.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.7.1" style="background-color:#EDEDED;">64.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.8.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.9.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.10.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.11" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.11.1" style="background-color:#EDEDED;">66.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.12" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.6.12.1" style="background-color:#EDEDED;">71.9/77.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.7" style="background-color:#EDEDED;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.7.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.1.1" style="background-color:#EDEDED;">Gemini-1.5-Flash <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib46" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.2.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.7.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.3.1" style="background-color:#EDEDED;">34.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.1" style="background-color:#EDEDED;">55.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.7.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.5.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.6.1" style="background-color:#EDEDED;">65.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.7.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.8.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.9.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.10.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.11" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.11.1" style="background-color:#EDEDED;">61.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.7.12" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.7.12.1" style="background-color:#EDEDED;">70.3/75.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8" style="background-color:#EDEDED;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.8.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.1.1" style="background-color:#EDEDED;">Gemini-1.5-Pro <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib46" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.2.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.8.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.3.1" style="background-color:#EDEDED;">36.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.4.1" style="background-color:#EDEDED;">57.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.8.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.6.1" style="background-color:#EDEDED;">72.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.7.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.8.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.9.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.10.1" style="background-color:#EDEDED;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.11" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.11.1" style="background-color:#EDEDED;">64.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.12" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.8.12.1" style="background-color:#EDEDED;">75.0/81.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="11" id="S4.T2.3.9.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.9.1.1">Open-source models</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.3.9.2" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.10.1" style="padding-left:1.0pt;padding-right:1.0pt;">VILA-40B <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib29" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.2" style="padding-left:1.0pt;padding-right:1.0pt;">3.37</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.10.3" style="padding-left:1.0pt;padding-right:1.0pt;">33.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.4" style="padding-left:1.0pt;padding-right:1.0pt;">58.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.10.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.36</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.6" style="padding-left:1.0pt;padding-right:1.0pt;">58.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.7" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.8" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.9" style="padding-left:1.0pt;padding-right:1.0pt;">67.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.10" style="padding-left:1.0pt;padding-right:1.0pt;">54.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.11" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.10.12" style="padding-left:1.0pt;padding-right:1.0pt;">60.1/61.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.11.1" style="padding-left:1.0pt;padding-right:1.0pt;">PLLaVA-34B <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib54" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.2" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.11.3" style="padding-left:1.0pt;padding-right:1.0pt;">28.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.4" style="padding-left:1.0pt;padding-right:1.0pt;">60.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.11.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.48</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.6" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.7" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.8" style="padding-left:1.0pt;padding-right:1.0pt;">58.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.9" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.10" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.11" style="padding-left:1.0pt;padding-right:1.0pt;">53.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.12" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.12">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.12.1" style="padding-left:1.0pt;padding-right:1.0pt;">LongVA-7B <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib66" title="">2024c</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.2" style="padding-left:1.0pt;padding-right:1.0pt;">3.14</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.12.3" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.4" style="padding-left:1.0pt;padding-right:1.0pt;">50.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.12.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.20</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.6" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.7" style="padding-left:1.0pt;padding-right:1.0pt;">56.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.8" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.9" style="padding-left:1.0pt;padding-right:1.0pt;">68.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.10" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.11" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.12.12" style="padding-left:1.0pt;padding-right:1.0pt;">52.6/54.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.13">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.13.1" style="padding-left:1.0pt;padding-right:1.0pt;">IXC-2.5-7B <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib65" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.2" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.13.3" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.4" style="padding-left:1.0pt;padding-right:1.0pt;">52.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.13.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.46</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.6" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.7" style="padding-left:1.0pt;padding-right:1.0pt;">37.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.8" style="padding-left:1.0pt;padding-right:1.0pt;">69.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.9" style="padding-left:1.0pt;padding-right:1.0pt;">71.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.10" style="padding-left:1.0pt;padding-right:1.0pt;">34.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.11" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.13.12" style="padding-left:1.0pt;padding-right:1.0pt;">55.8/58.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.14">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.14.1" style="padding-left:1.0pt;padding-right:1.0pt;">LLaVA-OV-7B <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.2" style="padding-left:1.0pt;padding-right:1.0pt;">3.75</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.14.3" style="padding-left:1.0pt;padding-right:1.0pt;">31.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.4" style="padding-left:1.0pt;padding-right:1.0pt;">56.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.14.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.51</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.6" style="padding-left:1.0pt;padding-right:1.0pt;">60.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.7" style="padding-left:1.0pt;padding-right:1.0pt;">64.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.8" style="padding-left:1.0pt;padding-right:1.0pt;">56.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.9" style="padding-left:1.0pt;padding-right:1.0pt;">79.4*</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.10" style="padding-left:1.0pt;padding-right:1.0pt;">57.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.11" style="padding-left:1.0pt;padding-right:1.0pt;">56.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.12" style="padding-left:1.0pt;padding-right:1.0pt;">58.2/61.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.15">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.15.1" style="padding-left:1.0pt;padding-right:1.0pt;">VideoLLaMA2-72B <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib9" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.2" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.15.3" style="padding-left:1.0pt;padding-right:1.0pt;">27.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.4" style="padding-left:1.0pt;padding-right:1.0pt;">55.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.15.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.16</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.6" style="padding-left:1.0pt;padding-right:1.0pt;">63.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.7" style="padding-left:1.0pt;padding-right:1.0pt;">61.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.8" style="padding-left:1.0pt;padding-right:1.0pt;">62.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.9" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.10" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.11" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.15.12" style="padding-left:1.0pt;padding-right:1.0pt;">61.4/63.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.16">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.3.16.1" style="padding-left:1.0pt;padding-right:1.0pt;">LLaVA-OV-72B <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.2" style="padding-left:1.0pt;padding-right:1.0pt;">3.60</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.16.3" style="padding-left:1.0pt;padding-right:1.0pt;">33.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.4" style="padding-left:1.0pt;padding-right:1.0pt;">62.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.16.5" style="padding-left:1.0pt;padding-right:1.0pt;">3.62</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.6" style="padding-left:1.0pt;padding-right:1.0pt;">62.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.7" style="padding-left:1.0pt;padding-right:1.0pt;">68.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.8" style="padding-left:1.0pt;padding-right:1.0pt;">59.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.9" style="padding-left:1.0pt;padding-right:1.0pt;">80.2*</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.10" style="padding-left:1.0pt;padding-right:1.0pt;">66.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.11" style="padding-left:1.0pt;padding-right:1.0pt;">61.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.12" style="padding-left:1.0pt;padding-right:1.0pt;">66.2/69.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.17" style="background-color:#F5FFFA;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.3.17.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.3.17.1.1" style="background-color:#F5FFFA;">LLaVA-Video</span><span class="ltx_text" id="S4.T2.3.17.1.2" style="background-color:#F5FFFA;">-7B</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.2.1" style="background-color:#F5FFFA;">3.66</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.17.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.3.1" style="background-color:#F5FFFA;">32.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.4.1" style="background-color:#F5FFFA;">56.5*</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.17.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.5.1" style="background-color:#F5FFFA;">3.52</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.6.1" style="background-color:#F5FFFA;">57.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.7.1" style="background-color:#F5FFFA;">70.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.8.1" style="background-color:#F5FFFA;">58.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.9.1" style="background-color:#F5FFFA;">83.2*</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.10.1" style="background-color:#F5FFFA;">67.9*</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.11" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.11.1" style="background-color:#F5FFFA;">58.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.12" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.17.12.1" style="background-color:#F5FFFA;">63.3/69.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.18" style="background-color:#F5FFFA;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.3.18.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.3.18.1.1" style="background-color:#F5FFFA;">LLaVA-Video</span><span class="ltx_text" id="S4.T2.3.18.1.2" style="background-color:#F5FFFA;">-72B</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.2.1" style="background-color:#F5FFFA;">3.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.18.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.3.1" style="background-color:#F5FFFA;">34.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.4.1" style="background-color:#F5FFFA;">63.4*</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.18.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.5.1" style="background-color:#F5FFFA;">3.62</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.6.1" style="background-color:#F5FFFA;">65.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.7.1" style="background-color:#F5FFFA;">74.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.8.1" style="background-color:#F5FFFA;">64.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.9.1" style="background-color:#F5FFFA;">85.4*</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.10.1" style="background-color:#F5FFFA;">74.3*</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.11" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.11.1" style="background-color:#F5FFFA;">61.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.18.12" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.3.18.12.1" style="background-color:#F5FFFA;">70.5/76.9</span></td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.T2" title="Table 2 ‣ 4.1 Overall Results ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare the performance of different models on various video benchmarks. The 72B model performs as well as the commercial, closed-source model Gemini-1.5-Flash <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib46" title="">2023</a>)</cite>, highlighting the effectiveness of open-source efforts in achieving comparable results. The <span class="ltx_text" id="S4.SS1.p2.1.1">LLaVA-Video</span>-7B model outperforms the previous top model, LLaVA-OV-7B, in seven out of ten datasets. Analysis of individual datasets shows some noteworthy trends. For instance, on benchmarks like MLVU, LongVideoBench, and VideoMME, which primarily use video data from YouTube, this improvement may be due to the inclusion of extensive YouTube data in <span class="ltx_text" id="S4.SS1.p2.1.2">LLaVA-Video-178K</span>, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F5" title="Figure 5 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>. Additionally, the improvement on ActivityNet-QA is small; this could be because many questions in ActivityNet-QA, such as “What’s the color of the ball?” can be answered by viewing a single frame. The visibility of the ball from the beginning to the end of the video means understanding the video sequence is unnecessary, so <span class="ltx_text" id="S4.SS1.p2.1.3">LLaVA-Video-178K</span> offers little advantage in this context.
We find that <span class="ltx_text" id="S4.SS1.p2.1.4">LLaVA-Video</span>-7B is notably weaker in the specialized task of EgoSchema, an ego-centric dataset. This weakness may be due to a significant reduction in the proportion of ego-centric data in the training dataset of <span class="ltx_text" id="S4.SS1.p2.1.5">LLaVA-Video</span>. However, this impact is less pronounced in larger models, as demonstrated by the <span class="ltx_text" id="S4.SS1.p2.1.6">LLaVA-Video</span>-72B model’s superior performance over LLaVA-OV-72B in EgoSchema.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Dataset Ablation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Note that the training set for <span class="ltx_text" id="S4.SS2.p1.1.1">LLaVA-Video</span> includes six datasets: <span class="ltx_text" id="S4.SS2.p1.1.2">LLaVA-Video-178K</span>, LLaVA-Hound <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib67" title="">2024d</a>)</cite>, NExT-QA <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>)</cite>, ActivityNet-QA <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib59" title="">2019</a>)</cite>, PerceptionTest <cite class="ltx_cite ltx_citemacro_citep">(Pătrăucean et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib40" title="">2023</a>)</cite>, and image data from LLaVA-OneVision <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib25" title="">2024c</a>)</cite>. In this section, we conduct ablation studies to assess the impact of each dataset. We separately fine-tune the LLaVA-OneVision (SI) model for each experimental setting, progressively adding datasets to the baseline. We use a video representation defined by <math alttext="\mathcal{V}=(64,679,1,2)" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.4"><semantics id="S4.SS2.p1.1.m1.4a"><mrow id="S4.SS2.p1.1.m1.4.5" xref="S4.SS2.p1.1.m1.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.1.m1.4.5.2" xref="S4.SS2.p1.1.m1.4.5.2.cmml">𝒱</mi><mo id="S4.SS2.p1.1.m1.4.5.1" xref="S4.SS2.p1.1.m1.4.5.1.cmml">=</mo><mrow id="S4.SS2.p1.1.m1.4.5.3.2" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml"><mo id="S4.SS2.p1.1.m1.4.5.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">(</mo><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">64</mn><mo id="S4.SS2.p1.1.m1.4.5.3.2.2" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">679</mn><mo id="S4.SS2.p1.1.m1.4.5.3.2.3" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.3.3" xref="S4.SS2.p1.1.m1.3.3.cmml">1</mn><mo id="S4.SS2.p1.1.m1.4.5.3.2.4" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.4.4" xref="S4.SS2.p1.1.m1.4.4.cmml">2</mn><mo id="S4.SS2.p1.1.m1.4.5.3.2.5" stretchy="false" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.4b"><apply id="S4.SS2.p1.1.m1.4.5.cmml" xref="S4.SS2.p1.1.m1.4.5"><eq id="S4.SS2.p1.1.m1.4.5.1.cmml" xref="S4.SS2.p1.1.m1.4.5.1"></eq><ci id="S4.SS2.p1.1.m1.4.5.2.cmml" xref="S4.SS2.p1.1.m1.4.5.2">𝒱</ci><vector id="S4.SS2.p1.1.m1.4.5.3.1.cmml" xref="S4.SS2.p1.1.m1.4.5.3.2"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">64</cn><cn id="S4.SS2.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.2.2">679</cn><cn id="S4.SS2.p1.1.m1.3.3.cmml" type="integer" xref="S4.SS2.p1.1.m1.3.3">1</cn><cn id="S4.SS2.p1.1.m1.4.4.cmml" type="integer" xref="S4.SS2.p1.1.m1.4.4">2</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.4c">\mathcal{V}=(64,679,1,2)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.4d">caligraphic_V = ( 64 , 679 , 1 , 2 )</annotation></semantics></math></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.T3" title="Table 3 ‣ 4.2 Dataset Ablation ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>. Initially, we used a basic model trained solely on the LLaVA-Hound dataset as our baseline. Compared to this baseline, adding the <span class="ltx_text" id="S4.SS2.p2.1.1">LLaVA-Video-178K</span> dataset significantly improved performance, enhancing scores in both in-domain and out-of-domain tasks. Specifically, we observed a 31.9-point increase in NExT-QA scores and a 9.1-point rise in VideoMME scores. Furthermore, including the PerceptionTest dataset significantly enhanced its associated task. Additionally, integrating high-quality image data provided modest benefits on EgoSchema.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study on the <span class="ltx_text" id="S4.T3.2.1">LLaVA-Video</span> model with various configurations of training data. Three Q&amp;A datasets indicate: NExT-QA, ActivityNet-QA and PerceptionTest.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.3">
<tr class="ltx_tr" id="S4.T3.3.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T3.3.1.1" style="padding-left:15.0pt;padding-right:15.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.3.1.2" style="padding-left:15.0pt;padding-right:15.0pt;">in-domain</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.3.1.3" style="padding-left:15.0pt;padding-right:15.0pt;">out-of-domain</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.3.2.1" rowspan="2" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text" id="S4.T3.3.2.1.1">Method</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.2.2" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.2.1">NExT-QA</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.2.3" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.3.1">PerceptionTest</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.2.4" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.4.1">EgoSchema</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.2.5" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.2.5.1">VideoMME</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.1" style="padding-left:15.0pt;padding-right:15.0pt;">mc</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.2" style="padding-left:15.0pt;padding-right:15.0pt;">val</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.3.3" style="padding-left:15.0pt;padding-right:15.0pt;">test</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.3.3.4" style="padding-left:15.0pt;padding-right:15.0pt;">wo</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.3.4.1" style="padding-left:15.0pt;padding-right:15.0pt;">LLaVA-Hound</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.2" style="padding-left:15.0pt;padding-right:15.0pt;">48.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.3" style="padding-left:15.0pt;padding-right:15.0pt;">51.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.3.4.4" style="padding-left:15.0pt;padding-right:15.0pt;">51.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.3.4.5" style="padding-left:15.0pt;padding-right:15.0pt;">54.1</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.3.5.1" style="padding-left:15.0pt;padding-right:15.0pt;">+<span class="ltx_text" id="S4.T3.3.5.1.1">LLaVA-Video-178K</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.5.2" style="padding-left:15.0pt;padding-right:15.0pt;">80.1</td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.5.3" style="padding-left:15.0pt;padding-right:15.0pt;">57.1</td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.5.4" style="padding-left:15.0pt;padding-right:15.0pt;">56.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.5.5" style="padding-left:15.0pt;padding-right:15.0pt;">63.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.3.6.1" style="padding-left:15.0pt;padding-right:15.0pt;">+Three Q&amp;A datasets</td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.6.2" style="padding-left:15.0pt;padding-right:15.0pt;">80.1</td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.6.3" style="padding-left:15.0pt;padding-right:15.0pt;">69.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.3.6.4" style="padding-left:15.0pt;padding-right:15.0pt;">55.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.3.6.5" style="padding-left:15.0pt;padding-right:15.0pt;">61.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.3.7.1" style="padding-left:15.0pt;padding-right:15.0pt;">+LLaVA-OV (images)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.7.2" style="padding-left:15.0pt;padding-right:15.0pt;">83.2</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.7.3" style="padding-left:15.0pt;padding-right:15.0pt;">67.9</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.3.7.4" style="padding-left:15.0pt;padding-right:15.0pt;">57.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.3.7.5" style="padding-left:15.0pt;padding-right:15.0pt;">63.4</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Dataset Comparison</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We conduct two ablation studies to further analyze our dataset and training strategy. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.T4" title="Table 4 ‣ 4.3 Dataset Comparison ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a>, we compared three datasets where the language annotations are from GPT-4V/GPT-4o. For each experiment, we fine-tune the LLaVA-OneVision (SI) model separately on each specific dataset setting, utilizing a video representation defined by <math alttext="\mathcal{V}=(64,679,1,2)" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.4"><semantics id="S4.SS3.p1.1.m1.4a"><mrow id="S4.SS3.p1.1.m1.4.5" xref="S4.SS3.p1.1.m1.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p1.1.m1.4.5.2" xref="S4.SS3.p1.1.m1.4.5.2.cmml">𝒱</mi><mo id="S4.SS3.p1.1.m1.4.5.1" xref="S4.SS3.p1.1.m1.4.5.1.cmml">=</mo><mrow id="S4.SS3.p1.1.m1.4.5.3.2" xref="S4.SS3.p1.1.m1.4.5.3.1.cmml"><mo id="S4.SS3.p1.1.m1.4.5.3.2.1" stretchy="false" xref="S4.SS3.p1.1.m1.4.5.3.1.cmml">(</mo><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">64</mn><mo id="S4.SS3.p1.1.m1.4.5.3.2.2" xref="S4.SS3.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.1.m1.2.2" xref="S4.SS3.p1.1.m1.2.2.cmml">679</mn><mo id="S4.SS3.p1.1.m1.4.5.3.2.3" xref="S4.SS3.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.1.m1.3.3" xref="S4.SS3.p1.1.m1.3.3.cmml">1</mn><mo id="S4.SS3.p1.1.m1.4.5.3.2.4" xref="S4.SS3.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.1.m1.4.4" xref="S4.SS3.p1.1.m1.4.4.cmml">2</mn><mo id="S4.SS3.p1.1.m1.4.5.3.2.5" stretchy="false" xref="S4.SS3.p1.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.4b"><apply id="S4.SS3.p1.1.m1.4.5.cmml" xref="S4.SS3.p1.1.m1.4.5"><eq id="S4.SS3.p1.1.m1.4.5.1.cmml" xref="S4.SS3.p1.1.m1.4.5.1"></eq><ci id="S4.SS3.p1.1.m1.4.5.2.cmml" xref="S4.SS3.p1.1.m1.4.5.2">𝒱</ci><vector id="S4.SS3.p1.1.m1.4.5.3.1.cmml" xref="S4.SS3.p1.1.m1.4.5.3.2"><cn id="S4.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1">64</cn><cn id="S4.SS3.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.2.2">679</cn><cn id="S4.SS3.p1.1.m1.3.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.3.3">1</cn><cn id="S4.SS3.p1.1.m1.4.4.cmml" type="integer" xref="S4.SS3.p1.1.m1.4.4">2</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.4c">\mathcal{V}=(64,679,1,2)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.4d">caligraphic_V = ( 64 , 679 , 1 , 2 )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Two group of experiments are considered to assess the data quality of <span class="ltx_text" id="S4.SS3.p2.1.1">LLaVA-Video-178K</span> compare to LLaVA-Hound and ShareGPT4Video. In the first group, to compare <span class="ltx_text" id="S4.SS3.p2.1.2">LLaVA-Video-178K</span> with LLaVA-Hound, we randomly selected 900K open-ended questions to match the number in LLaVA-Hound. We included all captions and did not sample the multiple-choice questions. In the second group, comparing <span class="ltx_text" id="S4.SS3.p2.1.3">LLaVA-Video-178K</span> to ShareGPT4Video, we randomly sampled 40K video captions to align with those in ShareGPT4Video. Since ShareGPT4Video lacks open-ended and multiple-choice questions, we supplemented with annotations from NExT-QA, PerceptionTest, and ActivityNet-QA.
In the first group of Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.T4" title="Table 4 ‣ 4.3 Dataset Comparison ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare <span class="ltx_text" id="S4.SS3.p2.1.4">LLaVA-Video-178K</span> with LLaVA-Hound. Although LLaVA-Hound has more captions than <span class="ltx_text" id="S4.SS3.p2.1.5">LLaVA-Video-178K</span>, our results are still better. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.T1" title="Table 1 ‣ Overview. ‣ 3.4 Dataset Statistics ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>, despite LLaVA-Hound annotates more videos, its quality is limited due to two main issues: (1) Static video: Its primary video source is WebVid <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib4" title="">2021</a>)</cite>, which tends to have relatively static content. (2) Sparse sampling: Although it includes data sources with dynamic videos, its sampling rate of 10 frames per video leads to annotations that do not fully capture the complete plot of the video. This underscores that the quality of video instruction-following data is more important than its quantity. Additionally, the second experiment group in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S4.T4" title="Table 4 ‣ 4.3 Dataset Comparison ‣ 4 Experiments ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the model trained with <span class="ltx_text" id="S4.SS3.p2.1.6">LLaVA-Video-178K</span> outperforms that of ShareGPT4Video, highlighting the superiority of our data’s quality.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of <span class="ltx_text" id="S4.T4.2.1">LLaVA-Video-178K</span> and other video instruction-following datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.3">
<tr class="ltx_tr" id="S4.T4.3.1">
<td class="ltx_td ltx_border_tt" id="S4.T4.3.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.3.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.3.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T4.3.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.3.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">in-domain</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.3.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">out-of-domain</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.2">
<td class="ltx_td" id="S4.T4.3.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.2.2" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.2.1">#Caption</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.2.3" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.3.1">#OE</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.3.2.4" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.4.1">#MC</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.5.1">NExT-QA</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.6.1">PerceptionTest</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.2.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.7.1">EgoSchema</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.3.2.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.8.1">VideoMME</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3">
<td class="ltx_td" id="S4.T4.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">mc</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">val</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">test</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">wo</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">LLaVA-Hound</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">900K</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">900k</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">64.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">51.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">51.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.3.4.8" style="padding-left:3.0pt;padding-right:3.0pt;">51.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.5">
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T4.3.5.1.1">LLaVA-Video-178K</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">178K</td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">900k</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.3.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">73.2 <span class="ltx_text" id="S4.T4.3.5.5.1" style="color:#00FF00;">(+8.8)</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">55.9 <span class="ltx_text" id="S4.T4.3.5.6.1" style="color:#00FF00;">(+4.5)</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">49.8 <span class="ltx_text" id="S4.T4.3.5.7.1" style="color:#FF0000;">(-1.2)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.3.5.8" style="padding-left:3.0pt;padding-right:3.0pt;">59.6 <span class="ltx_text" id="S4.T4.3.5.8.1" style="color:#00FF00;">(+8.6)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">ShareGPT4Video</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">40K</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">40K</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.3.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">19K</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">69.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">55.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">58.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.3.6.8" style="padding-left:3.0pt;padding-right:3.0pt;">51.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.3.7.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T4.3.7.1.1">LLaVA-Video-178K</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.3.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">40K</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.3.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">40K</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.3.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">19K</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.3.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">75.8 <span class="ltx_text" id="S4.T4.3.7.5.1" style="color:#00FF00;">(+6.2)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.3.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">55.4 <span class="ltx_text" id="S4.T4.3.7.6.1" style="color:#00FF00;">(+0.2)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.3.7.7" style="padding-left:3.0pt;padding-right:3.0pt;">55.8 <span class="ltx_text" id="S4.T4.3.7.7.1" style="color:#FF0000;">(-3.1)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T4.3.7.8" style="padding-left:3.0pt;padding-right:3.0pt;">53.5 <span class="ltx_text" id="S4.T4.3.7.8.1" style="color:#00FF00;">(+2.5)</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This study introduces the <span class="ltx_text" id="S5.p1.1.1">LLaVA-Video-178K</span> dataset, a high-quality synthetic dataset for video-language instruction-following. It is favored for its dense frame sampling rate in longer, untrimmed videos, covering diverse tasks such as captioning, open-ended and multi-choice QA. By training on the joint dataset of <span class="ltx_text" id="S5.p1.1.2">LLaVA-Video-178K</span> with existing visual instruction tuning data, we developed a new model family, <span class="ltx_text" id="S5.p1.1.3">LLaVA-Video</span>, which also considers video representation to effectively use GPU resources. This allows us to include more frames in the training process. The experimental results have demonstrated the effectiveness of the proposed synthetic dataset, and <span class="ltx_text" id="S5.p1.1.4">LLaVA-Video</span> models have achieved excellent performance on a wide range of video benchmarks.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.14198" title="">https://arxiv.org/abs/2204.14198</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anne Hendricks et al. (2017a)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.

</span>
<span class="ltx_bibblock">Localizing moments in video with natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE international conference on computer vision</em>, pp.  5803–5812, 2017a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anne Hendricks et al. (2017b)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.

</span>
<span class="ltx_bibblock">Localizing moments in video with natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE international conference on computer vision</em>, pp.  5803–5812, 2017b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain et al. (2021)</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Frozen in time: A joint video and image encoder for end-to-end retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE International Conference on Computer Vision</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caba Heilbron et al. (2015)</span>
<span class="ltx_bibblock">
Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.

</span>
<span class="ltx_bibblock">Activitynet: A large-scale video benchmark for human activity understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the ieee conference on computer vision and pattern recognition</em>, pp.  961–970, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen &amp; Dolan (2011)</span>
<span class="ltx_bibblock">
David Chen and William B Dolan.

</span>
<span class="ltx_bibblock">Collecting highly parallel data for paraphrase evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</em>, pp.  190–200, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Sharegpt4video: Improving video understanding and generation with better captions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2406.04325</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov.

</span>
<span class="ltx_bibblock">Panda-70m: Captioning 70m videos with multiple cross-modality teachers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2402.19479</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2024)</span>
<span class="ltx_bibblock">
Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing.

</span>
<span class="ltx_bibblock">Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.07476" title="">https://arxiv.org/abs/2406.07476</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer et al. (2019)</span>
<span class="ltx_bibblock">
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.

</span>
<span class="ltx_bibblock">Slowfast networks for video recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  6202–6211, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2024)</span>
<span class="ltx_bibblock">
Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al.

</span>
<span class="ltx_bibblock">Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2405.21075</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.

</span>
<span class="ltx_bibblock">The" something something" video database for learning and evaluating visual common sense.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE international conference on computer vision</em>, pp.  5842–5850, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al. (2022)</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.

</span>
<span class="ltx_bibblock">Ego4d: Around the world in 3,000 hours of egocentric video.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  18995–19012, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grunde-McLaughlin et al. (2021)</span>
<span class="ltx_bibblock">
Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Agqa: A benchmark for compositional spatio-temporal reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  11287–11297, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2023)</span>
<span class="ltx_bibblock">
Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang.

</span>
<span class="ltx_bibblock">Shot2story20k: A new benchmark for comprehensive understanding of multi-shot videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2311.17043</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz.

</span>
<span class="ltx_bibblock">Lita: Language instructed temporal-localization assistant.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2403.19046</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kay et al. (2017)</span>
<span class="ltx_bibblock">
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al.

</span>
<span class="ltx_bibblock">The kinetics human action video dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1705.06950</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">khattak et al. (2024)</span>
<span class="ltx_bibblock">
Muhammad Uzair khattak, Muhammad Ferjad Naeem, Jameel Hassan, Naseer Muzzamal, Federcio Tombari, Fahad Shahbaz Khan, and Salman Khan.

</span>
<span class="ltx_bibblock">How good is my video lmm? complex video reasoning and robustness evaluation suite for video-lmms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv:2405.03690</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2021)</span>
<span class="ltx_bibblock">
Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song.

</span>
<span class="ltx_bibblock">Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  10274–10284, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2018)</span>
<span class="ltx_bibblock">
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.

</span>
<span class="ltx_bibblock">Tvqa: Localized, compositional video question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1809.01696</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2021)</span>
<span class="ltx_bibblock">
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Less is more: Clipbert for video-and-language learning via sparse sampling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  7331–7341, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2022)</span>
<span class="ltx_bibblock">
Jie Lei, Tamara L Berg, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Revealing single frame bias for video-and-language learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2206.03428</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-next: What else influences visual instruction tuning beyond data?, May 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/" title="">https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/" title="">https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024c)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-onevision: Easy visual task transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2408.03326</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024d)</span>
<span class="ltx_bibblock">
Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al.

</span>
<span class="ltx_bibblock">Multimodal foundation models: From specialists to general-purpose assistants.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Foundations and Trends® in Computer Graphics and Vision</em>, 2024d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2301.12597" title="">https://arxiv.org/abs/2301.12597</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024e)</span>
<span class="ltx_bibblock">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Videochat: Chat-centric video understanding, 2024e.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.06355" title="">https://arxiv.org/abs/2305.06355</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han.

</span>
<span class="ltx_bibblock">Vila: On pre-training for visual language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  26689–26699, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in neural information processing systems</em>, 36, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou.

</span>
<span class="ltx_bibblock">Tempcompass: Do video llms really understand videos?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2403.00476</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LMMs-Lab (2024)</span>
<span class="ltx_bibblock">
LMMs-Lab.

</span>
<span class="ltx_bibblock">Video detail caption, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/lmms-lab/VideoDetailCaption" title="">https://huggingface.co/datasets/lmms-lab/VideoDetailCaption</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2023)</span>
<span class="ltx_bibblock">
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock"># instag: Instruction tagging for analyzing supervised fine-tuning of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2021)</span>
<span class="ltx_bibblock">
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.

</span>
<span class="ltx_bibblock">Clip4clip: An empirical study of clip for end to end video clip retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2104.08860</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al. (2024)</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.

</span>
<span class="ltx_bibblock">Video-chatgpt: Towards detailed video understanding via large vision and language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangalam et al. (2024)</span>
<span class="ltx_bibblock">
Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Egoschema: A diagnostic benchmark for very long-form video language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miech et al. (2019)</span>
<span class="ltx_bibblock">
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.

</span>
<span class="ltx_bibblock">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ICCV</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4v.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-4v-system-card/" title="">https://openai.com/index/gpt-4v-system-card/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Hello gpt-4o.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pătrăucean et al. (2023)</span>
<span class="ltx_bibblock">
Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira.

</span>
<span class="ltx_bibblock">Perception test: A diagnostic benchmark for multimodal video models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HYEGXFnPoq" title="">https://openreview.net/forum?id=HYEGXFnPoq</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International Conference on Machine Learning (ICML)</em>, pp.  8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers &amp; Gurevych (2020)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Making monolingual sentence embeddings multilingual using knowledge distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, 11 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2004.09813" title="">https://arxiv.org/abs/2004.09813</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al. (2015)</span>
<span class="ltx_bibblock">
Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele.

</span>
<span class="ltx_bibblock">A dataset for movie description.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  3202–3212, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang et al. (2019)</span>
<span class="ltx_bibblock">
Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Annotating objects and relations in user-generated videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 2019 on International Conference on Multimedia Retrieval</em>, pp.  279–287. ACM, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sigurdsson et al. (2016)</span>
<span class="ltx_bibblock">
Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta.

</span>
<span class="ltx_bibblock">Hollywood in homes: Crowdsourcing data collection for activity understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</em>, pp.  510–526. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Wang, Liping Yuan, and Yuchen Zhang.

</span>
<span class="ltx_bibblock">Tarsier: Recipes for training and evaluating large video description models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.00634" title="">https://arxiv.org/abs/2407.00634</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al.

</span>
<span class="ltx_bibblock">Internvid: A large-scale video-text dataset for multimodal understanding and generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024a)</span>
<span class="ltx_bibblock">
Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan.

</span>
<span class="ltx_bibblock">Star: A benchmark for situated reasoning in real-world videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2405.09711</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024b)</span>
<span class="ltx_bibblock">
Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li.

</span>
<span class="ltx_bibblock">Longvideobench: A benchmark for long-context interleaved video-language understanding, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.15754" title="">https://arxiv.org/abs/2407.15754</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2021)</span>
<span class="ltx_bibblock">
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Next-qa: Next phase of question-answering to explaining temporal actions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  9777–9786, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2017)</span>
<span class="ltx_bibblock">
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.

</span>
<span class="ltx_bibblock">Video question answering via gradually refined attention over appearance and motion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">ACM Multimedia</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2016)</span>
<span class="ltx_bibblock">
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.

</span>
<span class="ltx_bibblock">Msr-vtt: A large video description dataset for bridging video and language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  5288–5296, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng.

</span>
<span class="ltx_bibblock">Pllava: Parameter-free llava extension from images to videos for video dense captioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2404.16994</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan.

</span>
<span class="ltx_bibblock">Slowfast-llava: A strong training-free baseline for video large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2407.15841</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024c)</span>
<span class="ltx_bibblock">
Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan.

</span>
<span class="ltx_bibblock">Slowfast-llava: A strong training-free baseline for video large language models, 2024c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.15841" title="">https://arxiv.org/abs/2407.15841</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2022)</span>
<span class="ltx_bibblock">
Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo.

</span>
<span class="ltx_bibblock">Advancing high-resolution video-language representation with large-scale video transcriptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al.

</span>
<span class="ltx_bibblock">Qwen2 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2407.10671</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019)</span>
<span class="ltx_bibblock">
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Activitynet-qa: A dataset for understanding complex web videos via question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">AAAI</em>, pp.  9127–9134, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zadeh et al. (2019)</span>
<span class="ltx_bibblock">
Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency.

</span>
<span class="ltx_bibblock">Social-iq: A question answering benchmark for artificial social intelligence.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  8807–8817, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2021)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Merlot: Multimodal neural script knowledge models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in neural information processing systems</em>, 34:23634–23651, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  11975–11986, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing.

</span>
<span class="ltx_bibblock">Video-llama: An instruction-tuned audio-visual language model for video understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2306.02858</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.02858" title="">https://arxiv.org/abs/2306.02858</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al.

</span>
<span class="ltx_bibblock">Lmms-eval: Reality check on the evaluation of large multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2407.12772</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al.

</span>
<span class="ltx_bibblock">Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2407.03320</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024c)</span>
<span class="ltx_bibblock">
Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Long context transfer from language to vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2406.16852</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024d)</span>
<span class="ltx_bibblock">
Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang.

</span>
<span class="ltx_bibblock">Direct preference optimization of video large multimodal models from language model reward, 2024d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024e)</span>
<span class="ltx_bibblock">
Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-next: A strong zero-shot video understanding model, April 2024e.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/" title="">https://llava-vl.github.io/blog/2024-04-30-llava-next-video/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu.

</span>
<span class="ltx_bibblock">Mlvu: A comprehensive benchmark for multi-task long video understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2406.04264</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou &amp; Corso (2017)</span>
<span class="ltx_bibblock">
Luowei Zhou and Jason J. Corso.

</span>
<span class="ltx_bibblock">Youcookii dataset.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:19774151" title="">https://api.semanticscholar.org/CorpusID:19774151</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023a)</span>
<span class="ltx_bibblock">
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan.

</span>
<span class="ltx_bibblock">Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023b)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2304.10592</em>, 2023b.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author Contributions</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">- Yuanhan Zhang contributed to the LLaVA-NeXT-Video series by developing the video training and inference codebase, creating the annotation pipeline, designing video representations, and conducting exploratory experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">- Jinming Wu contributed to the collecting and curating the video data, providing consistent technical support throughout the project, including helping improve codebase and conducting experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">- Wei Li engaged in designing the video data collection pipeline and experimental setups.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">- Bo Li worked on providing LLaVA-OneVision codebase and training recipes as the initial foundation of current project, and offered help in integration to SGLang as well as demo service.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p" id="A1.p5.1">- Zejun Ma acquired a part of the data annotation and computational resources for the project.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p6">
<p class="ltx_p" id="A1.p6.1">- Ziwei Liu offered valuable suggestions throughout the project, contributing to discussions on the data collection pipeline design and experimental setups.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p7">
<p class="ltx_p" id="A1.p7.1">- Chunyuan Li initiated and led the project, designed the roadmap in the data collection, curation and expriments, and revised the paper.</p>
</div>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Video Representations</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Efficient Video Representations in LMMs</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">Current designs of large multimodal models (LMM) typically connect a vision encoder <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib41" title="">2021</a>; Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib62" title="">2023</a>)</cite> to a large language model <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib58" title="">2024</a>)</cite> through a lightweight projector <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib30" title="">2024a</a>)</cite> or a resampler <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib27" title="">2023</a>; Alayrac et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib1" title="">2022</a>)</cite>. These components transform a set of visual representations into “visual tokens” aligned with text embeddings. In contrast to image-based LMMs, which generate only a small number of visual tokens easily managed by a standard GPU, video LMMs face challenges due to a large number of visual tokens derived from multiple video frames. The LLaVA-NeXT-Video <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib68" title="">2024e</a>)</cite> and PLLaVA <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib54" title="">2024a</a>)</cite> models address this by simly considering average pooling to reduce the number of tokens representing each frame.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">Following the idea of SlowFast in the traditional video understanding <cite class="ltx_cite ltx_citemacro_citep">(Feichtenhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib10" title="">2019</a>)</cite>, adaptive reductions in visual tokens are demonstrated by recent video LMMs, LITA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib16" title="">2024</a>)</cite> and SlowFast-LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib56" title="">2024c</a>)</cite>. Initially, these methods represent all sampled frames with a minimal number of visual tokens (fast frame)— typically just one—by using a large pooling stride. They then switch to a smaller pooling stride for certain frames to retain more visual tokens (slow frame). Finally, they combine the visual tokens of fast frames with those of slow frames. However, this approach can lead to some frames being represented twice. In contrast, our method uses a larger pooling stride for sampled frames to maintain fewer visual tokens (fast frame) <span class="ltx_text ltx_font_italic" id="A2.SS1.p2.1.1">or</span> a smaller stride for others to keep more (slow frame). We then arrange slow and fast frames in an interleaving pattern.</p>
</div>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="A2.F8.g1" src="x8.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Video representations. A different number of tokens are utilized to represent frames.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span><span class="ltx_text" id="A2.SS2.1.1">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline" id="A2.SS2.1.1.m1.1"><semantics id="A2.SS2.1.1.m1.1b"><msub id="A2.SS2.1.1.m1.1.1" xref="A2.SS2.1.1.m1.1.1.cmml"><mi id="A2.SS2.1.1.m1.1.1b" xref="A2.SS2.1.1.m1.1.1.cmml"></mi><mi id="A2.SS2.1.1.m1.1.1.1" xref="A2.SS2.1.1.m1.1.1.1.cmml">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.1.1.m1.1c"><apply id="A2.SS2.1.1.m1.1.1.cmml" xref="A2.SS2.1.1.m1.1.1"><ci id="A2.SS2.1.1.m1.1.1.1.cmml" xref="A2.SS2.1.1.m1.1.1.1">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.1.1.m1.1d">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.1.1.m1.1e">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span>
</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.7">We represent each video as a sequence with maximum <math alttext="T" class="ltx_Math" display="inline" id="A2.SS2.p1.1.m1.1"><semantics id="A2.SS2.p1.1.m1.1a"><mi id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><ci id="A2.SS2.p1.1.m1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.1.m1.1d">italic_T</annotation></semantics></math> frames. Each frame is represented in <math alttext="M" class="ltx_Math" display="inline" id="A2.SS2.p1.2.m2.1"><semantics id="A2.SS2.p1.2.m2.1a"><mi id="A2.SS2.p1.2.m2.1.1" xref="A2.SS2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.2.m2.1b"><ci id="A2.SS2.p1.2.m2.1.1.cmml" xref="A2.SS2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.2.m2.1d">italic_M</annotation></semantics></math> tokens. FPS-based video representation can be considered in the future.
Specifically, each frame is encoded via an image encoder and a two-layer MLP for projection. These visual tokens are concatenated with word tokens and processed by a large language model (LLM).
Managing tokens for every frame can be computationally demanding.
For instance, employing the SigLIP <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib62" title="">2023</a>)</cite> encoder for a video with <math alttext="T=100" class="ltx_Math" display="inline" id="A2.SS2.p1.3.m3.1"><semantics id="A2.SS2.p1.3.m3.1a"><mrow id="A2.SS2.p1.3.m3.1.1" xref="A2.SS2.p1.3.m3.1.1.cmml"><mi id="A2.SS2.p1.3.m3.1.1.2" xref="A2.SS2.p1.3.m3.1.1.2.cmml">T</mi><mo id="A2.SS2.p1.3.m3.1.1.1" xref="A2.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="A2.SS2.p1.3.m3.1.1.3" xref="A2.SS2.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.3.m3.1b"><apply id="A2.SS2.p1.3.m3.1.1.cmml" xref="A2.SS2.p1.3.m3.1.1"><eq id="A2.SS2.p1.3.m3.1.1.1.cmml" xref="A2.SS2.p1.3.m3.1.1.1"></eq><ci id="A2.SS2.p1.3.m3.1.1.2.cmml" xref="A2.SS2.p1.3.m3.1.1.2">𝑇</ci><cn id="A2.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="A2.SS2.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.3.m3.1c">T=100</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.3.m3.1d">italic_T = 100</annotation></semantics></math> results in 67,600 tokens, assuming <math alttext="M=729" class="ltx_Math" display="inline" id="A2.SS2.p1.4.m4.1"><semantics id="A2.SS2.p1.4.m4.1a"><mrow id="A2.SS2.p1.4.m4.1.1" xref="A2.SS2.p1.4.m4.1.1.cmml"><mi id="A2.SS2.p1.4.m4.1.1.2" xref="A2.SS2.p1.4.m4.1.1.2.cmml">M</mi><mo id="A2.SS2.p1.4.m4.1.1.1" xref="A2.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="A2.SS2.p1.4.m4.1.1.3" xref="A2.SS2.p1.4.m4.1.1.3.cmml">729</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.4.m4.1b"><apply id="A2.SS2.p1.4.m4.1.1.cmml" xref="A2.SS2.p1.4.m4.1.1"><eq id="A2.SS2.p1.4.m4.1.1.1.cmml" xref="A2.SS2.p1.4.m4.1.1.1"></eq><ci id="A2.SS2.p1.4.m4.1.1.2.cmml" xref="A2.SS2.p1.4.m4.1.1.2">𝑀</ci><cn id="A2.SS2.p1.4.m4.1.1.3.cmml" type="integer" xref="A2.SS2.p1.4.m4.1.1.3">729</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.4.m4.1c">M=729</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.4.m4.1d">italic_M = 729</annotation></semantics></math> tokens per frame, which often exceeds GPU memory limits. This issue is exacerbated when using large-parameter LLMs; with the Qwen2-72B model, we could only process 8 frames before maxing out the memory on 128 NVIDIA H100 GPUs. Such a limited number of frames can introduce inconsistencies in language annotations, reducing model efficacy.
One strategy to incorporate more frames is by applying <math alttext="p\times p" class="ltx_Math" display="inline" id="A2.SS2.p1.5.m5.1"><semantics id="A2.SS2.p1.5.m5.1a"><mrow id="A2.SS2.p1.5.m5.1.1" xref="A2.SS2.p1.5.m5.1.1.cmml"><mi id="A2.SS2.p1.5.m5.1.1.2" xref="A2.SS2.p1.5.m5.1.1.2.cmml">p</mi><mo id="A2.SS2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.SS2.p1.5.m5.1.1.1.cmml">×</mo><mi id="A2.SS2.p1.5.m5.1.1.3" xref="A2.SS2.p1.5.m5.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.5.m5.1b"><apply id="A2.SS2.p1.5.m5.1.1.cmml" xref="A2.SS2.p1.5.m5.1.1"><times id="A2.SS2.p1.5.m5.1.1.1.cmml" xref="A2.SS2.p1.5.m5.1.1.1"></times><ci id="A2.SS2.p1.5.m5.1.1.2.cmml" xref="A2.SS2.p1.5.m5.1.1.2">𝑝</ci><ci id="A2.SS2.p1.5.m5.1.1.3.cmml" xref="A2.SS2.p1.5.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.5.m5.1c">p\times p</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.5.m5.1d">italic_p × italic_p</annotation></semantics></math> spatial average pooling to reduce <math alttext="M" class="ltx_Math" display="inline" id="A2.SS2.p1.6.m6.1"><semantics id="A2.SS2.p1.6.m6.1a"><mi id="A2.SS2.p1.6.m6.1.1" xref="A2.SS2.p1.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.6.m6.1b"><ci id="A2.SS2.p1.6.m6.1.1.cmml" xref="A2.SS2.p1.6.m6.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.6.m6.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.6.m6.1d">italic_M</annotation></semantics></math> to <math alttext="M/p^{2}" class="ltx_Math" display="inline" id="A2.SS2.p1.7.m7.1"><semantics id="A2.SS2.p1.7.m7.1a"><mrow id="A2.SS2.p1.7.m7.1.1" xref="A2.SS2.p1.7.m7.1.1.cmml"><mi id="A2.SS2.p1.7.m7.1.1.2" xref="A2.SS2.p1.7.m7.1.1.2.cmml">M</mi><mo id="A2.SS2.p1.7.m7.1.1.1" xref="A2.SS2.p1.7.m7.1.1.1.cmml">/</mo><msup id="A2.SS2.p1.7.m7.1.1.3" xref="A2.SS2.p1.7.m7.1.1.3.cmml"><mi id="A2.SS2.p1.7.m7.1.1.3.2" xref="A2.SS2.p1.7.m7.1.1.3.2.cmml">p</mi><mn id="A2.SS2.p1.7.m7.1.1.3.3" xref="A2.SS2.p1.7.m7.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.7.m7.1b"><apply id="A2.SS2.p1.7.m7.1.1.cmml" xref="A2.SS2.p1.7.m7.1.1"><divide id="A2.SS2.p1.7.m7.1.1.1.cmml" xref="A2.SS2.p1.7.m7.1.1.1"></divide><ci id="A2.SS2.p1.7.m7.1.1.2.cmml" xref="A2.SS2.p1.7.m7.1.1.2">𝑀</ci><apply id="A2.SS2.p1.7.m7.1.1.3.cmml" xref="A2.SS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="A2.SS2.p1.7.m7.1.1.3.1.cmml" xref="A2.SS2.p1.7.m7.1.1.3">superscript</csymbol><ci id="A2.SS2.p1.7.m7.1.1.3.2.cmml" xref="A2.SS2.p1.7.m7.1.1.3.2">𝑝</ci><cn id="A2.SS2.p1.7.m7.1.1.3.3.cmml" type="integer" xref="A2.SS2.p1.7.m7.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.7.m7.1c">M/p^{2}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.7.m7.1d">italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, thus lowering the token count per frame as suggested by recent studies <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib54" title="">2024a</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib68" title="">2024e</a>)</cite>. However, the number of visual tokens is crucial for preserving the informational content of each frame, which is vital for video comprehension.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.9">In our <span class="ltx_text" id="A2.SS2.p2.1.1">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline" id="A2.SS2.p2.1.1.m1.1"><semantics id="A2.SS2.p2.1.1.m1.1a"><msub id="A2.SS2.p2.1.1.m1.1.1" xref="A2.SS2.p2.1.1.m1.1.1.cmml"><mi id="A2.SS2.p2.1.1.m1.1.1a" xref="A2.SS2.p2.1.1.m1.1.1.cmml"></mi><mi id="A2.SS2.p2.1.1.m1.1.1.1" xref="A2.SS2.p2.1.1.m1.1.1.1.cmml">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.1.1.m1.1b"><apply id="A2.SS2.p2.1.1.m1.1.1.cmml" xref="A2.SS2.p2.1.1.m1.1.1"><ci id="A2.SS2.p2.1.1.m1.1.1.1.cmml" xref="A2.SS2.p2.1.1.m1.1.1.1">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.1.1.m1.1c">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.1.1.m1.1d">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span>, we categorize the frames into two groups, based on the a strike rate <math alttext="s" class="ltx_Math" display="inline" id="A2.SS2.p2.2.m1.1"><semantics id="A2.SS2.p2.2.m1.1a"><mi id="A2.SS2.p2.2.m1.1.1" xref="A2.SS2.p2.2.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.2.m1.1b"><ci id="A2.SS2.p2.2.m1.1.1.cmml" xref="A2.SS2.p2.2.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.2.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.2.m1.1d">italic_s</annotation></semantics></math>, where the every <math alttext="s" class="ltx_Math" display="inline" id="A2.SS2.p2.3.m2.1"><semantics id="A2.SS2.p2.3.m2.1a"><mi id="A2.SS2.p2.3.m2.1.1" xref="A2.SS2.p2.3.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.3.m2.1b"><ci id="A2.SS2.p2.3.m2.1.1.cmml" xref="A2.SS2.p2.3.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.3.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.3.m2.1d">italic_s</annotation></semantics></math> frames are uniformly selected to form the <span class="ltx_text ltx_font_italic" id="A2.SS2.p2.9.2">slow</span> frame group, and the rest of the frames are consdiered as the <span class="ltx_text ltx_font_italic" id="A2.SS2.p2.9.3">fast</span> frame group. Note that a special case <math alttext="s=1" class="ltx_Math" display="inline" id="A2.SS2.p2.4.m3.1"><semantics id="A2.SS2.p2.4.m3.1a"><mrow id="A2.SS2.p2.4.m3.1.1" xref="A2.SS2.p2.4.m3.1.1.cmml"><mi id="A2.SS2.p2.4.m3.1.1.2" xref="A2.SS2.p2.4.m3.1.1.2.cmml">s</mi><mo id="A2.SS2.p2.4.m3.1.1.1" xref="A2.SS2.p2.4.m3.1.1.1.cmml">=</mo><mn id="A2.SS2.p2.4.m3.1.1.3" xref="A2.SS2.p2.4.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.4.m3.1b"><apply id="A2.SS2.p2.4.m3.1.1.cmml" xref="A2.SS2.p2.4.m3.1.1"><eq id="A2.SS2.p2.4.m3.1.1.1.cmml" xref="A2.SS2.p2.4.m3.1.1.1"></eq><ci id="A2.SS2.p2.4.m3.1.1.2.cmml" xref="A2.SS2.p2.4.m3.1.1.2">𝑠</ci><cn id="A2.SS2.p2.4.m3.1.1.3.cmml" type="integer" xref="A2.SS2.p2.4.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.4.m3.1c">s=1</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.4.m3.1d">italic_s = 1</annotation></semantics></math> leads to only one group, reducing the SlowFast representation to the original simple representation. For each group, we apply different pooling rate using Pytorch function pooling <math alttext="\mathtt{avg\_pool2d}()" class="ltx_Math" display="inline" id="A2.SS2.p2.5.m4.1"><semantics id="A2.SS2.p2.5.m4.1a"><mrow id="A2.SS2.p2.5.m4.1.1" xref="A2.SS2.p2.5.m4.1.1.cmml"><mi id="A2.SS2.p2.5.m4.1.1.2" xref="A2.SS2.p2.5.m4.1.1.2.cmml">𝚊𝚟𝚐</mi><mo id="A2.SS2.p2.5.m4.1.1.1" xref="A2.SS2.p2.5.m4.1.1.1.cmml">⁢</mo><mi id="A2.SS2.p2.5.m4.1.1.3" mathvariant="normal" xref="A2.SS2.p2.5.m4.1.1.3.cmml">_</mi><mo id="A2.SS2.p2.5.m4.1.1.1a" xref="A2.SS2.p2.5.m4.1.1.1.cmml">⁢</mo><mi id="A2.SS2.p2.5.m4.1.1.4" xref="A2.SS2.p2.5.m4.1.1.4.cmml">𝚙𝚘𝚘𝚕𝟸𝚍</mi><mo id="A2.SS2.p2.5.m4.1.1.1b" xref="A2.SS2.p2.5.m4.1.1.1.cmml">⁢</mo><mrow id="A2.SS2.p2.5.m4.1.1.5.2" xref="A2.SS2.p2.5.m4.1.1.cmml"><mo id="A2.SS2.p2.5.m4.1.1.5.2.1" stretchy="false" xref="A2.SS2.p2.5.m4.1.1.5.1.cmml">(</mo><mo id="A2.SS2.p2.5.m4.1.1.5.2.2" stretchy="false" xref="A2.SS2.p2.5.m4.1.1.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.5.m4.1b"><apply id="A2.SS2.p2.5.m4.1.1.cmml" xref="A2.SS2.p2.5.m4.1.1"><times id="A2.SS2.p2.5.m4.1.1.1.cmml" xref="A2.SS2.p2.5.m4.1.1.1"></times><ci id="A2.SS2.p2.5.m4.1.1.2.cmml" xref="A2.SS2.p2.5.m4.1.1.2">𝚊𝚟𝚐</ci><ci id="A2.SS2.p2.5.m4.1.1.3.cmml" xref="A2.SS2.p2.5.m4.1.1.3">_</ci><ci id="A2.SS2.p2.5.m4.1.1.4.cmml" xref="A2.SS2.p2.5.m4.1.1.4">𝚙𝚘𝚘𝚕𝟸𝚍</ci><list id="A2.SS2.p2.5.m4.1.1.5.1.cmml" xref="A2.SS2.p2.5.m4.1.1.5.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.5.m4.1c">\mathtt{avg\_pool2d}()</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.5.m4.1d">typewriter_avg _ typewriter_pool2d ( )</annotation></semantics></math>. <math alttext="p\times p" class="ltx_Math" display="inline" id="A2.SS2.p2.6.m5.1"><semantics id="A2.SS2.p2.6.m5.1a"><mrow id="A2.SS2.p2.6.m5.1.1" xref="A2.SS2.p2.6.m5.1.1.cmml"><mi id="A2.SS2.p2.6.m5.1.1.2" xref="A2.SS2.p2.6.m5.1.1.2.cmml">p</mi><mo id="A2.SS2.p2.6.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.SS2.p2.6.m5.1.1.1.cmml">×</mo><mi id="A2.SS2.p2.6.m5.1.1.3" xref="A2.SS2.p2.6.m5.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.6.m5.1b"><apply id="A2.SS2.p2.6.m5.1.1.cmml" xref="A2.SS2.p2.6.m5.1.1"><times id="A2.SS2.p2.6.m5.1.1.1.cmml" xref="A2.SS2.p2.6.m5.1.1.1"></times><ci id="A2.SS2.p2.6.m5.1.1.2.cmml" xref="A2.SS2.p2.6.m5.1.1.2">𝑝</ci><ci id="A2.SS2.p2.6.m5.1.1.3.cmml" xref="A2.SS2.p2.6.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.6.m5.1c">p\times p</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.6.m5.1d">italic_p × italic_p</annotation></semantics></math> pooling and <math alttext="2p\times 2p" class="ltx_Math" display="inline" id="A2.SS2.p2.7.m6.1"><semantics id="A2.SS2.p2.7.m6.1a"><mrow id="A2.SS2.p2.7.m6.1.1" xref="A2.SS2.p2.7.m6.1.1.cmml"><mrow id="A2.SS2.p2.7.m6.1.1.2" xref="A2.SS2.p2.7.m6.1.1.2.cmml"><mrow id="A2.SS2.p2.7.m6.1.1.2.2" xref="A2.SS2.p2.7.m6.1.1.2.2.cmml"><mn id="A2.SS2.p2.7.m6.1.1.2.2.2" xref="A2.SS2.p2.7.m6.1.1.2.2.2.cmml">2</mn><mo id="A2.SS2.p2.7.m6.1.1.2.2.1" xref="A2.SS2.p2.7.m6.1.1.2.2.1.cmml">⁢</mo><mi id="A2.SS2.p2.7.m6.1.1.2.2.3" xref="A2.SS2.p2.7.m6.1.1.2.2.3.cmml">p</mi></mrow><mo id="A2.SS2.p2.7.m6.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A2.SS2.p2.7.m6.1.1.2.1.cmml">×</mo><mn id="A2.SS2.p2.7.m6.1.1.2.3" xref="A2.SS2.p2.7.m6.1.1.2.3.cmml">2</mn></mrow><mo id="A2.SS2.p2.7.m6.1.1.1" xref="A2.SS2.p2.7.m6.1.1.1.cmml">⁢</mo><mi id="A2.SS2.p2.7.m6.1.1.3" xref="A2.SS2.p2.7.m6.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.7.m6.1b"><apply id="A2.SS2.p2.7.m6.1.1.cmml" xref="A2.SS2.p2.7.m6.1.1"><times id="A2.SS2.p2.7.m6.1.1.1.cmml" xref="A2.SS2.p2.7.m6.1.1.1"></times><apply id="A2.SS2.p2.7.m6.1.1.2.cmml" xref="A2.SS2.p2.7.m6.1.1.2"><times id="A2.SS2.p2.7.m6.1.1.2.1.cmml" xref="A2.SS2.p2.7.m6.1.1.2.1"></times><apply id="A2.SS2.p2.7.m6.1.1.2.2.cmml" xref="A2.SS2.p2.7.m6.1.1.2.2"><times id="A2.SS2.p2.7.m6.1.1.2.2.1.cmml" xref="A2.SS2.p2.7.m6.1.1.2.2.1"></times><cn id="A2.SS2.p2.7.m6.1.1.2.2.2.cmml" type="integer" xref="A2.SS2.p2.7.m6.1.1.2.2.2">2</cn><ci id="A2.SS2.p2.7.m6.1.1.2.2.3.cmml" xref="A2.SS2.p2.7.m6.1.1.2.2.3">𝑝</ci></apply><cn id="A2.SS2.p2.7.m6.1.1.2.3.cmml" type="integer" xref="A2.SS2.p2.7.m6.1.1.2.3">2</cn></apply><ci id="A2.SS2.p2.7.m6.1.1.3.cmml" xref="A2.SS2.p2.7.m6.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.7.m6.1c">2p\times 2p</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.7.m6.1d">2 italic_p × 2 italic_p</annotation></semantics></math> pooling for slow and fast frames, respectively. To summarize, we paramterize the video representation configuration as <math alttext="\mathcal{V}=(T,M,s,p)" class="ltx_Math" display="inline" id="A2.SS2.p2.8.m7.4"><semantics id="A2.SS2.p2.8.m7.4a"><mrow id="A2.SS2.p2.8.m7.4.5" xref="A2.SS2.p2.8.m7.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS2.p2.8.m7.4.5.2" xref="A2.SS2.p2.8.m7.4.5.2.cmml">𝒱</mi><mo id="A2.SS2.p2.8.m7.4.5.1" xref="A2.SS2.p2.8.m7.4.5.1.cmml">=</mo><mrow id="A2.SS2.p2.8.m7.4.5.3.2" xref="A2.SS2.p2.8.m7.4.5.3.1.cmml"><mo id="A2.SS2.p2.8.m7.4.5.3.2.1" stretchy="false" xref="A2.SS2.p2.8.m7.4.5.3.1.cmml">(</mo><mi id="A2.SS2.p2.8.m7.1.1" xref="A2.SS2.p2.8.m7.1.1.cmml">T</mi><mo id="A2.SS2.p2.8.m7.4.5.3.2.2" xref="A2.SS2.p2.8.m7.4.5.3.1.cmml">,</mo><mi id="A2.SS2.p2.8.m7.2.2" xref="A2.SS2.p2.8.m7.2.2.cmml">M</mi><mo id="A2.SS2.p2.8.m7.4.5.3.2.3" xref="A2.SS2.p2.8.m7.4.5.3.1.cmml">,</mo><mi id="A2.SS2.p2.8.m7.3.3" xref="A2.SS2.p2.8.m7.3.3.cmml">s</mi><mo id="A2.SS2.p2.8.m7.4.5.3.2.4" xref="A2.SS2.p2.8.m7.4.5.3.1.cmml">,</mo><mi id="A2.SS2.p2.8.m7.4.4" xref="A2.SS2.p2.8.m7.4.4.cmml">p</mi><mo id="A2.SS2.p2.8.m7.4.5.3.2.5" stretchy="false" xref="A2.SS2.p2.8.m7.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.8.m7.4b"><apply id="A2.SS2.p2.8.m7.4.5.cmml" xref="A2.SS2.p2.8.m7.4.5"><eq id="A2.SS2.p2.8.m7.4.5.1.cmml" xref="A2.SS2.p2.8.m7.4.5.1"></eq><ci id="A2.SS2.p2.8.m7.4.5.2.cmml" xref="A2.SS2.p2.8.m7.4.5.2">𝒱</ci><vector id="A2.SS2.p2.8.m7.4.5.3.1.cmml" xref="A2.SS2.p2.8.m7.4.5.3.2"><ci id="A2.SS2.p2.8.m7.1.1.cmml" xref="A2.SS2.p2.8.m7.1.1">𝑇</ci><ci id="A2.SS2.p2.8.m7.2.2.cmml" xref="A2.SS2.p2.8.m7.2.2">𝑀</ci><ci id="A2.SS2.p2.8.m7.3.3.cmml" xref="A2.SS2.p2.8.m7.3.3">𝑠</ci><ci id="A2.SS2.p2.8.m7.4.4.cmml" xref="A2.SS2.p2.8.m7.4.4">𝑝</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.8.m7.4c">\mathcal{V}=(T,M,s,p)</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.8.m7.4d">caligraphic_V = ( italic_T , italic_M , italic_s , italic_p )</annotation></semantics></math>. The total number of tokens is <math alttext="\#tokens=\left\lfloor T/s\right\rfloor\times\left\lfloor M/p^{2}\right\rfloor+%
\left(T-\left\lfloor T/s\right\rfloor\right)\times\left\lfloor M/p^{2}\right\rfloor" class="ltx_Math" display="inline" id="A2.SS2.p2.9.m8.4"><semantics id="A2.SS2.p2.9.m8.4a"><mrow id="A2.SS2.p2.9.m8.4.4" xref="A2.SS2.p2.9.m8.4.4.cmml"><mrow id="A2.SS2.p2.9.m8.4.4.6" xref="A2.SS2.p2.9.m8.4.4.6.cmml"><mi id="A2.SS2.p2.9.m8.4.4.6.2" mathvariant="normal" xref="A2.SS2.p2.9.m8.4.4.6.2.cmml">#</mi><mo id="A2.SS2.p2.9.m8.4.4.6.1" xref="A2.SS2.p2.9.m8.4.4.6.1.cmml">⁢</mo><mi id="A2.SS2.p2.9.m8.4.4.6.3" xref="A2.SS2.p2.9.m8.4.4.6.3.cmml">t</mi><mo id="A2.SS2.p2.9.m8.4.4.6.1a" xref="A2.SS2.p2.9.m8.4.4.6.1.cmml">⁢</mo><mi id="A2.SS2.p2.9.m8.4.4.6.4" xref="A2.SS2.p2.9.m8.4.4.6.4.cmml">o</mi><mo id="A2.SS2.p2.9.m8.4.4.6.1b" xref="A2.SS2.p2.9.m8.4.4.6.1.cmml">⁢</mo><mi id="A2.SS2.p2.9.m8.4.4.6.5" xref="A2.SS2.p2.9.m8.4.4.6.5.cmml">k</mi><mo id="A2.SS2.p2.9.m8.4.4.6.1c" xref="A2.SS2.p2.9.m8.4.4.6.1.cmml">⁢</mo><mi id="A2.SS2.p2.9.m8.4.4.6.6" xref="A2.SS2.p2.9.m8.4.4.6.6.cmml">e</mi><mo id="A2.SS2.p2.9.m8.4.4.6.1d" xref="A2.SS2.p2.9.m8.4.4.6.1.cmml">⁢</mo><mi id="A2.SS2.p2.9.m8.4.4.6.7" xref="A2.SS2.p2.9.m8.4.4.6.7.cmml">n</mi><mo id="A2.SS2.p2.9.m8.4.4.6.1e" xref="A2.SS2.p2.9.m8.4.4.6.1.cmml">⁢</mo><mi id="A2.SS2.p2.9.m8.4.4.6.8" xref="A2.SS2.p2.9.m8.4.4.6.8.cmml">s</mi></mrow><mo id="A2.SS2.p2.9.m8.4.4.5" xref="A2.SS2.p2.9.m8.4.4.5.cmml">=</mo><mrow id="A2.SS2.p2.9.m8.4.4.4" xref="A2.SS2.p2.9.m8.4.4.4.cmml"><mrow id="A2.SS2.p2.9.m8.2.2.2.2" xref="A2.SS2.p2.9.m8.2.2.2.2.cmml"><mrow id="A2.SS2.p2.9.m8.1.1.1.1.1.1" xref="A2.SS2.p2.9.m8.1.1.1.1.1.2.cmml"><mo id="A2.SS2.p2.9.m8.1.1.1.1.1.1.2" xref="A2.SS2.p2.9.m8.1.1.1.1.1.2.1.cmml">⌊</mo><mrow id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.cmml"><mi id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.2" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.2.cmml">T</mi><mo id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.1" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.1.cmml">/</mo><mi id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.3" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.3.cmml">s</mi></mrow><mo id="A2.SS2.p2.9.m8.1.1.1.1.1.1.3" rspace="0.055em" xref="A2.SS2.p2.9.m8.1.1.1.1.1.2.1.cmml">⌋</mo></mrow><mo id="A2.SS2.p2.9.m8.2.2.2.2.3" rspace="0.222em" xref="A2.SS2.p2.9.m8.2.2.2.2.3.cmml">×</mo><mrow id="A2.SS2.p2.9.m8.2.2.2.2.2.1" xref="A2.SS2.p2.9.m8.2.2.2.2.2.2.cmml"><mo id="A2.SS2.p2.9.m8.2.2.2.2.2.1.2" xref="A2.SS2.p2.9.m8.2.2.2.2.2.2.1.cmml">⌊</mo><mrow id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.cmml"><mi id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.2" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.2.cmml">M</mi><mo id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.1" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.1.cmml">/</mo><msup id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.cmml"><mi id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.2" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.2.cmml">p</mi><mn id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.3" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.3.cmml">2</mn></msup></mrow><mo id="A2.SS2.p2.9.m8.2.2.2.2.2.1.3" xref="A2.SS2.p2.9.m8.2.2.2.2.2.2.1.cmml">⌋</mo></mrow></mrow><mo id="A2.SS2.p2.9.m8.4.4.4.5" xref="A2.SS2.p2.9.m8.4.4.4.5.cmml">+</mo><mrow id="A2.SS2.p2.9.m8.4.4.4.4" xref="A2.SS2.p2.9.m8.4.4.4.4.cmml"><mrow id="A2.SS2.p2.9.m8.3.3.3.3.1.1" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.cmml"><mo id="A2.SS2.p2.9.m8.3.3.3.3.1.1.2" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.cmml">(</mo><mrow id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.cmml"><mi id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.3" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.3.cmml">T</mi><mo id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.2" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.2.cmml">−</mo><mrow id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.2.cmml"><mo id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.2" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.2.1.cmml">⌊</mo><mrow id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.cmml"><mi id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.2" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.2.cmml">T</mi><mo id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.1" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.1.cmml">/</mo><mi id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.3" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.3.cmml">s</mi></mrow><mo id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.3" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.2.1.cmml">⌋</mo></mrow></mrow><mo id="A2.SS2.p2.9.m8.3.3.3.3.1.1.3" rspace="0.055em" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.cmml">)</mo></mrow><mo id="A2.SS2.p2.9.m8.4.4.4.4.3" rspace="0.222em" xref="A2.SS2.p2.9.m8.4.4.4.4.3.cmml">×</mo><mrow id="A2.SS2.p2.9.m8.4.4.4.4.2.1" xref="A2.SS2.p2.9.m8.4.4.4.4.2.2.cmml"><mo id="A2.SS2.p2.9.m8.4.4.4.4.2.1.2" xref="A2.SS2.p2.9.m8.4.4.4.4.2.2.1.cmml">⌊</mo><mrow id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.cmml"><mi id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.2" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.2.cmml">M</mi><mo id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.1" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.1.cmml">/</mo><msup id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.cmml"><mi id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.2" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.2.cmml">p</mi><mn id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.3" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.3.cmml">2</mn></msup></mrow><mo id="A2.SS2.p2.9.m8.4.4.4.4.2.1.3" xref="A2.SS2.p2.9.m8.4.4.4.4.2.2.1.cmml">⌋</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.9.m8.4b"><apply id="A2.SS2.p2.9.m8.4.4.cmml" xref="A2.SS2.p2.9.m8.4.4"><eq id="A2.SS2.p2.9.m8.4.4.5.cmml" xref="A2.SS2.p2.9.m8.4.4.5"></eq><apply id="A2.SS2.p2.9.m8.4.4.6.cmml" xref="A2.SS2.p2.9.m8.4.4.6"><times id="A2.SS2.p2.9.m8.4.4.6.1.cmml" xref="A2.SS2.p2.9.m8.4.4.6.1"></times><ci id="A2.SS2.p2.9.m8.4.4.6.2.cmml" xref="A2.SS2.p2.9.m8.4.4.6.2">#</ci><ci id="A2.SS2.p2.9.m8.4.4.6.3.cmml" xref="A2.SS2.p2.9.m8.4.4.6.3">𝑡</ci><ci id="A2.SS2.p2.9.m8.4.4.6.4.cmml" xref="A2.SS2.p2.9.m8.4.4.6.4">𝑜</ci><ci id="A2.SS2.p2.9.m8.4.4.6.5.cmml" xref="A2.SS2.p2.9.m8.4.4.6.5">𝑘</ci><ci id="A2.SS2.p2.9.m8.4.4.6.6.cmml" xref="A2.SS2.p2.9.m8.4.4.6.6">𝑒</ci><ci id="A2.SS2.p2.9.m8.4.4.6.7.cmml" xref="A2.SS2.p2.9.m8.4.4.6.7">𝑛</ci><ci id="A2.SS2.p2.9.m8.4.4.6.8.cmml" xref="A2.SS2.p2.9.m8.4.4.6.8">𝑠</ci></apply><apply id="A2.SS2.p2.9.m8.4.4.4.cmml" xref="A2.SS2.p2.9.m8.4.4.4"><plus id="A2.SS2.p2.9.m8.4.4.4.5.cmml" xref="A2.SS2.p2.9.m8.4.4.4.5"></plus><apply id="A2.SS2.p2.9.m8.2.2.2.2.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2"><times id="A2.SS2.p2.9.m8.2.2.2.2.3.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.3"></times><apply id="A2.SS2.p2.9.m8.1.1.1.1.1.2.cmml" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1"><floor id="A2.SS2.p2.9.m8.1.1.1.1.1.2.1.cmml" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.2"></floor><apply id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.cmml" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1"><divide id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.1.cmml" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.1"></divide><ci id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.2.cmml" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.2">𝑇</ci><ci id="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.3.cmml" xref="A2.SS2.p2.9.m8.1.1.1.1.1.1.1.3">𝑠</ci></apply></apply><apply id="A2.SS2.p2.9.m8.2.2.2.2.2.2.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1"><floor id="A2.SS2.p2.9.m8.2.2.2.2.2.2.1.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.2"></floor><apply id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1"><divide id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.1.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.1"></divide><ci id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.2.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.2">𝑀</ci><apply id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.1.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3">superscript</csymbol><ci id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.2.cmml" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.2">𝑝</ci><cn id="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.3.cmml" type="integer" xref="A2.SS2.p2.9.m8.2.2.2.2.2.1.1.3.3">2</cn></apply></apply></apply></apply><apply id="A2.SS2.p2.9.m8.4.4.4.4.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4"><times id="A2.SS2.p2.9.m8.4.4.4.4.3.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.3"></times><apply id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1"><minus id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.2.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.2"></minus><ci id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.3.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.3">𝑇</ci><apply id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.2.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1"><floor id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.2.1.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.2"></floor><apply id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1"><divide id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.1"></divide><ci id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.2.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.2">𝑇</ci><ci id="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.3.cmml" xref="A2.SS2.p2.9.m8.3.3.3.3.1.1.1.1.1.1.3">𝑠</ci></apply></apply></apply><apply id="A2.SS2.p2.9.m8.4.4.4.4.2.2.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1"><floor id="A2.SS2.p2.9.m8.4.4.4.4.2.2.1.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.2"></floor><apply id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1"><divide id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.1.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.1"></divide><ci id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.2.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.2">𝑀</ci><apply id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3"><csymbol cd="ambiguous" id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.1.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3">superscript</csymbol><ci id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.2.cmml" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.2">𝑝</ci><cn id="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.3.cmml" type="integer" xref="A2.SS2.p2.9.m8.4.4.4.4.2.1.1.3.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.9.m8.4c">\#tokens=\left\lfloor T/s\right\rfloor\times\left\lfloor M/p^{2}\right\rfloor+%
\left(T-\left\lfloor T/s\right\rfloor\right)\times\left\lfloor M/p^{2}\right\rfloor</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p2.9.m8.4d"># italic_t italic_o italic_k italic_e italic_n italic_s = ⌊ italic_T / italic_s ⌋ × ⌊ italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⌋ + ( italic_T - ⌊ italic_T / italic_s ⌋ ) × ⌊ italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⌋</annotation></semantics></math></p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Data</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Video Detail Description</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.SS2" title="3.2 Video Detail Description ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we show that generating <span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.1">level-1 description</span> should consider historical context. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.F9" title="Figure 9 ‣ C.1 Video Detail Description ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates the impact of excluding historical context on the quality of video descriptions. Specifically, including historical context helps accurately identify characters across different times as the same individual.</p>
</div>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="A3.F9.g1" src="x9.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Generating video captions with or without historical context.</figcaption>
</figure>
<figure class="ltx_table" id="A3.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Question types for video question answering in data creation. For each type, we provide its name, description, and the proportion it represents in the <span class="ltx_text" id="A3.T5.3.1">LLaVA-Video-178K</span>.</figcaption>
<div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_middle" id="A3.T5.1" style="width:377.6pt;">
<div class="ltx_para ltx_noindent" id="A3.T5.1.p1">
<svg class="ltx_picture" height="28779.46" id="A3.T5.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,28779.46) matrix(1 0 0 -1 0 0) translate(0,3040.68)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 25732.88 C 0 25736.14 2.64 25738.78 5.91 25738.78 L 594.09 25738.78 C 597.36 25738.78 600 25736.14 600 25732.88 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 25732.88 C 1.97 25735.05 3.73 25736.81 5.91 25736.81 L 594.09 25736.81 C 596.27 25736.81 598.03 25735.05 598.03 25732.88 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 -3040.68)"><foreignobject class="ltx_minipage" color="#000000" height="25711.22" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="402.3pt">
<table class="ltx_tabular ltx_align_middle" id="A3.T5.1.p1.pic1.1.1.1.1.1">
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1">Question type</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.2.1.1.1">Description</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="A3.T5.1.p1.pic1.1.1.1.1.1.1.3.1.1.1">Proportion</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.1.1.1">Temporal</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.2.1.1">Designed to assess reasoning about temporal relationships between actions/events. Questions involve previous, present, or next actions.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.2.3.1.1">7.2%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.1.1.1">Spatial</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.2.1.1">Tests ability to perceive spatial relationships between observed instances in a video scene.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.3.3.1.1">7.2%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.1.1.1">Causal</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.2.1.1">Focuses on explaining actions/events, determining intentions of actions or causes for subsequent events.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.4.3.1.1">7.2%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.1.1.1">Description-Scene</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.2.1.1">Assesses ability to describe the major scene of the video, like where it takes place and the overall environment.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.5.3.1.1">7.2%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.1.1.1">Description-Human</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.2.1.1">Involves describing actions or attributes of people, such as their activities and appearances.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.6.3.1.1">6.7%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.1.1.1">Description-Object</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.2.1.1">Assesses ability to describe attributes of objects, like their appearance and function.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.7.3.1.1">7.0%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.1.1.1">Count</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.2.1.1">Tests ability to count instances of objects, people, actions, and to distinguish between old and new elements in a scene.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.8.3.1.1">7.1%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.1.1.1">Binary</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.2.1.1">Involves yes or no questions related to the video content.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.9.3.1.1">7.2%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.1.1.1">Fine Grained Action Understanding</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.2.1.1">Creates questions challenging comprehension of subtle actions.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.10.3.1.1">6.5%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.1.1.1">Plot Understanding</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.2.1.1">Challenges ability to interpret the plot in the video.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.11.3.1.1">7.1%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.1.1.1">Non-Existent Actions with Existent Scene Depictions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.2.1.1">Assesses reasoning with introduced non-exist ent activities without changing physical details.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.12.3.1.1">6.6%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.1.1.1">Time Order Understanding</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.2.1.1">Challenges recognition of temporal sequence of activities in videos.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.13.3.1.1">6.9%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.1.1.1">Object Direction</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.2.1.1">Emphasizes perception of object movement direction.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.14.3.1.1">3.8%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.1.1.1">Camera Direction</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.2.1.1">Focuses on the direction of camera movement.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.15.3.1.1">4.1%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.1.1.1">Speed</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.2.1.1">Delves into discerning variations in speed, including absolute and relative speeds.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.16.3.1.1">3.6%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.p1.pic1.1.1.1.1.1.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.1" style="width:108.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.1.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.1.1.1">Attribute Change</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.2" style="width:260.2pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.2.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.2.1.1">Centers on how attributes of objects or the entire video change over time, like size, shape, color, and more.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.3" style="width:43.4pt;padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.3.1">
<span class="ltx_p" id="A3.T5.1.p1.pic1.1.1.1.1.1.17.3.1.1">4.5%</span>
</span>
</td>
</tr>
</table></foreignobject></g></g></svg>
</div>
</div>
</figure>
<figure class="ltx_table" id="A3.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_logical-block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A3.T6.1" style="width:429.3pt;">
<div class="ltx_para ltx_noindent" id="A3.T6.1.p1">
<svg class="ltx_picture" height="732.69" id="A3.T6.1.p1.pic1" overflow="visible" version="1.1" width="600"><g transform="translate(0,732.69) matrix(1 0 0 -1 0 0)"><g><g><g><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 726.79 C 0 730.05 2.64 732.69 5.91 732.69 L 594.09 732.69 C 597.36 732.69 600 730.05 600 726.79 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 726.79 C 1.97 728.96 3.73 730.72 5.91 730.72 L 594.09 730.72 C 596.27 730.72 598.03 728.96 598.03 726.79 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g></g><g><g fill-opacity="1.0"><g><g transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><g><g color="#000000"><foreignobject class="ltx_minipage" height="705.13" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="402.3pt">
<table class="ltx_tabular ltx_align_middle" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14">
<tr class="ltx_tr" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14">
<span class="ltx_inline-block ltx_align_top" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14">
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14" style="width:429.3pt;">
<span class="ltx_inline-block ltx_minipage ltx_align_middle" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14" style="width:429.3pt;">
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.15"><span class="ltx_text ltx_font_typewriter" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.15.1">tasks</span> = “</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2"># Temporal: this task is designed to assess the capability of reasoning …<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><lt id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1"><semantics id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mo id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><gt id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4">## caption-1: The video features a child sitting in a baby chair at a dining table, creating…<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1"><semantics id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1a"><mo id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1" xref="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1b"><lt id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1"><semantics id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1a"><mo id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1" xref="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1b"><gt id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.16">## question-1: What was the child doing as he sat on the baby chair?</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.17">## answer-1: The child was reading a book.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.18">…</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6">## caption-3: …<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1"><semantics id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1a"><mo id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1" xref="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1b"><lt id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1"><semantics id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1a"><mo id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1.1" xref="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1b"><gt id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8">## question-3: …<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1"><semantics id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1a"><mo id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1" xref="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1b"><lt id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1"><semantics id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1a"><mo id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1.1" xref="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1b"><gt id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10">## answer-3: …<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1"><semantics id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1a"><mo id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1" xref="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1b"><lt id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1"><semantics id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1a"><mo id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1" xref="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1b"><gt id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12"># Spatial: this task involves creating questions that test a person’s ability…<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1"><semantics id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1a"><mo id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1" xref="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1b"><lt id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1"><semantics id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1a"><mo id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1.1" xref="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1b"><gt id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14">…<math alttext="&lt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1"><semantics id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1a"><mo id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1" xref="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1b"><lt id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1.cmml" xref="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1d">&lt;</annotation></semantics></math>omitted<math alttext="&gt;" class="ltx_Math" display="inline" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1"><semantics id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1a"><mo id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1.1" xref="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1b"><gt id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1.1.cmml" xref="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m2.1d">&gt;</annotation></semantics></math> ”</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.19"><span class="ltx_text ltx_font_typewriter" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.19.1">system_message</span> = “</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.20">### Task:</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.21">Given a detailed description that summarizes the content of a video, generate question-answer pairs based on the description to help humans better understand the video.
The question-answer pairs should be faithful to the content of the video description and developed from different dimensions to promote comprehensive understanding of the video.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.22">Here are some question dimensions and their explanations and exampled question-answer pairs for reference:</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.23">{<span class="ltx_text ltx_font_italic" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.23.1">task_definitions</span>}</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.24">#### Guidelines For Question-Answer Pairs Generation:</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.25">- Read the video description provided carefully, paying attention to the content, such as the scene where the video takes place, the main characters and their behaviors, and the development of the events.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.26">- Generate appropriate question-answer pairs based on the description. The question-answer pairs should cover as many question dimensions and not deviate from the content of the video description.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.27">- Generate 1 question-answer pair for each dimension.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.28">### Output Format:</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.29">1. Your output should be formed in a JSON file.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.30">2. Only provide the Python dictionary string.</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.31">Your response should look like:</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.32"><span class="ltx_text ltx_font_typewriter" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.32.1">["Dimension": &lt;dimension-1&gt;, "Question": &lt;question-1&gt;, "Answer": &lt;answer-1&gt;,</span></span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.33"><span class="ltx_text ltx_font_typewriter" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.33.1">"Dimension": &lt;dimension-2&gt;, "Question": &lt;question-2&gt;, "Answer": &lt;answer-2&gt;...]</span> ”</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.34"><span class="ltx_text ltx_font_typewriter" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.34.1">user_message</span> = “</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.35">Please generate question-answer pairs for the following video description:</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.36">Description: {caption} ”</span>
<span class="ltx_p" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.37"><span class="ltx_text ltx_font_bold" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.37.1">for</span> <em class="ltx_emph ltx_font_italic" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.37.2"><span class="ltx_text ltx_font_typewriter ltx_font_upright" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.37.2.1">cur_video</span> in <span class="ltx_text ltx_font_typewriter ltx_font_upright" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.37.2.2">videos</span></em><span class="ltx_text ltx_font_bold" id="A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.37.3">:</span></span>
</span></span>
</span>
</td>
</tr>
</table></foreignobject></g></g></g></g></g></g></g></g></g></g></g></g></g></g></g></svg>
</div>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="A3.T6.2">
<div class="ltx_listingline ltx_centering ltx_figure_panel" id="A3.T6.2.1">
<span class="ltx_text" id="A3.T6.2.1.1" style="color:#000000;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" id="A3.T6.2.1.2" style="color:#000000;">   </span><span class="ltx_text" id="A3.T6.2.1.3" style="color:#000000;">
</span><span class="ltx_text ltx_font_typewriter" id="A3.T6.2.1.4" style="color:#000000;">sys_msg = system_messages.format(task_definitions=tasks)</span><span class="ltx_text" id="A3.T6.2.1.5" style="color:#000000;"> </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.T6.2.1.6" style="color:#000000;">usr_msg = user_messages.format(caption=cur_video) 
<br class="ltx_break"/>response = GPT4O(sys_msg,usr_msg)</span><span class="ltx_text" id="A3.T6.2.1.7" style="color:#000000;">
</span>
</div>
<div class="ltx_listingline ltx_centering" id="A3.T6.2.2">
<span class="ltx_text" id="A3.T6.2.2.1" style="color:#000000;">
</span><span class="ltx_text" id="A3.T6.2.2.2">
</span>
</div>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>We explain the process of creating prompts for GPT-4O to gather question-answer pairs from each video description. <span class="ltx_text ltx_font_typewriter" id="A3.T6.4.1" style="color:#0000E6;">tasks</span> includes the definition of all question types along with examples of question-answer pairs. We instruct GPT-4O to generate questions that cover as many question types as possible.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Video Question Answering</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.T5" title="Table 5 ‣ C.1 Video Detail Description ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>, we list the names and descriptions of different question types and their corresponding proportions in the <span class="ltx_text" id="A3.SS2.p1.1.1">LLaVA-Video-178K</span> dataset. The prompt used to generate video question-answer pairs from GPT-4O is shown in Table. <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.T6" title="Table 6 ‣ C.1 Video Detail Description ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#S3.F4" title="Figure 4 ‣ Filtering. ‣ 3.3 Video Question Answering ‣ 3 Video Instruction-Following Data Synthesis ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a>, we show an example of a video along with its detailed description, an open-ended question, and a multiple-choice question.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Dataset Comparison</h3>
<figure class="ltx_table" id="A3.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span><span class="ltx_text ltx_font_bold" id="A3.T7.2.1">Comparison of <span class="ltx_text" id="A3.T7.2.1.1">LLaVA-Video-178K</span> and other video-language datasets</span>. Average FPS represents the average number of frames per second that are used to prompt GPT-4o/GPT-4V for annotation.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T7.3">
<tr class="ltx_tr" id="A3.T7.3.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_tt" id="A3.T7.3.1.1" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.2" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.1.2.1">Text</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.3" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.1.3.1">#Video</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">Total Video</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.5" style="padding-left:1.0pt;padding-right:1.0pt;">Average</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.6" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.1.6.1">#Caption</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.7" style="padding-left:1.0pt;padding-right:1.0pt;">#OE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A3.T7.3.1.8" style="padding-left:1.0pt;padding-right:1.0pt;">#MC</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Length</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">FPS</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">QA</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">QA</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A3.T7.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">HowTo100M <cite class="ltx_cite ltx_citemacro_citep">(Miech et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib37" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">ASR</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">136M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.4" style="padding-left:1.0pt;padding-right:1.0pt;">134.5Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.6" style="padding-left:1.0pt;padding-right:1.0pt;">136M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.3.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">ACAV <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib19" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">ASR</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.3" style="padding-left:1.0pt;padding-right:1.0pt;">100M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">277.7Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.6" style="padding-left:1.0pt;padding-right:1.0pt;">100M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.4.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">YT-Temporal-180M <cite class="ltx_cite ltx_citemacro_citep">(Zellers et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib61" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.2" style="padding-left:1.0pt;padding-right:1.0pt;">ASR</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.3" style="padding-left:1.0pt;padding-right:1.0pt;">180M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.4" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.6" style="padding-left:1.0pt;padding-right:1.0pt;">180M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.5.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">HD-VILA-100M <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib57" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.2" style="padding-left:1.0pt;padding-right:1.0pt;">ASR</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.3" style="padding-left:1.0pt;padding-right:1.0pt;">103M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.4" style="padding-left:1.0pt;padding-right:1.0pt;">371.5Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.6" style="padding-left:1.0pt;padding-right:1.0pt;">103M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.6.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A3.T7.3.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">MSVD <cite class="ltx_cite ltx_citemacro_citep">(Chen &amp; Dolan, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib6" title="">2011</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.3" style="padding-left:1.0pt;padding-right:1.0pt;">1970</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.4" style="padding-left:1.0pt;padding-right:1.0pt;">5.3h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.6" style="padding-left:1.0pt;padding-right:1.0pt;">1K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.7.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.8.1" style="padding-left:1.0pt;padding-right:1.0pt;">LSMDC <cite class="ltx_cite ltx_citemacro_citep">(Rohrbach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib43" title="">2015</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.3" style="padding-left:1.0pt;padding-right:1.0pt;">118K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.4" style="padding-left:1.0pt;padding-right:1.0pt;">158h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.6" style="padding-left:1.0pt;padding-right:1.0pt;">118K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.8.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.9.1" style="padding-left:1.0pt;padding-right:1.0pt;">MSR-VTT <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib53" title="">2016</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.3" style="padding-left:1.0pt;padding-right:1.0pt;">10K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.4" style="padding-left:1.0pt;padding-right:1.0pt;">40h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.6" style="padding-left:1.0pt;padding-right:1.0pt;">10K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.9.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.10.1" style="padding-left:1.0pt;padding-right:1.0pt;">DiDeMo <cite class="ltx_cite ltx_citemacro_citep">(Anne Hendricks et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib3" title="">2017b</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.3" style="padding-left:1.0pt;padding-right:1.0pt;">27K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.4" style="padding-left:1.0pt;padding-right:1.0pt;">87h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.6" style="padding-left:1.0pt;padding-right:1.0pt;">27K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.10.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.11.1" style="padding-left:1.0pt;padding-right:1.0pt;">ActivityNet <cite class="ltx_cite ltx_citemacro_citep">(Caba Heilbron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib5" title="">2015</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.3" style="padding-left:1.0pt;padding-right:1.0pt;">100K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.4" style="padding-left:1.0pt;padding-right:1.0pt;">849h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.6" style="padding-left:1.0pt;padding-right:1.0pt;">100K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.11.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.12">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.12.1" style="padding-left:1.0pt;padding-right:1.0pt;">YouCook2 <cite class="ltx_cite ltx_citemacro_citep">(Zhou &amp; Corso, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib70" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.3" style="padding-left:1.0pt;padding-right:1.0pt;">14K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.4" style="padding-left:1.0pt;padding-right:1.0pt;">176h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.6" style="padding-left:1.0pt;padding-right:1.0pt;">14K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.12.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.13">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.13.1" style="padding-left:1.0pt;padding-right:1.0pt;">TVQA <cite class="ltx_cite ltx_citemacro_citep">(Lei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib20" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.3" style="padding-left:1.0pt;padding-right:1.0pt;">21K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.4" style="padding-left:1.0pt;padding-right:1.0pt;">3.39Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.6" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.13.8" style="padding-left:1.0pt;padding-right:1.0pt;">152K</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.14">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.14.1" style="padding-left:1.0pt;padding-right:1.0pt;">ActivityNet-QA <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib59" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.3" style="padding-left:1.0pt;padding-right:1.0pt;">5.8K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.4" style="padding-left:1.0pt;padding-right:1.0pt;">290h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.6" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.7" style="padding-left:1.0pt;padding-right:1.0pt;">58K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.14.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.15">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.15.1" style="padding-left:1.0pt;padding-right:1.0pt;">Social-IQ <cite class="ltx_cite ltx_citemacro_citep">(Zadeh et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib60" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.3" style="padding-left:1.0pt;padding-right:1.0pt;">1.2K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.4" style="padding-left:1.0pt;padding-right:1.0pt;">20h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.6" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.15.8" style="padding-left:1.0pt;padding-right:1.0pt;">7.5k</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.16">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.16.1" style="padding-left:1.0pt;padding-right:1.0pt;">NExT-QA <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib51" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.2" style="padding-left:1.0pt;padding-right:1.0pt;">Manual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.3" style="padding-left:1.0pt;padding-right:1.0pt;">5.4K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.4" style="padding-left:1.0pt;padding-right:1.0pt;">66h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.6" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.7" style="padding-left:1.0pt;padding-right:1.0pt;">52K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.16.8" style="padding-left:1.0pt;padding-right:1.0pt;">47K</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.17">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A3.T7.3.17.1" style="padding-left:1.0pt;padding-right:1.0pt;">MSVD-QA <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib52" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.2" style="padding-left:1.0pt;padding-right:1.0pt;">Open-source Model</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.3" style="padding-left:1.0pt;padding-right:1.0pt;">1.9K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.4" style="padding-left:1.0pt;padding-right:1.0pt;">5.3h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.6" style="padding-left:1.0pt;padding-right:1.0pt;">41K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.7" style="padding-left:1.0pt;padding-right:1.0pt;">50K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.17.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.18">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.18.1" style="padding-left:1.0pt;padding-right:1.0pt;">MSRVTT-QA <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib52" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.2" style="padding-left:1.0pt;padding-right:1.0pt;">Open-source Model</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.3" style="padding-left:1.0pt;padding-right:1.0pt;">10K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.4" style="padding-left:1.0pt;padding-right:1.0pt;">40h</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.6" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.7" style="padding-left:1.0pt;padding-right:1.0pt;">243K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.18.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.19">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.19.1" style="padding-left:1.0pt;padding-right:1.0pt;">Panda-70M <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib8" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.2" style="padding-left:1.0pt;padding-right:1.0pt;">Open-source Model</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.3" style="padding-left:1.0pt;padding-right:1.0pt;">70.8M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.4" style="padding-left:1.0pt;padding-right:1.0pt;">166.8Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.5" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.6" style="padding-left:1.0pt;padding-right:1.0pt;">70.8M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.19.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.20">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A3.T7.3.20.1" style="padding-left:1.0pt;padding-right:1.0pt;">LLaVA-Hound <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib67" title="">2024d</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.2" style="padding-left:1.0pt;padding-right:1.0pt;">GPT-4V</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.3" style="padding-left:1.0pt;padding-right:1.0pt;">900K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.4" style="padding-left:1.0pt;padding-right:1.0pt;">3Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.008</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.6" style="padding-left:1.0pt;padding-right:1.0pt;">900K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.7" style="padding-left:1.0pt;padding-right:1.0pt;">900K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A3.T7.3.20.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.21">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T7.3.21.1" style="padding-left:1.0pt;padding-right:1.0pt;">ShareGPT4Video <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib7" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.2" style="padding-left:1.0pt;padding-right:1.0pt;">GPT-4V</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.3" style="padding-left:1.0pt;padding-right:1.0pt;">40K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.2Khr</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.15</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.6" style="padding-left:1.0pt;padding-right:1.0pt;">40K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.7" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A3.T7.3.21.8" style="padding-left:1.0pt;padding-right:1.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="A3.T7.3.22" style="background-color:#F5FFFA;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r" id="A3.T7.3.22.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.1.1" style="background-color:#F5FFFA;">LLaVA-Video-178K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.2.1" style="background-color:#F5FFFA;">GPT-4o</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.3.1" style="background-color:#F5FFFA;">178K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.4.1" style="background-color:#F5FFFA;">2Khr</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.5.1" style="background-color:#F5FFFA;">1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.6.1" style="background-color:#F5FFFA;">178K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.7.1" style="background-color:#F5FFFA;">960K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.T7.3.22.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T7.3.22.8.1" style="background-color:#F5FFFA;">196K</span></td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">We provide a more comprehensive comparison of <span class="ltx_text" id="A3.SS3.p1.1.1">LLaVA-Video-178K</span> with other video-language datasets for the video caption task and video question answer task. Specifically, we organize the table into four groups, each characterized by its method of text annotation. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.T7" title="Table 7 ‣ C.3 Dataset Comparison ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a>, unlike other datasets, <span class="ltx_text" id="A3.SS3.p1.1.2">LLaVA-Video-178K</span> uniquely includes all three types of annotations: captions, open-ended questions, and multiple-choice questions.</p>
</div>
<figure class="ltx_table" id="A3.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Visual Representation Configurations and Performance Correlation. <math alttext="T^{\text{train}}" class="ltx_Math" display="inline" id="A3.T8.4.m1.1"><semantics id="A3.T8.4.m1.1b"><msup id="A3.T8.4.m1.1.1" xref="A3.T8.4.m1.1.1.cmml"><mi id="A3.T8.4.m1.1.1.2" xref="A3.T8.4.m1.1.1.2.cmml">T</mi><mtext id="A3.T8.4.m1.1.1.3" xref="A3.T8.4.m1.1.1.3a.cmml">train</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.T8.4.m1.1c"><apply id="A3.T8.4.m1.1.1.cmml" xref="A3.T8.4.m1.1.1"><csymbol cd="ambiguous" id="A3.T8.4.m1.1.1.1.cmml" xref="A3.T8.4.m1.1.1">superscript</csymbol><ci id="A3.T8.4.m1.1.1.2.cmml" xref="A3.T8.4.m1.1.1.2">𝑇</ci><ci id="A3.T8.4.m1.1.1.3a.cmml" xref="A3.T8.4.m1.1.1.3"><mtext id="A3.T8.4.m1.1.1.3.cmml" mathsize="70%" xref="A3.T8.4.m1.1.1.3">train</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.4.m1.1d">T^{\text{train}}</annotation><annotation encoding="application/x-llamapun" id="A3.T8.4.m1.1e">italic_T start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="T^{\text{test}}" class="ltx_Math" display="inline" id="A3.T8.5.m2.1"><semantics id="A3.T8.5.m2.1b"><msup id="A3.T8.5.m2.1.1" xref="A3.T8.5.m2.1.1.cmml"><mi id="A3.T8.5.m2.1.1.2" xref="A3.T8.5.m2.1.1.2.cmml">T</mi><mtext id="A3.T8.5.m2.1.1.3" xref="A3.T8.5.m2.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.T8.5.m2.1c"><apply id="A3.T8.5.m2.1.1.cmml" xref="A3.T8.5.m2.1.1"><csymbol cd="ambiguous" id="A3.T8.5.m2.1.1.1.cmml" xref="A3.T8.5.m2.1.1">superscript</csymbol><ci id="A3.T8.5.m2.1.1.2.cmml" xref="A3.T8.5.m2.1.1.2">𝑇</ci><ci id="A3.T8.5.m2.1.1.3a.cmml" xref="A3.T8.5.m2.1.1.3"><mtext id="A3.T8.5.m2.1.1.3.cmml" mathsize="70%" xref="A3.T8.5.m2.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.5.m2.1d">T^{\text{test}}</annotation><annotation encoding="application/x-llamapun" id="A3.T8.5.m2.1e">italic_T start_POSTSUPERSCRIPT test end_POSTSUPERSCRIPT</annotation></semantics></math> are the number of frames in the training and inference stage, respectively. <math alttext="M/{p^{2}}" class="ltx_Math" display="inline" id="A3.T8.6.m3.1"><semantics id="A3.T8.6.m3.1b"><mrow id="A3.T8.6.m3.1.1" xref="A3.T8.6.m3.1.1.cmml"><mi id="A3.T8.6.m3.1.1.2" xref="A3.T8.6.m3.1.1.2.cmml">M</mi><mo id="A3.T8.6.m3.1.1.1" xref="A3.T8.6.m3.1.1.1.cmml">/</mo><msup id="A3.T8.6.m3.1.1.3" xref="A3.T8.6.m3.1.1.3.cmml"><mi id="A3.T8.6.m3.1.1.3.2" xref="A3.T8.6.m3.1.1.3.2.cmml">p</mi><mn id="A3.T8.6.m3.1.1.3.3" xref="A3.T8.6.m3.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="A3.T8.6.m3.1c"><apply id="A3.T8.6.m3.1.1.cmml" xref="A3.T8.6.m3.1.1"><divide id="A3.T8.6.m3.1.1.1.cmml" xref="A3.T8.6.m3.1.1.1"></divide><ci id="A3.T8.6.m3.1.1.2.cmml" xref="A3.T8.6.m3.1.1.2">𝑀</ci><apply id="A3.T8.6.m3.1.1.3.cmml" xref="A3.T8.6.m3.1.1.3"><csymbol cd="ambiguous" id="A3.T8.6.m3.1.1.3.1.cmml" xref="A3.T8.6.m3.1.1.3">superscript</csymbol><ci id="A3.T8.6.m3.1.1.3.2.cmml" xref="A3.T8.6.m3.1.1.3.2">𝑝</ci><cn id="A3.T8.6.m3.1.1.3.3.cmml" type="integer" xref="A3.T8.6.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.6.m3.1d">M/{p^{2}}</annotation><annotation encoding="application/x-llamapun" id="A3.T8.6.m3.1e">italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>: number of visual tokens per frame.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T8.9">
<tr class="ltx_tr" id="A3.T8.9.4">
<td class="ltx_td ltx_border_tt" id="A3.T8.9.4.1"></td>
<td class="ltx_td ltx_border_tt" id="A3.T8.9.4.2"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="A3.T8.9.4.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A3.T8.9.4.4">in-domain</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A3.T8.9.4.5">out-of-domain</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.3">
<td class="ltx_td ltx_align_left" id="A3.T8.7.1.1" rowspan="2"><span class="ltx_text" id="A3.T8.7.1.1.1"><math alttext="T^{\text{train}}" class="ltx_Math" display="inline" id="A3.T8.7.1.1.1.m1.1"><semantics id="A3.T8.7.1.1.1.m1.1a"><msup id="A3.T8.7.1.1.1.m1.1.1" xref="A3.T8.7.1.1.1.m1.1.1.cmml"><mi id="A3.T8.7.1.1.1.m1.1.1.2" xref="A3.T8.7.1.1.1.m1.1.1.2.cmml">T</mi><mtext id="A3.T8.7.1.1.1.m1.1.1.3" xref="A3.T8.7.1.1.1.m1.1.1.3a.cmml">train</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.T8.7.1.1.1.m1.1b"><apply id="A3.T8.7.1.1.1.m1.1.1.cmml" xref="A3.T8.7.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A3.T8.7.1.1.1.m1.1.1.1.cmml" xref="A3.T8.7.1.1.1.m1.1.1">superscript</csymbol><ci id="A3.T8.7.1.1.1.m1.1.1.2.cmml" xref="A3.T8.7.1.1.1.m1.1.1.2">𝑇</ci><ci id="A3.T8.7.1.1.1.m1.1.1.3a.cmml" xref="A3.T8.7.1.1.1.m1.1.1.3"><mtext id="A3.T8.7.1.1.1.m1.1.1.3.cmml" mathsize="70%" xref="A3.T8.7.1.1.1.m1.1.1.3">train</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.7.1.1.1.m1.1c">T^{\text{train}}</annotation><annotation encoding="application/x-llamapun" id="A3.T8.7.1.1.1.m1.1d">italic_T start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" id="A3.T8.8.2.2" rowspan="3"><span class="ltx_text" id="A3.T8.8.2.2.1"><math alttext="T^{\text{test}}" class="ltx_Math" display="inline" id="A3.T8.8.2.2.1.m1.1"><semantics id="A3.T8.8.2.2.1.m1.1a"><msup id="A3.T8.8.2.2.1.m1.1.1" xref="A3.T8.8.2.2.1.m1.1.1.cmml"><mi id="A3.T8.8.2.2.1.m1.1.1.2" xref="A3.T8.8.2.2.1.m1.1.1.2.cmml">T</mi><mtext id="A3.T8.8.2.2.1.m1.1.1.3" xref="A3.T8.8.2.2.1.m1.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.T8.8.2.2.1.m1.1b"><apply id="A3.T8.8.2.2.1.m1.1.1.cmml" xref="A3.T8.8.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="A3.T8.8.2.2.1.m1.1.1.1.cmml" xref="A3.T8.8.2.2.1.m1.1.1">superscript</csymbol><ci id="A3.T8.8.2.2.1.m1.1.1.2.cmml" xref="A3.T8.8.2.2.1.m1.1.1.2">𝑇</ci><ci id="A3.T8.8.2.2.1.m1.1.1.3a.cmml" xref="A3.T8.8.2.2.1.m1.1.1.3"><mtext id="A3.T8.8.2.2.1.m1.1.1.3.cmml" mathsize="70%" xref="A3.T8.8.2.2.1.m1.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.8.2.2.1.m1.1c">T^{\text{test}}</annotation><annotation encoding="application/x-llamapun" id="A3.T8.8.2.2.1.m1.1d">italic_T start_POSTSUPERSCRIPT test end_POSTSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.3.3" rowspan="3"><span class="ltx_text" id="A3.T8.9.3.3.1"><math alttext="M/p^{2}" class="ltx_Math" display="inline" id="A3.T8.9.3.3.1.m1.1"><semantics id="A3.T8.9.3.3.1.m1.1a"><mrow id="A3.T8.9.3.3.1.m1.1.1" xref="A3.T8.9.3.3.1.m1.1.1.cmml"><mi id="A3.T8.9.3.3.1.m1.1.1.2" xref="A3.T8.9.3.3.1.m1.1.1.2.cmml">M</mi><mo id="A3.T8.9.3.3.1.m1.1.1.1" xref="A3.T8.9.3.3.1.m1.1.1.1.cmml">/</mo><msup id="A3.T8.9.3.3.1.m1.1.1.3" xref="A3.T8.9.3.3.1.m1.1.1.3.cmml"><mi id="A3.T8.9.3.3.1.m1.1.1.3.2" xref="A3.T8.9.3.3.1.m1.1.1.3.2.cmml">p</mi><mn id="A3.T8.9.3.3.1.m1.1.1.3.3" xref="A3.T8.9.3.3.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="A3.T8.9.3.3.1.m1.1b"><apply id="A3.T8.9.3.3.1.m1.1.1.cmml" xref="A3.T8.9.3.3.1.m1.1.1"><divide id="A3.T8.9.3.3.1.m1.1.1.1.cmml" xref="A3.T8.9.3.3.1.m1.1.1.1"></divide><ci id="A3.T8.9.3.3.1.m1.1.1.2.cmml" xref="A3.T8.9.3.3.1.m1.1.1.2">𝑀</ci><apply id="A3.T8.9.3.3.1.m1.1.1.3.cmml" xref="A3.T8.9.3.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="A3.T8.9.3.3.1.m1.1.1.3.1.cmml" xref="A3.T8.9.3.3.1.m1.1.1.3">superscript</csymbol><ci id="A3.T8.9.3.3.1.m1.1.1.3.2.cmml" xref="A3.T8.9.3.3.1.m1.1.1.3.2">𝑝</ci><cn id="A3.T8.9.3.3.1.m1.1.1.3.3.cmml" type="integer" xref="A3.T8.9.3.3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.9.3.3.1.m1.1c">M/p^{2}</annotation><annotation encoding="application/x-llamapun" id="A3.T8.9.3.3.1.m1.1d">italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.3.4"><span class="ltx_text ltx_font_bold" id="A3.T8.9.3.4.1">NExT-QA</span></td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.3.5"><span class="ltx_text ltx_font_bold" id="A3.T8.9.3.5.1">PerceptionTest</span></td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.3.6"><span class="ltx_text ltx_font_bold" id="A3.T8.9.3.6.1">EgoSchema</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.3.7"><span class="ltx_text ltx_font_bold" id="A3.T8.9.3.7.1">VideoMME</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.9.5.1">mc</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.9.5.2">val</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.9.5.3">test</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T8.9.5.4">wo</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.6">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="7" id="A3.T8.9.6.1"><span class="ltx_text ltx_font_italic" id="A3.T8.9.6.1.1">Training with more frames</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.7">
<td class="ltx_td ltx_align_left" id="A3.T8.9.7.1">32</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.7.2">32</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.7.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.7.4">80.4</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.7.5">68.2</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.7.6">56.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.7.7">59.1</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.8">
<td class="ltx_td ltx_align_left" id="A3.T8.9.8.1">64</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.8.2">64</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.8.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.8.4">81.4 <span class="ltx_text" id="A3.T8.9.8.4.1" style="color:#00FF00;">(+1.0)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.8.5">68.3 <span class="ltx_text" id="A3.T8.9.8.5.1" style="color:#00FF00;">(+0.1)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.8.6">58.4 <span class="ltx_text" id="A3.T8.9.8.6.1" style="color:#00FF00;">(+2.1)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.8.7">59.6 <span class="ltx_text" id="A3.T8.9.8.7.1" style="color:#00FF00;">(+0.5)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.9">
<td class="ltx_td ltx_align_left" id="A3.T8.9.9.1">110</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.9.2">110</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.9.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.9.4">82.0 <span class="ltx_text" id="A3.T8.9.9.4.1" style="color:#00FF00;">(+1.6)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.9.5">68.3 <span class="ltx_text" id="A3.T8.9.9.5.1" style="color:#00FF00;">(+0.1)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.9.6">59.1 <span class="ltx_text" id="A3.T8.9.9.6.1" style="color:#00FF00;">(+2.8)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.9.7">60.4 <span class="ltx_text" id="A3.T8.9.9.7.1" style="color:#00FF00;">(+1.3)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.10">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="7" id="A3.T8.9.10.1"><span class="ltx_text ltx_font_italic" id="A3.T8.9.10.1.1">Inference with more frames</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.11">
<td class="ltx_td ltx_align_left" id="A3.T8.9.11.1">32</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.11.2">32</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.11.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.11.4">80.4</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.11.5">68.2</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.11.6">56.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.11.7">59.1</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.12">
<td class="ltx_td ltx_align_left" id="A3.T8.9.12.1">32</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.12.2">64</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.12.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.12.4">80.7 <span class="ltx_text" id="A3.T8.9.12.4.1" style="color:#00FF00;">(+0.3)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.12.5">68.9 <span class="ltx_text" id="A3.T8.9.12.5.1" style="color:#00FF00;">(+0.7)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.12.6">56.3 <span class="ltx_text" id="A3.T8.9.12.6.1" style="color:#00FF00;">(+0.0)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.12.7">59.9 <span class="ltx_text" id="A3.T8.9.12.7.1" style="color:#00FF00;">(+0.8)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.13">
<td class="ltx_td ltx_align_left" id="A3.T8.9.13.1">32</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.13.2">110</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.13.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.13.4">80.5 <span class="ltx_text" id="A3.T8.9.13.4.1" style="color:#00FF00;">(+0.1)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.13.5">67.2 <span class="ltx_text" id="A3.T8.9.13.5.1" style="color:#FF0000;">(-1.0)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.13.6">55.2 <span class="ltx_text" id="A3.T8.9.13.6.1" style="color:#FF0000;">(-1.1)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.13.7">58.8 <span class="ltx_text" id="A3.T8.9.13.7.1" style="color:#FF0000;">(-0.3)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.14">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="7" id="A3.T8.9.14.1"><span class="ltx_text ltx_font_italic" id="A3.T8.9.14.1.1">Using more frames with fewer visual tokens per frame</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.15">
<td class="ltx_td ltx_align_left" id="A3.T8.9.15.1">32</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.15.2">32</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.15.3">729</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.15.4">79.4</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.15.5">69.5</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.15.6">58.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.15.7">59.1</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.16">
<td class="ltx_td ltx_align_left" id="A3.T8.9.16.1">110</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.16.2">110</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A3.T8.9.16.3">169</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.16.4">82.0 <span class="ltx_text" id="A3.T8.9.16.4.1" style="color:#00FF00;">(+2.6)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.16.5">68.3 <span class="ltx_text" id="A3.T8.9.16.5.1" style="color:#FF0000;">(-1.2)</span>
</td>
<td class="ltx_td ltx_align_left" id="A3.T8.9.16.6">59.1 <span class="ltx_text" id="A3.T8.9.16.6.1" style="color:#00FF00;">(+0.8)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.9.16.7">60.4 <span class="ltx_text" id="A3.T8.9.16.7.1" style="color:#00FF00;">(+1.3)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T8.9.17">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T8.9.17.1">440</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T8.9.17.2">440</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A3.T8.9.17.3">64</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T8.9.17.4">81.6 <span class="ltx_text" id="A3.T8.9.17.4.1" style="color:#00FF00;">(+2.2)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T8.9.17.5">67.2 <span class="ltx_text" id="A3.T8.9.17.5.1" style="color:#FF0000;">(-2.3)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T8.9.17.6">59.4 <span class="ltx_text" id="A3.T8.9.17.6.1" style="color:#00FF00;">(+1.1)</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T8.9.17.7">60.2 <span class="ltx_text" id="A3.T8.9.17.7.1" style="color:#00FF00;">(+1.1)</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A3.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Comparison of different video representations. The video representation <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="A3.T9.4.m1.1"><semantics id="A3.T9.4.m1.1b"><mi class="ltx_font_mathcaligraphic" id="A3.T9.4.m1.1.1" xref="A3.T9.4.m1.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="A3.T9.4.m1.1c"><ci id="A3.T9.4.m1.1.1.cmml" xref="A3.T9.4.m1.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.4.m1.1d">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="A3.T9.4.m1.1e">caligraphic_V</annotation></semantics></math> is consistent in training and inference for all methods, except that SlowFast-LLaVA considers simple representation <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="A3.T9.5.m2.1"><semantics id="A3.T9.5.m2.1b"><mi class="ltx_font_mathcaligraphic" id="A3.T9.5.m2.1.1" xref="A3.T9.5.m2.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="A3.T9.5.m2.1c"><ci id="A3.T9.5.m2.1.1.cmml" xref="A3.T9.5.m2.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.5.m2.1d">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="A3.T9.5.m2.1e">caligraphic_V</annotation></semantics></math> in training and its specified <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="A3.T9.6.m3.1"><semantics id="A3.T9.6.m3.1b"><mi class="ltx_font_mathcaligraphic" id="A3.T9.6.m3.1.1" xref="A3.T9.6.m3.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="A3.T9.6.m3.1c"><ci id="A3.T9.6.m3.1.1.cmml" xref="A3.T9.6.m3.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.6.m3.1d">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="A3.T9.6.m3.1e">caligraphic_V</annotation></semantics></math> in inference.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T9.8">
<tr class="ltx_tr" id="A3.T9.8.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" id="A3.T9.8.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" id="A3.T9.8.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_tt" id="A3.T9.8.3.3" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="2" id="A3.T9.8.3.4" style="padding-left:1.0pt;padding-right:1.0pt;">in-domain</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="2" id="A3.T9.8.3.5" style="padding-left:1.0pt;padding-right:1.0pt;">out-of-domain</td>
</tr>
<tr class="ltx_tr" id="A3.T9.7.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.7.1.2" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T9.7.1.2.1">Method</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.7.1.1" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T9.7.1.1.1"><math alttext="\mathcal{V}=(T,M,s,p)" class="ltx_Math" display="inline" id="A3.T9.7.1.1.1.m1.4"><semantics id="A3.T9.7.1.1.1.m1.4a"><mrow id="A3.T9.7.1.1.1.m1.4.5" xref="A3.T9.7.1.1.1.m1.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.T9.7.1.1.1.m1.4.5.2" xref="A3.T9.7.1.1.1.m1.4.5.2.cmml">𝒱</mi><mo id="A3.T9.7.1.1.1.m1.4.5.1" xref="A3.T9.7.1.1.1.m1.4.5.1.cmml">=</mo><mrow id="A3.T9.7.1.1.1.m1.4.5.3.2" xref="A3.T9.7.1.1.1.m1.4.5.3.1.cmml"><mo id="A3.T9.7.1.1.1.m1.4.5.3.2.1" stretchy="false" xref="A3.T9.7.1.1.1.m1.4.5.3.1.cmml">(</mo><mi id="A3.T9.7.1.1.1.m1.1.1" xref="A3.T9.7.1.1.1.m1.1.1.cmml">T</mi><mo id="A3.T9.7.1.1.1.m1.4.5.3.2.2" xref="A3.T9.7.1.1.1.m1.4.5.3.1.cmml">,</mo><mi id="A3.T9.7.1.1.1.m1.2.2" xref="A3.T9.7.1.1.1.m1.2.2.cmml">M</mi><mo id="A3.T9.7.1.1.1.m1.4.5.3.2.3" xref="A3.T9.7.1.1.1.m1.4.5.3.1.cmml">,</mo><mi id="A3.T9.7.1.1.1.m1.3.3" xref="A3.T9.7.1.1.1.m1.3.3.cmml">s</mi><mo id="A3.T9.7.1.1.1.m1.4.5.3.2.4" xref="A3.T9.7.1.1.1.m1.4.5.3.1.cmml">,</mo><mi id="A3.T9.7.1.1.1.m1.4.4" xref="A3.T9.7.1.1.1.m1.4.4.cmml">p</mi><mo id="A3.T9.7.1.1.1.m1.4.5.3.2.5" stretchy="false" xref="A3.T9.7.1.1.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.T9.7.1.1.1.m1.4b"><apply id="A3.T9.7.1.1.1.m1.4.5.cmml" xref="A3.T9.7.1.1.1.m1.4.5"><eq id="A3.T9.7.1.1.1.m1.4.5.1.cmml" xref="A3.T9.7.1.1.1.m1.4.5.1"></eq><ci id="A3.T9.7.1.1.1.m1.4.5.2.cmml" xref="A3.T9.7.1.1.1.m1.4.5.2">𝒱</ci><vector id="A3.T9.7.1.1.1.m1.4.5.3.1.cmml" xref="A3.T9.7.1.1.1.m1.4.5.3.2"><ci id="A3.T9.7.1.1.1.m1.1.1.cmml" xref="A3.T9.7.1.1.1.m1.1.1">𝑇</ci><ci id="A3.T9.7.1.1.1.m1.2.2.cmml" xref="A3.T9.7.1.1.1.m1.2.2">𝑀</ci><ci id="A3.T9.7.1.1.1.m1.3.3.cmml" xref="A3.T9.7.1.1.1.m1.3.3">𝑠</ci><ci id="A3.T9.7.1.1.1.m1.4.4.cmml" xref="A3.T9.7.1.1.1.m1.4.4">𝑝</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.7.1.1.1.m1.4c">\mathcal{V}=(T,M,s,p)</annotation><annotation encoding="application/x-llamapun" id="A3.T9.7.1.1.1.m1.4d">caligraphic_V = ( italic_T , italic_M , italic_s , italic_p )</annotation></semantics></math></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T9.7.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">#Visual</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.7.1.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T9.7.1.4.1">NExT-QA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.7.1.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T9.7.1.5.1">PerceptionTest</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.7.1.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T9.7.1.6.1">EgoSchema</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.7.1.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T9.7.1.7.1">VideoMME</span></td>
</tr>
<tr class="ltx_tr" id="A3.T9.8.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T9.8.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">Tokens</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">mc</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.4.3" style="padding-left:1.0pt;padding-right:1.0pt;">val</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">wo</td>
</tr>
<tr class="ltx_tr" id="A3.T9.8.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">Simple representation</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.5.2" style="padding-left:1.0pt;padding-right:1.0pt;">(32, 729, 1, 2)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A3.T9.8.5.3" style="padding-left:1.0pt;padding-right:1.0pt;">5,408</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.5.4" style="padding-left:1.0pt;padding-right:1.0pt;">80.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.5.5" style="padding-left:1.0pt;padding-right:1.0pt;">68.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.5.6" style="padding-left:1.0pt;padding-right:1.0pt;">56.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T9.8.5.7" style="padding-left:1.0pt;padding-right:1.0pt;">59.1</td>
</tr>
<tr class="ltx_tr" id="A3.T9.8.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A3.T9.8.2.1.1">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline" id="A3.T9.8.2.1.1.m1.1"><semantics id="A3.T9.8.2.1.1.m1.1a"><msub id="A3.T9.8.2.1.1.m1.1.1" xref="A3.T9.8.2.1.1.m1.1.1.cmml"><mi id="A3.T9.8.2.1.1.m1.1.1a" xref="A3.T9.8.2.1.1.m1.1.1.cmml"></mi><mi id="A3.T9.8.2.1.1.m1.1.1.1" xref="A3.T9.8.2.1.1.m1.1.1.1.cmml">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content" id="A3.T9.8.2.1.1.m1.1b"><apply id="A3.T9.8.2.1.1.m1.1.1.cmml" xref="A3.T9.8.2.1.1.m1.1.1"><ci id="A3.T9.8.2.1.1.m1.1.1.1.cmml" xref="A3.T9.8.2.1.1.m1.1.1.1">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.8.2.1.1.m1.1c">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun" id="A3.T9.8.2.1.1.m1.1d">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">(64, 729, 3, 2)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T9.8.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">5,396</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">81.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.2.5" style="padding-left:1.0pt;padding-right:1.0pt;">67.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.2.6" style="padding-left:1.0pt;padding-right:1.0pt;">57.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.2.7" style="padding-left:1.0pt;padding-right:1.0pt;">59.8</td>
</tr>
<tr class="ltx_tr" id="A3.T9.8.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">LITA</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.6.2" style="padding-left:1.0pt;padding-right:1.0pt;">(42, 729, 2, 2)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id="A3.T9.8.6.3" style="padding-left:1.0pt;padding-right:1.0pt;">5,313</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.6.4" style="padding-left:1.0pt;padding-right:1.0pt;">80.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.6.5" style="padding-left:1.0pt;padding-right:1.0pt;">68.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.6.6" style="padding-left:1.0pt;padding-right:1.0pt;">54.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A3.T9.8.6.7" style="padding-left:1.0pt;padding-right:1.0pt;">59.1</td>
</tr>
<tr class="ltx_tr" id="A3.T9.8.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T9.8.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">SlowFast-LLaVA</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T9.8.7.2" style="padding-left:1.0pt;padding-right:1.0pt;">(42, 729, 2, 2)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r" id="A3.T9.8.7.3" style="padding-left:1.0pt;padding-right:1.0pt;">5,313</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T9.8.7.4" style="padding-left:1.0pt;padding-right:1.0pt;">79.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T9.8.7.5" style="padding-left:1.0pt;padding-right:1.0pt;">68.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T9.8.7.6" style="padding-left:1.0pt;padding-right:1.0pt;">56.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A3.T9.8.7.7" style="padding-left:1.0pt;padding-right:1.0pt;">58.9</td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A4" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Beyond Singularity: Extensive Sampling Matters</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We perform experiments to explore how video representations affect the model’s performance. All experiments were carried out in a video-only setting, using video data with durations from 0 to 30 seconds as our training data. We focused on evaluating how the number of frames and the number of visual tokens per frame impact model performance. Regarding the frame count, it is noteworthy that observing the effects of a high number of frames—such as over 100—does not necessarily require long videos. Our results indicate that the dynamic properties of the data render even 100 frames insufficient to fully capture the condent of a 30-second video, which typically runs at 15 FPS.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.T8" title="Table 8 ‣ C.3 Dataset Comparison ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">8</span></a>, the first group shows an increase in the number of frames from 32 to 110. We set 110 frames as the upper limit to avoid overloading the GPU. With more frames, we see significant improvements in all datasets. While it’s generally expected that using more frames boosts performance, previous studies <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib34" title="">2021</a>; Lei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib21" title="">2021</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib22" title="">2022</a>)</cite> have noted that performance tends to plateau when training with more than 16 frames. We propose that the saturation observed in earlier studies arises due to the selection of training datasets such as MSVD <cite class="ltx_cite ltx_citemacro_citep">(Chen &amp; Dolan, <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib6" title="">2011</a>)</cite> and WebVid <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#bib.bib4" title="">2021</a>)</cite>, where the video content is highly static, allowing a small number of frames to represent the entire video effectively. In contrast, the dynamic nature of the videos and the detailed nature of the annotations in <span class="ltx_text" id="A4.p2.1.1">LLaVA-Video-178K</span> allow for continuous benefits from extensive sampling</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">The second group in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.T8" title="Table 8 ‣ C.3 Dataset Comparison ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">8</span></a> demonstrates the effects of varying the number of inference frames while keeping the number of training frames constant. A modest increase in the inference frames slightly enhances performance; however, excessively increasing the number of inference frames can degrade it.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<p class="ltx_p" id="A4.p4.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A3.T8" title="Table 8 ‣ C.3 Dataset Comparison ‣ Appendix C Data ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">8</span></a>’s third group, we illustrates the trade-off between the number of frames and the number of tokens per frame. Configurations with fewer tokens per frame but more frames yield superior results, even with a lower total count of visual tokens (18,590 versus 21,632). This finding emphasizes that increasing the number of frames, rather than the tokens per frame or the total number of tokens, enhances performance. However, a balance is necessary; as the number of frames increases to 440 and the tokens per frame decreases to 64, performance drops. This observation led us to use <span class="ltx_text" id="A4.p4.1.1">LLaVA-Video<math alttext="{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}" class="ltx_Math" display="inline" id="A4.p4.1.1.m1.1"><semantics id="A4.p4.1.1.m1.1a"><msub id="A4.p4.1.1.m1.1.1" xref="A4.p4.1.1.m1.1.1.cmml"><mi id="A4.p4.1.1.m1.1.1a" xref="A4.p4.1.1.m1.1.1.cmml"></mi><mi id="A4.p4.1.1.m1.1.1.1" xref="A4.p4.1.1.m1.1.1.1.cmml">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p4.1.1.m1.1b"><apply id="A4.p4.1.1.m1.1.1.cmml" xref="A4.p4.1.1.m1.1.1"><ci id="A4.p4.1.1.m1.1.1.1.cmml" xref="A4.p4.1.1.m1.1.1.1">𝚂𝚕𝚘𝚠𝙵𝚊𝚜𝚝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.1.m1.1c">{}_{\leavevmode\nobreak\ \mathtt{SlowFast}}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.1.1.m1.1d">start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT</annotation></semantics></math></span> for video representation.</p>
</div>
</section>
<section class="ltx_appendix" id="A5" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Capabilities</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">Beyong achieve good benchmark performance, Our observations of <span class="ltx_text" id="A5.p1.1.1">LLaVA-Video</span> reveal various capabilities in video
understanding. Specifically, it show a great abilities in the understanding video using real-world knowledge,including, but not limited to:</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.p2">
<ul class="ltx_itemize" id="A5.I1">
<li class="ltx_item" id="A5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A5.I1.i1.p1">
<p class="ltx_p" id="A5.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="A5.I1.i1.p1.1.1">Optical Illusion</span>: As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A5.T11" title="Table 11 ‣ Appendix E Capabilities ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">11</span></a>, <span class="ltx_text" id="A5.I1.i1.p1.1.2">LLaVA-Video</span> recognizes that the green dragon in the video is not a real 3D object. It appears three-dimensional due to an optical illusion that affects human perception.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A5.I1.i2.p1">
<p class="ltx_p" id="A5.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="A5.I1.i2.p1.1.1">Special Domain</span>: As indicated in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A5.T11" title="Table 11 ‣ Appendix E Capabilities ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">11</span></a>, <span class="ltx_text" id="A5.I1.i2.p1.1.2">LLaVA-Video</span> understands the content within special domains in the video, such as sketches and fights in video games.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A5.I1.i3.p1">
<p class="ltx_p" id="A5.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="A5.I1.i3.p1.1.1">Unusual Action</span>: As detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A5.T12" title="Table 12 ‣ Appendix E Capabilities ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">12</span></a>, <span class="ltx_text" id="A5.I1.i3.p1.1.2">LLaVA-Video</span> identifies atypical actions in the video, such as "physical therapy" for pets, beyond ordinary activities.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i4" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A5.I1.i4.p1">
<p class="ltx_p" id="A5.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="A5.I1.i4.p1.1.1">Physical Laws</span>: As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02713v2#A5.T13" title="Table 13 ‣ Appendix E Capabilities ‣ Video Instruction Tuning with Synthetic Data"><span class="ltx_text ltx_ref_tag">13</span></a>, <span class="ltx_text" id="A5.I1.i4.p1.1.2">LLaVA-Video</span> comprehends basic physical laws demonstrated in the video, like zero gravity in space stations, which allows objects to float without falling.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="A5.1">
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A5.1.1" style="width:393.5pt;">
<div class="ltx_para ltx_noindent" id="A5.1.1.p1">
<svg class="ltx_picture" height="330.16" id="A5.1.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,330.16) matrix(1 0 0 -1 0 0) translate(25,0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 312.11 C 0 315.37 2.64 318.02 5.91 318.02 L 544.09 318.02 C 547.36 318.02 550 315.37 550 312.11 L 550 5.91 C 550 2.64 547.36 0 544.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 312.11 C 1.97 314.29 3.73 316.05 5.91 316.05 L 544.09 316.05 C 546.27 316.05 548.03 314.29 548.03 312.11 L 548.03 5.91 C 548.03 3.73 546.27 1.97 544.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 308.02)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 208.93 22.14 C 210.56 22.14 211.88 20.82 211.88 19.19 L 211.88 2.95 C 211.88 1.32 210.56 0 208.93 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 208.93 22.14 C 210.56 22.14 211.88 20.82 211.88 19.19 L 211.88 2.95 C 211.88 1.32 210.56 0 208.93 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text color="#000000" transform="matrix(1 0 0 -1 0 0)">Real-world Knowledge: Illusion</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject class="ltx_minipage" color="#000000" height="284.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="366.2pt">
<div class="ltx_inline-block ltx_transformed_outer" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:353.6pt;height:205.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.9pt,34.1pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_align_middle" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<tr class="ltx_tr" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<td class="ltx_td" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:384.1pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="243" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="x10.png" width="735"/></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">
<td class="ltx_td ltx_align_left" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1"><span class="ltx_text" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1">
<span class="ltx_p" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.1" style="width:384.1pt;">Please describe this video.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"><span class="ltx_text" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1">
<span class="ltx_p" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.1" style="width:384.1pt;">The video features a person interacting with a large, cartoonish green dragon cutout in a well-lit, spacious room adorned with framed pictures on the walls. The person, dressed in a blue tank top and light blue jeans, is seen hugging and adjusting the dragon cutout, which has a wide, toothy grin and large, expressive eyes. The text overlay reads ’Reply to seloutria’s comment: Once you see it you can’t un see it,’ suggesting that the cutout is designed to be visually striking and memorable. The person moves around the cutout, showcasing different angles and perspectives.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1"><span class="ltx_text" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1">
<span class="ltx_p" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1.1" style="width:384.1pt;">Is this green dragon a real 3D structure?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1"><span class="ltx_text" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1">
<span class="ltx_p" id="A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1.1" style="width:384.1pt;">No, the green dragon is a 2D cutout that creates an optical illusion.</span>
</span>
</td>
</tr>
</table>
</span></div></foreignobject></g></g></svg>
</div>
<figure class="ltx_table" id="A5.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span><span class="ltx_text" id="A5.T10.3.1">LLaVA-Video</span> learns to understand the <span class="ltx_text ltx_font_italic" id="A5.T10.4.2">optical illusion</span> in the video.</figcaption>
</figure>
</div>
</figure>
<figure class="ltx_table" id="A5.2">
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A5.2.1" style="width:393.5pt;">
<div class="ltx_para ltx_noindent" id="A5.2.1.p1">
<svg class="ltx_picture" height="638" id="A5.2.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,638) matrix(1 0 0 -1 0 0) translate(25,0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 619.96 C 0 623.22 2.64 625.86 5.91 625.86 L 544.09 625.86 C 547.36 625.86 550 623.22 550 619.96 L 550 5.91 C 550 2.64 547.36 0 544.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 619.96 C 1.97 622.13 3.73 623.89 5.91 623.89 L 544.09 623.89 C 546.27 623.89 548.03 622.13 548.03 619.96 L 548.03 5.91 C 548.03 3.73 546.27 1.97 544.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 615.86)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 259.4 22.14 C 261.03 22.14 262.35 20.82 262.35 19.19 L 262.35 2.95 C 262.35 1.32 261.03 0 259.4 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 259.4 22.14 C 261.03 22.14 262.35 20.82 262.35 19.19 L 262.35 2.95 C 262.35 1.32 261.03 0 259.4 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text color="#000000" transform="matrix(1 0 0 -1 0 0)">Real-world Knowledge: Special Domain</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject class="ltx_minipage" color="#000000" height="592.34" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="366.2pt">
<div class="ltx_inline-block ltx_transformed_outer" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:353.6pt;height:428.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.9pt,71.2pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_align_middle" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
<tr class="ltx_tr" id="A5.2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<td class="ltx_td" id="A5.2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:384.1pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="244" id="A5.2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="x11.png" width="735"/></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3">
<td class="ltx_td ltx_align_left" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1"><span class="ltx_text" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2.1.1" style="width:384.1pt;">Please describe this video.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1"><span class="ltx_text" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2.1.1" style="width:384.1pt;">The video begins with a blank, lined sheet of paper. A hand holding a black pen starts drawing on the paper, creating the outline of a flower with five petals. The artist adds details to the petals, including lines and shading to give them a more realistic appearance. The stem and leaves are also drawn, completing the flower illustration. The artist adds final touches to the flower drawing, enhancing the details and shading to give it a more realistic appearance. The artist then switches to a yellow pen and begins coloring the center of the flower, adding depth and contrast to the drawing. The video concludes with the artist adding final touches to the coloring, enhancing the overall appearance of the flower illustration.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1"><span class="ltx_text" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2.1.1" style="width:384.1pt;">What could this flower be?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1"><span class="ltx_text" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2.1.1" style="width:384.1pt;">This flower could be a magnolia or a similar species.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
<td class="ltx_td ltx_border_tt" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1" style="width:384.1pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="247" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.g1" src="x12.png" width="735"/></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7">
<td class="ltx_td ltx_align_left" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.1"><span class="ltx_text" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2.1.1" style="width:384.1pt;">Please describe this video.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.1"><span class="ltx_text" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2">
<span class="ltx_inline-block ltx_align_top" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2.1">
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2.1.1" style="width:384.1pt;">The video showcases an intense battle scene set in a snowy, mountainous landscape. A character dressed in dark, heavy armor with glowing orange accents wields a sword and faces off against a large, menacing creature with glowing red eyes and antlers. The text ’EVERY BATTLE SCREAMS GOTY POTENTIAL!’ is prominently displayed at the top of each frame, emphasizing the high stakes and epic nature of the confrontation.</span>
<span class="ltx_p" id="A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2.1.2">The battle is fierce, with the character and the creature exchanging powerful attacks. The creature emits blue energy and lightning, while the character retaliates with fiery and icy attacks. The snowy ground is littered with debris and frost, adding to the harsh and brutal environment. The background features a large body of water and a distant lighthouse, enhancing the sense of isolation and danger.</span>
</span>
</td>
</tr>
</table>
</span></div></foreignobject></g></g></svg>
</div>
<figure class="ltx_table" id="A5.T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span><span class="ltx_text" id="A5.T11.3.1">LLaVA-Video</span> learns to understand the the video in <span class="ltx_text ltx_font_italic" id="A5.T11.4.2">special domain</span>.</figcaption>
</figure>
</div>
</figure>
<figure class="ltx_table" id="A5.3">
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A5.3.1" style="width:393.5pt;">
<div class="ltx_para ltx_noindent" id="A5.3.1.p1">
<svg class="ltx_picture" height="343.36" id="A5.3.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,343.36) matrix(1 0 0 -1 0 0) translate(25,0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 325.32 C 0 328.58 2.64 331.22 5.91 331.22 L 544.09 331.22 C 547.36 331.22 550 328.58 550 325.32 L 550 5.91 C 550 2.64 547.36 0 544.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 325.32 C 1.97 327.49 3.73 329.25 5.91 329.25 L 544.09 329.25 C 546.27 329.25 548.03 327.49 548.03 325.32 L 548.03 5.91 C 548.03 3.73 546.27 1.97 544.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 321.22)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 258.9 22.14 C 260.53 22.14 261.85 20.82 261.85 19.19 L 261.85 2.95 C 261.85 1.32 260.53 0 258.9 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 258.9 22.14 C 260.53 22.14 261.85 20.82 261.85 19.19 L 261.85 2.95 C 261.85 1.32 260.53 0 258.9 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text color="#000000" transform="matrix(1 0 0 -1 0 0)">Real-world Knowledge: Unusual Action</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject class="ltx_minipage" color="#000000" height="297.7" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="366.2pt">
<div class="ltx_inline-block ltx_transformed_outer" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:353.6pt;height:215.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.9pt,35.7pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_align_middle" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<tr class="ltx_tr" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<td class="ltx_td" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:384.1pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="245" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="x13.png" width="735"/></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">
<td class="ltx_td ltx_align_left" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1"><span class="ltx_text" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1">
<span class="ltx_p" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.1" style="width:384.1pt;">Please describe this video.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"><span class="ltx_text" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1">
<span class="ltx_p" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.1" style="width:384.1pt;">The video begins with a person holding a tabby cat, with the text ’DOGS WITH BACK PAIN’ and a crying emoji displayed at the bottom of the screen. The person, who has tattoos on their arms and is wearing a light-colored shirt, is in a room with large windows and a white wall. The scene transitions to the person petting a large, brown and white dog, who is sitting on the floor. The person then stands up and gestures with their hands, while the dog looks up at them. The next scene shows the person kneeling and petting a large, fluffy dog with a black and white coat, who is sitting on a table. The person continues to pet the dog, who appears calm and relaxed.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1"><span class="ltx_text" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1.1">User</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1">
<span class="ltx_p" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1.1" style="width:384.1pt;">What did this man do to his pets?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1"><span class="ltx_text" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1.1">LLaVA-Video</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1">
<span class="ltx_p" id="A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1.1" style="width:384.1pt;">This man provided physical therapy to his pets.</span>
</span>
</td>
</tr>
</table>
</span></div></foreignobject></g></g></svg>
</div>
<figure class="ltx_table" id="A5.T12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span><span class="ltx_text" id="A5.T12.3.1">LLaVA-Video</span> learns to understand the <span class="ltx_text ltx_font_italic" id="A5.T12.4.2">unusual action</span> in the video.</figcaption>
</figure>
</div>
</figure>
<figure class="ltx_table" id="A5.4">
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A5.4.1" style="width:393.5pt;">
<div class="ltx_para ltx_noindent" id="A5.4.1.p1">
<svg class="ltx_picture" height="290.86" id="A5.4.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,290.86) matrix(1 0 0 -1 0 0) translate(25,0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 272.82 C 0 276.08 2.64 278.72 5.91 278.72 L 544.09 278.72 C 547.36 278.72 550 276.08 550 272.82 L 550 5.91 C 550 2.64 547.36 0 544.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 272.82 C 1.97 274.99 3.73 276.75 5.91 276.75 L 544.09 276.75 C 546.27 276.75 548.03 274.99 548.03 272.82 L 548.03 5.91 C 548.03 3.73 546.27 1.97 544.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 268.72)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 250.13 22.14 C 251.76 22.14 253.09 20.82 253.09 19.19 L 253.09 2.95 C 253.09 1.32 251.76 0 250.13 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 250.13 22.14 C 251.76 22.14 253.09 20.82 253.09 19.19 L 253.09 2.95 C 253.09 1.32 251.76 0 250.13 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text color="#000000" transform="matrix(1 0 0 -1 0 0)">Real-world Knowledge: Physical Laws</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="245.2" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="506.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:366.2pt;">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:353.6pt;height:177.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.9pt,29.4pt) scale(0.75,0.75) ;">
<span class="ltx_tabular ltx_align_middle" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_tr" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_td" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:384.1pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="148" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="x14.png" width="735"/></span>
</span></span></span>
<span class="ltx_tr" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1"><span class="ltx_text" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1">User</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1">
<span class="ltx_p" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.1" style="width:384.1pt;">Please describe this video.</span>
</span></span></span>
<span class="ltx_tr" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"><span class="ltx_text" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.1">LLaVA-Video</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1">
<span class="ltx_p" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.1" style="width:384.1pt;">The video features a person in a space station environment, wearing a dark gray shirt and a wristwatch. They are holding two containers filled with a thick, yellowish substance. The person tilts the containers to demonstrate the fluid’s behavior, showing how it stretches and moves between the containers. The background includes various equipment and panels typical of a space station, with labels such as ’GALLEY’ and ’FUSION’. The person continues to manipulate the containers, emphasizing the fluid’s properties. The video concludes with the person holding one container and twisting the lid to close it, further demonstrating the fluid’s properties.</span>
</span></span></span>
<span class="ltx_tr" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1"><span class="ltx_text" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1.1">User</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1">
<span class="ltx_p" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1.1" style="width:384.1pt;">Why doesn’t this yellowish substance fall?</span>
</span></span></span>
<span class="ltx_tr" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1"><span class="ltx_text" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1.1">LLaVA-Video</span></span>
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1">
<span class="ltx_p" id="A5.4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1.1" style="width:384.1pt;">This yellowish substance doesn’t fall because it is in zero gravity.</span>
</span></span></span>
</span>
</span></span>
</span></foreignobject></g></g></svg>
</div>
<figure class="ltx_table" id="A5.T13">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span><span class="ltx_text" id="A5.T13.3.1">LLaVA-Video</span> learns to understand the <span class="ltx_text ltx_font_italic" id="A5.T13.4.2">physical laws</span> in the video.</figcaption>
</figure>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 13:32:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
