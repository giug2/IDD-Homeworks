<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.15864] FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems</title><meta property="og:description" content="The evolution of autonomous systems in the context of human-robot interaction systems necessitates a synergy between the continuous perception of the environment and the potential actions to navigate or interact within…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.15864">

<!--Generated on Thu Sep  5 16:40:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Timothée Dhaussy
<br class="ltx_break">LIA-CERI, Avignon University
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">timothee.dhaussy@univ-avignon.fr</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bassam Jabaian
<br class="ltx_break">LIA-CERI, Avignon University
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">bassam.jabaian@univ-avignon.fr</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fabrice Lefèvre
<br class="ltx_break">LIA-CERI, Avignon University
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">fabrice.lefevre@univ-avignon.fr</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">The evolution of autonomous systems in the context of human-robot interaction systems necessitates a synergy between the continuous perception of the environment and the potential actions to navigate or interact within it. We present Flowact, a proactive multimodal human-robot interaction architecture, working as an asynchronous endless loop of robot sensors into actuators and organized by two controllers, the Environment State Tracking (EST) and the Action Planner. The EST continuously collects and publishes a representation of the operative environment, ensuring a steady flow of perceptual data. This persistent perceptual flow is pivotal for our advanced Action Planner which orchestrates a collection of modular action subsystems, such as movement and speaking modules, governing their initiation or cessation based on the evolving environmental narrative. The EST employs a fusion of diverse sensory modalities to build a rich, real-time representation of the environment that is distributed to the Action Planner. This planner uses a decision-making framework to dynamically coordinate action modules, allowing them to respond proactively and coherently to changes in the environment. Through a series of real-world experiments, we exhibit the efficacy of the system in maintaining a continuous perception-action loop, substantially enhancing the responsiveness and adaptability of autonomous pro-active agents. The modular architecture of the action subsystems facilitates easy extensibility and adaptability to a broad spectrum of tasks and scenarios.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>*This work has been supported by French ANR project ANR-20-CE33-0008-01 muDialBot, ”MUlti-party perceptually-active situated DIALog for human-roBOT interaction”</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human-robot Interaction (HRI) has undergone a transformative journey, evolving from basic, task-oriented engagements to sophisticated, context-aware interactions that mirror human-like dynamism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. As robots become an integral part of our daily environments, there is a growing demand for systems that can continuously perceive, comprehend, and act within their surroundings in a way that is both intuitive and adaptive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The concept of continuous perception, where an autonomous entity is perpetually sensing and interpreting its environment, has become a fundamental pillar for modern HRI systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This is a departure from traditional systems that predominantly operated in a reactive mode, responding to stimuli based on pre-set algorithms or rules.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The compartmentalization of specific functionalities into distinct modules, such as those for movement or speech, has been recognized as a pivotal advancement in the field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. This modular approach not only ensures that the system remains relevant in various scenarios, but also facilitates the seamless integration of new functionalities and the optimization of existing ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present FlowAct, a proactive multimodal system that exemplifies the fusion of continuous perception with action planning and monitoring. Anchored in the innovative Environment State Tracker, FlowAct offers a nuanced representation of its surroundings, setting the stage for more informed and dynamic interactions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Interactive robots designed for human interaction have found widespread applications across diverse sectors. They are increasingly being used in service-oriented roles, such as serving as waiters in restaurants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, working as customer guides in shopping malls <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, or assisting passengers in train stations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to name a few. Moreover, these systems have made significant inroads into the healthcare sector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">To build an HRI system, the architecture should integrate various software components to facilitate efficient and concurrent execution of multiple tasks. These systems need to possess key capabilities, including recording historical events <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and constructing representations of others’ actions, beliefs, desires, and intentions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In their paper, Moulin-Firer <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> propose a cognitive architecture organization based on the Distributed Adaptive Control (DAC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> that deals with the processing of states of the world, or exteroception, the self, or interception, and action. Furthermore, the RoboCog model of the ADAPTA project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> enabled a sales robot to persuade potential customers to approach a sales booth. This robot was capable of identifying customers, gauging their willingness to follow, and responding to specific queries.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The BRILLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> architecture for bartender Social Robot, follows a three layers organization for its architecture: the execution layer, context awareness and decision-making layer and percepts layer, all implemented within the Robot Operating System (ROS), an efficient software with libraries and tools to build robot applications.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Although the architectures described above are specifically designed and adapted to a particular application task, they share a common processing structure with three layers: the perception layer, the representation layer and the action layer.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FlowAct model</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the following subsection, the specific features of FlowAct, a system designed to act as a perpetual conduit for sensory inputs and the resulting actions, are described. The overall structure follows the DAC-H3 architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The agent interacting with the world executes an infinite loop, taking sensors as inputs and actuators as outputs. The cognitive architecture involves three stages: perceiving the world through sensors, representing the internal scene to the agent, and decision-making.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Flowact separates perceptions into two levels, ”raw perceptions” and ”refined perceptions,” and adds a modular action selection mechanism. The concept of raw perception refers to the agent’s raw sensory data, reflecting the quintet of human senses, especially auditory and visual, as well as perceptions directly derived from these senses, such as depth. These elementary perceptions are intercepted by ”perception refinement modules,” a set where raw data are distilled, producing an enhanced interpretative layer of the environment, called ”refined perceptions.”</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The EST controller operates continuously, integrating both raw and refined perceptions to shape a dynamic representation of the environment. Responsible for the memory management, it can connect to a database. This controller utilizes various environmental modules, such as the re-identification of individuals and the assignment of perceptions, to update the scene analysis while considering its historical context.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The environment state is presented to the ’Action Planner’ controller, the strategic core of FlowAct. This module is responsible for the analysis of environmental data and the planning of action strategies, informing the action modules asynchronously. It has the unique ability to reflect on future actions while executing current actions. The action modules, acting as the actuators of the system, transmit precise behavioral directives to the robotic agent, thus realizing the transition from the environmental state to action within this autonomous loop.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2408.15864/assets/images/flowact.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="337" height="301" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the FlowAct architecture, a continual loop of perception and action.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Robot perceptions</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">With a height of 1.2 meters, Softbank Pepper, used in our experiments, is a robot with 20 degrees of freedom - 17 in the body and 3 in the omnidirectional base, enabling advanced holonomic control and motion planning. Its motion accuracy is enhanced by an Inertial Measurement Unit (IMU) and wheel encoders that provide odometry for dead reckoning navigation. Pepper’s obstacle avoidance system includes two sonars, two infrared sensors and six lasers - half pointing downwards for ground-level detection and the rest scanning the surrounding area.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For interactions with humans, Pepper is equipped with an array of four microphones, two loudspeakers and three tactile sensors. Non-verbal communication is activated by LED clusters located in the eyes, on the shoulders and around the ear speakers. It also possesses an Android tablet on its chest to display content or run integrated applications.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Robot perception is aided by two 640x480 resolution cameras, strategically placed on the forehead and mouth, and an ASUS Xtion 3D sensor in one eye, essential for localization and navigation. Given its size, the forehead camera is ideal for HRI, aligned with the average human body height.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Pepper runs on a 1.9GHz quad-core Atom processor and 4GB RAM, under the NAOqi OS operating system, a GNU/Linux distribution based on Gentoo. For our experiment, NAOqi 2.5.5 is used, coupled with ROS, so that the perceptions received and sent by NAOqi pass through topics (the message-passing framework implemented within ROS). These initial sensory data collected by the robot’s primary sensors are defined as the ’raw perceptions’.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Perception refiner modules</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The FlowAct approach involves augmenting the primary raw perceptions collected by the agent. This is achieved through the ’perception refinement modules.’ These modules process and enhance the initial data, yielding a more advanced and insightful set of perceptions, aptly termed ’refined perceptions.’ The refined perceptions will be combined with the raw perceptions and then sent to the environment state tracker, which will construct a representation of the scene from these data. More precisely, in our implementation, the following modules are used:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Voice activity detection</span>: detects speech segments in raw data, based on an adaptive threshold on energy of the sound signal.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Person tracker</span>: combines person location and body keypoints features from an implementation of YoloV7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> with a Deepsort algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to track the identity of the persons present in the images captured by the camera.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Gaze tracker</span>: uses RT Gene’s ROS package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> which transforms the image stream into various facial descriptors, gaze orientation, head pose and the position of key facial points.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Speaker diarization</span>: is based on a a temporal audio-visual fusion model for multi-users speaker diarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The method identifies the dominant speakers and tracks them over time by measuring the spatial coincidence between sound locations and visual presence.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Interaction acceptance belief (IAB)</span>: infers the level of IAB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> which commonly answers the question ”What chances are my interaction to be accepted by the targeted user?”, and is mainly based on the gaze of the user.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Automatic speech recognition and translation:</span> both functionality are based on API calls sending the segments of speech (detected by the VAD module) to Google speech recognition for ASR<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://cloud.google.com/speech-to-text" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/speech-to-text</a> </span></span></span> and, in case the chatbot system is based on a different language, the Deepl translation webhook<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.deepl.com/fr/docs-api" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.deepl.com/fr/docs-api</a></span></span></span> is used.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Representation of the environment</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The EST controller is responsible for constructing an accurate representation of the scene over time. It gathers refined perceptions, maintains a history, orchestrates the flow of perceptions, and provides a comprehensive representation of the environment. In this asynchronous collection of perceptions, the EST controller aligns and organizes perceptions to synchronously publish a representation of the environment.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The EST is therefore responsible for the associative memory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a concept defining the ability to link two initially unrelated elements, such as a name and an unknown face. In this context, we employ specific ’Environment analysis modules,’ like person re-identification, which tracks users in the scene, and a perception assigner, tasked with linking the perception of each detected user to a known or new identity.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Decision layer</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The agent’s behavioral dynamics is orchestrated through the synergistic operation of the ’Action Planner’ controller and a suite of ’Action modules’. The Action Planner serves as the cerebral core, tirelessly rendering decisions to either wait, return to a pre-defined (observation) position, initiate interaction, or proceed with ongoing activities. Decisions are seamlessly transmitted to the Action Modules, each designed to spring into operation responsive to the delineated behavior. Building upon the framework established by Kanda <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, we conceptualize the outputs of the action planner as ’global behavior.’ To adhere to this global behavior, ’local behaviors’ are relayed via action modules. These local behaviors are characterized by their more granular, atomic nature. The engagement behavior in the system is governed by a series of rules based one the level of the IAB model, coupled with implicit engagement requests from the user (a hand raise for instance).</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">For our experimental setup, we have implemented two pivotal Action Planner modules:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Moving Module</span>: capable of performing two distinct actions. Firstly, engaging a targeted individual by orienting the robot towards them and advancing to within 0.7 meters. Secondly, maneuvering the robot back to its observation position. It is equipped with internal states that ascertain whether the robot is stationed at its stand-by location and whether it is in the course of a navigational sequence.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Speaking Module</span>: this module governs the dialogue with users, engaging in a conversational cycle once engagement is achieved and communication parameters are established by the action modules. Answers are given by an API call to a large language model (LLM) configured as a conversational assistant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Conversation termination is contingent upon specific keyword recognition or user disengagement, including a 10-second non-response interval. This module also updates internal state indicators, ‘is speaking’ and ‘in a conversation’, to reflect the robot’s active speech and engagement in dialogue, to ensure continuity of the decisions from the Action Planner.</p>
</div>
</li>
</ul>
<p id="S3.SS5.p2.2" class="ltx_p">Together, these modules exemplify an agent’s behavior as a complex interplay of decision-making, backed by a feedback loop that refines the agent’s environmental awareness and adaptability over time.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Scenario</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In order to evaluate the effectiveness of the proposed model, we implemented the proactive interaction loop within a real-world setting, specifically designed to replicate a scenario where patients await their appointments in a hospital waiting room. To emulate the role of patients for this investigation, we enlisted a cohort of diverse participants, encompassing 20 individuals, 14 men and 6 women, all within the age range [22, 52]. Only 3 participants out of the 20 were familiar with robotics.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The participants were positioned in proximity of the robotic entity, either seated or standing. Subsequently, they were instructed to embody one of the following behaviors:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Engage in dialogues with individuals seated adjacent to them;</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Engage in active utilization of mobile devices, for instance, playing games or browsing the web;</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Exhibit a passive behavior, maintaining a stance of idleness and portraying a waiting state devoid of any particular engagement or activity;</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Display cues of interest and attentiveness towards the robotic agent;</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Initiate interaction with the robotic agent by seeking its attention or assistance, and requesting information, guidance, or support.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Upon concluding an interaction, the robot returns to its observation position to initiate another engagement. The participants are then instructed to resume their designated scenarios, which may have been interrupted during the interaction, once the robot reverts to its original position. Each scenario encompasses a blend of passive behavior towards the agent, along with active behaviors such as showing interest or requesting interaction. A scenario is considered complete either when a predetermined time limit is reached or after each participant has had an engagement with the robot.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ethical considerations</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In line with the relevant local laws and the rules of our institution, it was determined that this research involving human participants did not necessitate a dedicated ethical approval. The participants, comprising academics and students from Avignon University, gave their written consent to be part of this study. Furthermore, a written informed consent was obtained from the individuals for the publication of any identifiable images or data that might be included in publications related to the experiment.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To validate the functionality and usability of utilizing Pepper for proactive robotic interaction, we conducted a thorough evaluation focusing on user experience for checking the efficiency of the setup. The experience is seen as a task in which the robot as to display a proactive engagement behavior toward the humans muster in the room. Participant are informed of the task of the robot and rate the questionnaire accordingly to their posterior feelings.</p>
</div>
<figure id="S4.T1" class="ltx_table"><img src="/html/2408.15864/assets/images/umux.png" id="S4.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="284" height="210" alt="[Uncaptioned image]">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>UMUX questionnaire.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The User Experience Questionnaire, as outlined by Finstad’s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> study, was employed to gauge users’ interactions with the system. Each question represents a usability component evaluation of the system. Following the order of the questionnaire, we can measure: effectiveness, satisfaction, overall quality, efficiency. The Usability Metric for User Experience (UMUX), a concise and effective four-item Likert scale, was utilized for subjective evaluation of the application’s usability. This scale is strategically crafted to yield insights comparable to those derived from the more extensive 10-item System Usability Scale, ensuring thorough and reliable assessment of user experience.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Overall results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The outcomes of our experiments are reported in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.4 Overall results ‣ 4 Experiments ‣ FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The mean scores for each question were as follows: 5.56, 3.25, 6.13, and 3.13. Concerning the task performance, a significant majority of participants perceived the agent as effectively fulfilling its intended role. The robot demonstrated its capability to engage in hospital-contextual interactions, exhibiting both reactive engagement in response to explicit user signals and proactive engagement driven by the actor’s demonstrated interest in the agent.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2408.15864/assets/images/ratings_distribution.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>UMUX questionnaire results.</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The overall satisfaction with the system was mixed. While most participants did not experience excessive frustration, notable instances of frustration were primarily attributed to the response latency of the agent, typically ranging from 5 to 10 seconds. This delay was a consequence of the computational demand of the LLM-based conversational agent. Notably, participants with prior experience in robotics expressed higher levels of frustration related to this latency.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In terms of usability, the consensus was that the system was user-friendly, requiring no specific prerequisites for operation. During the experiments, it was observed that scores for Q4 tended to increase in situations where participants had to maintain eye contact with the agent for longer than anticipated (often exceeding 10 seconds), or when they were required to repeat themselves due to the robot’s inability to comprehend their speech.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2408.15864/assets/images/img_70.jpg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="288" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Screenshot of a scene, showing the IAB value for each person detected. Red box indicates a person available for engagement.</figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">In the course of the conducted trials, each experiment was executed successfully, demonstrating engagement and interaction with every participant. Notably, in two instances, the robot initiated engagement with individuals before they exhibited the reactive sign, typically a raised hand signaling the agent. This preemptive interaction by the robot was perceived by the users as proactive, as it occurred in response to their demonstrated interest prior to the conventional signal for engagement.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In summary, this research presents the FlowAct system, a preliminary approach in the field of continuous perception-action systems within a robotic context. Intial experimentation in a simulated hospital setting has highlighted the system’s proficiency in both proactive and responsive interactions with human participants. Central to this study is the development of a ROS architecture for a socially assistive robot, engineered to provide efficient services while engaging dynamically and personally with users. The comprehensive evaluation conducted in a controlled environment not only validated the functionality of each module but also the efficacy of the architecture as a whole. This evaluation specifically underscored the system’s adeptness in tailoring experiences to individual users and its agility in adapting behaviors to meet diverse needs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Emilia I. Barakova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Robots for social training of autistic children.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2011 World Congress on Information and Communication Technologies</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 14–19, 2011.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Min Chen, Francisco Herrera, and Kai Hwang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Cognitive computing: Architecture, technologies and intelligent applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 6:19774–19783, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Timothée Dhaussy, Bassam Jabaian, and Fabrice Lefèvre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Interaction acceptance modelling and estimation for a proactive engagement in the context of human-robot interactions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 3069–3074, October 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Timothée Dhaussy, Bassam Jabaian, Fabrice Lefèvre, and Radu Horaud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Audio-visual speaker diarization in the framework of multi-user human-robot interaction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 1–5, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Joshua Diehl, Charles Crowell, Michael Villano, Kristin Wier, Karen Tang, and Laurel Riek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Clinical Applications of Robots in Autism Spectrum Disorder Diagnosis and Treatment</span><span id="bib.bib6.3.2" class="ltx_text" style="font-size:90%;">, pages 411–422.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">01 2014.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kraig Finstad.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">The usability metric for user experience.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Interacting with Computers</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 22:323–327, 09 2010.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Tobias Fischer, Hyung Jin Chang, and Yiannis Demiris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Rt-gene: Real-time eye gaze estimation in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, September 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A. Garrell, M. Villamizar, F. Moreno-Noguer, and A. Sanfeliu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Teaching robot’s proactive behavior using human assistance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Social Robotics</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 9(2):231–249, Jan. 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Norina Gasteiger, Mehdi Hellou, Jong Lim, Bruce Macdonald, and Ho Ahn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">A theoretical approach to designing interactive robots, using restaurant assistants as an example.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">pages 980–985, 06 2023.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Antoni Grau, Marina Indri, Lucia Lo Bello, and Thilo Sauter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Robots in industry: The past, present, and future of a growing collaboration with humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Industrial Electronics Magazine</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 15(1):50–61, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Takayuki Kanda, Dylan F. Glas, Masahiro Shiomi, and Norihiro Hagita.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Abstracting people’s trajectories for social robots to proactively approach customers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Robotics</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 25(6):1382–1396, 2009.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Takayuki Kanda, Masahiro Shiomi, Zenta Miyashita, Hiroshi Ishiguro, and Norihiro Hagita.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">An affective guide robot in a shopping mall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 173–180, 2009.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Hugh J. McCann and M. E. Bratman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Intention, plans, and practical reason.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Noûs</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 25(2):230, apr 1991.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Juan Molina, Sergio Sierra Marín, Marcela Munera, and Carlos Cifuentes G.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Human robot interaction interface for a mobility assistance device.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">06 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Clément Moulin-Frier, Tobias Fischer, Maxime Petit, Gregoire Pointeau, Jordi-Ysard Puigbò, Ugo Pattacini, Sock Ching Low, Daniel Camilleri, Phuong Nguyen, Matej Hoffmann, Hyung Chang, Martina Zambelli, A.-L Mealier, Andreas Damianou, Giorgio Metta, Tony Prescott, Yiannis Demiris, Peter Dominey, and Paul Verschure.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Dac-h3: A proactive robot cognitive architecture to acquire and express knowledge about the world and the self.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Cognitive and Developmental Systems</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 10:1005–1022, 12 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Tony Prescott, Daniel Camilleri, Uriel Martinez-Hernandez, Andreas Damianou, and Neil Lawrence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Memory and mental time travel in humans and social robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Philosophical Transactions B: Biological Sciences</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 374, 03 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Tony J Prescott, Daniel Camilleri, Uriel Martinez-Hernandez, Andreas Damianou, and Neil D Lawrence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Memory and mental time travel in humans and social robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Philos. Trans. R. Soc. Lond. B Biol. Sci.</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 374(1771):20180025, Apr. 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Adrián Romero-Garcés, Luis Vicente Calderita, Jesús Martínez-Gómez, Juan Pedro Bandera, Rebeca Marfil, Luis J. Manso, Antonio Bandera, and Pablo Bustos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Testing a fully autonomous robotic salesman in real scenarios.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE International Conference on Autonomous Robot Systems and Competitions</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 124–130, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Alessandra Rossi, Maria Di Maro, Antonio Origlia, Agostino Palmiero, and Silvia Rossi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">A ros architecture for personalised hri with a bartender social robot, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Ralf Salomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Scaling behavior of the evolution strategy when evolving neuronal control architectures for autonomous agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In Peter J. Angeline, Robert G. Reynolds, John R. McDonnell, and Russ Eberhart, editors, </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Evolutionary Programming VI</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 47–57, Berlin, Heidelberg, 1997. Springer Berlin Heidelberg.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
M. Shiomi, D. Sakamoto, T. Kanda, C. T. Ishi, H. Ishiguro, and N. Hagita.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Field trial of a networked robot at a train station.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Social Robotics</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 3(1):27–40, 2011.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jan Tekülve, Adrien Fois, Yulia Sandamirskaya, and Gregor Schöner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Autonomous sequence generation for a neural dynamic robot: Scene perception, serial order, and object-oriented movement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Frontiers in Neurorobotics</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 13, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Paul F M J Verschure, Cyriel M A Pennartz, and Giovanni Pezzulo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">The why, what, where, when and how of goal-directed choice: neuronal and computational principles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Philos. Trans. R. Soc. Lond. B Biol. Sci.</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 369(1655):20130483, Nov. 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Paul F M J Verschure, Thomas Voegtlin, and Rodney J Douglas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Environmentally mediated synergy between perception and behaviour in mobile robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 425(6958):620–624, Oct. 2003.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Nicolai Wojke, Alex Bewley, and Dietrich Paulus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Simple online and realtime tracking with a deep association metric, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.15863" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.15864" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.15864">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.15864" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.15865" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:40:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
