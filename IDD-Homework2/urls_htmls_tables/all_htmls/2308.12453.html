<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.12453] Augmenting medical image classifiers with synthetic data from latent diffusion models</title><meta property="og:description" content="While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA)[1], many studies have shown inconsistent generalization or latent bias, particularly f…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Augmenting medical image classifiers with synthetic data from latent diffusion models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Augmenting medical image classifiers with synthetic data from latent diffusion models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.12453">

<!--Generated on Wed Feb 28 11:24:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Augmenting medical image classifiers with synthetic data from latent diffusion models
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luke W. Sagers<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">James A. Diao<span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luke Melas-Kyriazi<span id="footnotex3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Engineering Science, University of Oxford, Oxford, UK
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matthew Groh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Kellogg School of Management, Northwestern University, Evanston, IL, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pranav Rajpurkar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adewole S. Adamson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Division of Dermatology, Dell Medical School, The University of Texas at Austin, Austin, Texas, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Veronica Rotemberg
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Dermatology Service, Memorial Sloan Kettering Cancer Center, New York, NY, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roxana Daneshjou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Dermatology, Stanford School of Medicine, Redwood City, CA, USA
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Data Science, Stanford School of Medicine, Stanford, CA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arjun K. Manrai<span id="footnotex4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Correspondence to Arjun_Manrai@hms.harvard.edu</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>, many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>, <a href="#bib.bibx3" title="" class="ltx_ref">3</a>, <a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>. Some have proposed that generative AI<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>, <a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite> could reduce the need for real data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone.
Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images.
As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using several generation strategies.
Our results suggest that synthetic data could serve as a force-multiplier for model development, but the collection of diverse real-world data remains the most important step to improve medical AI algorithms.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.2" class="ltx_p"><em id="p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.2.2" class="ltx_text ltx_font_bold">eywords</span> Generative AI  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Medical Imaging  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Diffusion Models</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Image-based machine learning algorithms are increasingly deployed in healthcare settings and directly to patients<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>, <a href="#bib.bibx8" title="" class="ltx_ref">8</a>, <a href="#bib.bibx9" title="" class="ltx_ref">9</a>, <a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>. As these algorithms enter real world use, concerns persist regarding representation of rare diseases and historically underrepresented groups<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>, <a href="#bib.bibx12" title="" class="ltx_ref">12</a>, <a href="#bib.bibx13" title="" class="ltx_ref">13</a>, <a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>. Augmenting model training with synthetic data from generative AI has been proposed as a tool for achieving desired performance and fairness objectives<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>, <a href="#bib.bibx16" title="" class="ltx_ref">16</a>, <a href="#bib.bibx17" title="" class="ltx_ref">17</a>, <a href="#bib.bibx18" title="" class="ltx_ref">18</a>, <a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>. Recent breakthroughs in latent diffusion models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>, <a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite> have enabled generation of photorealistic images conditioned on text alone, images alone, or text and images in combination, with compelling results in dermatology applications<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>, <a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite>. Some have suggested that synthetic data may obviate the need for additional data collection or substitute completely for real datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>. This view is controversial given that reported results have been inconsistent, that primary causes of performance improvements are not well-characterized, and that synthetic data could itself contain hallucinations or encode bias.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To understand the potential opportunities and pitfalls of synthetic data, our study isolates several potential sources of performance gains arising from augmenting deep learning model training using synthetic images. In particular, we focus on algorithms for classifying skin disease, since the diversity of skin pathologies and skin tones provide an informative use case for probing issues of fairness and generalizability<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>, <a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>. Using 3,699 images from the Fitzpatrick 17K<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> dataset and 656 images from the Stanford Diverse Dermatology Images (DDI) dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>, we produced 458,920 synthetic images using the latent diffusion model Stable Diffusion<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite> fine-tuned with DreamBooth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>. We then developed and evaluated skin disease classifiers under various experimental settings, including increasing amounts of real data and synthetic data, across image generation strategies, and alongside traditional data augmentation techniques.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.12453/assets/1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="628" height="573" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.4.1" class="ltx_text ltx_font_bold">A.</span> Sample sizes, label derivations, and Fitzpatrick skin type (FST) distributions for the datasets used in this study. <span id="S1.F1.5.2" class="ltx_text ltx_font_bold">B.</span> Illustrations of four synthetic generation procedures. Inpainting replaces a central square segment with new generated pixels. Outpainting replaces border regions outside this center square. In-then-outpainting performs these two procedures sequentially to replace every pixel of the image. Text-to-image uses Stable Diffusion fine-tuned using DreamBooth to generate embedding tokens from a specific class. These tokens are then used to generate new images of that class. <span id="S1.F1.6.3" class="ltx_text ltx_font_bold">C.</span> Data, methodology, and other parameters that affect training and evaluation of skin classifiers. F17k-9 refers to the subset of Fitzpatrick 17k comprising the nine most common skin diseases. </figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Results</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generating half a million synthetic images across nine disease conditions</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Model training and testing were conducted using images from the nine most common skin conditions in Fitzpatrick 17k: psoriasis (624), squamous cell carcinoma (480), lichen planus (452), basal cell carcinoma (446), allergic contact dermatitis (398), lupus erythematosus (337), sarcoidosis (326), neutrophilic dermatoses (324), and photodermatoses (312), which we term the F17k-9 dataset (Figure<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>A). The Stanford DDI dataset is comprised of images of both malignant (171) and benign (485) skin conditions. Both datasets represent the full spectrum of skin tones from Fitzpatrick I-VI (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Using text prompts with and without reference images, we generated 458,920 synthetic images; generation methods included inpainting (replacement of central pixels with generated pixels), outpainting (replacement of peripheral pixels with generated pixels), in-then-outpainting (replacement of all pixels by sequential inpainting then outpainting), and text-to-image (de novo generation from text alone) (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>B). A sample of generated images can be found in Figure S-4.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.12453/assets/2A-overlay.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="368" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">A </span>Real images by disease condition</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.12453/assets/2B-overlay.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="368" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">B </span>Real versus synthetic images</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.12453/assets/2C.png" id="S2.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="266" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">C </span>Pairwise distances between image embeddings</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.4.1" class="ltx_text ltx_font_bold">A.</span> First two principal components of image embeddings for real and synthetic images, colored by disease condition, with overlaid images of randomly selected examples.

<br class="ltx_break"><span id="S2.F2.5.2" class="ltx_text ltx_font_bold">B.</span> Same as A, but colored to indicate real or synthetic data, with an overlaid pair of a synthetic image generated using the in-then-outpaint method alongside its reference image.

<br class="ltx_break"><span id="S2.F2.6.3" class="ltx_text ltx_font_bold">C.</span> Comparison of matched and unmatched pairwise distances between image embeddings of length 512 generated using the image encoder component of the Contrastive Language-Image Pre-Training (CLIP) Vision Transformer (ViT) B-32<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite>. Matched distances between in-then-outpaint images and their respective reference images (blue) were compared to unmatched distances between real and synthetic images and between pairs of real images (red).</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">Image examples are shown overlaid on the first two principal components of image embeddings generated across real and synthetic images (Figure <a href="#S2.F2.sf1" title="In Figure 2 ‣ 2.1 Generating half a million synthetic images across nine disease conditions ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2A</span></a>, <a href="#S2.F2.sf2" title="In Figure 2 ‣ 2.1 Generating half a million synthetic images across nine disease conditions ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2B</span></a>). Non-linear projections were also produced using uniform manifold approximation and projection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite> (Figures S-3). Mean cosine distance between random real-real image pairs (0.242) was similar to that between random real-synthetic image pairs generated using the in-then-outpaint method (0.241, Benjamini-Hochberg [BH] adjusted two-sample t-test p-value: 0.62) and larger than that between these synthetic images matched to their respective reference images (0.0844, BH adjusted two-sample t-test p-value: &lt;0.0001) (Figure <a href="#S2.F2.sf3" title="In Figure 2 ‣ 2.1 Generating half a million synthetic images across nine disease conditions ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2C</span></a>).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic data augmentation improves performance in low-data settings and is additive with image transforms</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">We trained and tested skin disease classifiers on a one-of-nine prediction task using the F17k-9 dataset. Two types of training augmentation were evaluated: synthetic augmentations, comprising 10 synthetic images for each real image, or image transforms, comprising random flipping, cropping, rotating, warping, and lighting changes. As expected, classifiers improved with more real images and with image transforms (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2 Synthetic data augmentation improves performance in low-data settings and is additive with image transforms ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In addition, synthetic training data improved accuracy of classifiers with and without image transforms. Accuracy gains were highest at 16, 32, or 64 real images per disease class. At 32 real images per disease label and with image transforms, adding synthetic data improved accuracy by up to 13.2% (BH adjusted p-value = <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="9.0\times 10^{-4}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">9.0</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">×</mo><msup id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml"><mn id="S2.SS2.p1.1.m1.1.1.3.2" xref="S2.SS2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS2.p1.1.m1.1.1.3.3" xref="S2.SS2.p1.1.m1.1.1.3.3.cmml"><mo id="S2.SS2.p1.1.m1.1.1.3.3a" xref="S2.SS2.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS2.p1.1.m1.1.1.3.3.2" xref="S2.SS2.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn type="float" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">9.0</cn><apply id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.p1.1.m1.1.1.3.2">10</cn><apply id="S2.SS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3"><minus id="S2.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">9.0\times 10^{-4}</annotation></semantics></math>). Accuracy gains diminished at sufficiently large samples of real images; at 228 real images per disease label and with image transforms, accuracy changed by 2.0% (95% bootstrapped CI: -1.2%, 5.3%) for inpainting, -0.4% (95% CI: -3.0%, 2.0%) for in-then-outpainting, and -2.2% for text-to-image generation (95% CI: -5.5%, 1.1%), with none meeting significance thresholds after adjusting for multiple testing.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Adding synthetic data occasionally improved a data-scarce model to the performance of a model trained with more real data. For example, using 32 real images per condition with image transforms and synthetic data (1:10 real:synthetic ratio) from in-then-outpainting performed similarly to using 64 real images with image transforms and without synthetic data (29.4% vs. 26.3%, p-value = 0.39) (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2 Synthetic data augmentation improves performance in low-data settings and is additive with image transforms ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Performance gains from synthetic data augmentation varied by disease condition and generation method (Figure S-1). Performance increased for all disease conditions except sarcoidosis when augmented using the text-to-image generation method. The greatest performance gains were accrued for prediction of allergic contact dermatitis using the text-to-image generation method.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2308.12453/assets/3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance of models trained for nine-way skin classification while varying the number of real images (1 to 228), use of synthetic images (0 or 10 synthetic images per real image), and use of an image transform augmentation utility (yes or no). Model accuracy is averaged across five runs on a balanced, held-out test set of 360 images. Synthetic augmentation was performed using one of three methods: inpaint only, in-then-outpaint, and text-to-image. Error bars represent 95% confidence intervals computed using the Wald method. Asterisks represent p-values computed using two-sample proportion tests comparing mean accuracy with and without synthetic data at each number of real training images, adjusted using the Benjamini-Hochberg procedure: *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Improvements from synthetic data exhibit dose-response and saturation effects</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">We trained skin disease classifiers on a one-of-nine prediction task while adding increasing numbers of synthetic images to the training set. Starting with training sets of 16, 32, and 64 real images per disease class, we added 0, 10, 25, 50, or 75 synthetic images per real image; synthetic examples were generated using the text-to-image method due to computational expense (Figure <a href="#S2.F4" title="Figure 4 ‣ 2.3 Improvements from synthetic data exhibit dose-response and saturation effects ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). With 16 or 32 real images per class, adding 10 synthetic images per real image led to significant increases in accuracy, up to an absolute increase of 13.3% (BH adjusted p-value = <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="5.3\times 10^{-4}" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mn id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">5.3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">×</mo><msup id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml"><mn id="S2.SS3.p1.1.m1.1.1.3.2" xref="S2.SS3.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p1.1.m1.1.1.3.3" xref="S2.SS3.p1.1.m1.1.1.3.3.cmml"><mo id="S2.SS3.p1.1.m1.1.1.3.3a" xref="S2.SS3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS3.p1.1.m1.1.1.3.3.2" xref="S2.SS3.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><cn type="float" id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">5.3</cn><apply id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.2">10</cn><apply id="S2.SS3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3"><minus id="S2.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">5.3\times 10^{-4}</annotation></semantics></math>). Further increases up to 75-fold augmentation sometimes improved performance, but effects were neither monotonic nor statistically significant (Figure <a href="#S2.F4" title="Figure 4 ‣ 2.3 Improvements from synthetic data exhibit dose-response and saturation effects ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2308.12453/assets/4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="471" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Model performance improvements from augmenting training with 0, 10, 25, 50, or 75 synthetic images per real training image, stratified by number of real images available (16, 32, or 64). Model performance is reported as the mean accuracy across five runs on a held-out test set. Evaluations were conducted with and without image transforms (e.g., flipping, cropping, rotating, and zooming). Synthetic images were produced using the text-to-image procedure DreamBooth. Error bars represent 95% confidence intervals computed using the Wald method. Asterisks represent p-values computed using two-sample proportion tests and adjusted using the Benjamini-Hochberg procedure: *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Synthetic augmentation improves skin malignancy classifiers across skin types</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">We trained skin disease classifiers to classify malignant versus benign skin conditions in the Stanford DDI dataset and compared classifier performance between Fitzpatrick Skin Type (FST) labels. Adding synthetic images improved the accuracy of malignancy classification for all FST categories. Notable improvements include 73.1% to 80.0% (difference of +6.9%, 95% CI: 1.2%, 13.1%) in FST I-II using the in-then-outpainting method, 70.0% to 81.9% (difference of +11.9%, 95% CI: 5.6%, 17.8%) in FST III-IV using the inpainting method, and 76.9% to 84.4% (difference of +7.5%, 95% CI: 2.8%, 13.8%) in FST V-VI using the inpainting method (Figure <a href="#S2.F5" title="Figure 5 ‣ 2.4 Synthetic augmentation improves skin malignancy classifiers across skin types ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Although adding synthetic images consistently improves accuracy, only one comparison, the addition of synthetic data without image transforms, remained significant following correction for multiple hypothesis testing (31.3% vs. 17.9%, BH adjusted p-value = <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="5.3\times 10^{-4}" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><mrow id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mn id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">5.3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p1.1.m1.1.1.1" xref="S2.SS4.p1.1.m1.1.1.1.cmml">×</mo><msup id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml"><mn id="S2.SS4.p1.1.m1.1.1.3.2" xref="S2.SS4.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS4.p1.1.m1.1.1.3.3" xref="S2.SS4.p1.1.m1.1.1.3.3.cmml"><mo id="S2.SS4.p1.1.m1.1.1.3.3a" xref="S2.SS4.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS4.p1.1.m1.1.1.3.3.2" xref="S2.SS4.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><times id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1.1"></times><cn type="float" id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">5.3</cn><apply id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.3.1.cmml" xref="S2.SS4.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS4.p1.1.m1.1.1.3.2.cmml" xref="S2.SS4.p1.1.m1.1.1.3.2">10</cn><apply id="S2.SS4.p1.1.m1.1.1.3.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3"><minus id="S2.SS4.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.SS4.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">5.3\times 10^{-4}</annotation></semantics></math>).</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2308.12453/assets/5.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="638" height="456" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Model performance improvements from augmenting training with 0, 10, 25, 50, or 75 synthetic images per real training image, stratified by number of real images available (16, 32, or 64). Model performance is reported as the mean accuracy across five runs on a held-out test set. Evaluations were conducted with and without image transforms (e.g., flipping, cropping, rotating, and zooming). Synthetic images were produced using the text-to-image procedure DreamBooth. Error bars represent 95% confidence intervals computed using the Wald method. Asterisks represent p-values computed using two-sample proportion tests and adjusted using the Benjamini-Hochberg procedure: *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Discussion</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this study, we systematically evaluated the effect of augmenting skin disease classifiers with synthetic images generated by latent diffusion models. In the process, we generated a repository of 458,920 synthetic images, which we will release for public use. We found that synthetic images may improve classifier performance in some data-limited settings with monotonic dose-response behavior, but that improvements saturated at synthetic-to-real data ratios above 10:1. Significant effects were observed across several generative methods and skin conditions, but the primary driver of performance was the number of real images. Prior work has suggested using synthetic images generated from majority groups to supplement groups that are underrepresented (e.g, images of dark skin tones) in training datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>. Our work demonstrates that real data is still key and suggests that imbalance in the use of real images versus synthetic images across protected classes could lead to performance differences.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">Prior work investigating the use of synthetic data across several medical domains has shown that synthetic images can improve disease and tissue classifiers<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>, <a href="#bib.bibx28" title="" class="ltx_ref">28</a>, <a href="#bib.bibx29" title="" class="ltx_ref">29</a>, <a href="#bib.bibx30" title="" class="ltx_ref">30</a>, <a href="#bib.bibx31" title="" class="ltx_ref">31</a>]</cite>, increase model performance and fairness on out of distribution data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>, <a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>, and capture feature distributions for data sharing<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>, <a href="#bib.bibx19" title="" class="ltx_ref">19</a>, <a href="#bib.bibx33" title="" class="ltx_ref">33</a>, <a href="#bib.bibx34" title="" class="ltx_ref">34</a>]</cite>. Although generative adversarial networks (GANs) have been able to produce photorealistic clinical dermatology images<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">35</a>, <a href="#bib.bibx26" title="" class="ltx_ref">26</a>, <a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite>, they have not been shown to meaningfully improve performance<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">35</a>]</cite>. Possible explanations for improved performance of diffusion models include higher image fidelity, greater image diversity, and an increased level of control in the generation process. Diffusion-based methods, which are able to produce high-fidelity and photorealistic images based on text input<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>, have produced promising results for skin disease<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">36</a>, <a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>. Our study builds off of preliminary findings that diffusion models can generate data to improve classifier performance for rare disease<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">37</a>]</cite> and across underrepresented groups<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite>, with experiments that isolate and better understand the impact of synthetic images from diffusion models for natural images in the medical domain.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">This study has limitations. First, our analysis was limited to nine skin conditions with several hundred images each. We therefore could not assess whether effects persist at higher sample sizes. Second, we did not assess all factors that may affect synthetic augmentation, e.g. model architecture. Third, the Fitzpatrick Skin Type is a measure of photosensitivity rather than skin tone, though it has been co-opted for assessing skin tone in machine learning applications<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">38</a>]</cite>. Nevertheless, this scale is not granular enough to capture the full spectrum of human skin tone diversity. Fourth, in the Fitzpatrick 17k dataset, diagnostic labels were not confirmed by biopsy and FST labels were derived retrospectively. Fifth, we cannot confirm that all test data in Fitzpatrick 17k was excluded from pre-training for Stable Diffusion or similar large-scale models; by contrast, images in DDI were released later in November 2022 and required registration. Future work may compare diffusion models head-to-head with previous approaches (e.g., GANs), directly quantify photorealism (e.g., using a Turing test for human clinicians), and investigate the use of text conditioning on anatomic location or disease subtypes.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">Recent rapid improvements in generative modeling have prompted significant interest in augmenting medical machine learning classifiers with synthetic images. Our data suggest that synthetic augmentation approaches are worth exploring when sufficient reliable test data is available to assess empirical benefits and harms. While collection of diverse real-world data remains the most important and rate-limiting step for improving skin classification models, achieving parity in representation or performance across hundreds of skin conditions remains challenging, especially across disease subtypes, anatomic locations, and other image characteristics<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">39</a>, <a href="#bib.bibx40" title="" class="ltx_ref">40</a>]</cite>. The concomitant use of synthetic data, along with traditional methods of re-weighting and upsampling, may act as a force-multiplier to continually improve classification models for healthcare applications.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data description</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">Skin disease image data are from the Fitzpatrick 17k dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> and the Stanford DDI dataset12<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>. Fitzpatrick 17k consists of 16,577 clinical images from two dermatology atlases, including both skin condition labels and FST labels. Although there are 114 skin conditions in the full Fitzpatrick 17k dataset, our analysis utilized only the nine most common conditions with sample sizes sufficient for the proposed experiments; these were selected in order to have at least 228 real images available for each dataset after allocation of validation and testing data. Potentially duplicated, mislabeled, and outlier samples were removed using the SelfClean algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">41</a>]</cite>. After filtering, selected conditions totaled 3,699 images (F17k-9). The Stanford DDI data consisted of 656 biopsy-confirmed clinical images representing 570 unique patients from pathology encounters at Stanford Clinics. Additionally, DDI contains FST labels for individuals intentionally matched on diagnostic category, age within 10 years, gender, and date of image within 3 years for comparison between FST I-II and FST V-VI.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Synthetic data generation</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Data was synthesized according to three different generation methods: (1) text-to-image, (2) inpainting, and (3) inpainting followed by outpainting. For text-to-image, we generated images by prompting a text-to-image diffusion model with the prompt “An image of <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">&lt;class-name&gt;</span>, a skin disease”, where <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">&lt;class-name&gt;</span> was replaced by the name of the corresponding disease (e.g., psoriasis). For inpainting, we used a text-to-image inpainting model to mask and replace a square region at the center of the image with width and height equal to half that of the original dimensions (Figures 1-2). Our inpainting model is also conditioned on text; we use the same text prompt as described above. For in-then-outpainting, we first performed inpainting as described above, and then masked and replaced the border of the image, which is an exact complement of the inpainting mask (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>B). As before, we used the same text prompt as input.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">All methods utilized Stable Diffusion<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite>, a large-scale latent diffusion model, for generating high-resolution synthetic images. The text-to-image method used Stable Diffusion v2.1 and the inpainting-based methods used Stable-Diffusion-Inpainting v1.5. For each of our three generation methods, we considered two variants of the generative model: one in which the pretrained diffusion model was used directly, and one in which the model was fine-tuned on a small number of training images from Fitzpatrick 17k using DreamBooth<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>. Images used for fine-tuning were never included in held-out test sets.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model training</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">Our experiments utilized the Efficient-Net V2-M image classification model pretrained on ImageNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">42</a>]</cite>. In all experiments, training was performed for 30 epochs; model training was stopped when validation accuracy did not improve in 3 consecutive epochs. The data was split into training and validation sets (Table S-1) and a held-out test set was used for final evaluation. Synthetic data was added to the training set only and was always produced independently of images in held-out test sets. Some experiments involved image transforms including random cropping, horizontal flips, warping, and lighting adjustments from the aug_transforms utility in fast.ai<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">43</a>]</cite>, applied to both real and synthetic images.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Model evaluation</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">We created separate training sets comprising 1, 16, 32, 64, 128, and 228 images per disease condition (totaling 9, 144, 288, 576, 1152, and 2052 real images) in F17k-9. For each training set, we trained skin disease classifiers in four ways: (1) using only the real images without image transforms (2) using only real images with image transforms, (3) using real images, 10 synthetic images per real image, and no image transforms, and (4) using real images, 10 synthetic images per real image, and image augmentation. Validation loss was calculated on a set of 40 images per disease label, totaling 360 images. After training, mean classification accuracy was calculated on a held-out test set of 40 images per disease label, totaling 360 images (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2 Synthetic data augmentation improves performance in low-data settings and is additive with image transforms ‣ 2 Results ‣ Augmenting medical image classifiers with synthetic data from latent diffusion models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). To account for stochasticity in model training, we report mean accuracy over five runs for each set of training criteria.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Dose-response and saturation experiments</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p">We measured the effect of adding increasing amounts of synthetic data to the training set of skin disease classifiers. The training set of each model comprised either 16, 32 or 64 real images per skin condition. Then, to each training set, we added either 10, 25, 50, or 75 synthetic images per real image, generated with the text-to-image method. Models were trained with and without image transforms. We evaluated mean classification accuracy for each model on the same held-out test set comprising 40 real images per skin condition.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Class imbalance experiments</h3>

<div id="S4.SS6.p1" class="ltx_para ltx_noindent">
<p id="S4.SS6.p1.1" class="ltx_p">We also simulated a commonly cited use case for synthetic data: mitigating class imbalance for under-represented labels in a training dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>, <a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>. Using the F17k-9 dataset, we selected 200 examples from each disease condition, and intentionally downsampled one of these to 25, 50, or 100 images during model training. For each imbalanced dataset, we trained a skin disease classifier and evaluated its accuracy for the rare disease class on a held-out test set. We then retrained the models after adding different numbers of synthetic images to the rare disease class. We added 1, 3, or 7 synthetic images per real image of the rare disease class to achieve parity with the non-downsampled conditions. As a control, we also tested an upsampling strategy of simple duplication for the rare disease images. We reported the mean classification accuracy of each model on the rare disease classes separately as well as in aggregate (Figure S-2).</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Skin malignancy classification by skin tone</h3>

<div id="S4.SS7.p1" class="ltx_para ltx_noindent">
<p id="S4.SS7.p1.1" class="ltx_p">We measured the effect of including synthetic data when training a malignant vs. benign skin-disease classifier on FST-balanced DDI data. We removed 32 images per FST category (I/II, III/IV, V/VI) to create a 96-image held-out test set and then fine-tuned a Stable Diffusion model for synthetic images generation using the remaining 560 images. We trained models to classify images as either malignant or benign while varying the following conditions: use of synthetic data (10 synthetic images per real image), use of image transforms, and generation method (inpaint or in-then-outpaint). For each combination, we reported the mean classification accuracy of models across FST categories.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">This project was funded in part by the National Heart Lung and Blood Institute (NHLBI) award K01HL138259 and by the National Library of Medicine (NLM) award 2T15LM007092-31.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Eric Wu et al.
</span>
<span class="ltx_bibblock">“How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Nat. Med.</em> <span id="bib.bibx1.2.2" class="ltx_text ltx_font_bold">27.4</span>, 2021, pp. 582–584
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Ziad Obermeyer, Brian Powers, Christine Vogeli and Sendhil Mullainathan
</span>
<span class="ltx_bibblock">“Dissecting racial bias in an algorithm used to manage the health of populations”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Science</em> <span id="bib.bibx2.2.2" class="ltx_text ltx_font_bold">366.6464</span>, 2019, pp. 447–453
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">P Rajpurkar, E Chen, O Banerjee and E J Topol
</span>
<span class="ltx_bibblock">“AI in health and medicine”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Nat. Med.</em>
</span>
<span class="ltx_bibblock">nature.com, 2022
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Ninareh Mehrabi et al.
</span>
<span class="ltx_bibblock">“A Survey on Bias and Fairness in Machine Learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em> <span id="bib.bibx4.2.2" class="ltx_text ltx_font_bold">54.6</span>
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2021, pp. 1–35
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Aditya Ramesh et al.
</span>
<span class="ltx_bibblock">“Zero-Shot Text-to-Image Generation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning</em> <span id="bib.bibx5.2.2" class="ltx_text ltx_font_bold">139</span>, Proceedings of Machine Learning Research
</span>
<span class="ltx_bibblock">PMLR, 2021, pp. 8821–8831
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Robin Rombach et al.
</span>
<span class="ltx_bibblock">“High-resolution image synthesis with latent diffusion models”, 2021, pp. 10684–10695
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2112.10752" title="" class="ltx_ref ltx_href">2112.10752 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Philippe M Burlina et al.
</span>
<span class="ltx_bibblock">“Assessment of Deep Generative Models for High-Resolution Synthetic Retinal Image Generation of Age-Related Macular Degeneration”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">JAMA Ophthalmol.</em> <span id="bib.bibx7.2.2" class="ltx_text ltx_font_bold">137.3</span>
</span>
<span class="ltx_bibblock">jamanetwork.com, 2019, pp. 258–264
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Andre Esteva et al.
</span>
<span class="ltx_bibblock">“Dermatologist-level classification of skin cancer with deep neural networks”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Nature</em> <span id="bib.bibx8.2.2" class="ltx_text ltx_font_bold">542.7639</span>, 2017, pp. 115–118
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Yuan Liu et al.
</span>
<span class="ltx_bibblock">“A deep learning system for differential diagnosis of skin diseases”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Nat. Med.</em> <span id="bib.bibx9.2.2" class="ltx_text ltx_font_bold">26.6</span>, 2020, pp. 900–908
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Sai Balasubramanian
</span>
<span class="ltx_bibblock">“You Can Now Use Google Lens On Your Phone To Look Up Skin Conditions”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Forbes Magazine</em>, 2023
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Matthew Groh et al.
</span>
<span class="ltx_bibblock">“Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>
</span>
<span class="ltx_bibblock">openaccess.thecvf.com, 2021, pp. 1820–1828
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Roxana Daneshjou et al.
</span>
<span class="ltx_bibblock">“Lack of Transparency and Potential Bias in Artificial Intelligence Data Sets and Algorithms: A Scoping Review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">JAMA Dermatol.</em> <span id="bib.bibx12.2.2" class="ltx_text ltx_font_bold">157.11</span>, 2021, pp. 1362–1369
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Roxana Daneshjou et al.
</span>
<span class="ltx_bibblock">“Disparities in dermatology AI performance on a diverse, curated clinical image set”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Sci Adv</em> <span id="bib.bibx13.2.2" class="ltx_text ltx_font_bold">8.32</span>, 2022, pp. eabq6147
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Newton M Kinyanjui et al.
</span>
<span class="ltx_bibblock">“Fairness of Classifiers Across Skin Tones in Dermatology”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</em>
</span>
<span class="ltx_bibblock">Springer International Publishing, 2020, pp. 320–329
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Ira Ktena et al.
</span>
<span class="ltx_bibblock">“Generative models improve fairness of medical classifiers under distribution shifts”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2304.09218" title="" class="ltx_ref ltx_href">2304.09218 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Jean-Francois Rajotte et al.
</span>
<span class="ltx_bibblock">“Synthetic data as an enabler for machine learning applications in medicine”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">iScience</em> <span id="bib.bibx16.2.2" class="ltx_text ltx_font_bold">25.11</span>, 2022, pp. 105331
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Shekoofeh Azizi et al.
</span>
<span class="ltx_bibblock">“Synthetic Data from Diffusion Models Improves ImageNet Classification”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2304.08466" title="" class="ltx_ref ltx_href">2304.08466 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Chitwan Saharia et al.
</span>
<span class="ltx_bibblock">“Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2205.11487" title="" class="ltx_ref ltx_href">2205.11487 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Jean-Francois Rajotte et al.
</span>
<span class="ltx_bibblock">“Reducing bias and increasing utility by federated generative modeling of medical images using a centralized adversary”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Information Technology for Social Good</em>, GoodIT ’21
</span>
<span class="ltx_bibblock">Roma, Italy: Association for Computing Machinery, 2021, pp. 79–84
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Aditya Ramesh et al.
</span>
<span class="ltx_bibblock">“Hierarchical Text-Conditional Image Generation with CLIP Latents”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2204.06125" title="" class="ltx_ref ltx_href">2204.06125 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Luke W Sagers et al.
</span>
<span class="ltx_bibblock">“Improving dermatology classifiers across populations using images generated by large diffusion models”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2211.13352" title="" class="ltx_ref ltx_href">2211.13352 [eess.IV]</a>
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Emad Mostaque
</span>
<span class="ltx_bibblock">“Stable Diffusion Public Release” Accessed: 2023-8-18
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Stability AI</em>, <a target="_blank" href="https://stability.ai/blog/stable-diffusion-public-release" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://stability.ai/blog/stable-diffusion-public-release</a>, 2022
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Nataniel Ruiz et al.
</span>
<span class="ltx_bibblock">“DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2208.12242" title="" class="ltx_ref ltx_href">2208.12242 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Alec Radford et al.
</span>
<span class="ltx_bibblock">“Learning Transferable Visual Models From Natural Language Supervision”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning</em> <span id="bib.bibx24.2.2" class="ltx_text ltx_font_bold">139</span>, Proceedings of Machine Learning Research
</span>
<span class="ltx_bibblock">PMLR, 2021, pp. 8748–8763
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Leland McInnes, John Healy, Nathaniel Saul and Lukas Großberger
</span>
<span class="ltx_bibblock">“UMAP: Uniform Manifold Approximation and Projection”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">Journal of Open Source Software</em> <span id="bib.bibx25.2.2" class="ltx_text ltx_font_bold">3.29</span>, 2018, pp. 861
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Eman Rezk, Mohamed Eltorki and Wael El-Dakhakhni
</span>
<span class="ltx_bibblock">“Improving Skin Color Diversity in Cancer Detection: Deep Learning Approach”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">JMIR Dermatology</em> <span id="bib.bibx26.2.2" class="ltx_text ltx_font_bold">5.3</span>
</span>
<span class="ltx_bibblock">JMIR Dermatology, 2022, pp. e39143
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Pierre Chambon, Christian Bluethgen, Curtis P Langlotz and Akshay Chaudhari
</span>
<span class="ltx_bibblock">“Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2210.04133" title="" class="ltx_ref ltx_href">2210.04133 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Adrian B Levine et al.
</span>
<span class="ltx_bibblock">“Synthesis of diagnostic quality cancer pathology images by generative adversarial networks”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">J. Pathol.</em> <span id="bib.bibx28.2.2" class="ltx_text ltx_font_bold">252.2</span>, 2020, pp. 178–188
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Richard J Chen et al.
</span>
<span class="ltx_bibblock">“Synthetic data in machine learning for medicine and healthcare”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Nat Biomed Eng</em> <span id="bib.bibx29.2.2" class="ltx_text ltx_font_bold">5.6</span>, 2021, pp. 493–497
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Yuan Xue et al.
</span>
<span class="ltx_bibblock">“Selective synthetic augmentation with HistoGAN for improved histopathology image classification”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">Med. Image Anal.</em> <span id="bib.bibx30.2.2" class="ltx_text ltx_font_bold">67</span>
</span>
<span class="ltx_bibblock">Elsevier, 2021, pp. 101816
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Felipe O Giuste et al.
</span>
<span class="ltx_bibblock">“Explainable synthetic image generation to improve risk assessment of rare pediatric heart transplant rejection”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">J. Biomed. Inform.</em> <span id="bib.bibx31.2.2" class="ltx_text ltx_font_bold">139</span>
</span>
<span class="ltx_bibblock">Elsevier, 2023, pp. 104303
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Sahra Ghalebikesabi et al.
</span>
<span class="ltx_bibblock">“Differentially Private Diffusion Models Generate Useful Synthetic Images”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2302.13861" title="" class="ltx_ref ltx_href">2302.13861 [cs.LG]</a>
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Kexin Ding et al.
</span>
<span class="ltx_bibblock">“A Large-scale Synthetic Pathological Dataset for Deep Learning-enabled Segmentation of Breast Cancer”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Sci Data</em> <span id="bib.bibx33.2.2" class="ltx_text ltx_font_bold">10.1</span>, 2023, pp. 231
</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">August DuMont Schütte et al.
</span>
<span class="ltx_bibblock">“Overcoming barriers to data sharing with medical image generation: a comprehensive evaluation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">npj Digital Medicine</em> <span id="bib.bibx34.2.2" class="ltx_text ltx_font_bold">4.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group, 2021, pp. 1–14
</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Amirata Ghorbani, Vivek Natarajan, David Coz and Yuan Liu
</span>
<span class="ltx_bibblock">“DermGAN: Synthetic Generation of Clinical Skin Images with Pathology”, 2019
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/1911.08716" title="" class="ltx_ref ltx_href">1911.08716 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Mohamed Akrout et al.
</span>
<span class="ltx_bibblock">“Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2301.04802" title="" class="ltx_ref ltx_href">2301.04802 [cs.LG]</a>
</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Kai Packhäuser, Lukas Folle, Florian Thamm and Andreas Maier
</span>
<span class="ltx_bibblock">“Generation of Anonymous Chest Radiographs Using Latent Diffusion Models for Training Thoracic Abnormality Classification Systems”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2211.01323" title="" class="ltx_ref ltx_href">2211.01323 [eess.IV]</a>
</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Matthew Groh et al.
</span>
<span class="ltx_bibblock">“Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx38.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Hum.-Comput. Interact.</em> <span id="bib.bibx38.2.2" class="ltx_text ltx_font_bold">6.CSCW2</span>
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2022, pp. 1–26
</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Marc Combalia et al.
</span>
<span class="ltx_bibblock">“Validation of artificial intelligence prediction models for skin cancer diagnosis using dermoscopy images: the 2019 International Skin Imaging Collaboration Grand Challenge”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">Lancet Digit Health</em> <span id="bib.bibx39.2.2" class="ltx_text ltx_font_bold">4.5</span>, 2022, pp. e330–e339
</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Samuel G Finlayson et al.
</span>
<span class="ltx_bibblock">“The Clinician and Dataset Shift in Artificial Intelligence”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx40.1.1" class="ltx_emph ltx_font_italic">N. Engl. J. Med.</em> <span id="bib.bibx40.2.2" class="ltx_text ltx_font_bold">385.3</span>, 2021, pp. 283–286
</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Fabian Gröger et al.
</span>
<span class="ltx_bibblock">“SelfClean: A Self-Supervised Data Cleaning Strategy”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2305.17048" title="" class="ltx_ref ltx_href">2305.17048 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Mingxing Tan and Quoc V Le
</span>
<span class="ltx_bibblock">“EfficientNetV2: Smaller Models and Faster Training”, 2021
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2104.00298" title="" class="ltx_ref ltx_href">2104.00298 [cs.CV]</a>
</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Jeremy Howard and Sylvain Gugger
</span>
<span class="ltx_bibblock">“fastai: A Layered API for Deep Learning”, 2020
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2002.04688" title="" class="ltx_ref ltx_href">2002.04688 [cs.LG]</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.12452" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.12453" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.12453">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.12453" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.12454" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 11:24:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
