<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2101.06399] Latent Variable Models for Visual Question Answering</title><meta property="og:description" content="Current work on Visual Question Answering (VQA) explore deterministic approaches conditioned on various types of image and question features.
We posit that, in addition to image and question pairs, other modalities areâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Latent Variable Models for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Latent Variable Models for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2101.06399">

<!--Generated on Fri Mar  1 14:41:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Latent Variable Models for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zixu Wang<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Yishu Miao<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Lucia Specia<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup> 
<br class="ltx_break"><sup id="id9.9.id4" class="ltx_sup">1</sup>Department of Computing, Imperial College London, UK
<br class="ltx_break"><sup id="id10.10.id5" class="ltx_sup">2</sup>Department of Computer Science, University of Sheffield, UK
<br class="ltx_break"><span id="id11.11.id6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{zixu.wang, y.miao20, l.specia}@imperial.ac.uk</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Current work on Visual Question Answering (VQA) explore deterministic approaches conditioned on various types of image and question features.
We posit that, in addition to image and question pairs, other modalities are useful for teaching machine to carry out question answering.
Hence in this paper, we propose latent variable models for VQA where extra information (<em id="id12.id1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id12.id1.2" class="ltx_text"></span> captions and answer categories) are incorporated as latent variables, which are observed during training but in turn benefit question-answering performance at test time.
Experiments on the VQA v2.0 benchmarking dataset demonstrate the effectiveness of our proposed models: they improve over strong baselines, especially those that do not rely on extensive language-vision pre-training.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As a classic multi-modal machine learning problem, Visual Question Answering (VQA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> systems are tasked with providing a correct textual answer given an image and a textual question.
Current VQA modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> are trained to learn the relationship between areas in an image and the question, and to choose the correct answer from a vocabulary of answer candidates, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p1.1.2" class="ltx_text"></span>, they are modelled as a classification problem.
The majorities of popular VQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> models are created in a deterministic manner and explore solely information from the given image-question pair.
There are other approaches attempting to incorporate extra information, such as image captionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and mutated inputsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
However, it in turn restricts the practical applications as the extra information is required to be explicitly available during testing.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we propose an approach to explore additional information as latent variables in VQA: we employ latent variables for VQA to exploit extra information (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span> image captions and answer categories) to complement limited textual information from image and question pairs.
We assume a realistic setting where this information â€“ esp. captions â€“ may only be available during the training phase.
To that end, we introduce a continuous latent variable as the caption representation to capture the essential information from this modality.
Moreover, the answer category is modelled as a discrete latent variable, which acts as an inductive bias to benefit the learning of answer prediction, and can be integrated out during testing.
The motivation is that the generative framework is able to incorporate many other types of information as continuous or discrete latent variables, and as such it effectively leverages additional resources to constrain the original image-question distribution while omitting them in testing.
This grants the models with stronger generalisation ability compared to its deterministic counterparts, which generally require off-the-shelf pipelines to model the information from external modalities.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Intuitively, image captions describe diverse aspects of an image and include attributes and relations of objects in a more informative way.
In our work, a continuous latent variable is employed for capturing the caption distributions and constraining the generative distribution conditioned on image and question pairs.
In this way, the joint multimodal representations from images and question can benefit from the caption modality during training, and it requires no explicit caption inputs in testing.
Similarly, there exists a strong connection between a question and answer pair when the question provides informative signals on its type or the category of possible answers.
For example, â€œHow manyâ€, â€œWhere isâ€ and â€œwhat isâ€ normally connect to numbers, locations, and objects respectively.
We propose a discrete latent variable is employed for modelling answer categories and providing better inductive bias from the question and answer pairs.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In summary, our <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">main contributions</span> are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A novel generative VQA framework combining the modularity of latent variables with the flexibility to introduce extra information as continuous and/or discrete latent variables.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A method to incorporate additional information which does not rely on building multiple deterministic pipelines, aiming at learning the underlying compositional, relational, and hierarchical structures of multiple modalities. The models benefit from the extra information during training without providing explicit inputs in testing.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The improvements over deterministic baseline models (<em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.I1.i3.p1.1.2" class="ltx_text"></span> UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>) in experiments with the VQA v2.0 dataset demonstrate the effectiveness of our proposed latent variable models.
Our qualitative analysis also indicates that using extra resources (<em id="S1.I1.i3.p1.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.I1.i3.p1.1.4" class="ltx_text"></span> captions and answer categories) as latent variables captures complementary information during training and benefits the VQA performance in testing.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2101.06399/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="264" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of our latent variable model for VQA. We use dotted lines to denote the process of proposed latent variables.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Model</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We first present an overview of our general model structure, followed by the encoders for different modalities, and the proposed corresponding latent variables.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>General Model Structure</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In a VQA task, images and questions are normally used to learn a joint multimodal distribution for answer predictions.
We postulate that the joint representation can be improved by other multimodal information.
Hence, we introduce captions and answer categories to our VQA model as continuous and discrete latent variables respectively to encourage a better learning in the joint distribution of image and question pairs during training.
A notable advantage of the latent variable models is that they do not explicitly require captions or answer categories during testing, and therefore can be easily extended to condition on any other useful information.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.9" class="ltx_p">Firstly we introduce the notations used in the general VQA model. <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">V</annotation></semantics></math>, <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">Q</annotation></semantics></math>, <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">A</annotation></semantics></math> are used to denote the input image, question, and answer instances respectively.
The image feature <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">v</annotation></semantics></math>, question representation <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">q</annotation></semantics></math>, and answer representation <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">a</annotation></semantics></math> are extracted from the image encoder, question encoder, and answer encoder.
The VQA task is constructed as a classification problem to output the most likely answer <math id="S2.SS1.p2.7.m7.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S2.SS1.p2.7.m7.1a"><mover accent="true" id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><mi id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml">a</mi><mo id="S2.SS1.p2.7.m7.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><ci id="S2.SS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1.1">^</ci><ci id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">\hat{a}</annotation></semantics></math> from a fixed set of answers based on the content of the image <math id="S2.SS1.p2.8.m8.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS1.p2.8.m8.1a"><mi id="S2.SS1.p2.8.m8.1.1" xref="S2.SS1.p2.8.m8.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.1b"><ci id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.1c">v</annotation></semantics></math> and question <math id="S2.SS1.p2.9.m9.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.p2.9.m9.1a"><mi id="S2.SS1.p2.9.m9.1.1" xref="S2.SS1.p2.9.m9.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m9.1b"><ci id="S2.SS1.p2.9.m9.1.1.cmml" xref="S2.SS1.p2.9.m9.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m9.1c">q</annotation></semantics></math>:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\hat{a}=\textrm{argmax}\ p(a|v,q)" display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mover accent="true" id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mi id="S2.E1.m1.3.3.3.2" xref="S2.E1.m1.3.3.3.2.cmml">a</mi><mo id="S2.E1.m1.3.3.3.1" xref="S2.E1.m1.3.3.3.1.cmml">^</mo></mover><mo id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml"><mtext id="S2.E1.m1.3.3.1.3" xref="S2.E1.m1.3.3.1.3a.cmml">argmax</mtext><mo lspace="0.500em" rspace="0em" id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.3.3.1.4" xref="S2.E1.m1.3.3.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.2a" xref="S2.E1.m1.3.3.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.3.1.cmml"><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">v</mi><mo id="S2.E1.m1.3.3.1.1.1.1.3.2.1" xref="S2.E1.m1.3.3.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"></eq><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><ci id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.1">^</ci><ci id="S2.E1.m1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.2">ğ‘</ci></apply><apply id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1"><times id="S2.E1.m1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1.2"></times><ci id="S2.E1.m1.3.3.1.3a.cmml" xref="S2.E1.m1.3.3.1.3"><mtext id="S2.E1.m1.3.3.1.3.cmml" xref="S2.E1.m1.3.3.1.3">argmax</mtext></ci><ci id="S2.E1.m1.3.3.1.4.cmml" xref="S2.E1.m1.3.3.1.4">ğ‘</ci><apply id="S2.E1.m1.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2">ğ‘</ci><list id="S2.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘£</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\hat{a}=\textrm{argmax}\ p(a|v,q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.8" class="ltx_p">In our latent variable model, we introduce image captions <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">C</annotation></semantics></math> to the training phase.
Similarly, we extract the caption features <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">c</annotation></semantics></math> by a caption encoder.
However, instead of directly feeding in the caption features <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">c</annotation></semantics></math> into to the model, we employ a continuous latent distribution <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">z</annotation></semantics></math> to be the caption representations.
Here <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="z\sim q(z|c)" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mrow id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">z</mi><mo id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">âˆ¼</mo><mrow id="S2.SS1.p3.5.m5.1.1.1" xref="S2.SS1.p3.5.m5.1.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.1.3" xref="S2.SS1.p3.5.m5.1.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.5.m5.1.1.1.2" xref="S2.SS1.p3.5.m5.1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.5.m5.1.1.1.1.1" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.5.m5.1.1.1.1.1.2" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.5.m5.1.1.1.1.1.1" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.1.1.1.1.2" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p3.5.m5.1.1.1.1.1.1.1" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.p3.5.m5.1.1.1.1.1.1.3" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS1.p3.5.m5.1.1.1.1.1.3" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="latexml" id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">similar-to</csymbol><ci id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">ğ‘§</ci><apply id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1.1"><times id="S2.SS1.p3.5.m5.1.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.1.2"></times><ci id="S2.SS1.p3.5.m5.1.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.1.3">ğ‘</ci><apply id="S2.SS1.p3.5.m5.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p3.5.m5.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p3.5.m5.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.2">ğ‘§</ci><ci id="S2.SS1.p3.5.m5.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.1.1.1.1.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">z\sim q(z|c)</annotation></semantics></math> is modelled as variational distribution.
We then build a generative distribution <math id="S2.SS1.p3.6.m6.3" class="ltx_Math" alttext="z\sim p(z|v,q)" display="inline"><semantics id="S2.SS1.p3.6.m6.3a"><mrow id="S2.SS1.p3.6.m6.3.3" xref="S2.SS1.p3.6.m6.3.3.cmml"><mi id="S2.SS1.p3.6.m6.3.3.3" xref="S2.SS1.p3.6.m6.3.3.3.cmml">z</mi><mo id="S2.SS1.p3.6.m6.3.3.2" xref="S2.SS1.p3.6.m6.3.3.2.cmml">âˆ¼</mo><mrow id="S2.SS1.p3.6.m6.3.3.1" xref="S2.SS1.p3.6.m6.3.3.1.cmml"><mi id="S2.SS1.p3.6.m6.3.3.1.3" xref="S2.SS1.p3.6.m6.3.3.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.6.m6.3.3.1.2" xref="S2.SS1.p3.6.m6.3.3.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.6.m6.3.3.1.1.1" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.6.m6.3.3.1.1.1.2" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.6.m6.3.3.1.1.1.1" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.cmml"><mi id="S2.SS1.p3.6.m6.3.3.1.1.1.1.2" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p3.6.m6.3.3.1.1.1.1.1" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.SS1.p3.6.m6.3.3.1.1.1.1.3.2" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.3.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml">v</mi><mo id="S2.SS1.p3.6.m6.3.3.1.1.1.1.3.2.1" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.p3.6.m6.2.2" xref="S2.SS1.p3.6.m6.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p3.6.m6.3.3.1.1.1.3" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.3b"><apply id="S2.SS1.p3.6.m6.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3"><csymbol cd="latexml" id="S2.SS1.p3.6.m6.3.3.2.cmml" xref="S2.SS1.p3.6.m6.3.3.2">similar-to</csymbol><ci id="S2.SS1.p3.6.m6.3.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.3">ğ‘§</ci><apply id="S2.SS1.p3.6.m6.3.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.1"><times id="S2.SS1.p3.6.m6.3.3.1.2.cmml" xref="S2.SS1.p3.6.m6.3.3.1.2"></times><ci id="S2.SS1.p3.6.m6.3.3.1.3.cmml" xref="S2.SS1.p3.6.m6.3.3.1.3">ğ‘</ci><apply id="S2.SS1.p3.6.m6.3.3.1.1.1.1.cmml" xref="S2.SS1.p3.6.m6.3.3.1.1.1"><csymbol cd="latexml" id="S2.SS1.p3.6.m6.3.3.1.1.1.1.1.cmml" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p3.6.m6.3.3.1.1.1.1.2.cmml" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.2">ğ‘§</ci><list id="S2.SS1.p3.6.m6.3.3.1.1.1.1.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.1.1.1.1.3.2"><ci id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">ğ‘£</ci><ci id="S2.SS1.p3.6.m6.2.2.cmml" xref="S2.SS1.p3.6.m6.2.2">ğ‘</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.3c">z\sim p(z|v,q)</annotation></semantics></math> to infer the caption information by conditioning on image and question pairs, which is optimised during training via neural variational inference.
We originally experimented using <math id="S2.SS1.p3.7.m7.4" class="ltx_Math" alttext="q(z|v,q,c)" display="inline"><semantics id="S2.SS1.p3.7.m7.4a"><mrow id="S2.SS1.p3.7.m7.4.4" xref="S2.SS1.p3.7.m7.4.4.cmml"><mi id="S2.SS1.p3.7.m7.4.4.3" xref="S2.SS1.p3.7.m7.4.4.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.7.m7.4.4.2" xref="S2.SS1.p3.7.m7.4.4.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.7.m7.4.4.1.1" xref="S2.SS1.p3.7.m7.4.4.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.7.m7.4.4.1.1.2" xref="S2.SS1.p3.7.m7.4.4.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.7.m7.4.4.1.1.1" xref="S2.SS1.p3.7.m7.4.4.1.1.1.cmml"><mi id="S2.SS1.p3.7.m7.4.4.1.1.1.2" xref="S2.SS1.p3.7.m7.4.4.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p3.7.m7.4.4.1.1.1.1" xref="S2.SS1.p3.7.m7.4.4.1.1.1.1.cmml">|</mo><mrow id="S2.SS1.p3.7.m7.4.4.1.1.1.3.2" xref="S2.SS1.p3.7.m7.4.4.1.1.1.3.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml">v</mi><mo id="S2.SS1.p3.7.m7.4.4.1.1.1.3.2.1" xref="S2.SS1.p3.7.m7.4.4.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.p3.7.m7.2.2" xref="S2.SS1.p3.7.m7.2.2.cmml">q</mi><mo id="S2.SS1.p3.7.m7.4.4.1.1.1.3.2.2" xref="S2.SS1.p3.7.m7.4.4.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.p3.7.m7.3.3" xref="S2.SS1.p3.7.m7.3.3.cmml">c</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p3.7.m7.4.4.1.1.3" xref="S2.SS1.p3.7.m7.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.4b"><apply id="S2.SS1.p3.7.m7.4.4.cmml" xref="S2.SS1.p3.7.m7.4.4"><times id="S2.SS1.p3.7.m7.4.4.2.cmml" xref="S2.SS1.p3.7.m7.4.4.2"></times><ci id="S2.SS1.p3.7.m7.4.4.3.cmml" xref="S2.SS1.p3.7.m7.4.4.3">ğ‘</ci><apply id="S2.SS1.p3.7.m7.4.4.1.1.1.cmml" xref="S2.SS1.p3.7.m7.4.4.1.1"><csymbol cd="latexml" id="S2.SS1.p3.7.m7.4.4.1.1.1.1.cmml" xref="S2.SS1.p3.7.m7.4.4.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p3.7.m7.4.4.1.1.1.2.cmml" xref="S2.SS1.p3.7.m7.4.4.1.1.1.2">ğ‘§</ci><list id="S2.SS1.p3.7.m7.4.4.1.1.1.3.1.cmml" xref="S2.SS1.p3.7.m7.4.4.1.1.1.3.2"><ci id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1">ğ‘£</ci><ci id="S2.SS1.p3.7.m7.2.2.cmml" xref="S2.SS1.p3.7.m7.2.2">ğ‘</ci><ci id="S2.SS1.p3.7.m7.3.3.cmml" xref="S2.SS1.p3.7.m7.3.3">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.4c">q(z|v,q,c)</annotation></semantics></math> as the vairational distribution.
However, this distribution is quite close (<em id="S2.SS1.p3.8.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS1.p3.8.2" class="ltx_text"></span> small KL divergence) to the generative distribution <math id="S2.SS1.p3.8.m8.3" class="ltx_Math" alttext="p(z|v,q)" display="inline"><semantics id="S2.SS1.p3.8.m8.3a"><mrow id="S2.SS1.p3.8.m8.3.3" xref="S2.SS1.p3.8.m8.3.3.cmml"><mi id="S2.SS1.p3.8.m8.3.3.3" xref="S2.SS1.p3.8.m8.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.3.3.2" xref="S2.SS1.p3.8.m8.3.3.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.8.m8.3.3.1.1" xref="S2.SS1.p3.8.m8.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.8.m8.3.3.1.1.2" xref="S2.SS1.p3.8.m8.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.8.m8.3.3.1.1.1" xref="S2.SS1.p3.8.m8.3.3.1.1.1.cmml"><mi id="S2.SS1.p3.8.m8.3.3.1.1.1.2" xref="S2.SS1.p3.8.m8.3.3.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p3.8.m8.3.3.1.1.1.1" xref="S2.SS1.p3.8.m8.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS1.p3.8.m8.3.3.1.1.1.3.2" xref="S2.SS1.p3.8.m8.3.3.1.1.1.3.1.cmml"><mi id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml">v</mi><mo id="S2.SS1.p3.8.m8.3.3.1.1.1.3.2.1" xref="S2.SS1.p3.8.m8.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.p3.8.m8.2.2" xref="S2.SS1.p3.8.m8.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p3.8.m8.3.3.1.1.3" xref="S2.SS1.p3.8.m8.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.3b"><apply id="S2.SS1.p3.8.m8.3.3.cmml" xref="S2.SS1.p3.8.m8.3.3"><times id="S2.SS1.p3.8.m8.3.3.2.cmml" xref="S2.SS1.p3.8.m8.3.3.2"></times><ci id="S2.SS1.p3.8.m8.3.3.3.cmml" xref="S2.SS1.p3.8.m8.3.3.3">ğ‘</ci><apply id="S2.SS1.p3.8.m8.3.3.1.1.1.cmml" xref="S2.SS1.p3.8.m8.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.p3.8.m8.3.3.1.1.1.1.cmml" xref="S2.SS1.p3.8.m8.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p3.8.m8.3.3.1.1.1.2.cmml" xref="S2.SS1.p3.8.m8.3.3.1.1.1.2">ğ‘§</ci><list id="S2.SS1.p3.8.m8.3.3.1.1.1.3.1.cmml" xref="S2.SS1.p3.8.m8.3.3.1.1.1.3.2"><ci id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1">ğ‘£</ci><ci id="S2.SS1.p3.8.m8.2.2.cmml" xref="S2.SS1.p3.8.m8.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.3c">p(z|v,q)</annotation></semantics></math>, which weakens the learning signal from KL divergence.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.2" class="ltx_p">In addition, we introduce a discrete latent variable <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">d</annotation></semantics></math> for modelling answer category inferred via <math id="S2.SS1.p4.2.m2.3" class="ltx_Math" alttext="d\sim p(d|v,q)" display="inline"><semantics id="S2.SS1.p4.2.m2.3a"><mrow id="S2.SS1.p4.2.m2.3.3" xref="S2.SS1.p4.2.m2.3.3.cmml"><mi id="S2.SS1.p4.2.m2.3.3.3" xref="S2.SS1.p4.2.m2.3.3.3.cmml">d</mi><mo id="S2.SS1.p4.2.m2.3.3.2" xref="S2.SS1.p4.2.m2.3.3.2.cmml">âˆ¼</mo><mrow id="S2.SS1.p4.2.m2.3.3.1" xref="S2.SS1.p4.2.m2.3.3.1.cmml"><mi id="S2.SS1.p4.2.m2.3.3.1.3" xref="S2.SS1.p4.2.m2.3.3.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.2.m2.3.3.1.2" xref="S2.SS1.p4.2.m2.3.3.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p4.2.m2.3.3.1.1.1" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p4.2.m2.3.3.1.1.1.2" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p4.2.m2.3.3.1.1.1.1" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.cmml"><mi id="S2.SS1.p4.2.m2.3.3.1.1.1.1.2" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.2.cmml">d</mi><mo fence="false" id="S2.SS1.p4.2.m2.3.3.1.1.1.1.1" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.SS1.p4.2.m2.3.3.1.1.1.1.3.2" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.3.1.cmml"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">v</mi><mo id="S2.SS1.p4.2.m2.3.3.1.1.1.1.3.2.1" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.p4.2.m2.2.2" xref="S2.SS1.p4.2.m2.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p4.2.m2.3.3.1.1.1.3" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.3b"><apply id="S2.SS1.p4.2.m2.3.3.cmml" xref="S2.SS1.p4.2.m2.3.3"><csymbol cd="latexml" id="S2.SS1.p4.2.m2.3.3.2.cmml" xref="S2.SS1.p4.2.m2.3.3.2">similar-to</csymbol><ci id="S2.SS1.p4.2.m2.3.3.3.cmml" xref="S2.SS1.p4.2.m2.3.3.3">ğ‘‘</ci><apply id="S2.SS1.p4.2.m2.3.3.1.cmml" xref="S2.SS1.p4.2.m2.3.3.1"><times id="S2.SS1.p4.2.m2.3.3.1.2.cmml" xref="S2.SS1.p4.2.m2.3.3.1.2"></times><ci id="S2.SS1.p4.2.m2.3.3.1.3.cmml" xref="S2.SS1.p4.2.m2.3.3.1.3">ğ‘</ci><apply id="S2.SS1.p4.2.m2.3.3.1.1.1.1.cmml" xref="S2.SS1.p4.2.m2.3.3.1.1.1"><csymbol cd="latexml" id="S2.SS1.p4.2.m2.3.3.1.1.1.1.1.cmml" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p4.2.m2.3.3.1.1.1.1.2.cmml" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.2">ğ‘‘</ci><list id="S2.SS1.p4.2.m2.3.3.1.1.1.1.3.1.cmml" xref="S2.SS1.p4.2.m2.3.3.1.1.1.1.3.2"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">ğ‘£</ci><ci id="S2.SS1.p4.2.m2.2.2.cmml" xref="S2.SS1.p4.2.m2.2.2">ğ‘</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.3c">d\sim p(d|v,q)</annotation></semantics></math>, which is also conditioned on image and question pairs.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.3" class="ltx_p">Hence, the training of the latent variable model is carried out by the samples <math id="S2.SS1.p5.1.m1.5" class="ltx_Math" alttext="(v,q,a,c,d)" display="inline"><semantics id="S2.SS1.p5.1.m1.5a"><mrow id="S2.SS1.p5.1.m1.5.6.2" xref="S2.SS1.p5.1.m1.5.6.1.cmml"><mo stretchy="false" id="S2.SS1.p5.1.m1.5.6.2.1" xref="S2.SS1.p5.1.m1.5.6.1.cmml">(</mo><mi id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml">v</mi><mo id="S2.SS1.p5.1.m1.5.6.2.2" xref="S2.SS1.p5.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p5.1.m1.2.2" xref="S2.SS1.p5.1.m1.2.2.cmml">q</mi><mo id="S2.SS1.p5.1.m1.5.6.2.3" xref="S2.SS1.p5.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p5.1.m1.3.3" xref="S2.SS1.p5.1.m1.3.3.cmml">a</mi><mo id="S2.SS1.p5.1.m1.5.6.2.4" xref="S2.SS1.p5.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p5.1.m1.4.4" xref="S2.SS1.p5.1.m1.4.4.cmml">c</mi><mo id="S2.SS1.p5.1.m1.5.6.2.5" xref="S2.SS1.p5.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p5.1.m1.5.5" xref="S2.SS1.p5.1.m1.5.5.cmml">d</mi><mo stretchy="false" id="S2.SS1.p5.1.m1.5.6.2.6" xref="S2.SS1.p5.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.5b"><vector id="S2.SS1.p5.1.m1.5.6.1.cmml" xref="S2.SS1.p5.1.m1.5.6.2"><ci id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">ğ‘£</ci><ci id="S2.SS1.p5.1.m1.2.2.cmml" xref="S2.SS1.p5.1.m1.2.2">ğ‘</ci><ci id="S2.SS1.p5.1.m1.3.3.cmml" xref="S2.SS1.p5.1.m1.3.3">ğ‘</ci><ci id="S2.SS1.p5.1.m1.4.4.cmml" xref="S2.SS1.p5.1.m1.4.4">ğ‘</ci><ci id="S2.SS1.p5.1.m1.5.5.cmml" xref="S2.SS1.p5.1.m1.5.5">ğ‘‘</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.5c">(v,q,a,c,d)</annotation></semantics></math>. During testing, the answer <math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><mi id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><ci id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">a</annotation></semantics></math> is predicted from the image and question pair <math id="S2.SS1.p5.3.m3.2" class="ltx_Math" alttext="(v,q)" display="inline"><semantics id="S2.SS1.p5.3.m3.2a"><mrow id="S2.SS1.p5.3.m3.2.3.2" xref="S2.SS1.p5.3.m3.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.p5.3.m3.2.3.2.1" xref="S2.SS1.p5.3.m3.2.3.1.cmml">(</mo><mi id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml">v</mi><mo id="S2.SS1.p5.3.m3.2.3.2.2" xref="S2.SS1.p5.3.m3.2.3.1.cmml">,</mo><mi id="S2.SS1.p5.3.m3.2.2" xref="S2.SS1.p5.3.m3.2.2.cmml">q</mi><mo stretchy="false" id="S2.SS1.p5.3.m3.2.3.2.3" xref="S2.SS1.p5.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.2b"><interval closure="open" id="S2.SS1.p5.3.m3.2.3.1.cmml" xref="S2.SS1.p5.3.m3.2.3.2"><ci id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1">ğ‘£</ci><ci id="S2.SS1.p5.3.m3.2.2.cmml" xref="S2.SS1.p5.3.m3.2.2">ğ‘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.2c">(v,q)</annotation></semantics></math>:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.13" class="ltx_Math" alttext="\hat{a}\!=\!\textrm{argmax}\!\sum_{d,z}p(a|v,q,d,z)p(d|v,q)p(z|v,q)" display="block"><semantics id="S2.E2.m1.13a"><mrow id="S2.E2.m1.13.13" xref="S2.E2.m1.13.13.cmml"><mover accent="true" id="S2.E2.m1.13.13.5" xref="S2.E2.m1.13.13.5.cmml"><mi id="S2.E2.m1.13.13.5.2" xref="S2.E2.m1.13.13.5.2.cmml">a</mi><mo id="S2.E2.m1.13.13.5.1" xref="S2.E2.m1.13.13.5.1.cmml">^</mo></mover><mo lspace="0.108em" rspace="0.108em" id="S2.E2.m1.13.13.4" xref="S2.E2.m1.13.13.4.cmml">=</mo><mrow id="S2.E2.m1.13.13.3" xref="S2.E2.m1.13.13.3.cmml"><mpadded width="3.496em"><mtext id="S2.E2.m1.13.13.3.5" xref="S2.E2.m1.13.13.3.5a.cmml">argmax</mtext></mpadded><mo lspace="0em" rspace="0em" id="S2.E2.m1.13.13.3.4" xref="S2.E2.m1.13.13.3.4.cmml">â€‹</mo><mrow id="S2.E2.m1.13.13.3.3" xref="S2.E2.m1.13.13.3.3.cmml"><munder id="S2.E2.m1.13.13.3.3.4" xref="S2.E2.m1.13.13.3.3.4.cmml"><mo movablelimits="false" id="S2.E2.m1.13.13.3.3.4.2" xref="S2.E2.m1.13.13.3.3.4.2.cmml">âˆ‘</mo><mrow id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">d</mi><mo id="S2.E2.m1.2.2.2.4.1" xref="S2.E2.m1.2.2.2.3.cmml">,</mo><mi id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml">z</mi></mrow></munder><mrow id="S2.E2.m1.13.13.3.3.3" xref="S2.E2.m1.13.13.3.3.3.cmml"><mi id="S2.E2.m1.13.13.3.3.3.5" xref="S2.E2.m1.13.13.3.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.13.13.3.3.3.4" xref="S2.E2.m1.13.13.3.3.3.4.cmml">â€‹</mo><mrow id="S2.E2.m1.11.11.1.1.1.1.1" xref="S2.E2.m1.11.11.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.11.11.1.1.1.1.1.2" xref="S2.E2.m1.11.11.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.11.11.1.1.1.1.1.1" xref="S2.E2.m1.11.11.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.11.11.1.1.1.1.1.1.2" xref="S2.E2.m1.11.11.1.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.E2.m1.11.11.1.1.1.1.1.1.1" xref="S2.E2.m1.11.11.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S2.E2.m1.11.11.1.1.1.1.1.1.3.2" xref="S2.E2.m1.11.11.1.1.1.1.1.1.3.1.cmml"><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">v</mi><mo id="S2.E2.m1.11.11.1.1.1.1.1.1.3.2.1" xref="S2.E2.m1.11.11.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">q</mi><mo id="S2.E2.m1.11.11.1.1.1.1.1.1.3.2.2" xref="S2.E2.m1.11.11.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">d</mi><mo id="S2.E2.m1.11.11.1.1.1.1.1.1.3.2.3" xref="S2.E2.m1.11.11.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml">z</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.11.11.1.1.1.1.1.3" xref="S2.E2.m1.11.11.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.13.13.3.3.3.4a" xref="S2.E2.m1.13.13.3.3.3.4.cmml">â€‹</mo><mi id="S2.E2.m1.13.13.3.3.3.6" xref="S2.E2.m1.13.13.3.3.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.13.13.3.3.3.4b" xref="S2.E2.m1.13.13.3.3.3.4.cmml">â€‹</mo><mrow id="S2.E2.m1.12.12.2.2.2.2.1" xref="S2.E2.m1.12.12.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.12.12.2.2.2.2.1.2" xref="S2.E2.m1.12.12.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E2.m1.12.12.2.2.2.2.1.1" xref="S2.E2.m1.12.12.2.2.2.2.1.1.cmml"><mi id="S2.E2.m1.12.12.2.2.2.2.1.1.2" xref="S2.E2.m1.12.12.2.2.2.2.1.1.2.cmml">d</mi><mo fence="false" id="S2.E2.m1.12.12.2.2.2.2.1.1.1" xref="S2.E2.m1.12.12.2.2.2.2.1.1.1.cmml">|</mo><mrow id="S2.E2.m1.12.12.2.2.2.2.1.1.3.2" xref="S2.E2.m1.12.12.2.2.2.2.1.1.3.1.cmml"><mi id="S2.E2.m1.7.7" xref="S2.E2.m1.7.7.cmml">v</mi><mo id="S2.E2.m1.12.12.2.2.2.2.1.1.3.2.1" xref="S2.E2.m1.12.12.2.2.2.2.1.1.3.1.cmml">,</mo><mi id="S2.E2.m1.8.8" xref="S2.E2.m1.8.8.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.12.12.2.2.2.2.1.3" xref="S2.E2.m1.12.12.2.2.2.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.13.13.3.3.3.4c" xref="S2.E2.m1.13.13.3.3.3.4.cmml">â€‹</mo><mi id="S2.E2.m1.13.13.3.3.3.7" xref="S2.E2.m1.13.13.3.3.3.7.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.13.13.3.3.3.4d" xref="S2.E2.m1.13.13.3.3.3.4.cmml">â€‹</mo><mrow id="S2.E2.m1.13.13.3.3.3.3.1" xref="S2.E2.m1.13.13.3.3.3.3.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.13.13.3.3.3.3.1.2" xref="S2.E2.m1.13.13.3.3.3.3.1.1.cmml">(</mo><mrow id="S2.E2.m1.13.13.3.3.3.3.1.1" xref="S2.E2.m1.13.13.3.3.3.3.1.1.cmml"><mi id="S2.E2.m1.13.13.3.3.3.3.1.1.2" xref="S2.E2.m1.13.13.3.3.3.3.1.1.2.cmml">z</mi><mo fence="false" id="S2.E2.m1.13.13.3.3.3.3.1.1.1" xref="S2.E2.m1.13.13.3.3.3.3.1.1.1.cmml">|</mo><mrow id="S2.E2.m1.13.13.3.3.3.3.1.1.3.2" xref="S2.E2.m1.13.13.3.3.3.3.1.1.3.1.cmml"><mi id="S2.E2.m1.9.9" xref="S2.E2.m1.9.9.cmml">v</mi><mo id="S2.E2.m1.13.13.3.3.3.3.1.1.3.2.1" xref="S2.E2.m1.13.13.3.3.3.3.1.1.3.1.cmml">,</mo><mi id="S2.E2.m1.10.10" xref="S2.E2.m1.10.10.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.13.13.3.3.3.3.1.3" xref="S2.E2.m1.13.13.3.3.3.3.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.13b"><apply id="S2.E2.m1.13.13.cmml" xref="S2.E2.m1.13.13"><eq id="S2.E2.m1.13.13.4.cmml" xref="S2.E2.m1.13.13.4"></eq><apply id="S2.E2.m1.13.13.5.cmml" xref="S2.E2.m1.13.13.5"><ci id="S2.E2.m1.13.13.5.1.cmml" xref="S2.E2.m1.13.13.5.1">^</ci><ci id="S2.E2.m1.13.13.5.2.cmml" xref="S2.E2.m1.13.13.5.2">ğ‘</ci></apply><apply id="S2.E2.m1.13.13.3.cmml" xref="S2.E2.m1.13.13.3"><times id="S2.E2.m1.13.13.3.4.cmml" xref="S2.E2.m1.13.13.3.4"></times><ci id="S2.E2.m1.13.13.3.5a.cmml" xref="S2.E2.m1.13.13.3.5"><mtext id="S2.E2.m1.13.13.3.5.cmml" xref="S2.E2.m1.13.13.3.5">argmax</mtext></ci><apply id="S2.E2.m1.13.13.3.3.cmml" xref="S2.E2.m1.13.13.3.3"><apply id="S2.E2.m1.13.13.3.3.4.cmml" xref="S2.E2.m1.13.13.3.3.4"><csymbol cd="ambiguous" id="S2.E2.m1.13.13.3.3.4.1.cmml" xref="S2.E2.m1.13.13.3.3.4">subscript</csymbol><sum id="S2.E2.m1.13.13.3.3.4.2.cmml" xref="S2.E2.m1.13.13.3.3.4.2"></sum><list id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.4"><ci id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1">ğ‘‘</ci><ci id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2">ğ‘§</ci></list></apply><apply id="S2.E2.m1.13.13.3.3.3.cmml" xref="S2.E2.m1.13.13.3.3.3"><times id="S2.E2.m1.13.13.3.3.3.4.cmml" xref="S2.E2.m1.13.13.3.3.3.4"></times><ci id="S2.E2.m1.13.13.3.3.3.5.cmml" xref="S2.E2.m1.13.13.3.3.3.5">ğ‘</ci><apply id="S2.E2.m1.11.11.1.1.1.1.1.1.cmml" xref="S2.E2.m1.11.11.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.11.11.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.11.11.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.E2.m1.11.11.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.11.11.1.1.1.1.1.1.2">ğ‘</ci><list id="S2.E2.m1.11.11.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.11.11.1.1.1.1.1.1.3.2"><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ğ‘£</ci><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">ğ‘</ci><ci id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5">ğ‘‘</ci><ci id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6">ğ‘§</ci></list></apply><ci id="S2.E2.m1.13.13.3.3.3.6.cmml" xref="S2.E2.m1.13.13.3.3.3.6">ğ‘</ci><apply id="S2.E2.m1.12.12.2.2.2.2.1.1.cmml" xref="S2.E2.m1.12.12.2.2.2.2.1"><csymbol cd="latexml" id="S2.E2.m1.12.12.2.2.2.2.1.1.1.cmml" xref="S2.E2.m1.12.12.2.2.2.2.1.1.1">conditional</csymbol><ci id="S2.E2.m1.12.12.2.2.2.2.1.1.2.cmml" xref="S2.E2.m1.12.12.2.2.2.2.1.1.2">ğ‘‘</ci><list id="S2.E2.m1.12.12.2.2.2.2.1.1.3.1.cmml" xref="S2.E2.m1.12.12.2.2.2.2.1.1.3.2"><ci id="S2.E2.m1.7.7.cmml" xref="S2.E2.m1.7.7">ğ‘£</ci><ci id="S2.E2.m1.8.8.cmml" xref="S2.E2.m1.8.8">ğ‘</ci></list></apply><ci id="S2.E2.m1.13.13.3.3.3.7.cmml" xref="S2.E2.m1.13.13.3.3.3.7">ğ‘</ci><apply id="S2.E2.m1.13.13.3.3.3.3.1.1.cmml" xref="S2.E2.m1.13.13.3.3.3.3.1"><csymbol cd="latexml" id="S2.E2.m1.13.13.3.3.3.3.1.1.1.cmml" xref="S2.E2.m1.13.13.3.3.3.3.1.1.1">conditional</csymbol><ci id="S2.E2.m1.13.13.3.3.3.3.1.1.2.cmml" xref="S2.E2.m1.13.13.3.3.3.3.1.1.2">ğ‘§</ci><list id="S2.E2.m1.13.13.3.3.3.3.1.1.3.1.cmml" xref="S2.E2.m1.13.13.3.3.3.3.1.1.3.2"><ci id="S2.E2.m1.9.9.cmml" xref="S2.E2.m1.9.9">ğ‘£</ci><ci id="S2.E2.m1.10.10.cmml" xref="S2.E2.m1.10.10">ğ‘</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.13c">\hat{a}\!=\!\textrm{argmax}\!\sum_{d,z}p(a|v,q,d,z)p(d|v,q)p(z|v,q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p5.6" class="ltx_p">where the discrete latent variable <math id="S2.SS1.p5.4.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS1.p5.4.m1.1a"><mi id="S2.SS1.p5.4.m1.1.1" xref="S2.SS1.p5.4.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.4.m1.1b"><ci id="S2.SS1.p5.4.m1.1.1.cmml" xref="S2.SS1.p5.4.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.4.m1.1c">d</annotation></semantics></math> is directly integrated out, and the <math id="S2.SS1.p5.5.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p5.5.m2.1a"><mi id="S2.SS1.p5.5.m2.1.1" xref="S2.SS1.p5.5.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.5.m2.1b"><ci id="S2.SS1.p5.5.m2.1.1.cmml" xref="S2.SS1.p5.5.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.5.m2.1c">z</annotation></semantics></math> is the Monte-Carlo sample from <math id="S2.SS1.p5.6.m3.3" class="ltx_Math" alttext="p(z|v,q)" display="inline"><semantics id="S2.SS1.p5.6.m3.3a"><mrow id="S2.SS1.p5.6.m3.3.3" xref="S2.SS1.p5.6.m3.3.3.cmml"><mi id="S2.SS1.p5.6.m3.3.3.3" xref="S2.SS1.p5.6.m3.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.6.m3.3.3.2" xref="S2.SS1.p5.6.m3.3.3.2.cmml">â€‹</mo><mrow id="S2.SS1.p5.6.m3.3.3.1.1" xref="S2.SS1.p5.6.m3.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p5.6.m3.3.3.1.1.2" xref="S2.SS1.p5.6.m3.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS1.p5.6.m3.3.3.1.1.1" xref="S2.SS1.p5.6.m3.3.3.1.1.1.cmml"><mi id="S2.SS1.p5.6.m3.3.3.1.1.1.2" xref="S2.SS1.p5.6.m3.3.3.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p5.6.m3.3.3.1.1.1.1" xref="S2.SS1.p5.6.m3.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS1.p5.6.m3.3.3.1.1.1.3.2" xref="S2.SS1.p5.6.m3.3.3.1.1.1.3.1.cmml"><mi id="S2.SS1.p5.6.m3.1.1" xref="S2.SS1.p5.6.m3.1.1.cmml">v</mi><mo id="S2.SS1.p5.6.m3.3.3.1.1.1.3.2.1" xref="S2.SS1.p5.6.m3.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.p5.6.m3.2.2" xref="S2.SS1.p5.6.m3.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p5.6.m3.3.3.1.1.3" xref="S2.SS1.p5.6.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.6.m3.3b"><apply id="S2.SS1.p5.6.m3.3.3.cmml" xref="S2.SS1.p5.6.m3.3.3"><times id="S2.SS1.p5.6.m3.3.3.2.cmml" xref="S2.SS1.p5.6.m3.3.3.2"></times><ci id="S2.SS1.p5.6.m3.3.3.3.cmml" xref="S2.SS1.p5.6.m3.3.3.3">ğ‘</ci><apply id="S2.SS1.p5.6.m3.3.3.1.1.1.cmml" xref="S2.SS1.p5.6.m3.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.p5.6.m3.3.3.1.1.1.1.cmml" xref="S2.SS1.p5.6.m3.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p5.6.m3.3.3.1.1.1.2.cmml" xref="S2.SS1.p5.6.m3.3.3.1.1.1.2">ğ‘§</ci><list id="S2.SS1.p5.6.m3.3.3.1.1.1.3.1.cmml" xref="S2.SS1.p5.6.m3.3.3.1.1.1.3.2"><ci id="S2.SS1.p5.6.m3.1.1.cmml" xref="S2.SS1.p5.6.m3.1.1">ğ‘£</ci><ci id="S2.SS1.p5.6.m3.2.2.cmml" xref="S2.SS1.p5.6.m3.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.6.m3.3c">p(z|v,q)</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Continuous Latent Variable: Caption</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As captions are modelled by a continuous latent variable, we only have explicit captions during training.
Here we present the generative distribution that is conditioned on images and questions during testing, and the variational distribution that is conditioned on explicit captions during training.
Therefore, the caption encoder is only used in the training phase.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.3" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Generative Distribution - <math id="S2.SS2.p2.1.1.m1.3" class="ltx_Math" alttext="p_{\theta}(z|v,q)" display="inline"><semantics id="S2.SS2.p2.1.1.m1.3a"><mrow id="S2.SS2.p2.1.1.m1.3.3" xref="S2.SS2.p2.1.1.m1.3.3.cmml"><msub id="S2.SS2.p2.1.1.m1.3.3.3" xref="S2.SS2.p2.1.1.m1.3.3.3.cmml"><mi id="S2.SS2.p2.1.1.m1.3.3.3.2" xref="S2.SS2.p2.1.1.m1.3.3.3.2.cmml">p</mi><mi id="S2.SS2.p2.1.1.m1.3.3.3.3" xref="S2.SS2.p2.1.1.m1.3.3.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p2.1.1.m1.3.3.2" xref="S2.SS2.p2.1.1.m1.3.3.2.cmml">â€‹</mo><mrow id="S2.SS2.p2.1.1.m1.3.3.1.1" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p2.1.1.m1.3.3.1.1.2" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS2.p2.1.1.m1.3.3.1.1.1" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.cmml"><mi id="S2.SS2.p2.1.1.m1.3.3.1.1.1.2" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS2.p2.1.1.m1.3.3.1.1.1.1" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS2.p2.1.1.m1.3.3.1.1.1.3.2" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S2.SS2.p2.1.1.m1.1.1" xref="S2.SS2.p2.1.1.m1.1.1.cmml">v</mi><mo id="S2.SS2.p2.1.1.m1.3.3.1.1.1.3.2.1" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS2.p2.1.1.m1.2.2" xref="S2.SS2.p2.1.1.m1.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS2.p2.1.1.m1.3.3.1.1.3" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.1.m1.3b"><apply id="S2.SS2.p2.1.1.m1.3.3.cmml" xref="S2.SS2.p2.1.1.m1.3.3"><times id="S2.SS2.p2.1.1.m1.3.3.2.cmml" xref="S2.SS2.p2.1.1.m1.3.3.2"></times><apply id="S2.SS2.p2.1.1.m1.3.3.3.cmml" xref="S2.SS2.p2.1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.1.1.m1.3.3.3.1.cmml" xref="S2.SS2.p2.1.1.m1.3.3.3">subscript</csymbol><ci id="S2.SS2.p2.1.1.m1.3.3.3.2.cmml" xref="S2.SS2.p2.1.1.m1.3.3.3.2">ğ‘</ci><ci id="S2.SS2.p2.1.1.m1.3.3.3.3.cmml" xref="S2.SS2.p2.1.1.m1.3.3.3.3">ğœƒ</ci></apply><apply id="S2.SS2.p2.1.1.m1.3.3.1.1.1.cmml" xref="S2.SS2.p2.1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S2.SS2.p2.1.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS2.p2.1.1.m1.3.3.1.1.1.2.cmml" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.2">ğ‘§</ci><list id="S2.SS2.p2.1.1.m1.3.3.1.1.1.3.1.cmml" xref="S2.SS2.p2.1.1.m1.3.3.1.1.1.3.2"><ci id="S2.SS2.p2.1.1.m1.1.1.cmml" xref="S2.SS2.p2.1.1.m1.1.1">ğ‘£</ci><ci id="S2.SS2.p2.1.1.m1.2.2.cmml" xref="S2.SS2.p2.1.1.m1.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.1.m1.3c">p_{\theta}(z|v,q)</annotation></semantics></math></span>. We use a latent distribution <math id="S2.SS2.p2.2.m1.3" class="ltx_Math" alttext="p_{\theta}(z|v,q)" display="inline"><semantics id="S2.SS2.p2.2.m1.3a"><mrow id="S2.SS2.p2.2.m1.3.3" xref="S2.SS2.p2.2.m1.3.3.cmml"><msub id="S2.SS2.p2.2.m1.3.3.3" xref="S2.SS2.p2.2.m1.3.3.3.cmml"><mi id="S2.SS2.p2.2.m1.3.3.3.2" xref="S2.SS2.p2.2.m1.3.3.3.2.cmml">p</mi><mi id="S2.SS2.p2.2.m1.3.3.3.3" xref="S2.SS2.p2.2.m1.3.3.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m1.3.3.2" xref="S2.SS2.p2.2.m1.3.3.2.cmml">â€‹</mo><mrow id="S2.SS2.p2.2.m1.3.3.1.1" xref="S2.SS2.p2.2.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p2.2.m1.3.3.1.1.2" xref="S2.SS2.p2.2.m1.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS2.p2.2.m1.3.3.1.1.1" xref="S2.SS2.p2.2.m1.3.3.1.1.1.cmml"><mi id="S2.SS2.p2.2.m1.3.3.1.1.1.2" xref="S2.SS2.p2.2.m1.3.3.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS2.p2.2.m1.3.3.1.1.1.1" xref="S2.SS2.p2.2.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS2.p2.2.m1.3.3.1.1.1.3.2" xref="S2.SS2.p2.2.m1.3.3.1.1.1.3.1.cmml"><mi id="S2.SS2.p2.2.m1.1.1" xref="S2.SS2.p2.2.m1.1.1.cmml">v</mi><mo id="S2.SS2.p2.2.m1.3.3.1.1.1.3.2.1" xref="S2.SS2.p2.2.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS2.p2.2.m1.2.2" xref="S2.SS2.p2.2.m1.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS2.p2.2.m1.3.3.1.1.3" xref="S2.SS2.p2.2.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m1.3b"><apply id="S2.SS2.p2.2.m1.3.3.cmml" xref="S2.SS2.p2.2.m1.3.3"><times id="S2.SS2.p2.2.m1.3.3.2.cmml" xref="S2.SS2.p2.2.m1.3.3.2"></times><apply id="S2.SS2.p2.2.m1.3.3.3.cmml" xref="S2.SS2.p2.2.m1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m1.3.3.3.1.cmml" xref="S2.SS2.p2.2.m1.3.3.3">subscript</csymbol><ci id="S2.SS2.p2.2.m1.3.3.3.2.cmml" xref="S2.SS2.p2.2.m1.3.3.3.2">ğ‘</ci><ci id="S2.SS2.p2.2.m1.3.3.3.3.cmml" xref="S2.SS2.p2.2.m1.3.3.3.3">ğœƒ</ci></apply><apply id="S2.SS2.p2.2.m1.3.3.1.1.1.cmml" xref="S2.SS2.p2.2.m1.3.3.1.1"><csymbol cd="latexml" id="S2.SS2.p2.2.m1.3.3.1.1.1.1.cmml" xref="S2.SS2.p2.2.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS2.p2.2.m1.3.3.1.1.1.2.cmml" xref="S2.SS2.p2.2.m1.3.3.1.1.1.2">ğ‘§</ci><list id="S2.SS2.p2.2.m1.3.3.1.1.1.3.1.cmml" xref="S2.SS2.p2.2.m1.3.3.1.1.1.3.2"><ci id="S2.SS2.p2.2.m1.1.1.cmml" xref="S2.SS2.p2.2.m1.1.1">ğ‘£</ci><ci id="S2.SS2.p2.2.m1.2.2.cmml" xref="S2.SS2.p2.2.m1.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m1.3c">p_{\theta}(z|v,q)</annotation></semantics></math> to model the joint multimodal distributions of images and questions. Compared to its deterministic counterpart using concatenated multimodal features, we parameterise the stochastic distribution with <math id="S2.SS2.p2.3.m2.5" class="ltx_Math" alttext="\mathcal{N}(z|\mu_{\theta}(v,q),\sigma^{2}_{\theta}(v,q))" display="inline"><semantics id="S2.SS2.p2.3.m2.5a"><mrow id="S2.SS2.p2.3.m2.5.5" xref="S2.SS2.p2.3.m2.5.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.3.m2.5.5.3" xref="S2.SS2.p2.3.m2.5.5.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.3.m2.5.5.2" xref="S2.SS2.p2.3.m2.5.5.2.cmml">â€‹</mo><mrow id="S2.SS2.p2.3.m2.5.5.1.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p2.3.m2.5.5.1.1.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.cmml">(</mo><mrow id="S2.SS2.p2.3.m2.5.5.1.1.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.cmml"><mi id="S2.SS2.p2.3.m2.5.5.1.1.1.4" xref="S2.SS2.p2.3.m2.5.5.1.1.1.4.cmml">z</mi><mo fence="false" id="S2.SS2.p2.3.m2.5.5.1.1.1.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.3.cmml">|</mo><mrow id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.3.cmml"><mrow id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.cmml"><msub id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.cmml"><mi id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.2.cmml">Î¼</mi><mi id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.2.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S2.SS2.p2.3.m2.1.1" xref="S2.SS2.p2.3.m2.1.1.cmml">v</mi><mo id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.SS2.p2.3.m2.2.2" xref="S2.SS2.p2.3.m2.2.2.cmml">q</mi><mo stretchy="false" id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.2.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.3.cmml">,</mo><mrow id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.cmml"><msubsup id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.cmml"><mi id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.2.cmml">Ïƒ</mi><mi id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.3.cmml">Î¸</mi><mn id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.3.cmml">2</mn></msubsup><mo lspace="0em" rspace="0em" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.1.cmml">â€‹</mo><mrow id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.1.cmml"><mo stretchy="false" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.2.1" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.1.cmml">(</mo><mi id="S2.SS2.p2.3.m2.3.3" xref="S2.SS2.p2.3.m2.3.3.cmml">v</mi><mo id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.2.2" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.1.cmml">,</mo><mi id="S2.SS2.p2.3.m2.4.4" xref="S2.SS2.p2.3.m2.4.4.cmml">q</mi><mo stretchy="false" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.2.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false" id="S2.SS2.p2.3.m2.5.5.1.1.3" xref="S2.SS2.p2.3.m2.5.5.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m2.5b"><apply id="S2.SS2.p2.3.m2.5.5.cmml" xref="S2.SS2.p2.3.m2.5.5"><times id="S2.SS2.p2.3.m2.5.5.2.cmml" xref="S2.SS2.p2.3.m2.5.5.2"></times><ci id="S2.SS2.p2.3.m2.5.5.3.cmml" xref="S2.SS2.p2.3.m2.5.5.3">ğ’©</ci><apply id="S2.SS2.p2.3.m2.5.5.1.1.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1"><csymbol cd="latexml" id="S2.SS2.p2.3.m2.5.5.1.1.1.3.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.3">conditional</csymbol><ci id="S2.SS2.p2.3.m2.5.5.1.1.1.4.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.4">ğ‘§</ci><list id="S2.SS2.p2.3.m2.5.5.1.1.1.2.3.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2"><apply id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1"><times id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.1"></times><apply id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.2.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.2">ğœ‡</ci><ci id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.3.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.2.3">ğœƒ</ci></apply><interval closure="open" id="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.1.1.1.3.2"><ci id="S2.SS2.p2.3.m2.1.1.cmml" xref="S2.SS2.p2.3.m2.1.1">ğ‘£</ci><ci id="S2.SS2.p2.3.m2.2.2.cmml" xref="S2.SS2.p2.3.m2.2.2">ğ‘</ci></interval></apply><apply id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2"><times id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.1"></times><apply id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2">subscript</csymbol><apply id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2">superscript</csymbol><ci id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.2.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.2">ğœ</ci><cn type="integer" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.3.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.2.3">2</cn></apply><ci id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.3.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.2.3">ğœƒ</ci></apply><interval closure="open" id="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.1.cmml" xref="S2.SS2.p2.3.m2.5.5.1.1.1.2.2.2.3.2"><ci id="S2.SS2.p2.3.m2.3.3.cmml" xref="S2.SS2.p2.3.m2.3.3">ğ‘£</ci><ci id="S2.SS2.p2.3.m2.4.4.cmml" xref="S2.SS2.p2.3.m2.4.4">ğ‘</ci></interval></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m2.5c">\mathcal{N}(z|\mu_{\theta}(v,q),\sigma^{2}_{\theta}(v,q))</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.4" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Variational Distribution - <math id="S2.SS2.p3.1.1.m1.1" class="ltx_Math" alttext="q_{\phi}(z|c)" display="inline"><semantics id="S2.SS2.p3.1.1.m1.1a"><mrow id="S2.SS2.p3.1.1.m1.1.1" xref="S2.SS2.p3.1.1.m1.1.1.cmml"><msub id="S2.SS2.p3.1.1.m1.1.1.3" xref="S2.SS2.p3.1.1.m1.1.1.3.cmml"><mi id="S2.SS2.p3.1.1.m1.1.1.3.2" xref="S2.SS2.p3.1.1.m1.1.1.3.2.cmml">q</mi><mi id="S2.SS2.p3.1.1.m1.1.1.3.3" xref="S2.SS2.p3.1.1.m1.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.1.m1.1.1.2" xref="S2.SS2.p3.1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS2.p3.1.1.m1.1.1.1.1" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p3.1.1.m1.1.1.1.1.2" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.p3.1.1.m1.1.1.1.1.1" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS2.p3.1.1.m1.1.1.1.1.1.2" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS2.p3.1.1.m1.1.1.1.1.1.1" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS2.p3.1.1.m1.1.1.1.1.1.3" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS2.p3.1.1.m1.1.1.1.1.3" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.1.m1.1b"><apply id="S2.SS2.p3.1.1.m1.1.1.cmml" xref="S2.SS2.p3.1.1.m1.1.1"><times id="S2.SS2.p3.1.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.1.m1.1.1.2"></times><apply id="S2.SS2.p3.1.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.1.1.m1.1.1.3.1.cmml" xref="S2.SS2.p3.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.1.1.m1.1.1.3.2.cmml" xref="S2.SS2.p3.1.1.m1.1.1.3.2">ğ‘</ci><ci id="S2.SS2.p3.1.1.m1.1.1.3.3.cmml" xref="S2.SS2.p3.1.1.m1.1.1.3.3">italic-Ï•</ci></apply><apply id="S2.SS2.p3.1.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.p3.1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS2.p3.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS2.p3.1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.2">ğ‘§</ci><ci id="S2.SS2.p3.1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS2.p3.1.1.m1.1.1.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.1.m1.1c">q_{\phi}(z|c)</annotation></semantics></math></span>.
We first apply a RNN model to embed the caption inputs <math id="S2.SS2.p3.2.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS2.p3.2.m1.1a"><mi id="S2.SS2.p3.2.m1.1.1" xref="S2.SS2.p3.2.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m1.1b"><ci id="S2.SS2.p3.2.m1.1.1.cmml" xref="S2.SS2.p3.2.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m1.1c">C</annotation></semantics></math> and a latent variable <math id="S2.SS2.p3.3.m2.1" class="ltx_Math" alttext="q_{\phi}(z|c)" display="inline"><semantics id="S2.SS2.p3.3.m2.1a"><mrow id="S2.SS2.p3.3.m2.1.1" xref="S2.SS2.p3.3.m2.1.1.cmml"><msub id="S2.SS2.p3.3.m2.1.1.3" xref="S2.SS2.p3.3.m2.1.1.3.cmml"><mi id="S2.SS2.p3.3.m2.1.1.3.2" xref="S2.SS2.p3.3.m2.1.1.3.2.cmml">q</mi><mi id="S2.SS2.p3.3.m2.1.1.3.3" xref="S2.SS2.p3.3.m2.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p3.3.m2.1.1.2" xref="S2.SS2.p3.3.m2.1.1.2.cmml">â€‹</mo><mrow id="S2.SS2.p3.3.m2.1.1.1.1" xref="S2.SS2.p3.3.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p3.3.m2.1.1.1.1.2" xref="S2.SS2.p3.3.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.p3.3.m2.1.1.1.1.1" xref="S2.SS2.p3.3.m2.1.1.1.1.1.cmml"><mi id="S2.SS2.p3.3.m2.1.1.1.1.1.2" xref="S2.SS2.p3.3.m2.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS2.p3.3.m2.1.1.1.1.1.1" xref="S2.SS2.p3.3.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS2.p3.3.m2.1.1.1.1.1.3" xref="S2.SS2.p3.3.m2.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS2.p3.3.m2.1.1.1.1.3" xref="S2.SS2.p3.3.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m2.1b"><apply id="S2.SS2.p3.3.m2.1.1.cmml" xref="S2.SS2.p3.3.m2.1.1"><times id="S2.SS2.p3.3.m2.1.1.2.cmml" xref="S2.SS2.p3.3.m2.1.1.2"></times><apply id="S2.SS2.p3.3.m2.1.1.3.cmml" xref="S2.SS2.p3.3.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m2.1.1.3.1.cmml" xref="S2.SS2.p3.3.m2.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.3.m2.1.1.3.2.cmml" xref="S2.SS2.p3.3.m2.1.1.3.2">ğ‘</ci><ci id="S2.SS2.p3.3.m2.1.1.3.3.cmml" xref="S2.SS2.p3.3.m2.1.1.3.3">italic-Ï•</ci></apply><apply id="S2.SS2.p3.3.m2.1.1.1.1.1.cmml" xref="S2.SS2.p3.3.m2.1.1.1.1"><csymbol cd="latexml" id="S2.SS2.p3.3.m2.1.1.1.1.1.1.cmml" xref="S2.SS2.p3.3.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS2.p3.3.m2.1.1.1.1.1.2.cmml" xref="S2.SS2.p3.3.m2.1.1.1.1.1.2">ğ‘§</ci><ci id="S2.SS2.p3.3.m2.1.1.1.1.1.3.cmml" xref="S2.SS2.p3.3.m2.1.1.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m2.1c">q_{\phi}(z|c)</annotation></semantics></math> to model the caption semantics and distributions, where <math id="S2.SS2.p3.4.m3.3" class="ltx_Math" alttext="z\sim\mathcal{N}(z|\mu_{\phi}(c),\sigma^{2}_{\phi}(c))" display="inline"><semantics id="S2.SS2.p3.4.m3.3a"><mrow id="S2.SS2.p3.4.m3.3.3" xref="S2.SS2.p3.4.m3.3.3.cmml"><mi id="S2.SS2.p3.4.m3.3.3.3" xref="S2.SS2.p3.4.m3.3.3.3.cmml">z</mi><mo id="S2.SS2.p3.4.m3.3.3.2" xref="S2.SS2.p3.4.m3.3.3.2.cmml">âˆ¼</mo><mrow id="S2.SS2.p3.4.m3.3.3.1" xref="S2.SS2.p3.4.m3.3.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p3.4.m3.3.3.1.3" xref="S2.SS2.p3.4.m3.3.3.1.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.4.m3.3.3.1.2" xref="S2.SS2.p3.4.m3.3.3.1.2.cmml">â€‹</mo><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.cmml"><mi id="S2.SS2.p3.4.m3.3.3.1.1.1.1.4" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.4.cmml">z</mi><mo fence="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.3" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.3.cmml">|</mo><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.3.cmml"><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.2.cmml">Î¼</mi><mi id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.3" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.3.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.3.2.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.SS2.p3.4.m3.1.1" xref="S2.SS2.p3.4.m3.1.1.cmml">c</mi><mo stretchy="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.3.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.3" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.3.cmml">,</mo><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.cmml"><msubsup id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.cmml"><mi id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.2.cmml">Ïƒ</mi><mi id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.3" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.3.cmml">Ï•</mi><mn id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.3" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.3.cmml">2</mn></msubsup><mo lspace="0em" rspace="0em" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.1.cmml">â€‹</mo><mrow id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.3.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.cmml"><mo stretchy="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.3.2.1" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.cmml">(</mo><mi id="S2.SS2.p3.4.m3.2.2" xref="S2.SS2.p3.4.m3.2.2.cmml">c</mi><mo stretchy="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.3.2.2" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false" id="S2.SS2.p3.4.m3.3.3.1.1.1.3" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m3.3b"><apply id="S2.SS2.p3.4.m3.3.3.cmml" xref="S2.SS2.p3.4.m3.3.3"><csymbol cd="latexml" id="S2.SS2.p3.4.m3.3.3.2.cmml" xref="S2.SS2.p3.4.m3.3.3.2">similar-to</csymbol><ci id="S2.SS2.p3.4.m3.3.3.3.cmml" xref="S2.SS2.p3.4.m3.3.3.3">ğ‘§</ci><apply id="S2.SS2.p3.4.m3.3.3.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1"><times id="S2.SS2.p3.4.m3.3.3.1.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.2"></times><ci id="S2.SS2.p3.4.m3.3.3.1.3.cmml" xref="S2.SS2.p3.4.m3.3.3.1.3">ğ’©</ci><apply id="S2.SS2.p3.4.m3.3.3.1.1.1.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1"><csymbol cd="latexml" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.3.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.3">conditional</csymbol><ci id="S2.SS2.p3.4.m3.3.3.1.1.1.1.4.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.4">ğ‘§</ci><list id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.3.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2"><apply id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1"><times id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.1"></times><apply id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.2">ğœ‡</ci><ci id="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.1.1.1.2.3">italic-Ï•</ci></apply><ci id="S2.SS2.p3.4.m3.1.1.cmml" xref="S2.SS2.p3.4.m3.1.1">ğ‘</ci></apply><apply id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2"><times id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.1"></times><apply id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2">subscript</csymbol><apply id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.1.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2">superscript</csymbol><ci id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.2.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.2">ğœ</ci><cn type="integer" id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.3.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.2.3">2</cn></apply><ci id="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.3.cmml" xref="S2.SS2.p3.4.m3.3.3.1.1.1.1.2.2.2.2.3">italic-Ï•</ci></apply><ci id="S2.SS2.p3.4.m3.2.2.cmml" xref="S2.SS2.p3.4.m3.2.2">ğ‘</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m3.3c">z\sim\mathcal{N}(z|\mu_{\phi}(c),\sigma^{2}_{\phi}(c))</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Discrete Latent Variable: Answer Category</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.4" class="ltx_p">Assuming that each image and question pair <math id="S2.SS3.p1.1.m1.2" class="ltx_Math" alttext="(v,q)" display="inline"><semantics id="S2.SS3.p1.1.m1.2a"><mrow id="S2.SS3.p1.1.m1.2.3.2" xref="S2.SS3.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS3.p1.1.m1.2.3.2.1" xref="S2.SS3.p1.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">v</mi><mo id="S2.SS3.p1.1.m1.2.3.2.2" xref="S2.SS3.p1.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS3.p1.1.m1.2.2" xref="S2.SS3.p1.1.m1.2.2.cmml">q</mi><mo stretchy="false" id="S2.SS3.p1.1.m1.2.3.2.3" xref="S2.SS3.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.2b"><interval closure="open" id="S2.SS3.p1.1.m1.2.3.1.cmml" xref="S2.SS3.p1.1.m1.2.3.2"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">ğ‘£</ci><ci id="S2.SS3.p1.1.m1.2.2.cmml" xref="S2.SS3.p1.1.m1.2.2">ğ‘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.2c">(v,q)</annotation></semantics></math> can be projected to an answer category to help find a correct answer, we are able to encourage the model to distinguishing candidates across answer categories instead of only the spurious relationships between questions and answers via simple linguistic features.
Therefore, in order to leverage this useful inductive bias, we propose a discrete latent variable to model the answer category given an image and question pair <math id="S2.SS3.p1.2.m2.2" class="ltx_Math" alttext="(v,q)" display="inline"><semantics id="S2.SS3.p1.2.m2.2a"><mrow id="S2.SS3.p1.2.m2.2.3.2" xref="S2.SS3.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S2.SS3.p1.2.m2.2.3.2.1" xref="S2.SS3.p1.2.m2.2.3.1.cmml">(</mo><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">v</mi><mo id="S2.SS3.p1.2.m2.2.3.2.2" xref="S2.SS3.p1.2.m2.2.3.1.cmml">,</mo><mi id="S2.SS3.p1.2.m2.2.2" xref="S2.SS3.p1.2.m2.2.2.cmml">q</mi><mo stretchy="false" id="S2.SS3.p1.2.m2.2.3.2.3" xref="S2.SS3.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.2b"><interval closure="open" id="S2.SS3.p1.2.m2.2.3.1.cmml" xref="S2.SS3.p1.2.m2.2.3.2"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ğ‘£</ci><ci id="S2.SS3.p1.2.m2.2.2.cmml" xref="S2.SS3.p1.2.m2.2.2">ğ‘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.2c">(v,q)</annotation></semantics></math>.
In particular, for each answer category <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">d</annotation></semantics></math>, we have a conditional independent distribution <math id="S2.SS3.p1.4.m4.4" class="ltx_Math" alttext="p(a|v,q,d)" display="inline"><semantics id="S2.SS3.p1.4.m4.4a"><mrow id="S2.SS3.p1.4.m4.4.4" xref="S2.SS3.p1.4.m4.4.4.cmml"><mi id="S2.SS3.p1.4.m4.4.4.3" xref="S2.SS3.p1.4.m4.4.4.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.4.m4.4.4.2" xref="S2.SS3.p1.4.m4.4.4.2.cmml">â€‹</mo><mrow id="S2.SS3.p1.4.m4.4.4.1.1" xref="S2.SS3.p1.4.m4.4.4.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.4.m4.4.4.1.1.2" xref="S2.SS3.p1.4.m4.4.4.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.4.m4.4.4.1.1.1" xref="S2.SS3.p1.4.m4.4.4.1.1.1.cmml"><mi id="S2.SS3.p1.4.m4.4.4.1.1.1.2" xref="S2.SS3.p1.4.m4.4.4.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS3.p1.4.m4.4.4.1.1.1.1" xref="S2.SS3.p1.4.m4.4.4.1.1.1.1.cmml">|</mo><mrow id="S2.SS3.p1.4.m4.4.4.1.1.1.3.2" xref="S2.SS3.p1.4.m4.4.4.1.1.1.3.1.cmml"><mi id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">v</mi><mo id="S2.SS3.p1.4.m4.4.4.1.1.1.3.2.1" xref="S2.SS3.p1.4.m4.4.4.1.1.1.3.1.cmml">,</mo><mi id="S2.SS3.p1.4.m4.2.2" xref="S2.SS3.p1.4.m4.2.2.cmml">q</mi><mo id="S2.SS3.p1.4.m4.4.4.1.1.1.3.2.2" xref="S2.SS3.p1.4.m4.4.4.1.1.1.3.1.cmml">,</mo><mi id="S2.SS3.p1.4.m4.3.3" xref="S2.SS3.p1.4.m4.3.3.cmml">d</mi></mrow></mrow><mo stretchy="false" id="S2.SS3.p1.4.m4.4.4.1.1.3" xref="S2.SS3.p1.4.m4.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.4b"><apply id="S2.SS3.p1.4.m4.4.4.cmml" xref="S2.SS3.p1.4.m4.4.4"><times id="S2.SS3.p1.4.m4.4.4.2.cmml" xref="S2.SS3.p1.4.m4.4.4.2"></times><ci id="S2.SS3.p1.4.m4.4.4.3.cmml" xref="S2.SS3.p1.4.m4.4.4.3">ğ‘</ci><apply id="S2.SS3.p1.4.m4.4.4.1.1.1.cmml" xref="S2.SS3.p1.4.m4.4.4.1.1"><csymbol cd="latexml" id="S2.SS3.p1.4.m4.4.4.1.1.1.1.cmml" xref="S2.SS3.p1.4.m4.4.4.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.4.m4.4.4.1.1.1.2.cmml" xref="S2.SS3.p1.4.m4.4.4.1.1.1.2">ğ‘</ci><list id="S2.SS3.p1.4.m4.4.4.1.1.1.3.1.cmml" xref="S2.SS3.p1.4.m4.4.4.1.1.1.3.2"><ci id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">ğ‘£</ci><ci id="S2.SS3.p1.4.m4.2.2.cmml" xref="S2.SS3.p1.4.m4.2.2">ğ‘</ci><ci id="S2.SS3.p1.4.m4.3.3.cmml" xref="S2.SS3.p1.4.m4.3.3">ğ‘‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.4c">p(a|v,q,d)</annotation></semantics></math> over the answers in the certain answer category.</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.10" class="ltx_Math" alttext="p(a|v,q)=\sum_{d}p(a|v,q,d)\cdot p(d|v,q)" display="block"><semantics id="S2.E3.m1.10a"><mrow id="S2.E3.m1.10.10" xref="S2.E3.m1.10.10.cmml"><mrow id="S2.E3.m1.8.8.1" xref="S2.E3.m1.8.8.1.cmml"><mi id="S2.E3.m1.8.8.1.3" xref="S2.E3.m1.8.8.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.8.8.1.2" xref="S2.E3.m1.8.8.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.8.8.1.1.1" xref="S2.E3.m1.8.8.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.8.8.1.1.1.2" xref="S2.E3.m1.8.8.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.8.8.1.1.1.1" xref="S2.E3.m1.8.8.1.1.1.1.cmml"><mi id="S2.E3.m1.8.8.1.1.1.1.2" xref="S2.E3.m1.8.8.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.E3.m1.8.8.1.1.1.1.1" xref="S2.E3.m1.8.8.1.1.1.1.1.cmml">|</mo><mrow id="S2.E3.m1.8.8.1.1.1.1.3.2" xref="S2.E3.m1.8.8.1.1.1.1.3.1.cmml"><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">v</mi><mo id="S2.E3.m1.8.8.1.1.1.1.3.2.1" xref="S2.E3.m1.8.8.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.E3.m1.8.8.1.1.1.3" xref="S2.E3.m1.8.8.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.E3.m1.10.10.4" xref="S2.E3.m1.10.10.4.cmml">=</mo><mrow id="S2.E3.m1.10.10.3" xref="S2.E3.m1.10.10.3.cmml"><munder id="S2.E3.m1.10.10.3.3" xref="S2.E3.m1.10.10.3.3.cmml"><mo movablelimits="false" id="S2.E3.m1.10.10.3.3.2" xref="S2.E3.m1.10.10.3.3.2.cmml">âˆ‘</mo><mi id="S2.E3.m1.10.10.3.3.3" xref="S2.E3.m1.10.10.3.3.3.cmml">d</mi></munder><mrow id="S2.E3.m1.10.10.3.2" xref="S2.E3.m1.10.10.3.2.cmml"><mrow id="S2.E3.m1.9.9.2.1.1" xref="S2.E3.m1.9.9.2.1.1.cmml"><mrow id="S2.E3.m1.9.9.2.1.1.1" xref="S2.E3.m1.9.9.2.1.1.1.cmml"><mi id="S2.E3.m1.9.9.2.1.1.1.3" xref="S2.E3.m1.9.9.2.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.9.9.2.1.1.1.2" xref="S2.E3.m1.9.9.2.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.9.9.2.1.1.1.1.1" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.9.9.2.1.1.1.1.1.2" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.9.9.2.1.1.1.1.1.1" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.9.9.2.1.1.1.1.1.1.2" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.E3.m1.9.9.2.1.1.1.1.1.1.1" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.2" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.1.cmml"><mi id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml">v</mi><mo id="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.2.1" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml">q</mi><mo id="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.2.2" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E3.m1.5.5" xref="S2.E3.m1.5.5.cmml">d</mi></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S2.E3.m1.9.9.2.1.1.1.1.1.3" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.E3.m1.9.9.2.1.1.2" xref="S2.E3.m1.9.9.2.1.1.2.cmml">â‹…</mo><mi id="S2.E3.m1.9.9.2.1.1.3" xref="S2.E3.m1.9.9.2.1.1.3.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.10.10.3.2.3" xref="S2.E3.m1.10.10.3.2.3.cmml">â€‹</mo><mrow id="S2.E3.m1.10.10.3.2.2.1" xref="S2.E3.m1.10.10.3.2.2.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.10.10.3.2.2.1.2" xref="S2.E3.m1.10.10.3.2.2.1.1.cmml">(</mo><mrow id="S2.E3.m1.10.10.3.2.2.1.1" xref="S2.E3.m1.10.10.3.2.2.1.1.cmml"><mi id="S2.E3.m1.10.10.3.2.2.1.1.2" xref="S2.E3.m1.10.10.3.2.2.1.1.2.cmml">d</mi><mo fence="false" id="S2.E3.m1.10.10.3.2.2.1.1.1" xref="S2.E3.m1.10.10.3.2.2.1.1.1.cmml">|</mo><mrow id="S2.E3.m1.10.10.3.2.2.1.1.3.2" xref="S2.E3.m1.10.10.3.2.2.1.1.3.1.cmml"><mi id="S2.E3.m1.6.6" xref="S2.E3.m1.6.6.cmml">v</mi><mo id="S2.E3.m1.10.10.3.2.2.1.1.3.2.1" xref="S2.E3.m1.10.10.3.2.2.1.1.3.1.cmml">,</mo><mi id="S2.E3.m1.7.7" xref="S2.E3.m1.7.7.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.E3.m1.10.10.3.2.2.1.3" xref="S2.E3.m1.10.10.3.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.10b"><apply id="S2.E3.m1.10.10.cmml" xref="S2.E3.m1.10.10"><eq id="S2.E3.m1.10.10.4.cmml" xref="S2.E3.m1.10.10.4"></eq><apply id="S2.E3.m1.8.8.1.cmml" xref="S2.E3.m1.8.8.1"><times id="S2.E3.m1.8.8.1.2.cmml" xref="S2.E3.m1.8.8.1.2"></times><ci id="S2.E3.m1.8.8.1.3.cmml" xref="S2.E3.m1.8.8.1.3">ğ‘</ci><apply id="S2.E3.m1.8.8.1.1.1.1.cmml" xref="S2.E3.m1.8.8.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.8.8.1.1.1.1.1.cmml" xref="S2.E3.m1.8.8.1.1.1.1.1">conditional</csymbol><ci id="S2.E3.m1.8.8.1.1.1.1.2.cmml" xref="S2.E3.m1.8.8.1.1.1.1.2">ğ‘</ci><list id="S2.E3.m1.8.8.1.1.1.1.3.1.cmml" xref="S2.E3.m1.8.8.1.1.1.1.3.2"><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">ğ‘£</ci><ci id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">ğ‘</ci></list></apply></apply><apply id="S2.E3.m1.10.10.3.cmml" xref="S2.E3.m1.10.10.3"><apply id="S2.E3.m1.10.10.3.3.cmml" xref="S2.E3.m1.10.10.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.3.3.1.cmml" xref="S2.E3.m1.10.10.3.3">subscript</csymbol><sum id="S2.E3.m1.10.10.3.3.2.cmml" xref="S2.E3.m1.10.10.3.3.2"></sum><ci id="S2.E3.m1.10.10.3.3.3.cmml" xref="S2.E3.m1.10.10.3.3.3">ğ‘‘</ci></apply><apply id="S2.E3.m1.10.10.3.2.cmml" xref="S2.E3.m1.10.10.3.2"><times id="S2.E3.m1.10.10.3.2.3.cmml" xref="S2.E3.m1.10.10.3.2.3"></times><apply id="S2.E3.m1.9.9.2.1.1.cmml" xref="S2.E3.m1.9.9.2.1.1"><ci id="S2.E3.m1.9.9.2.1.1.2.cmml" xref="S2.E3.m1.9.9.2.1.1.2">â‹…</ci><apply id="S2.E3.m1.9.9.2.1.1.1.cmml" xref="S2.E3.m1.9.9.2.1.1.1"><times id="S2.E3.m1.9.9.2.1.1.1.2.cmml" xref="S2.E3.m1.9.9.2.1.1.1.2"></times><ci id="S2.E3.m1.9.9.2.1.1.1.3.cmml" xref="S2.E3.m1.9.9.2.1.1.1.3">ğ‘</ci><apply id="S2.E3.m1.9.9.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.9.9.2.1.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.9.9.2.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.E3.m1.9.9.2.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.2">ğ‘</ci><list id="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.9.9.2.1.1.1.1.1.1.3.2"><ci id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3">ğ‘£</ci><ci id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4">ğ‘</ci><ci id="S2.E3.m1.5.5.cmml" xref="S2.E3.m1.5.5">ğ‘‘</ci></list></apply></apply><ci id="S2.E3.m1.9.9.2.1.1.3.cmml" xref="S2.E3.m1.9.9.2.1.1.3">ğ‘</ci></apply><apply id="S2.E3.m1.10.10.3.2.2.1.1.cmml" xref="S2.E3.m1.10.10.3.2.2.1"><csymbol cd="latexml" id="S2.E3.m1.10.10.3.2.2.1.1.1.cmml" xref="S2.E3.m1.10.10.3.2.2.1.1.1">conditional</csymbol><ci id="S2.E3.m1.10.10.3.2.2.1.1.2.cmml" xref="S2.E3.m1.10.10.3.2.2.1.1.2">ğ‘‘</ci><list id="S2.E3.m1.10.10.3.2.2.1.1.3.1.cmml" xref="S2.E3.m1.10.10.3.2.2.1.1.3.2"><ci id="S2.E3.m1.6.6.cmml" xref="S2.E3.m1.6.6">ğ‘£</ci><ci id="S2.E3.m1.7.7.cmml" xref="S2.E3.m1.7.7">ğ‘</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.10c">p(a|v,q)=\sum_{d}p(a|v,q,d)\cdot p(d|v,q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS3.p1.5" class="ltx_p">We trained an answer category classifier using joint image-question pairs as the input, given the true labels as shown at left bottom in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We then use the category distribution to modify the answer distribution through element-wise production to get more precise answer distribution.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets &amp; Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the VQA v2.0 dataset
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for our proposed latent variable model.
The answers are balanced in order to minimise the effectiveness of dataset priors.
We report the results on validation set and test-standard set through the official evaluation server.
The source of image captions in our work is the MSCOCO dataset
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We use answer categories from the annotations of Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
The answers in the VQA v2.0 dataset are annotated with a set of 15 categories for the top 500 answers that makes up the 82% <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Although the category definitions cannot cover all types of answers, and false prediction during testing might be observed, the latent variable can still maintain the robustness in predicting correct answers by summing over all the probabilities of predicted categories.</span></span></span> of the VQA v2.0 dataset; and the other answers are treated as an additional category.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we first describe the experimental results of our latent variable model, compared with both a UpDn (Bottom-up Top-down) baseline model and a state-of-the-art pre-trained visual-linguistic model (VL-BERT); then we conduct qualitative analysis to validate the effectiveness of proposed components.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:329.0pt;height:145.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.3pt,8.1pt) scale(0.9,0.9) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="4">VQA v2.0 test-dev (%)</th>
<th id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">test-std (%)</th>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.1.3.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">All</span></th>
<th id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">Yes/No</span></th>
<th id="S4.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">Num</span></th>
<th id="S4.T1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.3.2.5.1" class="ltx_text" style="font-size:80%;">Other</span></th>
<th id="S4.T1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.3.2.6.1" class="ltx_text" style="font-size:80%;">All</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.4.1" class="ltx_tr">
<th id="S4.T1.1.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">CaptionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S4.T1.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">68.37</td>
</tr>
<tr id="S4.T1.1.1.5.2" class="ltx_tr">
<th id="S4.T1.1.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">DFAFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<td id="S4.T1.1.1.5.2.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">70.22</td>
<td id="S4.T1.1.1.5.2.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">86.09</td>
<td id="S4.T1.1.1.5.2.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">53.32</td>
<td id="S4.T1.1.1.5.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">60.49</td>
<td id="S4.T1.1.1.5.2.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">70.34</td>
</tr>
<tr id="S4.T1.1.1.6.3" class="ltx_tr">
<th id="S4.T1.1.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">MLINÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="S4.T1.1.1.6.3.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">71.09</td>
<td id="S4.T1.1.1.6.3.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">87.07</td>
<td id="S4.T1.1.1.6.3.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">53.39</td>
<td id="S4.T1.1.1.6.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">60.49</td>
<td id="S4.T1.1.1.6.3.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">71.27</td>
</tr>
<tr id="S4.T1.1.1.7.4" class="ltx_tr">
<th id="S4.T1.1.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">UpDnÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T1.1.1.7.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">65.32</td>
<td id="S4.T1.1.1.7.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">81.82</td>
<td id="S4.T1.1.1.7.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">44.21</td>
<td id="S4.T1.1.1.7.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">56.05</td>
<td id="S4.T1.1.1.7.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">65.67</td>
</tr>
<tr id="S4.T1.1.1.8.5" class="ltx_tr">
<th id="S4.T1.1.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">+ latent (ours)</th>
<td id="S4.T1.1.1.8.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.8.5.2.1" class="ltx_text ltx_framed ltx_framed_underline">66.01</span></td>
<td id="S4.T1.1.1.8.5.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">82.96</td>
<td id="S4.T1.1.1.8.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">44.58</td>
<td id="S4.T1.1.1.8.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">55.94</td>
<td id="S4.T1.1.1.8.5.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.8.5.6.1" class="ltx_text ltx_framed ltx_framed_underline">66.29</span></td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">VL-BERT<math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\textrm{large}}" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.m1.1.1a" xref="S4.T1.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T1.1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"><ci id="S4.T1.1.1.1.1.m1.1.1.1a.cmml" xref="S4.T1.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.T1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">{}_{\textrm{large}}</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">71.79</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">72.22</td>
</tr>
<tr id="S4.T1.1.1.9.6" class="ltx_tr">
<th id="S4.T1.1.1.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">+ latent (ours)</th>
<td id="S4.T1.1.1.9.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.9.6.2.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">72.03</span></td>
<td id="S4.T1.1.1.9.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">88.03</td>
<td id="S4.T1.1.1.9.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">54.16</td>
<td id="S4.T1.1.1.9.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">62.42</td>
<td id="S4.T1.1.1.9.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S4.T1.1.1.9.6.6.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">72.37</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental results on VQA v2.0 test-dev and test-standard (test-std) set. Accuracies are reported in percentage (%) terms. The state-of-the-art scores are in bold; underlined scores are best among (baseline <em id="S4.T1.4.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S4.T1.5.2" class="ltx_text"></span> latent variable extension); and both underlines and bold scores are the overall best results.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Quantitative Analysis</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We compare the results of our latent variable model with the baseline model (UpDn), a state-of-the-art visual-linguistic pre-training model (VL-BERT), and three other related VQA models; whereÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> uses generated captions to assist answer predictions andÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> explore the interactions between visual and linguistic inputs.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As demonstrated in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiments â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our latent variable model outperforms when acting as an extension.
In particular, our latent variable model outperforms UpDn by 0.69% accuracy on test-dev set and by 0.62% accuracy on test-standard set.
In addition, our model improves the performance by 0.24% accuracy than its VL-BERT counterpart on test-dev set and by 0.15% accuracy on test-standard set.
These results indicate the effectiveness of including captions and answer categories as latent variables, to promote the distribution of image and caption pairs to be closer to the captionsâ€™ space, and to learn a better distinction among different kinds of answers, or different answers within the same answer category.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The result ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (68.37%) is a very strong baseline, which follows a traditional deterministic approach.
However, their model is trained to generate captions that can be used at test time, while in our case only image and question pairs are required for answer prediction.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> both achieve comparable performance (70.34% and 71.27% on test-standard set, respectively) to VL-BERT (72.22%) without pre-training, by dynamically modulating the intra-modality information and exploring the latent interaction between modalities.
Our latent variable model has the overall best result when combined with the strong pre-training VL-BERT, which indicates both the effectiveness of the visual-linguistic pre-training framework, and the incorporation of continuous (captions) and discrete (answer categories) latent variables.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Compared to the results on the standard baseline (UpDn), the improvements achieved by our proposed model on the VL-BERT framework is smaller.
This is because VL-BERT has been pre-trained on massive image captioning data, where the learning of visual features have largely benefited from the modality of captions already.
Nevertheless, based upon the strong baseline model, our proposed model can still improve performance slightly, which further indicates the effectiveness of the latent variable framework.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">The state-of-the-art performance on VQA v2.0 among pre-training frameworks is achieved by LXMERT, Oscar and Uniter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
They have been extensively pre-trained using massive datasets on languages and vision tasks (including VQA) in a multi-task learning fashion.
Our work is not directly comparable, and is not aimed at improving and beating the state-of-the-art performance.
Instead, it is focused on exploring the potential of latent variable models to represent additional useful information in multimodal learning and to contribute to pre-trained vision-language frameworks.
We draw attention to the advantage of using generative framework on the VQA task.
In this case, we can employ more information during training (which is omitted in testing) to regularise the original multimodal distribution.
This can be demonstrated by the improvements on VL-BERT brought by the latent variables.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:242.6pt;height:119.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.4pt,3.1pt) scale(0.95,0.95) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="4">VQA v2.0 val</th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<th id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T2.1.1.2.2.2.1" class="ltx_text" style="font-size:80%;">All</span></th>
<th id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T2.1.1.2.2.3.1" class="ltx_text" style="font-size:80%;">Yes/No</span></th>
<th id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T2.1.1.2.2.4.1" class="ltx_text" style="font-size:80%;">Num</span></th>
<th id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T2.1.1.2.2.5.1" class="ltx_text" style="font-size:80%;">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.3.1" class="ltx_tr">
<th id="S4.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">UpDn</th>
<td id="S4.T2.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">63.15</td>
<td id="S4.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">80.38</td>
<td id="S4.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">42.84</td>
<td id="S4.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">55.86</td>
</tr>
<tr id="S4.T2.1.1.4.2" class="ltx_tr">
<th id="S4.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">UpDn + caption</th>
<td id="S4.T2.1.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">63.85</td>
<td id="S4.T2.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">81.10</td>
<td id="S4.T2.1.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">43.63</td>
<td id="S4.T2.1.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">55.90</td>
</tr>
<tr id="S4.T2.1.1.5.3" class="ltx_tr">
<th id="S4.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">UpDn + category</th>
<td id="S4.T2.1.1.5.3.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">63.51</td>
<td id="S4.T2.1.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">81.62</td>
<td id="S4.T2.1.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">42.17</td>
<td id="S4.T2.1.1.5.3.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">55.38</td>
</tr>
<tr id="S4.T2.1.1.6.4" class="ltx_tr">
<th id="S4.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Ours w/o caption</th>
<td id="S4.T2.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T2.1.1.6.4.2.1" class="ltx_text ltx_font_bold">64.09</span></td>
<td id="S4.T2.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">81.82</td>
<td id="S4.T2.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">44.37</td>
<td id="S4.T2.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">55.74</td>
</tr>
<tr id="S4.T2.1.1.7.5" class="ltx_tr">
<th id="S4.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">Ours w/ caption</th>
<td id="S4.T2.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T2.1.1.7.5.2.1" class="ltx_text ltx_framed ltx_framed_underline">64.24</span></td>
<td id="S4.T2.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">82.36</td>
<td id="S4.T2.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">44.52</td>
<td id="S4.T2.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">56.02</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study to investigate the effect of each component: caption, and answer category. â€œOurs w/o captionâ€ indicates our final model in which only image and question pairs are needed at test time; while â€œOurs w/ captionâ€ represents the model using caption during evaluation. The result of our best model are in bold; while the best performance with captions as inputs during testing is underlined for comparison.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Qualitative Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We perform an ablation study to qualitatively analyse the effect of the components introduced in our work brought by the continuous (image caption) and discrete (answer category) latent variables, as shown in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.1 Quantitative Analysis â€£ 4 Experiments â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Effect of Captions</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The introduction of captions as a continuous latent variable improves the classification performance, with an additional modality as input to benefit the learning of multimodal representations.
According to the breakdown numbers in Â <a href="#S4.T2" title="Table 2 â€£ 4.1 Quantitative Analysis â€£ 4 Experiments â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
the improvements brought by the latent variables of captions and answer categories are 0.70 and 0.36 respectively for All questions altogether.
The combined strategy reaches 0.94 which indicates that the benefits from the two latent variables are complementary.
Note that neither the captions nor the answer categories is available during testing;
we only make use of these modalities during training.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.2" class="ltx_p">To further investigate potential benefit of the captions, we design an experiment that feed in ground truth captions via variational distribution for caption representations instead of inferring them from question and answer pairs (<em id="S4.SS2.SSS1.p2.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS2.SSS1.p2.2.2" class="ltx_text"></span> use <math id="S4.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="q_{\phi}(z|c)" display="inline"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml"><msub id="S4.SS2.SSS1.p2.1.m1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.1.1.3.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2.cmml">q</mi><mi id="S4.SS2.SSS1.p2.1.m1.1.1.3.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p2.1.m1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS2.SSS1.p2.1.m1.1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><apply id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1"><times id="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2"></times><apply id="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2">ğ‘</ci><ci id="S4.SS2.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3">italic-Ï•</ci></apply><apply id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.2">ğ‘§</ci><ci id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">q_{\phi}(z|c)</annotation></semantics></math> to replace <math id="S4.SS2.SSS1.p2.2.m2.3" class="ltx_Math" alttext="p_{\theta}(z|v,q)" display="inline"><semantics id="S4.SS2.SSS1.p2.2.m2.3a"><mrow id="S4.SS2.SSS1.p2.2.m2.3.3" xref="S4.SS2.SSS1.p2.2.m2.3.3.cmml"><msub id="S4.SS2.SSS1.p2.2.m2.3.3.3" xref="S4.SS2.SSS1.p2.2.m2.3.3.3.cmml"><mi id="S4.SS2.SSS1.p2.2.m2.3.3.3.2" xref="S4.SS2.SSS1.p2.2.m2.3.3.3.2.cmml">p</mi><mi id="S4.SS2.SSS1.p2.2.m2.3.3.3.3" xref="S4.SS2.SSS1.p2.2.m2.3.3.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p2.2.m2.3.3.2" xref="S4.SS2.SSS1.p2.2.m2.3.3.2.cmml">â€‹</mo><mrow id="S4.SS2.SSS1.p2.2.m2.3.3.1.1" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.2" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.cmml"><mi id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.2" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.2.cmml">z</mi><mo fence="false" id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.1" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.3.2" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.3.1.cmml"><mi id="S4.SS2.SSS1.p2.2.m2.1.1" xref="S4.SS2.SSS1.p2.2.m2.1.1.cmml">v</mi><mo id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.3.2.1" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.3.1.cmml">,</mo><mi id="S4.SS2.SSS1.p2.2.m2.2.2" xref="S4.SS2.SSS1.p2.2.m2.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.3" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.2.m2.3b"><apply id="S4.SS2.SSS1.p2.2.m2.3.3.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3"><times id="S4.SS2.SSS1.p2.2.m2.3.3.2.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.2"></times><apply id="S4.SS2.SSS1.p2.2.m2.3.3.3.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.2.m2.3.3.3.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p2.2.m2.3.3.3.2.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.3.2">ğ‘</ci><ci id="S4.SS2.SSS1.p2.2.m2.3.3.3.3.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.3.3">ğœƒ</ci></apply><apply id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.1">conditional</csymbol><ci id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.2">ğ‘§</ci><list id="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.3.3.1.1.1.3.2"><ci id="S4.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1">ğ‘£</ci><ci id="S4.SS2.SSS1.p2.2.m2.2.2.cmml" xref="S4.SS2.SSS1.p2.2.m2.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.2.m2.3c">p_{\theta}(z|v,q)</annotation></semantics></math>).
We test this out in the validation dataset and obtain 64.24 (â€˜Ours w/ captionâ€™) compared to 64.09 (â€˜Ours w/o captionâ€™).
It shows that having explicit captions as input gives slightly better performance.
However, the captions in these experiments are ground truth, which means that if we were to use instead automatically generated captions from an image captioning pipeline, the numbers might drop due to the possible captioning errors.
Primarily, our proposed model (â€˜Ours w/o captionâ€™) achieves the performance on par with with the model with ground truth captions, which demonstrates the effectiveness of the strategy that incorporates extra modality by latent variables.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2101.06399/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="264" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of our latent variable model outperforming the baseline UpDn model from the introduction of answer category as a discrete latent variable. The answer predicted by UpDn is highlighted in red and the answer from our model is in blue. We also show other sample answer candidates within each category.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Effect of Answer Category</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">As it can be observed from TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.1 Quantitative Analysis â€£ 4 Experiments â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, after introducing answer category as an additional discrete latent variable, our proposed model can also be improved over the UpDn baseline, where the largest improvement can be observed for the â€œYes/Noâ€ type.
For the question types â€œNumâ€ and â€œOtherâ€, the results of <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter">UpDn+category</span> are lower than the baseline.
This may be due to the multiple answer candidates under the two categories.
For example, although the category classier can accurately predict the answer category (<em id="S4.SS2.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.SSS2.p1.1.3" class="ltx_text"></span>, â€œcountâ€, â€œcolorâ€, <em id="S4.SS2.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">etc</em>.<span id="S4.SS2.SSS2.p1.1.5" class="ltx_text"></span>), it can be still difficult to distinguish among the answers - {â€œ9â€, â€œ20â€, â€¦, â€œmanyâ€ for â€œcountâ€; â€œblackâ€, â€œbrownâ€, â€¦, â€œblack and whiteâ€ for â€œcolorâ€}.
We highlight that the contribution of answer categories as a discrete latent variable is to introduce an inductive bias which helps predict the correct answer categories and answers given a specific image and question.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">In order to further elaborate the effectiveness of answer categories, we extract examples where our model predicted the correct answers while the UpDn baseline failed to do so, as shown in FigureÂ <a href="#S4.F2" title="Figure 2 â€£ 4.2.1 Effect of Captions â€£ 4.2 Qualitative Analysis â€£ 4 Experiments â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
For the two cases in the top row, both models predict answers under the same and correct answer categories, hence the answer space is similar; however, our latent variable model can effectively distinguish and learn the difference among the answers which fall within the same category.
The bottom row of FigureÂ <a href="#S4.F2" title="Figure 2 â€£ 4.2.1 Effect of Captions â€£ 4.2 Qualitative Analysis â€£ 4 Experiments â€£ Latent Variable Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows two cases where the two models predict answers in different answer categories, and therefore they are also very different in meaning.
Our model not only outputs the highest probability for the correct answer category, but also makes the correct final prediction.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose to tackle VQA under the framework of latent variable models, employing captions and answer categories as the continuous and the discrete latent variables respectively to constrain the original image-question distribution while omitting the extra information during the test phase.
Our experimental results and qualitative analysis show the effectiveness of the latent variables in boosting answering performance at test time when only image and question pairs are available.
This framework could be easily generalised to incorporate other types of information or modalities to enhance VQA and other tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 6077â€“6086, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C.Â Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Counterfactual samples synthesizing for robust visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yen-Chun Chen, Linjie Li, Licheng Yu, AhmedÂ El Kholy, Faisal Ahmed, Zhe Gan, Yu
Cheng, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Uniter: Universal image-text representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Peng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Multi-modality latent interaction network for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Mutant: A training paradigm for out-of-distribution generalization in
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.08566</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 6904â€“6913, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Vahid Kazemi and Ali Elqursh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Show, ask, attend, and answer: A strong baseline for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1704.03162</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Bilinear attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing
Systems 31</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1564â€“1574. Curran Associates, Inc., 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Michael Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Information maximizing visual question generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Oscar: Object-semantics aligned pre-training for vision-language
tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV 2020</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 740â€“755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Gao Peng, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven Hoi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Dynamic fusion with intra-and inter-modality attention flow for
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.05252</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
KevinÂ J. Shih, Saurabh Singh, and Derek Hoiem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Where to look: Focus regions for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Vl-bert: Pre-training of generic visual-linguistic representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">LXMERT: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 5100â€“5111, Hong Kong,
China, Nov. 2019. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Jialin Wu, Zeyuan Hu, and Raymond Mooney.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Generating question relevant captions to aid visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 3585â€“3594, Florence, Italy, July 2019.
Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Deep modular co-attention networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 6281â€“6290, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2101.06398" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2101.06399" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2101.06399">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2101.06399" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2101.06400" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 14:41:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
