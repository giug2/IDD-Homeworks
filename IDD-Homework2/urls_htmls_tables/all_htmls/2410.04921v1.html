<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Music-triggered fashion design: from songs to the metaverse</title>
<!--Generated on Thu Sep 26 12:46:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04921v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S1" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S2" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Bridging music and fashion design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S3" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Aesthetics, common sense and ambiguity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S4" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S5" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>How this work was conceived?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S6" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Code availability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S7" title="In Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknoledgments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Music-triggered fashion design: from songs to the metaverse</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martina Delgado, Marta Llopart, Eva Sarabia, 
<br class="ltx_break"/>Sandra Taboada, Pol Vierge, Fernando Vilariño, Joan Moya Kohler,
<br class="ltx_break"/>Julieta Grimberg Golijov, Matías Bilkis 
<br class="ltx_break"/>
<br class="ltx_break"/>Computer Vision Center, Barcelona, Spain. 
<br class="ltx_break"/>Universitat Autònoma de Barcelona, Barcelona, Spain
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The advent of increasingly-growing virtual realities poses unprecedented opportunities and challenges to different societies. Artistic collectives are not an exception, and we here aim to put special attention into musicians. Compositions, lyrics and even show-advertisements are constituents of a message that artists transmit about their reality. As such, artistic creations are ultimately linked to feelings and emotions, with aesthetics playing a crucial role when it comes to transmit artist’s intentions. In this context, we here analyze how virtual realities can help to broaden the opportunities for musicians to bridge with their audiences, by devising a dynamical fashion-design recommendation system inspired by sound stimulus. We present our first steps towards re-defining musical experiences in the metaverse, opening up alternative opportunities for artists to connect both with real and virtual (<span class="ltx_text ltx_font_italic" id="id1.id1.1">e.g.</span> machine-learning agents operating in the metaverse) in potentially broader ways.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Music is uniquely experienced by individuals, but it is often unified by the artist’s vision. This vision reflects an artistic understanding of reality, which is not only transmitted by the musical piece but also by other means, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">e.g.</span> visual representations such as album covers, scenographies or even dress designs for live-performances.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">More generally speaking, aesthetics plays a crucial role in conveying the mood and emotions that artists seek to transmit with a creation, <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">e.g.</span> a musical piece. While sound stimulus on its own strongly reverberate with the hearer (<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">e.g.</span> think on different moments in song), message intention can clearly be complemented by using available visual resources (<span class="ltx_text ltx_font_italic" id="S1.p2.1.3">e.g.</span> fireworks or smoke in a live performance).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">On the other hand, virtual realities are increasingly gaining terrain in our daily lives, and art is not excepted to this fact. In turn, artists are beginning to utilise tools such as the metaverse for concerts. Virtual art expositions and even fashions shows are only more examples of physical events that are shifting towards virtual scenarios. It is then of utmost importance to develop novel tools to get profit of opportunities offered by such venues nature, as well to address different challenges and social implications of such a paradigm shift.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Here, we provide first steps towards analyzing the role that virtual realities can play when it comes to artist’s aesthetics, with the goal of conveying a message towards the audience.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Events happening in the virtual realm clearly lack the particular experience of live performances, <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">e.g.</span> the experience of a musical performance is highly richer in real life, rather than broadcasted in the metaverse. shiftThis raises the need of bringing different parties closer, <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">e.g.</span> performers and audience, in such virtual experiences. Firstly, exploiting the possibilities offered by virtual realities might make (virtual) artistic performances richer and more enjoyable. Secondly, enhancing the connection between the performer with the audience is crucial to guarantee the transmittance of artist’s message. In this respect, the aesthetic scope can be thought to broaden, as novel technologies such as generative models can readily be implemented in order to compensate the lack of physicality.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">An overlooked aesthetic aspect in the virtual reality is that of cloth design, and is strongly linked to live performances. Most approaches rely on personalizing cloth shapes, with the base outfit being static. However, music dynamics can easily be linked to the outfit of either performer, attendant or both in a virtual reality. While music-triggered cloth changes are practically impossible in real life, getting profit of specific climax in a song can serve as a potential new bridge between the musician and her audience.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Here we introduce first steps towards a music-triggered recommendation system for fashion design, as captured by Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_tag">1</span></a>. Such is intended to be deployed at the metaverse, and to be used in real-time by interpreting current moods in both the audience and the musical piece being played; moreover our method is conceived as a pattern-retrieving device, whose outcome is to be shown in avatar’s dressing codes. Such a method is indented to automatize cohesively a temporal aesthetic conception derived from the song being played, and to perfectly encapsulate the vibe of the song as its creator intended. Interestingly enough, whether this or similar mechanisms can succeed in capturing the essence of artist’s feelings encoded in the song, or fail into (a potentially higher) ambiguity, is a matter of debate. In turn, we show evidence that even basic approaches linking colours to musical genres fail to encapsulate targeted aesthetic, being ambiguity at the core of the question. Whether state of the art machine learning methods deployerd in virtual realities can serve as a stepping stone towards complementing emotion communication by music, or this is merely an entertainment application inducing artificial and unnecessary ambiguities, is certainly an interesting debate, which we partially address in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S3" title="3 Aesthetics, common sense and ambiguity ‣ Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S1.F1.g1" src="extracted/5881851/catalogo.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Music-triggered recommendation system<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1">. We show several designs obtained by our recommendation system, each associated to a different songs sample. Here, song features are extracted and further processed by our model, that imprints visual patterns into an avatar’s metaverse dressing code. In this example, we show T-Shirt patterns of different songs based on: Moon Song by Phoebe Bridgers, 7 rings by Ariana Grande, Bad Guy by Billie Eilish and August by Taylor Swift.</span></span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Bridging music and fashion design</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In the following we describe our music-triggered recommendation system which retrieves cloth design fashion colour patterns. As such, it is indented as a method to capture the essence of artwork’s aesthetics, and we here take first steps towards automatizing this task. We remark that while more sophisticated versions of the different components are to be deployed, the value of this contribution resides in both framing and analyzing the role of extended aesthetics in virtual realities to convey artist’s messages.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Our implementation incorporates a palette generation algorithm based on image web scraping with 3D model integration for pattern design, and visualized on a standard T-shirt. A sketch representation of the overall workflow is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S2.F2" title="Figure 2 ‣ 2 Bridging music and fashion design ‣ Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_tag">2</span></a>, and in the following we explain each of the modules in detail.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.2"><span class="ltx_text ltx_font_italic" id="S2.p3.2.1">Colour palette and database generation</span>. A subset of 9K popular songs have been selected from the Spotify API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#bib.bib1" title="">1</a>]</cite>, in order to create a database with song data and colours sets. The image web scraping algorithm consists of a Google search with a defined <span class="ltx_text ltx_font_italic" id="S2.p3.2.2">song + artist</span> structure, and thereby storing the first five appearing images, understood as the most representative ones. The popularity threshold for song selection has been set to 50 (this is an internal score of Spotify API) to ensure that the web scraping yields representative pictures. We remark that this raises up a big concern of our method, which leaves non-mainstream artists aside due to a lack Google images visibility, an issue that we aim to adress in the near future. Following, pixel extraction and <math alttext="k-" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mrow id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mi id="S2.p3.1.m1.1.1.2" xref="S2.p3.1.m1.1.1.2.cmml">k</mi><mo id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><csymbol cd="latexml" id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1">limit-from</csymbol><ci id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.2">𝑘</ci><minus id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">k-</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_k -</annotation></semantics></math>mean clustering have been applied to retrieve a 10 colour palette (<math alttext="k=10" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mi id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">k</mi><mo id="S2.p3.2.m2.1.1.1" xref="S2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><eq id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1"></eq><ci id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">𝑘</ci><cn id="S2.p3.2.m2.1.1.3.cmml" type="integer" xref="S2.p3.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">k=10</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">italic_k = 10</annotation></semantics></math>) standing for a (simplified representation of) song’s aesthetic, based on cluster centres. The generated palettes are thus linked to each song and added to the database.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_italic" id="S2.p4.1.1">Cloth-design pattern generation</span>. The generative design process for creating patterns is Rhino <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#bib.bib2" title="">2</a>]</cite>, with strong usage of its 3D modelling-tool plugin, Grasshopper. These parametric design tools are crucial in transforming features that are captured by the recommendation system into tangible patterns. As such, they stand out as powerful platforms for computational modelling, offering designers, architects and artists a versatile toolkit to explore complex geometries and optimise designs.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The workflow of our design generation begins with a search on the previously-created songs database (web-scrapped database) is conducted based on the title of the song. If found, we extract the colour palette and randomly decide the number of colours appearing in the palette associated to that songs. Hence,the extracted RGB values are then passed as colour parameters to the clothing mesh to visualise the final cloth design (T-shirt in the examples shown here).</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="209" id="S2.F2.g1" src="extracted/5881851/grasshopper.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F2.5.2" style="font-size:90%;">Recommendation system workflow with Rhinoceros generation of Super Freaky Girl<span class="ltx_text ltx_font_medium" id="S2.F2.5.2.1">. Grasshoppers interface is shown on the left, linking the colour palette with the song. On the right, the first image appearing on the google search <span class="ltx_text ltx_font_italic" id="S2.F2.5.2.1.1">Super Freaky Girl Nicki Minaj</span> is shown, along with the generated colour-palette and the T-shirt example.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Having detailed our initial implementation of a generative colour-based pattern recommendation out of musical stimulus, we now turn to discuss potential implications in the virtual societies inhabiting the metaverse ecosystem.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Aesthetics, common sense and ambiguity</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Let us now discuss social implications of a potentially artist-to-audience communication enhancer device, such as the one introduced above.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">While we have restricted to a colour-palette being decided by pre-defined song features, we readily note that alternative song characteristics are also to be considered in the future, such as tempo, musical instruments timbre, lyrics among many others. With this, we aim to highlight that the constituents of aesthetics beneath a musical piece are perhaps not even a countable set, since the relationship between emotions, reflections and artworks are often highly complex and difficult to capture in a conceptual way.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">While we have restricted to a colour-palette being decided by pre-defined song features, we readily note that alternative song characteristics are also to be considered in the future, such as tempo, musical instruments timbre, lyrics among many others. With this, we aim to highlight that the constituents of aesthetics beneath a musical piece are perhaps not even a countable set, since the relationship between emotions, reflections and artworks are often highly complex and difficult to capture in a conceptual way.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">In this sense, capturing artwork aesthetics with a colour palette is clearly not a conclusive task. For example, while songs from the same album tend to share similar aesthetics, they do so with different feature weights. This induces a notion of ambiguity in how our model captures the aesthetics of an artwork. For instance, our method seems to handle nuances between songs coming from the same album, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S3.F3" title="Figure 3 ‣ 3 Aesthetics, common sense and ambiguity ‣ Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_tag">3</span></a>. Here, we show a comparison of patterns obtained by different songs of Planet Her (an album from Doja Cat’s), shedding light into the fact that capturing artwork aesthetics is a highly subjective task.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S3.F3.g1" src="extracted/5881851/planether3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F3.4.2" style="font-size:90%;">Generated T-shirts from songs from the same album. <span class="ltx_text ltx_font_medium" id="S3.F3.4.2.1">Comparison between Need to Know by Doja Cat (left) and You Right by Doja Cat featuring The Weeknd (right), both from Doja Cat’s third studio album Planet Her (centre).</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Such an ambiguous notion on aesthetic quantification has a counter-side on how biases are constructed, both by virtual and real agents. In turn, a key issue that we encountered is the inability of our model to capture the underlying patterns between song features and aesthetic colour palettes. This failure highlights the strong influence of the artist’s vision on its music and suggests that preconceived ideas about how a song or genre should look might not be as reality-based as commonly believed.
Moreover we remark that our recommendation system is trained at the level of songs, and not albums. While our implementation is arguably simple, it is not obvious that large-scale learning models trained on massive web-scrapped artworks data would succeed in this tasks; we note that we failed in training deep generative architectures under this premise, precisely because the webscrapped data was both ambiguous and scarse.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">As a matter of fact, genres are often associated with certain colours, such as pop music being linked with bright colours like pink. However, these preconceptions can easily be challenged. The model’s struggle to consistently link song features with specific colours indicates that genre-colour associations might not be as universally valid. This can be evidened, for instance, by performing cluster analysis on the data (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#S3.F4" title="Figure 4 ‣ 3 Aesthetics, common sense and ambiguity ‣ Music-triggered fashion design: from songs to the metaverse"><span class="ltx_text ltx_ref_tag">4</span></a>), which shows that colour associations varied significantly even within the same genre. This observation indicates that the unique synesthesia each artist experiences with music makes it difficult, if not impossible, for a model to extrapolate consistent patterns. This fact points to a bias in the collective perception of colour association in music, which is ultimately linked to cultural and collective constructions. Whether virtual societies can build different pre-conceptions of such associations, and via which new tools or technologies, is an intriguing line of research.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">On the other hand, while we have presented the recommendation device as an artist-to-audience communication channel, we might easily find different use cases. In turn, an interesting research direction is that of building stronger bonds beneath virtual communities, and whether a collective generation of fashion designs might be feasible by deploying future versions of the here-presented recommendation system. This is particularly important in the context of live performance and real-time audience-to-artist feedback, where the artist is influenced by its audience and vice versa; while this phenomena often happens in the physical world, a plethora of unexplored opportunities does open in virtual realities. In this line, it is necessary to understand whether a common sense can be constructed in a virtual society, and whether machine-learning technologies can help to elucidate it (for instance, by extracting relevant common features about different individuals). Finally, the question of whether a common sense about understanding and feeling an artwork beneath an hybrid human-machine society inhabiting the metaverse can be framed or not does also raise up.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="350" id="S3.F4.sf1.g1" src="extracted/5881851/PCA1.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S3.F4.sf2.g1" src="extracted/5881851/PCA2.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F4.4.2" style="font-size:90%;">Clusters of songs based on colour palettes.<span class="ltx_text ltx_font_medium" id="S3.F4.4.2.1"> Comparison between Butterflies by Kacey Musgraves (a) and Psychosocial by Slipknot (b).</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1">We aim to finish this contribution with some open questions. By introducing this music-triggered fashion recommendation system, some aspects regarding human-machine creativity are ought. Among them, questioning the nature of live music and the authenticity of concerts is almost unavoidable. Key questions include:</p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1"><span class="ltx_text ltx_font_italic" id="S3.p9.1.1">Redefining Live Music</span>. What constitutes live music in the context of increasing technological integration? Our device challenges traditional notions of live performances by potentially blending virtual elements with physical presence, prompting a reevaluation of what do audiences value in live-music performances.</p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.1"><span class="ltx_text ltx_font_italic" id="S3.p10.1.1">Authenticity of Virtual Concerts</span>. Can virtual concerts be considered less authentic when they lack the proximity and physical presence of the artist? This question delves into the essence of concert experiences, questioning whether virtual interactions can replicate the emotional and sensory engagement of being physically present at a live performance.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">State of the art regarding music and fashion AI-powered devices is scarce, we refer nevertheless to a recent prospective review about Fashion intelligence in the Metaverse in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#bib.bib3" title="">3</a>]</cite>. Moreover, Ref. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#bib.bib4" title="">4</a>]</cite> provides a comprehensive approach to the live music performance from IoT. Notwithstanding, a project sharing motivation with ours is Emotional Clothing, a clothing collection by Polish designer Weglinska composed of colour-changing clothing based on body temperature, stress levels, movements, and even motions. The work shows a garment that changed colour with every laugh, conversation, scream or whisper, or each time they made contact with skin. Departing from this, our approach focuses on colour changes on clothing depending on music, with future perspective on a full outfit generation device.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>How this work was conceived?</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This project is a continuation of a final project in the Social Innovation course of the AI degree at Autonomous University of Barcelona, held form January-July 2024. Such a course, whose modality is innovative on its own, consists on splitting the students into small working groups, each developing a project. In this case, the project was related to meta-verse technologies, and the goal of the course is to evaluate the social impact of the prototypes developed during it. For more information about this education modality, we refer to Ref. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04921v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Code availability</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our implementation of the our music-triggered recommendation system for cloth patterns is made publicly available in the GitHub repository
<a class="ltx_ref ltx_href" href="github.com/martiinsssssss/metaverse-clothing" title=""><span class="ltx_ERROR undefined" id="S6.p1.1.1.1">\faGithub</span> martiinsssssss/metaverse-clothing</a>. Note that a demo video can be found there as well.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknoledgments</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work was done partially under the support of the project Cátedra UAB-Cruilla, funded by the Spanish Governement (TSI-100929-2023-2 (Prov)). All our acknowledgement to the experts participating in the interviews and mentoring during the SI course at UAB, among them Carla Miernau, Andrea Ross, Abril Riera, Aloma Martí, Laia Torras.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
“Web API — Spotify for Developers — developer.spotify.com.” </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.spotify.com/documentation/web-api" style="font-size:90%;" title="">https://developer.spotify.com/documentation/web-api</a><span class="ltx_text" id="bib.bib1.2.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.3.1" style="font-size:90%;">[Accessed 16-08-2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
R. McNeel </span><span class="ltx_text ltx_font_italic" id="bib.bib2.2.2" style="font-size:90%;">et al.</span><span class="ltx_text" id="bib.bib2.3.3" style="font-size:90%;">, “Rhinoceros 3d, version 6.0,” </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.4" style="font-size:90%;">Robert McNeel &amp; Associates, Seattle, WA</span><span class="ltx_text" id="bib.bib2.5.5" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
X. Mu, H. Zhang, J. Shi, J. Hou, J. Ma, and Y. Yang, “Fashion intelligence in the metaverse: promise and future prospects,” </span><span class="ltx_text ltx_font_italic" id="bib.bib3.2.2" style="font-size:90%;">Artificial Intelligence Review</span><span class="ltx_text" id="bib.bib3.3.3" style="font-size:90%;">, vol. 57, p. 67, Feb 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
J. Breese, M. Fox, and G. Vaidyanathan, “Live music performances and the internet of things,” 01 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
M. Bilkis, J. Moyà-Köhler, and F. Vilariño, “Challenge-device-synthesis: A multi-disciplinary approach for the development of social innovation competences for students of artificial intelligence,” in </span><span class="ltx_text ltx_font_italic" id="bib.bib5.2.2" style="font-size:90%;">EDULEARN24 Proceedings</span><span class="ltx_text" id="bib.bib5.3.3" style="font-size:90%;">, 16th International Conference on Education and New Learning Technologies, IATED, 1-3 July, 2024 2024].
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 26 12:46:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
