<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.14793] Private Federated Learning in Gboard</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Private Federated Learning in Gboard">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Private Federated Learning in Gboard">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.14793">

<!--Generated on Wed Feb 28 22:51:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on June, 2023.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Private Federated Learning in Gboard</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuanbo Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Daniel Ramage
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zheng Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Yanxiang Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shumin Zhai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Peter Kairouz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google
</span></span></span>
</div>
<div class="ltx_dates">(June, 2023)</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Gboard relies on machine learning (ML) innovations to improve users‚Äô input experience with the product, which benefits from statistics and models derived from many users‚Äô typing data. To meet these needs, Gboard invests in privacy technologies that allow the data to be processed locally on device, to be aggregated as early as possible, and to have strong anonymization and differential privacy where possible.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Gboard has been one of the earliest adopters of federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite> at Google, which enabled mobile devices to collaboratively train models while keeping raw training data such as typing content on each user‚Äôs device. In the past few years, various language models and correction models have been built with FL to optimize Gboard input efficiency and quality by providing more accurate suggestions, auto corrections, smart compose prediction and next word predictions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Gboard has deployed FL models using a variety of privacy enhancing technologies with an eye toward a fully privacy preserving ML technology stack. This white paper describes recent advances that Gboard has made in bringing federated learning, secure aggregation, and differential privacy techniques to Gboard‚Äôs users through a multi-year effort combining research and product integrations, and looking forward, we observe how technologies such as trusted execution environments (TEEs) may play a larger role in supporting features powered by next-generation generative AI models.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Privacy Principles and Practices</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Privacy is multi-faceted. Following the broader discussion of privacy for FL introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>, we focus on several specific aspects of privacy:</p>
</div>
<div id="S2.p2" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Transparency and user control:</span> users can be aware of what data is used, what purpose it is used for and how it is processed, and have full control on whether to enable the collection and use of their data.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Data minimization:</span> data is only collected focusing on specific computation needs, aggregated as early as possible and discarded as soon as possible, with access limited at all data processing stages.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Data anonymization:</span> the final released output of the computation does not reveal anything unique to an individual.</p>
</div>
</li>
</ol>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In Gboard, whether FL and other types of machine learning are enabled is in users‚Äô control, and can be easily configured through Gboard settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Data minimization and data anonymization are achieved by a combination of technologies and policies at different data processing stages. As illustrated in the Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Privacy Principles and Practices ‚Ä£ Private Federated Learning in Gboard" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, federated learning makes progress in discrete training rounds. In each round, a set of currently available devices (from hundreds to thousands) are sampled from the training population, download a base model checkpoint (an initial model or the output of the previous round from the FL server), run stochastic gradient descent (SGD) for optimization on their own data locally, and upload model updates to server for immediate aggregation. The server aggregates model updates by averaging them, further processes the aggregate if needed, and produces a new checkpoint as an intermediate output. After sufficient rounds of updates, a final model checkpoint may be further processed (e.g. compressed or quantized), evaluated for quality, and rolled out for use in inference for Gboard‚Äôs users.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2306.14793/assets/system_overview.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Gboard FL system overview</figcaption>
</figure>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Google‚Äôs production FL service is a multi-tenant service operated by a service team separate from Gboard. Google‚Äôs FL service does not log or grant access to unaggregated model updates. Model updates are immediately aggregated within the isolated service‚Äôs boundaries as enforced by Access Control Lists (ACLs), Multi-Party Authorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>, production job role accounts, code reviews, etc. The isolation boundary is hardened with complementary technologies including distributed differential privacy (DDP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>, <a href="#bib.bibx7" title="" class="ltx_ref">7</a>, <a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite> and Secure Aggregation (SecAgg) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite> when so configured. (Gboard‚Äôs use of DDP and SecAgg is discussed below.) With these stronger protections and under a correctly implemented key exchange and aggregation protocol (i.e. an honest-but-curious server), even the aggregation service itself has no access to unaggregated updates. For data transmission security, Gboard makes gRPC or HTTPS requests to the FL system from within the Federated Compute Client Open Source (OSS) code <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>. Network requests include an API key for endpoint authorization not specific to a device and includes an attestation token to protect against botnets via an internal service similar to the Play Integrity API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>. The attestation token used by FL contains no device identifiers, and is not stored, logged, or analyzed by the FL service, which uses the attestation token only to verify client integrity with the attestation backend. The API key, attestation token, and connection metadata including source IP, are all discarded by FL service before aggregation. The service team makes use of telemetry logs (without persistent device identifiers) in order to properly operate the service.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Privacy Principles and Practices ‚Ä£ Private Federated Learning in Gboard" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> also illustrates the access points in the system. ML engineers subject to use-case specific ACLs may access model checkpoints after aggregation of each training round. Final models may be accessed by many more parties after public deployment. We therefore place the strongest data anonymization technologies such as formal user-level differential privacy guarantees to models going back to user devices. Gboard uses DP-FTRL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite> to bound how much the model parameters can memorize content unique to a particular individual. Case-by-case privacy reviews are required for all new uses of FL in Gboard, with code review used to guard against mistakes and to enforce required minimum aggregation sizes, DP and/or SecAgg requirements if appropriate, etc.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Towards Differentially Private FL for Gboard Language Models</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">Language models (LM) play an essential role in Gboard suggestion, correction and prediction functions. Ideally, LMs trained on user data should capture common patterns typed by many users, but should not encode content unique to a particular individual - i.e. LM parameters should be computed with appropriate <span id="S3.p1.3.1" class="ltx_text ltx_font_italic">data anonymization</span>. The relevant notion of privacy would be differential privacy, i.e. that the model parameters learned during training are not statistically distinct given a small change to the training data. This can be quantified as a ‚Äùsmall‚Äù <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">œÅ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ùúå</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\rho</annotation></semantics></math> value of zero-Concentrated-Differential-Privacy (zCDP) or <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Œµ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ùúÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\varepsilon</annotation></semantics></math> of <math id="S3.p1.3.m3.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S3.p1.3.m3.2a"><mrow id="S3.p1.3.m3.2.3.2" xref="S3.p1.3.m3.2.3.1.cmml"><mo stretchy="false" id="S3.p1.3.m3.2.3.2.1" xref="S3.p1.3.m3.2.3.1.cmml">(</mo><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">Œµ</mi><mo id="S3.p1.3.m3.2.3.2.2" xref="S3.p1.3.m3.2.3.1.cmml">,</mo><mi id="S3.p1.3.m3.2.2" xref="S3.p1.3.m3.2.2.cmml">Œ¥</mi><mo stretchy="false" id="S3.p1.3.m3.2.3.2.3" xref="S3.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.2b"><interval closure="open" id="S3.p1.3.m3.2.3.1.cmml" xref="S3.p1.3.m3.2.3.2"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ùúÄ</ci><ci id="S3.p1.3.m3.2.2.cmml" xref="S3.p1.3.m3.2.2">ùõø</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.2c">(\varepsilon,\delta)</annotation></semantics></math>-Differential Privacy (DP). A common use of DP is <span id="S3.p1.3.2" class="ltx_text ltx_font_italic">example-level</span>, where adding or removing a single training example changes model parameters in a provably minimal way. However, as users can contribute multiple examples to the training dataset, example-level DP may not be strong enough to ensure the users‚Äô data isn‚Äôt memorized. Instead, we choose to use <span id="S3.p1.3.3" class="ltx_text ltx_font_italic">user-level</span> DP, i.e. that the model parameters learned during training are not statistically distinct even if we add/remove all the training examples from any one particular Gboard user.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In Feb 2022 we deployed the DP-FTRL (Differentially Private Follow-the-Regularized-Leader) algorithm trained LM for next word prediction for Spanish language Gboard users, which for the first time offered a formal DP guarantee in production (with zCDP <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\rho=0.81" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">œÅ</mi><mo id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">0.81</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><eq id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></eq><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ùúå</ci><cn type="float" id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">0.81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\rho=0.81</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>. Since then we have been working on scaling DP-FTRL to learn LMs with larger parameter sizes for different features and on different user populations of various sizes. As of Feb 2023, DP-FTRL has been widely adopted in LM training to enable typing features, including neural network based next word prediction, smart compose and decoding suggestions on-the-fly rescore for British English, French, German, India English, Italian, Norwegian, Portuguese, Russian, Spanish and US English language Gboard users. In parallel, we are also refreshing existing FL LMs for all other languages.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.2" class="ltx_p">We continuously enhance the performance of DP algorithms, practices and settings over time and look for better DP guarantees without utility loss (even when comparing with existing high quality models previously trained without DP). Empirically, the <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">œÅ</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ùúå</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\rho</annotation></semantics></math> value is determined by the level of noise added, bounded device participation through sampling without replacement policy, and total number of training rounds. As a result, it‚Äôs a challenge to maintain utility (model quality) while keeping strong privacy guarantees, especially when training population size is small. We have employed model pre-training on public C4 corpus based on Common Crawl dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite> combined with training configuration tuning, and achieved almost neutral utility change measured by <span id="S3.p3.2.1" class="ltx_text ltx_font_italic">prediction picked ratio</span>: the ratio of picked prediction candidates among the shown NWP predictions, and <span id="S3.p3.2.2" class="ltx_text ltx_font_italic">prediction accuracy</span>: the ratio of prediction candidates matching the finally committed words among the NWP model predictions, for a <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="\rho=1.31" display="inline"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">œÅ</mi><mo id="S3.p3.2.m2.1.1.1" xref="S3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">1.31</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><eq id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1"></eq><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">ùúå</ci><cn type="float" id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">1.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">\rho=1.31</annotation></semantics></math> zCDP model (US English) compared to the previous model trained without DP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>. Other DP-FTRL trained LMs have been adopting this optimization practice.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Today all new FL tasks training LMs for Gboard production features require DP, with DP-FTRL recommended. We are on track to upgrade all existing FL LMs that have previously been launched without DP to be trained with DP in 2023.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Towards Secure Aggregation and Distributed DP</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">While DP-FTRL provides data anonymization after gradient aggregation on the centralized server, data minimization can be strengthened by limiting what data the centralized server sees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>, achieved by technologies such as secure aggregation (SecAgg) and distributed differential privacy (DDP).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">SecAgg runs a cryptographic secure multi-party computation (MPC) protocol during FL, such that the server can compute the mean update, but cannot see updates of individual devices, which minimizes access to gradient updates of individual devices potentially carrying sensitive information. Since payloads are encrypted, SecAgg also protects network traffic between client and server. SecAgg gives protection guarantees under the honest-but-curious server threat model, that is, the server and a sufficiently large number of the participants behave honestly to execute the protocol, and the key exchange portion of the secure aggregation protocol is correctly implemented. Under that assumption, a server implementing the secure aggregation protocol cannot see the contents of any individual update.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">To further protect the server from getting sensitive information in the mean update, DDP extends SecAgg to ensure that model-updates are slightly perturbed before they are sent to the server using the SecAgg protocol, adding another layer of protection, and also leading to a more bandwidth and memory efficient version of SecAgg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.3" class="ltx_p">Gboard has been exploring adopting SecAgg/DDP with FL, and we have achieved promising results. Based on research and infrastructure advancements, such as subgraph SecAgg protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>, which largely reduced communication and computation overheads, Gboard has trained various models with SecAgg, ranging from smaller sized classification models such as key-by-key correction triggering models (<math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\sim 1k" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml"></mi><mo id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">‚àº</mo><mrow id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml"><mn id="S4.p4.1.m1.1.1.3.2" xref="S4.p4.1.m1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.p4.1.m1.1.1.3.1" xref="S4.p4.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.p4.1.m1.1.1.3.3" xref="S4.p4.1.m1.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">absent</csymbol><apply id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3"><times id="S4.p4.1.m1.1.1.3.1.cmml" xref="S4.p4.1.m1.1.1.3.1"></times><cn type="integer" id="S4.p4.1.m1.1.1.3.2.cmml" xref="S4.p4.1.m1.1.1.3.2">1</cn><ci id="S4.p4.1.m1.1.1.3.3.cmml" xref="S4.p4.1.m1.1.1.3.3">ùëò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\sim 1k</annotation></semantics></math> parameters) and auto correction triggering models (<math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\sim 1.3m" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mi id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml"></mi><mo id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.1.cmml">‚àº</mo><mrow id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml"><mn id="S4.p4.2.m2.1.1.3.2" xref="S4.p4.2.m2.1.1.3.2.cmml">1.3</mn><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.1.1.3.1" xref="S4.p4.2.m2.1.1.3.1.cmml">‚Äã</mo><mi id="S4.p4.2.m2.1.1.3.3" xref="S4.p4.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="latexml" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">absent</csymbol><apply id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3"><times id="S4.p4.2.m2.1.1.3.1.cmml" xref="S4.p4.2.m2.1.1.3.1"></times><cn type="float" id="S4.p4.2.m2.1.1.3.2.cmml" xref="S4.p4.2.m2.1.1.3.2">1.3</cn><ci id="S4.p4.2.m2.1.1.3.3.cmml" xref="S4.p4.2.m2.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\sim 1.3m</annotation></semantics></math> parameters), to larger sized language models (<math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\sim 3m" display="inline"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mi id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml"></mi><mo id="S4.p4.3.m3.1.1.1" xref="S4.p4.3.m3.1.1.1.cmml">‚àº</mo><mrow id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml"><mn id="S4.p4.3.m3.1.1.3.2" xref="S4.p4.3.m3.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.p4.3.m3.1.1.3.1" xref="S4.p4.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S4.p4.3.m3.1.1.3.3" xref="S4.p4.3.m3.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">absent</csymbol><apply id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3"><times id="S4.p4.3.m3.1.1.3.1.cmml" xref="S4.p4.3.m3.1.1.3.1"></times><cn type="integer" id="S4.p4.3.m3.1.1.3.2.cmml" xref="S4.p4.3.m3.1.1.3.2">3</cn><ci id="S4.p4.3.m3.1.1.3.3.cmml" xref="S4.p4.3.m3.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\sim 3m</annotation></semantics></math> parameters). The key-by-key correction triggering model has been launched in production for English users with neutral quality impact compared with the baseline model without SecAgg. For LM, we are exploring combining with DP-FTRL for production launch, which is continued in Section 5.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Towards Combining Central and Distributed DP</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">Central differential privacy with DP-FTRL and distributed differential privacy with SecAgg/DDP protect against different types of threat vectors at different stages of model training, processing and deployment flow. DP-FTRL provides formal guarantees that models that are aggregated on the server side and to be deployed publicly won‚Äôt memorize training data unique to a particular individual. SecAgg makes sure an honest-but-curious FL server cannot learn individual updates. DDP extends SecAgg to ensure that model-updates are slightly perturbed before they are sent to the server using the SecAgg protocol, adding another layer of protection even against a malicious server. Their combination would allow Gboard to have an effective way to provide more complete end-to-end privacy protection for data minimization and anonymization. Gboard prioritized DP-FTRL for anonymization with formal DP guarantee on content sensitive models such as LMs, given deployed models have a broader attack surface, and there have been stringent security enforcements on the isolated FL server such as ACLs, production job role accounts, code reviews, and so on. We have also been improving SecAgg/DDP settings for training efficiency and testing their combination with DP-FTRL. In June 2023, we launched the first ever LM trained with DP-FTRL + SecAgg/DDP for US English next word prediction task. The model achieved the same utility measured by <span id="S5.p1.2.1" class="ltx_text ltx_font_italic">prediction picked ratio</span> and <span id="S5.p1.2.2" class="ltx_text ltx_font_italic">prediction accuracy</span> compared the the previously launched model trained with DP-FTRL only, and offered a even better zCDP with <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\rho=0.25" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">œÅ</mi><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><eq id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></eq><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">ùúå</ci><cn type="float" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\rho=0.25</annotation></semantics></math> (compared to <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="\rho=1.31" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">œÅ</mi><mo id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">1.31</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><eq id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></eq><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">ùúå</ci><cn type="float" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">1.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\rho=1.31</annotation></semantics></math>) by improving practice on pre-training, sampling without replacement policy and so on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>. We aim to enable their combination for more Gboard typing FL neural models in 2023.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Possibility of External Verifiability of Privacy Claims</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Over time, we aim to make more of Gboard‚Äôs privacy, security, and correctness claims externally verifiable, to client devices and more, under a variety of threat models. As richer Trusted Execution Environment (TEE) and trusted runtime binaries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>, <a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite> become available, further possibilities emerge. For example, can SecAgg be hardened in an OSS trusted aggregator runtime, which is verifiably built to prove the claim of honestly running the protocol? It may also open opportunities to design new types of learning or analytics tasks that involve raw data transmission to the trusted aggregator running on TEE, with guarantee that data leaving the trusted boundary is securely aggregated and differentially private.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We have made various efforts to advance privacy protection practices in Gboard toward stronger privacy guarantees while learning useful signals to better serve our users. This includes DP-FTRL, which offers a formal central DP guarantee for data anonymization, and SecAgg/DDP to further harden the isolation boundary of data access by limiting what data the centralized server sees using cryptographic secure multi-party computation protocol and adding local noise. As they are separately used to learn different Gboard models over time and proven to achieve both good utility and privacy guarantee, we have been working on their combination to provide more complete end-to-end privacy protection for data minimization and anonymization. Recent development of TEE infrastructure and OSS trusted runtime binaries at Google enables the possibility of verifiable privacy claims, which is an open area for us to further explore. We believe these work streams lead towards maximizing values of ML on Gboard user experience, at the same time minimizing potential privacy costs for our users participating in FL training.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Kallista A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chlo√© Kiddon, Jakub Koneƒçn√Ω, Stefano Mazzocchi, H. McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage and Jason Roselander
</span>
<span class="ltx_bibblock">‚ÄúTowards Federated Learning at Scale: System Design‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx1.2.2" class="ltx_text ltx_font_bold">abs/1902.01046</span>, 2019
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="http://arxiv.org/abs/1902.01046" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1902.01046</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage and Fran√ßoise Beaufays
</span>
<span class="ltx_bibblock">‚ÄúApplied Federated Learning: Improving Google Keyboard Query Suggestions‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx2.2.2" class="ltx_text ltx_font_bold">abs/1812.02903</span>, 2018
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="http://arxiv.org/abs/1812.02903" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1812.02903</a>
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Kallista Bonawitz, Peter Kairouz, Brendan McMahan and Daniel Ramage
</span>
<span class="ltx_bibblock">‚ÄúFederated Learning and Privacy: Building Privacy-Preserving Systems for Machine Learning and Data Science on Decentralized Data‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Queue</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">19.5</span>
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2021, pp. 87‚Äì114
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3494834.3500240" title="" class="ltx_ref ltx_href">10.1145/3494834.3500240</a>
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://support.google.com/gboard/answer/12373137" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://support.google.com/gboard/answer/12373137</a>
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://en.wikipedia.org/wiki/Multi-party_authorization" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Multi-party_authorization</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Peter Kairouz, Ziyu Liu and Thomas Steinke
</span>
<span class="ltx_bibblock">‚ÄúThe Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">abs/2102.06387</span>, 2021
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="https://arxiv.org/abs/2102.06387" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2102.06387</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Naman Agarwal, Peter Kairouz and Ziyu Liu
</span>
<span class="ltx_bibblock">‚ÄúThe Skellam Mechanism for Differentially Private Federated Learning‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx7.2.2" class="ltx_text ltx_font_bold">abs/2110.04995</span>, 2021
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="https://arxiv.org/abs/2110.04995" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2110.04995</a>
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://www.tensorflow.org/federated/api_docs/python/tff/learning/ddp_secure_aggregator" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated/api_docs/python/tff/learning/ddp_secure_aggregator</a>
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal and Karn Seth
</span>
<span class="ltx_bibblock">‚ÄúPractical Secure Aggregation for Privacy-Preserving Machine Learning‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</em>, CCS ‚Äô17
</span>
<span class="ltx_bibblock">Dallas, Texas, USA: Association for Computing Machinery, 2017, pp. 1175‚Äì1191
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3133956.3133982" title="" class="ltx_ref ltx_href">10.1145/3133956.3133982</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://github.com/google/federated-compute/blob/main/fcp/client/http/http_federated_protocol.cc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/federated-compute/blob/main/fcp/client/http/http_federated_protocol.cc</a>
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://github.com/google/federated-compute/blob/main/fcp/client/grpc_federated_protocol.cc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/federated-compute/blob/main/fcp/client/grpc_federated_protocol.cc</a>
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://developer.android.com/google/play/integrity" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.android.com/google/play/integrity</a>
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta and Zheng Xu
</span>
<span class="ltx_bibblock">‚ÄúPractical and Private (Deep) Learning without Sampling or Shuffling‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx13.2.2" class="ltx_text ltx_font_bold">abs/2103.00039</span>, 2021
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="https://arxiv.org/abs/2103.00039" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2103.00039</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Brendan McMahan and Abhradeep Thakurta
</span>
<span class="ltx_bibblock">‚ÄúFederated Learning with Formal Differential Privacy Guarantees‚Äù <a target="_blank" href="https://ai.googleblog.com/2022/02/federated-learning-with-formal.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.googleblog.com/2022/02/federated-learning-with-formal.html</a>, 2022
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> <a target="_blank" href="https://www.tensorflow.org/datasets/catalog/c4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/datasets/catalog/c4</a>
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo, Peter Kairouz, H. McMahan, Jesse Rosenstock and Yuanbo Zhang
</span>
<span class="ltx_bibblock">‚ÄúFederated Learning of Gboard Language Models with Differential Privacy‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2305.18465" title="" class="ltx_ref ltx_href">2305.18465 [cs.LG]</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">James Henry Bell, Kallista A. Bonawitz, Adri√† Gasc√≥n, Tancr√®de Lepoint and Mariana Raykova
</span>
<span class="ltx_bibblock">‚ÄúSecure Single-Server Aggregation with (Poly)Logarithmic Overhead‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security</em>, CCS ‚Äô20
</span>
<span class="ltx_bibblock">Virtual Event, USA: Association for Computing Machinery, 2020, pp. 1253‚Äì1269
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3372297.3417885" title="" class="ltx_ref ltx_href">10.1145/3372297.3417885</a>
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">‚ÄúProject Oak‚Äù <a target="_blank" href="https://github.com/project-oak/oak" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/project-oak/oak</a>
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">‚ÄúFederated Aggregation‚Äù <a target="_blank" href="https://github.com/google/federated-compute/tree/main/fcp/aggregation" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/federated-compute/tree/main/fcp/aggregation</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.14792" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.14793" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.14793">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.14793" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.14795" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 22:51:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
