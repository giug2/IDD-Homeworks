<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.15002] CommVQA: Situating Visual Question Answering in Communicative Contexts</title><meta property="og:description" content="Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge abo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CommVQA: Situating Visual Question Answering in Communicative Contexts">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CommVQA: Situating Visual Question Answering in Communicative Contexts">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.15002">

<!--Generated on Tue Mar  5 15:59:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CommVQA: Situating Visual Question Answering
<br class="ltx_break">in Communicative Contexts</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nandita Shankar Naik 
<br class="ltx_break">Stanford University 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">nanditan@cs.stanford.edu</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Christopher Potts 
<br class="ltx_break">Stanford University 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">cgpotts@stanford.edu</span> 
<br class="ltx_break">
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Elisa Kreiss 
<br class="ltx_break">UCLA 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">ekreiss@ucla.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes visual questions, we introduce <span id="id6.id1.1" class="ltx_text">CommVQA</span>, a VQA dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and <span id="id6.id1.2" class="ltx_text ltx_font_italic">follow-up</span> questions and answers conditioned on the scenario. We show that CommVQA poses a challenge for current models. Providing contextual information to VQA models improves performance broadly, highlighting the relevance of situating systems within a communicative scenario.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA), the task of providing an answer to a question given an image, measures a model’s ability to synthesize visual and textual modalities, and has many promising real-world applications. For example, when images online can’t be seen and accessed, it severely affects people’s abilities to educate themselves, socially engage, and stay informed <cite class="ltx_cite ltx_citemacro_cite">Morris et al. (<a href="#bib.bib28" title="" class="ltx_ref">2016</a>); MacLeod et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Voykinska et al. (<a href="#bib.bib42" title="" class="ltx_ref">2016</a>); Gleason et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, and VQA models are an opportunity for providing interactive accessibility to such visual content at scale <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>); Baker et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>. While most prior VQA datasets focus on investigating image-text alignment as a decontextualized task <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>); Goyal et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>); Hudson and Manning (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>); Marino et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, we aim to reframe it as a human-centric communicative problem, moving it closer to a real-world interactive setting.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">According to a pragmatic Bayesian view of communicative actions, people tend to ask questions that maximize the contextually relevant information gain based on their existing prior beliefs about the world <cite class="ltx_cite ltx_citemacro_cite">Frank and Goodman (<a href="#bib.bib9" title="" class="ltx_ref">2012</a>)</cite>, following fundamental pragmatic principles <cite class="ltx_cite ltx_citemacro_cite">Grice (<a href="#bib.bib13" title="" class="ltx_ref">1975</a>)</cite>. Based on these linguistic insights, we argue that prior VQA datasets largely do not consider two central communicative drives that limit their utility.
The information people aim to obtain (and consequently the types of questions they ask) varies with (1) the person’s <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">information needs</em> based on their goals when encountering the image, and (2) the person’s <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">prior knowledge</em> of the image content. Thus we introduce CommVQA, a benchmark that treats VQA as an inherently communicative task. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>All data and code will be made available at: <a target="_blank" href="https://github.com/nnaik39/commvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nnaik39/commvqa</a>.</span></span></span></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2402.15002/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="803" height="269" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Experimental design overview. First, images were crawled from Wikipedia and matched with plausible context scenarios. Descriptions for each image were initially elicited from GPT-4V and then edited by humans to ensure truthful and useful image descriptions. Then questions and answers were elicited from people, conditioned on the context and the image description, resulting in a minimum of three answers to each of the 1,215 unique visual questions. Instructions are simplified for illustration purposes (see Appendix <a href="#A1.SS1" title="A.1 Dataset Collection Overview ‣ Appendix A Appendix ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for full prompts and task setups). </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To investigate people’s <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">information needs</em>, CommVQA consists not only of images, questions, and answers, but also of image descriptions and plausible communicative scenarios for each image. Prior image accessibility research with blind and low-vision (BLV) participants shows that the information people want from an image is dependent upon the scenario in which the image appears <cite class="ltx_cite ltx_citemacro_cite">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>); Kreiss et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022a</a>); Muehlbradt and Kane (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>. For example, if a person encounters an image when they are shopping online, they are likely to ask questions about the brands within the image, while if they’re browsing the news, the perceived purpose of the image shifts, and they are likely to ask questions about the event occurring within the image <cite class="ltx_cite ltx_citemacro_cite">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>. We therefore define a scenario as the type of website (e.g., a shopping website) combined with a goal for viewing it (e.g., to buy a gift) and expect it to shape people’s information needs in a VQA task.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition, the relevant questions a person might ask are predicted to be guided by their <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">prior knowledge</em> about the image. For most images we encounter online, we can rely on rich cues that allow sophisticated inferences about what an image contains. An image on a shopping website, for instance, would likely be accompanied by an article label, such as “Colorful Summer Skirt”, or it could have an informative alt text description. In the standard VQA task, annotators are asked to write questions in isolation to “fool a smart robot”, an adversarial task where the goal of the questioner is to trick an AI model <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. However, in the naturalistic setting, these visual questions are better conceptualized as <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">follow-up</em> questions, since they are conditioned on already available information. In CommVQA, we situate the task by providing people with quality-controlled image descriptions instead of the image itself when collecting visual questions. Together with the contextual grounding of the images, this pipeline, presented in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, generates a challenging dataset with context-sensitive, highly diverse questions, as well as longer answers compared to prior VQA datasets.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We benchmark four state-of-the-art VQA models on CommVQA, and show that the situated nature of CommVQA poses a significant challenge for current state-of-the-art vision-language models (VLMs) and allows for analytical insights into model generation behavior due to a highly controlled data generation pipeline. Crucially, we find that models benefit from accessing the contextual information, further suggesting that these are important resources that shape VQA in communicative settings.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our main contributions are: (a) we introduce CommVQA, a benchmark consisting of images, contexts, descriptions, questions, and answers, where models must infer the details most relevant to the questioner’s goals, (b) we benchmark this dataset on current VQA approaches and explore whether models can be instructed to integrate this contextual condition, and (c) we analyze model performance on this task.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">With CommVQA, we aim to situate the abstract VQA task within communicative settings. This builds on prior VQA dataset work (Section <a href="#S2.SS1" title="2.1 VQA Datasets ‣ 2 Related Work ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>), and communicative insights from linguistics and human-computer interaction (Section <a href="#S2.SS2" title="2.2 VQA as Communication ‣ 2 Related Work ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>)</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>VQA Datasets</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image-Question-Answer Input</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">Most VQA datasets focus on image-question-answer triplets, which are constructed in isolation from the real-world contexts in which the images might appear <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>); Goyal et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>); Hudson and Manning (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>); Marino et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>.
The VizWiz dataset <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> stands out among VQA datasets as it focuses on real-world accessibility. The images in VizWiz are elicited from BLV users uploaded a picture to a phone app with a question about their real-world environment. The goals behind the questions and images is specific to BLV people exploring their physical environment. <span id="S2.SS1.SSS0.Px1.p1.1.1" class="ltx_text">CommVQA</span> is complementary to VizWiz, since it investigates how contextually situating images online affects a model’s VQA task performance.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image-Question-Answer-Context Input</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">Some VQA datasets have explored integrating supplementary information to the image. Visual Dialog <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> aims to create visual chatbots, which can answer a question based on an image and the prior dialogic context. Similarly to CommVQA, the questions in Visual Dialog can be similarly conceptualized as follow-up questions based on an image description. However, the images, questions, and answers are still decontextualized from a specific information goal beyond the questioner wanting to understand the image. CommVQA instead extends the focus on varying the questioner’s broader goals in understanding the image.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p2.1" class="ltx_p">The ScienceQA <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> and VQAOnline <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> datasets were constructed by crawling online resources. ScienceQA <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> consists of multimodal science understanding questions, where each multiple choice question and answer is associated with a lecture and explanation, functioning as the context. VQAOnline <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> consists of images, questions, and context sourced from StackExchange. The question is the title and the context is the body of the StackExchange post, which constitutes supplementary information that’s useful for answering the question.
The notion of context in these datasets is limited to texts that provide supplementary information, whereas we specifically focus on the effect of changing scenarios on the VQA task. Additionally, due to the controlled generation pipeline, CommVQA allows for insights on how models perform under varying contextual conditions.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>VQA as Communication</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Within pragmatics, there is a general consensus that questions are grounded in contexts and sensitive to the goals of the interlocutors <cite class="ltx_cite ltx_citemacro_citep">(Groenendijk and Stokhof, <a href="#bib.bib14" title="" class="ltx_ref">1984</a>; Ginzburg, <a href="#bib.bib10" title="" class="ltx_ref">1996</a>; Roberts, <a href="#bib.bib35" title="" class="ltx_ref">1996</a>; van Rooy, <a href="#bib.bib40" title="" class="ltx_ref">2003</a>)</cite>.
In line with this prediction, prior human-subject studies with BLV participants show that context influences the information that people want about an image <cite class="ltx_cite ltx_citemacro_cite">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>); Muehlbradt and Kane (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>); Kreiss et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022a</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> find that on social media, they wanted to know more about the people and the activity of the person who posted the image, while if people were on a shopping website to purchase a gift for a friend, they expressed a desire to learn more about the objects within the image. Inspired by <cite class="ltx_cite ltx_citemacro_citet">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>, we utilize a similar type of scenario. With CommVQA, we constitute a large-scale dataset where context-sensitivity emerges with sighted participants in question and answer behavior and can be studied at scale.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The CommVQA Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The CommVQA dataset was constructed in five main steps, as visualized in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. First, we sourced images from Wikipedia, and elicited both plausible scenarios and descriptions for the images from GPT-4V <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023a</a>)</cite>. To ensure description quality, we conducted a human-subject study to edit the descriptions. We then elicited questions and answers for the dataset from US-based crowdworkers on Prolific. All human-subject studies were conducted with IRB approval.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Image-Scenario Matching</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We extracted 150 images from Wikipedia pages of topics related to at least one of the scenario conditions. Since each image appears in two scenarios and each image-scenario pair is annotated with at minimum four questions and each question with three answers, this results in 3,642 unique question-answer pairs. We prioritized high coverage of individual datapoints over breadth since the focus of this work is on uncovering the diverse contextual effects on the VQA task.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We chose six potential scenarios: Shopping, Travel, Science Magazines, News, Health, and Social Media. These scenarios were informed by prior work that showed people’s information needs varied across these scenarios <cite class="ltx_cite ltx_citemacro_cite">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>, and allowed for an overlap between the images that appear in each scenario–for instance, images in travel blogs (e.g., a picture of a waterfall) could plausibly appear in an online science magazine.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">To assign plausible scenarios to the image, we instructed GPT-4V <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023a</a>)</cite> to rank the six scenarios in order of descending plausibility, using the following prompt, which was inspired by the framing in prior work <cite class="ltx_cite ltx_citemacro_cite">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>: “Imagine you are a person browsing the Internet. Please rank the following scenarios in which you might encounter this image: 1) You are browsing a shopping website, with the goal of purchasing an item or experience…” (The full prompt to GPT4-V is included in Appendix <a href="#A1.SS1" title="A.1 Dataset Collection Overview ‣ Appendix A Appendix ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.) Using a human-subject pilot study, we established that GPT-4V made reasonable choices for contexts, which enabled us to easily scale. From the top-three scenarios for each image, two scenarios were selected in order to balance out the co-occurrence of contexts.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Image Description Elicitation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We elicited descriptions for all images in our dataset. They form the basis for the follow-up questions participants asked, simulating the effect of someone encountering an alt text associated with the image online. Importantly, descriptions were generated out-of-context so we could analyze context effects on the questions and answers without the description being a confounding factor.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Automatic Description Draft Elicitation</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We elicited initial description drafts by prompting GPT4-V with the phrase: <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">Describe this image in less than 150 characters to someone who cannot see it.</span> The length constraint follows a commonly-issued guide on best-practices for accessibility alt text writing <cite class="ltx_cite ltx_citemacro_cite">OSU (<a href="#bib.bib32" title="" class="ltx_ref">2024</a>)</cite>. We chose Wikipedia for sourcing images due to the copyright permissions and image variety.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Description Editing</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">To ensure description quality, we conducted a human-subject study where participants edited the descriptions generated by GPT-4V <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023a</a>)</cite>. These edits were intended to help balance for potential inaccuracies, possible misalignments with human description preferences, and the current design choices of GPT-4V. For instance, as of October 2023, the model refrains from explicitly identifying the people within an image <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib31" title="" class="ltx_ref">2023b</a>)</cite>, even though proper names can be an important detail for a useful image description <cite class="ltx_cite ltx_citemacro_cite">Bennett et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>); MacLeod et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p">Each participant was shown six randomized trials with an image and description. They were instructed to edit the image description to correct any errors and make the it more useful to someone who cannot see it. Importantly, participants were not shown the context that each image was placed in to control for question variation across contexts. Participants could also choose to skip if no edits were needed. We recruited 44 participants and compensated them at the rate of $13.50/hr.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.1" class="ltx_p">We collected three description edits for each image description draft, and selected a random edited description for the final description, as long as the Jaccard distance between the original GPT-4V description and the edited description was less than <math id="S3.SS2.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="0.2" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.1.m1.1a"><mn id="S3.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.1.m1.1b"><cn type="float" id="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.1.m1.1c">0.2</annotation></semantics></math>, to ensure that we weren’t selecting outliers.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Question Elicitation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To elicit visual questions, we recruited 139 participants, who were paid $13.50/hr with an average completion time of seven minutes. In each trial, participants were given two pieces of information: an image description (e.g., “A group of people of various ages walking along a grassy path, with trees on one side.”), and a scenario for the image (e.g., Imagine you are browsing a Health website, with the goal of learning how to live a healthier lifestyle). Crucially, participants didn’t see the image to avoid priming for specific questions and simulate the visual inaccessibility of the image in the real-world scenario. Participants rated how likely the image would appear within the provided scenario, and were prompted to ask two questions they would like answered by someone who can see the image.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Through this setup, we elicited 1,569 questions in total, with at least four questions for each image-scenario pair.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Answer Elicitation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We elicited answers from 480 participants, who were paid $13.50/hr, with an average completion time of seven and a half minutes. Participants were shown the image, question, context, and description. They were told that another user asked the question based on this image description, and asked to write an answer that would help the other person visualize the image. In the final dataset, each question was answered by at least three separate participants. We also included a checkbox for participants to indicate if a question was unanswerable. If a question was voted unanswerable by two or more annotators, we excluded the question from the dataset, resulting in 355 exclusions (22.6%).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The final CommVQA dataset consists of 3,642 unique question-answer pairs, spanning across 300 unique image-scenario pairs. To better understand the challenges posed by this dataset, we now provide an analysis of <span id="S4.p1.1.1" class="ltx_text">CommVQA</span>. Examples from the dataset, sampled to cover all scenarios, are given in Appendix Figure <a href="#A1.F6" title="Figure 6 ‣ A.2.1 Answer analysis of named entities ‣ A.2 Additional Dataset Composition Analysis ‣ Appendix A Appendix ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Analysis of Descriptions</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">In CommVQA, the descriptions are the basis for the VQA task and were collected in a two-stage process: first an automatic description generation by GPT-4V and then a human editing phase. The automatically generated descriptions had an average length of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="99.36" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">99.36</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="float" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">99.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">99.36</annotation></semantics></math> characters and in <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">50</mn><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">50\%</annotation></semantics></math> of trials, people didn’t make any edits to the descriptions. Additions had a minimal effect on length–when participants chose to make edits, they added extra information, resulting in an average length of <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="106.04" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">106.04</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><cn type="float" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">106.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">106.04</annotation></semantics></math> characters.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Analysis of Questions</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2402.15002/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="279" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Heatmap displaying the accuracies that finetuned BERT achieved for classifying questions into each pair of scenarios. BERT achieved an accuracy of <math id="S4.F2.3.m1.1" class="ltx_Math" alttext="91.5\%" display="inline"><semantics id="S4.F2.3.m1.1b"><mrow id="S4.F2.3.m1.1.1" xref="S4.F2.3.m1.1.1.cmml"><mn id="S4.F2.3.m1.1.1.2" xref="S4.F2.3.m1.1.1.2.cmml">91.5</mn><mo id="S4.F2.3.m1.1.1.1" xref="S4.F2.3.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.3.m1.1c"><apply id="S4.F2.3.m1.1.1.cmml" xref="S4.F2.3.m1.1.1"><csymbol cd="latexml" id="S4.F2.3.m1.1.1.1.cmml" xref="S4.F2.3.m1.1.1.1">percent</csymbol><cn type="float" id="S4.F2.3.m1.1.1.2.cmml" xref="S4.F2.3.m1.1.1.2">91.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.3.m1.1d">91.5\%</annotation></semantics></math> for distinguishing science magazines and shopping, yet BERT only achieved a performance of <math id="S4.F2.4.m2.1" class="ltx_Math" alttext="86\%" display="inline"><semantics id="S4.F2.4.m2.1b"><mrow id="S4.F2.4.m2.1.1" xref="S4.F2.4.m2.1.1.cmml"><mn id="S4.F2.4.m2.1.1.2" xref="S4.F2.4.m2.1.1.2.cmml">86</mn><mo id="S4.F2.4.m2.1.1.1" xref="S4.F2.4.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.4.m2.1c"><apply id="S4.F2.4.m2.1.1.cmml" xref="S4.F2.4.m2.1.1"><csymbol cd="latexml" id="S4.F2.4.m2.1.1.1.cmml" xref="S4.F2.4.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.F2.4.m2.1.1.2.cmml" xref="S4.F2.4.m2.1.1.2">86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.m2.1d">86\%</annotation></semantics></math> to distinguish questions between science magazines and news.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The main goal of CommVQA is to situate the VQA task in a communicative context, assuming that the context shapes what becomes relevant and therefore has implications for downstream model performance. In this section, we investigate the context-sensitivity of the questions in CommVQA.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">If questions are context-sensitive, then a trained classifier should be able to predict the correct context from the question and achieve a performance reliably higher than random. To investigate whether this is true for CommVQA, we fine-tuned BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite> on the task of predicting whether each question appears within a certain context or not. We expect this task to be difficult even for a human, given that questions such as “What time of day is it?” might appear in any context. We split all the questions collected into 20 cross-validation folds, with an 80-10-10 train/test/val split, and fine-tuned for 200 epochs with LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. If questions written in different contexts do not differ, we should achieve only a random performance at <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="16\%" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">16</mn><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">16\%</annotation></semantics></math>. However, fine-tuned BERT achieved an accuracy of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="57\%" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">57</mn><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">57</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">57\%</annotation></semantics></math>, a significant improvement over random choice, showing that questions elicited within different contexts are distinguishable even from a language-only perspective.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">While we find overall evidence that questions vary between all contexts, a classifier analysis further allows us to investigate which individual contexts have the least and most overlap in the questions asked. Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Analysis of Questions ‣ 4 Dataset Analysis ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows BERT’s performance when fine-tuned on distinguishing pairs of scenarios (e.g., shopping from science magazines), and indicates which scenarios are more easily distinguishable.
Intuitively, certain contexts are more related than others–for instance, shopping and social media content is more likely to contain images of famous models, while shopping and science magazines are less likely to have overlap. Our analysis confirms this intuition and highlights where the translational accuracy of models might fail downstream.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.4" class="ltx_p">We further inspect the question interrogatives for a potential source of between-context variation. The results of a Welch’s t-test demonstrated that, for example, compared to all other scenarios, “Who” questions are significantly more likely to appear in the social media scenario (<math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="t(310)=2.86" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.2" xref="S4.SS2.p4.1.m1.1.2.cmml"><mrow id="S4.SS2.p4.1.m1.1.2.2" xref="S4.SS2.p4.1.m1.1.2.2.cmml"><mi id="S4.SS2.p4.1.m1.1.2.2.2" xref="S4.SS2.p4.1.m1.1.2.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.2.2.1" xref="S4.SS2.p4.1.m1.1.2.2.1.cmml">​</mo><mrow id="S4.SS2.p4.1.m1.1.2.2.3.2" xref="S4.SS2.p4.1.m1.1.2.2.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.1.2.2.3.2.1" xref="S4.SS2.p4.1.m1.1.2.2.cmml">(</mo><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">310</mn><mo stretchy="false" id="S4.SS2.p4.1.m1.1.2.2.3.2.2" xref="S4.SS2.p4.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.1.m1.1.2.1" xref="S4.SS2.p4.1.m1.1.2.1.cmml">=</mo><mn id="S4.SS2.p4.1.m1.1.2.3" xref="S4.SS2.p4.1.m1.1.2.3.cmml">2.86</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.2"><eq id="S4.SS2.p4.1.m1.1.2.1.cmml" xref="S4.SS2.p4.1.m1.1.2.1"></eq><apply id="S4.SS2.p4.1.m1.1.2.2.cmml" xref="S4.SS2.p4.1.m1.1.2.2"><times id="S4.SS2.p4.1.m1.1.2.2.1.cmml" xref="S4.SS2.p4.1.m1.1.2.2.1"></times><ci id="S4.SS2.p4.1.m1.1.2.2.2.cmml" xref="S4.SS2.p4.1.m1.1.2.2.2">𝑡</ci><cn type="integer" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">310</cn></apply><cn type="float" id="S4.SS2.p4.1.m1.1.2.3.cmml" xref="S4.SS2.p4.1.m1.1.2.3">2.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">t(310)=2.86</annotation></semantics></math>, <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><lt id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></lt><ci id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">p&lt;0.001</annotation></semantics></math>), and “What” questions are significantly more likely to appear in the shopping scenario (<math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="t(388)=2.86" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mrow id="S4.SS2.p4.3.m3.1.2" xref="S4.SS2.p4.3.m3.1.2.cmml"><mrow id="S4.SS2.p4.3.m3.1.2.2" xref="S4.SS2.p4.3.m3.1.2.2.cmml"><mi id="S4.SS2.p4.3.m3.1.2.2.2" xref="S4.SS2.p4.3.m3.1.2.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.3.m3.1.2.2.1" xref="S4.SS2.p4.3.m3.1.2.2.1.cmml">​</mo><mrow id="S4.SS2.p4.3.m3.1.2.2.3.2" xref="S4.SS2.p4.3.m3.1.2.2.cmml"><mo stretchy="false" id="S4.SS2.p4.3.m3.1.2.2.3.2.1" xref="S4.SS2.p4.3.m3.1.2.2.cmml">(</mo><mn id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">388</mn><mo stretchy="false" id="S4.SS2.p4.3.m3.1.2.2.3.2.2" xref="S4.SS2.p4.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.3.m3.1.2.1" xref="S4.SS2.p4.3.m3.1.2.1.cmml">=</mo><mn id="S4.SS2.p4.3.m3.1.2.3" xref="S4.SS2.p4.3.m3.1.2.3.cmml">2.86</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.2.cmml" xref="S4.SS2.p4.3.m3.1.2"><eq id="S4.SS2.p4.3.m3.1.2.1.cmml" xref="S4.SS2.p4.3.m3.1.2.1"></eq><apply id="S4.SS2.p4.3.m3.1.2.2.cmml" xref="S4.SS2.p4.3.m3.1.2.2"><times id="S4.SS2.p4.3.m3.1.2.2.1.cmml" xref="S4.SS2.p4.3.m3.1.2.2.1"></times><ci id="S4.SS2.p4.3.m3.1.2.2.2.cmml" xref="S4.SS2.p4.3.m3.1.2.2.2">𝑡</ci><cn type="integer" id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">388</cn></apply><cn type="float" id="S4.SS2.p4.3.m3.1.2.3.cmml" xref="S4.SS2.p4.3.m3.1.2.3">2.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">t(388)=2.86</annotation></semantics></math>, <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="p&lt;0.005" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mrow id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml"><mi id="S4.SS2.p4.4.m4.1.1.2" xref="S4.SS2.p4.4.m4.1.1.2.cmml">p</mi><mo id="S4.SS2.p4.4.m4.1.1.1" xref="S4.SS2.p4.4.m4.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.p4.4.m4.1.1.3" xref="S4.SS2.p4.4.m4.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><apply id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1"><lt id="S4.SS2.p4.4.m4.1.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1.1"></lt><ci id="S4.SS2.p4.4.m4.1.1.2.cmml" xref="S4.SS2.p4.4.m4.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.p4.4.m4.1.1.3.cmml" xref="S4.SS2.p4.4.m4.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">p&lt;0.005</annotation></semantics></math>) compared to any other scenario. These results indicate that different question types are more likely to be asked in certain scenarios, which carries implications that model evaluation should be contextual. A model that has poor performance on “Who” questions might seem competent in non-social media scenarios, but fail to generalize to social media due to the distinct nature of the user’s information needs.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Taken together, we find converging evidence that the questions in CommVQA fundamentally vary based on the scenario the images were presented in, highlighting the diverse requirements for building robust communicatively situated VQA models.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Analysis of Answers</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We now turn to a general analysis of the <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">answers</em> in CommVQA and investigate the extent to which they are contextually situated.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Firstly, with an average length of <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="9.8" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">9.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn type="float" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">9.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">9.8</annotation></semantics></math> words, the answers in CommVQA are surprisingly long compared to prior datasets that used a similar online answer elicitation setup (e.g., 1.1 words for VQA-v2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> and 2.1 words for Visual Dialog <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>). Contrasting prior work, the instructions of CommVQA are framed as a communicative task, where answerers were asked to help someone else who cannot see the image. It’s therefore plausible that the longer responses are partially due to the participant’s wish to faithfully communicate with the questioner <cite class="ltx_cite ltx_citemacro_cite">Grice (<a href="#bib.bib13" title="" class="ltx_ref">1975</a>)</cite>. In support of this hypothesis, we find that participants qualified many assumptions of theirs that were not directly supported by visual evidence with hedges and expressions of uncertainty, therefore lengthening the responses.
In fact, the most common trigram within the dataset was “appear(s) to be” (93 occurrences), followed by “it looks like” (45 occurrences).</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.3" class="ltx_p">While we find that the scenario significantly shapes the content of the <em id="S4.SS3.p3.3.1" class="ltx_emph ltx_font_italic">questions</em> in CommVQA (see Section <a href="#S4.SS2" title="4.2 Analysis of Questions ‣ 4 Dataset Analysis ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), it’s unclear to what extent we should expect the <em id="S4.SS3.p3.3.2" class="ltx_emph ltx_font_italic">answers</em> to be shaped by the scenario, as well. Since the questions are inherently affected by the context, it is difficult to establish a similar variation in the answers that isn’t already induced by the questions themselves. For example, we find that the rates at which named entities are mentioned largely varies between scenarios (see Appendix Figure <a href="#A1.F5" title="Figure 5 ‣ A.2 Additional Dataset Composition Analysis ‣ Appendix A Appendix ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), which is in line with the varied frequencies of “Who” questions. However, when we investigate the <em id="S4.SS3.p3.3.3" class="ltx_emph ltx_font_italic">type</em> of entities named in responses, we find additional effects of the scenario.
For example, organizations are more frequently referenced in the travel scenario but individual people are more referenced in the science journal scenario (see Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3 Analysis of Answers ‣ 4 Dataset Analysis ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
This is borne out as a significant interaction between the proportion of mentioned organizations vs. people and the travel vs. science journal scenarios (<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\beta=-2.22" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">β</mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml"><mo id="S4.SS3.p3.1.m1.1.1.3a" xref="S4.SS3.p3.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS3.p3.1.m1.1.1.3.2" xref="S4.SS3.p3.1.m1.1.1.3.2.cmml">2.22</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><eq id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></eq><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝛽</ci><apply id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><minus id="S4.SS3.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.p3.1.m1.1.1.3"></minus><cn type="float" id="S4.SS3.p3.1.m1.1.1.3.2.cmml" xref="S4.SS3.p3.1.m1.1.1.3.2">2.22</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\beta=-2.22</annotation></semantics></math>, <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="SE=0.67" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mrow id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2.2" xref="S4.SS3.p3.2.m2.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.2.m2.1.1.2.1" xref="S4.SS3.p3.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.SS3.p3.2.m2.1.1.2.3" xref="S4.SS3.p3.2.m2.1.1.2.3.cmml">E</mi></mrow><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">0.67</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><eq id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1"></eq><apply id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2"><times id="S4.SS3.p3.2.m2.1.1.2.1.cmml" xref="S4.SS3.p3.2.m2.1.1.2.1"></times><ci id="S4.SS3.p3.2.m2.1.1.2.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2.2">𝑆</ci><ci id="S4.SS3.p3.2.m2.1.1.2.3.cmml" xref="S4.SS3.p3.2.m2.1.1.2.3">𝐸</ci></apply><cn type="float" id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">0.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">SE=0.67</annotation></semantics></math>, <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mi id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS3.p3.3.m3.1.1.1" xref="S4.SS3.p3.3.m3.1.1.1.cmml">&lt;</mo><mn id="S4.SS3.p3.3.m3.1.1.3" xref="S4.SS3.p3.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><lt id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1.1"></lt><ci id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">𝑝</ci><cn type="float" id="S4.SS3.p3.3.m3.1.1.3.cmml" xref="S4.SS3.p3.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">p&lt;0.001</annotation></semantics></math>).
This suggests that the external knowledge VQA models need to reference varies across scenarios and provides evidence that contextual considerations matter on the answer level.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2402.15002/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_img_landscape" width="701" height="355" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Named entity recognition analysis showing significant variation within people’s answers between Travel and Science Journal scenarios. Organizations (e.g., companies, and institutions) were significantly more likely to be mentioned in the Travel over the Science scenario, and vice versa for people (e.g.., political figures, and celebrities). Asterisks indicate significant interaction. Error bars are 95% confidence intervals.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Model Benchmarking</h2>

<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">BLEU-1</th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">BLEU-4</th>
<th id="S5.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">METEOR</th>
<th id="S5.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ROUGE</th>
<th id="S5.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">CIDEr</th>
<th id="S5.T1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">CLIPScore</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">IDEFICS</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">0.326</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">0.106</td>
<td id="S5.T1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">0.189</td>
<td id="S5.T1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">0.401</td>
<td id="S5.T1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">0.825</td>
<td id="S5.T1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">0.667</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left">IDEFICS (Contextual)</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_left"><span id="S5.T1.1.3.2.2.1" class="ltx_text ltx_font_bold">0.356</span></td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_left"><span id="S5.T1.1.3.2.3.1" class="ltx_text ltx_font_bold">0.122</span></td>
<td id="S5.T1.1.3.2.4" class="ltx_td ltx_align_left"><span id="S5.T1.1.3.2.4.1" class="ltx_text ltx_font_bold">0.198</span></td>
<td id="S5.T1.1.3.2.5" class="ltx_td ltx_align_left"><span id="S5.T1.1.3.2.5.1" class="ltx_text ltx_font_bold">0.413</span></td>
<td id="S5.T1.1.3.2.6" class="ltx_td ltx_align_left"><span id="S5.T1.1.3.2.6.1" class="ltx_text ltx_font_bold">0.904</span></td>
<td id="S5.T1.1.3.2.7" class="ltx_td ltx_align_left">0.666</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_t">mPLUG-Owl</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t">0.191</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">0.045</td>
<td id="S5.T1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">0.166</td>
<td id="S5.T1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t">0.311</td>
<td id="S5.T1.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t">0.414</td>
<td id="S5.T1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t">0.720</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_left">mPLUG-Owl (Contextual)</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_left">0.178</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_left">0.043</td>
<td id="S5.T1.1.5.4.4" class="ltx_td ltx_align_left">0.169</td>
<td id="S5.T1.1.5.4.5" class="ltx_td ltx_align_left">0.284</td>
<td id="S5.T1.1.5.4.6" class="ltx_td ltx_align_left">0.335</td>
<td id="S5.T1.1.5.4.7" class="ltx_td ltx_align_left">0.756</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_t">LLaVA</td>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_t">0.192</td>
<td id="S5.T1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t">0.050</td>
<td id="S5.T1.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">0.171</td>
<td id="S5.T1.1.6.5.5" class="ltx_td ltx_align_left ltx_border_t">0.311</td>
<td id="S5.T1.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t">0.431</td>
<td id="S5.T1.1.6.5.7" class="ltx_td ltx_align_left ltx_border_t">0.729</td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<td id="S5.T1.1.7.6.1" class="ltx_td ltx_align_left">LLaVA (Contextual)</td>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_left">0.149</td>
<td id="S5.T1.1.7.6.3" class="ltx_td ltx_align_left">0.037</td>
<td id="S5.T1.1.7.6.4" class="ltx_td ltx_align_left">0.161</td>
<td id="S5.T1.1.7.6.5" class="ltx_td ltx_align_left">0.247</td>
<td id="S5.T1.1.7.6.6" class="ltx_td ltx_align_left">0.200</td>
<td id="S5.T1.1.7.6.7" class="ltx_td ltx_align_left"><span id="S5.T1.1.7.6.7.1" class="ltx_text ltx_font_bold">0.784</span></td>
</tr>
<tr id="S5.T1.1.8.7" class="ltx_tr">
<td id="S5.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_t">BLIP-2</td>
<td id="S5.T1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_t">0.289</td>
<td id="S5.T1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_t">0.067</td>
<td id="S5.T1.1.8.7.4" class="ltx_td ltx_align_left ltx_border_t">0.100</td>
<td id="S5.T1.1.8.7.5" class="ltx_td ltx_align_left ltx_border_t">0.286</td>
<td id="S5.T1.1.8.7.6" class="ltx_td ltx_align_left ltx_border_t">0.435</td>
<td id="S5.T1.1.8.7.7" class="ltx_td ltx_align_left ltx_border_t">0.604</td>
</tr>
<tr id="S5.T1.1.9.8" class="ltx_tr">
<td id="S5.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_bb">BLIP-2 (Contextual)</td>
<td id="S5.T1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_bb">0.059</td>
<td id="S5.T1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_bb">0.010</td>
<td id="S5.T1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_bb">0.078</td>
<td id="S5.T1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_bb">0.224</td>
<td id="S5.T1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_bb">0.343</td>
<td id="S5.T1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_bb">0.662</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results for the baseline and contextual condition across all models. The highest-scoring models in all columns are the ones which integrate the contextual condition. The CLIPScore metric improves for all models, except for IDEFICS, where it stays the same. This indicates the model’s generated answers contain more visual details with the contextual condition. Results were conducted over three confidence intervals.</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we investigate the performance of four state-of-the-art VLMs on CommVQA  and to what extent providing context to the models improves their performance.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We selected four models to benchmark on this dataset: LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>, BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, mPLUG-OWL <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, and IDEFICS <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. To maximally enable the reproducibility of this work and support the public development of models, we focus on open-source models. LLaVA is trained on a multimodal instruction-following dataset, and exhibits strong performance across several VQA datasets <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>); Hudson and Manning (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> is also instruction-tuned, and displays competitive performance on various VQA benchmarks, including VQA-v2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. IDEFICS <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite> is an open-access reproduction of Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, and was trained on a naturalistic web-scale dataset of interleaved image-text documents, including 141 million web pages. We selected LLaVA, mPLUG-Owl, and IDEFICS since their instruction-tuned nature allows us to straightforwardly integrate context. We also included a non-instruction-tuned model, BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, a general-purpose vision-language model that is additionally fine-tuned on the VQA task.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Metrics</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">We evaluated these models on the NLG metrics BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib33" title="" class="ltx_ref">2002</a>)</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">Vedantam et al. (<a href="#bib.bib41" title="" class="ltx_ref">2015</a>)</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a href="#bib.bib4" title="" class="ltx_ref">2005</a>)</cite>, and ROUGE <cite class="ltx_cite ltx_citemacro_cite">Lin (<a href="#bib.bib23" title="" class="ltx_ref">2004</a>)</cite>. However, there are still many aspects of performance these metrics cannot capture. For instance, studies show that these metrics do not adequately capture object hallucinations <cite class="ltx_cite ltx_citemacro_cite">Rohrbach et al. (<a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite>. To measure how closely an answer’s visual details matches with the image, we also include the CLIPScore metric <cite class="ltx_cite ltx_citemacro_cite">Hessel et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. Including the CLIPScore metric has the added benefit of including an assessment of how close a model-generated answer is to a visual description.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Experimental Setup</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">For all models, we ran <em id="S5.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">baseline</em> experiments where only the image and question were provided, and the full <em id="S5.SS0.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">contextual</em> condition where the scenario and description were additionally provided, just like for the participants who answered the questions. We include our prompt for the contextual condition in Appendix <a href="#A1.SS1" title="A.1 Dataset Collection Overview ‣ Appendix A Appendix ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>. All evaluations were conducted with the greedy decoding method to ensure reproducibility.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Overall Results</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">In Table <a href="#S5.T1" title="Table 1 ‣ 5 Model Benchmarking ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we show the performance of all models when provided with the image and question (baseline) and the full context that was available to human answerers (contextual).</p>
</div>
<div id="S5.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p2.1" class="ltx_p">The IDEFICS model with the contextual condition has the highest performance for all models across all metrics except CLIPScore, which remains unaffected. Evidently, integrating contextual information made the content itself more aligned with the ground-truth answer references, as evaluated by the reference-based metrics. The unchanged CLIPScore rating suggests that the contextually induced changes in the generated answers don’t change how they relate to the images.</p>
</div>
<div id="S5.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p3.1" class="ltx_p">While for IDEFICS (the best-performing model), CLIPScore is the only metric that doesn’t significantly improve in the contextual condition, this pattern is reversed for all other models. Here, the contextual integration primarily boosts CLIPScore (i.e., the similarity to the image) but results in worse performance according to the reference-based metrics. One potential explanation for this pattern is that the less well-performing models closely reiterate the visual information they receive from the description in the contextual condition, instead of adding new information to answer the question. This would result in higher CLIPScores but lower similarity with the ground-truth answers.</p>
</div>
<div id="S5.SS0.SSS0.Px3.p4" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p4.1" class="ltx_p">To investigate how much visual information models re-iterate within their generated answers, we compared the average cosine similarity (using Sentence-BERT <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite> embeddings) between the description and the model’s generated answers. Figure <a href="#S5.F4" title="Figure 4 ‣ 5.1 Quantifying Hallucinations ‣ 5 Model Benchmarking ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the similarity between the description and the generated answers significantly increases with the contextual condition across models, according to a two-sample t-test analysis.
However, the increase in description similarity between the baseline and contextual condition is the lowest for IDEFICS. We conclude that in their answers, the contextual versions of LLaVA, mPLUG-Owl, and BLIP-2 mimic the description more than IDEFICS does, which explains the CLIPScore increase across the baseline and contextual conditions for these three models. These findings suggest that those models might overly emphasize the descriptive details when available, leading to less alignment with the ground-truth answers.</p>
</div>
</section>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Quantifying Hallucinations</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">While IDEFICS (contextual) best approximates the human answers, it’s still far from achieving human-like performance. When a model is intended to make information available that can’t be verified by a user, it is especially important that this model only generates truthful content <cite class="ltx_cite ltx_citemacro_cite">MacLeod et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>. While this is straightforward to evaluate for accuracy-based VQA datasets, this isn’t easily captured by the similarity-based metrics that are used for long-form evaluation, as presented here. In this section, we aim to give a brief intuition about the rate of hallucinated or incorrect pieces of information contained in the answers of the best-performing model, IDEFICS (contextual).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To estimate how many answers contain wrong, hallucinated or unverifiable content generated by the model, three authors independently annotated a randomly sampled subset of 51 generated answers, with a Fleiss inter-annotator kappa agreement of 49.4%. The annotations show that 38.2% of the model-generated answers contained clearly erroneous information and for another 15%, the truthfulness couldn’t be established. These results indicate that even the best-performing model (IDEFICS (contextual)) is still far from human-like performance on CommVQA.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Since named entities are a likely source of error and hallucination for VLMs, we further analyze the potential accuracy of named entity recognition within the dataset at scale. Without the context, only 35.4% of the unique named entities in IDEFICS’ answer also appeared in any of the reference answers, suggesting that IDEFICS is frequently naming incorrect entities. Providing IDEFICS with the context guides the model to identify the named entities in the answers more accurately, at a rate of 51%. Most of the named entities that IDEFICS (contextual) struggles on are numbers, particularly in counting. Of the misidentified named entities, 38% of the misidentified named entities were cardinals, ordinals, and quantity estimations, and 25% were time and date indicators, indicating that this model still needs to improve in counting and quantity estimations.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Taken together, these results highlight that CommVQA poses a challenging problem for state-of-the-art models. Our analyses suggest that models might not be able to leverage contextual information effectively and the high degree of hallucinations makes them unreliable, highlighting two important areas for future research.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2402.15002/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="708" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Average cosine similarity to each description from the human-elicited answers and model-generated answers across all conditions. Asterisks indicate significance levels according to a two-sample t-test analysis.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Visual question answering models are a promising tool for making visual content accessible to all. With CommVQA, we move the contextually isolated VQA task into a communicative setting that starts reflecting the diversity of downstream use, while keeping close control over the nature of the dataset to ease interpretability. We find strong evidence from dataset analysis that the types of relevant questions and answers change with the contextual domains where images appear. We also find evidence that integrating contextually relevant information improves model performance. Our results suggest that the path towards building viable VQA systems requires a focus on the wider communicative context where images appear.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">In this work, we show that the scenarios images are presented in fundamentally affect the VQA task. To investigate this, we varied the broad type of website where we embedded the image (Social Media, Shopping, etc). However, context effects are likely much more diverse than the effects studied in this work. For example, recent work suggests that even topic changes within a website domain (a Wikipedia article on Mountains vs. Body of Water) change the information needs that sighted and BLV users have for image descriptions <cite class="ltx_cite ltx_citemacro_cite">Kreiss et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022b</a>)</cite>. This result likely translates to the VQA task and needs further investigation.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">To allow for easy manipulations of the context domains and a highly controlled recruitment, participants were put in simulated scenarios where they were told about the website domain where the image appears. While even in these induced contextual setups, we find significant contextual variations, future work needs to explore the way this extends to real-world user experience outside of simulated scenarios.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">In CommVQA, we elicited questions and answers from sighted participants. Evidence from prior work <cite class="ltx_cite ltx_citemacro_cite">Stangl et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> indicates that these results would likely transfer more directly to the accessibility scenario, but future work needs to analyze how the sighted user behavior translates to the BLV population more directly.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This research is supported in part by grants from Google and the Generative AI for the Future of Learning program at Stanford. We thank our experiment participants for their invaluable input. We also thank Bay Foley-Cox for his thorough feedback on previous drafts, and all members of the Coalas Lab.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:23716–23736.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1505.00468" title="" class="ltx_ref ltx_href">VQA: Visual Question Answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision</em>, abs/1505.00468.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baker et al. (2021)</span>
<span class="ltx_bibblock">
Katie Baker, Amit Parekh, Adrien Fabre, Angus Addlesee, Ruben Kruiper, and Oliver Lemon. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2021.reinact-1.5" title="" class="ltx_ref ltx_href">The spoon is in the sink: Assisting visually impaired people in the kitchen</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Reasoning and Interaction Conference (ReInAct 2021)</em>, pages 32–39, Gothenburg, Sweden. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W05-0909" title="" class="ltx_ref ltx_href">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</em>, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bennett et al. (2021)</span>
<span class="ltx_bibblock">
Cynthia L Bennett, Cole Gleason, Morgan Klaus Scheuerman, Jeffrey P Bigham, Anhong Guo, and Alexandra To. 2021.

</span>
<span class="ltx_bibblock">“It’s complicated”: Negotiating accessibility and (mis) representation in image descriptions of race, gender, and disability.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, pages 1–19.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Chongyan Chen, Mengchen Liu, Noel Codella, Yunsheng Li, Lu Yuan, and Danna Gurari. 2023.

</span>
<span class="ltx_bibblock">Fully authentic visual question answering dataset from online communities.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.15562</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2017)</span>
<span class="ltx_bibblock">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock">Visual dialog.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 326–335.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frank and Goodman (2012)</span>
<span class="ltx_bibblock">
Michael C Frank and Noah D Goodman. 2012.

</span>
<span class="ltx_bibblock">Predicting pragmatic reasoning in language games.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Science</em>, 336(6084):998–998.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ginzburg (1996)</span>
<span class="ltx_bibblock">
Jonathan Ginzburg. 1996.

</span>
<span class="ltx_bibblock">Dynamics and the semantics of dialogue.

</span>
<span class="ltx_bibblock">In Jerry Seligman, editor, <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Language, Logic, and Computation</em>, volume 1, pages 221–237. CSLI, Stanford, CA.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gleason et al. (2019)</span>
<span class="ltx_bibblock">
Cole Gleason, Patrick Carrington, Cameron Cassidy, Meredith Ringel Morris, Kris M Kitani, and Jeffrey P Bigham. 2019.

</span>
<span class="ltx_bibblock">"It’s almost like they’re trying to hide it”: How User-Provided Image Descriptions Have Failed to Make Twitter Accessible.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">The World Wide Web Conference</em>, pages 549–559.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grice (1975)</span>
<span class="ltx_bibblock">
Herbert P Grice. 1975.

</span>
<span class="ltx_bibblock">Logic and conversation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Speech acts</em>, pages 41–58. Brill.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groenendijk and Stokhof (1984)</span>
<span class="ltx_bibblock">
Jeroen Groenendijk and Martin Stokhof. 1984.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Studies in the Semantics of Questions and the Pragmatics of Answers</em>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, University of Amsterdam.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2018)</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018.

</span>
<span class="ltx_bibblock">VizWiz Grand Challenge: Answering Visual Questions from Blind People.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 3608–3617.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hessel et al. (2021)</span>
<span class="ltx_bibblock">
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock">CLIPScore: A Reference-free Evaluation Metric for Image Captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 7514–7528.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning. 2019.

</span>
<span class="ltx_bibblock">GQA: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 6700–6709.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreiss et al. (2022a)</span>
<span class="ltx_bibblock">
Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, and Christopher Potts. 2022a.

</span>
<span class="ltx_bibblock">Context matters for image descriptions for accessibility: Challenges for referenceless evaluation metrics.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 4685–4697.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreiss et al. (2022b)</span>
<span class="ltx_bibblock">
Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, and Christopher Potts. 2022b.

</span>
<span class="ltx_bibblock">Context matters for image descriptions for accessibility: Challenges for referenceless evaluation metrics.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 4685–4697.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Laurençon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. 2023.

</span>
<span class="ltx_bibblock">Obelics: An open web-scale filtered dataset of interleaved image-text documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W04-1013" title="" class="ltx_ref ltx_href">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08485</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:2507–2521.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacLeod et al. (2017)</span>
<span class="ltx_bibblock">
Haley MacLeod, Cynthia L Bennett, Meredith Ringel Morris, and Edward Cutrell. 2017.

</span>
<span class="ltx_bibblock">Understanding blind people’s experiences with computer-generated captions of social media images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">proceedings of the 2017 CHI conference on human factors in computing systems</em>, pages 5988–5999.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 3195–3204.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morris et al. (2016)</span>
<span class="ltx_bibblock">
Meredith Ringel Morris, Annuska Zolyomi, Catherine Yao, Sina Bahram, Jeffrey P Bigham, and Shaun K Kane. 2016.

</span>
<span class="ltx_bibblock">"With most of it being pictures now, I rarely use it" Understanding Twitter’s Evolving Accessibility to Blind Users.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 CHI conference on human factors in computing systems</em>, pages 5506–5516.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muehlbradt and Kane (2022)</span>
<span class="ltx_bibblock">
Annika Muehlbradt and Shaun K Kane. 2022.

</span>
<span class="ltx_bibblock">What’s in an ALT Tag? Exploring Caption Content Priorities through Collaborative Captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Accessible Computing (TACCESS)</em>, 15(1):1–32.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">GPT-4 Technical Report</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI. 2023b.

</span>
<span class="ltx_bibblock">GPT4-V System Card.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/research/gpt-4v-system-card" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/gpt-4v-system-card</a>.

</span>
<span class="ltx_bibblock">[Online; accessed 10-Feb-2024].

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OSU (2024)</span>
<span class="ltx_bibblock">
OSU. 2024.

</span>
<span class="ltx_bibblock">Alternative (Alt) Text Guide.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ets.osu.edu/digital-accessibility/alternative-alt-text-guide" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ets.osu.edu/digital-accessibility/alternative-alt-text-guide</a>.

</span>
<span class="ltx_bibblock">[Online; accessed 10-Feb-2024].

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: A method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, ACL ’02, page 311–318, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">SentenceBert: Sentence embeddings using siamese bertnetworks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),</em>, page 3982–3992, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts (1996)</span>
<span class="ltx_bibblock">
Craige Roberts. 1996.

</span>
<span class="ltx_bibblock">Information structure: Towards an integrated formal theory of pragmatics.

</span>
<span class="ltx_bibblock">In Jae Hak Yoon and Andreas Kathol, editors, <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">OSU Working Papers in Linguistics</em>, volume 49: Papers in Semantics, pages 91–136. The Ohio State University Department of Linguistics, Columbus, OH.

</span>
<span class="ltx_bibblock">Revised 1998.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al. (2018)</span>
<span class="ltx_bibblock">
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.

</span>
<span class="ltx_bibblock">Object hallucination in image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stangl et al. (2020)</span>
<span class="ltx_bibblock">
Abigale Stangl, Meredith Ringel Morris, and Danna Gurari. 2020.

</span>
<span class="ltx_bibblock">"Person, Shoes, Tree. Is the Person Naked?" What People with Vision Impairments Want in Image Descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 chi conference on human factors in computing systems</em>, pages 1–13.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stangl et al. (2021)</span>
<span class="ltx_bibblock">
Abigale Stangl, Nitin Verma, Kenneth Fleischmann, Meredith Ringel Morris, and Danna Gurari. 2021.

</span>
<span class="ltx_bibblock">Going Beyond One-Size-Fits-All Image Descriptions to Satisfy the Information Wants of People Who are Blind or Have Low Vision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">23rd International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’21)</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsvilodub et al. (2023)</span>
<span class="ltx_bibblock">
Polina Tsvilodub, Michael Franke, Robert D Hawkins, and Noah D Goodman. 2023.

</span>
<span class="ltx_bibblock">Overinformative Question Answering by Humans and Machines.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Meeting of the Cognitive Science Society</em>, volume 45.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Rooy (2003)</span>
<span class="ltx_bibblock">
Robert van Rooy. 2003.

</span>
<span class="ltx_bibblock">Questioning to resolve decision problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Linguistics and Philosophy</em>, 26(6):727–763.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam et al. (2015)</span>
<span class="ltx_bibblock">
Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">CIDEr: Consensus-based image description evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566–4575</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voykinska et al. (2016)</span>
<span class="ltx_bibblock">
Violeta Voykinska, Shiri Azenkot, Shaomei Wu, and Gilly Leshed. 2016.

</span>
<span class="ltx_bibblock">How blind people interact with visual content on social networking services.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th acm conference on computer-supported cooperative work &amp; social computing</em>, pages 1584–1595.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.14178" title="" class="ltx_ref ltx_href">mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</a>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dataset Collection Overview</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">In this section, we provide a full list of the tasks and prompts provided in each step of the dataset collection process.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">For the context matching stage, we prompted GPT-4V with this prompt: 
<br class="ltx_break"><span id="A1.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">Imagine you are a person browsing the Internet. Please rank the following scenarios in which you might encounter this image: 
<br class="ltx_break">1) You are browsing a shopping website, with the goal of purchasing an item or experience. 
<br class="ltx_break">2) You are browsing science magazines (such as National Geographic), with the goal of learning more about recent science developments. 
<br class="ltx_break">3) You are browsing news websites (such as New York Times), with the goal of learning more about recent news developments. 
<br class="ltx_break">4) You are browsing a health website, with the goal of learning how to live a healthier lifestyle. 
<br class="ltx_break">5) You are browsing social media, with the goal of learning more about your connections. 
<br class="ltx_break">6) You are browsing a travel website, with the goal of traveling to a new location.</span></p>
</div>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.1" class="ltx_p">For the description elicitation, we asked participants to: “Please edit this description of the image to correct any errors and make the description more useful to someone who cannot see it.” This phrase was intended to incentivize people to both fix errors and include any communicative details that they felt was missing.</p>
</div>
<div id="A1.SS1.p4" class="ltx_para">
<p id="A1.SS1.p4.1" class="ltx_p">For the question elicitation study, we asked participants: “Imagine you are browsing a {scenario} website when you encounter the following image: {description}. If you encounter this image on a {scenario} website, what are two questions you’d want to have answered by someone who can see the image?”</p>
</div>
<div id="A1.SS1.p5" class="ltx_para">
<p id="A1.SS1.p5.1" class="ltx_p">For the answer elicitation study, we asked participants to: “Imagine you are browsing a {scenario} website when you encounter the following image. {image} Another user cannot see the image directly but has access to the following image description: {description}. Based on the description, they asked a follow-up question. Please answer based on the image. {question}.”</p>
</div>
<div id="A1.SS1.p6" class="ltx_para">
<p id="A1.SS1.p6.1" class="ltx_p">We integrated context by prompting models with the format:</p>
</div>
<div id="A1.SS1.p7" class="ltx_para">
<p id="A1.SS1.p7.1" class="ltx_p"><span id="A1.SS1.p7.1.1" class="ltx_text ltx_font_typewriter">Assume someone is browsing a {scenario} website when they encounter this image. They cannot see the image directly, but they can access this image description: {description}. Based on this description, they asked this follow-up question. Please answer based on the image. In your answer, prioritize details relevant to this person. Question: {question}</span></p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Additional Dataset Composition Analysis</h3>

<figure id="A1.F5" class="ltx_figure"><img src="/html/2402.15002/assets/x5.png" id="A1.F5.g1" class="ltx_graphics ltx_img_landscape" width="458" height="266" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overall named entity recognition analysis on answers written by participants. Error bars represent 95% confidence intervals.</figcaption>
</figure>
<section id="A1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Answer analysis of named entities</h4>

<div id="A1.SS2.SSS1.p1" class="ltx_para">
<p id="A1.SS2.SSS1.p1.1" class="ltx_p">As reported for the analysis of the questions (Section <a href="#S4.SS2" title="4.2 Analysis of Questions ‣ 4 Dataset Analysis ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we find that there is also significant variation in the number of named entities referenced in the answers. As Figure <a href="#A1.F5" title="Figure 5 ‣ A.2 Additional Dataset Composition Analysis ‣ Appendix A Appendix ‣ CommVQA: Situating Visual Question Answering in Communicative Contexts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates, most organizations and people are referenced in the news context, and the least are mentioned in the health and social media scenarios, highlighting how the contextual variance in the questions trickles down to the answer distributions.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2402.15002/assets/img/qualitative_example_fig.png" id="A1.F6.g1" class="ltx_graphics ltx_img_landscape" width="586" height="267" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example entries in the CommVQA dataset.</figcaption>
</figure>
</section>
<section id="A1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Answer analysis of polar questions</h4>

<div id="A1.SS2.SSS2.p1" class="ltx_para">
<p id="A1.SS2.SSS2.p1.1" class="ltx_p">We find that participants readily volunteer information that are not strictly necessary for answering the question directly. For this purpose, polar questions in VQA are an interesting case study, since for polar questions, only an answer of “Yes” or “No” is strictly required. However, in CommVQA, the average length of answers to polar questions was 52.1 characters. A direct string matching analysis revealed that only 0.01% of the answers to polar questions were simply “Yes” or “No,” indicating that most of the time participants provided further information. For example, the question “Is this something you think I could do with limited sight?” received the answer “You could possibly do this with limited sight, but I wouldn’t suggest it.” Prior work suggests that such strictly overinformative behavior is driven by considerations of what information is relevant to a questioner’s goals <cite class="ltx_cite ltx_citemacro_cite">Tsvilodub et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite>. Consequently, the fact that we’re seeing this behavior could again be attributed to the communicative setup of the task, and suggests that positioning VQA as such results in answer patterns that explicitly consider relevancy to a questioner’s goals.</p>
</div>
<div id="A1.SS2.SSS2.p2" class="ltx_para">
<p id="A1.SS2.SSS2.p2.1" class="ltx_p">However, an answer longer than “Yes” or “No” might be only restating what was in the question–e.g., if the question was “Are there any trees in the image?”, an answer might be, “Yes, there are trees in the image.” As an exploratory analysis, we prompted GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023a</a>)</cite> to classify whether the provided answer strictly adhered to the scope of the question, or whether it went beyond and offered additional details. Surprisingly, we found that out of the 528 polar questions in our dataset, only 34% stopped at providing enough information to answer the question, while the other 63% answers were classified as providing information beyond what was necessary to answer the question.</p>
</div>
<div id="A1.SS2.SSS2.p3" class="ltx_para">
<p id="A1.SS2.SSS2.p3.1" class="ltx_p">Of the answers to polar questions that over-informed, after we conducted further GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023a</a>)</cite> analysis, approximately 39% were describing visual details not directly asked about in the question (e.g., Question: Are there any other objects in the image? Answer: Small fish) while 60% were expressing uncertainty and making an educated guess or inference (e.g., Question: Is the athlete a professional? Answer: It’s not explicitly clear whether or not the athlete is a professional although one can infer that the athlete is an enthusiast who clearly trains for the sport and enjoys participating in competitions).</p>
</div>
</section>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Models Chosen and Parameters</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">For running model evaluation, we used the following model versions:</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">LlaVA: llava-v1.5-13b-3GB</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">IDEFICS: idefics-9b-instruct</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">mPLUG-Owl: mplug-owl-llama-7b.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">BLIP-2: blip2-opt-2.7b.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.15001" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.15002" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.15002">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.15002" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.15003" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 15:59:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
