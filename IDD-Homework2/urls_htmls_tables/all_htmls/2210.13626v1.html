<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.13626] VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge</title><meta property="og:description" content="There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work, we focus on questions that require commonsense …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.13626">

<!--Generated on Thu Mar 14 00:45:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VLC-BERT: Visual Question Answering
<br class="ltx_break">with Contextualized Commonsense Knowledge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sahithya Ravi<sup id="8.8.1" class="ltx_sup"><span id="8.8.1.1" class="ltx_text ltx_font_italic">1,2</span></sup>   Aditya Chinchure<sup id="9.9.2" class="ltx_sup"><span id="9.9.2.1" class="ltx_text ltx_font_italic">1,2∗</span></sup>   Leonid Sigal<sup id="10.10.3" class="ltx_sup"><span id="10.10.3.1" class="ltx_text ltx_font_italic">1,2</span></sup>    Renjie Liao<sup id="11.11.4" class="ltx_sup">1</sup>   Vered Shwartz<sup id="12.12.5" class="ltx_sup"><span id="12.12.5.1" class="ltx_text ltx_font_italic">1,2</span></sup> 
<br class="ltx_break"><sup id="13.13.6" class="ltx_sup">1</sup> University of British Columbia   <sup id="14.14.7" class="ltx_sup">2</sup> Vector Institute for AI
<br class="ltx_break"><span id="15.15.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{sahiravi, aditya10, lsigal, vshwartz}@cs.ubc.ca, rjliao@ece.ubc.ca </span>
</span><span class="ltx_author_notes">Denotes equal contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="16.1" class="ltx_p">There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work, we focus on questions that require commonsense reasoning. In contrast to previous methods which inject knowledge from static knowledge bases, we investigate the incorporation of contextualized knowledge using Commonsense Transformer (COMET), an existing knowledge model trained on human-curated knowledge bases. We propose a method to generate, select, and encode external commonsense knowledge alongside visual and textual cues in a new pre-trained Vision-Language-Commonsense transformer model, VLC-BERT. Through our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets, we show that VLC-BERT is capable of outperforming existing models that utilize static knowledge bases. Furthermore, through a detailed analysis, we explain which questions benefit, and which don’t, from contextualized commonsense knowledge from COMET. Code: <a target="_blank" href="https://github.com/aditya10/VLC-BERT" title="" class="ltx_ref ltx_href">https://github.com/aditya10/VLC-BERT</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent progress in multimodal vision-language learning has been fueled by large-scale annotated datasets for Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, in which models are presented with questions about an image. To answer questions correctly, models are required to perform scene understanding and learn meaningful connections between the two modalities. In recent years, transformer-based vision and language (VL) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, pre-trained on large-scale multimodal corpora, have reached impressive accuracies on standard VQA datasets.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA often necessitates not only visual comprehension of the scene depicted by the image (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, “A plate with meat, potatoes and bread”) but also making inferences about plausible stories behind the image (<em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.4" class="ltx_text"></span>, “The plate is likely found at a restaurant”). Humans make such inferences based on prior experience and commonsense knowledge (<em id="S1.p2.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.6" class="ltx_text"></span>, “This is likely a lunch or dinner at a restaurant, people may be enjoying themselves…”). Most existing methods rely on world knowledge implicitly encoded by language models, which often lacks in both accuracy and coverage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. This is primarily due to the fact that commonsense knowledge is extremely broad, and frequently assumed. Commonsense knowledge learned from text suffers from reporting bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>: over-representation of exceptional facts (<em id="S1.p2.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.8" class="ltx_text"></span>, “people die in accidents”) in text corpora, at the expense of rarely discussed trivial facts known to everyone (<em id="S1.p2.1.9" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.10" class="ltx_text"></span>, “people eat”).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2210.13626/assets/figures/intro_example.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="137" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">OK-VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>: Where might one buy this?</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Several visual question answering benchmarks were proposed, in which the questions require either factual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or commonsense knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> beyond the visual scene comprehension. This prompted the development of neurosymbolic methods combining transformer-based representations with knowledge bases (KBs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
However, retrieving relevant facts directly from a KB is challenging due to lack of coverage, and because KB facts are only appropriate in certain contexts.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we propose VLC-BERT (Vision-Language-Commonsense BERT), a model designed to incorporate contextualized commonsense knowledge into a Vision-Language transformer built on VL-BERT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. As an alternative to the retrieval paradigm often used in knowledge-based VQA, our model generates contextualized commonsense inferences on the question phrase combined with image object tags using COMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, a language model trained on commonsense knowledge graphs. We augment sentence transformers<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to rank, filter and embed the commonsense inferences. We incorporate the filtered inferences into VLC-BERT using an attention-driven fusion mechanism that learns to focus on the most important inferences for each question. Commonsense knowledge may not be necessary for answering every question, as some questions are either purely visual, factual, or straight-forward. To eliminate injecting noisy knowledge in such cases, we employ weak supervision to help us discriminate between situations when commonsense knowledge may or may not be valuable.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our evaluations on the challenging OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and A-OKVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> datasets confirm that leveraging commonsense is consistently useful for knowledge-intensive visual question answering tasks. We analyze the successful predictions and show how the commonsense inferences help answering difficult questions.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2210.13626/assets/x1.png" id="S1.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="230" height="198" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Overall architecture</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2210.13626/assets/x2.png" id="S1.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="102" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Knowledge generation and selection</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.9.4.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Architecture of VLC-BERT<span id="S1.F2.6.3.3" class="ltx_text ltx_font_medium">: Given an image, VLC-BERT generates commonsense inferences for the question-object phrase using COMET. These inferences are relevance ranked, and top ones are selected and fed along with image regions into a VL-Transformer in order to produce an answer. We utilize semantic similarity between <math id="S1.F2.4.1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S1.F2.4.1.1.m1.1b"><mi id="S1.F2.4.1.1.m1.1.1" xref="S1.F2.4.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S1.F2.4.1.1.m1.1c"><ci id="S1.F2.4.1.1.m1.1.1.cmml" xref="S1.F2.4.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.4.1.1.m1.1d">Q</annotation></semantics></math> and <math id="S1.F2.5.2.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S1.F2.5.2.2.m2.1b"><mi id="S1.F2.5.2.2.m2.1.1" xref="S1.F2.5.2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S1.F2.5.2.2.m2.1c"><ci id="S1.F2.5.2.2.m2.1.1.cmml" xref="S1.F2.5.2.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.5.2.2.m2.1d">C</annotation></semantics></math> to select the final <math id="S1.F2.6.3.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S1.F2.6.3.3.m3.1b"><mi id="S1.F2.6.3.3.m3.1.1" xref="S1.F2.6.3.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S1.F2.6.3.3.m3.1c"><ci id="S1.F2.6.3.3.m3.1.1.cmml" xref="S1.F2.6.3.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.6.3.3.m3.1d">K</annotation></semantics></math> inferences that go into VLC-BERT.</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision-Language Transformer Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Pre-trained Vision-Language models based on BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> have shown impressive performances on downstream multimodal tasks such as Visual Question Answering. ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> use a two-stream architecture to first encode language and vision modalities independently, and then apply a cross-modality encoder to align textual and visual tokens. VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, OSCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and OSCAR+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> use a single-stream architecture to directly learn inter-modality interactions. Large-scale pre-training is commonly done using the Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> dataset, with objectives that are designed to encourage interaction between modalities, such as predicting masked tokens or image regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and using contrastive loss between modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. As a result, such models inherently capture some commonsense knowledge through their pre-training regime. While these models perform impressively on downstream tasks such as VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, they typically perform worse on questions requiring reasoning about knowledge beyond the image content or involving multiple reasoning hops. In our work, we introduce VLC-BERT, a multimodal transformer model based on VL-BERT that explicitly incorporates external knowledge to alleviate this issue.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Knowledge-based Visual Question Answering</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In recent years, several VQA datasets were designed specifically to require reasoning about external knowledge beyond the image, whether using factual and web information (FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, WebQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, a provided text passage (VLQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>), commonsense-driven reasoning (VCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>), or external commonsense knowledge (OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, A-OKVQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>).
This motivated a line of work on knowledge-enhanced VL transformer models. External knowledge is typically retrieved from a structured knowledge base like ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, in the form of a subgraph, and integrated into the VL transformer as an additional input <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Alternative sources of knowledge include image captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Google Search results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and textual and visual knowledge from Wikipedia, and Google Images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. In contrast to most of the preceding work, PICa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and Knowledge Augmented Transformer (KAT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> attempt to use GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in a few-shot setting on the VQA task, by building prompts containing the caption and object tags generated using the image, followed by the question statement, asking the model to produce an answer. In our proposed model, we focus on a specific subset of the knowledge-intensive datasets that require commonsense knowledge. Our approach, that uses COMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, for incorporating commonsense knowledge is distinctly different, far simpler and more cost-effective.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Knowledge incorporation in NLP</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Structured large-scale knowledge bases (KBs) like ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and ATOMIC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> are widely used in NLP tasks to provide additional commonsense knowledge to models. ConceptNet contains 3.4M assertions focusing on concept and entity relations (such as RelatedTo, Synonym, IsA, MadeOf). ATOMIC contains 1.33M triplets focusing on event-centric social commonsense about causes, effects, mental states of the event participants.
Several approaches were proposed for incorporating symbolic knowledge from these KBs into downstream NLP tasks such as encoding subgraphs of relevant knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and pre-training on commonsense knowledge bases or tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
Despite the performance improvements, incorporating knowledge directly from KBs suffers from two limitations: lack of coverage and lack of consideration for context. Commonsense Transformer, COMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, attempts to alleviate these issues by fine-tuning pre-trained language models on KBs. COMET can generate inferences for the various KB relations dynamically for new inputs. It has been successfully used for generating knowledge in language tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Inspired by the success of these models, we chose to use COMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to generate relevant contextual expansions rather than directly retrieving knowledge from KBs. To the best of our knowledge, we are the first to incorporate commonsense knowledge using COMET in VQA tasks.
Newer COMET variants<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> are less applicable to OK-VQA and A-OKVQA as they focus more on event commonsense than entities.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.11" class="ltx_p">We briefly outline the overall architecture of our model and then delve deeper into its individual components. Figure <a href="#S1.F2.sf1" title="In Figure 2 ‣ 1 Introduction ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> illustrates the VLC-BERT pipeline. Given an image with corresponding image regions <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">I</annotation></semantics></math> precomputed using Fast RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and a question <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">Q</annotation></semantics></math> related to the image, we generate commonsense inferences <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">C</annotation></semantics></math> on the events and entities in the question phrase and two object tags <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">O</annotation></semantics></math>, and select the set of commonsense inferences which is the most useful for answering the question, <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">C</annotation></semantics></math> = {<math id="S3.p1.6.m6.4" class="ltx_Math" alttext="C_{1},C_{2},...,C_{k}" display="inline"><semantics id="S3.p1.6.m6.4a"><mrow id="S3.p1.6.m6.4.4.3" xref="S3.p1.6.m6.4.4.4.cmml"><msub id="S3.p1.6.m6.2.2.1.1" xref="S3.p1.6.m6.2.2.1.1.cmml"><mi id="S3.p1.6.m6.2.2.1.1.2" xref="S3.p1.6.m6.2.2.1.1.2.cmml">C</mi><mn id="S3.p1.6.m6.2.2.1.1.3" xref="S3.p1.6.m6.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.p1.6.m6.4.4.3.4" xref="S3.p1.6.m6.4.4.4.cmml">,</mo><msub id="S3.p1.6.m6.3.3.2.2" xref="S3.p1.6.m6.3.3.2.2.cmml"><mi id="S3.p1.6.m6.3.3.2.2.2" xref="S3.p1.6.m6.3.3.2.2.2.cmml">C</mi><mn id="S3.p1.6.m6.3.3.2.2.3" xref="S3.p1.6.m6.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.p1.6.m6.4.4.3.5" xref="S3.p1.6.m6.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">…</mi><mo id="S3.p1.6.m6.4.4.3.6" xref="S3.p1.6.m6.4.4.4.cmml">,</mo><msub id="S3.p1.6.m6.4.4.3.3" xref="S3.p1.6.m6.4.4.3.3.cmml"><mi id="S3.p1.6.m6.4.4.3.3.2" xref="S3.p1.6.m6.4.4.3.3.2.cmml">C</mi><mi id="S3.p1.6.m6.4.4.3.3.3" xref="S3.p1.6.m6.4.4.3.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.4b"><list id="S3.p1.6.m6.4.4.4.cmml" xref="S3.p1.6.m6.4.4.3"><apply id="S3.p1.6.m6.2.2.1.1.cmml" xref="S3.p1.6.m6.2.2.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.2.2.1.1.1.cmml" xref="S3.p1.6.m6.2.2.1.1">subscript</csymbol><ci id="S3.p1.6.m6.2.2.1.1.2.cmml" xref="S3.p1.6.m6.2.2.1.1.2">𝐶</ci><cn type="integer" id="S3.p1.6.m6.2.2.1.1.3.cmml" xref="S3.p1.6.m6.2.2.1.1.3">1</cn></apply><apply id="S3.p1.6.m6.3.3.2.2.cmml" xref="S3.p1.6.m6.3.3.2.2"><csymbol cd="ambiguous" id="S3.p1.6.m6.3.3.2.2.1.cmml" xref="S3.p1.6.m6.3.3.2.2">subscript</csymbol><ci id="S3.p1.6.m6.3.3.2.2.2.cmml" xref="S3.p1.6.m6.3.3.2.2.2">𝐶</ci><cn type="integer" id="S3.p1.6.m6.3.3.2.2.3.cmml" xref="S3.p1.6.m6.3.3.2.2.3">2</cn></apply><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">…</ci><apply id="S3.p1.6.m6.4.4.3.3.cmml" xref="S3.p1.6.m6.4.4.3.3"><csymbol cd="ambiguous" id="S3.p1.6.m6.4.4.3.3.1.cmml" xref="S3.p1.6.m6.4.4.3.3">subscript</csymbol><ci id="S3.p1.6.m6.4.4.3.3.2.cmml" xref="S3.p1.6.m6.4.4.3.3.2">𝐶</ci><ci id="S3.p1.6.m6.4.4.3.3.3.cmml" xref="S3.p1.6.m6.4.4.3.3.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.4c">C_{1},C_{2},...,C_{k}</annotation></semantics></math>} (§<a href="#S3.SS1" title="3.1 Structured knowledge generation and selection ‣ 3 Method ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Finally, we embed <math id="S3.p1.7.m7.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.7.m7.1a"><mi id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">Q</annotation></semantics></math>, <math id="S3.p1.8.m8.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.8.m8.1a"><mi id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">I</annotation></semantics></math> and <math id="S3.p1.9.m9.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p1.9.m9.1a"><mi id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><ci id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">C</annotation></semantics></math>, as input to VLC-BERT and train it to predict an answer <math id="S3.p1.10.m10.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p1.10.m10.1a"><mi id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">A</annotation></semantics></math> to <math id="S3.p1.11.m11.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.11.m11.1a"><mi id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><ci id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">Q</annotation></semantics></math> (§<a href="#S3.SS2" title="3.2 VLC-BERT ‣ 3 Method ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Structured knowledge generation and selection</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Knowledge Generation</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">To generate commonsense knowledge, we employ the most recent version of COMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> initialized using BART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> in a zero-shot setting. COMET is trained to complete 50 relation types from both ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> (such as AtLocation, Madeof) and ATOMIC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> (such as xNeed, xWants), thus capturing concept as well as event oriented knowledge. We generate inferences based on 30 relation types most relevant to our work and supported by COMET.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We include the full list of relation types in the supplementary material.</span></span></span>Consider the example shown in Figure <a href="#S1.F2.sf2" title="In Figure 2 ‣ 1 Introduction ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. For the given question, <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">“What is the purpose of the umbrella?”</span> we first process each question using AllenNLP’s constituency parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and convert it into a declarative sentence, since COMET was mainly trained on declarative sentences. In the example shown, <span id="S3.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">“What is the purpose of the umbrella?”</span> is rephrased as <span id="S3.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_italic">“The purpose of the umbrellas is”</span>. We then adopt a state-of-the-art object detection model, YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, to translate the corresponding image into object tags that COMET can understand. We select the top two most confident object tags and combine it with the question phrase to obtain a question-object(QO) phrase, <span id="S3.SS1.SSS1.p1.1.4" class="ltx_text ltx_font_italic">“The purpose of the umbrella is, with dog and chair”</span>. We restrict the number of the object tags used in COMET’s input to two because the addition of multiple tags make the inferences more conflated and noisy. In this manner, we can obtain inferences that can provide additional knowledge about both the visual and language inputs to VLC-BERT.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.2" class="ltx_p">We use beam search to decode the top 5 inferences for each relation type, ranked according to the model’s confidence. Overall, we get <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="30\times 5=150" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mrow id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mrow id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml"><mn id="S3.SS1.SSS1.p2.1.m1.1.1.2.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.2.cmml">30</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p2.1.m1.1.1.2.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.1.cmml">×</mo><mn id="S3.SS1.SSS1.p2.1.m1.1.1.2.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.3.cmml">5</mn></mrow><mo id="S3.SS1.SSS1.p2.1.m1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml">150</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><eq id="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1"></eq><apply id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2"><times id="S3.SS1.SSS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS1.SSS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.2">30</cn><cn type="integer" id="S3.SS1.SSS1.p2.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.3">5</cn></apply><cn type="integer" id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3">150</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">30\times 5=150</annotation></semantics></math> inferences for each input phrase. Finally, we convert each inference to a sentence in natural language using relation-specific templates as defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In the shown example, the assertion <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_math_unparsed" alttext="&lt;umbrella,\textit{Located At},store&gt;" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mrow id="S3.SS1.SSS1.p2.2.m2.1b"><mo id="S3.SS1.SSS1.p2.2.m2.1.2">&lt;</mo><mi id="S3.SS1.SSS1.p2.2.m2.1.3">u</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.4">m</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.5">b</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.6">r</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.7">e</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.8">l</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.9">l</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.10">a</mi><mo id="S3.SS1.SSS1.p2.2.m2.1.11">,</mo><mtext class="ltx_mathvariant_italic" id="S3.SS1.SSS1.p2.2.m2.1.1">Located At</mtext><mo id="S3.SS1.SSS1.p2.2.m2.1.12">,</mo><mi id="S3.SS1.SSS1.p2.2.m2.1.13">s</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.14">t</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.15">o</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.16">r</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.17">e</mi><mo id="S3.SS1.SSS1.p2.2.m2.1.18">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">&lt;umbrella,\textit{Located At},store&gt;</annotation></semantics></math> is expressed as “You are likely to find umbrella at store”. In order to remove redundant sentences of the same relation type, we measure the lexical overlap by measuring the percentage of common words between two given sentences. We exclude the sentences which have more than 70% overlap with previously constructed sentences of the same relation.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Knowledge Selection</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.3" class="ltx_p">Due to the high cost of computation, and the noise associated with feeding such a large number of text tokens, feeding up to 150 COMET inferences into the VL Transformer model is impractical. In order to rank and select the inferences, we employ semantic search based on sentence transformers (SBERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which are pre-trained on tasks that retrieve candidate answers to a search query. In this method, the question and the inferences are embedded into the same vector space using SBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and cosine similarity between the question and the inference embeddings is used to rank the inferences. We prune the set of inference sentences <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mi id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><ci id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">C</annotation></semantics></math> by picking <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="K=5" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mrow id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">K</mi><mo id="S3.SS1.SSS2.p1.2.m2.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><eq id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1"></eq><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">𝐾</ci><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">K=5</annotation></semantics></math> inferences which are expected to be the most useful for answering the question <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mi id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><ci id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">Q</annotation></semantics></math>.</p>
</div>
<section id="S3.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Augmented-SBERT</h5>

<div id="S3.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS2.Px1.p1.1" class="ltx_p">We augment the SBERT used for semantic search by starting with a pre-trained SBERT model and continuing to train it for 2 epochs on question-inference instances from the <em id="S3.SS1.SSS2.Px1.p1.1.1" class="ltx_emph ltx_font_italic">training set</em> of our datasets. To achieve this, we label the inferences for each question with similarity scores based on the proportion of overlap with the human-annotated answers. Since SBERT is trained on corpora that are distinct from our task, the augmentation ensures that the model understands the nature of query-inference pairings in our tasks. The augmented SBERT especially helps with narrowing down the right relations to the question. For instance, the question in shown in Figure <a href="#S1.F2.sf2" title="In Figure 2 ‣ 1 Introduction ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> benefits most from the relations that talk about what the umbrella (<span id="S3.SS1.SSS2.Px1.p1.1.2" class="ltx_text ltx_font_italic">UsedFor</span>) is used for or capable of (<span id="S3.SS1.SSS2.Px1.p1.1.3" class="ltx_text ltx_font_italic">CapableOf</span>.)</p>
</div>
</section>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>VLC-BERT</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2210.13626/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="317" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.5.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VLC-BERT Transformer<span id="S3.F3.2.1.1" class="ltx_text ltx_font_medium"> is a single-stream Transformer that can attend across language, vision, and commonsense representations. We use the <math id="S3.F3.2.1.1.m1.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S3.F3.2.1.1.m1.1b"><mi id="S3.F3.2.1.1.m1.1.1" xref="S3.F3.2.1.1.m1.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.1.m1.1c"><ci id="S3.F3.2.1.1.m1.1.1.cmml" xref="S3.F3.2.1.1.m1.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.1.m1.1d">\operatorname{MHA}</annotation></semantics></math> block to fuse commonsense inferences into a useful commonsense representation.</span></span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use a single-stream multimodal transformer encoder, VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, as the basis of VLC-BERT. VL-BERT is pre-trained on large-scale vision-language and language-only datasets with a goal of aligning the visual and linguistic features and building robust multimodal representations for downstream tasks. It is trained on the vision-language Conceptual Captions dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, to predict regions-of-interests (RoIs) from language cues, and on the language-only BookCorpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and English Wikipedia corpora, with a masked language modeling objective.
Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 VLC-BERT ‣ 3 Method ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the VLC-BERT Transformer architecture. In the following paragraphs, we share how the input sequence is constructed and how the predicted answer is selected.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Inputs</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.7" class="ltx_p">Like VL-BERT, VLC-BERT accepts word token embeddings for language inputs and RoI token embeddings from the image for vision inputs. The architecture of VLC-BERT Transformer is shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 VLC-BERT ‣ 3 Method ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We use the  <svg id="S3.SS2.SSS1.p1.1.pic1" class="ltx_picture" height="11.07" overflow="visible" version="1.1" width="27.21"><g transform="translate(0,11.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 5.16 C 0 8.43 2.64 11.07 5.91 11.07 L 21.31 11.07 C 24.57 11.07 27.21 8.43 27.21 5.16 L 27.21 5.91 C 27.21 2.64 24.57 0 21.31 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 5.16 C 1.97 7.34 3.73 9.1 5.91 9.1 L 21.31 9.1 C 23.48 9.1 25.24 7.34 25.24 5.16 L 25.24 5.91 C 25.24 3.73 23.48 1.97 21.31 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 2.77)"><foreignObject width="27.21" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS2.SSS1.p1.1.pic1.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">[CLS]</span></foreignObject></g></g></svg> in the beginning of the sequence,  <svg id="S3.SS2.SSS1.p1.2.pic2" class="ltx_picture" height="11.07" overflow="visible" version="1.1" width="30.44"><g transform="translate(0,11.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 5.16 C 0 8.43 2.64 11.07 5.91 11.07 L 24.54 11.07 C 27.8 11.07 30.44 8.43 30.44 5.16 L 30.44 5.91 C 30.44 2.64 27.8 0 24.54 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 5.16 C 1.97 7.34 3.73 9.1 5.91 9.1 L 24.54 9.1 C 26.71 9.1 28.47 7.34 28.47 5.16 L 28.47 5.91 C 28.47 3.73 26.71 1.97 24.54 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 2.77)"><foreignObject width="30.44" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS2.SSS1.p1.2.pic2.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">[END]</span></foreignObject></g></g></svg> to mark the end of the sequence, and the separator token  <svg id="S3.SS2.SSS1.p1.3.pic3" class="ltx_picture" height="11.07" overflow="visible" version="1.1" width="27.37"><g transform="translate(0,11.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 5.16 C 0 8.43 2.64 11.07 5.91 11.07 L 21.46 11.07 C 24.72 11.07 27.37 8.43 27.37 5.16 L 27.37 5.91 C 27.37 2.64 24.72 0 21.46 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 5.16 C 1.97 7.34 3.73 9.1 5.91 9.1 L 21.46 9.1 C 23.64 9.1 25.4 7.34 25.4 5.16 L 25.4 5.91 C 25.4 3.73 23.64 1.97 21.46 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 2.77)"><foreignObject width="27.37" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS2.SSS1.p1.3.pic3.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">[SEP]</span></foreignObject></g></g></svg> between different inputs. We feed the question <math id="S3.SS2.SSS1.p1.4.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m1.1a"><mi id="S3.SS2.SSS1.p1.4.m1.1.1" xref="S3.SS2.SSS1.p1.4.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m1.1b"><ci id="S3.SS2.SSS1.p1.4.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m1.1c">Q</annotation></semantics></math> as a sequence of word tokens and the image regions <math id="S3.SS2.SSS1.p1.5.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m2.1a"><mi id="S3.SS2.SSS1.p1.5.m2.1.1" xref="S3.SS2.SSS1.p1.5.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m2.1b"><ci id="S3.SS2.SSS1.p1.5.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m2.1c">I</annotation></semantics></math> as sequences of RoIs. A  <svg id="S3.SS2.SSS1.p1.6.pic4" class="ltx_picture" height="11.07" overflow="visible" version="1.1" width="39.36"><g transform="translate(0,11.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 5.16 C 0 8.43 2.64 11.07 5.91 11.07 L 33.45 11.07 C 36.71 11.07 39.36 8.43 39.36 5.16 L 39.36 5.91 C 39.36 2.64 36.71 0 33.45 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 5.16 C 1.97 7.34 3.73 9.1 5.91 9.1 L 33.45 9.1 C 35.63 9.1 37.39 7.34 37.39 5.16 L 37.39 5.91 C 37.39 3.73 35.63 1.97 33.45 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 2.77)"><foreignObject width="39.36" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS2.SSS1.p1.6.pic4.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">[MASK]</span></foreignObject></g></g></svg> token is used to represent the unknown answer. In addition, we introduce a commonsense fusion token, <math id="S3.SS2.SSS1.p1.7.m3.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.SSS1.p1.7.m3.1a"><mi id="S3.SS2.SSS1.p1.7.m3.1.1" xref="S3.SS2.SSS1.p1.7.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m3.1b"><ci id="S3.SS2.SSS1.p1.7.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m3.1c">F</annotation></semantics></math>, to the input sequence, to incorporate our commonsense inferences.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.4" class="ltx_p">A straightforward way to leverage the commonsense inferences <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">C</annotation></semantics></math> = {<math id="S3.SS2.SSS1.p2.2.m2.4" class="ltx_Math" alttext="C_{1},C_{2},...,C_{k}" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.4a"><mrow id="S3.SS2.SSS1.p2.2.m2.4.4.3" xref="S3.SS2.SSS1.p2.2.m2.4.4.4.cmml"><msub id="S3.SS2.SSS1.p2.2.m2.2.2.1.1" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2.cmml">C</mi><mn id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.3" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS1.p2.2.m2.4.4.3.4" xref="S3.SS2.SSS1.p2.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS2.SSS1.p2.2.m2.3.3.2.2" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.3.3.2.2.2" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2.2.cmml">C</mi><mn id="S3.SS2.SSS1.p2.2.m2.3.3.2.2.3" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS1.p2.2.m2.4.4.3.5" xref="S3.SS2.SSS1.p2.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">…</mi><mo id="S3.SS2.SSS1.p2.2.m2.4.4.3.6" xref="S3.SS2.SSS1.p2.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS2.SSS1.p2.2.m2.4.4.3.3" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.4.4.3.3.2" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3.2.cmml">C</mi><mi id="S3.SS2.SSS1.p2.2.m2.4.4.3.3.3" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.4b"><list id="S3.SS2.SSS1.p2.2.m2.4.4.4.cmml" xref="S3.SS2.SSS1.p2.2.m2.4.4.3"><apply id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS2.SSS1.p2.2.m2.3.3.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.2.m2.3.3.2.2.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2.2">𝐶</ci><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.3.3.2.2.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">…</ci><apply id="S3.SS2.SSS1.p2.2.m2.4.4.3.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.2.m2.4.4.3.3.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS2.SSS1.p2.2.m2.4.4.3.3.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3.2">𝐶</ci><ci id="S3.SS2.SSS1.p2.2.m2.4.4.3.3.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.4.4.3.3.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.4c">C_{1},C_{2},...,C_{k}</annotation></semantics></math>} is to embed each word token in every inference sentence as an input token. However, this would lead to a very long input sequence, where the majority of inputs consist of inferences, thus potentially drawing the model’s attention away from the other inputs. To overcome the challenge, we summarize the information contained in each inference sentence <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><msub id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p2.3.m3.1.1.2" xref="S3.SS2.SSS1.p2.3.m3.1.1.2.cmml">C</mi><mi id="S3.SS2.SSS1.p2.3.m3.1.1.3" xref="S3.SS2.SSS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><apply id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1.2">𝐶</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">C_{i}</annotation></semantics></math> into a single token representation <math id="S3.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\vec{C_{i}}" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m4.1a"><mover accent="true" id="S3.SS2.SSS1.p2.4.m4.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.cmml"><msub id="S3.SS2.SSS1.p2.4.m4.1.1.2" xref="S3.SS2.SSS1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS1.p2.4.m4.1.1.2.2" xref="S3.SS2.SSS1.p2.4.m4.1.1.2.2.cmml">C</mi><mi id="S3.SS2.SSS1.p2.4.m4.1.1.2.3" xref="S3.SS2.SSS1.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.SSS1.p2.4.m4.1.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.1b"><apply id="S3.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1"><ci id="S3.SS2.SSS1.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1">→</ci><apply id="S3.SS2.SSS1.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.2.2">𝐶</ci><ci id="S3.SS2.SSS1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.1c">\vec{C_{i}}</annotation></semantics></math>, by embedding the inference using SBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>:</p>
<table id="S9.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\displaystyle\vec{C_{i}}=\operatorname{SBERT}(C_{i})" display="inline"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mover accent="true" id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml"><msub id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.3.2.cmml"><mi id="S3.E1.m1.2.2.3.2.2" xref="S3.E1.m1.2.2.3.2.2.cmml">C</mi><mi id="S3.E1.m1.2.2.3.2.3" xref="S3.E1.m1.2.2.3.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.3.1.cmml">→</mo></mover><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.2.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">SBERT</mi><mo id="S3.E1.m1.2.2.1.1a" xref="S3.E1.m1.2.2.1.2.cmml">⁡</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.2.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">C</mi><mi id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"><ci id="S3.E1.m1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.3.1">→</ci><apply id="S3.E1.m1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.2.1.cmml" xref="S3.E1.m1.2.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.3.2.2.cmml" xref="S3.E1.m1.2.2.3.2.2">𝐶</ci><ci id="S3.E1.m1.2.2.3.2.3.cmml" xref="S3.E1.m1.2.2.3.2.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">SBERT</ci><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">𝐶</ci><ci id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\displaystyle\vec{C_{i}}=\operatorname{SBERT}(C_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p3.6" class="ltx_p">Next, in order to obtain a fused representation of the <math id="S3.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><mi id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><ci id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">k</annotation></semantics></math> commonsense inferences, we attend to the corresponding SBERT embeddings, <math id="S3.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="[\vec{C_{i}}...\vec{C_{k}}]" display="inline"><semantics id="S3.SS2.SSS1.p3.2.m2.1a"><mrow id="S3.SS2.SSS1.p3.2.m2.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.1.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.1.cmml">[</mo><mrow id="S3.SS2.SSS1.p3.2.m2.1.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.cmml"><msub id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.2.cmml">C</mi><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.1.cmml">→</mo></mover><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1a" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.cmml">​</mo><mover accent="true" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.cmml"><msub id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.2.cmml">C</mi><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.1.cmml">→</mo></mover></mrow><mo stretchy="false" id="S3.SS2.SSS1.p3.2.m2.1.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.1b"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1"><times id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1"></times><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2"><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.1">→</ci><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.2">𝐶</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.2.2.3">𝑖</ci></apply></apply><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.3">…</ci><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4"><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.1">→</ci><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.2">𝐶</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.4.2.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.1c">[\vec{C_{i}}...\vec{C_{k}}]</annotation></semantics></math> against the SBERT embedding of the question, <math id="S3.SS2.SSS1.p3.3.m3.2" class="ltx_Math" alttext="\vec{Q}=\operatorname{SBERT}(Q)" display="inline"><semantics id="S3.SS2.SSS1.p3.3.m3.2a"><mrow id="S3.SS2.SSS1.p3.3.m3.2.3" xref="S3.SS2.SSS1.p3.3.m3.2.3.cmml"><mover accent="true" id="S3.SS2.SSS1.p3.3.m3.2.3.2" xref="S3.SS2.SSS1.p3.3.m3.2.3.2.cmml"><mi id="S3.SS2.SSS1.p3.3.m3.2.3.2.2" xref="S3.SS2.SSS1.p3.3.m3.2.3.2.2.cmml">Q</mi><mo stretchy="false" id="S3.SS2.SSS1.p3.3.m3.2.3.2.1" xref="S3.SS2.SSS1.p3.3.m3.2.3.2.1.cmml">→</mo></mover><mo id="S3.SS2.SSS1.p3.3.m3.2.3.1" xref="S3.SS2.SSS1.p3.3.m3.2.3.1.cmml">=</mo><mrow id="S3.SS2.SSS1.p3.3.m3.2.3.3.2" xref="S3.SS2.SSS1.p3.3.m3.2.3.3.1.cmml"><mi id="S3.SS2.SSS1.p3.3.m3.1.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.cmml">SBERT</mi><mo id="S3.SS2.SSS1.p3.3.m3.2.3.3.2a" xref="S3.SS2.SSS1.p3.3.m3.2.3.3.1.cmml">⁡</mo><mrow id="S3.SS2.SSS1.p3.3.m3.2.3.3.2.1" xref="S3.SS2.SSS1.p3.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p3.3.m3.2.3.3.2.1.1" xref="S3.SS2.SSS1.p3.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS2.SSS1.p3.3.m3.2.2" xref="S3.SS2.SSS1.p3.3.m3.2.2.cmml">Q</mi><mo stretchy="false" id="S3.SS2.SSS1.p3.3.m3.2.3.3.2.1.2" xref="S3.SS2.SSS1.p3.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.3.m3.2b"><apply id="S3.SS2.SSS1.p3.3.m3.2.3.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.3"><eq id="S3.SS2.SSS1.p3.3.m3.2.3.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.3.1"></eq><apply id="S3.SS2.SSS1.p3.3.m3.2.3.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.3.2"><ci id="S3.SS2.SSS1.p3.3.m3.2.3.2.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.3.2.1">→</ci><ci id="S3.SS2.SSS1.p3.3.m3.2.3.2.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.3.2.2">𝑄</ci></apply><apply id="S3.SS2.SSS1.p3.3.m3.2.3.3.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.3.3.2"><ci id="S3.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1">SBERT</ci><ci id="S3.SS2.SSS1.p3.3.m3.2.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.2.2">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.3.m3.2c">\vec{Q}=\operatorname{SBERT}(Q)</annotation></semantics></math>. The intuition behind this approach is that the model learns to assign a higher score to the most important inference to the question. The key (<math id="S3.SS2.SSS1.p3.4.m4.1" class="ltx_Math" alttext="K_{A}" display="inline"><semantics id="S3.SS2.SSS1.p3.4.m4.1a"><msub id="S3.SS2.SSS1.p3.4.m4.1.1" xref="S3.SS2.SSS1.p3.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p3.4.m4.1.1.2" xref="S3.SS2.SSS1.p3.4.m4.1.1.2.cmml">K</mi><mi id="S3.SS2.SSS1.p3.4.m4.1.1.3" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.4.m4.1b"><apply id="S3.SS2.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.2">𝐾</ci><ci id="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.4.m4.1c">K_{A}</annotation></semantics></math>), query (<math id="S3.SS2.SSS1.p3.5.m5.1" class="ltx_Math" alttext="Q_{A}" display="inline"><semantics id="S3.SS2.SSS1.p3.5.m5.1a"><msub id="S3.SS2.SSS1.p3.5.m5.1.1" xref="S3.SS2.SSS1.p3.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p3.5.m5.1.1.2" xref="S3.SS2.SSS1.p3.5.m5.1.1.2.cmml">Q</mi><mi id="S3.SS2.SSS1.p3.5.m5.1.1.3" xref="S3.SS2.SSS1.p3.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.5.m5.1b"><apply id="S3.SS2.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.2">𝑄</ci><ci id="S3.SS2.SSS1.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.5.m5.1c">Q_{A}</annotation></semantics></math>) and value (<math id="S3.SS2.SSS1.p3.6.m6.1" class="ltx_Math" alttext="V_{A}" display="inline"><semantics id="S3.SS2.SSS1.p3.6.m6.1a"><msub id="S3.SS2.SSS1.p3.6.m6.1.1" xref="S3.SS2.SSS1.p3.6.m6.1.1.cmml"><mi id="S3.SS2.SSS1.p3.6.m6.1.1.2" xref="S3.SS2.SSS1.p3.6.m6.1.1.2.cmml">V</mi><mi id="S3.SS2.SSS1.p3.6.m6.1.1.3" xref="S3.SS2.SSS1.p3.6.m6.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.6.m6.1b"><apply id="S3.SS2.SSS1.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1.2">𝑉</ci><ci id="S3.SS2.SSS1.p3.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.6.m6.1c">V_{A}</annotation></semantics></math>) are assigned as shown below,</p>
<table id="S9.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle K_{A}" display="inline"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">K</mi><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝐾</ci><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle K_{A}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.1" class="ltx_Math" alttext="\displaystyle=\vec{Q}" display="inline"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mi id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml"></mi><mo id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.cmml">=</mo><mover accent="true" id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml"><mi id="S3.E2.m2.1.1.3.2" xref="S3.E2.m2.1.1.3.2.cmml">Q</mi><mo stretchy="false" id="S3.E2.m2.1.1.3.1" xref="S3.E2.m2.1.1.3.1.cmml">→</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><eq id="S3.E2.m2.1.1.1.cmml" xref="S3.E2.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2">absent</csymbol><apply id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3"><ci id="S3.E2.m2.1.1.3.1.cmml" xref="S3.E2.m2.1.1.3.1">→</ci><ci id="S3.E2.m2.1.1.3.2.cmml" xref="S3.E2.m2.1.1.3.2">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\vec{Q}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\displaystyle Q_{A},V_{A}" display="inline"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.3.cmml"><msub id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">Q</mi><mi id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.3.cmml">,</mo><msub id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.cmml">V</mi><mi id="S3.E3.m1.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.3.cmml">A</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><list id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.2"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2">𝑄</ci><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">𝐴</ci></apply><apply id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2">𝑉</ci><ci id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.3">𝐴</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\displaystyle Q_{A},V_{A}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.3" class="ltx_Math" alttext="\displaystyle=\operatorname{append}([\vec{C_{i}}...\vec{C_{k}}],\vec{Q})" display="inline"><semantics id="S3.E3.m2.3a"><mrow id="S3.E3.m2.3.3" xref="S3.E3.m2.3.3.cmml"><mi id="S3.E3.m2.3.3.3" xref="S3.E3.m2.3.3.3.cmml"></mi><mo id="S3.E3.m2.3.3.2" xref="S3.E3.m2.3.3.2.cmml">=</mo><mrow id="S3.E3.m2.3.3.1.1" xref="S3.E3.m2.3.3.1.2.cmml"><mi id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml">append</mi><mo id="S3.E3.m2.3.3.1.1a" xref="S3.E3.m2.3.3.1.2.cmml">⁡</mo><mrow id="S3.E3.m2.3.3.1.1.1" xref="S3.E3.m2.3.3.1.2.cmml"><mo stretchy="false" id="S3.E3.m2.3.3.1.1.1.2" xref="S3.E3.m2.3.3.1.2.cmml">(</mo><mrow id="S3.E3.m2.3.3.1.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m2.3.3.1.1.1.1.1.2" xref="S3.E3.m2.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E3.m2.3.3.1.1.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m2.3.3.1.1.1.1.1.1.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.cmml"><msub id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.cmml">C</mi><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m2.3.3.1.1.1.1.1.1.2.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.1.cmml">→</mo></mover><mo lspace="0em" rspace="0em" id="S3.E3.m2.3.3.1.1.1.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.E3.m2.3.3.1.1.1.1.1.1.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.E3.m2.3.3.1.1.1.1.1.1.1a" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.cmml">​</mo><mover accent="true" id="S3.E3.m2.3.3.1.1.1.1.1.1.4" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.cmml"><msub id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.cmml"><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2.cmml">C</mi><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E3.m2.3.3.1.1.1.1.1.1.4.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.1.cmml">→</mo></mover></mrow><mo stretchy="false" id="S3.E3.m2.3.3.1.1.1.1.1.3" xref="S3.E3.m2.3.3.1.1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E3.m2.3.3.1.1.1.3" xref="S3.E3.m2.3.3.1.2.cmml">,</mo><mover accent="true" id="S3.E3.m2.2.2" xref="S3.E3.m2.2.2.cmml"><mi id="S3.E3.m2.2.2.2" xref="S3.E3.m2.2.2.2.cmml">Q</mi><mo stretchy="false" id="S3.E3.m2.2.2.1" xref="S3.E3.m2.2.2.1.cmml">→</mo></mover><mo stretchy="false" id="S3.E3.m2.3.3.1.1.1.4" xref="S3.E3.m2.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.3b"><apply id="S3.E3.m2.3.3.cmml" xref="S3.E3.m2.3.3"><eq id="S3.E3.m2.3.3.2.cmml" xref="S3.E3.m2.3.3.2"></eq><csymbol cd="latexml" id="S3.E3.m2.3.3.3.cmml" xref="S3.E3.m2.3.3.3">absent</csymbol><apply id="S3.E3.m2.3.3.1.2.cmml" xref="S3.E3.m2.3.3.1.1"><ci id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1">append</ci><apply id="S3.E3.m2.3.3.1.1.1.1.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m2.3.3.1.1.1.1.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1"><times id="S3.E3.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1"></times><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2"><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.1">→</ci><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2">𝐶</ci><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></apply><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.3">…</ci><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4"><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.4.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.1">→</ci><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2">subscript</csymbol><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2">𝐶</ci><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3">𝑘</ci></apply></apply></apply></apply><apply id="S3.E3.m2.2.2.cmml" xref="S3.E3.m2.2.2"><ci id="S3.E3.m2.2.2.1.cmml" xref="S3.E3.m2.2.2.1">→</ci><ci id="S3.E3.m2.2.2.2.cmml" xref="S3.E3.m2.2.2.2">𝑄</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.3c">\displaystyle=\operatorname{append}([\vec{C_{i}}...\vec{C_{k}}],\vec{Q})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle\vec{F}" display="inline"><semantics id="S3.E4.m1.1a"><mover accent="true" id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">F</mi><mo stretchy="false" id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><ci id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1">→</ci><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle\vec{F}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.4" class="ltx_Math" alttext="\displaystyle=\operatorname{MHA}(K_{A},Q_{A},V_{A})" display="inline"><semantics id="S3.E4.m2.4a"><mrow id="S3.E4.m2.4.4" xref="S3.E4.m2.4.4.cmml"><mi id="S3.E4.m2.4.4.5" xref="S3.E4.m2.4.4.5.cmml"></mi><mo id="S3.E4.m2.4.4.4" xref="S3.E4.m2.4.4.4.cmml">=</mo><mrow id="S3.E4.m2.4.4.3.3" xref="S3.E4.m2.4.4.3.4.cmml"><mi id="S3.E4.m2.1.1" xref="S3.E4.m2.1.1.cmml">MHA</mi><mo id="S3.E4.m2.4.4.3.3a" xref="S3.E4.m2.4.4.3.4.cmml">⁡</mo><mrow id="S3.E4.m2.4.4.3.3.3" xref="S3.E4.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.E4.m2.4.4.3.3.3.4" xref="S3.E4.m2.4.4.3.4.cmml">(</mo><msub id="S3.E4.m2.2.2.1.1.1.1" xref="S3.E4.m2.2.2.1.1.1.1.cmml"><mi id="S3.E4.m2.2.2.1.1.1.1.2" xref="S3.E4.m2.2.2.1.1.1.1.2.cmml">K</mi><mi id="S3.E4.m2.2.2.1.1.1.1.3" xref="S3.E4.m2.2.2.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.E4.m2.4.4.3.3.3.5" xref="S3.E4.m2.4.4.3.4.cmml">,</mo><msub id="S3.E4.m2.3.3.2.2.2.2" xref="S3.E4.m2.3.3.2.2.2.2.cmml"><mi id="S3.E4.m2.3.3.2.2.2.2.2" xref="S3.E4.m2.3.3.2.2.2.2.2.cmml">Q</mi><mi id="S3.E4.m2.3.3.2.2.2.2.3" xref="S3.E4.m2.3.3.2.2.2.2.3.cmml">A</mi></msub><mo id="S3.E4.m2.4.4.3.3.3.6" xref="S3.E4.m2.4.4.3.4.cmml">,</mo><msub id="S3.E4.m2.4.4.3.3.3.3" xref="S3.E4.m2.4.4.3.3.3.3.cmml"><mi id="S3.E4.m2.4.4.3.3.3.3.2" xref="S3.E4.m2.4.4.3.3.3.3.2.cmml">V</mi><mi id="S3.E4.m2.4.4.3.3.3.3.3" xref="S3.E4.m2.4.4.3.3.3.3.3.cmml">A</mi></msub><mo stretchy="false" id="S3.E4.m2.4.4.3.3.3.7" xref="S3.E4.m2.4.4.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.4b"><apply id="S3.E4.m2.4.4.cmml" xref="S3.E4.m2.4.4"><eq id="S3.E4.m2.4.4.4.cmml" xref="S3.E4.m2.4.4.4"></eq><csymbol cd="latexml" id="S3.E4.m2.4.4.5.cmml" xref="S3.E4.m2.4.4.5">absent</csymbol><apply id="S3.E4.m2.4.4.3.4.cmml" xref="S3.E4.m2.4.4.3.3"><ci id="S3.E4.m2.1.1.cmml" xref="S3.E4.m2.1.1">MHA</ci><apply id="S3.E4.m2.2.2.1.1.1.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.2.2.1.1.1.1.2.cmml" xref="S3.E4.m2.2.2.1.1.1.1.2">𝐾</ci><ci id="S3.E4.m2.2.2.1.1.1.1.3.cmml" xref="S3.E4.m2.2.2.1.1.1.1.3">𝐴</ci></apply><apply id="S3.E4.m2.3.3.2.2.2.2.cmml" xref="S3.E4.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.3.3.2.2.2.2.1.cmml" xref="S3.E4.m2.3.3.2.2.2.2">subscript</csymbol><ci id="S3.E4.m2.3.3.2.2.2.2.2.cmml" xref="S3.E4.m2.3.3.2.2.2.2.2">𝑄</ci><ci id="S3.E4.m2.3.3.2.2.2.2.3.cmml" xref="S3.E4.m2.3.3.2.2.2.2.3">𝐴</ci></apply><apply id="S3.E4.m2.4.4.3.3.3.3.cmml" xref="S3.E4.m2.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m2.4.4.3.3.3.3.1.cmml" xref="S3.E4.m2.4.4.3.3.3.3">subscript</csymbol><ci id="S3.E4.m2.4.4.3.3.3.3.2.cmml" xref="S3.E4.m2.4.4.3.3.3.3.2">𝑉</ci><ci id="S3.E4.m2.4.4.3.3.3.3.3.cmml" xref="S3.E4.m2.4.4.3.3.3.3.3">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.4c">\displaystyle=\operatorname{MHA}(K_{A},Q_{A},V_{A})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p3.11" class="ltx_p">where <math id="S3.SS2.SSS1.p3.7.m1.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S3.SS2.SSS1.p3.7.m1.1a"><mi id="S3.SS2.SSS1.p3.7.m1.1.1" xref="S3.SS2.SSS1.p3.7.m1.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.7.m1.1b"><ci id="S3.SS2.SSS1.p3.7.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.7.m1.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.7.m1.1c">\operatorname{MHA}</annotation></semantics></math> is the standard multi-head attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, that delivers a <em id="S3.SS2.SSS1.p3.11.1" class="ltx_emph ltx_font_italic">single vector</em> incorporating all relevant commonsense knowledge required to answer the question. Note that we append the question embedding <math id="S3.SS2.SSS1.p3.8.m2.1" class="ltx_Math" alttext="\vec{Q}" display="inline"><semantics id="S3.SS2.SSS1.p3.8.m2.1a"><mover accent="true" id="S3.SS2.SSS1.p3.8.m2.1.1" xref="S3.SS2.SSS1.p3.8.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p3.8.m2.1.1.2" xref="S3.SS2.SSS1.p3.8.m2.1.1.2.cmml">Q</mi><mo stretchy="false" id="S3.SS2.SSS1.p3.8.m2.1.1.1" xref="S3.SS2.SSS1.p3.8.m2.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.8.m2.1b"><apply id="S3.SS2.SSS1.p3.8.m2.1.1.cmml" xref="S3.SS2.SSS1.p3.8.m2.1.1"><ci id="S3.SS2.SSS1.p3.8.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.8.m2.1.1.1">→</ci><ci id="S3.SS2.SSS1.p3.8.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.8.m2.1.1.2">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.8.m2.1c">\vec{Q}</annotation></semantics></math> to list of commonsense inference embeddings for <math id="S3.SS2.SSS1.p3.9.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS2.SSS1.p3.9.m3.1a"><mi id="S3.SS2.SSS1.p3.9.m3.1.1" xref="S3.SS2.SSS1.p3.9.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.9.m3.1b"><ci id="S3.SS2.SSS1.p3.9.m3.1.1.cmml" xref="S3.SS2.SSS1.p3.9.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.9.m3.1c">Q</annotation></semantics></math> and <math id="S3.SS2.SSS1.p3.10.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.SSS1.p3.10.m4.1a"><mi id="S3.SS2.SSS1.p3.10.m4.1.1" xref="S3.SS2.SSS1.p3.10.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.10.m4.1b"><ci id="S3.SS2.SSS1.p3.10.m4.1.1.cmml" xref="S3.SS2.SSS1.p3.10.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.10.m4.1c">V</annotation></semantics></math> because there may be cases where none of the inferences are useful to answer the question. In such a case, the model may choose to ignore the inferences by attending to the question embedding <math id="S3.SS2.SSS1.p3.11.m5.1" class="ltx_Math" alttext="\vec{Q}" display="inline"><semantics id="S3.SS2.SSS1.p3.11.m5.1a"><mover accent="true" id="S3.SS2.SSS1.p3.11.m5.1.1" xref="S3.SS2.SSS1.p3.11.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p3.11.m5.1.1.2" xref="S3.SS2.SSS1.p3.11.m5.1.1.2.cmml">Q</mi><mo stretchy="false" id="S3.SS2.SSS1.p3.11.m5.1.1.1" xref="S3.SS2.SSS1.p3.11.m5.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.11.m5.1b"><apply id="S3.SS2.SSS1.p3.11.m5.1.1.cmml" xref="S3.SS2.SSS1.p3.11.m5.1.1"><ci id="S3.SS2.SSS1.p3.11.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p3.11.m5.1.1.1">→</ci><ci id="S3.SS2.SSS1.p3.11.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p3.11.m5.1.1.2">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.11.m5.1c">\vec{Q}</annotation></semantics></math> instead.</p>
</div>
<section id="S3.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Weak Supervision</h5>

<div id="S3.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p1.15" class="ltx_p">In order to train the <math id="S3.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.1.m1.1c">\operatorname{MHA}</annotation></semantics></math> block effectively, we employ weak supervision on the attention weights. For a small subset of the questions in the training set, we obtain label attention weights by following these steps: (1) we initialize a vector <math id="S3.SS2.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.2.m2.1a"><mover accent="true" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml">A</mi><mo id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1"><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1">^</ci><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.2.m2.1c">\hat{A}</annotation></semantics></math> of length <math id="S3.SS2.SSS1.Px1.p1.3.m3.1" class="ltx_Math" alttext="k+1" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.3.m3.1a"><mrow id="S3.SS2.SSS1.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2.cmml">k</mi><mo id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1.cmml">+</mo><mn id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1"><plus id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1"></plus><ci id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2">𝑘</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.3.m3.1c">k+1</annotation></semantics></math> where all values are <math id="S3.SS2.SSS1.Px1.p1.4.m4.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.4.m4.1a"><mn id="S3.SS2.SSS1.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.4.m4.1b"><cn type="float" id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.4.m4.1c">0.05</annotation></semantics></math>, (2) for each <math id="S3.SS2.SSS1.Px1.p1.5.m5.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.5.m5.1a"><msub id="S3.SS2.SSS1.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2.cmml">C</mi><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2">𝐶</ci><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.5.m5.1c">C_{i}</annotation></semantics></math>, if <math id="S3.SS2.SSS1.Px1.p1.6.m6.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.6.m6.1a"><msub id="S3.SS2.SSS1.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2.cmml">C</mi><mi id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.6.m6.1b"><apply id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2">𝐶</ci><ci id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.6.m6.1c">C_{i}</annotation></semantics></math> contains a word in the ground-truth answer list, then we set the <math id="S3.SS2.SSS1.Px1.p1.7.m7.1" class="ltx_Math" alttext="\hat{A}_{i}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.7.m7.1a"><msub id="S3.SS2.SSS1.Px1.p1.7.m7.1.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.cmml"><mover accent="true" id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.2.cmml">A</mi><mo id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.7.m7.1b"><apply id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2"><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.1">^</ci><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.2">𝐴</ci></apply><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.7.m7.1c">\hat{A}_{i}</annotation></semantics></math> to <math id="S3.SS2.SSS1.Px1.p1.8.m8.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.8.m8.1a"><mn id="S3.SS2.SSS1.Px1.p1.8.m8.1.1" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.8.m8.1b"><cn type="float" id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.8.m8.1c">0.8</annotation></semantics></math>, (3) if none of the <math id="S3.SS2.SSS1.Px1.p1.9.m9.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.9.m9.1a"><mi id="S3.SS2.SSS1.Px1.p1.9.m9.1.1" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.9.m9.1b"><ci id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.9.m9.1c">C</annotation></semantics></math> inferences contain answer words, we assign a weight of <math id="S3.SS2.SSS1.Px1.p1.10.m10.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.10.m10.1a"><mn id="S3.SS2.SSS1.Px1.p1.10.m10.1.1" xref="S3.SS2.SSS1.Px1.p1.10.m10.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.10.m10.1b"><cn type="float" id="S3.SS2.SSS1.Px1.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.10.m10.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.10.m10.1c">0.8</annotation></semantics></math> to <math id="S3.SS2.SSS1.Px1.p1.11.m11.1" class="ltx_Math" alttext="\hat{A}_{k+1}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.11.m11.1a"><msub id="S3.SS2.SSS1.Px1.p1.11.m11.1.1" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.cmml"><mover accent="true" id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.cmml"><mi id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.2" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.2.cmml">A</mi><mo id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.1" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.1.cmml">^</mo></mover><mrow id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.cmml"><mi id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.2" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.2.cmml">k</mi><mo id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.1" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.3" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.11.m11.1b"><apply id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1">subscript</csymbol><apply id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2"><ci id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.1">^</ci><ci id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.2.2">𝐴</ci></apply><apply id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3"><plus id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.1"></plus><ci id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.2">𝑘</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m11.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.11.m11.1c">\hat{A}_{k+1}</annotation></semantics></math> so that the question has the largest weight, and (4) we normalize <math id="S3.SS2.SSS1.Px1.p1.12.m12.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.12.m12.1a"><mover accent="true" id="S3.SS2.SSS1.Px1.p1.12.m12.1.1" xref="S3.SS2.SSS1.Px1.p1.12.m12.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.12.m12.1.1.2" xref="S3.SS2.SSS1.Px1.p1.12.m12.1.1.2.cmml">A</mi><mo id="S3.SS2.SSS1.Px1.p1.12.m12.1.1.1" xref="S3.SS2.SSS1.Px1.p1.12.m12.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.12.m12.1b"><apply id="S3.SS2.SSS1.Px1.p1.12.m12.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m12.1.1"><ci id="S3.SS2.SSS1.Px1.p1.12.m12.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m12.1.1.1">^</ci><ci id="S3.SS2.SSS1.Px1.p1.12.m12.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m12.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.12.m12.1c">\hat{A}</annotation></semantics></math> so that its values sum up to <math id="S3.SS2.SSS1.Px1.p1.13.m13.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.13.m13.1a"><mn id="S3.SS2.SSS1.Px1.p1.13.m13.1.1" xref="S3.SS2.SSS1.Px1.p1.13.m13.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.13.m13.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.p1.13.m13.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.13.m13.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.13.m13.1c">1</annotation></semantics></math>. We then apply cross-entropy loss between the predicted attention weights from <math id="S3.SS2.SSS1.Px1.p1.14.m14.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.14.m14.1a"><mi id="S3.SS2.SSS1.Px1.p1.14.m14.1.1" xref="S3.SS2.SSS1.Px1.p1.14.m14.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.14.m14.1b"><ci id="S3.SS2.SSS1.Px1.p1.14.m14.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.14.m14.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.14.m14.1c">\operatorname{MHA}</annotation></semantics></math> and our label attention weights <math id="S3.SS2.SSS1.Px1.p1.15.m15.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.15.m15.1a"><mover accent="true" id="S3.SS2.SSS1.Px1.p1.15.m15.1.1" xref="S3.SS2.SSS1.Px1.p1.15.m15.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.15.m15.1.1.2" xref="S3.SS2.SSS1.Px1.p1.15.m15.1.1.2.cmml">A</mi><mo id="S3.SS2.SSS1.Px1.p1.15.m15.1.1.1" xref="S3.SS2.SSS1.Px1.p1.15.m15.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.15.m15.1b"><apply id="S3.SS2.SSS1.Px1.p1.15.m15.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.15.m15.1.1"><ci id="S3.SS2.SSS1.Px1.p1.15.m15.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.15.m15.1.1.1">^</ci><ci id="S3.SS2.SSS1.Px1.p1.15.m15.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.15.m15.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.15.m15.1c">\hat{A}</annotation></semantics></math>, and sum this with the answer prediction loss.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p2.1" class="ltx_p">Finally, a positional encoding is added to all input tokens following the method described in VL-BERT. In addition, a different segment type encoding is applied to the four segments in the input sequence: the question segment, the commonsense segment, the masked answer segment, and the image region segment.</p>
</div>
</section>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Answer Selection</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We use the encoded  <svg id="S3.SS2.SSS2.p1.1.pic1" class="ltx_picture" height="11.07" overflow="visible" version="1.1" width="39.36"><g transform="translate(0,11.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 5.16 C 0 8.43 2.64 11.07 5.91 11.07 L 33.45 11.07 C 36.71 11.07 39.36 8.43 39.36 5.16 L 39.36 5.91 C 39.36 2.64 36.71 0 33.45 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 5.16 C 1.97 7.34 3.73 9.1 5.91 9.1 L 33.45 9.1 C 35.63 9.1 37.39 7.34 37.39 5.16 L 37.39 5.91 C 37.39 3.73 35.63 1.97 33.45 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 2.77)"><foreignObject width="39.36" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S3.SS2.SSS2.p1.1.pic1.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">[MASK]</span></foreignObject></g></g></svg> token to represent the answer, thereby making VQA a masked language modelling task with visual cues. To predict the final answer, we apply a classifier over the entire answer vocabulary, as done in VL-BERT. During training, we follow VL-BERT and use a cross-entropy loss over picking the correct answer from an answer vocabulary.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We perform experiments on the OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and A-OKVQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> datasets. In order to utilize the existing VL-BERT model effectively, we pre-train VLC-BERT on the larger VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">OK-VQA</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">In the Outside-Knowledge VQA dataset, questions require external knowledge in addition to the information in the images. The dataset is composed of 14,031 images and 14,055 questions, and the crowsourced questions are divided into ten knowledge categories: Vehicles and Transportation; Brands, Companies and Products; Objects, Materials and Clothing; Sports and Recreation; Cooking and Food; Geography, History,
Language and Culture; People and Everyday Life, Plants and Animals; Science and Technology; and Weather and Climate. OK-VQA only contains open-ended questions with five human-provided answers. Since OK-VQA does not have a validation set, we dedicate 1,000 of the 9,009 training questions for validation.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">A-OKVQA</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">A-OKVQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> is the augmented successor to OK-VQA and consists of 25K questions that require a combination of commonsense, visual, and physical knowledge.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">In contrast to other knowledge-based visual question answering datasets, the questions in A-OKVQA are conceptually diverse, involving knowledge that is not contained in the image, and cannot be resolved by a simple knowledge base query. A-OKVQA is split into training, validation, and test sets based on images used from the COCO 2017 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> dataset. Moreover, all questions in the dataset have human annotated direct answers as well as multiple-choice options, but we focus on the direct answers. The A-OKVQA test set is blind, requiring us to submit to the leaderboard to obtain a test accuracy.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">VQA 2.0</h5>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">The Visual Question Answering (v2.0) dataset contains 1.1 million crowdsourced questions about 204,721 images from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Each question is annotated with 10 ground truth answers obtained using Amazon Mechanical Turk. A majority of the questions in this dataset do not require external commonsense knowledge.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Accuracy of our model against other models for OK-VQA and A-OKVQA datasets. Our model improves upon existing knowledge base based models due to the contextualized commonsense inferences from COMET, which is trained on ConceptNet and ATOMIC. We compare favourably against the highlighted models that utilize external knowledge bases. Note: P.T. stands for Pre-Training.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Method</th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Knowledge Sources</th>
<td id="S4.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">OK-VQA</td>
<td id="S4.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">A-OKVQA</td>
<td id="S4.T1.4.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Approx. Params</td>
</tr>
<tr id="S4.T1.4.2.2" class="ltx_tr">
<th id="S4.T1.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T1.4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</th>
<td id="S4.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">25.85</td>
<td id="S4.T1.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">116M</td>
</tr>
<tr id="S4.T1.4.3.3" class="ltx_tr">
<th id="S4.T1.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T1.4.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">-</th>
<td id="S4.T1.4.3.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.3.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.89</td>
<td id="S4.T1.4.3.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">BAN + AN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<th id="S4.T1.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Wikipedia</th>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.61</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T1.4.5.5" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.5.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">BAN + KG-AUG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span></th>
<th id="S4.T1.4.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.5.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">Wikipedia + ConceptNet</span></th>
<td id="S4.T1.4.5.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.5.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">26.71</span></td>
<td id="S4.T1.4.5.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.5.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S4.T1.4.5.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.5.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
</tr>
<tr id="S4.T1.4.6.6" class="ltx_tr">
<th id="S4.T1.4.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">MUTAN + AN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<th id="S4.T1.4.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Wikipedia</th>
<td id="S4.T1.4.6.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">27.84</td>
<td id="S4.T1.4.6.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.6.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T1.4.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.4.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.7.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">ConceptBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span></th>
<th id="S4.T1.4.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.7.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">ConceptNet</span></th>
<td id="S4.T1.4.7.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">33.66</span></td>
<td id="S4.T1.4.7.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.7.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S4.T1.4.7.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.7.7.5.1" class="ltx_text" style="background-color:#E6E6E6;">118M</span></td>
</tr>
<tr id="S4.T1.4.8.8" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.4.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.8.8.1.1" class="ltx_text" style="background-color:#E6E6E6;">KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span></th>
<th id="S4.T1.4.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.8.8.2.1" class="ltx_text" style="background-color:#E6E6E6;">Wikipedia + ConceptNet</span></th>
<td id="S4.T1.4.8.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.8.8.3.1" class="ltx_text" style="background-color:#E6E6E6;">32.31</span></td>
<td id="S4.T1.4.8.8.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.8.8.4.1" class="ltx_text" style="background-color:#E6E6E6;">27.1</span></td>
<td id="S4.T1.4.8.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.8.8.5.1" class="ltx_text" style="background-color:#E6E6E6;">116M</span></td>
</tr>
<tr id="S4.T1.4.9.9" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.4.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.9.9.1.1" class="ltx_text" style="background-color:#E6E6E6;">KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span></th>
<th id="S4.T1.4.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.9.9.2.1" class="ltx_text" style="background-color:#E6E6E6;">Wikipedia + ConceptNet + VQA P.T.</span></th>
<td id="S4.T1.4.9.9.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.9.9.3.1" class="ltx_text" style="background-color:#E6E6E6;">38.9</span></td>
<td id="S4.T1.4.9.9.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.9.9.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S4.T1.4.9.9.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.9.9.5.1" class="ltx_text" style="background-color:#E6E6E6;">116M</span></td>
</tr>
<tr id="S4.T1.4.10.10" class="ltx_tr">
<th id="S4.T1.4.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Visual Retriever-Reader <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<th id="S4.T1.4.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Google Search</th>
<td id="S4.T1.4.10.10.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">39.2</td>
<td id="S4.T1.4.10.10.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.10.10.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T1.4.11.11" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.4.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.11.11.1.1" class="ltx_text" style="background-color:#E6E6E6;">MAVEx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite></span></th>
<th id="S4.T1.4.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.11.11.2.1" class="ltx_text" style="background-color:#E6E6E6;">Wikipedia + ConceptNet + Google Images</span></th>
<td id="S4.T1.4.11.11.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.11.11.3.1" class="ltx_text" style="background-color:#E6E6E6;">41.37</span></td>
<td id="S4.T1.4.11.11.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.11.11.4.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
<td id="S4.T1.4.11.11.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.11.11.5.1" class="ltx_text" style="background-color:#E6E6E6;">-</span></td>
</tr>
<tr id="S4.T1.4.12.12" class="ltx_tr">
<th id="S4.T1.4.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">GPV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T1.4.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Web Search (Web10k) + COCO P.T.</th>
<td id="S4.T1.4.12.12.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.12.12.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">40.7</td>
<td id="S4.T1.4.12.12.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">220M</td>
</tr>
<tr id="S4.T1.4.13.13" class="ltx_tr">
<th id="S4.T1.4.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">PICa-Base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</th>
<th id="S4.T1.4.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-3</th>
<td id="S4.T1.4.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">43.3</td>
<td id="S4.T1.4.13.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">175B</td>
</tr>
<tr id="S4.T1.4.14.14" class="ltx_tr">
<th id="S4.T1.4.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">PICa-Full <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</th>
<th id="S4.T1.4.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-3</th>
<td id="S4.T1.4.14.14.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">48.0</td>
<td id="S4.T1.4.14.14.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.14.14.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">175B</td>
</tr>
<tr id="S4.T1.4.15.15" class="ltx_tr">
<th id="S4.T1.4.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">KAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<th id="S4.T1.4.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Wikidata + GPT-3</th>
<td id="S4.T1.4.15.15.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">54.41</td>
<td id="S4.T1.4.15.15.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T1.4.15.15.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">175B</td>
</tr>
<tr id="S4.T1.4.16.16" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.4.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.16.16.1.1" class="ltx_text" style="background-color:#E6E6E6;">VLC-BERT (Ours)</span></th>
<th id="S4.T1.4.16.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.16.16.2.1" class="ltx_text" style="background-color:#E6E6E6;">VQA P.T. + COMET</span></th>
<td id="S4.T1.4.16.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.16.16.3.1" class="ltx_text" style="background-color:#E6E6E6;">43.14</span></td>
<td id="S4.T1.4.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.16.16.4.1" class="ltx_text" style="background-color:#E6E6E6;">38.05</span></td>
<td id="S4.T1.4.16.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.4.16.16.5.1" class="ltx_text" style="background-color:#E6E6E6;">118M</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Metric</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Both datasets use the same accuracy-based evaluation metric. Each question has a set of 10 ground truth answers provided by different annotators. Accuracy is calculated as the percentage of predicted answers that were proposed by at least 3 human annotators: <math id="S4.SS1.p1.1.m1.2" class="ltx_Math" alttext="\textrm{acc}=\textrm{min}(\frac{\textrm{ \# humans gave the answer}}{3},1)" display="inline"><semantics id="S4.SS1.p1.1.m1.2a"><mrow id="S4.SS1.p1.1.m1.2.3" xref="S4.SS1.p1.1.m1.2.3.cmml"><mtext id="S4.SS1.p1.1.m1.2.3.2" xref="S4.SS1.p1.1.m1.2.3.2a.cmml">acc</mtext><mo id="S4.SS1.p1.1.m1.2.3.1" xref="S4.SS1.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS1.p1.1.m1.2.3.3" xref="S4.SS1.p1.1.m1.2.3.3.cmml"><mtext id="S4.SS1.p1.1.m1.2.3.3.2" xref="S4.SS1.p1.1.m1.2.3.3.2a.cmml">min</mtext><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.2.3.3.1" xref="S4.SS1.p1.1.m1.2.3.3.1.cmml">​</mo><mrow id="S4.SS1.p1.1.m1.2.3.3.3.2" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S4.SS1.p1.1.m1.2.3.3.3.2.1" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml">(</mo><mfrac id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mtext id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2a.cmml"> # humans gave the answer</mtext><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">3</mn></mfrac><mo id="S4.SS1.p1.1.m1.2.3.3.3.2.2" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S4.SS1.p1.1.m1.2.3.3.3.2.3" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.2b"><apply id="S4.SS1.p1.1.m1.2.3.cmml" xref="S4.SS1.p1.1.m1.2.3"><eq id="S4.SS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.1"></eq><ci id="S4.SS1.p1.1.m1.2.3.2a.cmml" xref="S4.SS1.p1.1.m1.2.3.2"><mtext id="S4.SS1.p1.1.m1.2.3.2.cmml" xref="S4.SS1.p1.1.m1.2.3.2">acc</mtext></ci><apply id="S4.SS1.p1.1.m1.2.3.3.cmml" xref="S4.SS1.p1.1.m1.2.3.3"><times id="S4.SS1.p1.1.m1.2.3.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.3.1"></times><ci id="S4.SS1.p1.1.m1.2.3.3.2a.cmml" xref="S4.SS1.p1.1.m1.2.3.3.2"><mtext id="S4.SS1.p1.1.m1.2.3.3.2.cmml" xref="S4.SS1.p1.1.m1.2.3.3.2">min</mtext></ci><interval closure="open" id="S4.SS1.p1.1.m1.2.3.3.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.3.3.2"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><divide id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></divide><ci id="S4.SS1.p1.1.m1.1.1.2a.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><mtext mathsize="70%" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"> # humans gave the answer</mtext></ci><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">3</cn></apply><cn type="integer" id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.2c">\textrm{acc}=\textrm{min}(\frac{\textrm{ \# humans gave the answer}}{3},1)</annotation></semantics></math>.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Following the same evaluation, each of the 5 answers in OK-VQA is used twice</span></span></span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Implementation Details</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The implementation of our model builds on VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. To that end, we follow the fine-tuning steps provided in the official codebase of the VL-BERT model for VQA 2.0, and modify it to support the OK-VQA and A-OKVQA datasets. We maintain the recommended hyperparameter values, and train the <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="BERT_{BASE}" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1a" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.4" xref="S5.p1.1.m1.1.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1b" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><msub id="S5.p1.1.m1.1.1.5" xref="S5.p1.1.m1.1.1.5.cmml"><mi id="S5.p1.1.m1.1.1.5.2" xref="S5.p1.1.m1.1.1.5.2.cmml">T</mi><mrow id="S5.p1.1.m1.1.1.5.3" xref="S5.p1.1.m1.1.1.5.3.cmml"><mi id="S5.p1.1.m1.1.1.5.3.2" xref="S5.p1.1.m1.1.1.5.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.5.3.1" xref="S5.p1.1.m1.1.1.5.3.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.5.3.3" xref="S5.p1.1.m1.1.1.5.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.5.3.1a" xref="S5.p1.1.m1.1.1.5.3.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.5.3.4" xref="S5.p1.1.m1.1.1.5.3.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.5.3.1b" xref="S5.p1.1.m1.1.1.5.3.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.5.3.5" xref="S5.p1.1.m1.1.1.5.3.5.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝐵</ci><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">𝐸</ci><ci id="S5.p1.1.m1.1.1.4.cmml" xref="S5.p1.1.m1.1.1.4">𝑅</ci><apply id="S5.p1.1.m1.1.1.5.cmml" xref="S5.p1.1.m1.1.1.5"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.5.1.cmml" xref="S5.p1.1.m1.1.1.5">subscript</csymbol><ci id="S5.p1.1.m1.1.1.5.2.cmml" xref="S5.p1.1.m1.1.1.5.2">𝑇</ci><apply id="S5.p1.1.m1.1.1.5.3.cmml" xref="S5.p1.1.m1.1.1.5.3"><times id="S5.p1.1.m1.1.1.5.3.1.cmml" xref="S5.p1.1.m1.1.1.5.3.1"></times><ci id="S5.p1.1.m1.1.1.5.3.2.cmml" xref="S5.p1.1.m1.1.1.5.3.2">𝐵</ci><ci id="S5.p1.1.m1.1.1.5.3.3.cmml" xref="S5.p1.1.m1.1.1.5.3.3">𝐴</ci><ci id="S5.p1.1.m1.1.1.5.3.4.cmml" xref="S5.p1.1.m1.1.1.5.3.4">𝑆</ci><ci id="S5.p1.1.m1.1.1.5.3.5.cmml" xref="S5.p1.1.m1.1.1.5.3.5">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">BERT_{BASE}</annotation></semantics></math> size of the model, with a hidden feature dimension of 768. The model is trained for 20 epochs on the OK-VQA and A-OKVQA datasets. For all models, we use a batch size of 16 and gradient accumulation step size of 4. We train the models presented in the main result thrice and report the average test accuracy on the OK-VQA dataset, and the best (leaderboard) test accuracy on the A-OKVQA dataset.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.2" class="ltx_p"><span id="S5.p2.2.1" class="ltx_text ltx_font_bold">Answer Vocabulary.</span>
Due to the large number of unique answers to questions in visual question answering datasets, it is infeasible to use all answers in the answer vocabulary. For the OK-VQA dataset, following KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, we build an answer vocabulary of 2,249 answers by selecting all answers in the training set that appear at least 10 times. This answer vocabulary ignores the empty space answer, and includes an  <svg id="S5.p2.1.pic1" class="ltx_picture" height="8" overflow="visible" version="1.1" width="42.43"><g transform="translate(0,8) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 2.09 C 0 5.35 2.64 8 5.91 8 L 36.53 8 C 39.79 8 42.43 5.35 42.43 2.09 L 42.43 5.91 C 42.43 2.64 39.79 0 36.53 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 2.09 C 1.97 4.27 3.73 6.03 5.91 6.03 L 36.53 6.03 C 38.7 6.03 40.47 4.27 40.47 2.09 L 40.47 5.91 C 40.47 3.73 38.7 1.97 36.53 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 0.43)"><foreignObject width="42.43" height="8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S5.p2.1.pic1.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">&lt;UNK&gt;</span></foreignObject></g></g></svg> answer token. During training, if a ground truth answer is not present in the answer vocabulary, we assign it to the ( <svg id="S5.p2.2.pic2" class="ltx_picture" height="8" overflow="visible" version="1.1" width="42.43"><g transform="translate(0,8) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#E6E6E6" fill-opacity="1.0"><path d="M 0 5.91 L 0 2.09 C 0 5.35 2.64 8 5.91 8 L 36.53 8 C 39.79 8 42.43 5.35 42.43 2.09 L 42.43 5.91 C 42.43 2.64 39.79 0 36.53 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 2.09 C 1.97 4.27 3.73 6.03 5.91 6.03 L 36.53 6.03 C 38.7 6.03 40.47 4.27 40.47 2.09 L 40.47 5.91 C 40.47 3.73 38.7 1.97 36.53 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 0 0.43)"><foreignObject width="42.43" height="8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="S5.p2.2.pic2.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">&lt;UNK&gt;</span></foreignObject></g></g></svg> ) token. For the A-OKVQA dataset, we use the answer dictionary that is already provided in the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">VQA Pre-Training (VQA P.T)</span> Following the idea that pre-training is beneficial for Transformer models, we initialize VLC-BERT with weights obtained after fine-tuning VL-BERT on the VQA 2.0 dataset for 5 epochs. Note that KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> benefits from pre-training on the VQA 2.0 dataset, and PICa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and KAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> utilize GPT-3, a large-scale pre-trained model, for external commonsense. Furthermore, because OK-VQA and A-OKVQA are significantly smaller than VQA 2.0, this initialization favourably benefits the training process and gives us a stronger baseline to work with.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation</h2>

<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S6.T2.3.2" class="ltx_text" style="font-size:90%;">Ablation of various components in VLC-BERT, evaluated on the A-OKVQA validation set. We observe that all the components of our model play a critical role in empirical performance.</span></figcaption>
<table id="S6.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.4.1.1" class="ltx_tr">
<td id="S6.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">VQA P.T.</td>
<td id="S6.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Aug. SBERT</td>
<td id="S6.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">SBERT</td>
<td id="S6.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Attn.</td>
<td id="S6.T2.4.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Val</td>
</tr>
<tr id="S6.T2.4.2.2" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S6.T2.4.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="5"><span id="S6.T2.4.2.2.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">VQA Pre-training</span></td>
</tr>
<tr id="S6.T2.4.3.3" class="ltx_tr">
<td id="S6.T2.4.3.3.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.3.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.3.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.3.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.3.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.24</td>
</tr>
<tr id="S6.T2.4.4.4" class="ltx_tr">
<td id="S6.T2.4.4.4.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.4.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.4.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.4.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.46</td>
</tr>
<tr id="S6.T2.4.5.5" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S6.T2.4.5.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="5"><span id="S6.T2.4.5.5.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Comm. Inference Representation</span></td>
</tr>
<tr id="S6.T2.4.6.6" class="ltx_tr">
<td id="S6.T2.4.6.6.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.6.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.6.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.6.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.6.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.44</td>
</tr>
<tr id="S6.T2.4.7.7" class="ltx_tr">
<td id="S6.T2.4.7.7.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.7.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.7.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.7.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.7.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.64</td>
</tr>
<tr id="S6.T2.4.8.8" class="ltx_tr">
<td id="S6.T2.4.8.8.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.8.8.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.8.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.8.8.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.8.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T2.4.8.8.5.1" class="ltx_text ltx_font_bold">44.95</span></td>
</tr>
<tr id="S6.T2.4.9.9" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S6.T2.4.9.9.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="5"><span id="S6.T2.4.9.9.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Augmentation of SBERT</span></td>
</tr>
<tr id="S6.T2.4.10.10" class="ltx_tr">
<td id="S6.T2.4.10.10.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.10.10.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td id="S6.T2.4.10.10.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.10.10.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.10.10.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.10</td>
</tr>
<tr id="S6.T2.4.11.11" class="ltx_tr">
<td id="S6.T2.4.11.11.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.11.11.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.11.11.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.11.11.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S6.T2.4.11.11.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S6.T2.4.11.11.5.1" class="ltx_text ltx_font_bold">44.95</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we focus on evaluating VLC-BERT on the OK-VQA and A-OKVQA datasets and comparing against existing state-of-the-art models for VQA with external commonsense knowledge. Table <a href="#S4.T1" title="Table 1 ‣ VQA 2.0 ‣ 4 Datasets ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> highlights our performance improvements on the test set for OK-VQA and A-OKVQA against other models. Later in this section, we ablate on the components of our model.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Main Results</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ VQA 2.0 ‣ 4 Datasets ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> specifies which knowledge sources each model leverages. In the top section, we consider models that utilize knowledge bases such as ConceptNet and Wikipedia, as well as models that utilize web search APIs to obtain external knowledge. VLC-BERT incorporates COMET, which is trained on ConceptNet and ATOMIC, and we compare favourably against these models. Notably, VLC-BERT achieves an accuracy of 43.14 on OK-VQA, outperforming KRISP (Wikipedia + ConceptNet + VQA P.T.) by over 4 points, and MAVEx (Wikipedia + ConceptNet + Google Images) by about 2 points. While our model clearly outperforms previous methods that use knowledge bases, it does not outperform models with large-scale pre-training and large number of parameters such as GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and GPV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, which incorporate implicit commmonsense knowledge and require extensive resources to train. However, on OK-VQA, we achieve very similar results to PICa-Base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, despite not having access to GPT-3. We expect that the use of a large pre-trained model like GPT-3 can further boost the performance of VLC-BERT.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Ablation Tests</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We perform comprehensive ablations on the validation set of the A-OKVQA dataset, as represented in Table <a href="#S6.T2" title="Table 2 ‣ 6 Evaluation ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We present additional ablations in supplementary material Sec 2.3</span></span></span></p>
</div>
<section id="S6.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQA P.T</h5>

<div id="S6.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px1.p1.1" class="ltx_p">We begin by training A-OKVQA on the baseline VL-BERT model without VQA pre-training. This gives us a score of 36.24. Next, obtain a new baseline for our model with VQA pre-training, where we then initialize VLC-BERT with pre-trained weights on the VQA 2.0 dataset, and further train it on the A-OKVQA dataset. This results in a score of 43.46, over 7 points better, highlighting the impact of pre-training with a large-scale dataset. This model is a strong baseline for our VQA tasks.</p>
</div>
</section>
<section id="S6.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Comm. Inference Representation</h5>

<div id="S6.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px2.p1.2" class="ltx_p">In the full model, we use SBERT to summarize each commonsense inference into a single vector, and use the multi-head attention block to capture useful information from the list of inference vectors. To test the effectiveness of our commonsense inference representation method, we first ablate SBERT, i.e., we incorporate all inferences as an additional text input for VLC-BERT, feeding them token-by-token. This results in an accuracy score of 43.44, which is slightly lower than our baseline with VQA pre-training. Next, we use SBERT to summarize inferences, and feed the SBERT embeddings directly into VLC-BERT with only a linear projection layer rather than the <math id="S6.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S6.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S6.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S6.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S6.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS0.Px2.p1.1.m1.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px2.p1.1.m1.1c">\operatorname{MHA}</annotation></semantics></math> block. This variant performs worse than the model with the <math id="S6.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S6.SS2.SSS0.Px2.p1.2.m2.1a"><mi id="S6.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S6.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S6.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S6.SS2.SSS0.Px2.p1.2.m2.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px2.p1.2.m2.1c">\operatorname{MHA}</annotation></semantics></math> block by 1.25 points.</p>
</div>
</section>
<section id="S6.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Augmented SBERT</h5>

<div id="S6.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px3.p1.1" class="ltx_p">In order to familiarize SBERT with our question-inference pairs, we fine-tune SBERT on the training set of A-OKVQA and OK-VQA (Sec <a href="#S3.SS1.SSS2" title="3.1.2 Knowledge Selection ‣ 3.1 Structured knowledge generation and selection ‣ 3 Method ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>). We perform an ablation by evaluating our model on SBERT that has never been exposed to the question-inference-pairs. This results in a drop of 0.85 points in accuracy, which shows that our augmentation of SBERT is effective.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Analysis</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Commonsense subsets</h3>

<figure id="S7.F4" class="ltx_figure"><img src="/html/2210.13626/assets/figures/egs.png" id="S7.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="617" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S7.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Attention analysis:<span id="S7.F4.4.2.1" class="ltx_text ltx_font_medium"> (a) is from A-OKVQA, and (b) and (c) are from OK-VQA. We observe that the weakly supervised attention layer in VLC-BERT accurately picks useful commonsense inferences. In (c), we observe how object tags are useful to guide COMET to produce contextualized knowledge.</span></span></figcaption>
</figure>
<figure id="S7.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S7.T3.10.3.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S7.T3.4.4.2" class="ltx_text" style="font-size:90%;">Evaluation on the subsets of OK-VQA test (OK<sub id="S7.T3.4.4.2.1" class="ltx_sub"><span id="S7.T3.4.4.2.1.1" class="ltx_text ltx_font_italic">s</span></sub>) and A-OKVQA validation (A-OK<sub id="S7.T3.4.4.2.2" class="ltx_sub"><span id="S7.T3.4.4.2.2.1" class="ltx_text ltx_font_italic">s</span></sub>) sets, where factual, numerical and visual questions are pruned. The performance gain observed on the subsets shows a better picture of where external commonsense is effective.</span></figcaption>
<table id="S7.T3.6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T3.6.6.2" class="ltx_tr">
<th id="S7.T3.6.6.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Method</th>
<th id="S7.T3.6.6.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">OK</th>
<th id="S7.T3.5.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">OK<sub id="S7.T3.5.5.1.1.1" class="ltx_sub"><span id="S7.T3.5.5.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sub>
</th>
<th id="S7.T3.6.6.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">A-OK</th>
<th id="S7.T3.6.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">A-OK<sub id="S7.T3.6.6.2.2.1" class="ltx_sub"><span id="S7.T3.6.6.2.2.1.1" class="ltx_text ltx_font_italic">s</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T3.6.6.3.1" class="ltx_tr">
<th id="S7.T3.6.6.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Base</th>
<td id="S7.T3.6.6.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">42.29</td>
<td id="S7.T3.6.6.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">47.4</td>
<td id="S7.T3.6.6.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">43.46</td>
<td id="S7.T3.6.6.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">46.52</td>
</tr>
<tr id="S7.T3.6.6.4.2" class="ltx_tr">
<th id="S7.T3.6.6.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">w/ COMET</th>
<td id="S7.T3.6.6.4.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S7.T3.6.6.4.2.2.1" class="ltx_text ltx_font_bold">43.14</span></td>
<td id="S7.T3.6.6.4.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S7.T3.6.6.4.2.3.1" class="ltx_text ltx_font_bold">48.21</span></td>
<td id="S7.T3.6.6.4.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S7.T3.6.6.4.2.4.1" class="ltx_text ltx_font_bold">44.95</span></td>
<td id="S7.T3.6.6.4.2.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S7.T3.6.6.4.2.5.1" class="ltx_text ltx_font_bold">49.53</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Questions in OK-VQA and A-OKVQA datasets are diverse and require commonsense reasoning, visual understanding, as well as factual knowledge. While COMET can generate contextualized commonsense knowledge, it does not help with questions that require scene understanding (<em id="S7.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.2" class="ltx_text"></span>, “What is to the left of the computer?”), factual knowledge (<em id="S7.SS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.4" class="ltx_text"></span>, “Where was this food invented?”), or text/symbol recognition (<em id="S7.SS1.p1.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.6" class="ltx_text"></span>, “What does this sign say?”). Moreover, averaging results on the entirety of OK-VQA and A-OKVQA obfuscates the improvements brought about to a subset of questions that truly require commonsense knowledge. We propose subsets to assess the performance of our model on questions that are more likely to require external commonsense knowledge. We obtain the subsets by eliminating questions that are mostly factual or visual, and hence do not require commonsense, following these conditions: (1) <em id="S7.SS1.p1.1.7" class="ltx_emph ltx_font_italic">factual</em>: The question or answer contains named entities (<em id="S7.SS1.p1.1.8" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.9" class="ltx_text"></span>, “USA”); (2) <em id="S7.SS1.p1.1.10" class="ltx_emph ltx_font_italic">numerical</em>: The answers contain numbers or number words (<em id="S7.SS1.p1.1.11" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.12" class="ltx_text"></span>, “twenty”) or the question has date or time words (<em id="S7.SS1.p1.1.13" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.14" class="ltx_text"></span>, “century”); (3) <em id="S7.SS1.p1.1.15" class="ltx_emph ltx_font_italic">visual</em>: The question contains directional words (<em id="S7.SS1.p1.1.16" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.17" class="ltx_text"></span>, “left of”) and words referring to symbols (<em id="S7.SS1.p1.1.18" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.19" class="ltx_text"></span>, “mascot”).</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">In Table <a href="#S7.T3" title="Table 3 ‣ 7.1 Commonsense subsets ‣ 7 Analysis ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show that VLC-BERT with COMET performs 3 points better on the A-OKVQA subset, and maintains an 0.8 point improvement on the OK-VQA subset. This substantiates our claim that utilizing our COMET pipeline substantially increases VLC-BERT’s ability to answer questions that require external knowledge.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Attention Analysis</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">In this section, we show qualitative examples to demonstrate questions where VLC-BERT benefits from contextualized commonsense knowledge from COMET. We also show the corresponding attention weights, to show the effectiveness of the proposed weakly-supervised attention mechanism. Fig <a href="#S7.F4" title="Figure 4 ‣ 7.1 Commonsense subsets ‣ 7 Analysis ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a shows an example from A-OKVQA, where COMET’s inferences on the question and the object tags, weighted by the attention score, results in the correct answer. Fig <a href="#S7.F4" title="Figure 4 ‣ 7.1 Commonsense subsets ‣ 7 Analysis ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b shows an example from OK-VQA where VLC-BERT COMET exhibits higher attention towards the fire despite the object tags missing the fireplace. This is an example where deriving inferences from the question phrase is equally important as doing so with the object tags. Fig <a href="#S7.F4" title="Figure 4 ‣ 7.1 Commonsense subsets ‣ 7 Analysis ‣ VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>c shows that inferences on the object tag <span id="S7.SS2.p1.1.1" class="ltx_text ltx_font_italic">kite</span> drove the model to answer correctly. The supplementary material includes additional examples of improvements and failures.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We presented Vision-Language-Commonsense BERT (VLC-BERT) for external knowledge-driven VQA tasks. VLC-BERT outperforms previous models based on knowledge bases on the OK-VQA and A-OKVQA datasets by incorporating contextualized commonsense knowledge from COMET and combining it with visual and linguistic inputs. Through our evaluation, we show the effectiveness of our knowledge generation, selection, and incorporation strategies, and the positive impact of VQA pre-training.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Our analysis of VLC-BERT highlighted a few limitations of our model and the datasets we evaluate on. First, some questions require a deeper understanding and linking of multiple entities and events in the image, that object tags lack, for deriving relevant commonsense inferences. Second, condensing the commonsense inferences using SBERT and <math id="S8.p2.1.m1.1" class="ltx_Math" alttext="\operatorname{MHA}" display="inline"><semantics id="S8.p2.1.m1.1a"><mi id="S8.p2.1.m1.1.1" xref="S8.p2.1.m1.1.1.cmml">MHA</mi><annotation-xml encoding="MathML-Content" id="S8.p2.1.m1.1b"><ci id="S8.p2.1.m1.1.1.cmml" xref="S8.p2.1.m1.1.1">MHA</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.1.m1.1c">\operatorname{MHA}</annotation></semantics></math> leads to a compressed representation that may cause the model to lose some information. Finally, our model is limited by COMET, and the knowledge bases it is trained on, as we observe that large-scale models like GPT-3 outperform it.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">We view our work as a first step in analyzing the potential of <span id="S8.p3.1.1" class="ltx_text ltx_font_italic">generative commonsense incorporation</span>, and exploring approaches to <span id="S8.p3.1.2" class="ltx_text ltx_font_italic">decide when commonsense is needed</span>. In the future, our goal is to work towards creating a version of COMET that can utilize image context concerning multiple entities and events. We also plan to investigate the potential of multi-hop reasoning with COMET to bridge the question and image-based expansions closer.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgments</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chair, NSERC CRC, NSERC DG and Accelerator Grants, and a research gift from AI2. Hardware resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a href="www.vectorinstitute.ai/#partners" title="" class="ltx_ref ltx_href">www.vectorinstitute.ai/#partners</a></span></span></span>. Additional hardware support was provided by John R. Evans Leaders Fund CFI grant and Compute Canada under the Resource Allocation Competition award.
Finally, we sincerely thank Prof. Giuseppe Carenini for valuable feedback and discussions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli
Celikyilmaz, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">COMET: Commonsense transformers for automatic knowledge graph
construction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">57th Annual Meeting of the Association for Computational
Linguistics (ACL)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">,
volume 33, pages 1877–1901, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Tuhin Chakrabarty, Yejin Choi, and Vered Shwartz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">It’s not rocket science : Interpreting figurative language in
narratives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics
(TACL)</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and
Yonatan Bisk.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Webqa: Multihop and multimodal qa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, June 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,
José M.F. Moura, Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Visual Dialog.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Joe Davison, Joshua Feldman, and Alexander Rush.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Commonsense knowledge mining from pretrained models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 1173–1178, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">BERT: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 4171–4186, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
François Gardères, Maryam Ziaeefard, Baptiste Abeloos, and Freddy
Lécué.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Conceptbert: Concept-aware representation for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">FINDINGS</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Fast R-CNN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">,
2015.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Gordon and Benjamin Van Durme.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Reporting bias and knowledge acquisition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop on Automated Knowledge Base Construction</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, page
25–30, 2013.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and
Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Kat: A knowledge augmented transformer for vision-and-language, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and
Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">KAT: A knowledge augmented transformer for vision-and-language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">. Association for Computational Linguistics, July 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi,
Antoine Bosselut, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Comet-atomic 2020: On symbolic and neural commonsense knowledge
graphs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, NanoCode012, Yonghye
Kwon, TaoXie, Jiacong Fang, imyhxy, Kalen Michael, Lorna, Abhiram V, Diego
Montes, Jebastin Nadar, Laughing, tkianai, yxNONG, Piotr Skalski, Zhiqiang
Wang, Adam Hogan, Cristi Fati, Lorenzo Mammana, AlexWang1900, Deep Patel,
Ding Yiwei, Felix You, Jan Hajek, Laurentiu Diaconu, and Mai Thanh Minh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">ultralytics/yolov5: v6.1 - TensorRT, TensorFlow Edge TPU and
OpenVINO Export and Inference, Feb. 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
V. Joshi, Matthew E. Peters, and Mark Hopkins.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Extending a parser to distant domains using a few dozen partially
annotated examples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and
Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Webly supervised concept expansion for general purpose vision models,
2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">BART: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">58th Annual Meeting of the Association for Computational
Linguistics</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 7871–7880, Online, July 2020. Association for
Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Guohao Li, Xin Wang, and Wenwu Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Boosting visual question answering with context-aware knowledge
aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">28th ACM International Conference on Multimedia</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">VisualBERT: A simple and performant baseline for vision and
language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.03557</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Oscar: Object-semantics aligned pre-training for vision-language
tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Kagnet: Knowledge-aware graph networks for commonsense reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP-IJCNLP</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,
editors, </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2014</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755. Springer
International Publishing, 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">12-in-1: Multi-task vision and language representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Weakly-supervised visual-retriever-reader for knowledge-based
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-Kirkpatrick, and
Julian McAuley.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Like hiking? you probably enjoy nature: Persona-grounded dialog with
commonsense expansions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 9194–9206. Association for Computational
Linguistics, Nov. 2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">KRISP: Integrating implicit and symbolic knowledge for open-domain
knowledge-based vqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 14111–14121, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">OK-VQA: A visual question answering benchmark requiring external
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin
Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Visualcomet: Reasoning about the dynamic context of a still image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Nils Reimers and Iryna Gurevych.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Sentence-bert: Sentence embeddings using siamese bert-networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 Conference on Empirical Methods in Natural Language
Processing</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">A primer in BERTology: What we know about how BERT works.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">,
8:842–866, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, and Eneko
Agirre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Image captioning for effective use of language models in
knowledge-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Shailaja Keyur Sampat, Yezhou Yang, and Chitta Baral.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Visuo-linguistic question answering (VLQA) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Findings of the Association for Computational Linguistics:
EMNLP 2020</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">. Association for Computational Linguistics, Nov. 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Maarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie,
Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Atomic: An atlas of machine commonsense for if-then reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">A-okvqa: A benchmark for visual question answering using world
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Cycle-consistency for robust visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, pages 6642–6651, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ACL</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Unsupervised commonsense question answering with self-talk.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 4615–4629. Association for Computational
Linguistics, Nov. 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Robyn Speer, Joshua Chin, and Catherine Havasi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Conceptnet 5.5: An open multilingual graph of general knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Vl-bert: Pre-training of generic visual-linguistic representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">LXMERT: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural Language
Processing</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Yufei Tian, Arvind krishna Sridhar, and Nanyun Peng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">HypoGen: Hyperbole generation with commonsense and counterfactual
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Association for Computational Linguistics: EMNLP 2021</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages
1583–1593, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, volume 30. Curran Associates, Inc., 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Fvqa: Fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 40(10):2413–2427,
oct 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Peter West, Chandrasekhar Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang,
Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Symbolic knowledge distillation: from general language models to
commonsense models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Multi-Modal Answer Validation for Knowledge-based VQA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and
Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">An empirical study of gpt-3 for few-shot knowledge-based vqa, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">From recognition to cognition: Visual commonsense reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Vinvl: Revisiting visual representations in vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, June 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Improving question answering by commonsense-based pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CCF International Conference on Natural Language Processing
and Chinese Computing</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 16–28. Springer, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S.
Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">,
pages 19–27, 2015.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.13625" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.13626" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.13626">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.13626" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.13627" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 00:45:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
