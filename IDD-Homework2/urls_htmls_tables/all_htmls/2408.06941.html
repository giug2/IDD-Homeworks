<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>OpenResearcher: Unleashing AI for Accelerated Scientific Research</title>
<!--Generated on Tue Aug 13 14:52:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.06941v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S1" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S2" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S2.SS1" title="In 2 Related Work ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Academic Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S2.SS2" title="In 2 Related Work ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Industry Research Applications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>OpenResearcher</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3.SS1" title="In 3 OpenResearcher ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Query Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3.SS2" title="In 3 OpenResearcher ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Retrieval Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3.SS3" title="In 3 OpenResearcher ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Routing Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3.SS4" title="In 3 OpenResearcher ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Post-Processing Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3.SS5" title="In 3 OpenResearcher ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Generation Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S3.SS6" title="In 3 OpenResearcher ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Refinement Tools</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S4" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Demonstration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.SS1" title="In 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Evaluation Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.SS2" title="In 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation Applications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.SS3" title="In 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation Metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.SS4" title="In 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Human Preference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.SS5" title="In 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>LLM Preference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S6" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#A1" title="In OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Completed Case</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">OpenResearcher: Unleashing AI for Accelerated Scientific Research</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuxiang Zheng<sup class="ltx_sup" id="id1.1.id1">1,8</sup><span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution.</span></span></span>  Shichao Sun<sup class="ltx_sup" id="id2.2.id2">4,8</sup><span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution.</span></span></span>  Lin Qiu<sup class="ltx_sup" id="id3.3.id3">1</sup>  Dongyu Ru<sup class="ltx_sup" id="id4.4.id4">1</sup>  <span class="ltx_text ltx_font_bold" id="id5.5.id5">Cheng Jiayang<sup class="ltx_sup" id="id5.5.id5.1"><span class="ltx_text ltx_font_medium" id="id5.5.id5.1.1">5</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id6.6.id6">Xuefeng Li<sup class="ltx_sup" id="id6.6.id6.1"><span class="ltx_text ltx_font_medium" id="id6.6.id6.1.1">1,8</span></sup></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id7.7.id7">Jifan Lin<sup class="ltx_sup" id="id7.7.id7.1"><span class="ltx_text ltx_font_medium" id="id7.7.id7.1.1">1,8</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id8.8.id8">Binjie Wang<sup class="ltx_sup" id="id8.8.id8.1"><span class="ltx_text ltx_font_medium" id="id8.8.id8.1.1">3,8</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id9.9.id9">Yun Luo<sup class="ltx_sup" id="id9.9.id9.1"><span class="ltx_text ltx_font_medium" id="id9.9.id9.1.1">6</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id10.10.id10">Renjie Pan<sup class="ltx_sup" id="id10.10.id10.1"><span class="ltx_text ltx_font_medium" id="id10.10.id10.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id11.11.id11">Yang Xu<sup class="ltx_sup" id="id11.11.id11.1"><span class="ltx_text ltx_font_medium" id="id11.11.id11.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id12.12.id12">Qingkai Min<sup class="ltx_sup" id="id12.12.id12.1"><span class="ltx_text ltx_font_medium" id="id12.12.id12.1.1">6</span></sup></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id13.13.id13">Zizhao Zhang<sup class="ltx_sup" id="id13.13.id13.1"><span class="ltx_text ltx_font_medium" id="id13.13.id13.1.1">7</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id14.14.id14">Yiwen Wang<sup class="ltx_sup" id="id14.14.id14.1"><span class="ltx_text ltx_font_medium" id="id14.14.id14.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id15.15.id15">Wenjie Li<sup class="ltx_sup" id="id15.15.id15.1"><span class="ltx_text ltx_font_medium" id="id15.15.id15.1.1">4</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id16.16.id16">Pengfei Liu<sup class="ltx_sup" id="id16.16.id16.1"><span class="ltx_text ltx_font_medium" id="id16.16.id16.1.1">1,2,8</span></sup><span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">2</span></span><span class="ltx_text ltx_font_medium" id="footnotex3.5">Corresponding author.</span></span></span></span></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id17.17.id17">1</sup>Shanghai Jiao Tong University  <sup class="ltx_sup" id="id18.18.id18">2</sup>Shanghai Artificial Intelligence Laboratory  <sup class="ltx_sup" id="id19.19.id19">3</sup>Fudan University 
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id20">4</sup>The Hong Kong Polytechnic University  <sup class="ltx_sup" id="id21.21.id21">5</sup>Hong Kong University of Science and Technology 
<br class="ltx_break"/><sup class="ltx_sup" id="id22.22.id22">6</sup>Westlake University  <sup class="ltx_sup" id="id23.23.id23">7</sup>Tsinghua University  <sup class="ltx_sup" id="id24.24.id24">8</sup>Generative AI Research Lab (GAIR) 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id25.25.id25">catchiz.1@sjtu.edu.cn, pengfei@sjtu.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id26.id1">The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas. We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers. OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge. Moreover, we develop various tools for OpenResearcher to understand researchers’ queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and self-refine these answers. OpenResearcher can flexibly use these tools to balance efficiency and effectiveness. As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs. Demo, video, and code are available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GAIR-NLP/OpenResearcher" title="">https://github.com/GAIR-NLP/OpenResearcher</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">OpenResearcher: Unleashing AI for Accelerated Scientific Research</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Yuxiang Zheng<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.1.1">1,8</span></sup><span class="ltx_note ltx_role_footnote" id="footnotex4"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">1</span></span>Equal contribution.</span></span></span>  Shichao Sun<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.2.1">4,8</span></sup><span class="ltx_note ltx_role_footnote" id="footnotex5"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex5.1.1.1">1</span></span>Equal contribution.</span></span></span>  Lin Qiu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.3.1">1</span></sup>  Dongyu Ru<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.4.1">1</span></sup>  Cheng Jiayang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.5.1">5</span></sup>  Xuefeng Li<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.6"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.6.1">1,8</span></sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Jifan Lin<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.1.1.1">1,8</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.2">Binjie Wang<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.2.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.2.1.1">3,8</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.3">Yun Luo<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.3.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.3.1.1">6</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.4">Renjie Pan<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.4.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.4.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.5">Yang Xu<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.5.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.5.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.6">Qingkai Min<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.6.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.2.1.6.1.1">6</span></sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.3.3.1.1">Zizhao Zhang<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.3.3.1.1.1.1">7</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.3.3.1.2">Yiwen Wang<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.2.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.3.3.1.2.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.3.3.1.3">Wenjie Li<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.3.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.3.3.1.3.1.1">4</span></sup></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.3.3.1.4">Pengfei Liu<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.4.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.3.3.1.4.1.1">1,2,8</span></sup><span class="ltx_note ltx_role_footnote" id="footnotex6"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex6.1.1.1">2</span></span><span class="ltx_text ltx_font_medium" id="footnotex6.5">Corresponding author.</span></span></span></span></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.1">1</sup>Shanghai Jiao Tong University  <sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.2">2</sup>Shanghai Artificial Intelligence Laboratory  <sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.3">3</sup>Fudan University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><sup class="ltx_sup" id="p1.1.2.1.1.5.5.1.1">4</sup>The Hong Kong Polytechnic University  <sup class="ltx_sup" id="p1.1.2.1.1.5.5.1.2">5</sup>Hong Kong University of Science and Technology</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.6.6">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.6.1"><sup class="ltx_sup" id="p1.1.2.1.1.6.6.1.1">6</sup>Westlake University  <sup class="ltx_sup" id="p1.1.2.1.1.6.6.1.2">7</sup>Tsinghua University  <sup class="ltx_sup" id="p1.1.2.1.1.6.6.1.3">8</sup>Generative AI Research Lab (GAIR)</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.7.7">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.7.7.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.7.7.1.1">catchiz.1@sjtu.edu.cn, pengfei@sjtu.edu.cn</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Global scientific publications are growing annually by about 4%-5% <cite class="ltx_cite ltx_citemacro_citep">(Pinedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib21" title="">2024</a>)</cite>, leading researchers to invest significant time and effort in thoroughly reviewing countless academic papers to find the knowledge that propels their research. This involves daily engagement with a wide range of literature to stay updated with the latest developments in their field, which is essential for maintaining the relevance and innovation of their work.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recognizing the challenges and inefficiencies inherent in this process, considerable academic efforts have focused on AI-assisted scientific research <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib29" title="">2023a</a>; Zhai, <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib33" title="">2023</a>)</cite>. They aim to answer the researcher questions from both junior and senior researchers. These questions can be broadly classified into three categories: (1) Scientific Question Answering <cite class="ltx_cite ltx_citemacro_citep">(Pappas et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib20" title="">2020</a>; Ruggeri et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib23" title="">2023</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib12" title="">2023</a>; Pramanick et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib22" title="">2024</a>)</cite>, which seeks detailed information or clarification within a specific domain; (2) Scientific Text Summarization <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib30" title="">2022</a>; Ding et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib4" title="">2023</a>; Takeshita et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib27" title="">2024</a>; Hsu et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib5" title="">2024</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib34" title="">2024</a>)</cite>, aimed at condensing the latest findings and developments into comprehensive overviews; and (3) Scientific Paper Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib1" title="">2019</a>; Kreutz and Schenkel, <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib9" title="">2022</a>; Stergiopoulos et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib25" title="">2024</a>; Pinedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib21" title="">2024</a>)</cite>, which involves suggesting relevant literature and studies based on the researcher’s interests or current inquiries. However, academic applications typically focus on a <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">single</span> task, lacking a unified solution for all questions, allowing researchers to pose any inquiry freely.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Conversely, recent industry applications, like Perplexity AI,<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.perplexity.ai/" title="">https://www.perplexity.ai/</a></span></span></span> iAsk,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://iask.ai/" title="">https://iask.ai/</a></span></span></span> You.com,<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://you.com/" title="">https://you.com/</a></span></span></span> phind,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.phind.com/" title="">https://www.phind.com/</a></span></span></span> and SearchGPT,<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chatgpt.com/search" title="">https://chatgpt.com/search</a></span></span></span> allow users to inquire about anything beyond specific tasks. They use Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib13" title="">2020</a>)</cite> technique to offer an innovative integration of generative Large Language Model (LLM) with web search capability. The core idea behind them is to offer users not just any answer, but the most accurate and contextually relevant information available. However, the <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">proprietary</span> nature of industry applications has hindered their development and may impede academic research in this field.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="515" id="S1.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Main Workflow of OpenResearcher. </figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Besides, both academic and industry applications serve as <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">passive</span> assistants, focusing solely on responding to user inquiries rather than engaging in active communication. To address these issues in academic and industry contexts, we developed OpenResearcher, an open-source project that harnesses AI to accelerate scientific research. Its main workflow is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">1</span></a>, and its main contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Unified Application</span> OpenResearcher can address researchers’ diverse questions, such as Scientific Text Summarization, Scientific Paper Recommendation, etc.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Open-Source</span> OpenResearcher is an impressive open-source system to rival the performance of industry applications.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Active Assistant</span> OpenResearcher can connect in the mind or imagination to pose heuristic questions, guiding users to clarify queries for capturing their intent.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Retrieval Augmented</span> OpenResearcher can retrieve from the Internet and arXiv corpus to provide up-to-date, domain-specific, verified knowledge as supporting evidence.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">Fexible Tool Usage</span> OpenResearcher can flexibly utilize bespoke tools to build a workflow for a better answer. For example, OpenResearcher adaptively calls a refinement tool to refine its initial outcomes. This approach helps avoid the computational cost associated with the unnecessary use of some tools.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i6.p1">
<p class="ltx_p" id="S1.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i6.p1.1.1">Conversational Interaction</span> OpenResearcher enables users to engage in deep discussions through conversational follow-up questions.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Academic Works</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Academic works for scientific research target a specific task, including Scientific Question Answering, Scientific Text Summarization, and Scientific Paper Recommendation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Scientific Question Answering</span> generates answers for questions within extensive scientific articles. In the early days, cloze-style paper question answering datasets, such as emrQA <cite class="ltx_cite ltx_citemacro_citep">(Pampari et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib18" title="">2018</a>)</cite>, BioRead <cite class="ltx_cite ltx_citemacro_citep">(Pappas et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib19" title="">2018</a>)</cite> and BioMRC <cite class="ltx_cite ltx_citemacro_citep">(Pappas et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib20" title="">2020</a>)</cite>, are automatically created with the pre-defined question formats <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib10" title="">2019</a>)</cite>. On the other hand, PubMedQA <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib6" title="">2019</a>)</cite>, BioAsq <cite class="ltx_cite ltx_citemacro_citep">(Krallinger et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib8" title="">2020</a>)</cite> and QASPER <cite class="ltx_cite ltx_citemacro_citep">(Dasigi et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib2" title="">2021</a>)</cite> involve human annotators in question creation. However, the questions are based only on abstracts. Recently, QASA <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib12" title="">2023</a>)</cite> offers advanced questions with annotators reading the entire paper. KIWI <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib32" title="">2024</a>)</cite> uses expert and LLM interactions to refine initial answers into improved long-form answers. SPIQA <cite class="ltx_cite ltx_citemacro_citep">(Pramanick et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib22" title="">2024</a>)</cite> expands text question answering to multimodal question answering.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Scientific Text Summarization</span> aims to condense the long scientific articles into a concise summary. Early works primarily focus on a knowledge graph-centric view <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib30" title="">2022</a>)</cite>. Recently, <cite class="ltx_cite ltx_citemacro_citet">Ding et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib4" title="">2023</a>)</cite> present CocoSciSum, a novel toolkit for controlled summarization of scientific documents, tailored to the scientific community’s needs. <cite class="ltx_cite ltx_citemacro_citet">Takeshita et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib27" title="">2024</a>)</cite> introduce ACLSum, an expert-curated dataset for multi-aspect summarization of scientific papers, thoroughly covering challenges, approaches, and outcomes. <cite class="ltx_cite ltx_citemacro_citet">Hsu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib5" title="">2024</a>)</cite> release CHIME, a dataset that hierarchically organizes scientific studies to facilitate the generation of literature reviews. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib34" title="">2024</a>)</cite> introduces MASSW, a comprehensive dataset for summarizing multi-aspects of scientific workflows.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">Scientific Paper Recommendation</span> assists researchers in discovering relevant and suitable scientific information through recommendations. Early approaches <cite class="ltx_cite ltx_citemacro_citep">(Tanner et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib28" title="">2019</a>; Ma and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib16" title="">2019</a>; Sakib et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib24" title="">2020</a>; Manju et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib17" title="">2020</a>)</cite> in Big Scholarly Data <cite class="ltx_cite ltx_citemacro_citep">(Khan et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib7" title="">2017</a>)</cite> have evolved into recently proposed hybrid recommender systems. <cite class="ltx_cite ltx_citemacro_citet">Pinedo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib21" title="">2024</a>)</cite> develop ArZiGo, a web-based prototype system for searching, managing, and recommending scientific articles. <cite class="ltx_cite ltx_citemacro_citet">Stergiopoulos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib25" title="">2024</a>)</cite> present a novel multi-stage recommendation system employing clustering, graph modeling, and deep learning, capable of operating on a large-scale scientific digital library with millions of users and papers.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">However, these academic efforts focus on a single function without a unified solution for diverse inquiries and lack a user-friendly web application.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Industry Research Applications</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Recent advancements in LLMs have prompted the industry to explore AI assistants for scientific research, like Perplexity AI, iAsk, You.com, phind, and SearchGPT, designed to handle all kinds of research inquiries in a dialogue. These applications combine chatbot-driven search engines with LLMs, which is academically termed Retrieval Augmented Generation (RAG). These applications also provide citations for the evidence behind their responses. However, the closed-source nature has limited their development and academic research in this area.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>OpenResearcher</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">OpenResearcher is designed to leverage AI to speed up the research process by efficiently responding to researchers’ inquiries. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">1</span></a>, OpenResearcher employs RAG to combine LLMs’ internal knowledge with the latest external information. We design a Data Routing strategy for quick and precise information retrieval that can meet time and domain requirements. Lastly, we have developed multiple tools, including query tools, retrieval tools, post-processing tools, generation tools, and refinement tools. OpenResearcher can flexibly use these tools to customize a workflow for each query.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Query Tools</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">A key challenge in retrieval is its dependence on the user’s initial query, which, if imprecise or vague, leads to ineffective results. Junior researchers may struggle to articulate their questions, and scientific terms used across different disciplines add to this complexity. To address this, we have developed tools to help define straightforward questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Active Query</span> OpenResearcher enhances a query by adding extra content and context. It asks users to specify their interest area or discipline. It can ensure that generated answers are highly relevant by covering nuances not initially mentioned.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Query Rewriting</span> The users’ queries are always suboptimal for retrieval, especially in real-world scenarios. Besides, the queries are commonly entailed in complex conversational interactions. Therefore, OpenResearcher rewrites the queries for better clarity and effectiveness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Query Decomposition</span> OpenResearcher decomposes the complex query into a series of sub-queries, improving precision and efficiency for more satisfying responses. Then each sub-query is processed by information retrieval and LLM generation systems accordingly to get the sub-answer.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Retrieval Tools</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">OpenResearcher uses advanced retrieval tools to gather comprehensive and accurate information from the Internet and arXiv corpus.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Internet Retrieval</span> OpenResearcher conducts Internet Retrieval through search engines API to collect relevant online information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Hybrid Retrieval</span> OpenResearcher supports Hybrid Retrieval that employs sparse vector and dense vector representations of both queries and documents. By leveraging these compact vector embeddings, Hybrid Retrieval can more effectively capture semantic similarities and improve the relevance of retrieved documents.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">BM25 Retrieval</span> OpenResearcher conducts BM25 Retrieval, an advanced algorithm used by search engines to rank documents based on their relevance to a query, factoring in term frequency and document length. BM25 stands out for its effectiveness in handling various search queries, making it a widely adopted method in information retrieval.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Routing Strategy</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We develop an advanced Data Routing strategy aimed at optimizing the performance of our hybrid retrieval tool. This retrieval tool currently requires substantial processing times to calculate the similarity between a query and all arXiv paper chunks, which can be resource-intensive.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To address this issue, our strategy is to stratify the data based on both temporal and domain-specific information found in the metadata of the arXiv papers. It distributes data across multiple specialized databases, each aligned with a particular time frame and domain. Consequently, the retrieval tool only scans databases relevant to the query, which speeds up the search process and improves result accuracy by concentrating on the applicable data sets.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Post-Processing Tools</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We develop Post-Processing Tools to rerank, fuse, and filter retrieved information, removing noise and redundancy to provide the most pertinent outcomes for the generation of LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Reranking</span>: OpenResearcher can use a reranking tool to reorder document chunks, prioritizing the most relevant results to condense the retrieval pool.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">Fusion</span>: OpenResearcher can use a fusion tool to fuse the retrieved content from the same source into a single paragraph to enhance the context.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">Filtering</span>: OpenResearcher can use a filtering tool to filter out redundant and noisy content to preserve the most relevant information.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Generation Tools</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">OpenResearcher uses advanced LLMs to produce responses using retrieved information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.1">Generation</span>
OpenResearcher prompts LLMs to utilize retrieved information to generate appropriate responses for user queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.p3.1.1">Citation</span>
OpenResearcher can use a citation tool that employs the BM25 matching algorithm to link retrieved information with the response sentences, providing citations for each.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Refinement Tools</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">OpenResearcher utilizes LLMs to reflect and polish the initial responses, guaranteeing their accuracy and completeness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS6.p2.1.1">Reflection</span>
OpenResearcher prompts LLMs to evaluate the accuracy and completeness of generated responses, meanwhile highlighting grammatical and semantic flaws.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS6.p3.1.1">Polishing</span>
OpenResearcher instructs LLMs to polish responses according to feedback received.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Demonstration</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our web application is built with Streamlit.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://streamlit.io/" title="">https://streamlit.io/</a></span></span></span> Our databases encompass arXiv publications from Jan. 2023 to Jun. 2024, enriched with metadata. This is because most LLMs are trained on pre-2023 data, enabling them to retain this information. This fact also inspires OpenResearcher to answer simple questions without any retrieval, only using LLMs’ internal knowledge. We utilize the state-of-the-art GTE-large model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib15" title="">2023</a>)</cite> as dense vector and efficient-splade-VI-BT-large <cite class="ltx_cite ltx_citemacro_citep">(Lassance and Clinchant, <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib11" title="">2022</a>)</cite> as sparse vector to vectorize our queries and paper chunks. These vectors serve for Hybrid Retrieval, and we use Qdrant<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://qdrant.tech/" title="">https://qdrant.tech/</a></span></span></span> for the vector storage. This Hybrid Retrieval tool extracts the 30 most similar chunks from each selected database. Elasticsearch<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/elastic/elasticsearch" title="">https://github.com/elastic/elasticsearch</a></span></span></span> supports our implementation of BM25 retriever, which extracts up to 80 chunks. The Bing<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bing.com/" title="">https://www.bing.com/</a></span></span></span> API finds 10 relevant outcomes for the Internet Retrieval tool. Besides, we utilize bge-reranker-v2-m3<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/BAAI/bge-reranker-large" title="">https://huggingface.co/BAAI/bge-reranker-large</a></span></span></span> to implement our Reranking tool. This Reranking tool reduces the number of retrieved chunks to 10. Lastly, we use DeepSeek-V2-Chat <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib3" title="">2024</a>)</cite> as our backbone LLM to implement all LLM-powered tools, while also supporting various online LLM APIs and locally deployed LLMs through Ollama.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ollama.com/" title="">https://ollama.com/</a></span></span></span></p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1223" id="S4.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Case between user and OpenResearcher.</figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S4.F2" title="Figure 2 ‣ 4 Demonstration ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">2</span></a>, whose completed screenshot is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#A1.F3" title="Figure 3 ‣ Appendix A Completed Case ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">3</span></a> of Section <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#A1" title="Appendix A Completed Case ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">A</span></a>, demonstrates the strong capability of OpenResearcher. Firstly, OpenResearcher can flexibly construct a tailored workflow for different queries, including simple queries and complex queries. For simple questions like “What is PPO?”, it directly employs LLMs to produce answers. For more complex queries like “Summarize the recent latest developments and variants of PPO?”, it utilizes multiple tools and provides users with essential details, including active queries, rewritten query, decomposed sub-queries and their sub-answers, retrieved outcomes of each sub-query after post-processing, generated final answer, and citation. This example can showcase its flexibility in handling different queries. With this benefit, our OpenResearcher can speed up responses and reduce computational costs.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Secondly, this figure also shows OpenResearcher can pose questions to users for query clarification. Different from previous passive applications that only answer questions, OpenResearcher utilizes LLMs’ internal knowledge to help users specify their question details. This tool is very crucial for junior students who often struggle to clearly express their questions and confusion.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Thirdly, Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S4.F2" title="Figure 2 ‣ 4 Demonstration ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates that OpenResearcher supports conversational question answering, enabling users to engage in multi-turn dialogues. This feature allows for continuous and deeper discussions within OpenResearcher.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Lastly, this figure shows our OpenResearcher can enhance the quality and reliability of generated content by retrieving supporting evidence from the Internet and arXiv corpus. Additionally, we have developed a citation tool that links the generated text to the retrieved information, making it easy for researchers to verify the sources and delve deeper by reading the original papers.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiment</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation Data</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We have collected 109 research questions from more than 20 graduate students, comprising 38 questions on scientific paper recommendation, 38 on scientific text summarization, and 33 on others. These questions arise in their daily scientific research across areas including multimodal, agent, LLM alignment, tool learning, LLM safety, RAG, and others. Answers to these questions are commonly complex and lengthy, requiring graduate students to review many papers. Due to the considerable effort and cost of annotating ground truth answers, we opt to conduct a pairwise comparison instead of providing annotated ground truths.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Applications</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Our baseline includes recent industry applications, containing Perplexity AI, iAsk, You.com, and Phind, complemented by a Naive RAG that only utilizes our hybrid retrieval and LLM generation tools. Regarding our OpenResearcher, we remove the Active Query tool to directly obtain the answer. Our OpenResearcher flexibly uses these tools to generate answers without the need to follow the main workflow sequentially.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.1.1.1.1" rowspan="2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T1.1.1.1.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">Correctness</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T1.1.1.1.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1">Richness</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T1.1.1.1.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.4.1">Relevance</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.1.1">Win</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.2.1">Tie</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.3.1">Lose</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.4.1">Win</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.5.1">Tie</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.6" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.6.1">Lose</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.7" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.7.1">Win</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.8" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.8.1">Tie</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.9" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.2.9.1">Lose</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.3.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.3.1.1.1">Ask</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.5" style="padding-left:10.0pt;padding-right:10.0pt;">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.6" style="padding-left:10.0pt;padding-right:10.0pt;">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.7" style="padding-left:10.0pt;padding-right:10.0pt;">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.8" style="padding-left:10.0pt;padding-right:10.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.9" style="padding-left:10.0pt;padding-right:10.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.10" style="padding-left:10.0pt;padding-right:10.0pt;">20</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.4.2.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.4.2.1.1">You.com</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">21</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.5" style="padding-left:10.0pt;padding-right:10.0pt;">9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.6" style="padding-left:10.0pt;padding-right:10.0pt;">5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.7" style="padding-left:10.0pt;padding-right:10.0pt;">16</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.8" style="padding-left:10.0pt;padding-right:10.0pt;">4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.9" style="padding-left:10.0pt;padding-right:10.0pt;">13</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.10" style="padding-left:10.0pt;padding-right:10.0pt;">13</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.5.3.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.5.3.1.1">Phind</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">26</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.5" style="padding-left:10.0pt;padding-right:10.0pt;">15</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.6" style="padding-left:10.0pt;padding-right:10.0pt;">7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.7" style="padding-left:10.0pt;padding-right:10.0pt;">8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.8" style="padding-left:10.0pt;padding-right:10.0pt;">5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.9" style="padding-left:10.0pt;padding-right:10.0pt;">13</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.10" style="padding-left:10.0pt;padding-right:10.0pt;">12</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.6.4.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.6.4.1.1">Naive RAG</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.5" style="padding-left:10.0pt;padding-right:10.0pt;">14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.6" style="padding-left:10.0pt;padding-right:10.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.7" style="padding-left:10.0pt;padding-right:10.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.8" style="padding-left:10.0pt;padding-right:10.0pt;">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.9" style="padding-left:10.0pt;padding-right:10.0pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.6.4.10" style="padding-left:10.0pt;padding-right:10.0pt;">9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.1.7.5.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T1.1.7.5.1.1">OpenResearcher</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.7.5.2.1">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.7.5.5.1">25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.6" style="padding-left:10.0pt;padding-right:10.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.7" style="padding-left:10.0pt;padding-right:10.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.8" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.7.5.8.1">15</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.9" style="padding-left:10.0pt;padding-right:10.0pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.5.10" style="padding-left:10.0pt;padding-right:10.0pt;">2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Human Preference compared with Perplexity AI outcome. “Win” means that the current method beats Perplexity AI. More “Win” times means a superior application.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation Metric</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In all evaluations, we compared the candidate outcomes from Naive RAG, OpenResearcher, iAsk, You.com, and Phind with those from Perplexity AI. If the candidate outcome outperforms Perplexity AI, it is notated as a “Win”.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We evaluate the generations from the three quality dimensions:
(1) <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Information Correctness</span> assesses the factual accuracy of the answers provided by the candidates. It is critical to determine if the information in each output is correct, as inaccuracies can severely undermine the utility of a QA system. (2) <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.2">Information Richness</span> involves evaluating the depth and scope of the information provided in the answers. Information richness captures whether an answer provides a thorough explanation or context beyond just addressing the question directly. (3) <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.3">Information Relevance</span> judges whether the information presented in the outputs is directly relevant to the question asked. Even if an answer is rich in information and correct, it may not be useful if it does not directly address the query.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Human Preference</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We engaged 12 students with good research experience to conduct the human evaluation. Given the complexity of research questions, we randomly selected 30 questions for human evaluation, ensuring equal coverage of scientific question answering, scientific text summarization, and scientific paper recommendation. For quality control, each instance is annotated by two annotators whose agreement is measured. A third annotator can be involved to resolve disagreements between the two annotators.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">The result is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.T1" title="Table 1 ‣ 5.2 Evaluation Applications ‣ 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">1</span></a> with an overall agreement of 90.67%. Our OpenResearcher achieves superior information correctness, relevance, and richness compared to all other applications. OpenResearcher significantly outperforms Perplexity AI with more “Win” than “Lose”. Specifically, compared to Naive RAG, OpenResearcher demonstrates better performance in all metrics. This suggests that our various tools significantly enhance the quality of the answers.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>LLM Preference</h3>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T2.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Richness</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T2.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">Relevance</span></th>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.1.1">Win</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.2.1">Tie</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.3.1">Lose</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.4.1">Win</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.5.1">Tie</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.6.1">Lose</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.3.1.1.1">iAsk</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.1.6" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.1.7" style="padding-left:4.0pt;padding-right:4.0pt;">71</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.4.2.1.1">You.com</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">15</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">94</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">16</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">93</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.5.3.1.1">Phind</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">52</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">56</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">54</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.7" style="padding-left:4.0pt;padding-right:4.0pt;">55</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.6.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.6.4.1.1">Naive RAG</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.6.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.6.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.6.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.6.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.6.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.6.4.7" style="padding-left:4.0pt;padding-right:4.0pt;">52</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.1.7.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.7.5.1.1">OpenResearcher</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.7.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.5.2.1">62</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.7.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.7.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">45</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.7.5.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.5.5.1">74</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.7.5.6" style="padding-left:4.0pt;padding-right:4.0pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.7.5.7" style="padding-left:4.0pt;padding-right:4.0pt;">35</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>GPT-4 Preference Results compared with Perplexity AI outcome. </figcaption>
</figure>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Inspired by the widespread use of GPT-4 series for pairwise comparison <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib35" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib31" title="">2023b</a>; Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib26" title="">2024</a>)</cite> and their different preferences compared to humans <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#bib.bib14" title="">2024</a>)</cite>, we also utilize GPT-4o for LLM preference evaluation. We evaluate based on two criteria: information richness and relevance, since GPT-4o struggles to verify information accuracy without external knowledge. Despite the availability of citation papers, their length and quantity exceed LLMs’ capacity to confirm factuality.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S5.T2" title="Table 2 ‣ 5.5 LLM Preference ‣ 5 Experiment ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">2</span></a>. This supplemental LLM evaluation further demonstrates our system’s powerful performance. These results show our OpenResearcher achieves the best information relevance and richness among all applications. Furthermore, OpenResearcher surpasses Naive RAG in both metrics, demonstrating its superior performance due to our design.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduce OpenResearcher, an active AI assistant to accelerate the research process, catering to a broad spectrum of inquiries from researchers. OpenResearcher employs Retrieval-Augmented Generation (RAG) to enhance LLMs with the latest, verified, and domain-specific knowledge. It interacts with users to clarify their queries. Moreover, we have developed various tools for OpenResearcher to understand researchers’ queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and refine these answers. OpenResearcher can use these tools flexibly to build a pipeline that delivers accurate and comprehensive answers, outperforming those from industry applications, as judged by human and GPT-4.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">OpenResearcher integrates LLMs and search engines, known as retrieval-augmented generation (RAG), to accelerate scientific research. Despite being instructed to ground the generated responses in retrieved knowledge from scientific publications, LLMs may still generate hallucinations. Consequently, users are advised to verify crucial information derived from our LLM-based features.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2019)</span>
<span class="ltx_bibblock">
Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie Kong, and Feng Xia. 2019.

</span>
<span class="ltx_bibblock">Scientific paper recommendation: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Ieee Access</em>, 7:9324–9339.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et al. (2021)</span>
<span class="ltx_bibblock">
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.365" title="">A dataset of information-seeking questions and answers anchored in research papers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 4599–4610, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et al. (2024)</span>
<span class="ltx_bibblock">
DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen
Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2405.04434" title="">Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Preprint</em>, arXiv:2405.04434.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2023)</span>
<span class="ltx_bibblock">
Yixi Ding, Yanxia Qin, Qian Liu, and Min-Yen Kan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-demo.47" title="">CocoSciSum: A scientific summarization toolkit with compositional controllability</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 518–526, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2024)</span>
<span class="ltx_bibblock">
Chao-Chun Hsu, Erin Bransom, Jenna Sparks, Bailey Kuehl, Chenhao Tan, David Wadden, Lucy Lu Wang, and Aakanksha Naik. 2024.

</span>
<span class="ltx_bibblock">Chime: Llm-assisted hierarchical organization of scientific studies for literature review support.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2407.16148</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2019)</span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1259" title="">PubMedQA: A dataset for biomedical research question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2567–2577, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2017)</span>
<span class="ltx_bibblock">
Samiya Khan, Xiufeng Liu, Kashish A Shakil, and Mansaf Alam. 2017.

</span>
<span class="ltx_bibblock">A survey on scholarly data: From big data perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Information Processing &amp; Management</em>, 53(4):923–944.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krallinger et al. (2020)</span>
<span class="ltx_bibblock">
Martin Krallinger, Anastasia Krithara, Anastasios Nentidis, Georgios Paliouras, and Marta Villegas. 2020.

</span>
<span class="ltx_bibblock">Bioasq at clef2020: Large-scale biomedical semantic indexing and question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14–17, 2020, Proceedings, Part II 42</em>, pages 550–556. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreutz and Schenkel (2022)</span>
<span class="ltx_bibblock">
Christin Katharina Kreutz and Ralf Schenkel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2201.00682" title="">Scientific paper recommendation systems: a literature review of recent publications</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Preprint</em>, arXiv:2201.00682.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00276" title="">Natural questions: A benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Transactions of the Association for Computational Linguistics</em>, 7:452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lassance and Clinchant (2022)</span>
<span class="ltx_bibblock">
Carlos Lassance and Stéphane Clinchant. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3477495.3531833" title="">An efficiency study for splade models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, SIGIR ’22, page 2220–2226, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023.

</span>
<span class="ltx_bibblock">Qasa: advanced question answering on scientific articles.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, ICML’23. JMLR.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf" title="">Retrieval-augmented generation for knowledge-intensive nlp tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Advances in Neural Information Processing Systems</em>, volume 33, pages 9459–9474. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, and Pengfei Liu. 2024.

</span>
<span class="ltx_bibblock">Dissecting human and llm preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2402.11296</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023.

</span>
<span class="ltx_bibblock">Towards general text embeddings with multi-stage contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2308.03281</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma and Wang (2019)</span>
<span class="ltx_bibblock">
Xiao Ma and Ranran Wang. 2019.

</span>
<span class="ltx_bibblock">Personalized scientific paper recommendation based on heterogeneous graph representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Access</em>, 7:79887–79894.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manju et al. (2020)</span>
<span class="ltx_bibblock">
G Manju, P Abhinaya, MR Hemalatha, GG Manju, et al. 2020.

</span>
<span class="ltx_bibblock">Cold start problem alleviation in a research paper recommendation system using the random walk approach on a heterogeneous user-paper graph.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Journal of Intelligent Information Technologies (IJIIT)</em>, 16(2):24–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pampari et al. (2018)</span>
<span class="ltx_bibblock">
Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1258" title="">emrQA: A large corpus for question answering on electronic medical records</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2357–2368, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pappas et al. (2018)</span>
<span class="ltx_bibblock">
Dimitris Pappas, Ion Androutsopoulos, and Haris Papageorgiou. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/L18-1439" title="">BioRead: A new dataset for biomedical reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</em>, Miyazaki, Japan. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pappas et al. (2020)</span>
<span class="ltx_bibblock">
Dimitris Pappas, Petros Stavropoulos, Ion Androutsopoulos, and Ryan McDonald. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.bionlp-1.15" title="">BioMRC: A dataset for biomedical machine reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</em>, pages 140–149, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinedo et al. (2024)</span>
<span class="ltx_bibblock">
Iratxe Pinedo, Mikel Larrañaga, and Ana Arruarte. 2024.

</span>
<span class="ltx_bibblock">Arzigo: A recommendation system for scientific articles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Information Systems</em>, 122:102367.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pramanick et al. (2024)</span>
<span class="ltx_bibblock">
Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. 2024.

</span>
<span class="ltx_bibblock">Spiqa: A dataset for multimodal question answering on scientific papers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2407.09413</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruggeri et al. (2023)</span>
<span class="ltx_bibblock">
Federico Ruggeri, Mohsen Mesgar, and Iryna Gurevych. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.425" title="">A dataset of argumentative dialogues on scientific papers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 7684–7699, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakib et al. (2020)</span>
<span class="ltx_bibblock">
Nazmus Sakib, Rodina Binti Ahmad, and Khalid Haruna. 2020.

</span>
<span class="ltx_bibblock">A collaborative approach toward scientific paper recommendation using citation context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE Access</em>, 8:51246–51255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stergiopoulos et al. (2024)</span>
<span class="ltx_bibblock">
Vaios Stergiopoulos, Michael Vassilakopoulos, Eleni Tousidou, and Antonio Corral. 2024.

</span>
<span class="ltx_bibblock">An academic recommender system on large citation data based on clustering, graph modeling and deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Knowledge and Information Systems</em>, pages 1–34.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Shichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li, and Pengfei Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2406.00507" title="">Prompt chaining or stepwise prompt? refinement in text summarization</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Preprint</em>, arXiv:2406.00507.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Takeshita et al. (2024)</span>
<span class="ltx_bibblock">
Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.naacl-long.371" title="">ACLSum: A new dataset for aspect-based summarization of scientific publications</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, pages 6660–6675, Mexico City, Mexico. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanner et al. (2019)</span>
<span class="ltx_bibblock">
William Tanner, Esra Akbas, and Mir Hasan. 2019.

</span>
<span class="ltx_bibblock">Paper recommendation based on citation relation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2019 IEEE international conference on big data (big data)</em>, pages 3053–3059. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. 2023a.

</span>
<span class="ltx_bibblock">Scientific discovery in the age of artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Nature</em>, 620(7972):47–60.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, and Ting Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.543" title="">Multi-document scientific summarization from a knowledge graph-centric view</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 6222–6233, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023b.

</span>
<span class="ltx_bibblock">Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2306.05087</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Kyle Lo, Luca Soldaini, Bailey Kuehl, Eunsol Choi, and David Wadden. 2024.

</span>
<span class="ltx_bibblock">Kiwi: A dataset of knowledge-intensive writing instructions for answering research questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2403.03866</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai (2023)</span>
<span class="ltx_bibblock">
Xiaoming Zhai. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3589649" title="">Chatgpt for next generation science learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">XRDS</em>, 29(3):42–46.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, and Qiaozhu Mei. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2406.06357" title="">Massw: A new dataset and benchmark tasks for ai-assisted scientific workflows</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Preprint</em>, arXiv:2406.06357.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2306.05685</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Completed Case</h2>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1259" id="A1.F3.g1" src="x3.png" width="540"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Screenshot showing the completed case in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06941v1#S4.F2" title="Figure 2 ‣ 4 Demonstration ‣ OpenResearcher: Unleashing AI for Accelerated Scientific Research"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug 13 14:52:52 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
