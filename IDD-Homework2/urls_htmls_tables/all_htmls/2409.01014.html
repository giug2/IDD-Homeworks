<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.01014] From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model</title><meta property="og:description" content="We explore Bird’s-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various au…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.01014">

<!--Generated on Sun Oct  6 01:48:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaojie Xu, Tianshuo Xu, Fulong Ma and Yingcong Chen
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We explore Bird’s-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various autonomous driving applications. Creating accurate street-view images from BEV maps is essential for portraying complex traffic scenarios and enhancing driving algorithms. Concurrently, diffusion-based conditional image generation models have demonstrated remarkable outcomes, adept at producing diverse, high-quality, and condition-aligned results. Nonetheless, the training of these models demands substantial data and computational resources. Hence, exploring methods to fine-tune these advanced models, like Stable Diffusion, for specific conditional generation tasks emerges as a promising avenue. In this paper, we introduce a practical framework for generating images from a BEV layout. Our approach comprises two main components: the Neural View Transformation and the Street Image Generation. The Neural View Transformation phase converts the BEV map into aligned multi-view semantic segmentation maps by learning the shape correspondence between the BEV and perspective views. Subsequently, the Street Image Generation phase utilizes these segmentations as a condition to guide a fine-tuned latent diffusion model. This finetuning process ensures both view and style consistency. Our model leverages the generative capacity of large pretrained diffusion models within traffic contexts, effectively yielding diverse and condition-coherent street view images.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>The authors Xiaojie Xu, Tianshuo Xu and Fulong Ma are with The Hong Kong University of Science and Technology(Guangzhou), Nansha District, Guangzhou, Guangdong, China. <span id="footnotex1.1" class="ltx_text ltx_font_typewriter">{xxu763, txu647, fmaaf}@connect.hkust-gz.edu.cn</span>
The corresponding author Yingcong Chen is with The Hong Kong University of Science and Technology(Guangzhou), Nansha District, Guangzhou, Guangdong, China and The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong. <span id="footnotex1.2" class="ltx_text ltx_font_typewriter">yingcongchen@ust.hk</span>
</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The emerging era of autonomous driving hinges on the adoption of sophisticated technologies and representations to ensure optimal navigation and decision-making. Among these, the bird’s-eye view (BEV) holds a unique position. By offering a top-down, map-like representation, the BEV provides invaluable insights into the immediate environment, capturing pertinent obstacles and hazards.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While BEV perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> has been a focal point in recent studies, promising to bridge the transformation between street-level views and overhead perspectives, BEV generation—specifically the synthesis of realistic street-view images from a predefined BEV semantic layout—offers untapped potential.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">At its core, BEV generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> translates a semantic layout, which captures a traffic scenario, into tangible street-view images. This translation facilitates an enhanced visualization of traffic scenarios in a real-world setting, making the abstract more accessible. One of the most compelling applications of BEV generation is the intuitive interface it offers for traffic scene visualization and modification. BEV generation allows human operators and system designers to modify a layout effortlessly, producing corresponding street-view images via generative models. This not only streamlines the training of autonomous systems but also serves as an effective testing and validation tool.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.01014/assets/fig/teaser.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="454" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>From a bird’s-eye view semantic map, our framework is capable of generating high-quality and varied camera view images. In terms of map elements, our results closely match the ground truth images. The red boxes (seen in the left four images) represent vehicles, while the yellow lines (in the right two images) delineate road contours.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">BEVGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> represents a pioneering effort in addressing the BEV generation problem. Within its BEV representations, map components are bifurcated into two categories: vehicles and roads. The model employs an autoregressive transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> with a spatial attention design to comprehend the relationship between camera and map perspectives. While BEVGen sets a baseline by generating multi-view images consistent with its map perspective, it doesn’t consistently ensure condition coherence due to its implicit encoding mechanism.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In contrast to the previous method, our proposed method disentangles the view transformation and image generation processes. The view transformation phase focuses on learning the shape correspondence between map and camera perspectives. Here, to project the BEV map onto camera views using camera parameters, we assign height from a prior distribution to each BEV map segment. Using this projection as a preliminary estimate, a convolutional network is employed for shape refinement, achieving a more precise camera view segmentation. This refined segmentation acts as the conditional information for the image generator. For image synthesis, we resort to a conditional latent diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, chosen for its standout performance in conditional image generation tasks. Initially trained on diverse datasets, the diffusion model is fine-tuned using our driving scene imagery. Notably, the fine-tuning procedure encode camera viewpoint explicitly, ensuring that various views yield plausible outcomes (e.g., reasonable orientation of vehicles and roads). Leveraging precise transformed segmentation as condition and the generative ability of the diffusion model, our framework delivers high-quality, diverse, and condition-coherent results.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions are summarized as the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We develop a novel framework for street-view image generation from a BEV layout, leveraging a large, pretrained latent diffusion model. This encompasses view transformation, street-view adaptation, and conditional generation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We explore the methodology of encoding viewpoint for multi-view images and incorporating them into generative diffusion models, through which our method can produce diverse and flexible scenes that match the desired view and layout.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We investigate the potential of utilizing large generative models for the task of BEV image generation and conduct a thorough comparison with other methods that are trained from scratch. Our method is efficient and effective, achieving high-quality and diverse results. Our experimental results demonstrate that our approach outperforms or matches existing methods in terms of visual quality and condition consistency.
</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">RELATED WORK</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Conditional image generation:</span> The field of conditional image generation has seen notable advancements recently, with models predominantly conditioned on text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> or speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> inputs. Varied formats, such as class conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, sketches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, style <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and distinct human poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, can convey the envisaged image specifications. Furthermore, several scholars have explored methodologies with high level representations, including generating images from semantic masks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> or translating intricate constructs like scene graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> into equivalent semantic masks. Diverging from these mentioned paradigms, our emphasis lies on the bird’s-eye view map. Though akin to a semantic segmentation map, it offers a perspective distinct from the resulting image, which is seldom explored in earlier studies.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Image diffusion models:</span> Originally proposed by Sohl-Dickstein et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Image diffusion models have found recent applications in image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The Latent Diffusion Models(LDM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> execute diffusion in the latent image space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, optimizing computational efficiency. Text-to-image diffusion models, by encoding textual inputs into latent vectors using pretrained language models like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, set new benchmarks in image generation. Glide <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> stands out as a text-driven diffusion model for both image creation and editing. Stable Diffusion scales up the concept of latent diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and Imagen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> takes a distinct approach by diffusing pixels through a pyramid structure, bypassing latent imagery. We employ Stable Diffusion as our foundational pretrained model. Through fine-tuning, we adapt it to various viewpoints and driving scenes.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">BEV perception and generation:</span>
Recent growth in large 3D datasets in autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> has propelled studies on map-view perception. Given the disparity between the coordinate frames of inputs and outputs, this domain poses challenges. While inputs derive from calibrated cameras, outputs are rasterized onto a map. A prevalent method assumes a mostly planar scene, simplifying image-to-map transformations via homography <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. However, this can create artifacts for dynamic entities like vehicles. As a solution, some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> utilize depth and semantic maps to present objects in BEV. Alternatively, other methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> bypass explicit geometric modeling to generate map-view predictions directly from images.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">As its counterpart, generating from a BEV map layout remains relatively unexplored. BEVGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> pioneered this domain, employing an auto-regressive transformer to encode the connection between image and BEV representations. In contrast to BEVGen, our approach leverages a large, pretrained diffusion model as the backbone and finetunes it using driving scene images.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.01014/assets/fig/pipeline.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="284" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our two-staged pipeline. Initially, a BEV map is projected and refined to produce semantic maps from the camera’s perspective. These semantic maps, paired with the prompt, are then fed into a pretrained U-Net for iterative denoising. We’ve incorporated street-view adaptation layers into the network to ensure style and viewpoint alignment.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">METHOD</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">The objective of BEV generation is to generate multiple camera-view images from a semantic BEV layout. Earlier studies have represented the BEV layout in either rasterized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or vectorized forms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In this work, we favor the rasterized representation due to its aptness for creating from projections of 3D bounding boxes onto local street maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, or directly from driving simulation frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Consequently, the BEV layout is denoted by <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="B\in\mathbb{R}^{H_{b}\times W_{b}\times c}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">B</mi><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml"><msub id="S3.p1.1.m1.1.1.3.3.2" xref="S3.p1.1.m1.1.1.3.3.2.cmml"><mi id="S3.p1.1.m1.1.1.3.3.2.2" xref="S3.p1.1.m1.1.1.3.3.2.2.cmml">H</mi><mi id="S3.p1.1.m1.1.1.3.3.2.3" xref="S3.p1.1.m1.1.1.3.3.2.3.cmml">b</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.p1.1.m1.1.1.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.cmml"><mi id="S3.p1.1.m1.1.1.3.3.3.2" xref="S3.p1.1.m1.1.1.3.3.3.2.cmml">W</mi><mi id="S3.p1.1.m1.1.1.3.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.3.cmml">b</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1a" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.1.m1.1.1.3.3.4" xref="S3.p1.1.m1.1.1.3.3.4.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝐵</ci><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3"><times id="S3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.1"></times><apply id="S3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.p1.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.3.2.1.cmml" xref="S3.p1.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.p1.1.m1.1.1.3.3.2.2.cmml" xref="S3.p1.1.m1.1.1.3.3.2.2">𝐻</ci><ci id="S3.p1.1.m1.1.1.3.3.2.3.cmml" xref="S3.p1.1.m1.1.1.3.3.2.3">𝑏</ci></apply><apply id="S3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.p1.1.m1.1.1.3.3.3.2.cmml" xref="S3.p1.1.m1.1.1.3.3.3.2">𝑊</ci><ci id="S3.p1.1.m1.1.1.3.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3.3">𝑏</ci></apply><ci id="S3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.p1.1.m1.1.1.3.3.4">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">B\in\mathbb{R}^{H_{b}\times W_{b}\times c}</annotation></semantics></math>, where <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">c</annotation></semantics></math> represents the number of map element categories, such as vehicles and roads.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.7" class="ltx_p">Given the BEV map <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">B</annotation></semantics></math> and <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">n</annotation></semantics></math> camera views <math id="S3.p2.3.m3.3" class="ltx_Math" alttext="(K_{i},R_{i},t_{i})_{i=1}^{n}" display="inline"><semantics id="S3.p2.3.m3.3a"><msubsup id="S3.p2.3.m3.3.3" xref="S3.p2.3.m3.3.3.cmml"><mrow id="S3.p2.3.m3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.p2.3.m3.3.3.3.3.3.4" xref="S3.p2.3.m3.3.3.3.3.4.cmml">(</mo><msub id="S3.p2.3.m3.1.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.p2.3.m3.1.1.1.1.1.1.2" xref="S3.p2.3.m3.1.1.1.1.1.1.2.cmml">K</mi><mi id="S3.p2.3.m3.1.1.1.1.1.1.3" xref="S3.p2.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.3.m3.3.3.3.3.3.5" xref="S3.p2.3.m3.3.3.3.3.4.cmml">,</mo><msub id="S3.p2.3.m3.2.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.2.2.cmml"><mi id="S3.p2.3.m3.2.2.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.2.2.2.cmml">R</mi><mi id="S3.p2.3.m3.2.2.2.2.2.2.3" xref="S3.p2.3.m3.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.p2.3.m3.3.3.3.3.3.6" xref="S3.p2.3.m3.3.3.3.3.4.cmml">,</mo><msub id="S3.p2.3.m3.3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.3.3.cmml"><mi id="S3.p2.3.m3.3.3.3.3.3.3.2" xref="S3.p2.3.m3.3.3.3.3.3.3.2.cmml">t</mi><mi id="S3.p2.3.m3.3.3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p2.3.m3.3.3.3.3.3.7" xref="S3.p2.3.m3.3.3.3.3.4.cmml">)</mo></mrow><mrow id="S3.p2.3.m3.3.3.3.5" xref="S3.p2.3.m3.3.3.3.5.cmml"><mi id="S3.p2.3.m3.3.3.3.5.2" xref="S3.p2.3.m3.3.3.3.5.2.cmml">i</mi><mo id="S3.p2.3.m3.3.3.3.5.1" xref="S3.p2.3.m3.3.3.3.5.1.cmml">=</mo><mn id="S3.p2.3.m3.3.3.3.5.3" xref="S3.p2.3.m3.3.3.3.5.3.cmml">1</mn></mrow><mi id="S3.p2.3.m3.3.3.5" xref="S3.p2.3.m3.3.3.5.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.3b"><apply id="S3.p2.3.m3.3.3.cmml" xref="S3.p2.3.m3.3.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.3.3.4.cmml" xref="S3.p2.3.m3.3.3">superscript</csymbol><apply id="S3.p2.3.m3.3.3.3.cmml" xref="S3.p2.3.m3.3.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.3.3.3.4.cmml" xref="S3.p2.3.m3.3.3">subscript</csymbol><vector id="S3.p2.3.m3.3.3.3.3.4.cmml" xref="S3.p2.3.m3.3.3.3.3.3"><apply id="S3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.2">𝐾</ci><ci id="S3.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.2">𝑅</ci><ci id="S3.p2.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.p2.3.m3.3.3.3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.3.3.3.3.3.3.1.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3">subscript</csymbol><ci id="S3.p2.3.m3.3.3.3.3.3.3.2.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.2">𝑡</ci><ci id="S3.p2.3.m3.3.3.3.3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.3">𝑖</ci></apply></vector><apply id="S3.p2.3.m3.3.3.3.5.cmml" xref="S3.p2.3.m3.3.3.3.5"><eq id="S3.p2.3.m3.3.3.3.5.1.cmml" xref="S3.p2.3.m3.3.3.3.5.1"></eq><ci id="S3.p2.3.m3.3.3.3.5.2.cmml" xref="S3.p2.3.m3.3.3.3.5.2">𝑖</ci><cn type="integer" id="S3.p2.3.m3.3.3.3.5.3.cmml" xref="S3.p2.3.m3.3.3.3.5.3">1</cn></apply></apply><ci id="S3.p2.3.m3.3.3.5.cmml" xref="S3.p2.3.m3.3.3.5">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.3c">(K_{i},R_{i},t_{i})_{i=1}^{n}</annotation></semantics></math>, where <math id="S3.p2.4.m4.3" class="ltx_Math" alttext="K_{i},R_{i},t_{i}" display="inline"><semantics id="S3.p2.4.m4.3a"><mrow id="S3.p2.4.m4.3.3.3" xref="S3.p2.4.m4.3.3.4.cmml"><msub id="S3.p2.4.m4.1.1.1.1" xref="S3.p2.4.m4.1.1.1.1.cmml"><mi id="S3.p2.4.m4.1.1.1.1.2" xref="S3.p2.4.m4.1.1.1.1.2.cmml">K</mi><mi id="S3.p2.4.m4.1.1.1.1.3" xref="S3.p2.4.m4.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.4.m4.3.3.3.4" xref="S3.p2.4.m4.3.3.4.cmml">,</mo><msub id="S3.p2.4.m4.2.2.2.2" xref="S3.p2.4.m4.2.2.2.2.cmml"><mi id="S3.p2.4.m4.2.2.2.2.2" xref="S3.p2.4.m4.2.2.2.2.2.cmml">R</mi><mi id="S3.p2.4.m4.2.2.2.2.3" xref="S3.p2.4.m4.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.p2.4.m4.3.3.3.5" xref="S3.p2.4.m4.3.3.4.cmml">,</mo><msub id="S3.p2.4.m4.3.3.3.3" xref="S3.p2.4.m4.3.3.3.3.cmml"><mi id="S3.p2.4.m4.3.3.3.3.2" xref="S3.p2.4.m4.3.3.3.3.2.cmml">t</mi><mi id="S3.p2.4.m4.3.3.3.3.3" xref="S3.p2.4.m4.3.3.3.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.3b"><list id="S3.p2.4.m4.3.3.4.cmml" xref="S3.p2.4.m4.3.3.3"><apply id="S3.p2.4.m4.1.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.p2.4.m4.1.1.1.1.2.cmml" xref="S3.p2.4.m4.1.1.1.1.2">𝐾</ci><ci id="S3.p2.4.m4.1.1.1.1.3.cmml" xref="S3.p2.4.m4.1.1.1.1.3">𝑖</ci></apply><apply id="S3.p2.4.m4.2.2.2.2.cmml" xref="S3.p2.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.4.m4.2.2.2.2.1.cmml" xref="S3.p2.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.p2.4.m4.2.2.2.2.2.cmml" xref="S3.p2.4.m4.2.2.2.2.2">𝑅</ci><ci id="S3.p2.4.m4.2.2.2.2.3.cmml" xref="S3.p2.4.m4.2.2.2.2.3">𝑖</ci></apply><apply id="S3.p2.4.m4.3.3.3.3.cmml" xref="S3.p2.4.m4.3.3.3.3"><csymbol cd="ambiguous" id="S3.p2.4.m4.3.3.3.3.1.cmml" xref="S3.p2.4.m4.3.3.3.3">subscript</csymbol><ci id="S3.p2.4.m4.3.3.3.3.2.cmml" xref="S3.p2.4.m4.3.3.3.3.2">𝑡</ci><ci id="S3.p2.4.m4.3.3.3.3.3.cmml" xref="S3.p2.4.m4.3.3.3.3.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.3c">K_{i},R_{i},t_{i}</annotation></semantics></math> denotes the intrinsics, extrinsics rotation and extrinsics translation of the <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S3.p2.5.m5.1a"><msub id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">i</mi><mrow id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml"><mi id="S3.p2.5.m5.1.1.3.2" xref="S3.p2.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p2.5.m5.1.1.3.1" xref="S3.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.p2.5.m5.1.1.3.3" xref="S3.p2.5.m5.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝑖</ci><apply id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3"><times id="S3.p2.5.m5.1.1.3.1.cmml" xref="S3.p2.5.m5.1.1.3.1"></times><ci id="S3.p2.5.m5.1.1.3.2.cmml" xref="S3.p2.5.m5.1.1.3.2">𝑡</ci><ci id="S3.p2.5.m5.1.1.3.3.cmml" xref="S3.p2.5.m5.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">i_{th}</annotation></semantics></math> camera, our goal is to generate <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">n</annotation></semantics></math> corresponding images in camera view <math id="S3.p2.7.m7.5" class="ltx_Math" alttext="\mathcal{I}=\{\mathbf{I}^{i}\in\mathbb{R}^{H\times W\times 3}\mid i=1,...,n\}" display="inline"><semantics id="S3.p2.7.m7.5a"><mrow id="S3.p2.7.m7.5.5" xref="S3.p2.7.m7.5.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p2.7.m7.5.5.4" xref="S3.p2.7.m7.5.5.4.cmml">ℐ</mi><mo id="S3.p2.7.m7.5.5.3" xref="S3.p2.7.m7.5.5.3.cmml">=</mo><mrow id="S3.p2.7.m7.5.5.2.2" xref="S3.p2.7.m7.5.5.2.3.cmml"><mo stretchy="false" id="S3.p2.7.m7.5.5.2.2.3" xref="S3.p2.7.m7.5.5.2.3.1.cmml">{</mo><mrow id="S3.p2.7.m7.4.4.1.1.1" xref="S3.p2.7.m7.4.4.1.1.1.cmml"><msup id="S3.p2.7.m7.4.4.1.1.1.2" xref="S3.p2.7.m7.4.4.1.1.1.2.cmml"><mi id="S3.p2.7.m7.4.4.1.1.1.2.2" xref="S3.p2.7.m7.4.4.1.1.1.2.2.cmml">𝐈</mi><mi id="S3.p2.7.m7.4.4.1.1.1.2.3" xref="S3.p2.7.m7.4.4.1.1.1.2.3.cmml">i</mi></msup><mo id="S3.p2.7.m7.4.4.1.1.1.1" xref="S3.p2.7.m7.4.4.1.1.1.1.cmml">∈</mo><msup id="S3.p2.7.m7.4.4.1.1.1.3" xref="S3.p2.7.m7.4.4.1.1.1.3.cmml"><mi id="S3.p2.7.m7.4.4.1.1.1.3.2" xref="S3.p2.7.m7.4.4.1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p2.7.m7.4.4.1.1.1.3.3" xref="S3.p2.7.m7.4.4.1.1.1.3.3.cmml"><mi id="S3.p2.7.m7.4.4.1.1.1.3.3.2" xref="S3.p2.7.m7.4.4.1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p2.7.m7.4.4.1.1.1.3.3.1" xref="S3.p2.7.m7.4.4.1.1.1.3.3.1.cmml">×</mo><mi id="S3.p2.7.m7.4.4.1.1.1.3.3.3" xref="S3.p2.7.m7.4.4.1.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p2.7.m7.4.4.1.1.1.3.3.1a" xref="S3.p2.7.m7.4.4.1.1.1.3.3.1.cmml">×</mo><mn id="S3.p2.7.m7.4.4.1.1.1.3.3.4" xref="S3.p2.7.m7.4.4.1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><mo fence="true" lspace="0em" rspace="0em" id="S3.p2.7.m7.5.5.2.2.4" xref="S3.p2.7.m7.5.5.2.3.1.cmml">∣</mo><mrow id="S3.p2.7.m7.5.5.2.2.2" xref="S3.p2.7.m7.5.5.2.2.2.cmml"><mi id="S3.p2.7.m7.5.5.2.2.2.2" xref="S3.p2.7.m7.5.5.2.2.2.2.cmml">i</mi><mo id="S3.p2.7.m7.5.5.2.2.2.1" xref="S3.p2.7.m7.5.5.2.2.2.1.cmml">=</mo><mrow id="S3.p2.7.m7.5.5.2.2.2.3.2" xref="S3.p2.7.m7.5.5.2.2.2.3.1.cmml"><mn id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml">1</mn><mo id="S3.p2.7.m7.5.5.2.2.2.3.2.1" xref="S3.p2.7.m7.5.5.2.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.p2.7.m7.2.2" xref="S3.p2.7.m7.2.2.cmml">…</mi><mo id="S3.p2.7.m7.5.5.2.2.2.3.2.2" xref="S3.p2.7.m7.5.5.2.2.2.3.1.cmml">,</mo><mi id="S3.p2.7.m7.3.3" xref="S3.p2.7.m7.3.3.cmml">n</mi></mrow></mrow><mo stretchy="false" id="S3.p2.7.m7.5.5.2.2.5" xref="S3.p2.7.m7.5.5.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.5b"><apply id="S3.p2.7.m7.5.5.cmml" xref="S3.p2.7.m7.5.5"><eq id="S3.p2.7.m7.5.5.3.cmml" xref="S3.p2.7.m7.5.5.3"></eq><ci id="S3.p2.7.m7.5.5.4.cmml" xref="S3.p2.7.m7.5.5.4">ℐ</ci><apply id="S3.p2.7.m7.5.5.2.3.cmml" xref="S3.p2.7.m7.5.5.2.2"><csymbol cd="latexml" id="S3.p2.7.m7.5.5.2.3.1.cmml" xref="S3.p2.7.m7.5.5.2.2.3">conditional-set</csymbol><apply id="S3.p2.7.m7.4.4.1.1.1.cmml" xref="S3.p2.7.m7.4.4.1.1.1"><in id="S3.p2.7.m7.4.4.1.1.1.1.cmml" xref="S3.p2.7.m7.4.4.1.1.1.1"></in><apply id="S3.p2.7.m7.4.4.1.1.1.2.cmml" xref="S3.p2.7.m7.4.4.1.1.1.2"><csymbol cd="ambiguous" id="S3.p2.7.m7.4.4.1.1.1.2.1.cmml" xref="S3.p2.7.m7.4.4.1.1.1.2">superscript</csymbol><ci id="S3.p2.7.m7.4.4.1.1.1.2.2.cmml" xref="S3.p2.7.m7.4.4.1.1.1.2.2">𝐈</ci><ci id="S3.p2.7.m7.4.4.1.1.1.2.3.cmml" xref="S3.p2.7.m7.4.4.1.1.1.2.3">𝑖</ci></apply><apply id="S3.p2.7.m7.4.4.1.1.1.3.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.7.m7.4.4.1.1.1.3.1.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3">superscript</csymbol><ci id="S3.p2.7.m7.4.4.1.1.1.3.2.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3.2">ℝ</ci><apply id="S3.p2.7.m7.4.4.1.1.1.3.3.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3.3"><times id="S3.p2.7.m7.4.4.1.1.1.3.3.1.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3.3.1"></times><ci id="S3.p2.7.m7.4.4.1.1.1.3.3.2.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3.3.2">𝐻</ci><ci id="S3.p2.7.m7.4.4.1.1.1.3.3.3.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3.3.3">𝑊</ci><cn type="integer" id="S3.p2.7.m7.4.4.1.1.1.3.3.4.cmml" xref="S3.p2.7.m7.4.4.1.1.1.3.3.4">3</cn></apply></apply></apply><apply id="S3.p2.7.m7.5.5.2.2.2.cmml" xref="S3.p2.7.m7.5.5.2.2.2"><eq id="S3.p2.7.m7.5.5.2.2.2.1.cmml" xref="S3.p2.7.m7.5.5.2.2.2.1"></eq><ci id="S3.p2.7.m7.5.5.2.2.2.2.cmml" xref="S3.p2.7.m7.5.5.2.2.2.2">𝑖</ci><list id="S3.p2.7.m7.5.5.2.2.2.3.1.cmml" xref="S3.p2.7.m7.5.5.2.2.2.3.2"><cn type="integer" id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1">1</cn><ci id="S3.p2.7.m7.2.2.cmml" xref="S3.p2.7.m7.2.2">…</ci><ci id="S3.p2.7.m7.3.3.cmml" xref="S3.p2.7.m7.3.3">𝑛</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.5c">\mathcal{I}=\{\mathbf{I}^{i}\in\mathbb{R}^{H\times W\times 3}\mid i=1,...,n\}</annotation></semantics></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">As depicted in Fig.  <a href="#S2.F2" title="Figure 2 ‣ II RELATED WORK ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our pipeline operates in two stages. Initially, the BEV’s semantic information is projected into the camera view leveraging camera parameters, under a height assumption. This shape is subsequently refined using a CNN. In the succeeding stage, a pre-trained UNet undertakes the backward diffusion process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, where Gaussian noise is progressively eliminated. This UNet receives the polished semantic information coupled with the prompt as conditioning inputs. Furthermore, to ensure accurate viewpoints across various camera perspectives, we fine-tune the network.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Stage I: Neural View Transformation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Taking inspiration from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, we treat the BEV-to-camera view transformation as an image translation task, where the input and output share a pronounced spatial correspondence. We decompose this transformation into two phases: initial setup using camera parameters and shape refinement via a neural network.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p"><span id="S3.SS1.p2.3.1" class="ltx_text ltx_font_bold">Initial projection with camera parameters:</span>
For any world coordinate <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="X\in\mathbb{R}^{3}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">X</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><in id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></in><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑋</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">X\in\mathbb{R}^{3}</annotation></semantics></math>, the perspective transformation describes its corresponding image coordinate <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="x\in\mathbb{R}^{3}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><in id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></in><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑥</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">x\in\mathbb{R}^{3}</annotation></semantics></math> in the view of the <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">i</mi><mrow id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.3.1" xref="S3.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑖</ci><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><times id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.1"></times><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">𝑡</ci><ci id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">i_{th}</annotation></semantics></math> camera by</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="x=K_{i}R_{i}(X-t_{i})" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">x</mi><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">K</mi><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><msub id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.4.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2a" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">X</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">t</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><ci id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">𝑥</ci><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝐾</ci><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.4.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.4.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></minus><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑋</ci><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑡</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">x=K_{i}R_{i}(X-t_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.1" class="ltx_p">in homogeneous coordinates.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The lack of precise height data renders the world coordinates of BEV map data ambiguous, necessitating height estimation. While Inverse Perspective Mapping (IPM) techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> operate under the premise of a flat ground, this assumption can introduce distortions for objects of varied heights, such as buildings and vehicles. Given our focus on roads and vehicles, we retain this simplified assumption for roads.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">For vehicles, we posit that their height adheres to a predetermined distribution. Practically speaking, each vehicle on the BEV map is allocated a height randomly sampled from <math id="S3.SS1.p5.1.m1.2" class="ltx_Math" alttext="\mathcal{U}(1.5,2)" display="inline"><semantics id="S3.SS1.p5.1.m1.2a"><mrow id="S3.SS1.p5.1.m1.2.3" xref="S3.SS1.p5.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p5.1.m1.2.3.2" xref="S3.SS1.p5.1.m1.2.3.2.cmml">𝒰</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.2.3.1" xref="S3.SS1.p5.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS1.p5.1.m1.2.3.3.2" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p5.1.m1.2.3.3.2.1" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">(</mo><mn id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">1.5</mn><mo id="S3.SS1.p5.1.m1.2.3.3.2.2" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p5.1.m1.2.2" xref="S3.SS1.p5.1.m1.2.2.cmml">2</mn><mo stretchy="false" id="S3.SS1.p5.1.m1.2.3.3.2.3" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.2b"><apply id="S3.SS1.p5.1.m1.2.3.cmml" xref="S3.SS1.p5.1.m1.2.3"><times id="S3.SS1.p5.1.m1.2.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.1"></times><ci id="S3.SS1.p5.1.m1.2.3.2.cmml" xref="S3.SS1.p5.1.m1.2.3.2">𝒰</ci><interval closure="open" id="S3.SS1.p5.1.m1.2.3.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.3.2"><cn type="float" id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">1.5</cn><cn type="integer" id="S3.SS1.p5.1.m1.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2">2</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.2c">\mathcal{U}(1.5,2)</annotation></semantics></math>, offering a plausible initial height approximation. With the estimated heights for roads and vehicles in place, the BEV map is projected into camera views using Equ. <a href="#S3.E1" title="In III-A Stage I: Neural View Transformation ‣ III METHOD ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, given the camera parameters.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Shape refinement network:</span>
Through height estimation and projection, we obtain preliminary semantic maps in the camera view. Nonetheless, this simplistic initialization fails to preserve the intricate shapes of map elements accurately. Given that vehicles are rendered on the BEV map using their true 3D bounding boxes as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, our projection approach results in the vehicle appearing as a cube from the camera’s viewpoint. Hence, a shape-refinement post-processing step is imperative.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">The initial projection yields a low-resolution estimate. To address this, we employ an enhanced UNet architecture with residual connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. This network bridges the shape discrepancy between the estimated and the true semantic maps. Functioning as an upsampling module, it outputs high-resolution semantic maps with finer geometry. These refined maps subsequently serve as conditional inputs to the image generator. The contribution of this network to the final image generation outcome is illustrated in Fig.  <a href="#S3.F3" title="Figure 3 ‣ III-A Stage I: Neural View Transformation ‣ III METHOD ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.01014/assets/fig/refine.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The impact of shape refinement on the final image generation is evident. Without refinement, the resulting image (left) resembles a cube. In contrast, the refined version (right) exhibits a more natural form.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.01014/assets/fig/view.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We incorporate viewpoints into our foundational diffusion model by integrating specific views into the text prompts, resulting in distinct View Adaptation Layers. During sampling from the model, we can generate images from a designated camera by invoking its learned novel prompt. </figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Stage II: Street Image Generation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We utilize Stable Diffusion, which is a strong pretrained image generator based on latent diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> framework, as our generative backbone. In this section we discuss how the conditional generation mechanism works and how to adapt the large pretrained model to our driving domain.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p"><span id="S3.SS2.p2.4.1" class="ltx_text ltx_font_bold">Conditional generation with latent diffusion model:</span> Diffusion models can be conceptualized as a uniformly weighted sequence of denoising autoencoders, given by <math id="S3.SS2.p2.1.m1.3" class="ltx_Math" alttext="\begin{aligned} \epsilon_{\theta}(x_{t},t);t=1\ldots T\end{aligned}" display="inline"><semantics id="S3.SS2.p2.1.m1.3a"><mtable id="S3.SS2.p2.1.m1.3.3" xref="S3.SS2.p2.1.m1.3.3.cmml"><mtr id="S3.SS2.p2.1.m1.3.3a" xref="S3.SS2.p2.1.m1.3.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.SS2.p2.1.m1.3.3b" xref="S3.SS2.p2.1.m1.3.3.cmml"><mrow id="S3.SS2.p2.1.m1.3.3.3.3.3" xref="S3.SS2.p2.1.m1.3.3.3.3.3.cmml"><mrow id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.2.cmml"><mrow id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.cmml"><msub id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.2" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.2.cmml">ϵ</mi><mi id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.3" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.2" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.2.cmml">​</mo><mrow id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.2" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.2.cmml">(</mo><msub id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.3" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.2.cmml">,</mo><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.4" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.2" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.2.cmml">;</mo><mi id="S3.SS2.p2.1.m1.2.2.2.2.2.2" xref="S3.SS2.p2.1.m1.2.2.2.2.2.2.cmml">t</mi></mrow><mo id="S3.SS2.p2.1.m1.3.3.3.3.3.4" xref="S3.SS2.p2.1.m1.3.3.3.3.3.4.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.3.3.3.3.3.5" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.cmml"><mn id="S3.SS2.p2.1.m1.3.3.3.3.3.5.2" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.3.3.3.3.3.5.1" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.1.m1.3.3.3.3.3.5.3" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.3.3.3.3.3.5.1a" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.3.3.3.3.3.5.4" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.4.cmml">T</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><matrix id="S3.SS2.p2.1.m1.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3"><matrixrow id="S3.SS2.p2.1.m1.3.3a.cmml" xref="S3.SS2.p2.1.m1.3.3"><apply id="S3.SS2.p2.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3"><eq id="S3.SS2.p2.1.m1.3.3.3.3.3.4.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.4"></eq><list id="S3.SS2.p2.1.m1.3.3.3.3.3.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1"><apply id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1"><times id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.2.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.2"></times><apply id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.2">italic-ϵ</ci><ci id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.3.3">𝜃</ci></apply><interval closure="open" id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1"><apply id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.3.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1">𝑡</ci></interval></apply><ci id="S3.SS2.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2.2.2.2.2">𝑡</ci></list><apply id="S3.SS2.p2.1.m1.3.3.3.3.3.5.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5"><times id="S3.SS2.p2.1.m1.3.3.3.3.3.5.1.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.3.3.3.3.3.5.2.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.2">1</cn><ci id="S3.SS2.p2.1.m1.3.3.3.3.3.5.3.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.3">…</ci><ci id="S3.SS2.p2.1.m1.3.3.3.3.3.5.4.cmml" xref="S3.SS2.p2.1.m1.3.3.3.3.3.5.4">𝑇</ci></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">\begin{aligned} \epsilon_{\theta}(x_{t},t);t=1\ldots T\end{aligned}</annotation></semantics></math>, These autoencoders aim to predict a denoised version of their input <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="x_{t}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑥</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">x_{t}</annotation></semantics></math>, where <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="x_{t}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑥</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">x_{t}</annotation></semantics></math> represents a noisy variant of the original input <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">x</annotation></semantics></math>. This leads to the following objective:</p>
<table id="S3.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2X.2.1.1.m1.8" class="ltx_Math" alttext="\displaystyle L_{DM}=\mathbb{E}_{x,\epsilon\sim\mathcal{N}(0,1),t}\left[\|\epsilon-\epsilon_{\theta}(x_{t},t)\|_{2}^{2}\right]" display="inline"><semantics id="S3.E2X.2.1.1.m1.8a"><mrow id="S3.E2X.2.1.1.m1.8.8" xref="S3.E2X.2.1.1.m1.8.8.cmml"><msub id="S3.E2X.2.1.1.m1.8.8.3" xref="S3.E2X.2.1.1.m1.8.8.3.cmml"><mi id="S3.E2X.2.1.1.m1.8.8.3.2" xref="S3.E2X.2.1.1.m1.8.8.3.2.cmml">L</mi><mrow id="S3.E2X.2.1.1.m1.8.8.3.3" xref="S3.E2X.2.1.1.m1.8.8.3.3.cmml"><mi id="S3.E2X.2.1.1.m1.8.8.3.3.2" xref="S3.E2X.2.1.1.m1.8.8.3.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.8.8.3.3.1" xref="S3.E2X.2.1.1.m1.8.8.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.8.8.3.3.3" xref="S3.E2X.2.1.1.m1.8.8.3.3.3.cmml">M</mi></mrow></msub><mo id="S3.E2X.2.1.1.m1.8.8.2" xref="S3.E2X.2.1.1.m1.8.8.2.cmml">=</mo><mrow id="S3.E2X.2.1.1.m1.8.8.1" xref="S3.E2X.2.1.1.m1.8.8.1.cmml"><msub id="S3.E2X.2.1.1.m1.8.8.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.3.cmml"><mi id="S3.E2X.2.1.1.m1.8.8.1.3.2" xref="S3.E2X.2.1.1.m1.8.8.1.3.2.cmml">𝔼</mi><mrow id="S3.E2X.2.1.1.m1.6.6.6.6" xref="S3.E2X.2.1.1.m1.6.6.6.7.cmml"><mrow id="S3.E2X.2.1.1.m1.6.6.6.6.1" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.cmml"><mrow id="S3.E2X.2.1.1.m1.6.6.6.6.1.2.2" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.2.1.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.3.3" xref="S3.E2X.2.1.1.m1.3.3.3.3.cmml">x</mi><mo id="S3.E2X.2.1.1.m1.6.6.6.6.1.2.2.1" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.2.1.cmml">,</mo><mi id="S3.E2X.2.1.1.m1.4.4.4.4" xref="S3.E2X.2.1.1.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S3.E2X.2.1.1.m1.6.6.6.6.1.1" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.1.cmml">∼</mo><mrow id="S3.E2X.2.1.1.m1.6.6.6.6.1.3" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.2" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.1" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.1.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.2" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.2.1" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">(</mo><mn id="S3.E2X.2.1.1.m1.1.1.1.1" xref="S3.E2X.2.1.1.m1.1.1.1.1.cmml">0</mn><mo id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.2.2" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">,</mo><mn id="S3.E2X.2.1.1.m1.2.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.2.cmml">1</mn><mo stretchy="false" id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.2.3" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.6.6.6.6.2" xref="S3.E2X.2.1.1.m1.6.6.6.7a.cmml">,</mo><mi id="S3.E2X.2.1.1.m1.5.5.5.5" xref="S3.E2X.2.1.1.m1.5.5.5.5.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.8.8.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.2.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.8.8.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.2.cmml"><mo id="S3.E2X.2.1.1.m1.8.8.1.1.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.2.1.cmml">[</mo><msubsup id="S3.E2X.2.1.1.m1.8.8.1.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.3.cmml">ϵ</mi><mo id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.2.cmml">ϵ</mi><mi id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2X.2.1.1.m1.7.7" xref="S3.E2X.2.1.1.m1.7.7.cmml">t</mi><mo stretchy="false" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.3.cmml">2</mn></msubsup><mo id="S3.E2X.2.1.1.m1.8.8.1.1.1.3" xref="S3.E2X.2.1.1.m1.8.8.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.8b"><apply id="S3.E2X.2.1.1.m1.8.8.cmml" xref="S3.E2X.2.1.1.m1.8.8"><eq id="S3.E2X.2.1.1.m1.8.8.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.2"></eq><apply id="S3.E2X.2.1.1.m1.8.8.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.8.8.3.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.8.8.3.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.3.2">𝐿</ci><apply id="S3.E2X.2.1.1.m1.8.8.3.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.3.3"><times id="S3.E2X.2.1.1.m1.8.8.3.3.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.3.3.1"></times><ci id="S3.E2X.2.1.1.m1.8.8.3.3.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.3.3.2">𝐷</ci><ci id="S3.E2X.2.1.1.m1.8.8.3.3.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.3.3.3">𝑀</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.8.8.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1"><times id="S3.E2X.2.1.1.m1.8.8.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.2"></times><apply id="S3.E2X.2.1.1.m1.8.8.1.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.8.8.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.8.8.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.3.2">𝔼</ci><apply id="S3.E2X.2.1.1.m1.6.6.6.7.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.6.6.6.7a.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.2">formulae-sequence</csymbol><apply id="S3.E2X.2.1.1.m1.6.6.6.6.1.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1"><csymbol cd="latexml" id="S3.E2X.2.1.1.m1.6.6.6.6.1.1.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.1">similar-to</csymbol><list id="S3.E2X.2.1.1.m1.6.6.6.6.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.2.2"><ci id="S3.E2X.2.1.1.m1.3.3.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3">𝑥</ci><ci id="S3.E2X.2.1.1.m1.4.4.4.4.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3"><times id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.1"></times><ci id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.2">𝒩</ci><interval closure="open" id="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml" xref="S3.E2X.2.1.1.m1.6.6.6.6.1.3.3.2"><cn type="integer" id="S3.E2X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1">0</cn><cn type="integer" id="S3.E2X.2.1.1.m1.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2">1</cn></interval></apply></apply><ci id="S3.E2X.2.1.1.m1.5.5.5.5.cmml" xref="S3.E2X.2.1.1.m1.5.5.5.5">𝑡</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.8.8.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1"><csymbol cd="latexml" id="S3.E2X.2.1.1.m1.8.8.1.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1">superscript</csymbol><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1">subscript</csymbol><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1"><minus id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.2"></minus><ci id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.3">italic-ϵ</ci><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1"><times id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.2">italic-ϵ</ci><ci id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.3.3">𝜃</ci></apply><interval closure="open" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E2X.2.1.1.m1.7.7.cmml" xref="S3.E2X.2.1.1.m1.7.7">𝑡</ci></interval></apply></apply></apply><cn type="integer" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E2X.2.1.1.m1.8.8.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.8.8.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.8c">\displaystyle L_{DM}=\mathbb{E}_{x,\epsilon\sim\mathcal{N}(0,1),t}\left[\|\epsilon-\epsilon_{\theta}(x_{t},t)\|_{2}^{2}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
<p id="S3.SS2.p2.6" class="ltx_p">with <math id="S3.SS2.p2.5.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p2.5.m1.1a"><mi id="S3.SS2.p2.5.m1.1.1" xref="S3.SS2.p2.5.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m1.1b"><ci id="S3.SS2.p2.5.m1.1.1.cmml" xref="S3.SS2.p2.5.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m1.1c">t</annotation></semantics></math> uniformly sampled from <math id="S3.SS2.p2.6.m2.3" class="ltx_Math" alttext="\{1,\ldots,T\}" display="inline"><semantics id="S3.SS2.p2.6.m2.3a"><mrow id="S3.SS2.p2.6.m2.3.4.2" xref="S3.SS2.p2.6.m2.3.4.1.cmml"><mo stretchy="false" id="S3.SS2.p2.6.m2.3.4.2.1" xref="S3.SS2.p2.6.m2.3.4.1.cmml">{</mo><mn id="S3.SS2.p2.6.m2.1.1" xref="S3.SS2.p2.6.m2.1.1.cmml">1</mn><mo id="S3.SS2.p2.6.m2.3.4.2.2" xref="S3.SS2.p2.6.m2.3.4.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.6.m2.2.2" xref="S3.SS2.p2.6.m2.2.2.cmml">…</mi><mo id="S3.SS2.p2.6.m2.3.4.2.3" xref="S3.SS2.p2.6.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p2.6.m2.3.3" xref="S3.SS2.p2.6.m2.3.3.cmml">T</mi><mo stretchy="false" id="S3.SS2.p2.6.m2.3.4.2.4" xref="S3.SS2.p2.6.m2.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m2.3b"><set id="S3.SS2.p2.6.m2.3.4.1.cmml" xref="S3.SS2.p2.6.m2.3.4.2"><cn type="integer" id="S3.SS2.p2.6.m2.1.1.cmml" xref="S3.SS2.p2.6.m2.1.1">1</cn><ci id="S3.SS2.p2.6.m2.2.2.cmml" xref="S3.SS2.p2.6.m2.2.2">…</ci><ci id="S3.SS2.p2.6.m2.3.3.cmml" xref="S3.SS2.p2.6.m2.3.3">𝑇</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m2.3c">\{1,\ldots,T\}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.3" class="ltx_p">As a large text-to-image diffusion model, latent diffusion introduces CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> encoder <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{T}_{\theta}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">𝒯</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝒯</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathcal{T}_{\theta}</annotation></semantics></math> that projects the text prompt <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">y</annotation></semantics></math> to an intermediate representation <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{T}_{\theta}(y)" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.2" xref="S3.SS2.p3.3.m3.1.2.cmml"><msub id="S3.SS2.p3.3.m3.1.2.2" xref="S3.SS2.p3.3.m3.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.3.m3.1.2.2.2" xref="S3.SS2.p3.3.m3.1.2.2.2.cmml">𝒯</mi><mi id="S3.SS2.p3.3.m3.1.2.2.3" xref="S3.SS2.p3.3.m3.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.2.1" xref="S3.SS2.p3.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS2.p3.3.m3.1.2.3.2" xref="S3.SS2.p3.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.1.2.3.2.1" xref="S3.SS2.p3.3.m3.1.2.cmml">(</mo><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS2.p3.3.m3.1.2.3.2.2" xref="S3.SS2.p3.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.2.cmml" xref="S3.SS2.p3.3.m3.1.2"><times id="S3.SS2.p3.3.m3.1.2.1.cmml" xref="S3.SS2.p3.3.m3.1.2.1"></times><apply id="S3.SS2.p3.3.m3.1.2.2.cmml" xref="S3.SS2.p3.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.2.2.1.cmml" xref="S3.SS2.p3.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.2.2.2.cmml" xref="S3.SS2.p3.3.m3.1.2.2.2">𝒯</ci><ci id="S3.SS2.p3.3.m3.1.2.2.3.cmml" xref="S3.SS2.p3.3.m3.1.2.2.3">𝜃</ci></apply><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\mathcal{T}_{\theta}(y)</annotation></semantics></math>, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)\cdot V" display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.5" xref="S3.E3.m1.4.5.cmml"><mrow id="S3.E3.m1.4.5.2" xref="S3.E3.m1.4.5.2.cmml"><mtext id="S3.E3.m1.4.5.2.2" xref="S3.E3.m1.4.5.2.2a.cmml">Attention</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.2.1" xref="S3.E3.m1.4.5.2.1.cmml">​</mo><mrow id="S3.E3.m1.4.5.2.3.2" xref="S3.E3.m1.4.5.2.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.5.2.3.2.1" xref="S3.E3.m1.4.5.2.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Q</mi><mo id="S3.E3.m1.4.5.2.3.2.2" xref="S3.E3.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">K</mi><mo id="S3.E3.m1.4.5.2.3.2.3" xref="S3.E3.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.E3.m1.4.5.2.3.2.4" xref="S3.E3.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.5.1" xref="S3.E3.m1.4.5.1.cmml">=</mo><mrow id="S3.E3.m1.4.5.3" xref="S3.E3.m1.4.5.3.cmml"><mrow id="S3.E3.m1.4.5.3.2" xref="S3.E3.m1.4.5.3.2.cmml"><mtext id="S3.E3.m1.4.5.3.2.2" xref="S3.E3.m1.4.5.3.2.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.2.1" xref="S3.E3.m1.4.5.3.2.1.cmml">​</mo><mrow id="S3.E3.m1.4.5.3.2.3.2" xref="S3.E3.m1.4.4.cmml"><mo id="S3.E3.m1.4.5.3.2.3.2.1" xref="S3.E3.m1.4.4.cmml">(</mo><mfrac id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.4.4.2" xref="S3.E3.m1.4.4.2.cmml"><mi id="S3.E3.m1.4.4.2.2" xref="S3.E3.m1.4.4.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.1" xref="S3.E3.m1.4.4.2.1.cmml">​</mo><msup id="S3.E3.m1.4.4.2.3" xref="S3.E3.m1.4.4.2.3.cmml"><mi id="S3.E3.m1.4.4.2.3.2" xref="S3.E3.m1.4.4.2.3.2.cmml">K</mi><mi id="S3.E3.m1.4.4.2.3.3" xref="S3.E3.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml"><mi id="S3.E3.m1.4.4.3.2" xref="S3.E3.m1.4.4.3.2.cmml">d</mi></msqrt></mfrac><mo rspace="0.055em" id="S3.E3.m1.4.5.3.2.3.2.2" xref="S3.E3.m1.4.4.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E3.m1.4.5.3.1" xref="S3.E3.m1.4.5.3.1.cmml">⋅</mo><mi id="S3.E3.m1.4.5.3.3" xref="S3.E3.m1.4.5.3.3.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.5.cmml" xref="S3.E3.m1.4.5"><eq id="S3.E3.m1.4.5.1.cmml" xref="S3.E3.m1.4.5.1"></eq><apply id="S3.E3.m1.4.5.2.cmml" xref="S3.E3.m1.4.5.2"><times id="S3.E3.m1.4.5.2.1.cmml" xref="S3.E3.m1.4.5.2.1"></times><ci id="S3.E3.m1.4.5.2.2a.cmml" xref="S3.E3.m1.4.5.2.2"><mtext id="S3.E3.m1.4.5.2.2.cmml" xref="S3.E3.m1.4.5.2.2">Attention</mtext></ci><vector id="S3.E3.m1.4.5.2.3.1.cmml" xref="S3.E3.m1.4.5.2.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑄</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝐾</ci><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">𝑉</ci></vector></apply><apply id="S3.E3.m1.4.5.3.cmml" xref="S3.E3.m1.4.5.3"><ci id="S3.E3.m1.4.5.3.1.cmml" xref="S3.E3.m1.4.5.3.1">⋅</ci><apply id="S3.E3.m1.4.5.3.2.cmml" xref="S3.E3.m1.4.5.3.2"><times id="S3.E3.m1.4.5.3.2.1.cmml" xref="S3.E3.m1.4.5.3.2.1"></times><ci id="S3.E3.m1.4.5.3.2.2a.cmml" xref="S3.E3.m1.4.5.3.2.2"><mtext id="S3.E3.m1.4.5.3.2.2.cmml" xref="S3.E3.m1.4.5.3.2.2">softmax</mtext></ci><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.5.3.2.3.2"><divide id="S3.E3.m1.4.4.1.cmml" xref="S3.E3.m1.4.5.3.2.3.2"></divide><apply id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2"><times id="S3.E3.m1.4.4.2.1.cmml" xref="S3.E3.m1.4.4.2.1"></times><ci id="S3.E3.m1.4.4.2.2.cmml" xref="S3.E3.m1.4.4.2.2">𝑄</ci><apply id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.3.1.cmml" xref="S3.E3.m1.4.4.2.3">superscript</csymbol><ci id="S3.E3.m1.4.4.2.3.2.cmml" xref="S3.E3.m1.4.4.2.3.2">𝐾</ci><ci id="S3.E3.m1.4.4.2.3.3.cmml" xref="S3.E3.m1.4.4.2.3.3">𝑇</ci></apply></apply><apply id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"><root id="S3.E3.m1.4.4.3a.cmml" xref="S3.E3.m1.4.4.3"></root><ci id="S3.E3.m1.4.4.3.2.cmml" xref="S3.E3.m1.4.4.3.2">𝑑</ci></apply></apply></apply><ci id="S3.E3.m1.4.5.3.3.cmml" xref="S3.E3.m1.4.5.3.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)\cdot V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.5" class="ltx_p">, with <math id="S3.SS2.p3.4.m1.7" class="ltx_Math" alttext="Q=W_{Q}^{(i)}\cdot\varphi_{i}(z_{t}),K=W_{K}^{(i)}\cdot\tau_{\theta}(y),V=W_{V}^{(i)}\cdot\tau_{\theta}(y)" display="inline"><semantics id="S3.SS2.p3.4.m1.7a"><mrow id="S3.SS2.p3.4.m1.7.7.2" xref="S3.SS2.p3.4.m1.7.7.3.cmml"><mrow id="S3.SS2.p3.4.m1.6.6.1.1" xref="S3.SS2.p3.4.m1.6.6.1.1.cmml"><mi id="S3.SS2.p3.4.m1.6.6.1.1.3" xref="S3.SS2.p3.4.m1.6.6.1.1.3.cmml">Q</mi><mo id="S3.SS2.p3.4.m1.6.6.1.1.2" xref="S3.SS2.p3.4.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.SS2.p3.4.m1.6.6.1.1.1" xref="S3.SS2.p3.4.m1.6.6.1.1.1.cmml"><mrow id="S3.SS2.p3.4.m1.6.6.1.1.1.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.cmml"><msubsup id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.cmml"><mi id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.2.cmml">W</mi><mi id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.3.cmml">Q</mi><mrow id="S3.SS2.p3.4.m1.1.1.1.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m1.1.1.1.3.1" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.cmml">(</mo><mi id="S3.SS2.p3.4.m1.1.1.1.1" xref="S3.SS2.p3.4.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p3.4.m1.1.1.1.3.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.4.m1.6.6.1.1.1.3.1" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.1.cmml">⋅</mo><msub id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.cmml"><mi id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.2.cmml">φ</mi><mi id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m1.6.6.1.1.1.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.2" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.3" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS2.p3.4.m1.7.7.2.3" xref="S3.SS2.p3.4.m1.7.7.3a.cmml">,</mo><mrow id="S3.SS2.p3.4.m1.7.7.2.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.3.cmml"><mrow id="S3.SS2.p3.4.m1.7.7.2.2.1.1" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.cmml"><mi id="S3.SS2.p3.4.m1.7.7.2.2.1.1.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.2.cmml">K</mi><mo id="S3.SS2.p3.4.m1.7.7.2.2.1.1.1" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.cmml"><mrow id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.cmml"><msubsup id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.cmml"><mi id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.2.cmml">W</mi><mi id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.3" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.3.cmml">K</mi><mrow id="S3.SS2.p3.4.m1.2.2.1.3" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m1.2.2.1.3.1" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.cmml">(</mo><mi id="S3.SS2.p3.4.m1.2.2.1.1" xref="S3.SS2.p3.4.m1.2.2.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p3.4.m1.2.2.1.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.1" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.1.cmml">⋅</mo><msub id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.cmml"><mi id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.2.cmml">τ</mi><mi id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.3" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.1" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.1.cmml">​</mo><mrow id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.3.2.1" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.cmml">(</mo><mi id="S3.SS2.p3.4.m1.4.4" xref="S3.SS2.p3.4.m1.4.4.cmml">y</mi><mo stretchy="false" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.3.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS2.p3.4.m1.7.7.2.2.2.3" xref="S3.SS2.p3.4.m1.7.7.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p3.4.m1.7.7.2.2.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.cmml"><mi id="S3.SS2.p3.4.m1.7.7.2.2.2.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.2.cmml">V</mi><mo id="S3.SS2.p3.4.m1.7.7.2.2.2.2.1" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.1.cmml">=</mo><mrow id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.cmml"><mrow id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.cmml"><msubsup id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.cmml"><mi id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.2.cmml">W</mi><mi id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.3" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.3.cmml">V</mi><mrow id="S3.SS2.p3.4.m1.3.3.1.3" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m1.3.3.1.3.1" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.cmml">(</mo><mi id="S3.SS2.p3.4.m1.3.3.1.1" xref="S3.SS2.p3.4.m1.3.3.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p3.4.m1.3.3.1.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.1" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.1.cmml">⋅</mo><msub id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.cmml"><mi id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.2.cmml">τ</mi><mi id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.3" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.1" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.1.cmml">​</mo><mrow id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.3.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.3.2.1" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.cmml">(</mo><mi id="S3.SS2.p3.4.m1.5.5" xref="S3.SS2.p3.4.m1.5.5.cmml">y</mi><mo stretchy="false" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.3.2.2" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m1.7b"><apply id="S3.SS2.p3.4.m1.7.7.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.3a.cmml" xref="S3.SS2.p3.4.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.4.m1.6.6.1.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1"><eq id="S3.SS2.p3.4.m1.6.6.1.1.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.2"></eq><ci id="S3.SS2.p3.4.m1.6.6.1.1.3.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.3">𝑄</ci><apply id="S3.SS2.p3.4.m1.6.6.1.1.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1"><times id="S3.SS2.p3.4.m1.6.6.1.1.1.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.2"></times><apply id="S3.SS2.p3.4.m1.6.6.1.1.1.3.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3"><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.3.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.1">⋅</ci><apply id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2">superscript</csymbol><apply id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.2">𝑊</ci><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.3.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.2.2.3">𝑄</ci></apply><ci id="S3.SS2.p3.4.m1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m1.1.1.1.1">𝑖</ci></apply><apply id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.2">𝜑</ci><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.3.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.3.3.3">𝑖</ci></apply></apply><apply id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.4.m1.6.6.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply><apply id="S3.SS2.p3.4.m1.7.7.2.2.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.3a.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.4.m1.7.7.2.2.1.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1"><eq id="S3.SS2.p3.4.m1.7.7.2.2.1.1.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.1"></eq><ci id="S3.SS2.p3.4.m1.7.7.2.2.1.1.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.2">𝐾</ci><apply id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3"><times id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.1"></times><apply id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2"><ci id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.1">⋅</ci><apply id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2">superscript</csymbol><apply id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2">subscript</csymbol><ci id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.2">𝑊</ci><ci id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.2.2.3">𝐾</ci></apply><ci id="S3.SS2.p3.4.m1.2.2.1.1.cmml" xref="S3.SS2.p3.4.m1.2.2.1.1">𝑖</ci></apply><apply id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3">subscript</csymbol><ci id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.2">𝜏</ci><ci id="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.1.1.3.2.3.3">𝜃</ci></apply></apply><ci id="S3.SS2.p3.4.m1.4.4.cmml" xref="S3.SS2.p3.4.m1.4.4">𝑦</ci></apply></apply><apply id="S3.SS2.p3.4.m1.7.7.2.2.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2"><eq id="S3.SS2.p3.4.m1.7.7.2.2.2.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.1"></eq><ci id="S3.SS2.p3.4.m1.7.7.2.2.2.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.2">𝑉</ci><apply id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3"><times id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.1"></times><apply id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2"><ci id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.1">⋅</ci><apply id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2">superscript</csymbol><apply id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2">subscript</csymbol><ci id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.2">𝑊</ci><ci id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.2.2.3">𝑉</ci></apply><ci id="S3.SS2.p3.4.m1.3.3.1.1.cmml" xref="S3.SS2.p3.4.m1.3.3.1.1">𝑖</ci></apply><apply id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.1.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3">subscript</csymbol><ci id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.2.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.2">𝜏</ci><ci id="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.3.cmml" xref="S3.SS2.p3.4.m1.7.7.2.2.2.2.3.2.3.3">𝜃</ci></apply></apply><ci id="S3.SS2.p3.4.m1.5.5.cmml" xref="S3.SS2.p3.4.m1.5.5">𝑦</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m1.7c">Q=W_{Q}^{(i)}\cdot\varphi_{i}(z_{t}),K=W_{K}^{(i)}\cdot\tau_{\theta}(y),V=W_{V}^{(i)}\cdot\tau_{\theta}(y)</annotation></semantics></math>.
In this context, <math id="S3.SS2.p3.5.m2.1" class="ltx_Math" alttext="\varphi_{i}(z_{t})" display="inline"><semantics id="S3.SS2.p3.5.m2.1a"><mrow id="S3.SS2.p3.5.m2.1.1" xref="S3.SS2.p3.5.m2.1.1.cmml"><msub id="S3.SS2.p3.5.m2.1.1.3" xref="S3.SS2.p3.5.m2.1.1.3.cmml"><mi id="S3.SS2.p3.5.m2.1.1.3.2" xref="S3.SS2.p3.5.m2.1.1.3.2.cmml">φ</mi><mi id="S3.SS2.p3.5.m2.1.1.3.3" xref="S3.SS2.p3.5.m2.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m2.1.1.2" xref="S3.SS2.p3.5.m2.1.1.2.cmml">​</mo><mrow id="S3.SS2.p3.5.m2.1.1.1.1" xref="S3.SS2.p3.5.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m2.1.1.1.1.2" xref="S3.SS2.p3.5.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.5.m2.1.1.1.1.1" xref="S3.SS2.p3.5.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.5.m2.1.1.1.1.1.2" xref="S3.SS2.p3.5.m2.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS2.p3.5.m2.1.1.1.1.1.3" xref="S3.SS2.p3.5.m2.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS2.p3.5.m2.1.1.1.1.3" xref="S3.SS2.p3.5.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m2.1b"><apply id="S3.SS2.p3.5.m2.1.1.cmml" xref="S3.SS2.p3.5.m2.1.1"><times id="S3.SS2.p3.5.m2.1.1.2.cmml" xref="S3.SS2.p3.5.m2.1.1.2"></times><apply id="S3.SS2.p3.5.m2.1.1.3.cmml" xref="S3.SS2.p3.5.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m2.1.1.3.1.cmml" xref="S3.SS2.p3.5.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.5.m2.1.1.3.2.cmml" xref="S3.SS2.p3.5.m2.1.1.3.2">𝜑</ci><ci id="S3.SS2.p3.5.m2.1.1.3.3.cmml" xref="S3.SS2.p3.5.m2.1.1.3.3">𝑖</ci></apply><apply id="S3.SS2.p3.5.m2.1.1.1.1.1.cmml" xref="S3.SS2.p3.5.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.5.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.5.m2.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS2.p3.5.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.5.m2.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m2.1c">\varphi_{i}(z_{t})</annotation></semantics></math> symbolizes a flattened representation of the U-Net at an intermediate stage.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Our generation task encompasses more than just using the prompt as conditional information. The semantic data transformed from the BEV map serves as a superior control mechanism, given that the resultant image should align spatially with these semantic maps in pixel space. This necessitates a more precise conditioning mechanism for our objective.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Drawing inspiration from ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which employs zero convolution and a trainable duplicate of the original neural network, our approach manipulates the input conditions of neural network blocks. This strategy allows for a more nuanced control over the entire neural network’s behavior. We integrate the pretrained ControlNet layers, designed for semantic segmentation, into our architecture (as depicted in Fig.  <a href="#S2.F2" title="Figure 2 ‣ II RELATED WORK ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). These layers act as conditioning controllers for the image generation process. Even though these semantic control layers were trained on a broader dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, they exhibit robust generalization capabilities in our driving scenarios.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Street-view adaption:</span> Our street view adaptation module serves a dual purpose. Firstly, it emulates the driving scene’s image style found in our dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Secondly, it encapsulates the viewpoints associated with various cameras.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">While fine-tuning the diffusion model using Equ. <a href="#S3.E2" title="In III-B Stage II: Street Image Generation ‣ III METHOD ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> aids in capturing a realistic style specific to driving scenarios, it’s crucial to remember that street scenes, when viewed from different camera perspectives, can vary significantly. For instance, when viewed through our front camera, a vehicle directly ahead should align with our car’s driving direction. In contrast, the same vehicle observed from a side camera would appear at an angle. Likewise, driveable areas typically extend more prominently when viewed from the front and rear cameras but appear more constrained from the side angles.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2409.01014/assets/fig/comp.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>We compare our method (left) with UViT (middle) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and BevGen(right) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Our results demonstrate greater stability and more effective use of conditional information, especially in the highlighted yellow regions where the condition should take effect. For best results, it is recommended to zoom in.</figcaption>
</figure>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">Informed by these insights, we fine-tune our image generator for specific viewpoints. The mechanism for view encoding is detailed in Fig.  <a href="#S3.F4" title="Figure 4 ‣ III-A Stage I: Neural View Transformation ‣ III METHOD ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Taking a cue from DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which hones in on a personalized concept (e.g., a particular dog) as a unique prompt, we treat the viewpoint as an abstract concept and introduce a view-specific loss to optimize the diffusion model. This ensures that the viewpoint is distinct from foundational concepts like cars or streets within the prompts. The training loss is articulated as:</p>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<table id="S3.E4" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E4X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4X.2.1.1.m1.6" class="ltx_math_unparsed" alttext="\displaystyle\mathbb{E}_{\mathbf{x},\mathbf{c},\boldsymbol{\epsilon},\boldsymbol{\epsilon^{\prime}},t}[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}\boldsymbol{\epsilon},\mathbf{c})-\mathbf{x}\|_{2}^{2}+" display="inline"><semantics id="S3.E4X.2.1.1.m1.6a"><mrow id="S3.E4X.2.1.1.m1.6b"><msub id="S3.E4X.2.1.1.m1.6.7"><mi id="S3.E4X.2.1.1.m1.6.7.2">𝔼</mi><mrow id="S3.E4X.2.1.1.m1.5.5.5.5"><mi id="S3.E4X.2.1.1.m1.1.1.1.1">𝐱</mi><mo id="S3.E4X.2.1.1.m1.5.5.5.5.2">,</mo><mi id="S3.E4X.2.1.1.m1.2.2.2.2">𝐜</mi><mo id="S3.E4X.2.1.1.m1.5.5.5.5.3">,</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic" id="S3.E4X.2.1.1.m1.3.3.3.3">ϵ</mi><mo id="S3.E4X.2.1.1.m1.5.5.5.5.4">,</mo><msup id="S3.E4X.2.1.1.m1.5.5.5.5.1"><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic" id="S3.E4X.2.1.1.m1.5.5.5.5.1.2">ϵ</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E4X.2.1.1.m1.5.5.5.5.1.3">′</mo></msup><mo id="S3.E4X.2.1.1.m1.5.5.5.5.5">,</mo><mi id="S3.E4X.2.1.1.m1.4.4.4.4">t</mi></mrow></msub><mrow id="S3.E4X.2.1.1.m1.6.8"><mo stretchy="false" id="S3.E4X.2.1.1.m1.6.8.1">[</mo><msub id="S3.E4X.2.1.1.m1.6.8.2"><mi id="S3.E4X.2.1.1.m1.6.8.2.2">w</mi><mi id="S3.E4X.2.1.1.m1.6.8.2.3">t</mi></msub><mo lspace="0em" rspace="0.167em" id="S3.E4X.2.1.1.m1.6.8.3">∥</mo><msub id="S3.E4X.2.1.1.m1.6.8.4"><mover accent="true" id="S3.E4X.2.1.1.m1.6.8.4.2"><mi id="S3.E4X.2.1.1.m1.6.8.4.2.2">𝐱</mi><mo id="S3.E4X.2.1.1.m1.6.8.4.2.1">^</mo></mover><mi id="S3.E4X.2.1.1.m1.6.8.4.3">θ</mi></msub><mrow id="S3.E4X.2.1.1.m1.6.8.5"><mo stretchy="false" id="S3.E4X.2.1.1.m1.6.8.5.1">(</mo><msub id="S3.E4X.2.1.1.m1.6.8.5.2"><mi id="S3.E4X.2.1.1.m1.6.8.5.2.2">α</mi><mi id="S3.E4X.2.1.1.m1.6.8.5.2.3">t</mi></msub><mi id="S3.E4X.2.1.1.m1.6.8.5.3">𝐱</mi><mo id="S3.E4X.2.1.1.m1.6.8.5.4">+</mo><msub id="S3.E4X.2.1.1.m1.6.8.5.5"><mi id="S3.E4X.2.1.1.m1.6.8.5.5.2">σ</mi><mi id="S3.E4X.2.1.1.m1.6.8.5.5.3">t</mi></msub><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic" id="S3.E4X.2.1.1.m1.6.8.5.6">ϵ</mi><mo id="S3.E4X.2.1.1.m1.6.8.5.7">,</mo><mi id="S3.E4X.2.1.1.m1.6.6">𝐜</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.6.8.5.8">)</mo></mrow><mo id="S3.E4X.2.1.1.m1.6.8.6">−</mo><mi id="S3.E4X.2.1.1.m1.6.8.7">𝐱</mi><msubsup id="S3.E4X.2.1.1.m1.6.8.8"><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.6.8.8.2.2">∥</mo><mn id="S3.E4X.2.1.1.m1.6.8.8.2.3">2</mn><mn id="S3.E4X.2.1.1.m1.6.8.8.3">2</mn></msubsup><mo lspace="0em" id="S3.E4X.2.1.1.m1.6.8.9">+</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E4X.2.1.1.m1.6c">\displaystyle\mathbb{E}_{\mathbf{x},\mathbf{c},\boldsymbol{\epsilon},\boldsymbol{\epsilon^{\prime}},t}[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}\boldsymbol{\epsilon},\mathbf{c})-\mathbf{x}\|_{2}^{2}+</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
<tr id="S3.E4Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4Xa.2.1.1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{view}+\sigma_{t^{\prime}}\boldsymbol{\epsilon^{\prime}},\mathbf{c}_{view})-\mathbf{x}_{view}\|_{2}^{2}]" display="inline"><semantics id="S3.E4Xa.2.1.1.m1.1a"><mrow id="S3.E4Xa.2.1.1.m1.1b"><mi id="S3.E4Xa.2.1.1.m1.1.1">λ</mi><msub id="S3.E4Xa.2.1.1.m1.1.2"><mi id="S3.E4Xa.2.1.1.m1.1.2.2">w</mi><msup id="S3.E4Xa.2.1.1.m1.1.2.3"><mi id="S3.E4Xa.2.1.1.m1.1.2.3.2">t</mi><mo id="S3.E4Xa.2.1.1.m1.1.2.3.3">′</mo></msup></msub><mo lspace="0em" rspace="0.167em" id="S3.E4Xa.2.1.1.m1.1.3">∥</mo><msub id="S3.E4Xa.2.1.1.m1.1.4"><mover accent="true" id="S3.E4Xa.2.1.1.m1.1.4.2"><mi id="S3.E4Xa.2.1.1.m1.1.4.2.2">𝐱</mi><mo id="S3.E4Xa.2.1.1.m1.1.4.2.1">^</mo></mover><mi id="S3.E4Xa.2.1.1.m1.1.4.3">θ</mi></msub><mrow id="S3.E4Xa.2.1.1.m1.1.5"><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.1.5.1">(</mo><msub id="S3.E4Xa.2.1.1.m1.1.5.2"><mi id="S3.E4Xa.2.1.1.m1.1.5.2.2">α</mi><msup id="S3.E4Xa.2.1.1.m1.1.5.2.3"><mi id="S3.E4Xa.2.1.1.m1.1.5.2.3.2">t</mi><mo id="S3.E4Xa.2.1.1.m1.1.5.2.3.3">′</mo></msup></msub><msub id="S3.E4Xa.2.1.1.m1.1.5.3"><mi id="S3.E4Xa.2.1.1.m1.1.5.3.2">𝐱</mi><mrow id="S3.E4Xa.2.1.1.m1.1.5.3.3"><mi id="S3.E4Xa.2.1.1.m1.1.5.3.3.2">v</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.5.3.3.1">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.5.3.3.3">i</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.5.3.3.1a">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.5.3.3.4">e</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.5.3.3.1b">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.5.3.3.5">w</mi></mrow></msub><mo id="S3.E4Xa.2.1.1.m1.1.5.4">+</mo><msub id="S3.E4Xa.2.1.1.m1.1.5.5"><mi id="S3.E4Xa.2.1.1.m1.1.5.5.2">σ</mi><msup id="S3.E4Xa.2.1.1.m1.1.5.5.3"><mi id="S3.E4Xa.2.1.1.m1.1.5.5.3.2">t</mi><mo id="S3.E4Xa.2.1.1.m1.1.5.5.3.3">′</mo></msup></msub><msup id="S3.E4Xa.2.1.1.m1.1.5.6"><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic" id="S3.E4Xa.2.1.1.m1.1.5.6.2">ϵ</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E4Xa.2.1.1.m1.1.5.6.3">′</mo></msup><mo id="S3.E4Xa.2.1.1.m1.1.5.7">,</mo><msub id="S3.E4Xa.2.1.1.m1.1.5.8"><mi id="S3.E4Xa.2.1.1.m1.1.5.8.2">𝐜</mi><mrow id="S3.E4Xa.2.1.1.m1.1.5.8.3"><mi id="S3.E4Xa.2.1.1.m1.1.5.8.3.2">v</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.5.8.3.1">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.5.8.3.3">i</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.5.8.3.1a">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.5.8.3.4">e</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.5.8.3.1b">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.5.8.3.5">w</mi></mrow></msub><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.1.5.9">)</mo></mrow><mo id="S3.E4Xa.2.1.1.m1.1.6">−</mo><msub id="S3.E4Xa.2.1.1.m1.1.7"><mi id="S3.E4Xa.2.1.1.m1.1.7.2">𝐱</mi><mrow id="S3.E4Xa.2.1.1.m1.1.7.3"><mi id="S3.E4Xa.2.1.1.m1.1.7.3.2">v</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.7.3.1">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.7.3.3">i</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.7.3.1a">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.7.3.4">e</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.7.3.1b">​</mo><mi id="S3.E4Xa.2.1.1.m1.1.7.3.5">w</mi></mrow></msub><msubsup id="S3.E4Xa.2.1.1.m1.1.8"><mo lspace="0em" rspace="0.167em" id="S3.E4Xa.2.1.1.m1.1.8.2.2">∥</mo><mn id="S3.E4Xa.2.1.1.m1.1.8.2.3">2</mn><mn id="S3.E4Xa.2.1.1.m1.1.8.3">2</mn></msubsup><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.1.9">]</mo></mrow><annotation encoding="application/x-tex" id="S3.E4Xa.2.1.1.m1.1c">\displaystyle\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{view}+\sigma_{t^{\prime}}\boldsymbol{\epsilon^{\prime}},\mathbf{c}_{view})-\mathbf{x}_{view}\|_{2}^{2}]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS2.p9.5" class="ltx_p">, where <math id="S3.SS2.p9.1.m1.1" class="ltx_Math" alttext="\mathbf{x}_{\theta}" display="inline"><semantics id="S3.SS2.p9.1.m1.1a"><msub id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2" xref="S3.SS2.p9.1.m1.1.1.2.cmml">𝐱</mi><mi id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2">𝐱</ci><ci id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">\mathbf{x}_{\theta}</annotation></semantics></math> represents the base model, <math id="S3.SS2.p9.2.m2.1" class="ltx_Math" alttext="\sigma_{t}" display="inline"><semantics id="S3.SS2.p9.2.m2.1a"><msub id="S3.SS2.p9.2.m2.1.1" xref="S3.SS2.p9.2.m2.1.1.cmml"><mi id="S3.SS2.p9.2.m2.1.1.2" xref="S3.SS2.p9.2.m2.1.1.2.cmml">σ</mi><mi id="S3.SS2.p9.2.m2.1.1.3" xref="S3.SS2.p9.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.2.m2.1b"><apply id="S3.SS2.p9.2.m2.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.2.m2.1.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p9.2.m2.1.1.2.cmml" xref="S3.SS2.p9.2.m2.1.1.2">𝜎</ci><ci id="S3.SS2.p9.2.m2.1.1.3.cmml" xref="S3.SS2.p9.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.2.m2.1c">\sigma_{t}</annotation></semantics></math> and <math id="S3.SS2.p9.3.m3.1" class="ltx_Math" alttext="\sigma_{t^{\prime}}" display="inline"><semantics id="S3.SS2.p9.3.m3.1a"><msub id="S3.SS2.p9.3.m3.1.1" xref="S3.SS2.p9.3.m3.1.1.cmml"><mi id="S3.SS2.p9.3.m3.1.1.2" xref="S3.SS2.p9.3.m3.1.1.2.cmml">σ</mi><msup id="S3.SS2.p9.3.m3.1.1.3" xref="S3.SS2.p9.3.m3.1.1.3.cmml"><mi id="S3.SS2.p9.3.m3.1.1.3.2" xref="S3.SS2.p9.3.m3.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p9.3.m3.1.1.3.3" xref="S3.SS2.p9.3.m3.1.1.3.3.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.3.m3.1b"><apply id="S3.SS2.p9.3.m3.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.3.m3.1.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p9.3.m3.1.1.2.cmml" xref="S3.SS2.p9.3.m3.1.1.2">𝜎</ci><apply id="S3.SS2.p9.3.m3.1.1.3.cmml" xref="S3.SS2.p9.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p9.3.m3.1.1.3.1.cmml" xref="S3.SS2.p9.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p9.3.m3.1.1.3.2.cmml" xref="S3.SS2.p9.3.m3.1.1.3.2">𝑡</ci><ci id="S3.SS2.p9.3.m3.1.1.3.3.cmml" xref="S3.SS2.p9.3.m3.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.3.m3.1c">\sigma_{t^{\prime}}</annotation></semantics></math> refer to distinct Gaussian noises, and <math id="S3.SS2.p9.4.m4.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="S3.SS2.p9.4.m4.1a"><mi id="S3.SS2.p9.4.m4.1.1" xref="S3.SS2.p9.4.m4.1.1.cmml">𝐜</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.4.m4.1b"><ci id="S3.SS2.p9.4.m4.1.1.cmml" xref="S3.SS2.p9.4.m4.1.1">𝐜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.4.m4.1c">\mathbf{c}</annotation></semantics></math> and <math id="S3.SS2.p9.5.m5.1" class="ltx_Math" alttext="\mathbf{c}_{view}" display="inline"><semantics id="S3.SS2.p9.5.m5.1a"><msub id="S3.SS2.p9.5.m5.1.1" xref="S3.SS2.p9.5.m5.1.1.cmml"><mi id="S3.SS2.p9.5.m5.1.1.2" xref="S3.SS2.p9.5.m5.1.1.2.cmml">𝐜</mi><mrow id="S3.SS2.p9.5.m5.1.1.3" xref="S3.SS2.p9.5.m5.1.1.3.cmml"><mi id="S3.SS2.p9.5.m5.1.1.3.2" xref="S3.SS2.p9.5.m5.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.5.m5.1.1.3.1" xref="S3.SS2.p9.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p9.5.m5.1.1.3.3" xref="S3.SS2.p9.5.m5.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.5.m5.1.1.3.1a" xref="S3.SS2.p9.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p9.5.m5.1.1.3.4" xref="S3.SS2.p9.5.m5.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.5.m5.1.1.3.1b" xref="S3.SS2.p9.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p9.5.m5.1.1.3.5" xref="S3.SS2.p9.5.m5.1.1.3.5.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.5.m5.1b"><apply id="S3.SS2.p9.5.m5.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.5.m5.1.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p9.5.m5.1.1.2.cmml" xref="S3.SS2.p9.5.m5.1.1.2">𝐜</ci><apply id="S3.SS2.p9.5.m5.1.1.3.cmml" xref="S3.SS2.p9.5.m5.1.1.3"><times id="S3.SS2.p9.5.m5.1.1.3.1.cmml" xref="S3.SS2.p9.5.m5.1.1.3.1"></times><ci id="S3.SS2.p9.5.m5.1.1.3.2.cmml" xref="S3.SS2.p9.5.m5.1.1.3.2">𝑣</ci><ci id="S3.SS2.p9.5.m5.1.1.3.3.cmml" xref="S3.SS2.p9.5.m5.1.1.3.3">𝑖</ci><ci id="S3.SS2.p9.5.m5.1.1.3.4.cmml" xref="S3.SS2.p9.5.m5.1.1.3.4">𝑒</ci><ci id="S3.SS2.p9.5.m5.1.1.3.5.cmml" xref="S3.SS2.p9.5.m5.1.1.3.5">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.5.m5.1c">\mathbf{c}_{view}</annotation></semantics></math> signify the prompt, either with or without the explicit inclusion of the viewpoint.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para">
<p id="S3.SS2.p10.1" class="ltx_p">Rather than fine-tuning the entire network, we leverage the Low Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> technique to achieve rapid training and enhanced flexibility. The base model is shared across views.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENTS AND RESULTS</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is a comprehensive collection encompassing 1,000 diverse street-view scenes, captured under varied weathers, times of day, and traffic conditions. Spanning over 20 seconds, each scene consists of 40 frames, amounting to a total of 40,000 samples within the entire dataset. Designed to provide a 360° perspective around the ego-vehicle, the data is derived from six distinct camera views, capturing images from the side, front, and back of the vehicle. Every camera view comes with calibrated intrinsics (K) and extrinsics (R, t) for each timestep. Furthermore, objects, including vehicles, are consistently tracked across frames and annotated using 3D bounding boxes derived from LiDAR data. The dataset is organized into 700 training, 150 validation, and 150 testing scenes.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the semantic mask of the vehicle in BEV is rendered with a resolution of (200,200). This is achieved by orthographically projecting the 3D box annotations onto the ground plane, which corresponds to a (100m,100m) region in the real-world context. The road masks are formulated using the NuScenes map devkit, which integrates both lanes and road segments.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Implementations</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Shape refinement network:</span> The shape refinement network is a convnet comprising three down-sampling blocks and four up-sampling blocks. It accepts inputs with a resolution of (56,100) and produces outputs with a resolution of (224,400). Given that the original nuScenes dataset does not include image semantic labels, we employ SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> to generate pseudo labels. We train the network for 10 epoches with a learning rate 1e-7.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Pretrained Stable Diffusion and control module:</span> We utilize the pretrained Stable Diffusion model ”RealisticVision” available on HuggingFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. The control module is adapted from ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which was originally trained on the ADE20K dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and captioned using BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Street view adaptation module:</span> For each camera view, we use a set of 100 images to train the respective adaptation module. Our foundational prompts for regularization include ”road”, ”car”, and ”street background”. To specify viewpoints, we use alphanumeric designations (e.g., cam0) to prevent any overlap with existing concepts within the pretrained CLIP text encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. During finetuning, the image resolution is set at (400, 224). The training extends over 5000 steps with a batch size of 4 and a learning rate set at 1e-4. The rank for LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> is set to 16.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Qualitative result:</span> We juxtapose our approach against BEVGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and a from-scratch trained latent diffusion model using a transformer architecture, specifically UViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Notably, our strategy involves finetuning a pre-trained, expansive model, while the other two approaches train their models from the ground up. The results can be observed in Fig.  <a href="#S3.F5" title="Figure 5 ‣ III-B Stage II: Street Image Generation ‣ III METHOD ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Our method showcases superior stability in image quality, and its conditioning mechanism proves to be effective. Both UViT and BevGen employ cross attention to manage conditional information. However, their models occasionally falter due to the absence of explicit spatial relationships between the semantic and the resultant generated images. This makes it challenging for their conditioning mechanisms to consistently function effectively. Concerning image quality and diversity, methods that are trained from scratch tend to be closely tied to specific datasets, often risking overfitting. In particular, the UViT-based diffusion model faces challenges when trained with a limited dataset.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.01014/assets/fig/diverse.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Leveraging the robust generative capabilities of the large pretrained Stable Diffusion model, our generated outcomes display remarkable diversity. This figure presents results under varying weather conditions, all derived from a consistent BEV input. The red region illustrates the road variations in the driving scene image due to changing weather conditions. For best results, it is recommended to zoom in.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In Fig.  <a href="#S4.F6" title="Figure 6 ‣ IV-C Results ‣ IV EXPERIMENTS AND RESULTS ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we showcase additional illustrations underscoring the diversity of our generated outcomes. Our methodology effortlessly facilitates the generation of images under various weather scenarios, significantly enhancing the model’s adaptability.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Quantitative result:</span> In Table.  <a href="#S4.T1" title="TABLE I ‣ IV-C Results ‣ IV EXPERIMENTS AND RESULTS ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we juxtapose our approach with the benchmark BEVGen and a transformer-driven diffusion model. Utilizing the Frechet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, akin to BEVGen, we evaluate the congruence between the generated images and the training dataset. While our outputs are visually appealing and consistent, our FID score lags behind BEVGen. This can be attributed to our reliance on limited data for fine-tuning, hence the visual style largely remains anchored to the foundational diffusion model. For a more equitable comparison, we trained a UViT-based latent diffusion model from scratch, which yielded an even less favorable FID score. This suggests that the scope of the training dataset might be insufficient, complicating the task of cultivating a robust diffusion model from scratch.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">Further, we assessed our methodology using a pretrained BEV segmentation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. To gauge the congruity between the predicted and actual BEV segmentation maps, we employed the mean Intersection over Union (mIOU). The findings reveal that in the context of roads, our model stands shoulder to shoulder with the baseline. Given that roads are consistently obscured, it poses a challenge for our refinement model to assimilate an accurate road contour. Conversely, for vehicles, our method substantially outperforms the baseline, underscoring the potency of our segment-focused conditioning and viewpoint encoding techniques.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.3" class="ltx_tr">
<th id="S4.T1.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID<math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Road mIOU<math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Vehicle mIOU<math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.4.1" class="ltx_tr">
<th id="S4.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BEVGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S4.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">25.54</td>
<td id="S4.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">50.20</td>
<td id="S4.T1.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">5.89</td>
</tr>
<tr id="S4.T1.3.5.2" class="ltx_tr">
<th id="S4.T1.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<td id="S4.T1.3.5.2.2" class="ltx_td ltx_align_center">79.22</td>
<td id="S4.T1.3.5.2.3" class="ltx_td ltx_align_center">37.69</td>
<td id="S4.T1.3.5.2.4" class="ltx_td ltx_align_center">9.16</td>
</tr>
<tr id="S4.T1.3.6.3" class="ltx_tr">
<th id="S4.T1.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Ours</th>
<td id="S4.T1.3.6.3.2" class="ltx_td ltx_align_center ltx_border_bb">48.65</td>
<td id="S4.T1.3.6.3.3" class="ltx_td ltx_align_center ltx_border_bb">47.45</td>
<td id="S4.T1.3.6.3.4" class="ltx_td ltx_align_center ltx_border_bb">17.70</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span> Quantitative comparision between BEVGen, UViT and our method. </figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Ablation Studies</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In our research, we carried out ablation studies, specifically honing in on two of our core innovations: the shape refinement process and the street view adaptation technique. The detailed results of these studies can be found in Table.  <a href="#S4.T2" title="TABLE II ‣ IV-D Ablation Studies ‣ IV EXPERIMENTS AND RESULTS ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The shape refinement process is pivotal in ensuring that map elements are accurately positioned. When the shape within the camera’s perspective aligns more semantically, it resonates more effectively with the given prompt. On the other hand, the street view adaptation module plays a crucial role as a style encoder. Its primary function is to make sure that the generated images bear a strong resemblance to those in the training dataset. Moreover, this module greatly assists the image generator by enabling it to achieve proper and accurate orientations for the various map elements.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3" class="ltx_tr">
<th id="S4.T2.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Road mIOU<math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Vehicle mIOU<math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.4.1" class="ltx_tr">
<th id="S4.T2.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Base diffusion</th>
<td id="S4.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">82.25</td>
<td id="S4.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">46.76</td>
<td id="S4.T2.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">11.82</td>
</tr>
<tr id="S4.T2.3.5.2" class="ltx_tr">
<th id="S4.T2.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ shape refinement</th>
<td id="S4.T2.3.5.2.2" class="ltx_td ltx_align_center">78.13</td>
<td id="S4.T2.3.5.2.3" class="ltx_td ltx_align_center">47.92</td>
<td id="S4.T2.3.5.2.4" class="ltx_td ltx_align_center">15.69</td>
</tr>
<tr id="S4.T2.3.6.3" class="ltx_tr">
<th id="S4.T2.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">+ view adaptation</th>
<td id="S4.T2.3.6.3.2" class="ltx_td ltx_align_center ltx_border_bb">48.65</td>
<td id="S4.T2.3.6.3.3" class="ltx_td ltx_align_center ltx_border_bb">47.45</td>
<td id="S4.T2.3.6.3.4" class="ltx_td ltx_align_center ltx_border_bb">17.70</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span> Ablation study on our core design: shape refinement and street view adaptation. </figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">LIMITATIONS AND FUTURE WORKS</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the specific setup we’ve devised, the integration of multiple cameras has the capability to produce a comprehensive panoramic image that boasts a significantly extended aspect ratio. This is a departure from traditional images and poses a unique challenge. Ideally, the most efficient approach would be to directly generate a panoramic or multi-view image, as this would inherently uphold and maintain the consistency of the view throughout the image. But herein lies the challenge: the vast majority of large-scale image diffusion models available today have been fundamentally trained to cater to standard, more conventional aspect ratios. As a result, these models, when applied to our specific need, fall short. This limitation is clearly demonstrated in Fig.  <a href="#S5.F7" title="Figure 7 ‣ V LIMITATIONS AND FUTURE WORKS ‣ From Bird’s-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. These models face considerable difficulty when tasked with rendering high-quality images that demand a broad and expansive field-of-view.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2409.01014/assets/fig/ring.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Creating consistently aligned multi-view images poses a challenge for large pretrained diffusion models, given their typical training on standard datasets. </figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Recognizing this gap, our future endeavors will be centered around delving deeper and exploring more robust and effective techniques that can leverage these large image diffusion models to seamlessly produce multi-view images.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSION</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We introduced an innovative framework for generating street-view images from a BEV layout by harnessing the power of a robust, pretrained latent diffusion model. Our methodology integrates view transformation, street-view adaptation, and conditional generation. When compared to baseline models trained from scratch, our model excels in terms of image quality, conditioning precision, and diversity.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Philion and S. Fidler, “Lift, splat, shoot: Encoding images from arbitrary
camera rigs by implicitly unprojecting to 3d,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XIV 16</em>.   Springer, 2020, pp. 194–210.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B. Zhou and P. Krähenbühl, “Cross-view transformers for real-time
map-view semantic segmentation,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2022, pp.
13 760–13 769.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai,
“Bevformer: Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">European conference on
computer vision</em>.   Springer, 2022, pp.
1–18.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Swerdlow, R. Xu, and B. Zhou, “Street-view image generation from a
bird’s-eye view layout,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.04634</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution
image synthesis with latent diffusion models,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp.
10 684–10 695.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. D. M. Reddy, M. S. M. Basha, M. M. C. Hari, and M. N. Penchalaiah, “Dall-e:
Creating images from text,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">UGC Care Group I Journal</em>, vol. 8, no. 14,
pp. 71–75, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour,
R. Gontijo Lopes, B. Karagol Ayan, T. Salimans <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Photorealistic text-to-image diffusion models with deep language
understanding,” <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol. 35, pp. 36 479–36 494, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L. Chen, S. Srivastava, Z. Duan, and C. Xu, “Deep cross-modal audio-visual
generation,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the on Thematic Workshops of ACM
Multimedia 2017</em>, 2017, pp. 349–357.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for high
fidelity natural image synthesis,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.11096</em>,
2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2017, pp. 1125–1134.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using
convolutional neural networks,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference
on computer vision and pattern recognition</em>, 2016, pp. 2414–2423.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool, “Pose
guided person image generation,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image synthesis with
spatially-adaptive normalization,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2019, pp. 2337–2346.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Dhamo, A. Farshad, I. Laina, N. Navab, G. D. Hager, F. Tombari, and
C. Rupprecht, “Semantic image manipulation using scene graphs,” in
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition</em>, 2020, pp. 5213–5222.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Li, J. Wu, I. Koh, Y. Tang, and L. Sun, “Image synthesis from layout with
locality-aware mask adaption,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 13 819–13 828.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
unsupervised learning using nonequilibrium thermodynamics,” in
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2015, pp. 2256–2265.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,”
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 34, pp.
8780–8794, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution
image synthesis,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2021, pp. 12 873–12 883.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Learning transferable visual
models from natural language supervision,” in <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">International conference
on machine learning</em>.   PMLR, 2021, pp.
8748–8763.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew,
I. Sutskever, and M. Chen, “Glide: Towards photorealistic image generation
and editing with text-guided diffusion models,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2112.10741</em>, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for
autonomous driving,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</em>, 2020, pp. 11 621–11 631.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo,
Y. Zhou, Y. Chai, B. Caine <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Scalability in perception for
autonomous driving: Waymo open dataset,” in <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp.
2446–2454.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh, S. Khandelwal, B. Pan,
R. Kumar, A. Hartnett, J. K. Pontes <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Argoverse 2: Next
generation datasets for self-driving perception and forecasting,”
<em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.00493</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S. Sengupta, P. Sturgess, L. Ladickỳ, and P. H. Torr, “Automatic dense
visual semantic mapping from street-level imagery,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2012 IEEE/RSJ
International Conference on Intelligent Robots and Systems</em>.   IEEE, 2012, pp. 857–862.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Wang, B. Liu, S. Schulter, and M. Chandraker, “A parametric top-view
representation of complex road scenes,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
10 325–10 333.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Schulter, M. Zhai, N. Jacobs, and M. Chandraker, “Learning to look around
objects for top-view representations of outdoor scenes,” in
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</em>,
2018, pp. 787–802.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Liu, T. Yuan, Y. Wang, Y. Wang, and H. Zhao, “Vectormapnet: End-to-end
vectorized hd map learning,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>.   PMLR, 2023, pp.
22 352–22 369.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive: Composing
diverse driving scenarios for generalizable reinforcement learning,”
<em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>,
vol. 45, no. 3, pp. 3461–3475, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Saha, O. Mendez, C. Russell, and R. Bowden, “Translating images into
maps,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2022 International conference on robotics and automation
(ICRA)</em>.   IEEE, 2022, pp. 9200–9206.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
H. A. Mallot, H. H. Bülthoff, J. Little, and S. Bohrer, “Inverse
perspective mapping simplifies optical flow computation and obstacle
detection,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Biological cybernetics</em>, vol. 64, no. 3, pp. 177–185,
1991.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Chaurasia and E. Culurciello, “Linknet: Exploiting encoder representations
for efficient semantic segmentation,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2017 IEEE visual
communications and image processing (VCIP)</em>.   IEEE, 2017, pp. 1–4.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
L. Zhang and M. Agrawala, “Adding conditional control to text-to-image
diffusion models,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.05543</em>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, “Scene
parsing through ade20k dataset,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference
on computer vision and pattern recognition</em>, 2017, pp. 633–641.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, “All are worth
words: A vit backbone for diffusion models,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp.
22 669–22 679.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman,
“Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
generation,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>, 2023, pp. 22 500–22 510.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen, “Lora: Low-rank adaptation of large language models,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer:
Simple and efficient design for semantic segmentation with transformers,”
<em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp.
12 077–12 090, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. M. Jain, “Hugging face,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Introduction to Transformers for NLP:
With the Hugging Face Library and Models to Solve Problems</em>.   Springer, 2022, pp. 51–67.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image
pre-training for unified vision-language understanding and generation,” in
<em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2022, pp. 12 888–12 900.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans
trained by a two time-scale update rule converge to a local nash
equilibrium,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
vol. 30, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.01013" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.01014" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.01014">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.01014" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.01015" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:48:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
