<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.10966] Visual Question Answering Using Semantic Information from Image Descriptions</title><meta property="og:description" content="In this work, we propose a deep neural architecture that uses an attention mechanism which utilizes region based image features, the natural language question asked, and semantic knowledge extracted from the regions of…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering Using Semantic Information from Image Descriptions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering Using Semantic Information from Image Descriptions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.10966">

<!--Generated on Sun Mar 17 07:17:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Visual Question Answering Using Semantic Information from Image Descriptions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tasmia Tasrin 
<br class="ltx_break">University of Kentucky
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">tta245@uky.edu</span>
<span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Md Sultan Al Nahian<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break">University of Kentucky
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">mna245@uky.edu</span>
<span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Brent Harrison
<br class="ltx_break">University of Kentucky
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">harrison@cs.uky.edu</span>
</span><span class="ltx_author_notes">These authors have contributed equally.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<blockquote id="id6.id1" class="ltx_quote">
<p id="id6.id1.1" class="ltx_p">In this work, we propose a deep neural architecture that uses an attention mechanism which utilizes region based image features, the natural language question asked, and semantic knowledge extracted from the regions of an image to produce open-ended answers for questions asked in a visual question answering (VQA) task.
The combination of both region based features and region based textual information about the image bolsters a model to more accurately respond to questions and potentially do so with less required training data.
We evaluate our proposed architecture on a VQA task against a strong baseline and show that our method achieves excellent results on this task.
</p>
</blockquote>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Visual Question Answering (VQA) is a task in which a system provides natural language answers to questions concerning an image.
This is typically accomplished using deep learning systems that extract textual features from the question and image features from the related image.
One difficulty that arises with this approach is that there is very little text signal that networks can use to derive semantic information in the image.
This means that either 1) Large amounts of question and answer data must be gathered for ML techniques to learn effectively or 2) Approaches will struggle to draw a connection between image features and text semantics.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">To overcome these shortcomings, we propose to augment a deep learning architecture that utilizes neural attention with additional, external knowledge about the image.
This type of approach has been used in the past (<span id="Sx1.p2.1.1" class="ltx_text ltx_font_bold">?</span>; <span id="Sx1.p2.1.2" class="ltx_text ltx_font_bold">?</span>; <span id="Sx1.p2.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="Sx1.p2.1.4" class="ltx_text ltx_font_bold">?</span>); however, our work seeks to take advantage of a different form of knowledge.
Our resulting network, which we call the Visual Question Answering-Contextual Information network (VQA-CoIn), improves upon past work by extending it to incorporate semantic information extracted from every regions an image via image descriptions.
We hypothesize that this will help bridge the gap between image features and natural language text, which should improve network understanding.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">In our proposed model, we have extend the work in (<span id="Sx1.p3.1.1" class="ltx_text ltx_font_bold">?</span>) by incorporating semantic information of an image’s regions in addition to the image features and questions.
To evaluate the effectiveness that this additional information has, We have used the VQA v2.0 (<span id="Sx1.p3.1.2" class="ltx_text ltx_font_bold">?</span>) dataset to train and evaluate our architecture.
For generating semantic information for each image of the dataset, we utilize Densecap(<span id="Sx1.p3.1.3" class="ltx_text ltx_font_bold">?</span>) to extract all possible image regions and produce image captions for them.
We preprocess these captions to extract the important words out of them as we want to make sure that the information limits to a fixed length and the deep learning network used to encode these words focus on the meaningful texts.
For evaluation of our network, we use accuracy as the evaluation metric like all prior works.
To measure the test accuracy score, we use VQA challenge hosting site EvalAI where we submit the generated results and the site measures the scores for 3 categorical questions and overall test accuracy for the test split data.
In addition, we also evaluate how well our techniques scales with data compared to other techniques by testing with different percentages of the training dataset.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Works</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The introduction of the VQA v1.0 and VQA v2.0 datasets (<span id="Sx2.p1.1.1" class="ltx_text ltx_font_bold">?</span>; <span id="Sx2.p1.1.2" class="ltx_text ltx_font_bold">?</span>) have drastically accelerated research in this area.
As a result, some interesting works like (<span id="Sx2.p1.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="Sx2.p1.1.4" class="ltx_text ltx_font_bold">?</span>; <span id="Sx2.p1.1.5" class="ltx_text ltx_font_bold">?</span>; <span id="Sx2.p1.1.6" class="ltx_text ltx_font_bold">?</span>; <span id="Sx2.p1.1.7" class="ltx_text ltx_font_bold">?</span>) have been proposed for the advancement in VQA.
For instance, while the work in (<span id="Sx2.p1.1.8" class="ltx_text ltx_font_bold">?</span>) focuses on putting stacked attention on images, (<span id="Sx2.p1.1.9" class="ltx_text ltx_font_bold">?</span>) has assigned bottom-up attention to figure out the regions and then used top-down mechanism to determine the important features.
Both (<span id="Sx2.p1.1.10" class="ltx_text ltx_font_bold">?</span>) and (<span id="Sx2.p1.1.11" class="ltx_text ltx_font_bold">?</span>) are the winners of the VQA challenge 2016 and 2017 respectively.
The region based features introduced by (<span id="Sx2.p1.1.12" class="ltx_text ltx_font_bold">?</span>) have been utilized by several approaches (<span id="Sx2.p1.1.13" class="ltx_text ltx_font_bold">?</span>; <span id="Sx2.p1.1.14" class="ltx_text ltx_font_bold">?</span>) including ours.
In the 2018 VQA challenge, authors improved upon the 2017 winner by changing learning schedule, fine tuning the model, and using both grid level features and region features of images (<span id="Sx2.p1.1.15" class="ltx_text ltx_font_bold">?</span>).
But recently a research work has revisited grid based features of VQA and used them in end-to-end training (<span id="Sx2.p1.1.16" class="ltx_text ltx_font_bold">?</span>), which has produced strong results.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">There has been prior work that merges elements of image and text for the VQA task.
For example, Kim and Bansal utilized both paragraph captions and object properties described using sentences as prior knowledge to aid in the VQA task on the visual genome dataset (<span id="Sx2.p2.1.1" class="ltx_text ltx_font_bold">?</span>).
Wu <span id="Sx2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> proposed a free-form VQA model where internal textual representations of an image are merged with textual information sourced from a knowledge base (<span id="Sx2.p2.1.3" class="ltx_text ltx_font_bold">?</span>).
They claim that by using this method, a VQA system can answer both complex and broad questions conditioned on images.
In work by Wu, Hu, and Mooney (<span id="Sx2.p2.1.4" class="ltx_text ltx_font_bold">?</span>), captions are generated from questions and are used as an input to the neural network.
The model we propose in this work focuses on using region-based contextual information as we believe that performance of a VQA model can be bolstered if more knowledge about an image is provided through both region-based visual features and textual information.
These semantic texts about images can better help our model to bridge the gap between images and natural language questions as well.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Method</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">Given an image and associated question, the aim of our proposed model is to produce an answer of the question utilizing the salient image features and semantic information of the image.
In all upcoming sections, semantic information will be approached as SI.
We name our model as VQA-contextual information (VQA-CoIn) model and following this, in the rest of the paper, we will address the method by VQA-CoIn model.
Through our network, we emphasize that adding contextual information of the image can help a VQA agent to learn more about the content of the target image to answer relative questions about the image.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">In our proposed architecture, there are three different modules which make an end to end deep learning architecture for the Visual Question Answering task.
The first module:<span id="Sx3.p2.1.1" class="ltx_text ltx_font_italic">Input Encoder</span>, where visual features are extracted using Faster r-CNN(<span id="Sx3.p2.1.2" class="ltx_text ltx_font_bold">?</span>) method and textual features like natural language questions and semantic knowledge of the images are embedded using gated recurrent units (GRUs) (<span id="Sx3.p2.1.3" class="ltx_text ltx_font_bold">?</span>).</p>
</div>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.1" class="ltx_p">The next process in our model involves attending to images and SI of images works using a bilinear attention mechanism(<span id="Sx3.p3.1.1" class="ltx_text ltx_font_bold">?</span>) conditioned on question embedding vectors.
We also apply self attention on questions to learn the importance of the words residing in the given questions.
The third component is the <span id="Sx3.p3.1.2" class="ltx_text ltx_font_italic">classifier</span> which predicts candidate answers using the concatenated vector produced from the sum pooling of the three output vectors of the previous two modules.
Figure 1 shows an overview of our VQA-CoIn model.
We will discuss each module of our architecture in greater detail below.</p>
</div>
<figure id="Sx3.F1" class="ltx_figure"><img src="/html/2004.10966/assets/figures/vqa_model_up.png" id="Sx3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the proposed VQA-CoIn architecture. </figcaption>
</figure>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Input Encoder</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.3" class="ltx_p">The Input Encoder takes an image, associated question, and the SI of the image as inputs and produces three embedding vectors, one from each of the inputs.
For the image features, we use the pre-trained bottom-up attention features which were generated in (<span id="Sx3.SSx1.p1.3.1" class="ltx_text ltx_font_bold">?</span>).
They used Faster r-CNN algorithm (<span id="Sx3.SSx1.p1.3.2" class="ltx_text ltx_font_bold">?</span>) with ResNet-101 to train the model.
Adaptive number of features <math id="Sx3.SSx1.p1.1.m1.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="Sx3.SSx1.p1.1.m1.1a"><msub id="Sx3.SSx1.p1.1.m1.1.1" xref="Sx3.SSx1.p1.1.m1.1.1.cmml"><mi id="Sx3.SSx1.p1.1.m1.1.1.2" xref="Sx3.SSx1.p1.1.m1.1.1.2.cmml">f</mi><mi id="Sx3.SSx1.p1.1.m1.1.1.3" xref="Sx3.SSx1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.1.m1.1b"><apply id="Sx3.SSx1.p1.1.m1.1.1.cmml" xref="Sx3.SSx1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.1.m1.1.1.1.cmml" xref="Sx3.SSx1.p1.1.m1.1.1">subscript</csymbol><ci id="Sx3.SSx1.p1.1.m1.1.1.2.cmml" xref="Sx3.SSx1.p1.1.m1.1.1.2">𝑓</ci><ci id="Sx3.SSx1.p1.1.m1.1.1.3.cmml" xref="Sx3.SSx1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.1.m1.1c">f_{i}</annotation></semantics></math> per image is considered to generate the vector representation <math id="Sx3.SSx1.p1.2.m2.1" class="ltx_Math" alttext="f_{i}\times d_{i}" display="inline"><semantics id="Sx3.SSx1.p1.2.m2.1a"><mrow id="Sx3.SSx1.p1.2.m2.1.1" xref="Sx3.SSx1.p1.2.m2.1.1.cmml"><msub id="Sx3.SSx1.p1.2.m2.1.1.2" xref="Sx3.SSx1.p1.2.m2.1.1.2.cmml"><mi id="Sx3.SSx1.p1.2.m2.1.1.2.2" xref="Sx3.SSx1.p1.2.m2.1.1.2.2.cmml">f</mi><mi id="Sx3.SSx1.p1.2.m2.1.1.2.3" xref="Sx3.SSx1.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="Sx3.SSx1.p1.2.m2.1.1.1" xref="Sx3.SSx1.p1.2.m2.1.1.1.cmml">×</mo><msub id="Sx3.SSx1.p1.2.m2.1.1.3" xref="Sx3.SSx1.p1.2.m2.1.1.3.cmml"><mi id="Sx3.SSx1.p1.2.m2.1.1.3.2" xref="Sx3.SSx1.p1.2.m2.1.1.3.2.cmml">d</mi><mi id="Sx3.SSx1.p1.2.m2.1.1.3.3" xref="Sx3.SSx1.p1.2.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.2.m2.1b"><apply id="Sx3.SSx1.p1.2.m2.1.1.cmml" xref="Sx3.SSx1.p1.2.m2.1.1"><times id="Sx3.SSx1.p1.2.m2.1.1.1.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.1"></times><apply id="Sx3.SSx1.p1.2.m2.1.1.2.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.2.m2.1.1.2.1.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.2">subscript</csymbol><ci id="Sx3.SSx1.p1.2.m2.1.1.2.2.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.2.2">𝑓</ci><ci id="Sx3.SSx1.p1.2.m2.1.1.2.3.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.2.3">𝑖</ci></apply><apply id="Sx3.SSx1.p1.2.m2.1.1.3.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.2.m2.1.1.3.1.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.3">subscript</csymbol><ci id="Sx3.SSx1.p1.2.m2.1.1.3.2.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.3.2">𝑑</ci><ci id="Sx3.SSx1.p1.2.m2.1.1.3.3.cmml" xref="Sx3.SSx1.p1.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.2.m2.1c">f_{i}\times d_{i}</annotation></semantics></math> for each image where <math id="Sx3.SSx1.p1.3.m3.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="Sx3.SSx1.p1.3.m3.1a"><msub id="Sx3.SSx1.p1.3.m3.1.1" xref="Sx3.SSx1.p1.3.m3.1.1.cmml"><mi id="Sx3.SSx1.p1.3.m3.1.1.2" xref="Sx3.SSx1.p1.3.m3.1.1.2.cmml">d</mi><mi id="Sx3.SSx1.p1.3.m3.1.1.3" xref="Sx3.SSx1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.3.m3.1b"><apply id="Sx3.SSx1.p1.3.m3.1.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.1.1.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.1">subscript</csymbol><ci id="Sx3.SSx1.p1.3.m3.1.1.2.cmml" xref="Sx3.SSx1.p1.3.m3.1.1.2">𝑑</ci><ci id="Sx3.SSx1.p1.3.m3.1.1.3.cmml" xref="Sx3.SSx1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.3.m3.1c">d_{i}</annotation></semantics></math> is the image feature size.</p>
</div>
<div id="Sx3.SSx1.p2" class="ltx_para">
<p id="Sx3.SSx1.p2.1" class="ltx_p">Our model uses semantic information (SI) extracted from the image as external knowledge to learn more about the image.
To encode these semantic features, each word is embedded using pre-trained tf-idf word embeddings.
Then the embedded word vectors are propagated through GRU cell which gives us hidden vector for each corresponding word.
The hidden vectors are used in the attention layer to create the encoding vector of the semantic features.</p>
</div>
<figure id="Sx3.F2" class="ltx_figure"><img src="/html/2004.10966/assets/figures/q_att.jpg" id="Sx3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1355" height="284" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A detailed architecture of self attention mechanism of questions with smaller hidden states <math id="Sx3.F2.2.m1.1" class="ltx_Math" alttext="h\_q_{sn}" display="inline"><semantics id="Sx3.F2.2.m1.1b"><mrow id="Sx3.F2.2.m1.1.1" xref="Sx3.F2.2.m1.1.1.cmml"><mi id="Sx3.F2.2.m1.1.1.2" xref="Sx3.F2.2.m1.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.F2.2.m1.1.1.1" xref="Sx3.F2.2.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.F2.2.m1.1.1.3" xref="Sx3.F2.2.m1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.F2.2.m1.1.1.1b" xref="Sx3.F2.2.m1.1.1.1.cmml">​</mo><msub id="Sx3.F2.2.m1.1.1.4" xref="Sx3.F2.2.m1.1.1.4.cmml"><mi id="Sx3.F2.2.m1.1.1.4.2" xref="Sx3.F2.2.m1.1.1.4.2.cmml">q</mi><mrow id="Sx3.F2.2.m1.1.1.4.3" xref="Sx3.F2.2.m1.1.1.4.3.cmml"><mi id="Sx3.F2.2.m1.1.1.4.3.2" xref="Sx3.F2.2.m1.1.1.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.F2.2.m1.1.1.4.3.1" xref="Sx3.F2.2.m1.1.1.4.3.1.cmml">​</mo><mi id="Sx3.F2.2.m1.1.1.4.3.3" xref="Sx3.F2.2.m1.1.1.4.3.3.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.F2.2.m1.1c"><apply id="Sx3.F2.2.m1.1.1.cmml" xref="Sx3.F2.2.m1.1.1"><times id="Sx3.F2.2.m1.1.1.1.cmml" xref="Sx3.F2.2.m1.1.1.1"></times><ci id="Sx3.F2.2.m1.1.1.2.cmml" xref="Sx3.F2.2.m1.1.1.2">ℎ</ci><ci id="Sx3.F2.2.m1.1.1.3.cmml" xref="Sx3.F2.2.m1.1.1.3">_</ci><apply id="Sx3.F2.2.m1.1.1.4.cmml" xref="Sx3.F2.2.m1.1.1.4"><csymbol cd="ambiguous" id="Sx3.F2.2.m1.1.1.4.1.cmml" xref="Sx3.F2.2.m1.1.1.4">subscript</csymbol><ci id="Sx3.F2.2.m1.1.1.4.2.cmml" xref="Sx3.F2.2.m1.1.1.4.2">𝑞</ci><apply id="Sx3.F2.2.m1.1.1.4.3.cmml" xref="Sx3.F2.2.m1.1.1.4.3"><times id="Sx3.F2.2.m1.1.1.4.3.1.cmml" xref="Sx3.F2.2.m1.1.1.4.3.1"></times><ci id="Sx3.F2.2.m1.1.1.4.3.2.cmml" xref="Sx3.F2.2.m1.1.1.4.3.2">𝑠</ci><ci id="Sx3.F2.2.m1.1.1.4.3.3.cmml" xref="Sx3.F2.2.m1.1.1.4.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.F2.2.m1.1d">h\_q_{sn}</annotation></semantics></math>. Here, n defines the number of words in a question. These hidden states are passed into two FC layers and then multiplied to create a joint representation. This joint representation is forwarded through ReLU and FC layers to get the attention weights. The question embedding vector is element-wise multiplied with the attention weights to achieve the context vector. </figcaption>
</figure>
<div id="Sx3.SSx1.p3" class="ltx_para">
<p id="Sx3.SSx1.p3.6" class="ltx_p">The final input for our task is the question related to the image which is embedded twice with two different hidden vector sizes in our model.
We do this because we realize from our experiments that questions with large hidden vectors can contain more information from image while attending them.
But in the case of prioritizing information from its own features, hidden layers with smaller dimensions can perform this task more effectively.
In Figure <a href="#Sx3.F1" title="Figure 1 ‣ Method ‣ Visual Question Answering Using Semantic Information from Image Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, yellow is used for questions with large vector sizes and blue is for questions with smaller dimensions.
As with the SI embedding, we use the same pre-trained tf-idf word embeddings to embed each word of a question.
The obtained word embedding vector is then passed through two separate GRU cells (<span id="Sx3.SSx1.p3.6.1" class="ltx_text ltx_font_bold">?</span>) represented as GRU<sub id="Sx3.SSx1.p3.6.2" class="ltx_sub">l</sub> and GRU<sub id="Sx3.SSx1.p3.6.3" class="ltx_sub">s</sub> for larger and smaller hidden state respectively in Figure <a href="#Sx3.F1" title="Figure 1 ‣ Method ‣ Visual Question Answering Using Semantic Information from Image Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The GRU cells have <math id="Sx3.SSx1.p3.1.m1.1" class="ltx_Math" alttext="n_{q}" display="inline"><semantics id="Sx3.SSx1.p3.1.m1.1a"><msub id="Sx3.SSx1.p3.1.m1.1.1" xref="Sx3.SSx1.p3.1.m1.1.1.cmml"><mi id="Sx3.SSx1.p3.1.m1.1.1.2" xref="Sx3.SSx1.p3.1.m1.1.1.2.cmml">n</mi><mi id="Sx3.SSx1.p3.1.m1.1.1.3" xref="Sx3.SSx1.p3.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.1.m1.1b"><apply id="Sx3.SSx1.p3.1.m1.1.1.cmml" xref="Sx3.SSx1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.1.m1.1.1.1.cmml" xref="Sx3.SSx1.p3.1.m1.1.1">subscript</csymbol><ci id="Sx3.SSx1.p3.1.m1.1.1.2.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.2">𝑛</ci><ci id="Sx3.SSx1.p3.1.m1.1.1.3.cmml" xref="Sx3.SSx1.p3.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.1.m1.1c">n_{q}</annotation></semantics></math> numbers of hidden states <math id="Sx3.SSx1.p3.2.m2.1" class="ltx_Math" alttext="h\_q_{l}" display="inline"><semantics id="Sx3.SSx1.p3.2.m2.1a"><mrow id="Sx3.SSx1.p3.2.m2.1.1" xref="Sx3.SSx1.p3.2.m2.1.1.cmml"><mi id="Sx3.SSx1.p3.2.m2.1.1.2" xref="Sx3.SSx1.p3.2.m2.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.2.m2.1.1.1" xref="Sx3.SSx1.p3.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx1.p3.2.m2.1.1.3" xref="Sx3.SSx1.p3.2.m2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.2.m2.1.1.1a" xref="Sx3.SSx1.p3.2.m2.1.1.1.cmml">​</mo><msub id="Sx3.SSx1.p3.2.m2.1.1.4" xref="Sx3.SSx1.p3.2.m2.1.1.4.cmml"><mi id="Sx3.SSx1.p3.2.m2.1.1.4.2" xref="Sx3.SSx1.p3.2.m2.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx1.p3.2.m2.1.1.4.3" xref="Sx3.SSx1.p3.2.m2.1.1.4.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.2.m2.1b"><apply id="Sx3.SSx1.p3.2.m2.1.1.cmml" xref="Sx3.SSx1.p3.2.m2.1.1"><times id="Sx3.SSx1.p3.2.m2.1.1.1.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.1"></times><ci id="Sx3.SSx1.p3.2.m2.1.1.2.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.2">ℎ</ci><ci id="Sx3.SSx1.p3.2.m2.1.1.3.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.3">_</ci><apply id="Sx3.SSx1.p3.2.m2.1.1.4.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.2.m2.1.1.4.1.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.4">subscript</csymbol><ci id="Sx3.SSx1.p3.2.m2.1.1.4.2.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.4.2">𝑞</ci><ci id="Sx3.SSx1.p3.2.m2.1.1.4.3.cmml" xref="Sx3.SSx1.p3.2.m2.1.1.4.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.2.m2.1c">h\_q_{l}</annotation></semantics></math> and <math id="Sx3.SSx1.p3.3.m3.1" class="ltx_Math" alttext="h\_q_{s}" display="inline"><semantics id="Sx3.SSx1.p3.3.m3.1a"><mrow id="Sx3.SSx1.p3.3.m3.1.1" xref="Sx3.SSx1.p3.3.m3.1.1.cmml"><mi id="Sx3.SSx1.p3.3.m3.1.1.2" xref="Sx3.SSx1.p3.3.m3.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.3.m3.1.1.1" xref="Sx3.SSx1.p3.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx1.p3.3.m3.1.1.3" xref="Sx3.SSx1.p3.3.m3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.3.m3.1.1.1a" xref="Sx3.SSx1.p3.3.m3.1.1.1.cmml">​</mo><msub id="Sx3.SSx1.p3.3.m3.1.1.4" xref="Sx3.SSx1.p3.3.m3.1.1.4.cmml"><mi id="Sx3.SSx1.p3.3.m3.1.1.4.2" xref="Sx3.SSx1.p3.3.m3.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx1.p3.3.m3.1.1.4.3" xref="Sx3.SSx1.p3.3.m3.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.3.m3.1b"><apply id="Sx3.SSx1.p3.3.m3.1.1.cmml" xref="Sx3.SSx1.p3.3.m3.1.1"><times id="Sx3.SSx1.p3.3.m3.1.1.1.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.1"></times><ci id="Sx3.SSx1.p3.3.m3.1.1.2.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.2">ℎ</ci><ci id="Sx3.SSx1.p3.3.m3.1.1.3.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.3">_</ci><apply id="Sx3.SSx1.p3.3.m3.1.1.4.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.3.m3.1.1.4.1.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.4">subscript</csymbol><ci id="Sx3.SSx1.p3.3.m3.1.1.4.2.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.4.2">𝑞</ci><ci id="Sx3.SSx1.p3.3.m3.1.1.4.3.cmml" xref="Sx3.SSx1.p3.3.m3.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.3.m3.1c">h\_q_{s}</annotation></semantics></math>.
Here, <math id="Sx3.SSx1.p3.4.m4.1" class="ltx_Math" alttext="n_{q}" display="inline"><semantics id="Sx3.SSx1.p3.4.m4.1a"><msub id="Sx3.SSx1.p3.4.m4.1.1" xref="Sx3.SSx1.p3.4.m4.1.1.cmml"><mi id="Sx3.SSx1.p3.4.m4.1.1.2" xref="Sx3.SSx1.p3.4.m4.1.1.2.cmml">n</mi><mi id="Sx3.SSx1.p3.4.m4.1.1.3" xref="Sx3.SSx1.p3.4.m4.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.4.m4.1b"><apply id="Sx3.SSx1.p3.4.m4.1.1.cmml" xref="Sx3.SSx1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.4.m4.1.1.1.cmml" xref="Sx3.SSx1.p3.4.m4.1.1">subscript</csymbol><ci id="Sx3.SSx1.p3.4.m4.1.1.2.cmml" xref="Sx3.SSx1.p3.4.m4.1.1.2">𝑛</ci><ci id="Sx3.SSx1.p3.4.m4.1.1.3.cmml" xref="Sx3.SSx1.p3.4.m4.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.4.m4.1c">n_{q}</annotation></semantics></math> is the word count in the question and <math id="Sx3.SSx1.p3.5.m5.1" class="ltx_Math" alttext="h\_q_{l}" display="inline"><semantics id="Sx3.SSx1.p3.5.m5.1a"><mrow id="Sx3.SSx1.p3.5.m5.1.1" xref="Sx3.SSx1.p3.5.m5.1.1.cmml"><mi id="Sx3.SSx1.p3.5.m5.1.1.2" xref="Sx3.SSx1.p3.5.m5.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.5.m5.1.1.1" xref="Sx3.SSx1.p3.5.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx1.p3.5.m5.1.1.3" xref="Sx3.SSx1.p3.5.m5.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.5.m5.1.1.1a" xref="Sx3.SSx1.p3.5.m5.1.1.1.cmml">​</mo><msub id="Sx3.SSx1.p3.5.m5.1.1.4" xref="Sx3.SSx1.p3.5.m5.1.1.4.cmml"><mi id="Sx3.SSx1.p3.5.m5.1.1.4.2" xref="Sx3.SSx1.p3.5.m5.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx1.p3.5.m5.1.1.4.3" xref="Sx3.SSx1.p3.5.m5.1.1.4.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.5.m5.1b"><apply id="Sx3.SSx1.p3.5.m5.1.1.cmml" xref="Sx3.SSx1.p3.5.m5.1.1"><times id="Sx3.SSx1.p3.5.m5.1.1.1.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.1"></times><ci id="Sx3.SSx1.p3.5.m5.1.1.2.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.2">ℎ</ci><ci id="Sx3.SSx1.p3.5.m5.1.1.3.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.3">_</ci><apply id="Sx3.SSx1.p3.5.m5.1.1.4.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.5.m5.1.1.4.1.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.4">subscript</csymbol><ci id="Sx3.SSx1.p3.5.m5.1.1.4.2.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.4.2">𝑞</ci><ci id="Sx3.SSx1.p3.5.m5.1.1.4.3.cmml" xref="Sx3.SSx1.p3.5.m5.1.1.4.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.5.m5.1c">h\_q_{l}</annotation></semantics></math> and <math id="Sx3.SSx1.p3.6.m6.1" class="ltx_Math" alttext="h\_q_{s}" display="inline"><semantics id="Sx3.SSx1.p3.6.m6.1a"><mrow id="Sx3.SSx1.p3.6.m6.1.1" xref="Sx3.SSx1.p3.6.m6.1.1.cmml"><mi id="Sx3.SSx1.p3.6.m6.1.1.2" xref="Sx3.SSx1.p3.6.m6.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.6.m6.1.1.1" xref="Sx3.SSx1.p3.6.m6.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx1.p3.6.m6.1.1.3" xref="Sx3.SSx1.p3.6.m6.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx1.p3.6.m6.1.1.1a" xref="Sx3.SSx1.p3.6.m6.1.1.1.cmml">​</mo><msub id="Sx3.SSx1.p3.6.m6.1.1.4" xref="Sx3.SSx1.p3.6.m6.1.1.4.cmml"><mi id="Sx3.SSx1.p3.6.m6.1.1.4.2" xref="Sx3.SSx1.p3.6.m6.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx1.p3.6.m6.1.1.4.3" xref="Sx3.SSx1.p3.6.m6.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p3.6.m6.1b"><apply id="Sx3.SSx1.p3.6.m6.1.1.cmml" xref="Sx3.SSx1.p3.6.m6.1.1"><times id="Sx3.SSx1.p3.6.m6.1.1.1.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.1"></times><ci id="Sx3.SSx1.p3.6.m6.1.1.2.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.2">ℎ</ci><ci id="Sx3.SSx1.p3.6.m6.1.1.3.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.3">_</ci><apply id="Sx3.SSx1.p3.6.m6.1.1.4.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx1.p3.6.m6.1.1.4.1.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.4">subscript</csymbol><ci id="Sx3.SSx1.p3.6.m6.1.1.4.2.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.4.2">𝑞</ci><ci id="Sx3.SSx1.p3.6.m6.1.1.4.3.cmml" xref="Sx3.SSx1.p3.6.m6.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p3.6.m6.1c">h\_q_{s}</annotation></semantics></math> denote the hidden states from GRU<sub id="Sx3.SSx1.p3.6.4" class="ltx_sub">L</sub> and GRU<sub id="Sx3.SSx1.p3.6.5" class="ltx_sub">S</sub> respectively.
Hidden states of both GRU cells are passed to the attention layer to generate the context vector of the questions.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Attention Layer</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">In the attention layer, we employ an attention mechanism to find out the importance of different parts of the input sequence based on a query vector. We use two different attention mechanisms on the input vectors: 1. Self-Attention, applied to question embedding vector and 2. Bi-Linear Attention, applied to image embedding, question embedding and SI embedding vectors.</p>
</div>
<section id="Sx3.SSx2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Self-Attention</h4>

<div id="Sx3.SSx2.SSSx1.p1" class="ltx_para">
<p id="Sx3.SSx2.SSSx1.p1.1" class="ltx_p">Self-attention is an attention mechanism where relations among different parts of a sequence are computed using the same sequence as query.
In our proposed architecture, we apply self-attention on a question to figure out the internal relations among the words of a question.
For example, in a question like ‘what is the color of the bus?’, invoking self-attention on itself would enable the model to identify that the words ‘color’ and ‘bus’ are interrelated and should be more emphasized to learn about the question.</p>
</div>
<div id="Sx3.SSx2.SSSx1.p2" class="ltx_para">
<p id="Sx3.SSx2.SSSx1.p2.6" class="ltx_p">Figure <a href="#Sx3.F2" title="Figure 2 ‣ Input Encoder ‣ Method ‣ Visual Question Answering Using Semantic Information from Image Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the detailed architecture of the self-attention module.
We implement the self-attention mechanism inspired by the idea of multi-headed attention which are featured in many transformer architectures (<span id="Sx3.SSx2.SSSx1.p2.6.1" class="ltx_text ltx_font_bold">?</span>).
In this process, we take into account all of the hidden states of the GRU<sub id="Sx3.SSx2.SSSx1.p2.6.2" class="ltx_sub">S</sub> instead of the final hidden state as RNNs have a tendency to forget the information encountered in the early steps of the sequence.
All of the hidden states are passed through two fully connected (FC) layers which generate two vectors, a query vector <math id="Sx3.SSx2.SSSx1.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="Sx3.SSx2.SSSx1.p2.1.m1.1a"><mi id="Sx3.SSx2.SSSx1.p2.1.m1.1.1" xref="Sx3.SSx2.SSSx1.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx1.p2.1.m1.1b"><ci id="Sx3.SSx2.SSSx1.p2.1.m1.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx1.p2.1.m1.1c">q</annotation></semantics></math> and a value vector <math id="Sx3.SSx2.SSSx1.p2.2.m2.1" class="ltx_Math" alttext="v" display="inline"><semantics id="Sx3.SSx2.SSSx1.p2.2.m2.1a"><mi id="Sx3.SSx2.SSSx1.p2.2.m2.1.1" xref="Sx3.SSx2.SSSx1.p2.2.m2.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx1.p2.2.m2.1b"><ci id="Sx3.SSx2.SSSx1.p2.2.m2.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.2.m2.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx1.p2.2.m2.1c">v</annotation></semantics></math>.
In each FC layer, weight normalization and ReLU activation are performed on the input vectors.
Then the resultant vectors <math id="Sx3.SSx2.SSSx1.p2.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="Sx3.SSx2.SSSx1.p2.3.m3.1a"><mi id="Sx3.SSx2.SSSx1.p2.3.m3.1.1" xref="Sx3.SSx2.SSSx1.p2.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx1.p2.3.m3.1b"><ci id="Sx3.SSx2.SSSx1.p2.3.m3.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx1.p2.3.m3.1c">q</annotation></semantics></math> and <math id="Sx3.SSx2.SSSx1.p2.4.m4.1" class="ltx_Math" alttext="v" display="inline"><semantics id="Sx3.SSx2.SSSx1.p2.4.m4.1a"><mi id="Sx3.SSx2.SSSx1.p2.4.m4.1.1" xref="Sx3.SSx2.SSSx1.p2.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx1.p2.4.m4.1b"><ci id="Sx3.SSx2.SSSx1.p2.4.m4.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.4.m4.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx1.p2.4.m4.1c">v</annotation></semantics></math> are multiplied together to create a new context vector.
Here, the multiplication operation is the element-wise multiplication (Hadamard product) of the vectors.
The new context vector is forwarded to another FC layer followed by a softmax layer to generate the attention weights of the input question embedding.
Afterwards, these weights are multiplied with the initial question embedding to construct the final context vector <math id="Sx3.SSx2.SSSx1.p2.5.m5.1" class="ltx_Math" alttext="c\_q_{s}" display="inline"><semantics id="Sx3.SSx2.SSSx1.p2.5.m5.1a"><mrow id="Sx3.SSx2.SSSx1.p2.5.m5.1.1" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.cmml"><mi id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.2" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.1" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.3" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.1a" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.1.cmml">​</mo><msub id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.cmml"><mi id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.2" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.3" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx1.p2.5.m5.1b"><apply id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1"><times id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.1"></times><ci id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.2.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.2">𝑐</ci><ci id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.3.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.3">_</ci><apply id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.1.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.2.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.2">𝑞</ci><ci id="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.3.cmml" xref="Sx3.SSx2.SSSx1.p2.5.m5.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx1.p2.5.m5.1c">c\_q_{s}</annotation></semantics></math>.
This final question context vector represents the prioritized words in the input question sequence.
As a next step, <math id="Sx3.SSx2.SSSx1.p2.6.m6.1" class="ltx_Math" alttext="c\_q_{s}" display="inline"><semantics id="Sx3.SSx2.SSSx1.p2.6.m6.1a"><mrow id="Sx3.SSx2.SSSx1.p2.6.m6.1.1" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.cmml"><mi id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.2" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.1" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.3" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.1a" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.1.cmml">​</mo><msub id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.cmml"><mi id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.2" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.3" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx1.p2.6.m6.1b"><apply id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1"><times id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.1.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.1"></times><ci id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.2.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.2">𝑐</ci><ci id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.3.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.3">_</ci><apply id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.1.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.2.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.2">𝑞</ci><ci id="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.3.cmml" xref="Sx3.SSx2.SSSx1.p2.6.m6.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx1.p2.6.m6.1c">c\_q_{s}</annotation></semantics></math> is used to put bilinear attention (<span id="Sx3.SSx2.SSSx1.p2.6.3" class="ltx_text ltx_font_bold">?</span>) on the semantic concepts of the images.</p>
</div>
</section>
<section id="Sx3.SSx2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Bilinear Attention</h4>

<div id="Sx3.SSx2.SSSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.SSSx2.p1.1" class="ltx_p">Bilinear attention is usually applied on two inputs with multiple channels so that the two input channels decrease their dimensionality concurrently.
We adopt this attention mechanism from Bilinear attention networks (<span id="Sx3.SSx2.SSSx2.p1.1.1" class="ltx_text ltx_font_bold">?</span>).
In our case, we have two input groups to apply the attention: one group is the combined group of the image and question and the other is the combined group of the SI and question context vector.
In the attention procedure, at first an attention map is generated using image features conditioned on given questions embedded using GRU<sub id="Sx3.SSx2.SSSx2.p1.1.2" class="ltx_sub">L</sub>.
Similar to (<span id="Sx3.SSx2.SSSx2.p1.1.3" class="ltx_text ltx_font_bold">?</span>), this attention map is then run through eight glimpses.
In each glimpse, a vector representation from the image and question is produced using the bilinear attention map.
Next, with this representation, for every glimpse, we keep integrating the resultant vectors of the residual learning network and counter module (<span id="Sx3.SSx2.SSSx2.p1.1.4" class="ltx_text ltx_font_bold">?</span>).
As a result, at the last glimpse, we get a final output vector <math id="Sx3.SSx2.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="b\_c_{v}" display="inline"><semantics id="Sx3.SSx2.SSSx2.p1.1.m1.1a"><mrow id="Sx3.SSx2.SSSx2.p1.1.m1.1.1" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.cmml"><mi id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.2" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.1" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.3" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.1a" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.1.cmml">​</mo><msub id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.cmml"><mi id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.2" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.2.cmml">c</mi><mi id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.3" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx2.p1.1.m1.1b"><apply id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1"><times id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.1.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.1"></times><ci id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.2.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.2">𝑏</ci><ci id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.3.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.3">_</ci><apply id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.1.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.2.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.2">𝑐</ci><ci id="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.3.cmml" xref="Sx3.SSx2.SSSx2.p1.1.m1.1.1.4.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx2.p1.1.m1.1c">b\_c_{v}</annotation></semantics></math>.</p>
</div>
<div id="Sx3.SSx2.SSSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.SSSx2.p2.3" class="ltx_p">For the input group of SI and the question context vector <math id="Sx3.SSx2.SSSx2.p2.1.m1.1" class="ltx_Math" alttext="c\_q_{s}" display="inline"><semantics id="Sx3.SSx2.SSSx2.p2.1.m1.1a"><mrow id="Sx3.SSx2.SSSx2.p2.1.m1.1.1" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.cmml"><mi id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.2" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.1" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.3" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.1a" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.1.cmml">​</mo><msub id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.cmml"><mi id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.2" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.3" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx2.p2.1.m1.1b"><apply id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1"><times id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.1.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.1"></times><ci id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.2.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.2">𝑐</ci><ci id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.3.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.3">_</ci><apply id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.1.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.2.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.2">𝑞</ci><ci id="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.3.cmml" xref="Sx3.SSx2.SSSx2.p2.1.m1.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx2.p2.1.m1.1c">c\_q_{s}</annotation></semantics></math>, we similarly produce an attention map using the two input vectors.
But unlike the image-question input group, we use one glimpse on the attention map of SI-question group to generate a vector.
Also, we element-wise add the context vector <math id="Sx3.SSx2.SSSx2.p2.2.m2.1" class="ltx_Math" alttext="c\_q_{s}" display="inline"><semantics id="Sx3.SSx2.SSSx2.p2.2.m2.1a"><mrow id="Sx3.SSx2.SSSx2.p2.2.m2.1.1" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.cmml"><mi id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.2" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.1" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.3" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.1a" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.1.cmml">​</mo><msub id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.cmml"><mi id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.2" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.3" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx2.p2.2.m2.1b"><apply id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1"><times id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.1.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.1"></times><ci id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.2.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.2">𝑐</ci><ci id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.3.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.3">_</ci><apply id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.1.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.2.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.2">𝑞</ci><ci id="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.3.cmml" xref="Sx3.SSx2.SSSx2.p2.2.m2.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx2.p2.2.m2.1c">c\_q_{s}</annotation></semantics></math> of question with the resultant vector from the glimpse.
This creates an output vector <math id="Sx3.SSx2.SSSx2.p2.3.m3.1" class="ltx_Math" alttext="b\_c_{si}" display="inline"><semantics id="Sx3.SSx2.SSSx2.p2.3.m3.1a"><mrow id="Sx3.SSx2.SSSx2.p2.3.m3.1.1" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.cmml"><mi id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.2" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.1" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.3" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.1a" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.1.cmml">​</mo><msub id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.cmml"><mi id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.2" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.2.cmml">c</mi><mrow id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.cmml"><mi id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.2" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.1" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.1.cmml">​</mo><mi id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.3" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.SSSx2.p2.3.m3.1b"><apply id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1"><times id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.1.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.1"></times><ci id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.2.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.2">𝑏</ci><ci id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.3.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.3">_</ci><apply id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.1.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4">subscript</csymbol><ci id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.2.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.2">𝑐</ci><apply id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3"><times id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.1.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.1"></times><ci id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.2.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.2">𝑠</ci><ci id="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.3.cmml" xref="Sx3.SSx2.SSSx2.p2.3.m3.1.1.4.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.SSSx2.p2.3.m3.1c">b\_c_{si}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Classifier Layer</h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.3" class="ltx_p"><math id="Sx3.SSx3.p1.1.m1.1" class="ltx_Math" alttext="b\_c_{v}" display="inline"><semantics id="Sx3.SSx3.p1.1.m1.1a"><mrow id="Sx3.SSx3.p1.1.m1.1.1" xref="Sx3.SSx3.p1.1.m1.1.1.cmml"><mi id="Sx3.SSx3.p1.1.m1.1.1.2" xref="Sx3.SSx3.p1.1.m1.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.1.m1.1.1.1" xref="Sx3.SSx3.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx3.p1.1.m1.1.1.3" xref="Sx3.SSx3.p1.1.m1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.1.m1.1.1.1a" xref="Sx3.SSx3.p1.1.m1.1.1.1.cmml">​</mo><msub id="Sx3.SSx3.p1.1.m1.1.1.4" xref="Sx3.SSx3.p1.1.m1.1.1.4.cmml"><mi id="Sx3.SSx3.p1.1.m1.1.1.4.2" xref="Sx3.SSx3.p1.1.m1.1.1.4.2.cmml">c</mi><mi id="Sx3.SSx3.p1.1.m1.1.1.4.3" xref="Sx3.SSx3.p1.1.m1.1.1.4.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.1.m1.1b"><apply id="Sx3.SSx3.p1.1.m1.1.1.cmml" xref="Sx3.SSx3.p1.1.m1.1.1"><times id="Sx3.SSx3.p1.1.m1.1.1.1.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.1"></times><ci id="Sx3.SSx3.p1.1.m1.1.1.2.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.2">𝑏</ci><ci id="Sx3.SSx3.p1.1.m1.1.1.3.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.3">_</ci><apply id="Sx3.SSx3.p1.1.m1.1.1.4.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.1.m1.1.1.4.1.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.4">subscript</csymbol><ci id="Sx3.SSx3.p1.1.m1.1.1.4.2.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.4.2">𝑐</ci><ci id="Sx3.SSx3.p1.1.m1.1.1.4.3.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.4.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.1.m1.1c">b\_c_{v}</annotation></semantics></math>, <math id="Sx3.SSx3.p1.2.m2.1" class="ltx_Math" alttext="b\_c_{si}" display="inline"><semantics id="Sx3.SSx3.p1.2.m2.1a"><mrow id="Sx3.SSx3.p1.2.m2.1.1" xref="Sx3.SSx3.p1.2.m2.1.1.cmml"><mi id="Sx3.SSx3.p1.2.m2.1.1.2" xref="Sx3.SSx3.p1.2.m2.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.2.m2.1.1.1" xref="Sx3.SSx3.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx3.p1.2.m2.1.1.3" xref="Sx3.SSx3.p1.2.m2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.2.m2.1.1.1a" xref="Sx3.SSx3.p1.2.m2.1.1.1.cmml">​</mo><msub id="Sx3.SSx3.p1.2.m2.1.1.4" xref="Sx3.SSx3.p1.2.m2.1.1.4.cmml"><mi id="Sx3.SSx3.p1.2.m2.1.1.4.2" xref="Sx3.SSx3.p1.2.m2.1.1.4.2.cmml">c</mi><mrow id="Sx3.SSx3.p1.2.m2.1.1.4.3" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.cmml"><mi id="Sx3.SSx3.p1.2.m2.1.1.4.3.2" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.2.m2.1.1.4.3.1" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.1.cmml">​</mo><mi id="Sx3.SSx3.p1.2.m2.1.1.4.3.3" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.2.m2.1b"><apply id="Sx3.SSx3.p1.2.m2.1.1.cmml" xref="Sx3.SSx3.p1.2.m2.1.1"><times id="Sx3.SSx3.p1.2.m2.1.1.1.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.1"></times><ci id="Sx3.SSx3.p1.2.m2.1.1.2.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.2">𝑏</ci><ci id="Sx3.SSx3.p1.2.m2.1.1.3.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.3">_</ci><apply id="Sx3.SSx3.p1.2.m2.1.1.4.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.2.m2.1.1.4.1.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4">subscript</csymbol><ci id="Sx3.SSx3.p1.2.m2.1.1.4.2.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4.2">𝑐</ci><apply id="Sx3.SSx3.p1.2.m2.1.1.4.3.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4.3"><times id="Sx3.SSx3.p1.2.m2.1.1.4.3.1.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.1"></times><ci id="Sx3.SSx3.p1.2.m2.1.1.4.3.2.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.2">𝑠</ci><ci id="Sx3.SSx3.p1.2.m2.1.1.4.3.3.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.4.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.2.m2.1c">b\_c_{si}</annotation></semantics></math> and <math id="Sx3.SSx3.p1.3.m3.1" class="ltx_Math" alttext="c\_q_{s}" display="inline"><semantics id="Sx3.SSx3.p1.3.m3.1a"><mrow id="Sx3.SSx3.p1.3.m3.1.1" xref="Sx3.SSx3.p1.3.m3.1.1.cmml"><mi id="Sx3.SSx3.p1.3.m3.1.1.2" xref="Sx3.SSx3.p1.3.m3.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.3.m3.1.1.1" xref="Sx3.SSx3.p1.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx3.p1.3.m3.1.1.3" xref="Sx3.SSx3.p1.3.m3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx3.p1.3.m3.1.1.1a" xref="Sx3.SSx3.p1.3.m3.1.1.1.cmml">​</mo><msub id="Sx3.SSx3.p1.3.m3.1.1.4" xref="Sx3.SSx3.p1.3.m3.1.1.4.cmml"><mi id="Sx3.SSx3.p1.3.m3.1.1.4.2" xref="Sx3.SSx3.p1.3.m3.1.1.4.2.cmml">q</mi><mi id="Sx3.SSx3.p1.3.m3.1.1.4.3" xref="Sx3.SSx3.p1.3.m3.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.3.m3.1b"><apply id="Sx3.SSx3.p1.3.m3.1.1.cmml" xref="Sx3.SSx3.p1.3.m3.1.1"><times id="Sx3.SSx3.p1.3.m3.1.1.1.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.1"></times><ci id="Sx3.SSx3.p1.3.m3.1.1.2.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.2">𝑐</ci><ci id="Sx3.SSx3.p1.3.m3.1.1.3.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.3">_</ci><apply id="Sx3.SSx3.p1.3.m3.1.1.4.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.4"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.3.m3.1.1.4.1.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.4">subscript</csymbol><ci id="Sx3.SSx3.p1.3.m3.1.1.4.2.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.4.2">𝑞</ci><ci id="Sx3.SSx3.p1.3.m3.1.1.4.3.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.4.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.3.m3.1c">c\_q_{s}</annotation></semantics></math> are the inputs of our classifier layer.
The sum pooling of these three input vectors are concatenated as the next step of the classifier.
And the concatenated vector is then redirected to two FC layers to gather predicted answers for the questions.
In FC layers, we use ReLU as the activation function and the output dimension is set to the number of unique answers.
We have selected these answers that appear at least 9 times for the distinct questions in the training dataset.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experimental Setup</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">In this section, we are going to have a detailed discussion about the implementation procedure and experimental setup for VQA-CoIn model. First, we discuss the dataset we use for our task and then the preprocessing of our additional prior semantic knowledge.
We will then outline the network parameters for our proposed model that we use in the experiments.
To evaluate our method VQA-CoIn, we have used the available VQA challenge guidelines.
We use BAN-8 (<span id="Sx4.p1.1.1" class="ltx_text ltx_font_bold">?</span>) as our baseline.
And we compare our validation and test scores with bottom-up attention model (<span id="Sx4.p1.1.2" class="ltx_text ltx_font_bold">?</span>) as well.</p>
</div>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Dataset</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">We evaluate our proposed model on VQA v2.0 dataset. We use the provided train/validation split of the dataset to train our network.
In the training dataset, more than 400k questions and 82k images and in the validation split, 200k questions and 40k images are available.
Though we are utilizing full dataset, our model is trained to learn from the selective answers from the train split.
Recall that these are chosen, because they appear as answers at least 9 times for the unique questions of the split.
The number of these selective answers is 3,129.
The full test split of the dataset has around 82k images and 440k relevant questions on which we test our network.
As VQA task is an open challenge, the ground truth answers for the questions in the test dataset are not available and cannot be compared with.
</p>
</div>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Preprocessing</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">As we have mentioned, our method exploits the semantic concepts of images available in the dataset as an input of our architecture.
To generate this information, we use Densecap (<span id="Sx4.SSx2.p1.1.1" class="ltx_text ltx_font_bold">?</span>), an image captioning model.
For each image, Densecap generates a variable number of captions.
We have found that after a certain number of generated captions, the generated information tend to be duplicates.
For example, Densecap has generated both ‘man wearing a hat’ and ‘a man wearing a hat’ captions for an image.
To avoid these duplicate words, we have removed a sentence which has at least 80% similarity with any previous selected sentence.</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.p2.1" class="ltx_p">After discarding the similar sentences, from the resultant list, first 10 sentences are taken and preprocessed.
As preprocessing steps, we tag the words of each sentence with the NLTK part-of-speech tagger and then get rid of the stop words such as ‘the’, as well as any preposition and auxiliary verbs from the sentences.
Afterwards, the remaining words are gathered in a list which we have used as the SI for the respective image.</p>
</div>
<figure id="Sx4.T1" class="ltx_table">
<table id="Sx4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T1.1.1.1" class="ltx_tr">
<th id="Sx4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Scale%</th>
<th id="Sx4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">VQA-CoIn</th>
<th id="Sx4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">BAN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T1.1.2.1" class="ltx_tr">
<th id="Sx4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">25</th>
<td id="Sx4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="Sx4.T1.1.2.1.2.1" class="ltx_text ltx_font_bold">54.84</span></td>
<td id="Sx4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">54.09</td>
</tr>
<tr id="Sx4.T1.1.3.2" class="ltx_tr">
<th id="Sx4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">50</th>
<td id="Sx4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">61.76</td>
<td id="Sx4.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T1.1.3.2.3.1" class="ltx_text ltx_font_bold">62.42</span></td>
</tr>
<tr id="Sx4.T1.1.4.3" class="ltx_tr">
<th id="Sx4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">75</th>
<td id="Sx4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="Sx4.T1.1.4.3.2.1" class="ltx_text ltx_font_bold">65.08</span></td>
<td id="Sx4.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">64.92</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Validation accuracy after training VQA-CoIn with different scales of train split. </figcaption>
</figure>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Network Parameters</h3>

<div id="Sx4.SSx3.p1" class="ltx_para">
<p id="Sx4.SSx3.p1.3" class="ltx_p">We consider our image feature size <math id="Sx4.SSx3.p1.1.m1.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="Sx4.SSx3.p1.1.m1.1a"><msub id="Sx4.SSx3.p1.1.m1.1.1" xref="Sx4.SSx3.p1.1.m1.1.1.cmml"><mi id="Sx4.SSx3.p1.1.m1.1.1.2" xref="Sx4.SSx3.p1.1.m1.1.1.2.cmml">d</mi><mi id="Sx4.SSx3.p1.1.m1.1.1.3" xref="Sx4.SSx3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.1.m1.1b"><apply id="Sx4.SSx3.p1.1.m1.1.1.cmml" xref="Sx4.SSx3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx3.p1.1.m1.1.1.1.cmml" xref="Sx4.SSx3.p1.1.m1.1.1">subscript</csymbol><ci id="Sx4.SSx3.p1.1.m1.1.1.2.cmml" xref="Sx4.SSx3.p1.1.m1.1.1.2">𝑑</ci><ci id="Sx4.SSx3.p1.1.m1.1.1.3.cmml" xref="Sx4.SSx3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.1.m1.1c">d_{i}</annotation></semantics></math> as 2048 and the number of features <math id="Sx4.SSx3.p1.2.m2.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="Sx4.SSx3.p1.2.m2.1a"><msub id="Sx4.SSx3.p1.2.m2.1.1" xref="Sx4.SSx3.p1.2.m2.1.1.cmml"><mi id="Sx4.SSx3.p1.2.m2.1.1.2" xref="Sx4.SSx3.p1.2.m2.1.1.2.cmml">f</mi><mi id="Sx4.SSx3.p1.2.m2.1.1.3" xref="Sx4.SSx3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.2.m2.1b"><apply id="Sx4.SSx3.p1.2.m2.1.1.cmml" xref="Sx4.SSx3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Sx4.SSx3.p1.2.m2.1.1.1.cmml" xref="Sx4.SSx3.p1.2.m2.1.1">subscript</csymbol><ci id="Sx4.SSx3.p1.2.m2.1.1.2.cmml" xref="Sx4.SSx3.p1.2.m2.1.1.2">𝑓</ci><ci id="Sx4.SSx3.p1.2.m2.1.1.3.cmml" xref="Sx4.SSx3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.2.m2.1c">f_{i}</annotation></semantics></math> as variant between 10 to 100 per image.
For word embedding, pre-trained GLoVe vectors of size 300 have been used.
As we mention in section 3.1, questions are embedded twice in our architecture.
Question embeddings which are used for attending image features utilize GRU cells with a dimension of 1024 and the question features utilized for self-attention and external knowledge prioritization consists of a 512 sized vector.
The maximum word length <math id="Sx4.SSx3.p1.3.m3.1" class="ltx_Math" alttext="n_{q}" display="inline"><semantics id="Sx4.SSx3.p1.3.m3.1a"><msub id="Sx4.SSx3.p1.3.m3.1.1" xref="Sx4.SSx3.p1.3.m3.1.1.cmml"><mi id="Sx4.SSx3.p1.3.m3.1.1.2" xref="Sx4.SSx3.p1.3.m3.1.1.2.cmml">n</mi><mi id="Sx4.SSx3.p1.3.m3.1.1.3" xref="Sx4.SSx3.p1.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.3.m3.1b"><apply id="Sx4.SSx3.p1.3.m3.1.1.cmml" xref="Sx4.SSx3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx4.SSx3.p1.3.m3.1.1.1.cmml" xref="Sx4.SSx3.p1.3.m3.1.1">subscript</csymbol><ci id="Sx4.SSx3.p1.3.m3.1.1.2.cmml" xref="Sx4.SSx3.p1.3.m3.1.1.2">𝑛</ci><ci id="Sx4.SSx3.p1.3.m3.1.1.3.cmml" xref="Sx4.SSx3.p1.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.3.m3.1c">n_{q}</annotation></semantics></math> for any embedded question in the proposed model is 14.
To embed the semantic concept of an image, we fix the size of the GRU units to 512.
The additional semantic knowledge about an image can consist of maximum 40 words.
For training, we find that 18 epochs are enough to sufficiently train the network.
We have used the batch size of 180 for training and testing the dataset.
The Adamax optimizer is used to optimize the classifier and dropout value of the classifier is set to 0.5 while FC layers have dropout of 0.2.
</p>
</div>
</section>
<section id="Sx4.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Baselines</h3>

<div id="Sx4.SSx4.p1" class="ltx_para">
<p id="Sx4.SSx4.p1.1" class="ltx_p">BAN-8 (<span id="Sx4.SSx4.p1.1.1" class="ltx_text ltx_font_bold">?</span>) is our baseline architecture.
To compare with the baseline model, we have re-trained the BAN model(<span id="Sx4.SSx4.p1.1.2" class="ltx_text ltx_font_bold">?</span>) from their github repository to reproduce the results.
A note to mention, we deploy some changes to BAN model while reproducing it.
First, we use batch size 180 to run their model as we deploy for our VQA-CoIn model.
Second, we have omitted the effect of data augmentation of visual genome dataset from BAN model as we have not employ any data augmentation.
The reason of making these changes is to appropriately compare our model with the baseline model.</p>
</div>
</section>
<section id="Sx4.SSx5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Evaluation Criterion</h3>

<div id="Sx4.SSx5.p1" class="ltx_para">
<p id="Sx4.SSx5.p1.1" class="ltx_p">For VQA tasks, question accuracy is the preferred evaluation metric.
In this section, we are going to illustrate the evaluation process we have followed.
Like previous VQA approaches, we have computed accuracy to decide how our architecture is performing to figure out the correct answers for a given question using both image features and contextual information about the image.
According to the employed dataset, validation and test accuracy scores are calculated.
Validation accuracy is measured by comparing against the ground truth answers available in the validation data split.
And to obtain the test score, we have generated answers for the questions in the test split based on our model and submitted the results in the VQA challenge hosting site EvalAI.
It provides scores for each category of questions which are already defined in the dataset.</p>
</div>
<figure id="Sx4.T2" class="ltx_table">
<table id="Sx4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T2.1.1.1" class="ltx_tr">
<th id="Sx4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="Sx4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Validation Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T2.1.2.1" class="ltx_tr">
<th id="Sx4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">bottom-up</th>
<td id="Sx4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">63.20</td>
</tr>
<tr id="Sx4.T2.1.3.2" class="ltx_tr">
<th id="Sx4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">BAN-8</th>
<td id="Sx4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">66.28</td>
</tr>
<tr id="Sx4.T2.1.4.3" class="ltx_tr">
<th id="Sx4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">VQA-CoIn(I+Q)</th>
<td id="Sx4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">66.03</td>
</tr>
<tr id="Sx4.T2.1.5.4" class="ltx_tr">
<th id="Sx4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">VQA-CoIn(I+Q+SI)</th>
<td id="Sx4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="Sx4.T2.1.5.4.2.1" class="ltx_text ltx_font_bold">66.33</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Validation scores computed on the full VQA v2.0 dataset for bottom-up model, BAN-8 and our architecture. </figcaption>
</figure>
<figure id="Sx4.T3" class="ltx_table ltx_align_center">
<table id="Sx4.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T3.1.1.1" class="ltx_tr">
<th id="Sx4.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx4.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.1.1.1.1.1" class="ltx_p" style="width:45.5pt;">Method</span>
</span>
</th>
<th id="Sx4.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.1.1.2.1.1" class="ltx_p" style="width:22.8pt;">yes/no</span>
</span>
</th>
<th id="Sx4.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.1.1.3.1.1" class="ltx_p" style="width:25.6pt;">number</span>
</span>
</th>
<th id="Sx4.T3.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T3.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.1.1.4.1.1" class="ltx_p" style="width:22.8pt;">other</span>
</span>
</th>
<th id="Sx4.T3.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T3.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.1.1.5.1.1" class="ltx_p" style="width:22.8pt;">overall</span>
</span>
</th>
<th id="Sx4.T3.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T3.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.1.1.6.1.1" class="ltx_p" style="width:28.5pt;">test-dev</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T3.1.2.1" class="ltx_tr">
<td id="Sx4.T3.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt">
<span id="Sx4.T3.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.2.1.1.1.1" class="ltx_p" style="width:45.5pt;">bottom-up</span>
</span>
</td>
<td id="Sx4.T3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="Sx4.T3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.2.1.2.1.1" class="ltx_p" style="width:22.8pt;">-</span>
</span>
</td>
<td id="Sx4.T3.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="Sx4.T3.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.2.1.3.1.1" class="ltx_p" style="width:25.6pt;">-</span>
</span>
</td>
<td id="Sx4.T3.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="Sx4.T3.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.2.1.4.1.1" class="ltx_p" style="width:22.8pt;">-</span>
</span>
</td>
<td id="Sx4.T3.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="Sx4.T3.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.2.1.5.1.1" class="ltx_p" style="width:22.8pt;">65.67</span>
</span>
</td>
<td id="Sx4.T3.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="Sx4.T3.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.2.1.6.1.1" class="ltx_p" style="width:28.5pt;">65.32</span>
</span>
</td>
</tr>
<tr id="Sx4.T3.1.3.2" class="ltx_tr">
<td id="Sx4.T3.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="Sx4.T3.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.3.2.1.1.1" class="ltx_p" style="width:45.5pt;">BAN-8</span>
</span>
</td>
<td id="Sx4.T3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="Sx4.T3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.3.2.2.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx4.T3.1.3.2.2.1.1.1" class="ltx_text ltx_font_bold">83.61</span></span>
</span>
</td>
<td id="Sx4.T3.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="Sx4.T3.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.3.2.3.1.1" class="ltx_p" style="width:25.6pt;">50.45</span>
</span>
</td>
<td id="Sx4.T3.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="Sx4.T3.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.3.2.4.1.1" class="ltx_p" style="width:22.8pt;">58.12</span>
</span>
</td>
<td id="Sx4.T3.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="Sx4.T3.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.3.2.5.1.1" class="ltx_p" style="width:22.8pt;">67.75</span>
</span>
</td>
<td id="Sx4.T3.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="Sx4.T3.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.3.2.6.1.1" class="ltx_p" style="width:28.5pt;">68.07</span>
</span>
</td>
</tr>
<tr id="Sx4.T3.1.4.3" class="ltx_tr">
<td id="Sx4.T3.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r">
<span id="Sx4.T3.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.4.3.1.1.1" class="ltx_p" style="width:45.5pt;">VQA-CoIn</span>
</span>
</td>
<td id="Sx4.T3.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="Sx4.T3.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.4.3.2.1.1" class="ltx_p" style="width:22.8pt;">83.57</span>
</span>
</td>
<td id="Sx4.T3.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="Sx4.T3.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.4.3.3.1.1" class="ltx_p" style="width:25.6pt;"><span id="Sx4.T3.1.4.3.3.1.1.1" class="ltx_text ltx_font_bold">50.91</span></span>
</span>
</td>
<td id="Sx4.T3.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="Sx4.T3.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.4.3.4.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx4.T3.1.4.3.4.1.1.1" class="ltx_text ltx_font_bold">58.33</span></span>
</span>
</td>
<td id="Sx4.T3.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="Sx4.T3.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.4.3.5.1.1" class="ltx_p" style="width:22.8pt;"><span id="Sx4.T3.1.4.3.5.1.1.1" class="ltx_text ltx_font_bold">67.88</span></span>
</span>
</td>
<td id="Sx4.T3.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="Sx4.T3.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T3.1.4.3.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="Sx4.T3.1.4.3.6.1.1.1" class="ltx_text ltx_font_bold">68.2</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of Test-standard scores among VQA-CoIn, BAN-8 and bottom-up model. VQA-CoIn are trained using VQA v2.0 train and validation splits and tested on test split. Note that all 3 models are single models. </figcaption>
</figure>
<figure id="Sx4.T4" class="ltx_table">
<table id="Sx4.T4.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T4.3.3.3" class="ltx_tr">
<th id="Sx4.T4.3.3.3.4" class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="Sx4.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.1.1.1.1.1.1" class="ltx_p" style="width:108.1pt;"><img src="/html/2004.10966/assets/figures/fig2.jpg" id="Sx4.T4.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="350" height="306" alt="[Uncaptioned image]"></span>
</span>
</th>
<th id="Sx4.T4.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T4.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.2.2.2.2.1.1" class="ltx_p" style="width:108.1pt;"><img src="/html/2004.10966/assets/figures/fig3.jpg" id="Sx4.T4.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="375" height="300" alt="[Uncaptioned image]"></span>
</span>
</th>
<th id="Sx4.T4.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.3.3.1.1" class="ltx_p" style="width:108.1pt;"><img src="/html/2004.10966/assets/figures/fig4.jpg" id="Sx4.T4.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_square" width="368" height="308" alt="[Uncaptioned image]"></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T4.3.3.4.1" class="ltx_tr">
<td id="Sx4.T4.3.3.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.4.1.1.1.1" class="ltx_p" style="width:79.7pt;">question</span>
</span>
</td>
<td id="Sx4.T4.3.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.4.1.2.1.1" class="ltx_p" style="width:108.1pt;">What is he doing at night?</span>
</span>
</td>
<td id="Sx4.T4.3.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.4.1.3.1.1" class="ltx_p" style="width:108.1pt;">What sport is the man participating in?</span>
</span>
</td>
<td id="Sx4.T4.3.3.4.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.4.1.4.1.1" class="ltx_p" style="width:108.1pt;">What color is the man wearing?</span>
</span>
</td>
</tr>
<tr id="Sx4.T4.3.3.5.2" class="ltx_tr">
<td id="Sx4.T4.3.3.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.5.2.1.1.1" class="ltx_p" style="width:79.7pt;">semantic info</span>
</span>
</td>
<td id="Sx4.T4.3.3.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.5.2.2.1.1" class="ltx_p" style="width:108.1pt;">‘man <span id="Sx4.T4.3.3.5.2.2.1.1.1" class="ltx_text ltx_font_bold">playing frisbee</span>’, ‘green grass field’, ‘man wearing white shirt’, ‘sky clear’, ‘man wearing shorts’, ‘man short hair’</span>
</span>
</td>
<td id="Sx4.T4.3.3.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.5.2.3.1.1" class="ltx_p" style="width:108.1pt;">‘large blue sky’, ‘person skiing’, ‘man wearing black jacket’, ‘person <span id="Sx4.T4.3.3.5.2.3.1.1.1" class="ltx_text ltx_font_bold">snowboarding</span>’, ‘snow covered mountain’</span>
</span>
</td>
<td id="Sx4.T4.3.3.5.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.5.2.4.1.1" class="ltx_p" style="width:108.1pt;">‘black and white cow’, ‘man wearing hat’, ‘hat man’, ‘man and woman sitting bench’,
‘<span id="Sx4.T4.3.3.5.2.4.1.1.1" class="ltx_text ltx_font_bold">red and white striped shirt</span>’</span>
</span>
</td>
</tr>
<tr id="Sx4.T4.3.3.6.3" class="ltx_tr">
<td id="Sx4.T4.3.3.6.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.6.3.1.1.1" class="ltx_p" style="width:79.7pt;">ground truth answer</span>
</span>
</td>
<td id="Sx4.T4.3.3.6.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.6.3.2.1.1" class="ltx_p" style="width:108.1pt;">playing frisbee</span>
</span>
</td>
<td id="Sx4.T4.3.3.6.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.6.3.3.1.1" class="ltx_p" style="width:108.1pt;">snowborading</span>
</span>
</td>
<td id="Sx4.T4.3.3.6.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.6.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.6.3.4.1.1" class="ltx_p" style="width:108.1pt;">red, white and blue</span>
</span>
</td>
</tr>
<tr id="Sx4.T4.3.3.7.4" class="ltx_tr">
<td id="Sx4.T4.3.3.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.7.4.1.1.1" class="ltx_p" style="width:79.7pt;">VQA-CoIn answer</span>
</span>
</td>
<td id="Sx4.T4.3.3.7.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.7.4.2.1.1" class="ltx_p" style="width:108.1pt;">playing frisbee</span>
</span>
</td>
<td id="Sx4.T4.3.3.7.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.7.4.3.1.1" class="ltx_p" style="width:108.1pt;">snowboarding</span>
</span>
</td>
<td id="Sx4.T4.3.3.7.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.7.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.7.4.4.1.1" class="ltx_p" style="width:108.1pt;">red and white</span>
</span>
</td>
</tr>
<tr id="Sx4.T4.3.3.8.5" class="ltx_tr">
<td id="Sx4.T4.3.3.8.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.8.5.1.1.1" class="ltx_p" style="width:79.7pt;">BAN-8 answer</span>
</span>
</td>
<td id="Sx4.T4.3.3.8.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.8.5.2.1.1" class="ltx_p" style="width:108.1pt;">playing</span>
</span>
</td>
<td id="Sx4.T4.3.3.8.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.8.5.3.1.1" class="ltx_p" style="width:108.1pt;">skiing</span>
</span>
</td>
<td id="Sx4.T4.3.3.8.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="Sx4.T4.3.3.8.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="Sx4.T4.3.3.8.5.4.1.1" class="ltx_p" style="width:108.1pt;">white</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of answers generated for questions and images from the validation data by VQA-CoIn and BAN-8 models. </figcaption>
</figure>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Results &amp; Discussion</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">We have considered BAN-8 (<span id="Sx5.p1.1.1" class="ltx_text ltx_font_bold">?</span>) model as our baseline and compared our results with it’s single model validation and test scores.
Unlike other approaches, we also execute an experiment in which we check the validation accuracy while a model is trained with different scales of training data.
Through this investigation, we want to figure out whether our model can learn and generate precise answers for the validation questions though it has been trained on different sets of train dataset.</p>
</div>
<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Quantitative Results</h3>

<div id="Sx5.SSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.p1.1" class="ltx_p">Table 1 demonstrates the results for our data scaling experiment.
We perform this experiment using on our VQA-CoIn model and our baseline model.
We find that for one fourth and three fourth of the dataset, when we train our model, it is capable of functioning better than BAN-8 model.
But while trained on 50% of the training split, BAN model perform better than VQA-CoIn.
The reason behind this could be, as we are enforcing contextual information of the images generated by a pre-trained model in our method, some of these information may not carry knowledge related to the question to answer it correctly.
This observation can lead our study to further investigation by producing and invoking SI using other pre-trained models in future.
We still feel that this gives strong evidence that our approach can better utilize small amounts of data when compared to state-of-the-art approaches.
Through Table 2, we estimate the validation score of our model for the whole dataset with the state-of-the-art VQA models.
The validation score on the <span id="Sx5.SSx1.p1.1.1" class="ltx_text ltx_font_italic">VQA-CoIn (I+Q)</span> row of the table portrays the importance of the region based SI of images we employ through our model.
While SI along with image(I) and question(Q) as inputs are given to the network, VQA-CoIn outperforms the state-of-art baseline architectures in terms of accuracy.</p>
</div>
<div id="Sx5.SSx1.p2" class="ltx_para">
<p id="Sx5.SSx1.p2.1" class="ltx_p">In order to receive scores for the test set, we submitted the results produced by our model to the VQA competition using EValAI.
We also submit the reproduced answers of BAN in the same site to find out and compare the test-dev and test-standard scores with ours.
According to the results returned, displayed in Table 3, We can observe that VQA-CoIn has outperformed BAN and bottom-up(<span id="Sx5.SSx1.p2.1.1" class="ltx_text ltx_font_bold">?</span>) in test-dev and test-standard challenges.
If we consider each category of questions for BAN-8 and VQA-CoIn models, we can see that VQA-CoIn network has surpassed the scores of BAN-8 for ‘number’ and ‘other’ categorical questions.
For ‘yes/no’ category of questions, BAN has performed better than ours.
We feel that these results are significant, especially our performance on the ‘other’ category.
To answer any question from ‘other’ category, a model needs to understand more complex relation among the contents of an image (where to search for an answer).
SI provides support behind this logic and helps our model to generate more accurate answers than our baseline models.</p>
</div>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Qualitative Results</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">After the quantitative comparison of our and two state-of-the-art models, we do a qualitative contrast between VQA-CoIn and BAN-8 using the data of validation split.
This is not meant to be a formal evaluation, but mainly meant to provide additional context to the results that our approach gives compared to our baselines.
Table <a href="#Sx4.T4" title="Table 4 ‣ Evaluation Criterion ‣ Experimental Setup ‣ Visual Question Answering Using Semantic Information from Image Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> represents the contrast.
We have image, question and SI for each of three examples.
The human annotated ground truth answers for the examples are also added so that the answers generated by both of the models can be compared with it.
From the table, we can see that for image (a) and (b), our model generates correct answers.
For the same images, BAN model generates answers that are very close to the answers from the dataset, but not accurate.
Here, the reason of the success of our model is both image features and SI for images.
The answers for the questions are already available in the SI.
For ease of reading, we bold the texts on the row named as semantic info in the table.
Now, if we match answers for image (c) of both models, answers are not exact to the ground truth answers.
Our model is able to detect only two colors using both image features and SI (bold texts in semantic info row under image (c)) available for the input question.
So, VQA-CoIn chooses these two colors as answer.
It also means that, if better SI is used, our model can generate more correct answers.</p>
</div>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">In this paper, we have proposed a novel VQA architecture, VQA-CoIn, which incorporates contextual information of every possible region of an image to understand and represent features of an image with already available textual information about it.
Our motivation behind incorporating SI in the form of natural language descriptions is to better enable ML models to bridge the gap between the questions being asked and the image itself.
We also hypothesize that this should result in better data scaling, and enable ML models to perform well with less data.
We have compared our VQA-CoIn model with two state-of-the-art models and showed that our model performs better than those models both in terms of raw accuracy, and in terms of scaling performance.
As our future work, we intend to apply our model to build applications to help visually impaired to guide them with human understandable texts.
We also have a plan to do human evaluation of the results we achieve.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Anderson et al<span id="bib.bibx1.1.1.1" class="ltx_text">.</span> 2018]</span>
<span class="ltx_bibblock">
Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and
Zhang, L.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.3.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Antol et al<span id="bib.bibx2.1.1.1" class="ltx_text">.</span> 2015]</span>
<span class="ltx_bibblock">
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick, C. L.; and
Parikh, D.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Cho et al<span id="bib.bibx3.1.1.1" class="ltx_text">.</span> 2014]</span>
<span class="ltx_bibblock">
Cho; van Merrienboer, B.; Bahdanau, D.; ; and Bengio, Y.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">On the properties of neural machinetranslation: Encoder-decoder
approaches.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx3.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1259</span>.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chung et al<span id="bib.bibx4.1.1.1" class="ltx_text">.</span> 2014]</span>
<span class="ltx_bibblock">
Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Empirical evaluation of gated recurrent neural networks on sequence
modeling.

</span>
<span class="ltx_bibblock">cite arxiv:1412.3555Comment: Presented in NIPS 2014 Deep Learning and
Representation Learning Workshop.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Das et al<span id="bib.bibx5.1.1.1" class="ltx_text">.</span> 2016]</span>
<span class="ltx_bibblock">
Das, A.; Agrawal, H.; Zitnick, L.; Parikh, D.; and Batra, D.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Human attention in visual question answering: Do humans and deep
networks look at the same regions?

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</span>, 932–937.

</span>
<span class="ltx_bibblock">Austin, Texas: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Goyal et al<span id="bib.bibx6.1.1.1" class="ltx_text">.</span> 2016]</span>
<span class="ltx_bibblock">
Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; and Parikh, D.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx6.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1612.00837.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Jiang et al<span id="bib.bibx7.1.1.1" class="ltx_text">.</span> 2018]</span>
<span class="ltx_bibblock">
Jiang, Y.; Natarajan, V.; Chen, X.; Rohrbach, M.; Batra, D.; and Parikh, D.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Pythia v0.1: the winning entry to the VQA challenge 2018.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1807.09956.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Jiang et al<span id="bib.bibx8.1.1.1" class="ltx_text">.</span> 2020]</span>
<span class="ltx_bibblock">
Jiang, H.; Misra, I.; Rohrbach, M.; Learned-Miller, E. G.; and Chen, X.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">In defense of grid features for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.3.1" class="ltx_text ltx_font_italic">CoRR</span> 2001.03615.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Johnson, Karpathy, and Li 2015]</span>
<span class="ltx_bibblock">
Johnson, J.; Karpathy, A.; and Li, F.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Densecap: Fully convolutional localization networks for dense
captioning.

</span>
<span class="ltx_bibblock"><span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1511.07571.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kazemi and Elqursh 2017]</span>
<span class="ltx_bibblock">
Kazemi, V., and Elqursh, A.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Show, ask, attend, and answer: A strong baseline for visual
question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1704.03162.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kim and Bansal 2019]</span>
<span class="ltx_bibblock">
Kim, H., and Bansal, M.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Improving visual question answering by referring to generated
paragraph captions.

</span>
<span class="ltx_bibblock"><span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1906.06216.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kim, Jun, and Zhang 2018]</span>
<span class="ltx_bibblock">
Kim, J.-H.; Jun, J.; and Zhang, B.-T.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock">In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.;
Cesa-Bianchi, N.; and Garnett, R., eds., <span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems 31</span>. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">1564–1574.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ren et al<span id="bib.bibx13.1.1.1" class="ltx_text">.</span> 2015]</span>
<span class="ltx_bibblock">
Ren, S.; He, K.; Girshick, R.; and Sun, J.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock">In Cortes, C.; Lawrence, N. D.; Lee, D. D.; Sugiyama, M.; and
Garnett, R., eds., <span id="bib.bibx13.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
28</span>. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">91–99.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Teney et al<span id="bib.bibx14.1.1.1" class="ltx_text">.</span> 2017]</span>
<span class="ltx_bibblock">
Teney, D.; Anderson, P.; He, X.; and van den Hengel, A.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Tips and tricks for visual question answering: Learnings from the
2017 challenge.

</span>
<span class="ltx_bibblock"><span id="bib.bibx14.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1708.02711.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al<span id="bib.bibx15.1.1.1" class="ltx_text">.</span> 2017]</span>
<span class="ltx_bibblock">
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.;
Kaiser, L. u.; and Polosukhin, I.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.;
Vishwanathan, S.; and Garnett, R., eds., <span id="bib.bibx15.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems 30</span>. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">5998–6008.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wu et al<span id="bib.bibx16.1.1.1" class="ltx_text">.</span> 2016]</span>
<span class="ltx_bibblock">
Wu, Q.; Wang, P.; Shen, C.; Dick, A.; and v. d. Hengel, A.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx16.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)</span>, 4622–4630.

</span>
<span class="ltx_bibblock">Las Vegas, NV: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wu, Hu, and Mooney 2019]</span>
<span class="ltx_bibblock">
Wu, J.; Hu, Z.; and Mooney, R. J.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Generating question relevant captions to aid visual question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1906.00513.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al<span id="bib.bibx18.1.1.1" class="ltx_text">.</span> 2015]</span>
<span class="ltx_bibblock">
Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. J.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx18.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1511.02274.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[You et al<span id="bib.bibx19.1.1.1" class="ltx_text">.</span> 2016]</span>
<span class="ltx_bibblock">
You, Q.; Jin, H.; Wang, Z.; Fang, C.; and Luo, J.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Image captioning with semantic attention.

</span>
<span class="ltx_bibblock"><span id="bib.bibx19.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1603.03925.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhang, Hare, and
Prügel-Bennett 2018]</span>
<span class="ltx_bibblock">
Zhang, Y.; Hare, J.; and Prügel-Bennett, A.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Learning to count objects in natural images for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.10965" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.10966" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.10966">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.10966" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.10967" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 07:17:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
