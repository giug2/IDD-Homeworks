<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>NutriVision: A System for Automatic Diet Management in Smart Healthcare</title>
<!--Generated on Mon Sep 30 15:31:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.20508v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S1" title="In NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S2" title="In NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span>Novel Contributions of the Current Paper</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S2.SS1" title="In II Novel Contributions of the Current Paper ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A) </span>Problem Addressed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S2.SS2" title="In II Novel Contributions of the Current Paper ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B) </span>Solution Proposed by NutriVision</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S2.SS3" title="In II Novel Contributions of the Current Paper ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C) </span>Significance of the Proposed Solution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S3" title="In NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4" title="In NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span>The Innovative Framework: NutriVision</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1" title="In IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A) </span>Framework</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1.SSS1" title="In A) Framework ‣ IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1) </span>Acquisition of Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1.SSS2" title="In A) Framework ‣ IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2) </span>Reference Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1.SSS3" title="In A) Framework ‣ IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3) </span>Image Preprocessing and Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1.SSS4" title="In A) Framework ‣ IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4) </span>Model Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1.SSS5" title="In A) Framework ‣ IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5) </span>Food Quantification and Nutrition Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4.SS1.SSS6" title="In A) Framework ‣ IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6) </span>Nutritional Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S5" title="In NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span>Validation of NutriVision</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S5.SS1" title="In V Validation of NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A) </span>Validation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S5.SS2" title="In V Validation of NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B) </span>Comparison with existing research</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S6" title="In NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">NutriVision: A System for Automatic Diet Management in Smart Healthcare</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Maintaining excellent health and fitness depends on Maintaining health and fitness through a balanced diet is essential for preventing non-communicable diseases such as heart disease, diabetes, and cancer. NutriVision combines smart healthcare with computer vision and machine learning to address the challenges of nutrition and dietary management. This paper introduces a novel system that can identify food items, estimate quantities, and provide comprehensive nutritional information. NutriVision employs the Faster Region-based Convolutional Neural Network (Faster R-CNN), a deep learning algorithm that improves object detection by generating region proposals and then classifying those regions, making it highly effective for accurate and fast food identification even in complex and disorganized meal settings. Through smartphone-based image capture, NutriVision delivers instant nutritional data, including macronutrient breakdown, calorie count, and micronutrient details. One of the standout features of NutriVision is its personalized nutritional analysis and diet recommendations, which are tailored to each user’s dietary preferences, nutritional needs, and health history. By providing customized advice, NutriVision helps users achieve specific health and fitness goals, such as managing dietary restrictions or controlling weight. In addition to offering precise food detection and nutritional assessment, NutriVision supports smarter dietary decisions by integrating user data with recommendations that promote a balanced, healthful diet. This system presents a practical and advanced solution for nutrition management and has the potential to significantly influence how people approach their dietary choices, promoting healthier eating habits and overall well-being. This paper discusses the design, performance evaluation, and prospective applications of the NutriVision system.</p>
</div>
<div class="ltx_logical-block" id="id2">
<div class="ltx_para" id="id2.p1">
<p class="ltx_p ltx_align_center" id="id1.1"><a class="ltx_ref ltx_href" href="https://orcid.org/0009-0004-9279-449X" title=""><span class="ltx_text ltx_font_bold" id="id1.1.2.1">Madhumita Veeramreddy</span>   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="id1.1.1.g1" src="extracted/5886629/orcid-icon-2048x2048-q87cnnge.png" width="12"/></a></p>
<p class="ltx_p ltx_align_center" id="id2.p1.1">Department of Computer Science &amp; Engineering</p>
<p class="ltx_p ltx_align_center" id="id2.p1.2">SRM University AP, Amaravati, India</p>
<p class="ltx_p ltx_align_center" id="id2.p1.3"><a class="ltx_ref ltx_href ltx_font_typewriter" href="mailto:madhumita_v@srmap.edu.in" title="">madhumita_v@srmap.edu.in</a></p>
</div>
</div>
<div class="ltx_para" id="id4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="id4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id4.2.2">
<td class="ltx_td ltx_align_center" id="id3.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="id3.1.1.1.1">
<tr class="ltx_tr" id="id3.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="id3.1.1.1.1.1.1"><a class="ltx_ref ltx_href" href="https://orcid.org/0000-0002-2201-9402" title=""><span class="ltx_text ltx_font_bold" id="id3.1.1.1.1.1.1.2.1">Ashok Kumar Pradhan</span>   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="id3.1.1.1.1.1.1.1.g1" src="extracted/5886629/orcid-icon-2048x2048-q87cnnge.png" width="12"/></a></td>
</tr>
<tr class="ltx_tr" id="id3.1.1.1.1.2">
<td class="ltx_td ltx_align_center" id="id3.1.1.1.1.2.1">Department of Computer Science &amp; Engineering</td>
</tr>
<tr class="ltx_tr" id="id3.1.1.1.1.3">
<td class="ltx_td ltx_align_center" id="id3.1.1.1.1.3.1">SRM University AP, Amaravati, India</td>
</tr>
<tr class="ltx_tr" id="id3.1.1.1.1.4">
<td class="ltx_td ltx_align_center" id="id3.1.1.1.1.4.1"><a class="ltx_ref ltx_href ltx_font_typewriter" href="mailto:ashokkumar.p@srmap.edu.in" title="">ashokkumar.p@srmap.edu.in</a></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="id4.2.2.2">
<table class="ltx_tabular ltx_align_middle" id="id4.2.2.2.1">
<tr class="ltx_tr" id="id4.2.2.2.1.1">
<td class="ltx_td ltx_align_center" id="id4.2.2.2.1.1.1"><a class="ltx_ref ltx_href" href="https://orcid.org/0009-0005-5912-2138" title=""><span class="ltx_text ltx_font_bold" id="id4.2.2.2.1.1.1.2.1">Swetha Ghanta</span>   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="id4.2.2.2.1.1.1.1.g1" src="extracted/5886629/orcid-icon-2048x2048-q87cnnge.png" width="12"/></a></td>
</tr>
<tr class="ltx_tr" id="id4.2.2.2.1.2">
<td class="ltx_td ltx_align_center" id="id4.2.2.2.1.2.1">Department of Computer Science &amp; Engineering</td>
</tr>
<tr class="ltx_tr" id="id4.2.2.2.1.3">
<td class="ltx_td ltx_align_center" id="id4.2.2.2.1.3.1">SRM University AP, Amaravati, India</td>
</tr>
<tr class="ltx_tr" id="id4.2.2.2.1.4">
<td class="ltx_td ltx_align_center" id="id4.2.2.2.1.4.1"><a class="ltx_ref ltx_href ltx_font_typewriter" href="mailto:swetha_ghanta@srmap.edu.in" title="">swetha_ghanta@srmap.edu.in</a></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="id5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="id5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id5.1.1">
<td class="ltx_td ltx_align_center" id="id5.1.1.1"><a class="ltx_ref ltx_href" href="https://orcid.org/0000-0002-7089-9029" title=""><span class="ltx_text ltx_font_bold" id="id5.1.1.1.2.1">Laavanya Rachakonda</span>   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="id5.1.1.1.1.g1" src="extracted/5886629/orcid-icon-2048x2048-q87cnnge.png" width="12"/></a></td>
</tr>
<tr class="ltx_tr" id="id5.1.2.1">
<td class="ltx_td ltx_align_center" id="id5.1.2.1.1">Department of Computer Science</td>
</tr>
<tr class="ltx_tr" id="id5.1.3.2">
<td class="ltx_td ltx_align_center" id="id5.1.3.2.1">University of North Carolina, Wilmington, USA</td>
</tr>
<tr class="ltx_tr" id="id5.1.4.3">
<td class="ltx_td ltx_align_center" id="id5.1.4.3.1"><a class="ltx_ref ltx_href ltx_font_typewriter" href="mailto:rachakondal@uncw.edu" title="">rachakondal@uncw.edu</a></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_logical-block" id="id7">
<div class="ltx_para" id="id7.p1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="id6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id6.1.1">
<td class="ltx_td ltx_align_center" id="id6.1.1.1"><a class="ltx_ref ltx_href" href="https://orcid.org/0000-0003-2959-6541" title=""><span class="ltx_text ltx_font_bold" id="id6.1.1.1.2.1">Saraju P Mohanty</span>   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="id6.1.1.1.1.g1" src="extracted/5886629/orcid-icon-2048x2048-q87cnnge.png" width="12"/></a></td>
</tr>
<tr class="ltx_tr" id="id6.1.2.1">
<td class="ltx_td ltx_align_center" id="id6.1.2.1.1">Department of Computer Science &amp; Engineering</td>
</tr>
<tr class="ltx_tr" id="id6.1.3.2">
<td class="ltx_td ltx_align_center" id="id6.1.3.2.1">University of North Texas, Denton, USA</td>
</tr>
<tr class="ltx_tr" id="id6.1.4.3">
<td class="ltx_td ltx_align_center" id="id6.1.4.3.1"><a class="ltx_ref ltx_href ltx_font_typewriter" href="mailto:saraju.mohanty@unt.edu" title="">saraju.mohanty@unt.edu</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="ltx_logical-block" id="id8">
<div class="ltx_para" id="id8.p1">
<p class="ltx_p ltx_align_center" id="id8.p1.1"><span class="ltx_text" id="id8.p1.1.1">September 29, 2024</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.1.1">K</em><span class="ltx_text ltx_font_bold" id="p1.1.2">eywords</span> Smart Healthcare, Smart Agriculture, Diet Management, Diet Estimation, Food Quality, Food Nutrition, Convolutional Neural Networks (CNN)</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Indeed, the saying "wealth is nothing without health" underscores the pivotal role of nutrition in our overall well-being. Nutrients derived from food are essential for the proper functioning of our bodies, and deviations from a balanced diet can lead to various health issues. Fig. 1 illustrates the critical impact of both overeating and undernutrition on the development of chronic illnesses such as diabetes, heart disease, kidney disease, hypertension, obesity, and even life-threatening conditions like cancer (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib52" title="">52</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib53" title="">53</a>]</cite>). Obesity and inflammation emerge as primary catalysts for many diseases, and the consumption of certain food elements, including color additives, chemicals, trans fats, refined sugar, salt, and processed foods, is linked to these health concerns (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib54" title="">54</a>]</cite>). Consequently, individuals are modifying their dietary habits to address these challenges, prompting a widespread focus on diet regulation. Achieving a balance between dietary intake and monitoring is crucial for effective diet management.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="S1.F1.g1" src="x1.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Impact of Unbalanced Diet</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite the proliferation of nutritional tracking devices in the market, many rely heavily on user input, necessitating manual entry of food intake (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib55" title="">55</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib56" title="">56</a>]</cite>). This manual entry process can be cumbersome and time-consuming, leading to users hesitating to utilize these systems consistently. Additionally, dependence on user input raises the risk of inaccurate data entry, resulting in unreliable results. Commonly, these systems provide data on calorie quantities in food, consumption levels, and remaining allowances for the day, aligning with their primary goals of promoting a healthy weight and fitting into a particular diet framework. However, a crucial point emerges: being healthy doesn’t always equate to adhering strictly to a low-calorie diet. A holistic approach to health considers various factors beyond mere calorie counts, emphasizing the importance of balanced nutrition and overall lifestyle choices.

<br class="ltx_break"/>
<br class="ltx_break"/>In response to these challenges, there is a growing need for innovative solutions that offer accurate nutritional tracking without placing undue burdens on users, fostering sustained adherence and promoting a more comprehensive understanding of healthful dietary practices (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib57" title="">57</a>]</cite>). Individuals who have high blood pressure ought to stick to a low-sodium diet, diabetics need to avoid sugar, and so on. Thus, monitoring sugar, sodium, saturated fat, protein, carbs, and other nutrients is crucial.

<br class="ltx_break"/>
<br class="ltx_break"/>The research presented here explores the intersection of personalized healthcare, nutritional science, and computer vision, presenting a comprehensive system known as "NutriVision". The goal of the NutriVision system is to address the growing demand for a more informed and health-conscious approach to food selection. Essentially, NutriVision recognizes and detects food ingredients in user-provided photos using image recognition and AI-driven algorithms. After that, it embarks on a complicated journey of nutritional evaluation, quantifying the macronutrients, vitamins, and minerals present in the foods it has identified. However, what truly sets NutriVision apart is not just its capacity to deliver an accurate nutritional analysis but also its capacity to deliver personalized dietary suggestions and health guidance according to each user’s unique health reports and goals.

<br class="ltx_break"/>Moreover, the adaptability of the NutriVision system to accommodate various dietary preferences and restrictions ensures that it can serve a wide audience, including vegetarians, vegans, and individuals with food allergies. By providing customizable options, NutriVision empowers users to make choices that align with their personal health needs and lifestyle preferences. This versatility not only enhances user engagement but also promotes a more inclusive approach to nutrition, ultimately fostering a supportive environment for diverse dietary practices.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="897" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Smart Healthcare as a Healthcare Cyber-Physical System (H-CPS)</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Fig. 2 illustrates the Healthcare Cyber-Physical System (H-CPS) as a multi-layered model integrating various devices and technologies to monitor and enhance patient health in real time. At the core is the Patient Hub, comprising devices like fitness trackers, glucometers, and blood pressure monitors that directly track vital signs. Surrounding it, the Processing Hub includes AI algorithms, cloud computing, and data processors that analyze and interpret the collected data. NutriVision integrates within this hub as a nutritional management system, utilizing image recognition and machine learning to assess food intake and provide personalized dietary guidance.

<br class="ltx_break"/>
<br class="ltx_break"/>NutriVision’s role in H-CPS focuses on nutrition as a key element of preventive healthcare. By offering real-time insights into food choices, it helps users make healthier decisions tailored to their unique health needs. The Connectivity Hub ensures smooth communication between devices via 5G, Wi-Fi, and other networks, enabling NutriVision to provide instant feedback and seamlessly integrate its recommendations into broader healthcare monitoring. Through this smart integration, NutriVision supports healthier eating habits, reducing the risk of diet-related diseases and contributing to overall health and wellness within the smart healthcare system.

<br class="ltx_break"/>
<br class="ltx_break"/>As society increasingly recognizes the importance of nutrition in disease prevention and overall health, tools like NutriVision stand to play a transformative role in empowering individuals to make informed dietary choices. By leveraging advanced technologies such as image recognition and machine learning, NutriVision not only simplifies the process of tracking nutritional intake but also enhances users’ awareness of their eating habits. This intuitive approach fosters a deeper connection between individuals and their food choices, encouraging them to embrace healthier eating patterns. Furthermore, by providing personalized insights and recommendations tailored to specific health conditions, NutriVision aims to bridge the gap between dietary knowledge and practical application, ultimately supporting users in achieving their health goals and improving their quality of life. This integration of technology and nutrition is poised to contribute significantly to public health initiatives aimed at reducing diet-related diseases.

<br class="ltx_break"/>
<br class="ltx_break"/>This is how the other parts of the paper are arranged. The innovative aspects of the suggested solution are discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S2" title="II Novel Contributions of the Current Paper ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_tag">II</span></a>. We review several related works and provide background information for our work in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S3" title="III Related Work ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_tag">III</span></a>. Together with the test findings, a thorough explanation of the recommended solution is given in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S4" title="IV The Innovative Framework: NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_tag">IV</span></a>. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S5" title="V Validation of NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_tag">V</span></a> contains the results and some comparative analysis. The paper is concluded in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S6" title="VI Conclusion and Future Work ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_tag">VI</span></a> with a discussion of upcoming projects.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span>Novel Contributions of the Current Paper</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A) </span>Problem Addressed</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Current food nutrition monitoring systems often lack personalization and real-time estimation capabilities. Users frequently face challenges with generic dietary advice that doesn’t account for individual health data. Additionally, there is a need for systems that provide immediate feedback on the nutritional content of food and tailor recommendations based on daily nutrient requirements.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B) </span>Solution Proposed by NutriVision</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">NutriVision addresses these issues with a state-of-the-art system that offers several significant advantages. First, it provides automated, real-time nutritional content estimation, allowing users to understand the nutritional value of their food before consumption. This feature ensures that users receive instant feedback on their meals, enhancing their awareness of nutritional intake. Second, NutriVision integrates user-specific health data to deliver personalized dietary recommendations tailored to individual needs, based on metrics such as BMI and dietary preferences. The system also considers daily nutrient intake requirements when suggesting meals, ensuring that advice is relevant and promotes a balanced diet. By eliminating the need for manual entry of food data, NutriVision streamlines the monitoring process, reducing user effort and minimizing errors. Additionally, the system’s real-time adaptation feature allows it to adjust recommendations based on continuous feedback and data, providing ongoing improvement in dietary guidance. This combination of real-time data, personalization, and adaptability enhances user convenience, making NutriVision an ideal solution for managing nutritional intake with minimal effort and maximum efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C) </span>Significance of the Proposed Solution</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">NutriVision’s innovative approach revolutionizes food nutrition monitoring by offering an interactive platform that improves dietary habits. Its real-time estimation and personalized recommendations provide users with actionable insights that are directly applicable to their daily lives. The system’s ability to adapt recommendations based on feedback ensures continuous improvement and relevance. This combination of real-time data, personalization, and adaptability makes NutriVision an ideal solution for those seeking to enhance their dietary health with minimal effort and maximum efficiency.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span>Related Work</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In light of the contemporary surge in advanced hardware technologies, particularly in high-performance computing processors and devices, coupled with the evolution of diverse computing paradigms such as the Internet of Things (IoT), edge-based computing platforms, and tailored artificial intelligence (AI) models for edge computing, innovative models like smart healthcare and intelligent agriculture have materialized into practical applications. Notably, the realm of automatic dietary intake estimations has witnessed notable advancements, predominantly manifesting through mobile app-centric solutions. This segment delves into pertinent research endeavors within this domain.

<br class="ltx_break"/>
<br class="ltx_break"/>A systematic classification of diet monitoring approaches is shown in Fig. 3, which differentiates between automated and manual techniques. Manual diet monitoring methods require individuals to actively engage in tracking their food intake, with users being responsible for recording their dietary habits. One of the simplest forms is maintaining a food journal, where individuals document what they eat throughout the day, either on paper or digitally through spreadsheets or text files. This approach heavily relies on user memory and honesty, making it prone to inconsistencies. In recent years, various smartphone apps have emerged to streamline this process, offering a digital interface for meal input and integrating nutritional databases to estimate caloric intake and macronutrient breakdowns.

<br class="ltx_break"/>
<br class="ltx_break"/>Calorie counting is another widely used manual approach, involving the calculation of the caloric value of consumed foods through nutritional labels, online resources, or databases. While some apps assist with this, users remain responsible for ensuring the accuracy of their entries, introducing potential for human error. Issues such as overestimating portions or misidentifying food types can lead to inaccurate caloric calculations, which may derail dietary goals. These manual methods are often time-consuming and heavily dependent on user motivation, discipline, and consistency. This reliance on active participation increases the likelihood of under- or over-reporting, as human error—such as forgetting to log certain meals or incorrectly estimating portion sizes—significantly affect the results of manual diet monitoring.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="378" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Diet Monitoring Approaches</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">On the other hand, automated approaches utilize cutting-edge technology to reduce user burden, making the process more efficient, precise, and less prone to human error. One key approach in the automated category involves sensor-based systems. For instance, food-weight sensing utilizes smart plates, bowls, or scales that can automatically quantify the weight of the food. These systems are often integrated with AI to recognize food types and calculate nutritional values. Another type of sensor-based approach tracks hand-to-mouth movement through wearable devices like smartwatches or smart glasses. These wearables detect movement patterns and infer eating episodes by monitoring hand gestures and bite counts. This method can automatically log meals and estimate intake frequency, reducing reliance on self-reporting.

<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>Another advanced technique for automated diet tracking is image-based monitoring. This approach leverages artificial intelligence (AI) and image recognition technologies to analyze photos of food and estimate the portion size and nutritional content. Users simply take a picture of their meal, and AI algorithms trained on large food datasets identify the food items and generate estimates for calories, macronutrients, and portion sizes.

<br class="ltx_break"/>
<br class="ltx_break"/>In addition to visual and sensor-based methods, physiological monitoring offers another layer of precision in automated diet tracking. These techniques include continuous glucose monitoring (CGM) devices and biomarker analysis. For example, CGM systems monitor blood sugar levels in real-time, providing insights into how specific foods impact an individual’s glucose levels. Similarly, biomarker tracking analyzes bodily fluids such as saliva, blood, or urine to detect key nutritional markers. These physiological indicators help to track metabolic responses and offer a more comprehensive picture of how the body processes consumed foods.

<br class="ltx_break"/>
<br class="ltx_break"/>An edge-cloud procedure has been used and food in the cloud is classified using a convolutional neural network (CNN) in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib1" title="">1</a>]</cite>. Although food has been identified, this work has not estimated nutritional value. The user is informed whether or not he is stress eating, and some exercise is advised. Only the number of calories has been estimated. It does not give any personalized diet advice to the users. While in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib2" title="">2</a>]</cite>, 19 classes of Food-101 dataset has been identified. The user does not need to provide any kind of manual input , given it is fully automated. It uses a CNN(convolutional neural network) approach for food detection. It provides estimation of nutrients and not just calories. It has difficulty in handling customized food orders. But any kind of personalized advice and suggestions are not given.

<br class="ltx_break"/>
<br class="ltx_break"/>An edge-cloud method for dietary assessment has been used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib8" title="">8</a>]</cite>. The food images have been processed at the edge devices using image textural features, and the food in the cloud is classified using a convolutional neural network (CNN). Despite the fact that food has been identified in the Food-101 dataset, no nutritional value has been estimated in this work. Real-time estimations of food attributes are presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib9" title="">9</a>]</cite>. A textual corpus has been employed for the identification of food attributes, integrating deep learning methodologies for the recognition of food items. While the context of our work is the same, the nutrients are estimated based on the image.

<br class="ltx_break"/>
<br class="ltx_break"/>Food nutrients were analyzed and calories were estimated using mask-RCNN and union post processing in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib10" title="">10</a>]</cite>. The estimation of food weight involves the utilization of both the pixel count within the mask and the implementation of a linear regression model. Food serving sizes and their nutritional and calorific contents are estimated in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib20" title="">20</a>]</cite>. There has been no quantification in this case.

<br class="ltx_break"/>
<br class="ltx_break"/>A deep autoencoder network has been used to determine the vitamin A content based on the color of the pureed food in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib21" title="">21</a>]</cite>. Only one particular nutrient has been found in this work. To detect food for diabetic patients, researchers have implemented the bag-of-features model, as referenced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib18" title="">18</a>]</cite>. It calculates dense local features in the HSV color space through scale-invariant feature transforms to generate a visual dictionary consisting of 10,000 words. Subsequently, food photos are classified using linear support vector machine classifiers. Here, no nutrient value is computed. In a different investigation, the Food-Ingredient Joint Learning module was employed for ingredient recognition, and the Attention Fusion Network was utilized for fine-grained food recognition, as documented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib22" title="">22</a>]</cite>. Although the accuracy in food recognition is substantial, the accuracy in ingredient recognition varies from moderate to low.

<br class="ltx_break"/>
<br class="ltx_break"/>For ingredient recognition and food classification, a deep CNN has been employed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib19" title="">19</a>]</cite>. However, this work does not show any nutritional value. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib5" title="">5</a>]</cite> is dispersed across multiple computer platforms. There is a suggested food calorie estimation app that is mobile and cloud-based. Here, the food is identified using a cloud-based Support Vector Machine using the MapReduce technique. Calorie counting is done both before meals and after eating leftovers. For that, image graph cut segmentation has been employed. To establish a reference point for measurement, it is essential to have a thumb present within the frame of the food photo. The calorific value has also been determined here. Variations in thumb size are another problem. The person’s height affects the thumb size, which can lead to inaccurate results.

<br class="ltx_break"/>
<br class="ltx_break"/>The work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib11" title="">11</a>]</cite> introduces a model for food recognition based on Convolutional Neural Networks (CNN) and utilizes text2vec for attribute estimation. The model is client-server based. But the system does not include cooked foods, mixed foods, or liquid foods. The calorific value of the food is the main focus here as well. A wearable system based on piezoelectricity was used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib23" title="">23</a>]</cite> to identify food and calculate caloric intake. When eating, skin movement from the lower trachea is detected by the piezoelectric sensors. The food’s weight was determined, and then the final calorie count was carried out. Here, no additional nutritional value has been tallied.

<br class="ltx_break"/>
<br class="ltx_break"/>Diverse methodologies have been documented across several publications for determining calorific values from food volume. Notably, a multilayer perceptron model is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib24" title="">24</a>]</cite>, an image analysis-based approach is outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib25" title="">25</a>]</cite>, and a CNN-based method is detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib26" title="">26</a>]</cite>. Additionally, some works, including <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib27" title="">27</a>]</cite>, extend their focus beyond calorific values to encompass ingredient lists and cooking instructions, employing multitask CNN for this purpose. In a distinct context, the work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib28" title="">28</a>]</cite> categorizes various food types. This observation underscores that, while numerous papers address the computation of calorific values, a comparatively limited number delve into the broader spectrum of food nutritional value.

<br class="ltx_break"/>
<br class="ltx_break"/>Several studies have utilized various methodologies for dietary intake estimation. In the work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib45" title="">45</a>]</cite>, an innovative solution using inertial sensors was introduced. This system tracks eating gestures and combines them with image-based food identification algorithms to estimate calorie intake. However, it lacks real-time feedback and nutrition advice tailored to the user. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib42" title="">42</a>]</cite> employed a multimodal approach that integrates visual and acoustic data for food intake monitoring, though this system also focuses solely on calorific estimation rather than complete nutritional analysis.

<br class="ltx_break"/>
<br class="ltx_break"/>In an effort to automate dietary tracking, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib49" title="">49</a>]</cite> proposed a smartphone-based system that detects food using deep learning models. This system emphasizes ease of use, eliminating the need for user input, but suffers from accuracy issues when it comes to complex food mixtures. Nutritional value estimation is limited to caloric content, with no focus on personalized diet suggestions.

<br class="ltx_break"/>
<br class="ltx_break"/>In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib48" title="">48</a>]</cite>, a cloud-based mobile app that allows users to capture images of their meals is used. The food recognition is powered by CNNs, and the app estimates macronutrient content. However, this approach is limited by its reliance on cloud resources, which might hinder its performance in low-connectivity environments. Another study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib46" title="">46</a>]</cite> explored an AI-driven mobile app that utilizes food diaries and manual data entry to track food intake. While this tool offers a holistic nutritional breakdown, it lacks automation, as users must input data manually.

<br class="ltx_break"/>
<br class="ltx_break"/>Other researchers have focused on ingredient recognition as an intermediary step toward dietary analysis. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib47" title="">47</a>]</cite> introduced a model that uses a CNN to recognize ingredients in complex dishes. Their work emphasizes ingredient classification rather than a complete nutritional breakdown. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib39" title="">39</a>]</cite> employed a CNN-based approach for ingredient recognition in mixed dishes, but like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib47" title="">47</a>]</cite>, their system struggles with accurate nutritional estimation.

<br class="ltx_break"/>
<br class="ltx_break"/>A unique contribution is made by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib44" title="">44</a>]</cite>, where a system using a graph-based approach to food recognition is discussed. This method segments the food image into portions and then uses those segments for caloric and nutritional estimation. However, the system is prone to inaccuracies when dealing with complex meals. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib50" title="">50</a>]</cite> tackled this issue by proposing a method that integrates portion size estimation with ingredient recognition, though their model is still primarily concerned with calorie counting rather than a full nutritional assessment.

<br class="ltx_break"/>
<br class="ltx_break"/>In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib38" title="">38</a>]</cite>, a hybrid system that combines image analysis with manual data entry for diet tracking is introduced. Although effective in estimating calories and basic nutrients, it lacks the level of automation seen in other state-of-the-art systems. Another notable mention is the research conducted by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib43" title="">43</a>]</cite>, which explored deep learning algorithms for estimating the nutritional value of food in a fully automated way. However, the model was trained on a limited dataset, and its real-world application may be hindered by a lack of diversity in the food items it can recognize.

<br class="ltx_break"/>
<br class="ltx_break"/>In a similar way, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib41" title="">41</a>]</cite> presented a dietary tracking system that uses a knowledge-based approach, drawing on food ontologies to estimate nutrients and suggest personalized meal plans. While innovative, the system relies heavily on an external database, which may not cover all local food variations. Another edge-cloud system designed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib51" title="">51</a>]</cite> focused on identifying food in real-time but offered limited nutritional data, focusing primarily on calorie estimation. The study conducted by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib40" title="">40</a>]</cite> uses a mask-RCNN approach to segment food images and estimate portion sizes for caloric calculation. While their system is effective for identifying meal portions, it does not extend to broader nutritional analyses, leaving a gap in personalized dietary recommendations.

<br class="ltx_break"/>
<br class="ltx_break"/>Table 1 offers an overview of several studies for a more comprehensive context, highlighting significant results and insights from a variety of research work. By comparing these systems side by side, the table underscores the unique contributions of NutriVision, particularly in areas where other systems fall short, such as nutritional analysis and the provision of an interactive platform.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparative Analysis of Self-Tracking Systems</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1.1">Tracking System</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1" style="width:28.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1.1.1">Input</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.3.1">
<span class="ltx_p" id="S3.T1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1.1.1">Analysis</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.4.1">
<span class="ltx_p" id="S3.T1.1.1.1.4.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1.1.1">Fully-Automated System</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.5.1">
<span class="ltx_p" id="S3.T1.1.1.1.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.5.1.1.1">Nutritional Value Estimation</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.6.1">
<span class="ltx_p" id="S3.T1.1.1.1.6.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.6.1.1.1">Personalized Advice</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.7.1">
<span class="ltx_p" id="S3.T1.1.1.1.7.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.7.1.1.1">Interactive Platform</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.1.1">
<span class="ltx_p" id="S3.T1.1.2.1.1.1.1">Harrison, et al. (2010) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib29" title="">29</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.2.1">
<span class="ltx_p" id="S3.T1.1.2.1.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.3.1">
<span class="ltx_p" id="S3.T1.1.2.1.3.1.1">Not Feasible</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.4.1">
<span class="ltx_p" id="S3.T1.1.2.1.4.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.5.1">
<span class="ltx_p" id="S3.T1.1.2.1.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.6.1">
<span class="ltx_p" id="S3.T1.1.2.1.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S3.T1.1.2.1.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.7.1">
<span class="ltx_p" id="S3.T1.1.2.1.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.1.1">
<span class="ltx_p" id="S3.T1.1.3.2.1.1.1">O. Beijbom, et al. (2015) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib30" title="">30</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.2.1">
<span class="ltx_p" id="S3.T1.1.3.2.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.3.1">
<span class="ltx_p" id="S3.T1.1.3.2.3.1.1">Not Entirely Possible to be Done</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.4.1">
<span class="ltx_p" id="S3.T1.1.3.2.4.1.1" style="width:51.2pt;">Semi-Automated</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.5.1">
<span class="ltx_p" id="S3.T1.1.3.2.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.6.1">
<span class="ltx_p" id="S3.T1.1.3.2.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.7.1">
<span class="ltx_p" id="S3.T1.1.3.2.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.1.1">
<span class="ltx_p" id="S3.T1.1.4.3.1.1.1">Jiang, et al. (2018) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib31" title="">31</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.2.1">
<span class="ltx_p" id="S3.T1.1.4.3.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.3.1">
<span class="ltx_p" id="S3.T1.1.4.3.3.1.1">Not Very Beneficial</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.4.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.4.1">
<span class="ltx_p" id="S3.T1.1.4.3.4.1.1" style="width:51.2pt;">Semi-Automated</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.4.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.5.1">
<span class="ltx_p" id="S3.T1.1.4.3.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.4.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.6.1">
<span class="ltx_p" id="S3.T1.1.4.3.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.4.3.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.7.1">
<span class="ltx_p" id="S3.T1.1.4.3.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.1.1">
<span class="ltx_p" id="S3.T1.1.5.4.1.1.1">Pouladzadeh, et al. (2015) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib34" title="">34</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.2.1">
<span class="ltx_p" id="S3.T1.1.5.4.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.3.1">
<span class="ltx_p" id="S3.T1.1.5.4.3.1.1">Not Very Beneficial</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.5.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.4.1">
<span class="ltx_p" id="S3.T1.1.5.4.4.1.1" style="width:51.2pt;">Semi-Automated</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.5.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.5.1">
<span class="ltx_p" id="S3.T1.1.5.4.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.5.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.6.1">
<span class="ltx_p" id="S3.T1.1.5.4.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.5.4.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.7.1">
<span class="ltx_p" id="S3.T1.1.5.4.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.1.1">
<span class="ltx_p" id="S3.T1.1.6.5.1.1.1">L. Rachakonda, et al. (2019) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib35" title="">35</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.2.1">
<span class="ltx_p" id="S3.T1.1.6.5.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.3.1">
<span class="ltx_p" id="S3.T1.1.6.5.3.1.1">Yes</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.6.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.4.1">
<span class="ltx_p" id="S3.T1.1.6.5.4.1.1" style="width:51.2pt;">Semi-Automated</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.6.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.5.1">
<span class="ltx_p" id="S3.T1.1.6.5.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.6.5.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.6.1">
<span class="ltx_p" id="S3.T1.1.6.5.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.6.5.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.7.1">
<span class="ltx_p" id="S3.T1.1.6.5.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.1.1">
<span class="ltx_p" id="S3.T1.1.7.6.1.1.1">Taichi, et al. (2009) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib32" title="">32</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.2.1">
<span class="ltx_p" id="S3.T1.1.7.6.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.3.1">
<span class="ltx_p" id="S3.T1.1.7.6.3.1.1">No Nutritional Info</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.7.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.4.1">
<span class="ltx_p" id="S3.T1.1.7.6.4.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.7.6.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.5.1">
<span class="ltx_p" id="S3.T1.1.7.6.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.7.6.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.6.1">
<span class="ltx_p" id="S3.T1.1.7.6.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.7.6.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.7.1">
<span class="ltx_p" id="S3.T1.1.7.6.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.1.1">
<span class="ltx_p" id="S3.T1.1.8.7.1.1.1">L. Rachakond, et.al (2020) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib1" title="">1</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.2.1">
<span class="ltx_p" id="S3.T1.1.8.7.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.8.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.3.1">
<span class="ltx_p" id="S3.T1.1.8.7.3.1.1">Yes, Calorie Estimation and Stress Detection</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.8.7.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.4.1">
<span class="ltx_p" id="S3.T1.1.8.7.4.1.1" style="width:51.2pt;">Yes</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.8.7.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.5.1">
<span class="ltx_p" id="S3.T1.1.8.7.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.8.7.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.6.1">
<span class="ltx_p" id="S3.T1.1.8.7.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.8.7.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.7.1">
<span class="ltx_p" id="S3.T1.1.8.7.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.9.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.1.1">
<span class="ltx_p" id="S3.T1.1.9.8.1.1.1">A. Mitra, et al. (2022) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib2" title="">2</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.9.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.2.1">
<span class="ltx_p" id="S3.T1.1.9.8.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.9.8.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.3.1">
<span class="ltx_p" id="S3.T1.1.9.8.3.1.1">Yes, Nutrition Estimation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.9.8.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.4.1">
<span class="ltx_p" id="S3.T1.1.9.8.4.1.1" style="width:51.2pt;">Yes</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.9.8.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.5.1">
<span class="ltx_p" id="S3.T1.1.9.8.5.1.1" style="width:42.7pt;">Yes</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.9.8.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.6.1">
<span class="ltx_p" id="S3.T1.1.9.8.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.9.8.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.7.1">
<span class="ltx_p" id="S3.T1.1.9.8.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.10.9.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.1.1">
<span class="ltx_p" id="S3.T1.1.10.9.1.1.1">M.-L.Chian, et al. (2019) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib10" title="">10</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.10.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.2.1">
<span class="ltx_p" id="S3.T1.1.10.9.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.10.9.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.3.1">
<span class="ltx_p" id="S3.T1.1.10.9.3.1.1">Yes, Nutritional Estimation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.10.9.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.4.1">
<span class="ltx_p" id="S3.T1.1.10.9.4.1.1" style="width:51.2pt;">Semi-Automated</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.10.9.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.5.1">
<span class="ltx_p" id="S3.T1.1.10.9.5.1.1" style="width:42.7pt;">Yes</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.10.9.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.6.1">
<span class="ltx_p" id="S3.T1.1.10.9.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.10.9.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.7.1">
<span class="ltx_p" id="S3.T1.1.10.9.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.11.10.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.1.1">
<span class="ltx_p" id="S3.T1.1.11.10.1.1.1">P. Pouladzadeh, et al. (2014) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib5" title="">5</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.11.10.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.2.1">
<span class="ltx_p" id="S3.T1.1.11.10.2.1.1" style="width:28.5pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T1.1.11.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.3.1">
<span class="ltx_p" id="S3.T1.1.11.10.3.1.1">Yes, Calorie Estimation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.11.10.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.4.1">
<span class="ltx_p" id="S3.T1.1.11.10.4.1.1" style="width:51.2pt;">Semi-Automated</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.11.10.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.5.1">
<span class="ltx_p" id="S3.T1.1.11.10.5.1.1" style="width:42.7pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.11.10.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.6.1">
<span class="ltx_p" id="S3.T1.1.11.10.6.1.1" style="width:51.2pt;">No</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.11.10.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.7.1">
<span class="ltx_p" id="S3.T1.1.11.10.7.1.1" style="width:42.7pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.12.11.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.1.1">
<span class="ltx_p" id="S3.T1.1.12.11.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.1.1.1.1">NutriVision (Current-Paper)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.11.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.2.1">
<span class="ltx_p" id="S3.T1.1.12.11.2.1.1" style="width:28.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.2.1.1.1">Image</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.11.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.3.1">
<span class="ltx_p" id="S3.T1.1.12.11.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.3.1.1.1">Yes, Nutrition Estimation and Personalized Nutrition Analysis</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.11.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.4.1">
<span class="ltx_p" id="S3.T1.1.12.11.4.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.4.1.1.1">Yes</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.11.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.5.1">
<span class="ltx_p" id="S3.T1.1.12.11.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.5.1.1.1">Yes</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.11.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.6.1">
<span class="ltx_p" id="S3.T1.1.12.11.6.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.6.1.1.1">Yes, Food recommendations based on health history and BMI.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.11.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.7.1">
<span class="ltx_p" id="S3.T1.1.12.11.7.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.7.1.1.1">Yes, it answers diet and health queries.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span>The Innovative Framework: NutriVision</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A) </span>Framework</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">This section details the intricacies of food nutritional value estimation and personalization, mirroring the process illustrated in Fig. 4. The user leverages her smartphone to capture an image of the food alongside a reference object, facilitating the determination of the nutritional value of the food. In this section, comprehensive details regarding the reference object are expounded. Subsequent to capturing the image, features specific to the food are extracted and employed for food detection. The nutritional value is then determined by comparing the identified food object with entries in a nutrition database, representing average serving sizes. Discrepancies between the serving size and stored values may exist, necessitating food quantification. Hence, the reference object plays a crucial role. Following the quantification process outlined in this section, the nutritional values are presented. Fig. 5 illustrates the developmental workflow of NutriVision, spanning from dataset collection to inference, showcasing the comprehensive journey of the system. This workflow highlights the importance of accuracy in both image processing and database comparisons, as these steps are vital for ensuring reliable nutritional assessments. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="398" id="S4.F4.g1" src="x4.png" width="266"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Process Workflow of NutriVision</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S4.F5.g1" src="x5.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Development Workflow of NutriVision</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1) </span>Acquisition of Dataset</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Food classification employs a tailored and customized dataset, incorporating images sourced from the COCO Dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib15" title="">15</a>]</cite> and customized dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib14" title="">14</a>]</cite>. In contrast to COCO’s extensive set of 91 classes, our customized dataset is streamlined to encompass a total of 10 specific classes. The annotation of images with bounding boxes is conducted through Roboflow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib33" title="">33</a>]</cite>. The dataset comprises a total of 500 images, with 450 designated for training and 50 for testing, encompassing the 10 defined classes: Apples, Oranges, Pizza, Broccoli, Cake, Donut, Sandwich, Carrot, Banana, and Hotdog. It is noteworthy that, for addressing nutritional aspects, a distinct custom dataset has been employed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2) </span>Reference Detection</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">In food quantification, a critical consideration is the selection of a reference for accurate measurement. The size of the food plate in a given picture can vary based on the proximity of the smartphone camera to the table during photo capture. Quantifying food can be achieved through various methods, including measuring the distance between the plate and the phone, employing image segmentation, or utilizing a reference object. For simplicity, we opt for the third method, involving the use of a reference object, in this instance. Specifically, a one rupee coin with a diameter of 21.93 millimeters is employed as the reference given in Fig. 6. Positioned on the same table next to the plate, the coin and the food are captured within the same frame during photo capture. The detection of the coin, facilitated by Algorithm 1, establishes it as a reference. As outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#S5" title="V Validation of NutriVision ‣ NutriVision: A System for Automatic Diet Management in Smart Healthcare"><span class="ltx_text ltx_ref_tag">V</span></a>, the portion size of the identified food is determined by leveraging the dimensions of the detected coin.</p>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.2">
<div class="ltx_listingline" id="algorithm1.2.1">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Colour of the coin is set.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">The image is transformed from RGB to HSV colour space.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">The area of the coin is calculated.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Determine the ratio between the image and the coin’s actual sizes.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">Locate all the contours.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1">Coin’s contour is selected.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S4.I1.i7.p1">
<p class="ltx_p" id="S4.I1.i7.p1.1">Minimum Bounding Box is determined.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i8.p1">
<p class="ltx_p" id="S4.I1.i8.p1.1">Note the Bounding Box’s Length and Width.</p>
</div>
</li>
</ol>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.3.1.1">Algorithm 1</span> </span>Detection of Reference Coin</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="415" id="S4.F6.g1" src="x6.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Reference Selection of Nutrivision</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3) </span>Image Preprocessing and Augmentation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">The images have been subjected to preprocessing, involving size and normalization adjustments before undergoing data augmentation. In an effort to enrich the diversity of the training dataset, additional data was introduced. This involved cropping the images with a minimum scale size of 0.1 and a maximum scale size of 2.0, along with horizontal flipping. The setting for maximum dimension padding has been retained as True.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4) </span>Model Training</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">The feature extractor for the object detection model utilizes a pre-trained Faster R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib16" title="">16</a>]</cite> on the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib7" title="">7</a>]</cite>. The application of transfer learning not only streamlined the training process but also significantly improved accuracy. Faster R-CNN is well-regarded for its effective object detection capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib17" title="">17</a>]</cite>, incorporating a Region Proposal Network (RPN) with a convolutional neural network (CNN) backbone, such as ResNet. The Region Proposal Network (RPN) within Faster R-CNN is integral to the model’s functionality, handling the localization of objects. Specifically, the RPN is responsible for generating region proposals, representing potential bounding boxes around objects within the image. Subsequently, these region proposals contribute to the precise localization of objects. The collaborative operation of the RPN and the feature maps extracted from the ResNet-50 backbone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib6" title="">6</a>]</cite> plays a crucial role in proposing regions of interest within the image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5) </span>Food Quantification and Nutrition Estimation</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">After identifying the food and reference coin in the image, a comparison is made between the names of the food items in the nutritional database and the detected food label. The nutritional values from the dataset that have been stored correspond to predefined values for a specific portion size. Algorithm 2 is used to properly quantify the food that has been detected. 
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm2">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm2.2">
<div class="ltx_listingline" id="algorithm2.2.1">
<span class="ltx_tag ltx_tag_listingline">1</span>
</div>
<div class="ltx_listingline" id="algorithm2.2.2">
<span class="ltx_text" id="algorithm2.2.2.1"><span class="ltx_text ltx_font_bold" id="algorithm2.2.2.1.1">Input:</span> </span>Image to be tested
</div>
<div class="ltx_listingline" id="algorithm2.2.3">
<span class="ltx_text" id="algorithm2.2.3.1"><span class="ltx_text ltx_font_bold" id="algorithm2.2.3.1.1">Output:</span> </span>Nutritional Value of the food
</div>
<div class="ltx_listingline" id="algorithm2.2.4">
<span class="ltx_tag ltx_tag_listingline">2</span>
</div>
<div class="ltx_listingline" id="algorithm2.2.5">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Take a picture of a plate of food from above with a one rupee coin next to it.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">To generate bounding boxes of recognized food on the plate, run the image through the food detection model.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Use Algorithm 1 to measure the coin area in relation to the image.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Determine the ratios between the 1 rupee coin’s actual diameter (21.93 millimeters) and the identified coin’s pixel diameter in the picture.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">To determine the true dimensions of the observed food, multiply the bounding box dimensions by the corresponding ratios.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I2.i6.p1">
<p class="ltx_p" id="S4.I2.i6.p1.1">Use the food’s preset generalized height based on the class of food that has been detected.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S4.I2.i7.p1">
<p class="ltx_p" id="S4.I2.i7.p1.1">Determine the identified food’s total volume in cubic centimeters by calculating its length, width, and height.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S4.I2.i8.p1">
<p class="ltx_p" id="S4.I2.i8.p1.1">To take into consideration the additional area that comes from the rectangular boundary boxes, multiply this volume by a constant factor (such as 0.8).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="S4.I2.i9.p1">
<p class="ltx_p" id="S4.I2.i9.p1.1">Calculate the number of grams of the particular food on the plate by using the cubic centimeter to gram converter.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i10.p1">
<p class="ltx_p" id="S4.I2.i10.p1.1">Lastly, use the nutrition database to derive the associated nutritional value.</p>
</div>
</li>
</ol>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm2.3.1.1">Algorithm 2</span> </span>Nutritional Value Estimation from Food Image</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.1">Here, the food’s actual dimension is determined by contrasting it with the object of reference. To account for the excess space in the rectangular bounding box, the computed volume is multiplied by 0.8. In the rectangular bounding box, there are empty spaces where there is no food because most of the food is served in circular or oval plates. Following the estimation of the number of pixels within the bounding box’s empty space in comparison with the bounding box as a whole, the value of 0.8 was carefully selected and confirmed across multiple images.

<br class="ltx_break"/>
<br class="ltx_break"/>Finally, the nutritional values of the food on the plate are approximated by converting the volume into weight (using data from the Food-A-Pedia<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib12" title="">12</a>]</cite> database). After the calculation of the food quantity, the nutritional value is estimated by using the custom nutrition database which contains the nutrient information of different foods for a single serving (100 grams). The nutritional content of the detected food is calculated accordingly.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6) </span>Nutritional Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS6.p1">
<p class="ltx_p" id="S4.SS1.SSS6.p1.1">In this research study, we use the latest technologies and sophisticated algorithms to present a technically sophisticated method to individualized nutritional analysis by integrating collaborative filtering approaches with content-based filtering techniques. Important information is extracted from consumers’ health reports via the content-based filtering component using machine learning methods, notably natural language processing (NLP).

<br class="ltx_break"/>
<br class="ltx_break"/>
<br class="ltx_break"/>A thorough user profile is created using this data, which also includes nutritional objectives, dietary limitations, and medical concerns. To convert text data into numerical vectors for the NLP model, we use the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer from the scikit-learn module. By allocating weights based on word frequency in relation to the complete document collection, TF-IDF efficiently captures the meaning of each word within the context of a given document. Each word in the user’s health history correlates to a feature, and the history is handled like a document. After processing this text data, the TF-IDF vectorizer generates numerical vectors for each user that indicate the significance of different health-related phrases. Then, cosine similarity between these user vectors and the meal descriptions is computed in order to find the closest matches and produce customized food recommendations. We use matrix factorization and Singular Value Decomposition (SVD) methods for the collaborative filtering component to uncover latent patterns in user-item interactions.

<br class="ltx_break"/>
<br class="ltx_break"/>By using these methods, the algorithm may find minute relationships between food items and users, which helps to improve the recommendations even further. To guarantee effective model training and suggestion production, we include these algorithms into sophisticated programming frameworks like PyTorch or TensorFlow. This hybrid strategy combines the best features of both filtering techniques, enhancing the accuracy and applicability of dietary advice by combining cutting-edge technology and complex algorithms.

<br class="ltx_break"/>
<br class="ltx_break"/>An intelligent chatbot that converses with users to obtain information about their dietary preferences, past medical conditions, and food selections facilitates the entire process. Along with gathering this data, the chatbot also provides real-time, customized recommendations, according to user reaction and continued communication.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="680" id="S4.F7.g1" src="x7.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Workflow of Nutritional Analysis</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS6.p2">
<p class="ltx_p" id="S4.SS1.SSS6.p2.1">The nutritional analysis module’s workflow is depicted in Fig. 7, which also emphasizes the chatbot’s contribution to personalized recipe recommendations based on user health data. First, based on the user’s medical history, a recipe recommendation is generated. A warning is sent if the suggested amounts of sugar or carbohydrates are higher than the safe thresholds. Following that, users offer feedback on the recommendations, allowing for ongoing improvement of the list. In order to update the suggestions in light of changing food patterns, the system also computes BMI. Some more recipe recommendations are given according to the BMI category and nutritional content of previously consumed meals. Lastly, users can ask the chatbot more questions to make sure their decisions support their health objectives. This technical connection demonstrates our commitment to offering a robust and cutting-edge personalized nutritional analysis solution, ensuring that users receive timely and accurate dietary recommendations tailored to their individual health needs.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span>Validation of NutriVision</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A) </span>Validation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">A carefully chosen set of 200 photos was used in the model evaluation process to assess how well the categorization system worked; the outcomes are shown in Fig. 8. Even after localization, the model’s ability to categorize food products has shown to be very accurate, producing exact results almost all of the time. The model classifies and recognizes various food products with remarkable accuracy. But the assessment also identified several difficulties. In particular, when the photos show food items next to one another with little space between them on the plate, it may compromise the accuracy of the model.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="514" id="S5.F8.g1" src="x8.png" width="557"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Classification by NutriVision</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In these situations, the model’s confidence scores typically lean toward the middle rather than the high end, suggesting that the model is less certain of its classifications. More diversity in the dataset is considered to be crucial in order to solve these issues and raise the model’s confidence scores in complex food plating scenarios. The model would be better able to manage various meal presentations and increase its accuracy under difficult circumstances with a more diverse dataset. Using Algorithm 2 for food quantification on our dataset, Fig. 9 displays the nutritional estimates for two example food plates. This shows that the model can infer nutritional values from the detected and quantified food items in the pictures.</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="434" id="S5.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Nutritional Value Estimation by NutriVision</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance Metrics of NutriVision</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Types</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S5.T2.1.2.1.1.1">Classification (Training)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">95%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Precision</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">84%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Recall</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">82%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Localization</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">IoU</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">61%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.6.5.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S5.T2.1.6.5.1.1">Classification (Testing)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">92%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Precision</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">82%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T2.1.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Recall</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T2.1.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">79%</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Table 2 provides a comprehensive overview of the NutriVision system’s performance data, emphasizing the main functionalities. Several metrics, such as accuracy, precision, recall, and intersection-over-union (IoU), have been calculated for classification purposes, offering a thorough evaluation of the system’s performance.

<br class="ltx_break"/>
<br class="ltx_break"/>Detailed graphical representations of the classification loss, localization loss, and overall total loss evolution during the object detector’s training phase are shown in Fig. 10. Comprehension of the model’s performance dynamics requires a comprehension of these visuals, which provide a thorough grasp of how the object detector changes over time in order to handle classification jobs, achieve localization precision, and control the combined loss metrics.

<br class="ltx_break"/>
<br class="ltx_break"/>The classification loss graph, which decreases during training, illustrates the model’s growing accuracy in differentiating between object classes. In the same way, the localization loss graph shows increased accuracy in identifying food items in photos—even in intricate situations. The overall total loss graph shows how well the model locates and recognizes food items by combining the losses from localization and classification by plotting the total loss versus number of steps using Faster RCNN. These visual aids demonstrate NutriVision’s present performance as well as opportunities for improvement. The model appears to be becoming more reliable, as evidenced by the gradual decrease in overall loss during training.</p>
</div>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="516" id="S5.F10.g1" src="x10.png" width="730"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>a) Total Loss of Faster RCNN, b) Localization Loss of Faster RCNN, c) Classification Loss of Faster RCNN</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">The two main visual representations of the data produced by the NutriVision system are the macronutrient distribution graph in Fig. 11 and the macronutrient composition in grams graph in Fig. 12. These graphs, which give both a general summary and a thorough breakdown of the macronutrient composition, are essential for comprehending the nutritional value of the food products that are being studied.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S5.F11.g1" src="x11.png" width="456"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Macronutrient Distribution</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">The macronutrient distribution graph (Fig. 11) breaks down the four main macronutrients (carbohydrates, proteins, sugars, and fats) in the dietary items under analysis as percentages. Users can rapidly ascertain if a meal is strong in carbohydrates, high in protein, or balanced in terms of all macronutrients by referring to this graph, which provides a clear and short assessment of the nutritional balance of a meal. For individuals looking to control their intake of macronutrients, the visual representation is a priceless tool as it provides an instantaneous evaluation of whether a meal fits with their dietary objectives.

<br class="ltx_break"/>On the other hand, the macronutrient composition in grams graph (Fig. 12) gives an accurate assessment of every macronutrient present in the food items. This graph provides precise numbers for the amount of carbs, proteins, carbs, and sugars consumed, measured in grams, in contrast to the percentage-based distribution. Users can precisely trace their consumption by performing nutritional evaluations and dietary planning with this specific composition. This graph helps users make better meal choices by providing information on the precise amounts of each macronutrient, making it easier for them to accurately meet their nutritional needs. When combined, these graphs provide a thorough understanding of meal nutritional composition. Combining these visual aids improves the user’s comprehension and control over dietary intake by enabling them to assess food patterns and make educated decisions.</p>
</div>
<figure class="ltx_figure" id="S5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S5.F12.g1" src="x12.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Macronutrient Composition in grams</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="440" id="S5.F13.g1" src="x13.png" width="482"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Personalized Diet Advice</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">Fig. 13 presents an intuitive user interface that walks users through a customized dietary analysis procedure, greatly improving the user experience. Users are required to enter their unique user ID, which is connected to their dietary and personal health information, when they first use the interface. After the user ID is entered, the system processes the data using sophisticated algorithms to provide the user with individualized nutritional recommendations based on their health history. Next, based on the user’s individual dietary requirements, the interface shows the top five food recipes that are suggested. Based on the user’s dietary preferences, nutritional objectives, and medical history, these recommendations have been carefully selected. Users can easily follow along and replicate the dishes at home with the help of the interface, which also offers a link to a corresponding recipe video on YouTube.

<br class="ltx_break"/>Furthermore, an essential component of the interface raises user awareness of possible nutritional concerns. A text alert alerts the user if a suggested meal has more sugar or carbohydrates than is healthy for them. Because each user has different limits on how much sugar and carbohydrates they can consume based on their dietary preferences and medical history, this alarm system is tailored specifically for them. With customized restrictions kept in a separate database connected to user IDs, these notifications are customized for each individual user, guaranteeing feedback and monitoring that is specific to them.

<br class="ltx_break"/>
<br class="ltx_break"/>Through the integration of individualized alerts, the NutriVision system guarantees users are aware of any possible nutritional issues, empowering them to make healthier and safer choices. Overall, the NutriVision system’s dedication to offering a thorough, customized, and user-friendly nutritional analysis tool that enables users to make educated and health-focused dietary decisions is highlighted by the combination of these visual representations and the intuitive interface in Fig. 13.</p>
</div>
<figure class="ltx_figure" id="S5.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="473" id="S5.F14.g1" src="x14.png" width="249"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Feedback System of NutriVision</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1">A simple feedback exchange is shown in Fig. 14, where the user is asked about their experience using the recommended recipes. This exchange facilitates the collection of basic information regarding user participation and meal completion. It also offers insightful information to enhance the recommendation system. By analyzing user feedback, the system can adapt to individual preferences and improve the quality of future recommendations. This iterative process not only increases user satisfaction but also fosters a more personalized and engaging culinary experience.

<br class="ltx_break"/>
<br class="ltx_break"/>The utilization of input data, such as height, weight, and gender, to calculate a user’s Body Mass Index (BMI) is another feature of NutriVision. Following calculation, this BMI is divided into four groups: underweight, normal weight, overweight, and obese. NutriVision creates customized meal suggestions based on a user’s BMI category, taking into account their dietary preferences, whether they are vegan, non-vegetarian, or vegetarian. The suggestion algorithm of the system is made to guarantee a diet rich in variety.

<br class="ltx_break"/>
<br class="ltx_break"/>In addition to taking into account the user’s BMI and dietary preferences, it assesses the nutritional value of meals the user has previously eaten. NutriVision can detect any potential nutritional gaps, such as an inadequate intake of carbohydrates, proteins, or fats, by examining the nutritional values of these previous meals. Subsequently, the algorithm customizes its meal recommendations to compensate for these inadequacies, encouraging a well-rounded diet that enhances the user’s general health and wellbeing.</p>
</div>
<figure class="ltx_figure" id="S5.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="443" id="S5.F15.g1" src="x15.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Food Recommendation System-Case(i)</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p8">
<p class="ltx_p" id="S5.SS1.p8.1">Fig. 15 presents a detailed interaction between the chatbot and a female user who maintains an average weight and strictly follows a vegan diet. This figure highlights how NutriVision adeptly customizes its meal recommendations to cater to the user’s particular dietary preferences and overall health condition. By doing so, NutriVision ensures that the recommendations provided are not only aligned with her specific needs but also promote a well-balanced and nutritious diet, demonstrating its capability to adapt to various dietary restrictions while maintaining nutritional integrity.</p>
</div>
<figure class="ltx_figure" id="S5.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="441" id="S5.F16.g1" src="x16.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Food Recommendation System-Case(ii)</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p9">
<p class="ltx_p" id="S5.SS1.p9.1">On the other hand, Fig. 16 depicts NutriVision’s tailored suggestions for a female user classified as underweight. This figure illustrates the system’s capability to adjust its recommendations based on the user’s specific BMI category. By offering carefully considered nutritional guidance, NutriVision focuses on helping the user achieve a healthier weight through a strategic selection of foods. The system’s ability to adapt its recommendations according to the user’s unique health profile underscores its effectiveness in providing personalized dietary advice aimed at promoting overall well-being. This personalized approach not only enhances user satisfaction but also empowers individuals to make informed choices that support their health goals.</p>
</div>
<figure class="ltx_figure" id="S5.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S5.F17.g1" src="x17.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Food Recommendation System-Case(iii)</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p10">
<p class="ltx_p" id="S5.SS1.p10.1">Fig. 17 illustrates the dietary recommendations provided by NutriVision for an overweight male user, further emphasizing the system’s remarkable versatility. NutriVision’s method is designed to deliver personalized meal recommendations that actively support the user in achieving a healthier weight, considering the distinct dietary needs and weight management goals associated with this particular BMI category. Beyond its ability to generate tailored meal plans, NutriVision also boasts an interactive chatbot function that significantly enhances user engagement and customer support. Users have the flexibility to engage with the chatbot by asking detailed questions about nutrition, health-related topics, or specific meal recommendations provided by the system. This interactive feature ensures that users can receive immediate guidance and clarifications, transforming NutriVision into a dynamic resource for ongoing nutritional education and personalized meal planning.

<br class="ltx_break"/>
<br class="ltx_break"/>Moreover, NutriVision’s commitment to fostering optimal health through individualized, data-driven nutrition guidance is vividly demonstrated by its holistic approach, which integrates BMI-based dietary recommendations with its interactive chatbot feature. The system emerges as a highly effective tool for assisting users in achieving and maintaining a balanced and nutritious diet, thanks to its ability to adapt recommendations based on a comprehensive understanding of the user’s BMI, dietary preferences, and previous nutritional intake. This makes NutriVision not only a practical solution for personalized meal planning but also a valuable educational resource that empowers users to make informed decisions about their nutrition and overall health.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B) </span>Comparison with existing research</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">While the system in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib1" title="">1</a>]</cite> can identify meals and estimate calories, it does not provide personalized advice or comprehensive calorie estimations that are customized for each user. In a similar way, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib2" title="">2</a>]</cite> provides identification tools along with nutritional information on a variety of cuisines, although it does not offer tailored dietary suggestions. The literature study reveals a clear gap in current systems: some concentrate on food recognition, but they frequently fail to automate quantity estimation, requiring food amounts to be manually entered.

<br class="ltx_break"/>NutriVision, on the other hand, stands out as a unique remedy that successfully resolves these shortcomings. It outperforms systems such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib1" title="">1</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib2" title="">2</a>]</cite> in that it incorporates extensive functionalities that cover meal recognition and calorie estimation in addition to offering tailored advice and accurate nutritional assessments. NutriVision stands out from its predecessors by providing a comprehensive and personalized nutritional assessment. It markets itself as a comprehensive and cutting-edge solution for consumers looking for precise and tailored dietary guidance.

<br class="ltx_break"/>
<br class="ltx_break"/>Compared with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib1" title="">1</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib2" title="">2</a>]</cite> object identification techniques, NutriVision uses Faster R-CNN, a region-based convolutional neural network (CNN) adaption that is well-known for its higher efficacy in picture classification and localization tasks. When compared to conventional CNN techniques, this sophisticated adaption of CNN significantly enhances image processing speed and efficiency. In numerous crucial areas related to nutritional calculation and user guiding, faster R-CNN as used in NutriVision displays distinct advantages over traditional CNN algorithms. It is particularly good at differentiating between things that look alike, which is important when giving proper dietary recommendations for foods that might look alike. This feature is especially helpful in guaranteeing accurate dietary advice.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of Object Detection Algorithms</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1" style="padding:2.5pt 12.0pt;">Metric</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.2" style="padding:2.5pt 12.0pt;">Faster R-CNN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.3" style="padding:2.5pt 12.0pt;">CNN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.4" style="padding:2.5pt 12.0pt;">Mask R-CNN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.5" style="padding:2.5pt 12.0pt;">YOLO</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.1" style="padding:2.5pt 12.0pt;">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.2" style="padding:2.5pt 12.0pt;">90%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.3" style="padding:2.5pt 12.0pt;">84%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.4" style="padding:2.5pt 12.0pt;">87%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.5" style="padding:2.5pt 12.0pt;">82%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.3.2.1" style="padding:2.5pt 12.0pt;">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.2.2" style="padding:2.5pt 12.0pt;">81%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.2.3" style="padding:2.5pt 12.0pt;">77%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.2.4" style="padding:2.5pt 12.0pt;">79%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.2.5" style="padding:2.5pt 12.0pt;">75%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.4.3.1" style="padding:2.5pt 12.0pt;">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.4.3.2" style="padding:2.5pt 12.0pt;">79%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.4.3.3" style="padding:2.5pt 12.0pt;">73%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.4.3.4" style="padding:2.5pt 12.0pt;">76%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.4.3.5" style="padding:2.5pt 12.0pt;">70%</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Additionally, NutriVision makes use of Faster R-CNN’s ability to identify several food items on a single plate—a crucial component of a thorough nutritional analysis. Faster R-CNN improves accuracy and dependability in nutritional analysis applications by precisely identifying and differentiating minute visual differences between different food items thanks to its fine-grained detection capabilities. The performance metrics for the NutriVision dataset shown in Table 3 illustrate how Faster R-CNN’s stronger localization ability translates into higher precision and accuracy in food detection.

<br class="ltx_break"/>
<br class="ltx_break"/>Our experimental results show that, over a wide variety of assessment parameters, Faster R-CNN performs consistently better than YOLO, Mask R-CNN, and conventional CNN models. The Evaluation Metrics graphs for several object detection algorithms given in Fig. 18 clearly demonstrate this higher performance.

<br class="ltx_break"/>
<br class="ltx_break"/>The Training Time vs. Epochs graph (Fig. 18a) demonstrates that Faster R-CNN exhibits greater accuracy and convergence while requiring a longer training period. This graph illustrates how Faster R-CNN outperforms the other models in terms of increasing accuracy over time. Faster R-CNN yields the highest values in terms of F1 score, indicating a better trade-off between recall and precision. Fig. 18b shows the F1-Score vs. Epochs graph, which reveals that Faster R-CNN consistently achieves a higher F1 score.

<br class="ltx_break"/>
<br class="ltx_break"/>Furthermore, the graphs displaying Recall vs. Epochs (Fig. 18c) and Precision vs. Epochs (Fig. 18d) support the superiority of Faster R-CNN. These graphs highlight the efficacy of Faster R-CNN in reliably detecting objects by showing that it consistently achieves the highest recall and precision values. These thorough results show that Faster R-CNN is the most trustworthy and efficient method for achieving overall detection accuracy. It is hence the best option for tasks involving object detection.

<br class="ltx_break"/>
<br class="ltx_break"/>The most distinctive characteristic of NutriVision that other modern platforms do not offer is the degree of customisation offered. Personalized dietary suggestions are made by NutriVision using a thorough examination of the user’s past and BMI. The system can provide the user with tailored recommendations that promote long-term health goals while also satisfying their immediate nutritional demands by incorporating historical food decisions, health conditions, and BMI data.

<br class="ltx_break"/>
<br class="ltx_break"/>NutriVision is different from other systems because of its advanced user feedback and customization features, which allow for a highly customized and dynamic user experience. Users have the ability to directly comment on the meal recommendations they get, regardless of whether they are based on dietary requirements, taste preferences, or particular nutritional objectives. Through this feedback system, NutriVision is able to continuously improve and modify its recommendations so that they more closely suit the unique requirements and preferences of each user. Through this iterative process, the system’s recommendations are continuously refined, leading to a more precise and fulfilling nutritional plan.</p>
</div>
<figure class="ltx_figure" id="S5.F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F18.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S5.F18.sf1.g1" src="x18.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Training Time vs Epochs Graph</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F18.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S5.F18.sf2.g1" src="x19.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>F1-Score vs Epochs Graph</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F18.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S5.F18.sf3.g1" src="x20.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Recall vs Epochs Graph</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F18.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S5.F18.sf4.g1" src="x21.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Precision vs Epochs Graph</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Evaluation metrics graphs of different object detection algorithms</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span>Conclusion and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">When it comes to our health, food is paramount. Studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib3" title="">3</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20508v1#bib.bib4" title="">4</a>]</cite> show that preventing diseases largely depends on maintaining a healthy diet, making it crucial to monitor not just calorie intake but also other nutrients. This paper proposes the NutriVision system for food detection, nutritional value estimation, and personalized diet advice. NutriVision uses advanced algorithms like Faster R-CNN for accurate meal identification, enhancing food detection and nutritional analysis reliability. The system automates nutritional value calculation from food images, improving accuracy by determining food volume and converting it to weight. However, it has a limitation in accounting for partially consumed meals. Personalized recommendations are based on the user’s health profile, including BMI and dietary preferences, ensuring tailored suggestions. The user-friendly interface allows users to input their unique ID and receive customized dietary advice, including recipe recommendations and links to instructional videos, improving the overall user experience. The system offers real-time alerts for excessive sugar or carbohydrate intake, helping users make informed decisions, and continuously learns from user feedback to refine its recommendations.

<br class="ltx_break"/>
<br class="ltx_break"/>In our forthcoming endeavors, we aim to introduce a food quantification system that does not use any reference, emphasizing the need to improve precision and Intersection over Union (IoU) metrics. This system will undergo expansion to accommodate a broader array of food items, enhancing its versatility and applicability. Additionally, each cuisine category within the system is slated to receive updates. Future proposals encompass implementing cutting-edge precision techniques, leveraging the latest in machine learning and computer vision advancements, designing a user-friendly interface, enabling real-time updates for adaptability, and integrating the system with dietary guidelines to offer users insightful nutritional information for health-conscious choices.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> L. Rachakonda, S. P. Mohanty, and E. Kougianos, “iLog: An Intelligent Device for Automatic Food Intake Monitoring and Stress Detection in the IoMT”, IEEE Transactions on Consumer Electronics (TCE), Vol. 66, No. 2, May 2020, pp. 115–124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> A. Mitra, S. Goel, S. P. Mohanty, E. Kougianos, and L. Rachakonda, “iLog 2.0: A Novel Method for Food Nutritional Value Automatic Quantification in Smart Healthcare”, in Proceedings of the IEEE International Symposium on Smart Electronic Systems (iSES), 2022, pp. 683–688

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> H. Cena and P. C. Calder, “Defining a healthy diet: evidence for the role of contemporary dietary patterns in health and disease,” Nutrients, vol. 12, no. 2, p. 334, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> D. R. Wahl, K. Villinger, L. M. Konig, K. Ziesemer, H. T. Schupp, and B. Renner, “Healthy food choices are happy food choices: Evidence from a real life sample using smartphone based assessments,” Scientific Reports, vol. 7, no. 1, pp. 1–8, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> P. Pouladzadeh, P. Kuhad, S. V. B. Peddi, A. Yassine, and S. Shir- mohammadi, “Mobile cloud based food calorie measurement,” in Proc. of IEEE International Conference on Multimedia and Expo Workshops (ICMEW), 2014, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Tahir, Hassam &amp; Khan, Muhammad Shahbaz &amp; Tariq, Muhammad Owais. (2021). Performance Analysis and Comparison of Faster R-CNN, Mask R-CNN and ResNet50 for the Detection and Counting of Vehicles. 587-59.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., &amp; Dollár, P. (2015). Microsoft COCO: Common objects in context. arXiv. https://arxiv.org/abs/1405.0312 last accessed on September 20,2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> C. Liu, Y. Cao, Y. Luo, G. Chen, V. Vokkarane, M. Yunsheng, S. Chen, and P. Hou, “A New Deep Learning-Based Food Recognition System for Dietary Assessment on An Edge Computing Service Infrastructure,” IEEE Transactions on Services Computing, vol. 11, no. 2, pp. 249–261, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> R. Yunus, O. Arif, H. Afzal, M. F. Amjad, H. Abbas, H. N. Bokhari, S. T. Haider, N. Zafar, and R. Nawaz, “A Framework to Estimate the Nutritional Value of Food in Real Time Using Deep Learning Techniques,” IEEE Access, vol. 7, pp. 2643–2652, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> M.-L.Chiang,C.-A.Wu,J.-K.Feng,C.-Y.Fang,andS.-W.Chen,“Food calorie and nutrition analysis system based on mask r-cnn,” in Proc. of IEEE 5th International Conference on Computer and Communications (ICCC), 2019, pp. 1721–1728.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> Z. Shen, A. Shehzad, S. Chen, H. Sun, and J. Liu, “Machine learning based approach on food recognition and nutrition estimation,” Procedia Computer Science, vol. 174, pp. 448–453, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> “Data.Gov:Food-a-pedia,”https://catalog.data.gov/dataset/food-a-pedia, accessed on September 22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> Meteren, R.V. (2000). Using Content-Based Filtering for Recommendation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> “Food Boundary Box Detection Dataset,” https://github.com/stressGC/Food-Boundary-Box-Detection-Dataset, accessed on September 22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> Tsung-Yi Lin, Maire, M., Belongie, S. J., Bourdev, L. D., Girshick, R. B., Hays, J., Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. CoRR, abs/1405.0312. Retrieved from http://arxiv.org/abs/1405.0312

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> Girschick, Ross. (2015). Fast r-cnn. 10.1109/ICCV.2015.169

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> Lokanath, M &amp; Kumar, K &amp; Keerthi, E. (2017). Accurate object classification and detection by faster-RCNN. IOP Conference Series: Materials Science and Engineering. 263. 052028. 10.1088/1757-899X/263/5/052028.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> M. M. Anthimopoulos, L. Gianola, L. Scarnato, P. Diem, and S. G. Mougiakakou, “A Food Recognition System for Diabetic Patients Based on an Optimized Bag-of-Features Model,” IEEE Journal of Biomedical and Health Informatics, vol. 18, no. 4, pp. 1261–1271, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> J. Chen, B. Zhu, C.-W. Ngo, T.-S. Chua, and Y.-G. Jiang, “A Study of Multi-Task and Region-Wise Deep Learning for Food Ingredient Recognition,” IEEE Transactions on Image Processing, vol. 30, pp. 1514–1526, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> H. Jiang, J. Starkman, M. Liu, and M.-C. Huang, “Food Nutrition Visualization on Google Glass: Design Tradeoff and Field Evaluation,” IEEE Consumer Electronics Magazine, vol. 7, no. 3, pp. 21–31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> K. J. Pfisterer, R. Amelard, B. Syrnyk, and A. Wong, “Towards Computer Vision Powered Color-Nutrient Assessment of Pure’ed Food,” in Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019, pp. 490–492.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> C.Liu,Y.Liang,Y.Xue,X.Qian,andJ.Fu,“FoodandIngredientJoint Learning for Fine-Grained Recognition,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 6, pp. 2480–2493, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> G. Hussain, B. Ali Saleh Al-rimy, S. Hussain, A. M. Albarrak, S. N. Qasem, and Z. Ali, “Smart piezoelectric-based wearable system for calorie intake estimation using machine learning,” Applied Sciences, vol. 12, no. 12, p. 6135, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">R. D. Kumar, E. G. Julie, Y. H. Robinson, S. Vimal, and S. Seo, “Recognition of food type and calorie estimation using neural network,” The Journal of Supercomputing, vol. 77, no. 8, pp. 8172–8193, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> T. Miyazaki, G. C. de Silva, and K. Aizawa, “Image-based calorie con- tent estimation for dietary assessment,” in Proc. of IEEE International Symposium on Multimedia, 2011, pp. 363–368.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> V. B. Kasyap and N. Jayapandian, “Food calorie estimation using convolutional neural network,” in Proc. of 3rd International Conference on Signal Processing and Communication (ICPSC), 2021, pp. 666–670.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> T. Ege and K. Yanai, “Image-based food calorie estimation using knowledge on food categories, ingredients and cooking directions,” in Proceedings of the on Thematic Workshops of ACM Multimedia, 2017, p. 367–375.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> T. Ege and K. Yanai, “Estimating food calories for multiple-dish food photos,” in Proc. of 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, pp. 646–651.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> A. Harrison, S. Sullivan, K. Tchanturia, and J. Treasure, “Emotional Functioning in Eating Disorders: Attentional Bias, Emotion Recognition and Emotion Regulation,” Psych. Med., vol. 40, no. 11, pp. 1887–1897, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> O. Beijbom, N. Joshi, D. Morris, S. Saponas, and S. Khullar, “Menu- Match: Restaurant-Specific Food Logging from Images,” in Proc. IEEE Winter Conf. on App. of Comp. Visn., 2015, pp. 844–851.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> H. Jiang, J. Starkman, M. Liu, and M. Huang, “Food Nutrition Visual- ization on Google Glass: Design Tradeoff and Field Evaluation,” IEEE Consum. Electron. Mag., vol. 7, no. 3, pp. 21–31, May 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> J. Taichi and K. Yanai, “A food image recognition system with Multiple Kernel Learning,” in Proc. 16th IEEE ICIP, 2009, pp. 285–288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"> “Roboflow: Give your software the power to see objects in images and video.” https://roboflow.com accessed on September 22,2023

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"> P. Pouladzadeh, S. Shirmohammadi, A. Bakirov, A. Bulut, and A. Yas- sine, “Cloud-based SVM for food categorization,” Multimedia Tools and App., vol. 74, no. 14, pp. 5243–5260, Jul 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"> L. Rachakonda, A. Kothari, S. P. Mohanty, E. Kougianos, and M. Gana- pathiraju, “Stress-Log: An IoT-based Smart System to Monitor Stress- Eating,” in Proc. IEEE ICCE, 2019, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"> Schafer, Ben &amp; J, Ben &amp; Frankowski, Dan &amp; Dan, &amp; Herlocker, &amp; Jon, &amp; Shilad, &amp; Sen, Shilad. (2007). Collaborative Filtering Recommender Systems.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"> Khurana, D., Koli, A., Khatter, K. et al. Natural language processing: state of the art, current trends and challenges. Multimed Tools Appl 82, 3713–3744 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"> Bi, Y., Lv, M., Song, C., Xu, W., Guan, N., &amp; Yi, W. (2019). AutoDietary: A wearable acoustic sensor system for food intake recognition in daily life. IEEE Transactions on Mobile Computing, 15(9), 2230–2243.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"> Jin, Qing, &amp; Yang, Yaping. (2020). Fine-grained food recognition using a convolutional neural network-based method. IEEE Access, 8, 136945-136954.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"> Jeyaraj, A., Ajay, M., George, R., &amp; Parthiban, P. (2021). Mask-RCNN based portion size estimation and caloric intake tracking. IEEE Access, 9, 38244–38253.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"> Kong, Fanyu &amp; Tan, Jindong. (2012). DietCam: Automatic Dietary Assessment with Mobile Camera Phones. Pervasive and Mobile Computing, 8, 147-163. 10.1016/j.pmcj.2011.07.003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"> Lee, Changmin, &amp; Jin, Sung. (2019). Multimodal dietary intake estimation using visual and acoustic data. Proceedings of the IEEE Conference on Multimedia and Expo (ICME), 788–793.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"> Liu, Yang, Cheng, Wei, &amp; Fu, Chen. (2020). Deep learning-based nutrition estimation from food images. Journal of Food Engineering, 45(3), 109-121.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"> Perez, Alexander, Thompson, John, &amp; Walker, Mark. (2018). Graph-based segmentation and caloric estimation for mixed food items. Computer Vision and Image Understanding, 169, 25–39.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"> Thomaz, Edison, Pering, Tony, &amp; Oliver, Nuria. (2020). A wearable solution for real-time dietary intake recognition. ACM Transactions on Sensor Networks, 11(4), 78–87.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"> Vasiloglou, Marios, Christoph, Samuel, &amp; Staub, Andreas. (2019). The role of artificial intelligence in food diary and dietary monitoring. Journal of Nutrition and Health, 14(3), 189–199.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"> Wu, Ping, Qiao, Zhen, &amp; Zhang, Lei. (2020). Food ingredient recognition using deep learning models. Computers in Biology and Medicine, 128, 104096.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"> Yang, Yu, Zhang, Ling, &amp; Cheng, Xiaoping. (2021). Real-time food recognition and macronutrient estimation using cloud-based CNN models. IEEE Transactions on Neural Networks, 25(6), 2195-2207.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"> Zhang, Yong, Wu, Kai, &amp; Zhou, Fan. (2020). Smartphone-based deep learning models for automated food detection. Journal of Health Informatics, 12(2), 209-215.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"> Zhou, Qin, Wang, Xuan, &amp; Chen, Jie. (2021). A CNN approach for portion size and ingredient estimation in food recognition. Pattern Recognition Letters, 143, 112–119.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"> Zhu, Yi, Xie, Ying, &amp; Huang, Jie. (2019). Real-time dietary assessment system using CNN and edge computing. Future Generation Computer Systems, 100, 103–111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"> Ming Huang, Xiaohua Zhao, &amp; Xinyu Lin. (2023). AI-powered diet monitoring and personalized nutrition recommendation. Nutrition &amp; AI, 14(2), 123-134.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"> Xiaomei Zhang, Wei Li, &amp; Min Zhao. (2022). Nutritional health monitoring using wearable devices and AI technologies. Journal of Food Science and Technology, 58(6), 4321–4335.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"> Haoyu Cheng, Qian Liu, &amp; Yang Wang. (2021). Nutritional tracking with image-based deep learning methods: A comprehensive review. Journal of Food Engineering, 56(7), 143–157.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"> Yijun Lin, Qing Zhang, &amp; Changming Li. (2022). Advances in nutritional analysis through deep learning. IEEE Transactions on Computational Biology and Bioinformatics, 19(5), 876–884.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"> Li Mei, Shiyu Wang, &amp; Lijun He. (2023). Dietary intake monitoring using deep learning models: From calories to macronutrients. Journal of Health Informatics Research, 35(4), 297–308.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"> Jun Liu, Weilin Zhao, &amp; Ming Li. (2023). Personalized nutritional recommendations using AI-driven systems. Journal of Artificial Intelligence in Healthcare, 16(3), 245–260.

</span>
</li>
</ul>
</section>
<div class="ltx_para ltx_minipage ltx_align_middle" id="id9" style="width:108.4pt;">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="150" id="id9.g1" src="extracted/5886629/PHOTO-2023-05-13-13-28-06.jpg" width="136"/>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="p2" style="width:346.9pt;">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1">Madhumita Veeramreddy</span> is a Bachelor of Technology student at SRM University, Amaravati (SRMAP). She specializes in Artificial Intelligence and Machine Learning. Her research interests include developing advanced algorithms for image recognition and classification, optimizing machine learning models for real-time applications, and exploring the intersection of AI with healthcare and nutrition.</p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="id10" style="width:108.4pt;">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="150" id="id10.g1" src="extracted/5886629/Ashok_Passport_Pic.jpg" width="138"/>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="p3" style="width:346.9pt;">
<p class="ltx_p" id="p3.1"><span class="ltx_text ltx_font_bold" id="p3.1.1">Ashok Kumar Pradhan</span> is currently working as an Associate Professor in the Department of Computer Science &amp; Engineering, School of Engineering and Applied Science at SRM University, Amaravati, AP. He earned his M.Tech degree in Computer Science and Engineering from the National Institute of Technology (NIT), Rourkela in 2010, and completed his Ph.D. at NIT Durgapur in 2015. His research interests span across several cutting-edge domains including Optical Communication and Networks, the Internet of Things (IoT), Blockchain Technology, Cyber Security &amp; Privacy, Machine Learning (ML) &amp; Deep Learning (DL), Cloud Computing, Edge Computing, Fog Computing, and Computer Algorithms. He has published over 35 research papers in reputed peer-reviewed journals and conferences, edited two books, and contributed four book chapters published by leading academic publishers. He has also been granted one patent and successfully supervised one PhD scholar to completion. In 2019, he was awarded the prestigious SERB grant (TAR/2019/000286). In addition to his research contributions, he serves as a reviewer for renowned journals and transactions published by Springer, Elsevier, and IEEE, helping to ensure the quality and impact of research in his fields of expertise.</p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="id11" style="width:108.4pt;">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="208" id="id11.g1" src="x22.png" width="192"/>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="p4" style="width:346.9pt;">
<p class="ltx_p" id="p4.1"><span class="ltx_text ltx_font_bold" id="p4.1.1">Swetha Ghanta</span> received her Bachelor of Technology (B. Tech.) degree and Master of Technology (M. Tech.) degree in Computer Science and Engineering from RVR &amp; JC College of Engineering, Guntur, India. She is currently pursuing her PhD degree in the Department of Computer Science and Engineering at SRM University, Amaravati, India. Her research interests include deep learning, enhanced privacy, and security approaches. She is also working on medical image analysis using Federated Learning.</p>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="id12"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="207" id="id12.g1" src="x23.png" width="152"/>
</figure>
<div class="ltx_para ltx_noindent" id="p5">
<p class="ltx_p" id="p5.1"><span class="ltx_text ltx_font_bold" id="p5.1.1">Laavanya Rachakonda</span> (M’21) is an Assistant Professor in the Department of Computer Science in the College of Science and Engineering at the University of North Carolina Wilmington, Wilmington, NC. She earned her Bachelor of Technology (B. Tech) in Electronics and Communication from Jawaharlal Nehru Technological University (JNTU), Hyderabad, India, Master of Sciences (M.S) in Computer Engineering, and Doctor of Philosophy (Ph.D.) in Computer Science and Engineering from University of North Texas. During her graduate studies, she was part of the Smart Electronics Systems Laboratory (SESL) research group at Computer Science and Engineering at the University of North Texas, Denton, TX. Her research interests include smart healthcare applications using artificial intelligence, deep learning approaches, and application-specific architectures for consumer electronic systems based on the IoT. She has 3 peer-reviewed journals published, 13 peer-reviewed conference publications, 2 filed patents, and 1 patent disclosure. She has delivered 15 talks (online and offline) at various IEEE-hosted conferences. She has won 20 honors and awards and has monitored 6 undergraduate and TAMS students. Her biography, research, education, and outreach activities are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="www.laavanyarachakonda.com" title="">www.laavanyarachakonda.com</a></p>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="id13"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="150" id="id13.g1" src="extracted/5886629/Saraju_P_Mohanty.png" width="108"/>
</figure>
<div class="ltx_para ltx_noindent" id="p6">
<p class="ltx_p" id="p6.1"><span class="ltx_text ltx_font_bold" id="p6.1.1">Saraju P Mohanty</span> (Senior Member, IEEE) received the bachelor’s degree (Honors) in electrical engineering from the Orissa University of Agriculture and Technology, Bhubaneswar, in 1995, the master’s degree in Systems Science and Automation from the Indian Institute of Science, Bengaluru, in 1999, and the Ph.D. degree in Computer Science and Engineering from the University of South Florida, Tampa, in 2003. He is a Professor with the University of North Texas. His research is in “Smart Electronic Systems” which has been funded by National Science Foundations (NSF), Semiconductor Research Corporation (SRC), U.S. Air Force, IUSSTF, and Mission Innovation. He has authored 550 research articles, 5 books, and 10 granted and pending patents. His Google Scholar h-index is 58 and i10-index is 269 with 15,000 citations. He is regarded as a visionary researcher on Smart Cities technology in which his research deals with security and energy aware, and AI/ML-integrated smart components. He introduced the Secure Digital Camera (SDC) in 2004 with built-in security features designed using Hardware Assisted Security (HAS) or Security by Design (SbD) principle. He is widely credited as the designer for the first digital watermarking chip in 2004 and first the low-power digital watermarking chip in 2006. He is a recipient of 19 best paper awards, Fulbright Specialist Award in 2021, IEEE Consumer Electronics Society Outstanding Service Award in 2020, the IEEE-CS-TCVLSI Distinguished Leadership Award in 2018, and the PROSE Award for Best Textbook in Physical Sciences and Mathematics category in 2016. He has delivered 30 keynotes and served on 15 panels at various International Conferences. He has been serving on the editorial board of several peer-reviewed international transactions/journals, including IEEE Transactions on Big Data (TBD), IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), IEEE Transactions on Consumer Electronics (TCE), and ACM Journal on Emerging Technologies in Computing Systems (JETC). He has been the Editor-in-Chief (EiC) of the IEEE Consumer Electronics Magazine (MCE) during 2016-2021. He served as the Chair of Technical Committee on Very Large Scale Integration (TCVLSI), IEEE Computer Society (IEEE-CS) during 2014-2018 and on the Board of Governors of the IEEE Consumer Electronics Society during 2019-2021. He serves on the steering, organizing, and program committees of several international conferences. He is the steering committee chair/vice-chair for the IEEE International Symposium on Smart Electronic Systems (IEEE-iSES), the IEEE-CS Symposium on VLSI (ISVLSI), and the OITS International Conference on Information Technology (OCIT). He has supervised 3 post-doctoral researchers, 17 Ph.D. dissertations, 28 M.S. theses, and 28 undergraduate projects.</p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 15:31:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
