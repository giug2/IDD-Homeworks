<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.14115] Training point-based deep learning networks for forest segmentation with synthetic data</title><meta property="og:description" content="Remote sensing through unmanned aerial systems (UAS) has been increasing in forestry in recent years, along with using machine learning for data processing. Deep learning architectures, extensively applied in natural lâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training point-based deep learning networks for forest segmentation with synthetic data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training point-based deep learning networks for forest segmentation with synthetic data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.14115">

<!--Generated on Fri Apr  5 15:36:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Deep Learning Point Cloud Segmentation Forest Simulator">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Instituto en Ciencias de la ComputaciÃ³n (UBA-CONICET), Ciudad AutÃ³noma de Buenos Aires C1428EGA, Argentina 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>fraverta@icc.fcen.uba.ar</span></span></span> </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Departamento de ComputaciÃ³n, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires, Ciudad AutÃ³noma de Buenos Aires C1428EGA, Argentina 
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{jschandin,pdecris}@dc.uba.ar</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Training point-based deep learning networks for forest segmentation with synthetic data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francisco Raverta Capua 
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0004-3337-7741" title="ORCID identifier" class="ltx_ref">0009-0004-3337-7741</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Juan Schandin
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0009-9994-3503" title="ORCID identifier" class="ltx_ref">0009-0009-9994-3503</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pablo De CristÃ³foris
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7551-7720" title="ORCID identifier" class="ltx_ref">0000-0002-7551-7720</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Remote sensing through unmanned aerial systems (UAS) has been increasing in forestry in recent years, along with using machine learning for data processing. Deep learning architectures, extensively applied in natural language and image processing, have recently been extended to the point cloud domain. However, the availability of point cloud datasets for training and testing remains limited.
Creating forested environment point cloud datasets is expensive, requires high-precision sensors, and is time-consuming as manual point classification is required.
Moreover, forest areas could be inaccessible or dangerous for humans, further complicating data collection. Then, a question arises whether it is possible to use synthetic data to train deep learning networks without the need to rely on large volumes of real forest data.
To answer this question, we developed a realistic simulator that procedurally generates synthetic forest scenes. Thanks to this, we have conducted a comparative study of different state-of-the-art point-based deep learning networks for forest segmentation. Using created datasets, we determined the feasibility of using synthetic data to train deep learning networks to classify point clouds from real forest datasets. Both the simulator and the datasets are released as part of this work.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Deep Learning Point Cloud Segmentation Forest Simulator
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The use of remote sensing for environmental monitoring has grown significantly in recent years. In particular, the advancements in LiDAR and camera technology made possible the development of Terrestrial Laser Scanning (TLS), Aerial Laser Scanning (ALS), and Aerial Photogrammetry, techniques widely used in precision forestry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. LiDAR sensors made it possible to easily acquire three-dimensional data of the studied environment, accurately representing it with point clouds with a very high precision level. ASL is considered the most accurate method for estimating the forest structure, as both the canopy and the ground can be detected Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, it is more expensive, heavier, and consumes more energy. On the other hand, the camera obtains two-dimensional imagery that pictures the scene in front of the lens, but thanks to computer vision techniques, several images from the same scene can be processed to generate a three-dimensional representation using point clouds.Cameras are lighter, cheaper, and more energy efficient, but the resulting 3D point clouds are less accurate and prone to occlusion as they cannot detect the ground if the canopy occludes it. Both sensors have been widely used in forest environments for health monitoring, species classification, tree parameter estimation, and even illegal logging detection amidst other applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Deep learning architectures, popular in natural language and image processing nowadays, have recently been extended to point cloud processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Multiple techniques have already been adapted to the goals of classification, segmentation, and point completion in point clouds, including residual networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, U-Net networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, fully convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and even attention layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Unfortunately, few point cloud datasets are publicly available for training, validating, and testing deep learning architectures, among which we can mention ScanNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, ScanObjectNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and ModelNet40Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for object classification and ShapeNetPartÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and SemanticKITTIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for part segmentation and scene completion. Generally speaking, none of them are designed for specific environments, such as forests. Therefore, if training a deep learning architecture for such an environment is needed, a new dataset for this purpose has to be generated. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> developed a dataset from the regions of the Southern Sierra Nevada Mountains, USA, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> from Australia and New Zealand, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> from Evo, Finland. All these works were conducted for forest segmentation. Of the three, only the latter dataset is publicly available, limiting the repeatability of the experiments and the comparison between the cited works.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Creating a specific point cloud dataset for forested environments is expensive, as high-end equipment, including UAVs and high-precision sensors, is required to survey the studied area. It is also time-consuming, as it implies labeling the points manually. Moreover, forest areas could be inaccessible or dangerous for humans, further complicating data collection.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This work aims to answer whether synthetic data for training point-based deep learning networks is suitable for segmenting real forest point clouds generated from LiDAR or camera sensors. To do that, we developed a forest simulator based on Unity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> that allows us to generate several forested scenes with high realism procedurally. We extract the point clouds from the synthetic scenes, which are then used to train the deep learning architectures instead of using real forest data. The main contributions of this work are as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The development of a novel open-source forest simulator based on Unity that procedurally generates forest scenes. It also includes a configurable survey mission planner mode that takes pictures of the scene like a camera from an up-down UAV view.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Public-domain synthetic datasets of forest scenes that can be used to train or test different deep-learning networks.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">A comparative study of state-of-the-art point-based deep learning networks to determine whether training with synthetic data is suitable for segmenting point clouds from real forest datasets.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This paper is organized as follows: Section 2 overviews related works. Section 3 briefly explains the deep-learning architectures selected for this work, the developed forest simulator, and the dataset generation. Section 4 presents and discusses the experimental results, and Section 5 ends with conclusions and future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The development of TLS and ALS has made acquiring an accurate 3D reconstruction of the studied environment possible. Using tools and techniques of computer vision and artificial intelligence models, a digital terrain model (DTM) can be estimated from this reconstruction. In a similar way, the application of aerial photogrammetry and computer vision techniques, such as Structure from Motion, made it possible to compute a 3D model (i.e. point cloud) based on the captured images of the environment, typically taken from a UAV.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the state-of-the-art, morphological techniques for estimating the DTM are well studied <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, artificial intelligence models have grown in relevance, driven by new technologies with more precise architectures and lower training times. In particular, the development of the transformer technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, very popular nowadays for its use in applications such as OpenAIâ€™s ChatGPT and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which has generated a revolution in the area of natural language processing, and that is being transferred to other areas such as image and point cloud processing.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The development of deep learning architectures designed for point cloud processing started to receive more attention with PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. This architecture was taken as a starting point and as a comparison model for several new networks that seek to classify or segment point clouds. PointNet++ is based on a U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> with an encoder-decoder structure. The first is to extract features from point clouds, and the second is to reinterpret the extracted features, built mostly with multi-layer perceptrons. Several networks have been based on PointNet++, such as KPConv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, PointConv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, ConvPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, which incorporated their own convolutional operator designed to work with point clouds, and PFCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which was implemented using a fully convolutional network. PointNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> updates PointNet++ to the state-of-the-art using more modern training strategies, such as newer optimization techniques and data augmentation, and modifying the architecture with the incorporation of newer residual connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, an inverse bottleneck design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and separable multi-layer perceptrons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Transformer technology, based entirely on self-attention layersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, presents advantages over the more standardized use of convolutional layers and multi-layer perceptrons in encoder-decoder architectures. This translates into more accurate results with lower training time as they are more parallelizable but at the cost of using a greater number of parameters, which requires a greater volume of input data for training.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">The first networks that incorporated transformer technology for point cloud processing were Engelâ€™s PointTransformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Zhaoâ€™s PointTransformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, Fast Point Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and PCT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, all of which try to adapt the structure of transformers into the point cloud data type. Following the same idea, PointBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, that follows the ideas of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and BEiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> for natural language processing and image processing respectively, divides the point cloud into several smaller local clouds, and codifies each of them into tokens through a Discrete Variational AutoEncoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. It then masks a given proportion of the local clouds and uses them as an input to the transformer block built with attention encoders only, which is trained to recover the original tokens of the masked clouds. Then it recovers the clouds from the tokens via a Decoder, and reconstructs the full point cloud.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Along the same lines, PointMAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> follows the ideas of MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> for Image Processing, dividing the point cloud into smaller local clouds which are masked without the need for a Discrete Variational AutoEncoder. PointGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, inspired by GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, also divides the point cloud into smaller local clouds but orders them using a Morton curve by spatial proximity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. It then masks a given proportion of the local clouds with a dual mask strategy and uses them as an input to the transformer block built exclusively with attention decoders. It aims to predict future representations of local clouds in an autoregressive way.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Thanks to the ability of the deep learning neural network to adapt to several tasks, such as classification, segmentation, and completion, its use has expanded to several applications. The study of different natural environments has been favored by the quick development of these networks. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> use deep learning techniques for the extraction of DTM from camera images and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> uses them for landform classification. Specifically in point cloud processing, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> use deep learning for the extraction of DTM from LiDAR captured data. In a forest environment, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> segments the LiDAR captured point cloud into different categories: terrain, vegetation, coarse woody debris (CWD) and stems. It is noticeable that segmenting the terrain from the rest of the points results equivalent to finding the DTM of the studied environment. Similarly,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> uses segmentation techniques to classify the points into terrain, understorey, tree trunk, or foliage categories; the labeled point cloud used in this work is publicly available.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Regarding the use of synthetic datasets in forest environments, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, a simulator is presented using procedural techniques, where sensors like LiDAR, an RGB camera, and a depth camera are also simulated for data extraction, all of them with the capacity of segmenting the scene in the following categories: background, terrain, traversable, trunks, canopy, shrubs, herbaceous plants, and rocks. A synthetic dataset using this simulator is available in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. It includes RGB images, semantic segmentation maps, depth maps and the projection of LiDAR point clouds on the RGB field of view for two different LiDAR scanning patterns, but it does not include the point clouds of the environment. Finally,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> uses this dataset for training networks to detect fuel for preventing spread of forest fires. This work concludes that the synthetic data fails to generalize to real data.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Materials and Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Point cloud deep learning networks</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we aim to train different deep learning networks to segment the forest point clouds into trunks, canopy, understorey, and terrain. This lets us differentiate forest strata and the DTM that corresponds with the terrain points. We selected four well-known state-of-the-art architectures, PointNeXt, PointBERT, PointMAP, and PointGPT, and trained them with synthetic data generated by our forest simulator. Of these four selected architectures, the last three are built using transformers, and pre-trained versions are available with the ShapeNetPart dataset, so these networks can be fine-tuned specializing them in the respective study area. On the other hand, PointNext is not built using transformer technology but multi-layer perceptrons, and as we do not count on pre-trained versions of the network, we train it from scratch.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Forest Simulator</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For this work, a forest simulator based on the Unity engine was developed, from which synthetic data with a similar appearance to real-world forests was extracted. We aim to train the mentioned architectures with synthetic data and test their performance with real forest data. As <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> notices, the manual labeling of the extracted point cloud is a very demanding task, and in several cases, it is impossible to discern which category each point belongs to. Using a simulator for dataset generation overcomes this problem, as the point cloud labeling can be carried out automatically. Moreover, as transformer technology requires a large volume of data for training, generating synthetic data procedurally becomes even more relevant. Below we detail the most important modules of the presented simulator.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Terrain generation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The simulator first generates a terrain mesh using fractal noise to build a heightmap for all its vertices. These noise samples are from Perlin noise layers at different scales or octaves. This permits controlling the amount of detail and the general aspect of the terrain.
As the implementation is easily parallelizable, we took advantage of the Perlin noise function provided by Unity and the Unity JOBS framework for parallel execution. The random appearance and realistic aspect of this methodâ€™s results are notorious and well-regarded in the video game development community.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Trees, bushes, and plants generation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Vegetation (excluding grass) is generated via pipelines. Each pipeline is a Directed Acyclic Graph (DAG) that links prefabs (i.e. reusable pre-generated game objects, like individual trees or bushes) with their position over the terrain. This is done through textures that determine the spawn probability and density over the terrain mesh. Each pipeline is built using different nodes:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Source: imports a texture from a file or another pipeline, or generates a new one via a Voronoi diagram or sampling noise.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Logic: applies logic operations over textures</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Sampling: variants of Poisson disk sampling method.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Placement: generates instancing parameters for the assigned prefabs.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.12" class="ltx_p">Regarding the sampling process, an implementation of Bridsonâ€™s Poisson disk sampling method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and a modification was implemented. In Bridsonâ€™s algorithm, given an object <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="a_{0}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><msub id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">a</mi><mn id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">ğ‘</ci><cn type="integer" id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">a_{0}</annotation></semantics></math> with radius <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mi id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><ci id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">r</annotation></semantics></math>, new objects of the same radius are added in an annulus of size <math id="S3.SS2.SSS2.p2.3.m3.2" class="ltx_Math" alttext="[r,2r]" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.2a"><mrow id="S3.SS2.SSS2.p2.3.m3.2.2.1" xref="S3.SS2.SSS2.p2.3.m3.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p2.3.m3.2.2.1.2" xref="S3.SS2.SSS2.p2.3.m3.2.2.2.cmml">[</mo><mi id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml">r</mi><mo id="S3.SS2.SSS2.p2.3.m3.2.2.1.3" xref="S3.SS2.SSS2.p2.3.m3.2.2.2.cmml">,</mo><mrow id="S3.SS2.SSS2.p2.3.m3.2.2.1.1" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.cmml"><mn id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.2" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.1" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.3" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.3.cmml">r</mi></mrow><mo stretchy="false" id="S3.SS2.SSS2.p2.3.m3.2.2.1.4" xref="S3.SS2.SSS2.p2.3.m3.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.2b"><interval closure="closed" id="S3.SS2.SSS2.p2.3.m3.2.2.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.2.2.1"><ci id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1"><times id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.2">2</cn><ci id="S3.SS2.SSS2.p2.3.m3.2.2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.3.m3.2.2.1.1.3">ğ‘Ÿ</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.2c">[r,2r]</annotation></semantics></math> without overlapping, until it reaches a maximum quantity, or until there can not be placed any more, and then this process is repeated with a new object. Using a cell size of <math id="S3.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="r/\sqrt{n}" display="inline"><semantics id="S3.SS2.SSS2.p2.4.m4.1a"><mrow id="S3.SS2.SSS2.p2.4.m4.1.1" xref="S3.SS2.SSS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p2.4.m4.1.1.2" xref="S3.SS2.SSS2.p2.4.m4.1.1.2.cmml">r</mi><mo id="S3.SS2.SSS2.p2.4.m4.1.1.1" xref="S3.SS2.SSS2.p2.4.m4.1.1.1.cmml">/</mo><msqrt id="S3.SS2.SSS2.p2.4.m4.1.1.3" xref="S3.SS2.SSS2.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.4.m4.1.1.3.2" xref="S3.SS2.SSS2.p2.4.m4.1.1.3.2.cmml">n</mi></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.4.m4.1b"><apply id="S3.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1"><divide id="S3.SS2.SSS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.1"></divide><ci id="S3.SS2.SSS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.3"><root id="S3.SS2.SSS2.p2.4.m4.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.3"></root><ci id="S3.SS2.SSS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.3.2">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.4.m4.1c">r/\sqrt{n}</annotation></semantics></math>, where <math id="S3.SS2.SSS2.p2.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS2.p2.5.m5.1a"><mi id="S3.SS2.SSS2.p2.5.m5.1.1" xref="S3.SS2.SSS2.p2.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.5.m5.1b"><ci id="S3.SS2.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.5.m5.1c">n</annotation></semantics></math> is the dimension of the background grid for storing samples, each cell can contain only one placed object. This process is fast for object placement, but it produces a distribution of points that may appear equidistant, especially for small values of <math id="S3.SS2.SSS2.p2.6.m6.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.SSS2.p2.6.m6.1a"><mi id="S3.SS2.SSS2.p2.6.m6.1.1" xref="S3.SS2.SSS2.p2.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.6.m6.1b"><ci id="S3.SS2.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.6.m6.1c">r</annotation></semantics></math> giving an unrealistic point distribution. To face this issue we propose a variation to the method: new points are seeked in an annulus of size <math id="S3.SS2.SSS2.p2.7.m7.2" class="ltx_Math" alttext="[r_{min},r_{max}]" display="inline"><semantics id="S3.SS2.SSS2.p2.7.m7.2a"><mrow id="S3.SS2.SSS2.p2.7.m7.2.2.2" xref="S3.SS2.SSS2.p2.7.m7.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p2.7.m7.2.2.2.3" xref="S3.SS2.SSS2.p2.7.m7.2.2.3.cmml">[</mo><msub id="S3.SS2.SSS2.p2.7.m7.1.1.1.1" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.2" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.2.cmml">r</mi><mrow id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.2" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.1" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.3" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.1a" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.4" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.4.cmml">n</mi></mrow></msub><mo id="S3.SS2.SSS2.p2.7.m7.2.2.2.4" xref="S3.SS2.SSS2.p2.7.m7.2.2.3.cmml">,</mo><msub id="S3.SS2.SSS2.p2.7.m7.2.2.2.2" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.2" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.2.cmml">r</mi><mrow id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.2" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.1" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.3" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.1a" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.4" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.4.cmml">x</mi></mrow></msub><mo stretchy="false" id="S3.SS2.SSS2.p2.7.m7.2.2.2.5" xref="S3.SS2.SSS2.p2.7.m7.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.7.m7.2b"><interval closure="closed" id="S3.SS2.SSS2.p2.7.m7.2.2.3.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2"><apply id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3"><times id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p2.7.m7.1.1.1.1.3.4">ğ‘›</ci></apply></apply><apply id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3"><times id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.1"></times><ci id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.2">ğ‘š</ci><ci id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.3">ğ‘</ci><ci id="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.4.cmml" xref="S3.SS2.SSS2.p2.7.m7.2.2.2.2.3.4">ğ‘¥</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.7.m7.2c">[r_{min},r_{max}]</annotation></semantics></math>, where <math id="S3.SS2.SSS2.p2.8.m8.1" class="ltx_Math" alttext="r_{min}" display="inline"><semantics id="S3.SS2.SSS2.p2.8.m8.1a"><msub id="S3.SS2.SSS2.p2.8.m8.1.1" xref="S3.SS2.SSS2.p2.8.m8.1.1.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.2.cmml">r</mi><mrow id="S3.SS2.SSS2.p2.8.m8.1.1.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.2" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.8.m8.1.1.3.1" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.3" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.8.m8.1.1.3.1a" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.8.m8.1.1.3.4" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.8.m8.1b"><apply id="S3.SS2.SSS2.p2.8.m8.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3"><times id="S3.SS2.SSS2.p2.8.m8.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.1"></times><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.SSS2.p2.8.m8.1.1.3.4.cmml" xref="S3.SS2.SSS2.p2.8.m8.1.1.3.4">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.8.m8.1c">r_{min}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p2.9.m9.1" class="ltx_Math" alttext="r_{max}" display="inline"><semantics id="S3.SS2.SSS2.p2.9.m9.1a"><msub id="S3.SS2.SSS2.p2.9.m9.1.1" xref="S3.SS2.SSS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.SSS2.p2.9.m9.1.1.2" xref="S3.SS2.SSS2.p2.9.m9.1.1.2.cmml">r</mi><mrow id="S3.SS2.SSS2.p2.9.m9.1.1.3" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.9.m9.1.1.3.2" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.9.m9.1.1.3.1" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.9.m9.1.1.3.3" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.9.m9.1.1.3.1a" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.9.m9.1.1.3.4" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.9.m9.1b"><apply id="S3.SS2.SSS2.p2.9.m9.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.3"><times id="S3.SS2.SSS2.p2.9.m9.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.1"></times><ci id="S3.SS2.SSS2.p2.9.m9.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.SSS2.p2.9.m9.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.3">ğ‘</ci><ci id="S3.SS2.SSS2.p2.9.m9.1.1.3.4.cmml" xref="S3.SS2.SSS2.p2.9.m9.1.1.3.4">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.9.m9.1c">r_{max}</annotation></semantics></math> are a given minimum radius and a maximum radius respectively, interpolating the distance linearly using the value of a greyscale texture at each point. This means that for values near 0 (where the texture is black), we will generate points at a distance <math id="S3.SS2.SSS2.p2.10.m10.1" class="ltx_Math" alttext="r_{min}" display="inline"><semantics id="S3.SS2.SSS2.p2.10.m10.1a"><msub id="S3.SS2.SSS2.p2.10.m10.1.1" xref="S3.SS2.SSS2.p2.10.m10.1.1.cmml"><mi id="S3.SS2.SSS2.p2.10.m10.1.1.2" xref="S3.SS2.SSS2.p2.10.m10.1.1.2.cmml">r</mi><mrow id="S3.SS2.SSS2.p2.10.m10.1.1.3" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.10.m10.1.1.3.2" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.10.m10.1.1.3.1" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.10.m10.1.1.3.3" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.10.m10.1.1.3.1a" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.10.m10.1.1.3.4" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.10.m10.1b"><apply id="S3.SS2.SSS2.p2.10.m10.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.10.m10.1.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.10.m10.1.1.2.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.10.m10.1.1.3.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.3"><times id="S3.SS2.SSS2.p2.10.m10.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.1"></times><ci id="S3.SS2.SSS2.p2.10.m10.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.SSS2.p2.10.m10.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.SSS2.p2.10.m10.1.1.3.4.cmml" xref="S3.SS2.SSS2.p2.10.m10.1.1.3.4">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.10.m10.1c">r_{min}</annotation></semantics></math> from <math id="S3.SS2.SSS2.p2.11.m11.1" class="ltx_Math" alttext="a_{0}" display="inline"><semantics id="S3.SS2.SSS2.p2.11.m11.1a"><msub id="S3.SS2.SSS2.p2.11.m11.1.1" xref="S3.SS2.SSS2.p2.11.m11.1.1.cmml"><mi id="S3.SS2.SSS2.p2.11.m11.1.1.2" xref="S3.SS2.SSS2.p2.11.m11.1.1.2.cmml">a</mi><mn id="S3.SS2.SSS2.p2.11.m11.1.1.3" xref="S3.SS2.SSS2.p2.11.m11.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.11.m11.1b"><apply id="S3.SS2.SSS2.p2.11.m11.1.1.cmml" xref="S3.SS2.SSS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.11.m11.1.1.1.cmml" xref="S3.SS2.SSS2.p2.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.11.m11.1.1.2.cmml" xref="S3.SS2.SSS2.p2.11.m11.1.1.2">ğ‘</ci><cn type="integer" id="S3.SS2.SSS2.p2.11.m11.1.1.3.cmml" xref="S3.SS2.SSS2.p2.11.m11.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.11.m11.1c">a_{0}</annotation></semantics></math>, and the inverse holds for values near 1. To do that, we tweaked Bridsonâ€™s algorithm to use a spatial cell of size <math id="S3.SS2.SSS2.p2.12.m12.1" class="ltx_Math" alttext="r_{min}/\sqrt{n}" display="inline"><semantics id="S3.SS2.SSS2.p2.12.m12.1a"><mrow id="S3.SS2.SSS2.p2.12.m12.1.1" xref="S3.SS2.SSS2.p2.12.m12.1.1.cmml"><msub id="S3.SS2.SSS2.p2.12.m12.1.1.2" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.cmml"><mi id="S3.SS2.SSS2.p2.12.m12.1.1.2.2" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.2.cmml">r</mi><mrow id="S3.SS2.SSS2.p2.12.m12.1.1.2.3" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.cmml"><mi id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.2" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.1" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.3" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.1a" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.4" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.4.cmml">n</mi></mrow></msub><mo id="S3.SS2.SSS2.p2.12.m12.1.1.1" xref="S3.SS2.SSS2.p2.12.m12.1.1.1.cmml">/</mo><msqrt id="S3.SS2.SSS2.p2.12.m12.1.1.3" xref="S3.SS2.SSS2.p2.12.m12.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.12.m12.1.1.3.2" xref="S3.SS2.SSS2.p2.12.m12.1.1.3.2.cmml">n</mi></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.12.m12.1b"><apply id="S3.SS2.SSS2.p2.12.m12.1.1.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1"><divide id="S3.SS2.SSS2.p2.12.m12.1.1.1.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.1"></divide><apply id="S3.SS2.SSS2.p2.12.m12.1.1.2.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.12.m12.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p2.12.m12.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.2">ğ‘Ÿ</ci><apply id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3"><times id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.1.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.1"></times><ci id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.2.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.2">ğ‘š</ci><ci id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.3.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.3">ğ‘–</ci><ci id="S3.SS2.SSS2.p2.12.m12.1.1.2.3.4.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.2.3.4">ğ‘›</ci></apply></apply><apply id="S3.SS2.SSS2.p2.12.m12.1.1.3.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.3"><root id="S3.SS2.SSS2.p2.12.m12.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.3"></root><ci id="S3.SS2.SSS2.p2.12.m12.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.12.m12.1.1.3.2">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.12.m12.1c">r_{min}/\sqrt{n}</annotation></semantics></math> but instead of each cell holding the index of only one placed object, it holds a list of indices for the objects that shadow that cell but are at an acceptable distance between themselves.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">The texture of spawn probability acts as the probability of effectively instantiating an object at a given point. Having this as a separate node from the sampling process is useful since the number of objects spawned in an area can effectively be reduced, even to 0, making clearings of arbitrary shapes possible.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">They can be created as many pipelines as required. The simulator already counts with basic pipelines for trees and bushes. The prefab models of trees, bushes, and other plants were generated with the free-to-use TreeIt softwareÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. A small sample of them can be seen in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3.2.2 Trees, bushes, and plants generation â€£ 3.2 Forest Simulator â€£ 3 Materials and Methods â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. More models can be easily added if needed. Before instantiating these models, some transformations are applied to include more variability to the scene: a random spin around its up axis, a random twist to bend the up direction with regards to the worldâ€™s up direction, and a random scale, all of them with customizable minimum and maximum values.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.1" class="ltx_figure ltx_figure_panel"><img src="/html/2403.14115/assets/PrefabPine0.png" id="S3.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="464" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.2" class="ltx_figure ltx_figure_panel"><img src="/html/2403.14115/assets/PrefabFir2.png" id="S3.F1.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="464" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.3" class="ltx_figure ltx_figure_panel"><img src="/html/2403.14115/assets/PrefabFir1.png" id="S3.F1.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="464" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.5.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.6.2" class="ltx_text" style="font-size:90%;">Sample models of trees and bushes used in the simulator, generated with software TreeItÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite></span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Grass generation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">None of the sampling methods for vegetation distribution could scale to generate millions of points while keeping the frame rate manageable. Thus, a parallelizable method was devised for placing grass. It is generated by indirect instancing, where the geometry is produced via a compute shader and sent to the graphics pipeline through a shared memory buffer. The sampling algorithm for the grass leaves is depicted in AlgorithmÂ <a href="#alg1" title="Algorithm 1 â€£ 3.2.3 Grass generation â€£ 3.2 Forest Simulator â€£ 3 Materials and Methods â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As the tiles are non-overlapping, the processing for each tile can be parallelized over the number of tiles. The generated points are then transformed into the terrain coordinates using parallel raycasting. For a <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p1.1.m1.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><times id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">256\times 256</annotation></semantics></math> pixels texture, a tile size of 4 pixels and a maximum of 1024 points per tile, the shader takes less than one second to run approximately 4 million points, running in an Intel Core i9-10900 processor, with 32 GB RAM and a NVIDIA GTX 1060 board.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">The points are then fed into a compute shader that proceeds to generate the geometry for a single blade. The number of blade segments can be customized. To add a realistic feeling to the grass, the following transformations are also applied to each blade: a random jitter to the anchor point because of the grid-like pattern of the sampling algorithm; a random rotation that sets which direction the blade will face; a random bend for the tip of the blade and a random scaling. After these transformations, the points are returned to world space coordinates to be placed over the terrain. Finally, a shader applies a grass texture to each leaf to add volume and color.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">Approximately 4 million blades of grass, each composed of 9 points, can be generated and updated at <math id="S3.SS2.SSS3.p3.1.m1.1" class="ltx_Math" alttext="20\sim 30" display="inline"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><mrow id="S3.SS2.SSS3.p3.1.m1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p3.1.m1.1.1.2" xref="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml">20</mn><mo id="S3.SS2.SSS3.p3.1.m1.1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.1.cmml">âˆ¼</mo><mn id="S3.SS2.SSS3.p3.1.m1.1.1.3" xref="S3.SS2.SSS3.p3.1.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.1.m1.1b"><apply id="S3.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.1">similar-to</csymbol><cn type="integer" id="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.2">20</cn><cn type="integer" id="S3.SS2.SSS3.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.1.m1.1c">20\sim 30</annotation></semantics></math> fps in the mentioned hardware, displaying all grass blades simultaneously. It is worth noting that this instancing method is not used with trees, bushes, and other plants because the random jitter prevents us from enforcing a minimum radius distance between instanced objects to avoid collisions.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.6.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Grass Sampling</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.4" class="ltx_p ltx_figure_panel"><span id="alg1.4.1" class="ltx_text ltx_font_bold">Input:</span> texture <math id="alg1.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.1.m1.1a"><mi id="alg1.1.m1.1.1" xref="alg1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.1.m1.1b"><ci id="alg1.1.m1.1.1.cmml" xref="alg1.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.1.m1.1c">T</annotation></semantics></math>, tile size <math id="alg1.2.m2.1" class="ltx_Math" alttext="t_{s}" display="inline"><semantics id="alg1.2.m2.1a"><msub id="alg1.2.m2.1.1" xref="alg1.2.m2.1.1.cmml"><mi id="alg1.2.m2.1.1.2" xref="alg1.2.m2.1.1.2.cmml">t</mi><mi id="alg1.2.m2.1.1.3" xref="alg1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.2.m2.1b"><apply id="alg1.2.m2.1.1.cmml" xref="alg1.2.m2.1.1"><csymbol cd="ambiguous" id="alg1.2.m2.1.1.1.cmml" xref="alg1.2.m2.1.1">subscript</csymbol><ci id="alg1.2.m2.1.1.2.cmml" xref="alg1.2.m2.1.1.2">ğ‘¡</ci><ci id="alg1.2.m2.1.1.3.cmml" xref="alg1.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.m2.1c">t_{s}</annotation></semantics></math>, maximum number of points for a tile <math id="alg1.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="alg1.3.m3.1a"><mi id="alg1.3.m3.1.1" xref="alg1.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="alg1.3.m3.1b"><ci id="alg1.3.m3.1.1.cmml" xref="alg1.3.m3.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.3.m3.1c">P</annotation></semantics></math>
<br class="ltx_break">Â Â Â Â Â  <span id="alg1.4.2" class="ltx_text ltx_font_bold">Output:</span> points for grass leaf instancing <math id="alg1.4.m4.1" class="ltx_Math" alttext="G" display="inline"><semantics id="alg1.4.m4.1a"><mi id="alg1.4.m4.1.1" xref="alg1.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.4.m4.1b"><ci id="alg1.4.m4.1.1.cmml" xref="alg1.4.m4.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.4.m4.1c">G</annotation></semantics></math></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.7" class="ltx_listing ltx_figure_panel ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>Divide the texture <math id="alg1.l1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">T</annotation></semantics></math> into non-overlapping tiles according to a tile size <math id="alg1.l1.m2.1" class="ltx_Math" alttext="t_{s}" display="inline"><semantics id="alg1.l1.m2.1a"><msub id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">t</mi><mi id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1">subscript</csymbol><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">ğ‘¡</ci><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">t_{s}</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span>Define an empty list <math id="alg1.l2.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="alg1.l2.m1.1a"><mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">G</annotation></semantics></math> for the points for grass instancing

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.2" class="ltx_text ltx_font_bold">for</span>Â each tile <math id="alg1.l3.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="alg1.l3.m1.1a"><mi id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">t</annotation></semantics></math>Â <span id="alg1.l3.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>Â Â Â Â Â Define density <math id="alg1.l4.m1.4" class="ltx_Math" alttext="d=\frac{1}{t_{s}^{2}}\sum\limits_{(i,j)\in t}p_{i,j}" display="inline"><semantics id="alg1.l4.m1.4a"><mrow id="alg1.l4.m1.4.5" xref="alg1.l4.m1.4.5.cmml"><mi id="alg1.l4.m1.4.5.2" xref="alg1.l4.m1.4.5.2.cmml">d</mi><mo id="alg1.l4.m1.4.5.1" xref="alg1.l4.m1.4.5.1.cmml">=</mo><mrow id="alg1.l4.m1.4.5.3" xref="alg1.l4.m1.4.5.3.cmml"><mfrac id="alg1.l4.m1.4.5.3.2" xref="alg1.l4.m1.4.5.3.2.cmml"><mn id="alg1.l4.m1.4.5.3.2.2" xref="alg1.l4.m1.4.5.3.2.2.cmml">1</mn><msubsup id="alg1.l4.m1.4.5.3.2.3" xref="alg1.l4.m1.4.5.3.2.3.cmml"><mi id="alg1.l4.m1.4.5.3.2.3.2.2" xref="alg1.l4.m1.4.5.3.2.3.2.2.cmml">t</mi><mi id="alg1.l4.m1.4.5.3.2.3.2.3" xref="alg1.l4.m1.4.5.3.2.3.2.3.cmml">s</mi><mn id="alg1.l4.m1.4.5.3.2.3.3" xref="alg1.l4.m1.4.5.3.2.3.3.cmml">2</mn></msubsup></mfrac><mo lspace="0em" rspace="0em" id="alg1.l4.m1.4.5.3.1" xref="alg1.l4.m1.4.5.3.1.cmml">â€‹</mo><mrow id="alg1.l4.m1.4.5.3.3" xref="alg1.l4.m1.4.5.3.3.cmml"><munder id="alg1.l4.m1.4.5.3.3.1" xref="alg1.l4.m1.4.5.3.3.1.cmml"><mo movablelimits="false" id="alg1.l4.m1.4.5.3.3.1.2" xref="alg1.l4.m1.4.5.3.3.1.2.cmml">âˆ‘</mo><mrow id="alg1.l4.m1.2.2.2" xref="alg1.l4.m1.2.2.2.cmml"><mrow id="alg1.l4.m1.2.2.2.4.2" xref="alg1.l4.m1.2.2.2.4.1.cmml"><mo stretchy="false" id="alg1.l4.m1.2.2.2.4.2.1" xref="alg1.l4.m1.2.2.2.4.1.cmml">(</mo><mi id="alg1.l4.m1.1.1.1.1" xref="alg1.l4.m1.1.1.1.1.cmml">i</mi><mo id="alg1.l4.m1.2.2.2.4.2.2" xref="alg1.l4.m1.2.2.2.4.1.cmml">,</mo><mi id="alg1.l4.m1.2.2.2.2" xref="alg1.l4.m1.2.2.2.2.cmml">j</mi><mo stretchy="false" id="alg1.l4.m1.2.2.2.4.2.3" xref="alg1.l4.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="alg1.l4.m1.2.2.2.3" xref="alg1.l4.m1.2.2.2.3.cmml">âˆˆ</mo><mi id="alg1.l4.m1.2.2.2.5" xref="alg1.l4.m1.2.2.2.5.cmml">t</mi></mrow></munder><msub id="alg1.l4.m1.4.5.3.3.2" xref="alg1.l4.m1.4.5.3.3.2.cmml"><mi id="alg1.l4.m1.4.5.3.3.2.2" xref="alg1.l4.m1.4.5.3.3.2.2.cmml">p</mi><mrow id="alg1.l4.m1.4.4.2.4" xref="alg1.l4.m1.4.4.2.3.cmml"><mi id="alg1.l4.m1.3.3.1.1" xref="alg1.l4.m1.3.3.1.1.cmml">i</mi><mo id="alg1.l4.m1.4.4.2.4.1" xref="alg1.l4.m1.4.4.2.3.cmml">,</mo><mi id="alg1.l4.m1.4.4.2.2" xref="alg1.l4.m1.4.4.2.2.cmml">j</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.4b"><apply id="alg1.l4.m1.4.5.cmml" xref="alg1.l4.m1.4.5"><eq id="alg1.l4.m1.4.5.1.cmml" xref="alg1.l4.m1.4.5.1"></eq><ci id="alg1.l4.m1.4.5.2.cmml" xref="alg1.l4.m1.4.5.2">ğ‘‘</ci><apply id="alg1.l4.m1.4.5.3.cmml" xref="alg1.l4.m1.4.5.3"><times id="alg1.l4.m1.4.5.3.1.cmml" xref="alg1.l4.m1.4.5.3.1"></times><apply id="alg1.l4.m1.4.5.3.2.cmml" xref="alg1.l4.m1.4.5.3.2"><divide id="alg1.l4.m1.4.5.3.2.1.cmml" xref="alg1.l4.m1.4.5.3.2"></divide><cn type="integer" id="alg1.l4.m1.4.5.3.2.2.cmml" xref="alg1.l4.m1.4.5.3.2.2">1</cn><apply id="alg1.l4.m1.4.5.3.2.3.cmml" xref="alg1.l4.m1.4.5.3.2.3"><csymbol cd="ambiguous" id="alg1.l4.m1.4.5.3.2.3.1.cmml" xref="alg1.l4.m1.4.5.3.2.3">superscript</csymbol><apply id="alg1.l4.m1.4.5.3.2.3.2.cmml" xref="alg1.l4.m1.4.5.3.2.3"><csymbol cd="ambiguous" id="alg1.l4.m1.4.5.3.2.3.2.1.cmml" xref="alg1.l4.m1.4.5.3.2.3">subscript</csymbol><ci id="alg1.l4.m1.4.5.3.2.3.2.2.cmml" xref="alg1.l4.m1.4.5.3.2.3.2.2">ğ‘¡</ci><ci id="alg1.l4.m1.4.5.3.2.3.2.3.cmml" xref="alg1.l4.m1.4.5.3.2.3.2.3">ğ‘ </ci></apply><cn type="integer" id="alg1.l4.m1.4.5.3.2.3.3.cmml" xref="alg1.l4.m1.4.5.3.2.3.3">2</cn></apply></apply><apply id="alg1.l4.m1.4.5.3.3.cmml" xref="alg1.l4.m1.4.5.3.3"><apply id="alg1.l4.m1.4.5.3.3.1.cmml" xref="alg1.l4.m1.4.5.3.3.1"><csymbol cd="ambiguous" id="alg1.l4.m1.4.5.3.3.1.1.cmml" xref="alg1.l4.m1.4.5.3.3.1">subscript</csymbol><sum id="alg1.l4.m1.4.5.3.3.1.2.cmml" xref="alg1.l4.m1.4.5.3.3.1.2"></sum><apply id="alg1.l4.m1.2.2.2.cmml" xref="alg1.l4.m1.2.2.2"><in id="alg1.l4.m1.2.2.2.3.cmml" xref="alg1.l4.m1.2.2.2.3"></in><interval closure="open" id="alg1.l4.m1.2.2.2.4.1.cmml" xref="alg1.l4.m1.2.2.2.4.2"><ci id="alg1.l4.m1.1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1.1">ğ‘–</ci><ci id="alg1.l4.m1.2.2.2.2.cmml" xref="alg1.l4.m1.2.2.2.2">ğ‘—</ci></interval><ci id="alg1.l4.m1.2.2.2.5.cmml" xref="alg1.l4.m1.2.2.2.5">ğ‘¡</ci></apply></apply><apply id="alg1.l4.m1.4.5.3.3.2.cmml" xref="alg1.l4.m1.4.5.3.3.2"><csymbol cd="ambiguous" id="alg1.l4.m1.4.5.3.3.2.1.cmml" xref="alg1.l4.m1.4.5.3.3.2">subscript</csymbol><ci id="alg1.l4.m1.4.5.3.3.2.2.cmml" xref="alg1.l4.m1.4.5.3.3.2.2">ğ‘</ci><list id="alg1.l4.m1.4.4.2.3.cmml" xref="alg1.l4.m1.4.4.2.4"><ci id="alg1.l4.m1.3.3.1.1.cmml" xref="alg1.l4.m1.3.3.1.1">ğ‘–</ci><ci id="alg1.l4.m1.4.4.2.2.cmml" xref="alg1.l4.m1.4.4.2.2">ğ‘—</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.4c">d=\frac{1}{t_{s}^{2}}\sum\limits_{(i,j)\in t}p_{i,j}</annotation></semantics></math>, where <math id="alg1.l4.m2.2" class="ltx_Math" alttext="p_{i,j}" display="inline"><semantics id="alg1.l4.m2.2a"><msub id="alg1.l4.m2.2.3" xref="alg1.l4.m2.2.3.cmml"><mi id="alg1.l4.m2.2.3.2" xref="alg1.l4.m2.2.3.2.cmml">p</mi><mrow id="alg1.l4.m2.2.2.2.4" xref="alg1.l4.m2.2.2.2.3.cmml"><mi id="alg1.l4.m2.1.1.1.1" xref="alg1.l4.m2.1.1.1.1.cmml">i</mi><mo id="alg1.l4.m2.2.2.2.4.1" xref="alg1.l4.m2.2.2.2.3.cmml">,</mo><mi id="alg1.l4.m2.2.2.2.2" xref="alg1.l4.m2.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l4.m2.2b"><apply id="alg1.l4.m2.2.3.cmml" xref="alg1.l4.m2.2.3"><csymbol cd="ambiguous" id="alg1.l4.m2.2.3.1.cmml" xref="alg1.l4.m2.2.3">subscript</csymbol><ci id="alg1.l4.m2.2.3.2.cmml" xref="alg1.l4.m2.2.3.2">ğ‘</ci><list id="alg1.l4.m2.2.2.2.3.cmml" xref="alg1.l4.m2.2.2.2.4"><ci id="alg1.l4.m2.1.1.1.1.cmml" xref="alg1.l4.m2.1.1.1.1">ğ‘–</ci><ci id="alg1.l4.m2.2.2.2.2.cmml" xref="alg1.l4.m2.2.2.2.2">ğ‘—</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m2.2c">p_{i,j}</annotation></semantics></math> is the textureâ€™s value in pixel <math id="alg1.l4.m3.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="alg1.l4.m3.2a"><mrow id="alg1.l4.m3.2.3.2" xref="alg1.l4.m3.2.3.1.cmml"><mo stretchy="false" id="alg1.l4.m3.2.3.2.1" xref="alg1.l4.m3.2.3.1.cmml">(</mo><mi id="alg1.l4.m3.1.1" xref="alg1.l4.m3.1.1.cmml">i</mi><mo id="alg1.l4.m3.2.3.2.2" xref="alg1.l4.m3.2.3.1.cmml">,</mo><mi id="alg1.l4.m3.2.2" xref="alg1.l4.m3.2.2.cmml">j</mi><mo stretchy="false" id="alg1.l4.m3.2.3.2.3" xref="alg1.l4.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m3.2b"><interval closure="open" id="alg1.l4.m3.2.3.1.cmml" xref="alg1.l4.m3.2.3.2"><ci id="alg1.l4.m3.1.1.cmml" xref="alg1.l4.m3.1.1">ğ‘–</ci><ci id="alg1.l4.m3.2.2.cmml" xref="alg1.l4.m3.2.2">ğ‘—</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m3.2c">(i,j)</annotation></semantics></math>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>Â Â Â Â Â Define the number of sampling points in <math id="alg1.l5.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="alg1.l5.m1.1a"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">t</annotation></semantics></math> by <math id="alg1.l5.m2.1" class="ltx_Math" alttext="p=d\times P" display="inline"><semantics id="alg1.l5.m2.1a"><mrow id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml"><mi id="alg1.l5.m2.1.1.2" xref="alg1.l5.m2.1.1.2.cmml">p</mi><mo id="alg1.l5.m2.1.1.1" xref="alg1.l5.m2.1.1.1.cmml">=</mo><mrow id="alg1.l5.m2.1.1.3" xref="alg1.l5.m2.1.1.3.cmml"><mi id="alg1.l5.m2.1.1.3.2" xref="alg1.l5.m2.1.1.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="alg1.l5.m2.1.1.3.1" xref="alg1.l5.m2.1.1.3.1.cmml">Ã—</mo><mi id="alg1.l5.m2.1.1.3.3" xref="alg1.l5.m2.1.1.3.3.cmml">P</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><apply id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1"><eq id="alg1.l5.m2.1.1.1.cmml" xref="alg1.l5.m2.1.1.1"></eq><ci id="alg1.l5.m2.1.1.2.cmml" xref="alg1.l5.m2.1.1.2">ğ‘</ci><apply id="alg1.l5.m2.1.1.3.cmml" xref="alg1.l5.m2.1.1.3"><times id="alg1.l5.m2.1.1.3.1.cmml" xref="alg1.l5.m2.1.1.3.1"></times><ci id="alg1.l5.m2.1.1.3.2.cmml" xref="alg1.l5.m2.1.1.3.2">ğ‘‘</ci><ci id="alg1.l5.m2.1.1.3.3.cmml" xref="alg1.l5.m2.1.1.3.3">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">p=d\times P</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>Â Â Â Â Â Define points <math id="alg1.l6.m1.1" class="ltx_Math" alttext="G_{t}" display="inline"><semantics id="alg1.l6.m1.1a"><msub id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">G</mi><mi id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1">subscript</csymbol><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">ğº</ci><ci id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">G_{t}</annotation></semantics></math> by distributing <math id="alg1.l6.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="alg1.l6.m2.1a"><mi id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><ci id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">p</annotation></semantics></math> in a grid-like pattern in the tile

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>Â Â Â Â Â Append <math id="alg1.l7.m1.1" class="ltx_Math" alttext="G_{t}" display="inline"><semantics id="alg1.l7.m1.1a"><msub id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">G</mi><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1">subscript</csymbol><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">ğº</ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">G_{t}</annotation></semantics></math> to <math id="alg1.l7.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="alg1.l7.m2.1a"><mi id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">G</annotation></semantics></math>

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text ltx_font_bold">end</span>Â <span id="alg1.l8.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text ltx_font_bold">return</span> generated points <math id="alg1.l9.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="alg1.l9.m1.1a"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">G</annotation></semantics></math>

</div>
</div>
</div>
</div>
</figure>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Repeatability</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">Each scene is generated by a seed to ensure repeatability. This seed is transmitted to every vegetation pipeline and to the grass and terrain generators. Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3.2.4 Repeatability â€£ 3.2 Forest Simulator â€£ 3 Materials and Methods â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of the pipelines for generating trees and grass and a top-down view of the resulting forest scene, and Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.2.4 Repeatability â€£ 3.2 Forest Simulator â€£ 3 Materials and Methods â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the same scene from a front view and a closer view.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/x1.jpeg" id="S3.F2.1.g1" class="ltx_graphics ltx_img_square" width="664" height="626" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/x2.jpeg" id="S3.F2.2.g1" class="ltx_graphics ltx_img_square" width="664" height="623" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/Pipeline_trees.png" id="S3.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/Pipeline_grass.png" id="S3.F2.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.6.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.7.2" class="ltx_text" style="font-size:90%;">Above: Forest scene with trees (left), and trees, bushes, and grass (right). Below: the correspondence pipelines for instancing trees (left), and grass (right).</span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/frontish_view.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="259" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/view_from_gully.png" id="S3.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="315" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.5.2" class="ltx_text" style="font-size:90%;">Left: Full frontal view of an example forest scene. Right: Close up view</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Point Cloud Extraction</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">The point cloud of the generated scene can be extracted directly from the Unity Editor as a .csv file. By tagging the instanced objects with meshes, they can be exported as various categories, including but not limited to terrain, canopy, trunk, branches, bushes, understorey, grass, cacti, and deadwood, assigning the corresponding label to each point of the point cloud. The size of the sceneâ€™s point cloud can be altered by adding more points to the terrain mesh, by generating more grass leaves or changing their number of segments, or by importing other vegetation prefabs with the desired quantity. This customization allows the user to generate scenes where its point cloud can vary in size, and thus be adapted to specific needs, such as training large deep learning networks.</p>
</div>
<div id="S3.SS2.SSS5.p2" class="ltx_para">
<p id="S3.SS2.SSS5.p2.1" class="ltx_p">As one of the contributions of this work, the code of the presented forest simulator was released at <a target="_blank" href="https://github.com/lrse/forest-simulator" title="" class="ltx_ref">https://github.com/lrse/forest-simulator</a>.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dataset Assembling</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Two datasets were created to train the selected deep learning architectures, simulating the point clouds obtained by LiDAR and applying structure from motion algorithm to synthetic camera images, both from a top-down view. For the camera-like dataset, a method to include occlusion to the point cloud was used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, as several points should not be visible from above. Then, random noise with zero mean is added to give variability to the point clouds, and it is partitioned in the <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="xy" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ‘¥</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">xy</annotation></semantics></math> plane using <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">K</annotation></semantics></math>-Means clustering to assemble subclouds with which to train the networks. Both datasets, LiDAR-like and Camera-like, are publicly at <a target="_blank" href="https://github.com/lrse/synthetic-forest-datasets" title="" class="ltx_ref">https://github.com/lrse/synthetic-forest-datasets</a> to ensure this workâ€™s results are reproducible and to increase the point cloud datasets available to the scientific community.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The Evo Dataset, given by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, was employed to test the trained architectures. We used an occluded version of this dataset to test the architectures trained with the synthetic Camera-like dataset data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">To create a synthetic dataset that resembles the Evo Dataset, we have extracted four categories from the simulator: terrain, trunks, canopy, and understorey, the latter including grass, bushes, and all other vegetation that are not trees.
In Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.3 Dataset Assembling â€£ 3 Materials and Methods â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, an example of both datasets segmented using <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">K</annotation></semantics></math>-means and segmented into the studied categories can be seen.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/Parts-wo.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="465" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/Classes-wo.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="465" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/segmented.png" id="S3.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="465" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.14115/assets/categorized.png" id="S3.F4.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="465" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.6.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.7.2" class="ltx_text" style="font-size:90%;">Above: Example scene without occlusion segmented using Kmeans (left) and segmented via labels: blue, green, yellow, and red corresponding to terrain, trunks, canopy, and understorey, respectively (right). Below: Same example for another scene, but using occlusion from a top-view point.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">For experimental results, the four selected architectures were trained with an AMD Phenom II X6 1075T Processor CPU, with 32GB RAM, and two NVIDIA RTX 3090 connected with an SLI bridge. 50 epochs were used for every network, with an average training time of 472 seconds per epoch for the LiDAR-like dataset and 239 seconds for the Camera-like dataset.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>LiDAR-like experiment</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 LiDAR-like experiment â€£ 4 Results and Discussion â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of testing the networks trained with the LiDAR-like dataset with the EVO dataset. The confusion matrix of each network is presented in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.1 LiDAR-like experiment â€£ 4 Results and Discussion â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It can be seen that, regardless of having an overall good accuracy, the networks still struggle to differentiate understorey from terrain, especially when the vegetation is on a near-ground level. Despite this, we can notice that the networks perform considerably well in segmenting the trees from the rest of the categories and even segmenting trunks from the canopy. PointNeXt had a better result in classifying the points in the EVO dataset and segmenting each category, and therefore it seems more suitable for forest environments.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Results obtained testing both on the created Synthetic Forest Dataset and in the EVO dataset, using the LiDAR-like dataset.</span></figcaption>
<div id="S4.T1.4" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:120.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(8.4pt,-2.4pt) scale(1.04054785981458,1.04054785981458) ;">
<table id="S4.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" rowspan="2"><span id="S4.T1.4.1.1.1.1.1" class="ltx_text">Network</span></th>
<th id="S4.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3"><span id="S4.T1.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Sythetic Forest Dataset</span></th>
<th id="S4.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="S4.T1.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Evo Dataset</span></th>
</tr>
<tr id="S4.T1.4.1.2.2" class="ltx_tr">
<th id="S4.T1.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.1.1" class="ltx_text">
<span id="S4.T1.4.1.2.2.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T1.4.1.2.2.1.1.1.1" class="ltx_p">Overall</span>
<span id="S4.T1.4.1.2.2.1.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T1.4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.2.1" class="ltx_text">
<span id="S4.T1.4.1.2.2.2.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T1.4.1.2.2.2.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T1.4.1.2.2.2.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T1.4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.4.1.2.2.3.1" class="ltx_text">
<span id="S4.T1.4.1.2.2.3.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T1.4.1.2.2.3.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T1.4.1.2.2.3.1.1.2" class="ltx_p">mIoU</span>
</span></span></th>
<th id="S4.T1.4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.4.1" class="ltx_text">
<span id="S4.T1.4.1.2.2.4.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T1.4.1.2.2.4.1.1.1" class="ltx_p">Overall</span>
<span id="S4.T1.4.1.2.2.4.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T1.4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.5.1" class="ltx_text">
<span id="S4.T1.4.1.2.2.5.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T1.4.1.2.2.5.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T1.4.1.2.2.5.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T1.4.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.6.1" class="ltx_text">
<span id="S4.T1.4.1.2.2.6.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T1.4.1.2.2.6.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T1.4.1.2.2.6.1.1.2" class="ltx_p">mIoU</span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.1.3.1" class="ltx_tr">
<th id="S4.T1.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PointNeXt</th>
<td id="S4.T1.4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.9348</td>
<td id="S4.T1.4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.8339</td>
<td id="S4.T1.4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7314</td>
<td id="S4.T1.4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.1.3.1.5.1" class="ltx_text ltx_font_bold">0.7695</span></td>
<td id="S4.T1.4.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.6436</td>
<td id="S4.T1.4.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.1.3.1.7.1" class="ltx_text ltx_font_bold">0.5153</span></td>
</tr>
<tr id="S4.T1.4.1.4.2" class="ltx_tr">
<th id="S4.T1.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PointBERT</th>
<td id="S4.T1.4.1.4.2.2" class="ltx_td ltx_align_center">0.9551</td>
<td id="S4.T1.4.1.4.2.3" class="ltx_td ltx_align_center">0.8719</td>
<td id="S4.T1.4.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">0.8021</td>
<td id="S4.T1.4.1.4.2.5" class="ltx_td ltx_align_center">0.7195</td>
<td id="S4.T1.4.1.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.4.1.4.2.6.1" class="ltx_text ltx_font_bold">0.6462</span></td>
<td id="S4.T1.4.1.4.2.7" class="ltx_td ltx_align_center">0.4206</td>
</tr>
<tr id="S4.T1.4.1.5.3" class="ltx_tr">
<th id="S4.T1.4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PointMAE</th>
<td id="S4.T1.4.1.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.4.1.5.3.2.1" class="ltx_text ltx_font_bold">0.9575</span></td>
<td id="S4.T1.4.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.4.1.5.3.3.1" class="ltx_text ltx_font_bold">0.8743</span></td>
<td id="S4.T1.4.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.1.5.3.4.1" class="ltx_text ltx_font_bold">0.8029</span></td>
<td id="S4.T1.4.1.5.3.5" class="ltx_td ltx_align_center">0.72788</td>
<td id="S4.T1.4.1.5.3.6" class="ltx_td ltx_align_center">0.61103</td>
<td id="S4.T1.4.1.5.3.7" class="ltx_td ltx_align_center">0.4278</td>
</tr>
<tr id="S4.T1.4.1.6.4" class="ltx_tr">
<th id="S4.T1.4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PointGPT</th>
<td id="S4.T1.4.1.6.4.2" class="ltx_td ltx_align_center">0.9414</td>
<td id="S4.T1.4.1.6.4.3" class="ltx_td ltx_align_center">0.8257</td>
<td id="S4.T1.4.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">0.7516</td>
<td id="S4.T1.4.1.6.4.5" class="ltx_td ltx_align_center">0.6417</td>
<td id="S4.T1.4.1.6.4.6" class="ltx_td ltx_align_center">0.5271</td>
<td id="S4.T1.4.1.6.4.7" class="ltx_td ltx_align_center">0.3620</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Confusion matrix for the four studied networks.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T2.4" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:84.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.5pt,11.6pt) scale(0.784628093733911,0.784628093733911) ;">
<table id="S4.T2.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">PointNeXt</span></th>
</tr>
<tr id="S4.T2.4.1.2.2" class="ltx_tr">
<th id="S4.T2.4.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T2.4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T2.4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T2.4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.1.3.1" class="ltx_tr">
<th id="S4.T2.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T2.4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.1.3.1.2.1" class="ltx_text ltx_font_bold">1055469</span></td>
<td id="S4.T2.4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">4234</td>
<td id="S4.T2.4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">423</td>
<td id="S4.T2.4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">386498</td>
</tr>
<tr id="S4.T2.4.1.4.2" class="ltx_tr">
<th id="S4.T2.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T2.4.1.4.2.2" class="ltx_td ltx_align_center">468</td>
<td id="S4.T2.4.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.4.2.3.1" class="ltx_text ltx_font_bold">191869</span></td>
<td id="S4.T2.4.1.4.2.4" class="ltx_td ltx_align_center">135275</td>
<td id="S4.T2.4.1.4.2.5" class="ltx_td ltx_align_center">59160</td>
</tr>
<tr id="S4.T2.4.1.5.3" class="ltx_tr">
<th id="S4.T2.4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T2.4.1.5.3.2" class="ltx_td ltx_align_center">10201</td>
<td id="S4.T2.4.1.5.3.3" class="ltx_td ltx_align_center">254302</td>
<td id="S4.T2.4.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.5.3.4.1" class="ltx_text ltx_font_bold">8310787</span></td>
<td id="S4.T2.4.1.5.3.5" class="ltx_td ltx_align_center">1220544</td>
</tr>
<tr id="S4.T2.4.1.6.4" class="ltx_tr">
<th id="S4.T2.4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T2.4.1.6.4.2" class="ltx_td ltx_align_center">1105198</td>
<td id="S4.T2.4.1.6.4.3" class="ltx_td ltx_align_center">21323</td>
<td id="S4.T2.4.1.6.4.4" class="ltx_td ltx_align_center">1067</td>
<td id="S4.T2.4.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.1.6.4.5.1" class="ltx_text ltx_font_bold">1128622</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T2.5" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:84.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.5pt,11.9pt) scale(0.778933352249278,0.778933352249278) ;">
<table id="S4.T2.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.5.1.1.1" class="ltx_tr">
<th id="S4.T2.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T2.5.1.1.1.1.1" class="ltx_text ltx_font_bold">PointBERT</span></th>
</tr>
<tr id="S4.T2.5.1.2.2" class="ltx_tr">
<th id="S4.T2.5.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T2.5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T2.5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T2.5.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T2.5.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.5.1.3.1" class="ltx_tr">
<th id="S4.T2.5.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T2.5.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">756515</td>
<td id="S4.T2.5.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">16905</td>
<td id="S4.T2.5.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">6451</td>
<td id="S4.T2.5.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.1.3.1.5.1" class="ltx_text ltx_font_bold">1314363</span></td>
</tr>
<tr id="S4.T2.5.1.4.2" class="ltx_tr">
<th id="S4.T2.5.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T2.5.1.4.2.2" class="ltx_td ltx_align_center">3386</td>
<td id="S4.T2.5.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.5.1.4.2.3.1" class="ltx_text ltx_font_bold">401490</span></td>
<td id="S4.T2.5.1.4.2.4" class="ltx_td ltx_align_center">184272</td>
<td id="S4.T2.5.1.4.2.5" class="ltx_td ltx_align_center">67868</td>
</tr>
<tr id="S4.T2.5.1.5.3" class="ltx_tr">
<th id="S4.T2.5.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T2.5.1.5.3.2" class="ltx_td ltx_align_center">11031</td>
<td id="S4.T2.5.1.5.3.3" class="ltx_td ltx_align_center">1088940</td>
<td id="S4.T2.5.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.5.1.5.3.4.1" class="ltx_text ltx_font_bold">6703359</span></td>
<td id="S4.T2.5.1.5.3.5" class="ltx_td ltx_align_center">765463</td>
</tr>
<tr id="S4.T2.5.1.6.4" class="ltx_tr">
<th id="S4.T2.5.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T2.5.1.6.4.2" class="ltx_td ltx_align_center">373703</td>
<td id="S4.T2.5.1.6.4.3" class="ltx_td ltx_align_center">38963</td>
<td id="S4.T2.5.1.6.4.4" class="ltx_td ltx_align_center">22493</td>
<td id="S4.T2.5.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.5.1.6.4.5.1" class="ltx_text ltx_font_bold">2129238</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T2.6" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:85.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.0pt,11.2pt) scale(0.793328444763732,0.793328444763732) ;">
<table id="S4.T2.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.6.1.1.1" class="ltx_tr">
<th id="S4.T2.6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T2.6.1.1.1.1.1" class="ltx_text ltx_font_bold">PointMAE</span></th>
</tr>
<tr id="S4.T2.6.1.2.2" class="ltx_tr">
<th id="S4.T2.6.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T2.6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T2.6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T2.6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T2.6.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.6.1.3.1" class="ltx_tr">
<th id="S4.T2.6.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T2.6.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">315040</td>
<td id="S4.T2.6.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">2506</td>
<td id="S4.T2.6.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">18805</td>
<td id="S4.T2.6.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.1.3.1.5.1" class="ltx_text ltx_font_bold">1756467</span></td>
</tr>
<tr id="S4.T2.6.1.4.2" class="ltx_tr">
<th id="S4.T2.6.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T2.6.1.4.2.2" class="ltx_td ltx_align_center">193</td>
<td id="S4.T2.6.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.1.4.2.3.1" class="ltx_text ltx_font_bold">340098</span></td>
<td id="S4.T2.6.1.4.2.4" class="ltx_td ltx_align_center">187716</td>
<td id="S4.T2.6.1.4.2.5" class="ltx_td ltx_align_center">129111</td>
</tr>
<tr id="S4.T2.6.1.5.3" class="ltx_tr">
<th id="S4.T2.6.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T2.6.1.5.3.2" class="ltx_td ltx_align_center">6316</td>
<td id="S4.T2.6.1.5.3.3" class="ltx_td ltx_align_center">779671</td>
<td id="S4.T2.6.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.1.5.3.4.1" class="ltx_text ltx_font_bold">6986603</span></td>
<td id="S4.T2.6.1.5.3.5" class="ltx_td ltx_align_center">796919</td>
</tr>
<tr id="S4.T2.6.1.6.4" class="ltx_tr">
<th id="S4.T2.6.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T2.6.1.6.4.2" class="ltx_td ltx_align_center">49187</td>
<td id="S4.T2.6.1.6.4.3" class="ltx_td ltx_align_center">13302</td>
<td id="S4.T2.6.1.6.4.4" class="ltx_td ltx_align_center">38252</td>
<td id="S4.T2.6.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.1.6.4.5.1" class="ltx_text ltx_font_bold">2465254</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T2.7" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:84.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.5pt,11.9pt) scale(0.778933352249278,0.778933352249278) ;">
<table id="S4.T2.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.7.1.1.1" class="ltx_tr">
<th id="S4.T2.7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T2.7.1.1.1.1.1" class="ltx_text ltx_font_bold">PointGPT</span></th>
</tr>
<tr id="S4.T2.7.1.2.2" class="ltx_tr">
<th id="S4.T2.7.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T2.7.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T2.7.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T2.7.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T2.7.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.7.1.3.1" class="ltx_tr">
<th id="S4.T2.7.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T2.7.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">363932</td>
<td id="S4.T2.7.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">8427</td>
<td id="S4.T2.7.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">13425</td>
<td id="S4.T2.7.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.7.1.3.1.5.1" class="ltx_text ltx_font_bold">1709711</span></td>
</tr>
<tr id="S4.T2.7.1.4.2" class="ltx_tr">
<th id="S4.T2.7.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T2.7.1.4.2.2" class="ltx_td ltx_align_center">593</td>
<td id="S4.T2.7.1.4.2.3" class="ltx_td ltx_align_center">219103</td>
<td id="S4.T2.7.1.4.2.4" class="ltx_td ltx_align_center">206613</td>
<td id="S4.T2.7.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.7.1.4.2.5.1" class="ltx_text ltx_font_bold">229875</span></td>
</tr>
<tr id="S4.T2.7.1.5.3" class="ltx_tr">
<th id="S4.T2.7.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T2.7.1.5.3.2" class="ltx_td ltx_align_center">11898</td>
<td id="S4.T2.7.1.5.3.3" class="ltx_td ltx_align_center">1049754</td>
<td id="S4.T2.7.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.7.1.5.3.4.1" class="ltx_text ltx_font_bold">5682647</span></td>
<td id="S4.T2.7.1.5.3.5" class="ltx_td ltx_align_center">1824946</td>
</tr>
<tr id="S4.T2.7.1.6.4" class="ltx_tr">
<th id="S4.T2.7.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T2.7.1.6.4.2" class="ltx_td ltx_align_center">112227</td>
<td id="S4.T2.7.1.6.4.3" class="ltx_td ltx_align_center">23866</td>
<td id="S4.T2.7.1.6.4.4" class="ltx_td ltx_align_center">31023</td>
<td id="S4.T2.7.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.7.1.6.4.5.1" class="ltx_text ltx_font_bold">2387380</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Camera-like experiment</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.2 Camera-like experiment â€£ 4 Results and Discussion â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of testing the networks trained with the Camera-like dataset with the EVO dataset with occlusion. The confusion matrices of each network can be seen in TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.2 Camera-like experiment â€£ 4 Results and Discussion â€£ Training point-based deep learning networks for forest segmentation with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Similar to the previous case, the networks struggle to differentiate terrain points from understorey points, and as few trunk points remain visible, especially the ones at the base of the trunk, it is also often confused with understorey points. We can see that the overall performance is significantly lower than in the case without occlusion. PointMAE achieves slightly better accuracy than the other networks, although all have similar responses.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Results obtained testing both on the created Synthetic Forest Dataset and in the EVO dataset, using the Camera-like dataset.</span></figcaption>
<div id="S4.T3.4" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:120.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(8.4pt,-2.4pt) scale(1.04054785981458,1.04054785981458) ;">
<table id="S4.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" rowspan="2"><span id="S4.T3.4.1.1.1.1.1" class="ltx_text">Network</span></th>
<th id="S4.T3.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3"><span id="S4.T3.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Sythetic Forest Dataset</span></th>
<th id="S4.T3.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="S4.T3.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Evo Dataset</span></th>
</tr>
<tr id="S4.T3.4.1.2.2" class="ltx_tr">
<th id="S4.T3.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.4.1.2.2.1.1" class="ltx_text">
<span id="S4.T3.4.1.2.2.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T3.4.1.2.2.1.1.1.1" class="ltx_p">Overall</span>
<span id="S4.T3.4.1.2.2.1.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T3.4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.4.1.2.2.2.1" class="ltx_text">
<span id="S4.T3.4.1.2.2.2.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T3.4.1.2.2.2.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T3.4.1.2.2.2.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T3.4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.2.2.3.1" class="ltx_text">
<span id="S4.T3.4.1.2.2.3.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T3.4.1.2.2.3.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T3.4.1.2.2.3.1.1.2" class="ltx_p">mIoU</span>
</span></span></th>
<th id="S4.T3.4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.4.1.2.2.4.1" class="ltx_text">
<span id="S4.T3.4.1.2.2.4.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T3.4.1.2.2.4.1.1.1" class="ltx_p">Overall</span>
<span id="S4.T3.4.1.2.2.4.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T3.4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.4.1.2.2.5.1" class="ltx_text">
<span id="S4.T3.4.1.2.2.5.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T3.4.1.2.2.5.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T3.4.1.2.2.5.1.1.2" class="ltx_p">Accuracy</span>
</span></span></th>
<th id="S4.T3.4.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.4.1.2.2.6.1" class="ltx_text">
<span id="S4.T3.4.1.2.2.6.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T3.4.1.2.2.6.1.1.1" class="ltx_p">Class Avg.</span>
<span id="S4.T3.4.1.2.2.6.1.1.2" class="ltx_p">mIoU</span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.1.3.1" class="ltx_tr">
<th id="S4.T3.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PointNeXt</th>
<td id="S4.T3.4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.3.1.2.1" class="ltx_text ltx_font_bold">0.9497</span></td>
<td id="S4.T3.4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.6875</td>
<td id="S4.T3.4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5926</td>
<td id="S4.T3.4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.6236</td>
<td id="S4.T3.4.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.4868</td>
<td id="S4.T3.4.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.4120</td>
</tr>
<tr id="S4.T3.4.1.4.2" class="ltx_tr">
<th id="S4.T3.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PointBERT</th>
<td id="S4.T3.4.1.4.2.2" class="ltx_td ltx_align_center">0.9369</td>
<td id="S4.T3.4.1.4.2.3" class="ltx_td ltx_align_center">0.7246</td>
<td id="S4.T3.4.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">0.6303</td>
<td id="S4.T3.4.1.4.2.5" class="ltx_td ltx_align_center">0.6549</td>
<td id="S4.T3.4.1.4.2.6" class="ltx_td ltx_align_center">0.5085</td>
<td id="S4.T3.4.1.4.2.7" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.4.2.7.1" class="ltx_text ltx_font_bold">0.4533</span></td>
</tr>
<tr id="S4.T3.4.1.5.3" class="ltx_tr">
<th id="S4.T3.4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PointMAE</th>
<td id="S4.T3.4.1.5.3.2" class="ltx_td ltx_align_center">0.9401</td>
<td id="S4.T3.4.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.3.3.1" class="ltx_text ltx_font_bold">0.7289</span></td>
<td id="S4.T3.4.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.4.1.5.3.4.1" class="ltx_text ltx_font_bold">0.6419</span></td>
<td id="S4.T3.4.1.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.3.5.1" class="ltx_text ltx_font_bold">0.6822</span></td>
<td id="S4.T3.4.1.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.3.6.1" class="ltx_text ltx_font_bold">0.5448</span></td>
<td id="S4.T3.4.1.5.3.7" class="ltx_td ltx_align_center">0.4248</td>
</tr>
<tr id="S4.T3.4.1.6.4" class="ltx_tr">
<th id="S4.T3.4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PointGPT</th>
<td id="S4.T3.4.1.6.4.2" class="ltx_td ltx_align_center">0.9336</td>
<td id="S4.T3.4.1.6.4.3" class="ltx_td ltx_align_center">0.7180</td>
<td id="S4.T3.4.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">0.6175</td>
<td id="S4.T3.4.1.6.4.5" class="ltx_td ltx_align_center">0.5794</td>
<td id="S4.T3.4.1.6.4.6" class="ltx_td ltx_align_center">0.5445</td>
<td id="S4.T3.4.1.6.4.7" class="ltx_td ltx_align_center">0.3909</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Confusion matrix for the four studied networks.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T4.4" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:87.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.5pt,10.4pt) scale(0.808181188540614,0.808181188540614) ;">
<table id="S4.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.1.1.1" class="ltx_tr">
<th id="S4.T4.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T4.4.1.1.1.1.1" class="ltx_text ltx_font_bold">PointNeXt</span></th>
</tr>
<tr id="S4.T4.4.1.2.2" class="ltx_tr">
<th id="S4.T4.4.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T4.4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T4.4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T4.4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T4.4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.1.3.1" class="ltx_tr">
<th id="S4.T4.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T4.4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.3.1.2.1" class="ltx_text ltx_font_bold">6566</span></td>
<td id="S4.T4.4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">6229</td>
</tr>
<tr id="S4.T4.4.1.4.2" class="ltx_tr">
<th id="S4.T4.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T4.4.1.4.2.2" class="ltx_td ltx_align_center">68</td>
<td id="S4.T4.4.1.4.2.3" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.4.1.4.2.4" class="ltx_td ltx_align_center">24</td>
<td id="S4.T4.4.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.4.2.5.1" class="ltx_text ltx_font_bold">299</span></td>
</tr>
<tr id="S4.T4.4.1.5.3" class="ltx_tr">
<th id="S4.T4.4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T4.4.1.5.3.2" class="ltx_td ltx_align_center">100</td>
<td id="S4.T4.4.1.5.3.3" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.4.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.5.3.4.1" class="ltx_text ltx_font_bold">6990</span></td>
<td id="S4.T4.4.1.5.3.5" class="ltx_td ltx_align_center">909</td>
</tr>
<tr id="S4.T4.4.1.6.4" class="ltx_tr">
<th id="S4.T4.4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T4.4.1.6.4.2" class="ltx_td ltx_align_center">2391</td>
<td id="S4.T4.4.1.6.4.3" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.4.1.6.4.4" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.4.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.1.6.4.5.1" class="ltx_text ltx_font_bold">3048</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T4.5" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:87.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.5pt,10.4pt) scale(0.808181188540614,0.808181188540614) ;">
<table id="S4.T4.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.5.1.1.1" class="ltx_tr">
<th id="S4.T4.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T4.5.1.1.1.1.1" class="ltx_text ltx_font_bold">PointBERT</span></th>
</tr>
<tr id="S4.T4.5.1.2.2" class="ltx_tr">
<th id="S4.T4.5.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T4.5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T4.5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T4.5.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T4.5.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.5.1.3.1" class="ltx_tr">
<th id="S4.T4.5.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T4.5.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.1.3.1.2.1" class="ltx_text ltx_font_bold">8923</span></td>
<td id="S4.T4.5.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.5.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.5.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">5388</td>
</tr>
<tr id="S4.T4.5.1.4.2" class="ltx_tr">
<th id="S4.T4.5.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T4.5.1.4.2.2" class="ltx_td ltx_align_center">102</td>
<td id="S4.T4.5.1.4.2.3" class="ltx_td ltx_align_center">23</td>
<td id="S4.T4.5.1.4.2.4" class="ltx_td ltx_align_center">40</td>
<td id="S4.T4.5.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.5.1.4.2.5.1" class="ltx_text ltx_font_bold">484</span></td>
</tr>
<tr id="S4.T4.5.1.5.3" class="ltx_tr">
<th id="S4.T4.5.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T4.5.1.5.3.2" class="ltx_td ltx_align_center">140</td>
<td id="S4.T4.5.1.5.3.3" class="ltx_td ltx_align_center">2</td>
<td id="S4.T4.5.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.1.5.3.4.1" class="ltx_text ltx_font_bold">6226</span></td>
<td id="S4.T4.5.1.5.3.5" class="ltx_td ltx_align_center">639</td>
</tr>
<tr id="S4.T4.5.1.6.4" class="ltx_tr">
<th id="S4.T4.5.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T4.5.1.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.5.1.6.4.2.1" class="ltx_text ltx_font_bold">2392</span></td>
<td id="S4.T4.5.1.6.4.3" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.5.1.6.4.4" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.5.1.6.4.5" class="ltx_td ltx_align_center">2265</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T4.6" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:87.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.5pt,10.4pt) scale(0.808181188540614,0.808181188540614) ;">
<table id="S4.T4.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.6.1.1.1" class="ltx_tr">
<th id="S4.T4.6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T4.6.1.1.1.1.1" class="ltx_text ltx_font_bold">PointMAE</span></th>
</tr>
<tr id="S4.T4.6.1.2.2" class="ltx_tr">
<th id="S4.T4.6.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T4.6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T4.6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T4.6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T4.6.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.6.1.3.1" class="ltx_tr">
<th id="S4.T4.6.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T4.6.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.1.3.1.2.1" class="ltx_text ltx_font_bold">9508</span></td>
<td id="S4.T4.6.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.6.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.6.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">4653</td>
</tr>
<tr id="S4.T4.6.1.4.2" class="ltx_tr">
<th id="S4.T4.6.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T4.6.1.4.2.2" class="ltx_td ltx_align_center">71</td>
<td id="S4.T4.6.1.4.2.3" class="ltx_td ltx_align_center">75</td>
<td id="S4.T4.6.1.4.2.4" class="ltx_td ltx_align_center">38</td>
<td id="S4.T4.6.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.1.4.2.5.1" class="ltx_text ltx_font_bold">445</span></td>
</tr>
<tr id="S4.T4.6.1.5.3" class="ltx_tr">
<th id="S4.T4.6.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T4.6.1.5.3.2" class="ltx_td ltx_align_center">108</td>
<td id="S4.T4.6.1.5.3.3" class="ltx_td ltx_align_center">18</td>
<td id="S4.T4.6.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.6.1.5.3.4.1" class="ltx_text ltx_font_bold">6028</span></td>
<td id="S4.T4.6.1.5.3.5" class="ltx_td ltx_align_center">951</td>
</tr>
<tr id="S4.T4.6.1.6.4" class="ltx_tr">
<th id="S4.T4.6.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T4.6.1.6.4.2" class="ltx_td ltx_align_center">2176</td>
<td id="S4.T4.6.1.6.4.3" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.6.1.6.4.4" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.6.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.1.6.4.5.1" class="ltx_text ltx_font_bold">2553</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T4.7" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:214.6pt;height:87.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.5pt,10.4pt) scale(0.808181188540614,0.808181188540614) ;">
<table id="S4.T4.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.7.1.1.1" class="ltx_tr">
<th id="S4.T4.7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5"><span id="S4.T4.7.1.1.1.1.1" class="ltx_text ltx_font_bold">PointGPT</span></th>
</tr>
<tr id="S4.T4.7.1.2.2" class="ltx_tr">
<th id="S4.T4.7.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T4.7.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Terrain</th>
<th id="S4.T4.7.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Trunk</th>
<th id="S4.T4.7.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Canopy</th>
<th id="S4.T4.7.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Understorey</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.7.1.3.1" class="ltx_tr">
<th id="S4.T4.7.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Terrain</th>
<td id="S4.T4.7.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.7.1.3.1.2.1" class="ltx_text ltx_font_bold">7283</span></td>
<td id="S4.T4.7.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T4.7.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T4.7.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">7069</td>
</tr>
<tr id="S4.T4.7.1.4.2" class="ltx_tr">
<th id="S4.T4.7.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Trunk</th>
<td id="S4.T4.7.1.4.2.2" class="ltx_td ltx_align_center">80</td>
<td id="S4.T4.7.1.4.2.3" class="ltx_td ltx_align_center">219</td>
<td id="S4.T4.7.1.4.2.4" class="ltx_td ltx_align_center">25</td>
<td id="S4.T4.7.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.7.1.4.2.5.1" class="ltx_text ltx_font_bold">308</span></td>
</tr>
<tr id="S4.T4.7.1.5.3" class="ltx_tr">
<th id="S4.T4.7.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Canopy</th>
<td id="S4.T4.7.1.5.3.2" class="ltx_td ltx_align_center">137</td>
<td id="S4.T4.7.1.5.3.3" class="ltx_td ltx_align_center">415</td>
<td id="S4.T4.7.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.7.1.5.3.4.1" class="ltx_text ltx_font_bold">5278</span></td>
<td id="S4.T4.7.1.5.3.5" class="ltx_td ltx_align_center">1126</td>
</tr>
<tr id="S4.T4.7.1.6.4" class="ltx_tr">
<th id="S4.T4.7.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Understorey</th>
<td id="S4.T4.7.1.6.4.2" class="ltx_td ltx_align_center">2018</td>
<td id="S4.T4.7.1.6.4.3" class="ltx_td ltx_align_center">17</td>
<td id="S4.T4.7.1.6.4.4" class="ltx_td ltx_align_center">0</td>
<td id="S4.T4.7.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.7.1.6.4.5.1" class="ltx_text ltx_font_bold">2646</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and future work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we developed an open-source simulator based on Unity Engine that generates realistic forest scenes procedurally. With it, we have created synthetic point-based datasets, with each point labeled into one of the categories: terrain, trunk, canopy, and understorey (including grass, bushes, and other vegetation that are not trees). We then employed these datasets to train four state-of-the-art deep-learning point-based networks. Finally, we tested and compared them in the real forest EVO dataset. The results show that synthetic data can be used to train deep-learning networks for posterior forest segmentation with real data. Among the tested networks, PointNeXt seemed to give better results when trained with the LiDAR-like dataset than the other networks, whereas PointMAE obtained slightly better accuracy when trained with the Camera-Like dataset.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">As future work, we will use synthetic data to pre-train the deep learning networks and then fine-tune them using real data, expecting better accuracy. This result is also relevant since it would allow data collection in only a portion of the forest area of interest to conduct deep learning network training.

</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Murtiyoso, A., Holm, S., RiihimÃ¤ki, H., Krucher, A., Griess, H., Griess, V. C., Schweier, J: Virtual forests: a review on emerging questions in the use and application of 3D data in forestry. International Journal of Forest Engineering <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">35</span>(1), 29â€“42 (2024)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Pessacg, F., GÃ³mez-FernÃ¡ndez, F., Nitsche, M., Chamo, N., Torrella, S., Ginzburg, R., De CristÃ³foris, P.: Simplifying UAV-based photogrammetry in forestry: How to generate accurate digital terrain model and assess flight mission settings. Forests <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">13</span>(2), 173 (2022)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
GuimarÃ£es, N., PÃ¡dua, L., Marques, P., Silva, N., Peres, E., Sousa, J. J.: Forestry remote sensing from unmanned aerial vehicles: A review focusing on the data, processing and potentialities. Remote Sensing <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">12</span>(6), 1046 (2020)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.: Deep learning for 3d point clouds: A survey. IEEE transactions on pattern analysis and machine intelligence <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">43</span>(12), 4338â€“4364 (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770â€“778. IEEE Xplore. Las Vegas, United States of America (2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2015: 18th International Conference. Proceedings, Part III 18, pp. 234-241. Springer International Publishing, Munich, Germany (2015)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431â€“3440. IEEE Xplore, Boston, United States of America (2015)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., NieÃŸner, M.: Scannet: Richly-annotated 3d reconstructions of indoor scenes. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5828â€“5839. IEEE Xplore, Honolulu, United States of America (2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Uy, M. A., Pham, Q. H., Hua, B. S., Nguyen, T., Yeung, S. K.: Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In: Proceedings of the IEEE/CVF international conference on computer vision, pp. 1588â€“1597. IEEE Xplore, Seoul, Korea (2019)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A deep representation for volumetric shapes. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912â€“1920. IEEE Xplore. Boston, United States of America (2015)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yi, L., Kim, V. G., Ceylan, D., Shen, I. C., Yan, M., Su, H., Lu, C., Huang, Q., Sheffer, A., Guibas, L.: A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">35</span>(6), 1â€“12 (2016)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.: Semantickitti: A dataset for semantic scene understanding of lidar sequences. In: Proceedings of the IEEE/CVF international conference on computer vision, pp. 9297â€“9307. IEEE Xplore, Seoul, Korea (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jin, S., Su, Y., Zhao, X., Hu, T., Guo, Q.: A point-based fully convolutional neural network for airborne lidar ground point filtering in forested environments. IEEE journal of selected topics in applied earth observations and remote sensing <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">13</span>, 3958â€“3974 (2020)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Krisanski, S., Taskhiri, M. S., Gonzalez Aracil, S., Herries, D., Turner, P.: Sensor agnostic semantic segmentation of structurally diverse and complex forest point clouds using deep learning. Remote Sensing <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">13</span>(8), 1413 (2021)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kaijaluoto, R., Kukko, A., El Issaoui, A., HyyppÃ¤, J., Kaartinen, H.: Semantic segmentation of point cloud data using raw laser scanner measurements and deep neural networks. ISPRS Open Journal of Photogrammetry and Remote Sensing <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">3</span>, 100011 (2022)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Unity Homepage, <a target="_blank" href="https://unity.com/" title="" class="ltx_ref">https://unity.com/</a>. Last accessed 20 Feb 2024

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Pingel, T. J., Clarke, K. C., McBride, W. A.: An improved simple morphological filter for the terrain classification of airborne LIDAR data. ISPRS journal of photogrammetry and remote sensing. ISPRS journal of photogrammetry and remote sensing <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">77</span>, 21-30 (2013)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Vastaranta, M., Wulder, M. A., White, J. C., Pekkarinen, A., Tuominen, S., Ginzler, C., Kankare, V., Holopainen, M., HyyppÃ¤, J., HyyppÃ¤, H.: Airborne laser scanning and digital stereo imagery measures of forest structure: Comparative results and implications to forest mapping and inventory update. Canadian Journal of Remote Sensing <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">39</span>(5), 382â€“395 (2013)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lin, T., Wang, Y., Liu, X., Qiu, X.: A survey of transformers. AI Open <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">3</span>, 111â€“132 (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
OpenAI Homepage, <a target="_blank" href="https://openai.com/" title="" class="ltx_ref">https://openai.com/</a>. Last accessed 7 Feb 2024

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Qi, C. R., Yi, L., Su, H., Guibas, L. J.: Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Thomas, H., Qi, C. R., Deschaud, J. E., Marcotegui, B., Goulette, F., Guibas, L. J.: Kpconv: Flexible and deformable convolution for point clouds. In: Proceedings of the IEEE/CVF international conference on computer vision, pp. 6411â€“6420. IEEE Xplore. Seoul, Korea (2019)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Wu, W., Qi, Z., Fuxin, L.: Pointconv: Deep convolutional networks on 3d point clouds.. In: Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pp. 9621â€“9630. IEEE Xplore. California, United States of America (2019)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Boulch, A.: ConvPoint: Continuous convolutions for point cloud processing. Computers &amp; Graphics <span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">88</span>, 24â€“34 (2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lu, J., Liu, T., Luo, M., Cheng, H., Zhang, K.: PFCN: a fully convolutional network for point cloud semantic segmentation. Electronics Letters <span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">55</span>(20), 1088â€“1090 (2019)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H., Elhoseiny, M., Ghanem, B.: Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Advances in Neural Information Processing Systems <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">35</span>, 23192â€“23204 (2022)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L. C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510â€“4520. IEEE Xplore, Salt Lake City, United States of America (2018)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Qian, G., Hammoud, H. A. A. K., Li, G., Thabet, A. K., Ghanem, B.: Assanet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning. Advances n Neural Information Processing Systems <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">34</span>, 28119â€“28130 (2021)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Engel, N., Belagiannis, V., Dietmayer, K.: Point transformer. IEEE access <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">9</span>, 134826â€“134840 (2021)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zhao, H., Jiang, L., Jia, J., Torr, P. H., Koltun, V.: Point transformer. In: Proceedings of the IEEE/CVF international conference on computer vision, pp. 16259â€“16268. IEEE Xplore, Virtual (2021)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Park, C., Jeong, Y., Cho, M., Park, J.: Fast point transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16949â€“16958. IEEE Xplore, New Orleans, United States of America (2022)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Guo, M. H., Cai, J. X., Liu, Z. N., Mu, T. J., Martin, R. R., Hu, S. M.: Pct: Point cloud transformer. Computational Visual Media <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">7</span>, 187â€“199 (2021)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., Lu, J.: Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19313â€“19322. IEEE Xplore, New Orleans, United States of America (2022)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M. W., Lee, K., Toutanova, K. N.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805 (2016)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Bao, H., Dong, L., Piao, S., Wei, F.: BEiT: BERT Pre-Training of Image Transformers. In: International Conference on Learning Representations (2022)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Rolfe, J. T.: Discrete Variational Autoencoders. In: 9th International Conference on Learning Representations (2017)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Pang, Y., Wang, W., Tay, F. E., Liu, W., Tian, Y., Yuan, L.: Masked autoencoders for point cloud self-supervised learning. In: EComputer Visionâ€“ECCV, pp. 604â€“621. Springer (2022)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000â€“16009. IEEE Xplore, New Orleans, United States of America (2022)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Chen, G., Wang, M., Yang, Y., Yu, K., Yuan, L., Yue, Y.: PointGPT: Auto-regressively Generative Pre-training from Point Clouds. Advances in Neural Information Processing Systems <span id="bib.bib40.1.1" class="ltx_text ltx_font_bold">36</span> (2024)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training. In preprint (2018)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Morton, G. M.: A computer oriented geodetic data base and a new technique in file sequencing. International Business
Machines Company New York (1966)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Gevaert, C. M., Persello, C., Nex, F., Vosselman, G.: A deep learning approach to DTM extraction from imagery using rule-based training labels. ISPRS journal of photogrammetry and remote sensing <span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">142</span>, 106â€“123 (2018)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Amirkolaee, H. A., Arefi, H., Ahmadlou, M., Raikwar, V.: DTM extraction from DSM using a multi-scale DTM fusion strategy based on deep learning. Remote Sensing of Environment <span id="bib.bib44.1.1" class="ltx_text ltx_font_bold">274</span>, 113014 (2022)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Li, S., Xiong, L., Tang, G., Strobl, J.: Deep learning-based approach for landform classification from integrated data sources of digital elevation model and imagery. Geomorphology <span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">354</span>, 107045 (2020)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Hu, X., Yuan, Y.: Deep-learning-based classification for DTM extraction from ALS point cloud. Remote sensing <span id="bib.bib46.1.1" class="ltx_text ltx_font_bold">8</span>(9), 730 (2016)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
LÃª, H. Ã‚., Guiotte, F., Pham, M. T., LefÃ¨vre, S., Corpetti, T.: Learning Digital Terrain Models From Point Clouds: ALS2DTM Dataset and Rasterization-Based GAN. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing <span id="bib.bib47.1.1" class="ltx_text ltx_font_bold">15</span>, 4980â€“4989 (2022)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Li, B., Lu, H., Wang, H., Qi, J., Yang, G., Pang, Y., Dong, H., Lian, Y.: Terrain-Net: A Highly-Efficient, Parameter-Free, and Easy-to-Use Deep Neural Network for Ground Filtering of UAV LiDAR Data in Forested Environments. Remote Sensing <span id="bib.bib48.1.1" class="ltx_text ltx_font_bold">14</span>(22), 5798 (2022)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Nunes, R., Ferreira, J. F., Peixoto, P.: Procedural generation of synthetic forest environments to train machine learning algorithms. In: ICRA 2022 Workshop in Innovation in Forestry Robotics: Research and Industry Adoption (2022)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
SynPhoRest Dataset Homepage, <a target="_blank" href="https://zenodo.org/records/6369446" title="" class="ltx_ref">https://zenodo.org/records/6369446</a>. Last accessed 21 Feb 2024

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Russell, D. J., Arevalo-Ramirez, T., Garg, C., Kuang, W., Yandun, F., Wettergreen, D., Kantor, G.: UAV Mapping with Semantic and Traversability Metrics for Forest Fire Mitigation. In: ICRA 2022 Workshop in Innovation in Forestry Robotics: Research and Industry Adoption (2022)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Bridson, R.: Fast Poisson disk sampling in arbitrary dimensions. SIGGRAPH sketches <span id="bib.bib52.1.1" class="ltx_text ltx_font_bold">10</span>(1), 1 (2007)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
TreeIt Homepage, <a target="_blank" href="http://www.evolved-software.com/treeit/treeit" title="" class="ltx_ref">http://www.evolved-software.com/treeit/treeit</a>. Last accessed 27 Feb 2024

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Katz, S., Tal, A., Basri, R.: Direct visibility of point sets. ACM Transactions on Graphics <span id="bib.bib54.1.1" class="ltx_text ltx_font_bold">26</span>(3), 24 (2007)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.14114" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.14115" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.14115">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.14115" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.14116" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 15:36:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
