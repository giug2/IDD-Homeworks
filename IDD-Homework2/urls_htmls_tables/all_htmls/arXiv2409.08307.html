<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation</title>
<!--Generated on Fri Oct  4 20:15:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Biomedical Image Processing Mamba Deep Learning Semantic Segmentation Neuroimaging" lang="en" name="keywords"/>
<base href="/html/2409.08307v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S1" title="In MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2" title="In MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Materials and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS1" title="In 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS2" title="In 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Model Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS2.SSS1" title="In 2.2 Model Architecture ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>3D Selective Scan (SS3D)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS2.SSS2" title="In 2.2 Model Architecture ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>VSS3D block</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS2.SSS3" title="In 2.2 Model Architecture ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Overall Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS3" title="In 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS4" title="In 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Model Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.SS5" title="In 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Model Evaluation and Statistical Test</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S3" title="In MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S4" title="In MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S4.SS1" title="In 4 Discussion ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S4.SS2" title="In 4 Discussion ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Conclusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S5" title="In MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Acknowledgments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of California, Santa Barbara, Santa Barbara, CA, USA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Department of Biomedical Engineering, Columbia University, New York, NY, USA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Department of Psychiatry, Columbia University, New York, NY, USA </span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA</span></span></span>
<h1 class="ltx_title ltx_title_document">MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aaron Cao 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zongyu Li
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jia Guo
</span><span class="ltx_author_notes">Correspondence: Jia Guo, jg3400@columbia.edu223344</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Widely used traditional pipelines for subcortical brain segmentation are often inefficient and slow, particularly when processing large datasets. Furthermore, deep learning models face challenges due to the high resolution of MRI images and the large number of anatomical classes involved. To address these limitations, we developed a 3D patch-based hybrid CNN-Mamba model that leverages Mamba’s selective scan algorithm, thereby enhancing segmentation accuracy and efficiency for 3D inputs.
This retrospective study utilized 1784 T1-weighted MRI scans from a diverse, multi-site dataset of healthy individuals. The dataset was divided into training, validation, and testing sets with a 1076/345/363 split. The scans were obtained from 1.5T and 3T MRI machines.
Our model’s performance was validated against several benchmarks, including other CNN-Mamba, CNN-Transformer, and pure CNN networks, using FreeSurfer-generated ground truths. We employed the Dice Similarity Coefficient (DSC), Volume Similarity (VS), and Average Symmetric Surface Distance (ASSD) as evaluation metrics. Statistical significance was determined using the Wilcoxon signed-rank test with a threshold of <span class="ltx_text ltx_font_italic" id="id1.id1.1">P</span> &lt; 0.05.
The proposed model achieved the highest overall performance across all metrics (DSC 0.88383; VS 0.97076; ASSD 0.33604), significantly outperforming all non-Mamba-based models (<span class="ltx_text ltx_font_italic" id="id1.id1.2">P</span> &lt; 0.001). While the model did not show significant improvement in DSC or VS compared to another Mamba-based model (<span class="ltx_text ltx_font_italic" id="id1.id1.3">P</span>-values of 0.114 and 0.425), it demonstrated a significant enhancement in ASSD (<span class="ltx_text ltx_font_italic" id="id1.id1.4">P</span> &lt; 0.001) with approximately 20% fewer parameters.
In conclusion, our proposed hybrid CNN-Mamba architecture offers an efficient and accurate approach for 3D subcortical brain segmentation, demonstrating potential advantages over existing methods. Code is available at: <a class="ltx_ref" href="https://github.com/aaroncao06/MedSegMamba" title="">https://github.com/aaroncao06/MedSegMamba</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Biomedical Image Processing Mamba Deep Learning Semantic Segmentation Neuroimaging
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Subcortical brain segmentation is a significant application in medical image processing, as it enables the extraction of quantitative structural information on subcortical regions within MRI scans. This can aid in detecting and tracking morphological deficits in various neuropsychiatric conditions, including Schizophrenia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib9" title="">9</a>]</cite>, Major Depressive Disorder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib14" title="">14</a>]</cite>, and Dementia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib28" title="">28</a>]</cite>. However, achieving accurate brain segmentation has remained a challenging task due to the intricate 3D structures within the brain, the large number of anatomical labels, and the substantial computational resources required to process scans at full resolution.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While manual segmentation stands as the most trusted method, it is a labor intensive and difficult task, even for experienced clinicians. Automated tools like FreeSurfer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib6" title="">6</a>]</cite> have been developed to address these challenges, providing a widely accepted standard for subcortical segmentation. Despite its widespread use, FreeSurfer’s traditional methods can be slow—requiring up to hours to process a single scan—and are often sensitive to data quality issues, which limits their efficiency when dealing with large, heterogeneous datasets.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">One of the leading deep learning-based alternatives to FreeSurfer is the FastSurfer pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib12" title="">12</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib13" title="">13</a>]</cite>, which is capable of performing whole-brain level segmentations. For a 2.5D approach, FastSurfer aggregates three 2D fully convolutional neural networks which utilize the classic encoder-decoder structure originating from the U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib24" title="">24</a>]</cite>. While FastSurfer has demonstrated superior performance compared to models such as 3D U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib1" title="">1</a>]</cite>, QuickNAT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib8" title="">8</a>]</cite>, and SDNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib25" title="">25</a>]</cite>, its reliance on 2D models inherently limits its ability to capture the full 3D spatial dependencies of the brain’s anatomical structures.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">On the other hand, 3D patch-based solutions are better suited to capture such geometries. While full 3D volume deep learning models for segmenting many classes are currently not feasible due to data and memory constraints, a patch-based approach is more computationally efficient. It also generates more training samples per subject and better captures local 3D information.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Additionally, traditional pure CNN architectures like used in FastSurfer can suffer from the local receptive fields in each layer, making them more prone to missing out on the full global context in 3D. The Vision Transformer (ViT) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib3" title="">3</a>]</cite> addresses this limitation by employing a self-attention mechanism—originally developed for natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib27" title="">27</a>]</cite>—that achieves state-of-the-art performance in image recognition without relying on convolutions. Each self-attention layer has a global receptive field, enabling the model to extract deeper long-range spatial dependencies. For medical image segmentation tasks, hybrid CNN-Transformer architectures modeled after the U-Net have become a popular choice, showing improved generalization and performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib2" title="">2</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib30" title="">30</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib10" title="">10</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">However, the computational cost of Transformers scales quadratically with sequence length, which leads to substantial hardware memory requirements. This makes their application to dense prediction tasks challenging, particularly for large, high-resolution biomedical images. A shifted window-based self-attention approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib17" title="">17</a>]</cite> can mitigate this issue by reducing the computational burden, enabling the use of pure Transformer models. However, it also restricts the receptive fields in each layer, which may result in the loss of global relationships that ViT was designed to capture.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Recent advancements in state space models offer a promising alternative. Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib7" title="">7</a>]</cite> introduces the selective scan mechanism (S6) with a hardware aware algorithm to enhance both training and inference efficiency. This allows the model to scale linearly with sequence length while maintaining state-of-the-art performance across various long-sequence processing tasks. Since its introduction, Mamba has been adapted to numerous vision tasks, consistently demonstrating competitive results compared to both Transformer and CNN-based architectures, but with improved memory and parameter efficiency. In 2D applications, Vision Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib34" title="">34</a>]</cite> introduces the Vim block, which incorporates a bidirectional State-Space Model (SSM) combined with positional embeddings. Meanwhile, VMamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib16" title="">16</a>]</cite> introduced the Visual State-Space (VSS) block, centered around the 2D-Selective-Scan (SS2D) module. Because the visual components in images are not sequential like language, the Vim block unravels the image in 2 ways (forward and backward) and the SS2D block unravels it along 4 traversal paths, to mitigate any unidirectional bias in the final outputs. MambaAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib11" title="">11</a>]</cite> demonstrates that processing additional traversal paths within each Mamba module can enhance performance in 2D anomaly detection tasks. However, in the realm of 3D segmentation, current methods do not fully exploit the 3D context. For example, U-Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib19" title="">19</a>]</cite> unravels the image in only one direction, and SegMamba’s Tri-Oriented Mamba module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib32" title="">32</a>]</cite> unfolds the image along just three directions.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In this paper, we introduce MedSegMamba, a hybrid CNN-Mamba architecture that fully leverages Mamba’s selective scan algorithm for 3D inputs, aiming to enhance subcortical brain segmentation.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This study did not require ethical approval as it only utilized publicly available MRI scans previously acquired for studies approved by local institutional review boards, research ethics committees, or human investigation committees.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pipeline</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Our pipeline follows a 3D patch-based approach with a hybrid CNN-Mamba model. Initially, the input scans are centered and conformed to LIA orientation, followed by intensity rescaling from 0 to 1, consistent with the steps in the FastSurfer pipeline. These input volumes with dimensions 256 x 256 x 256 are cropped and padded before patch extraction. Patches are then generated with dimensions 96 x 96 x 96, with a step size of 16 voxels between consecutive patches. Each patch is sequentially fed into the model, and the output class probabilities are reconstructed to match the original input image dimensions. The predicted probabilities for each patch are combined through a voting mechanism to determine the class for each voxel, with the final values mapped to the corresponding FreeSurfer labels. This pipeline, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.F1" title="Figure 1 ‣ 2.1 Pipeline ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, ensures that the model can segment an entire scan into 32 classes in under 90 seconds (all 31 subcortical structures covered by FastSurferVINN excluding cortical white matter).</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="350" id="S2.F1.g1" src="x1.png" width="623"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Our pipeline extracts 3D patches from the input scan, feeds them into our model, and reconstructs the output predicted label maps to generate the subcortical segmentation of the input scan.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model Architecture</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>3D Selective Scan (SS3D)</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The selected 3D scanning pattern, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.F2" title="Figure 2 ‣ 2.2.1 3D Selective Scan (SS3D) ‣ 2.2 Model Architecture ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, can unravel the volume along 48 unique paths. Our core SS3D module is designed to fully exploit each possible unfolded sequence, with each module processing one of 6 possible groups of 8 sequences.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">First, the input volume’s axes are transposed in one of the 6 possible ways (labeled o0 to o5), determined by the assigned orientation index of the SS3D module. The eight sequences are then extracted from the volume by rotating the volume in three different ways, unfolding it along the scanning pattern, and reversing the sequences. These eight sequences are processed independently and in parallel by the S6 blocks, with their outputs reordered and merged to reconstruct the final output volume.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">Each method of unraveling the volume (48 in total) is intended to train an independent S6 block to capture the unique geometries best represented by that specific sequence order.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1">For the S6 blocks, we increase the SSM state dimension from the standard 16 to 64. Although this adjustment slightly reduces processing speed, it enables the model to extract more detailed information about complex structures and possible patch locations, with minimal impact on parameters and memory usage. The SSM feature expansion factor is kept at 1, as the CNN encoder before the bottleneck has already expanded the feature dimension to 1024.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="171" id="S2.F2.sf1.g1" src="x2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F2.sf1.3.2" style="font-size:90%;">SS3D layout</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="554" id="S2.F2.sf2.g1" src="x3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F2.sf2.3.2" style="font-size:90%;">Scanning pattern</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">SS3D overview</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>VSS3D block</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Our 3D Visual State Space (VSS3D) block resembles a traditional Transformer block, with self-attention replaced by our SS3D module, inspired by VMamba. It consists of two residual modules. The first module includes a sequence of layer normalization, linear projection, depth-wise convolution, SiLU activation, SS3D, followed by another layer normalization and linear projection. The second residual module consists of a layer normalization followed by a multilayer perceptron (MLP). This is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.F3" title="Figure 3 ‣ 2.2.2 VSS3D block ‣ 2.2 Model Architecture ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="103" id="S2.F3.g1" src="x4.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">VSS3D block</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Overall Architecture</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">The overall model architecture features a 3D CNN encoder and decoder with skip connections, and a series of VSS3D blocks as the bottleneck. Passing through the encoder, four layers of residual blocks and 3 max pooling operations downsample the input patch for an encoded feature tensor. A subsequent convolution brings the channel size to 1024, before the tensor is fed into the bottleneck. The bottleneck comprises a sequence of nine VSS3D blocks, each assigned an orientation index for its inner SS3D module. The output of the bottleneck is normalized and then passed to the decoder, which reconstructs the image to its original input dimensions. Finally, a convolution operation followed by a Softmax activation function are applied to generate a 32-channel output, where each channel represents the probability of an individual class. Each residual block within the encoder and decoder layers consists of a residual connection and two sequences of 3D Convolution, Group Normalization, and Rectified Linear Unit (ReLU). This is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.F4" title="Figure 4 ‣ 2.2.3 Overall Architecture ‣ 2.2 Model Architecture ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S2.F4.g1" src="x5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">MedSegMamba model architecture. Only 6 VSS3D layers are shown here, but the model contains 9 in total. The last Conv1x1 and Softmax layers are also not shown.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">A total of 1784 T1-weighted (T1w) MRI scans were selected from a large-scale heterogeneous dataset representing a uniformly healthy population, compiled from multiple publicly available sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib5" title="">5</a>]</cite>. All scans were acquired at a resolution of 1mm x 1mm x 1mm. Selected subjects came from the Australian Imaging Biomarkers and Lifestyle Study of Ageing (AIBL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib4" title="">4</a>]</cite>, Frontotemporal Lobar Degeneration Neuroimaging Initiative (NIFD), Information eXtraction from Images (IXI), Open Access Series of Imaging Studies-1 (OASIS-1) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib21" title="">21</a>]</cite>, Open Access Series of Imaging Studies-2 (OASIS-2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib20" title="">20</a>]</cite>, Southwest University Adult life-span Dataset (SALD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib31" title="">31</a>]</cite>, Southwest University Longitudinal Imaging Multimodal Brain Data Repository (SLIM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib15" title="">15</a>]</cite>, Parkinson’s Progression Markers Initiative (PPMI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib22" title="">22</a>]</cite>, SchizConnect (SchizConnect) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib29" title="">29</a>]</cite>, and Consortium for Reliability and Reproducibility (CoRR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S2.F5.g1" src="extracted/5902845/figs/dataset_agegender_distributions_combined.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;">Age and Gender Distributions for each Dataset</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The dataset was partitioned into training, validation, and test sets with a roughly 3:1:1 ratio. Specifically, the training set included 1076 scans, the validation set contained 345 scans, and the test set comprised 363 scans. A balanced age and gender distribution was achieved across the datasets, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S2.F5" title="Figure 5 ‣ 2.3 Data ‣ 2 Materials and Methods ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>. Ground truth segmentations were generated using FreeSurfer, and the input T1w scans were preprocessed with skull-stripping and intensity normalization.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Model Training</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The described model was trained and evaluated on a 24 GB NVIDIA Quadro 6000 GPU. We employed the AdamW optimizer in conjunction with a Cosine Annealing Warm Restarts learning rate scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib18" title="">18</a>]</cite>. The loss function used was a combination of Dice Loss and Weighted Cross Entropy during the first epoch, followed by Dice Loss alone in subsequent epochs. Training was conducted on 27 patches per scan (three steps per dimension), utilizing gradient accumulation to simulate each image as a single batch, accounting for the variability in class presence across patches. A dropout rate of 0.1 and a drop path rate of 0.3 were applied within the bottleneck.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Model Evaluation and Statistical Test</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">To evaluate the efficacy of our novel SS3D module over other 3D mamba-based vision modules for latent space learning, we also trained a model identical to our MedSegMamba architecture, except substituting the first residual module in each VSS3D block with SegMamba’s Tri-oriented Mamba module. The resulting block is identical to SegMamba’s Tri-oriented Spatial Mamba block except excluding the Gated Spatial Convolution module, in order to focus specifically on evaluating the core Mamba modules. This model, referred to as SegMambaBot, was trained following the same procedure as MedSegMamba.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">MedSegMamba was also evaluated against a retrained TABSurfer and pretrained FastSurferVINN (obtained from the FastSurfer GitHub repository) using 363 FreeSurfer segmentations as ground truths. Both SegMambaBot and TABSurfer were trained using 3D patch-based approaches with input patch sizes identical to those used in MedSegMamba.</p>
</div>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1">The evaluation metrics included the Dice Similarity Coefficient (DSC), Volume Similarity (VS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib26" title="">26</a>]</cite>, and Average Symmetric Surface Distance (ASSD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#bib.bib33" title="">33</a>]</cite>, assessing both the overall similarity of the segmentations and the contour quality relative to the ground truth. The Wilcoxon signed-rank test was employed to assess the significance of improvements in MedSegMamba’s performance over the other models for each metric. Statistical significance was set to <span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1">P</span> &lt; 0.05.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="631" id="S3.F6.g1" src="x6.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">3D renderings for a sample segmented by each method.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The average metrics from evaluating MedSegMamba, SegMambaBot, TABSurfer, and FastSurferVINN against the FreeSurfer-generated ground truths are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S3.T1" title="Table 1 ‣ 3 Results ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. MedSegMamba consistently achieved high scores in Dice Similarity Coefficient (DSC), Volume Similarity (VS), and Average Symmetric Surface Distance (ASSD) across all datasets, demonstrating superior overall performance. MedSegMamba’s improvement was statistically significant across all metrics, except for the DSC and VS metrics when compared to SegMamba (<span class="ltx_text ltx_font_italic" id="S3.p1.1.1">P</span>-values of 0.114 and 0.425, respectively).</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">All three 3D patch-based methods also significantly outperformed the pretrained 2.5D FastSurfer benchmark. Furthermore, both Mamba-based models significantly outperformed TABSurfer, their transformer-based counterpart.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.2" style="padding-left:7.0pt;padding-right:7.0pt;">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.3" style="padding-left:7.0pt;padding-right:7.0pt;">DSC ↑</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.4" style="padding-left:7.0pt;padding-right:7.0pt;">VS ↑</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.5" style="padding-left:7.0pt;padding-right:7.0pt;">ASSD ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.2.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">AIBL</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.2.1.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.1.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.2.1.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.1.3.1">0.89593±0.009</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.2.1.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.1.4.1">0.97406±0.006</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.2.1.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.29069±0.042</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.2">
<td class="ltx_td" id="S3.T1.2.3.2.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.3.2.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.3.2.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.89580±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.3.2.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97396±0.006</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.3.2.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.2.5.1">0.29038±0.040</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.3">
<td class="ltx_td" id="S3.T1.2.4.3.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.3.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.3.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.89094±0.010</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.3.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97082±0.007</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.3.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.30257±0.044</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.4">
<td class="ltx_td" id="S3.T1.2.5.4.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.4.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.4.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87856±0.015</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.4.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96485±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.4.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.33453±0.059</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.6.5.1" style="padding-left:7.0pt;padding-right:7.0pt;">CoRR</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.6.5.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.5.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.6.5.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.5.3.1">0.88635±0.020</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.6.5.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.5.4.1">0.97123±0.008</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.6.5.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.5.5.1">0.31922±0.078</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.6">
<td class="ltx_td" id="S3.T1.2.7.6.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.6.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.6.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88627±0.020</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.6.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97040±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.6.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.32062±0.077</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8.7">
<td class="ltx_td" id="S3.T1.2.8.7.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.7.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.7.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87949±0.020</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.7.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96765±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.7.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.33669±0.074</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.9.8">
<td class="ltx_td" id="S3.T1.2.9.8.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.9.8.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.9.8.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.86607±0.027</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.9.8.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.95989±0.016</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.9.8.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.38024±0.104</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.10.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.10.9.1" style="padding-left:7.0pt;padding-right:7.0pt;">IXI</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.10.9.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.9.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.10.9.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.9.3.1">0.86773±0.023</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.10.9.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.9.4.1">0.96485±0.010</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.10.9.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.10.9.5.1">0.41766±0.102</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.11.10">
<td class="ltx_td" id="S3.T1.2.11.10.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.11.10.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.11.10.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.86729±0.023</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.11.10.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96460±0.010</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.11.10.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.42425±0.106</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.12.11">
<td class="ltx_td" id="S3.T1.2.12.11.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.12.11.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.12.11.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.85877±0.027</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.12.11.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96023±0.011</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.12.11.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.43849±0.108</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.13.12">
<td class="ltx_td" id="S3.T1.2.13.12.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.13.12.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.13.12.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.81217±0.034</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.13.12.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.93340±0.018</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.13.12.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.61388±0.140</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.14.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.14.13.1" style="padding-left:7.0pt;padding-right:7.0pt;">NIFD</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.14.13.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.14.13.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.14.13.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.14.13.3.1">0.90060±0.007</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.14.13.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97734±0.006</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.14.13.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.14.13.5.1">0.26797±0.030</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.15.14">
<td class="ltx_td" id="S3.T1.2.15.14.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.15.14.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.15.14.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.90026±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.15.14.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.15.14.4.1">0.97745±0.005</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.15.14.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.27077±0.032</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.16.15">
<td class="ltx_td" id="S3.T1.2.16.15.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.16.15.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.16.15.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.89260±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.16.15.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97361±0.005</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.16.15.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.29188±0.035</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.17.16">
<td class="ltx_td" id="S3.T1.2.17.16.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.17.16.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.17.16.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88791±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.17.16.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97240±0.005</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.17.16.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.30494±0.030</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.18.17">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.18.17.1" style="padding-left:7.0pt;padding-right:7.0pt;">OAS1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.18.17.2" style="padding-left:7.0pt;padding-right:7.0pt;">MedSegMamba</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.18.17.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88954±0.012</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.18.17.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97351±0.006</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.18.17.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.30856±0.050</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.19.18">
<td class="ltx_td" id="S3.T1.2.19.18.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.19.18.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.19.18.2.1">SegMambaBot</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.19.18.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.19.18.3.1">0.88977±0.012</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.19.18.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.19.18.4.1">0.97387±0.007</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.19.18.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.19.18.5.1">0.30839±0.050</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.20.19">
<td class="ltx_td" id="S3.T1.2.20.19.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.20.19.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.20.19.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88305±0.011</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.20.19.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96808±0.005</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.20.19.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.32129±0.041</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.21.20">
<td class="ltx_td" id="S3.T1.2.21.20.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.21.20.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.21.20.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87504±0.010</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.21.20.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96240±0.005</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.21.20.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.34141±0.047</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.22.21">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.22.21.1" style="padding-left:7.0pt;padding-right:7.0pt;">OAS2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.22.21.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.22.21.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.22.21.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.22.21.3.1">0.88680±0.013</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.22.21.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97017±0.009</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.22.21.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.22.21.5.1">0.31079±0.047</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.23.22">
<td class="ltx_td" id="S3.T1.2.23.22.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.23.22.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.23.22.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88670±0.012</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.23.22.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.23.22.4.1">0.97048±0.008</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.23.22.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.31382±0.047</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.24.23">
<td class="ltx_td" id="S3.T1.2.24.23.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.24.23.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.24.23.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88247±0.013</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.24.23.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96611±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.24.23.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.32258±0.047</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.25.24">
<td class="ltx_td" id="S3.T1.2.25.24.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.25.24.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.25.24.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87984±0.013</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.25.24.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96454±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.25.24.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.32390±0.049</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.26.25">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.26.25.1" style="padding-left:7.0pt;padding-right:7.0pt;">PPMI</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.26.25.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.26.25.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.26.25.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.26.25.3.1">0.89373±0.007</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.26.25.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97345±0.005</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.26.25.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.26.25.5.1">0.29935±0.037</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.27.26">
<td class="ltx_td" id="S3.T1.2.27.26.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.27.26.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.27.26.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.89333±0.007</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.27.26.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.27.26.4.1">0.97422±0.005</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.27.26.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.30096±0.036</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.28.27">
<td class="ltx_td" id="S3.T1.2.28.27.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.28.27.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.28.27.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88941±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.28.27.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97158±0.006</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.28.27.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.30483±0.032</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.29.28">
<td class="ltx_td" id="S3.T1.2.29.28.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.29.28.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.29.28.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87892±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.29.28.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96673±0.006</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.29.28.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.32847±0.033</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.30.29">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.30.29.1" style="padding-left:7.0pt;padding-right:7.0pt;">SALD</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.30.29.2" style="padding-left:7.0pt;padding-right:7.0pt;">MedSegMamba</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.30.29.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87785±0.028</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.30.29.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96998±0.009</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.30.29.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.35457±0.094</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.31.30">
<td class="ltx_td" id="S3.T1.2.31.30.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.31.30.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.31.30.2.1">SegMambaBot</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.31.30.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.31.30.3.1">0.87795±0.028</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.31.30.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.31.30.4.1">0.97015±0.009</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.31.30.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.31.30.5.1">0.35236±0.093</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.32.31">
<td class="ltx_td" id="S3.T1.2.32.31.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.32.31.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.32.31.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87046±0.027</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.32.31.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96560±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.32.31.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.37473±0.092</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.33.32">
<td class="ltx_td" id="S3.T1.2.33.32.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.33.32.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.33.32.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.84189±0.021</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.33.32.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.94426±0.014</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.33.32.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.48194±0.089</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.34.33">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.34.33.1" style="padding-left:7.0pt;padding-right:7.0pt;">Schiz</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.34.33.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.34.33.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.34.33.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88204±0.011</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.34.33.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.34.33.4.1">0.97118±0.007</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.34.33.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.34.33.5.1">0.33188±0.044</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.35.34">
<td class="ltx_td" id="S3.T1.2.35.34.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.35.34.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.35.34.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.35.34.3.1">0.88205±0.011</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.35.34.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97082±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.35.34.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.33257±0.042</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.36.35">
<td class="ltx_td" id="S3.T1.2.36.35.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.36.35.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.36.35.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87613±0.011</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.36.35.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96747±0.007</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.36.35.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.34891±0.053</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.37.36">
<td class="ltx_td" id="S3.T1.2.37.36.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.37.36.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.37.36.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.83832±0.022</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.37.36.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.94248±0.015</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.37.36.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.48469±0.094</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.38.37">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.38.37.1" style="padding-left:7.0pt;padding-right:7.0pt;">SLIM</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.38.37.2" style="padding-left:7.0pt;padding-right:7.0pt;">MedSegMamba</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.38.37.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88692±0.006</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.38.37.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97101±0.006</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.38.37.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.38.37.5.1">0.30611±0.024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.39.38">
<td class="ltx_td" id="S3.T1.2.39.38.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.39.38.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.39.38.2.1">SegMambaBot</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.39.38.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.39.38.3.1">0.88740±0.007</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.39.38.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.39.38.4.1">0.97169±0.006</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.39.38.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.30679±0.025</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.40.39">
<td class="ltx_td" id="S3.T1.2.40.39.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.40.39.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.40.39.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88254±0.006</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.40.39.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96972±0.005</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.40.39.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.31617±0.022</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.41.40">
<td class="ltx_td" id="S3.T1.2.41.40.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.41.40.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.41.40.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.85483±0.012</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.41.40.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.94886±0.008</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.41.40.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.42471±0.051</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.42.41">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.42.41.1" style="padding-left:7.0pt;padding-right:7.0pt;">Overall</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.42.41.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.42.41.2.1">MedSegMamba</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.42.41.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.42.41.3.1">0.88383±0.021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.42.41.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.42.41.4.1">0.97076±0.009</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.42.41.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.42.41.5.1">0.33604±0.086</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.43.42">
<td class="ltx_td" id="S3.T1.2.43.42.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.43.42.2" style="padding-left:7.0pt;padding-right:7.0pt;">SegMambaBot</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.43.42.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.88372±0.021</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.43.42.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.97068±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.43.42.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.33761±0.087</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.44.43">
<td class="ltx_td" id="S3.T1.2.44.43.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.44.43.2" style="padding-left:7.0pt;padding-right:7.0pt;">TABSurfer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.44.43.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.87711±0.022</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.44.43.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.96684±0.009</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.44.43.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.35264±0.088</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.45.44">
<td class="ltx_td ltx_border_bb" id="S3.T1.2.45.44.1" style="padding-left:7.0pt;padding-right:7.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.45.44.2" style="padding-left:7.0pt;padding-right:7.0pt;">FastSurfer</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.45.44.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.85350±0.035</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.45.44.4" style="padding-left:7.0pt;padding-right:7.0pt;">0.95229±0.018</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.45.44.5" style="padding-left:7.0pt;padding-right:7.0pt;">0.43554±0.143</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Comparing MedSegMamba, SegMambaBot, TABSurfer, and FastsurferVINN metrics across datasets. Bold text indicates superior performance. ↑ indicates that higher numbers correspond to better performance and ↓ indicates that lower numbers correspond to better performance.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Views of 3D renderings and 2D slices of segmentations produced by each method on a sample subject are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S3.F6" title="Figure 6 ‣ 3 Results ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S3.F7" title="Figure 7 ‣ 3 Results ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">7</span></a>. Although FreeSurfer’s atlas-based method yielded the noisiest segmentation, all deep learning methods produced smoother contours. FastSurfer’s 2.5D method retained the most noise from the FreeSurfer output, whereas the 3D patch-based methods achieved similarly smooth segmentations, differing only in minor areas for this sample. However, SegMamba exhibited slight under-segmentation in certain regions, and TABSurfer demonstrated less consistency across different scans.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">While both Mamba-based models showed competitive performance, MedSegMamba demonstrated slight improvements over SegMambaBot, and has approximately 20% fewer parameters while requiring similar GPU memory during inference (1.988 GiB for MedSegMamba vs. 1.932 GiB for SegMambaBot).</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08307v2#S3.F8" title="Figure 8 ‣ 3 Results ‣ MedSegMamba: 3D CNN-Mamba Hybrid Architecture for Brain Segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>, MedSegMamba demonstrates superior performance while remaining parameter efficient compared to the other 3D patch-based methods. While FastSurfer’s fully convolutional architecture requires very few parameters, each of its three sub-models requires more than double the GPU memory during inference (just under 5 GiB) compared to the 3D models, all of which operate with under 2 GiB of GPU memory.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="591" id="S3.F7.g1" src="x7.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S3.F7.3.2" style="font-size:90%;">2D slices of a sample segmented by each method.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="S3.F8.g1" src="extracted/5902845/figs/parametersvsdice_withfastsurfer.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S3.F8.3.2" style="font-size:90%;">Plot of millions of parameters versus overall dice score for each model.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This study introduces the SS3D module and VSS3D block, an extension of Mamba’s selective scan operation tailored for complex 3D image processing tasks. Integrated within the MedSegMamba hybrid CNN-Mamba architecture, the VSS3D block demonstrates robust latent-space learning capabilities, outperforming existing traditional and deep learning approaches across multiple datasets.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The transition from a Transformer bottleneck to a Mamba-based bottleneck enhances memory efficiency, enabling the use of larger convolutional layers on identical hardware. This enhancement boosts local feature extraction capabilities while preserving global context extraction, resulting in both SegMambaBot and MedSegMamba outperforming TABSurfer. Comparing SegMambaBot and MedSegMamba, the SS3D module allows MedSegMamba to attain comparable overall performance in terms of DSC and VS metrics but offers superior region boundary delineation as reflected by the ASSD, all while utilizing significantly fewer parameters than the Tri-oriented Mamba module. Increasing the number of selective scan operations to process variously unraveled sequences can achieve performance parity with larger modules that process fewer sequences.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Compared to FastSurfer’s standard 2.5D approach, our 3D inputs preserve more intricate spatial relationships within the continuity of anatomy compared to 2D slices. The 3D strided reconstruction process, which employs voting on the predicted class, further mitigates noise artifacts present in the ground truth. This improvement is observed qualitatively when comparing the noisier segmentations of FastSurfer to the smoother outputs of the 3D patch-based models.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Limitations</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The primary limitation of this approach is that MedSegMamba exhibits slower performance than SegMambaBot, despite its lower parameter count. However, this issue can be mitigated by increasing the step size during the strided reconstruction process from 16 to 32, resulting in a roughly 4x speedup, reducing the processing time per scan to around 22 seconds with minimal impact on performance. When both using a step size of 32, MedSegMamba is only a few seconds slower than SegMambaBot per scan, making them roughly comparable in terms of overall efficiency while retaining the superior performance afforded by the SS3D module.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conclusion</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">These results showcase the advantages of our hybrid architecture combined with a 3D patch-based approach. Future research should explore the application of our SS3D module across different architectures, including its integration at various latent space levels throughout the encoder, as seen in SegMamba’s original design. Ablation studies focused on determining the optimal number of scanning directions or unraveling methods for 3D segmentation may further enhance our model’s performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">No funding was received for conducting this study and there are no relevant financial or non-financial interests to disclose.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf: 3d u-net: Learning dense volumetric segmentation from sparse annotation. In: Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. pp. 424–432. Springer (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. ICLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ellis, K.A., Bush, A.I., Darby, D., De Fazio, D., Foster, J., Hudson, P., Lautenschlager, N.T., Lenzo, N., Martins, R.N., Maruff, P., et al.: The australian imaging, biomarkers and lifestyle (aibl) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of alzheimer’s disease. International psychogeriatrics <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">21</span>(4), 672–687 (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Feng, X., Lipton, Z.C., Yang, J., Small, S.A., Provenzano, F.A.: Estimating brain age based on a uniform healthy population with deep learning and structural magnetic resonance imaging. Neurobiology of Aging <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">91</span>, 15–25 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Fischl, B., Salat, D.H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., van der Kouwe, A., Killiany, R., Kennedy, D., Klaveness, S., et al.: Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain. Neuron <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">33</span>(3), 341–355 (2002)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Guha Roy, A., Conjeti, S., Navab, N., Wachinger, C.: Quicknat: A fully convolutional network for quick and accurate segmentation of neuroanatomy. NeuroImage <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">186</span>, 713–727 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Gutman, B.A., Van Erp, T.G., Alpert, K., Ching, C.R.K., Isaev, D., Ragothaman, A., Jahanshad, N., Saremi, A., Zavaliangos-Petropulu, A., Glahn, D.C., et al.: A meta-analysis of deep brain structural shape and asymmetry abnormalities in 2,833 individuals with schizophrenia compared with 3,929 healthy volunteers via the enigma consortium. Human Brain Mapping <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">43</span>(1), 352–372 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 574–584 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
He, H., Bai, Y., Zhang, J., He, Q., Chen, H., Gan, Z., Wang, C., Li, X., Tian, G., Xie, L.: Mambaad: Exploring state space models for multi-class unsupervised anomaly detection. arXiv preprint arXiv:2404.06564 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Henschel, L., Conjeti, S., Estrada, S., Diers, K., Fischl, B., Reuter, M.: Fastsurfer - a fast and accurate deep learning based neuroimaging pipeline. NeuroImage <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">219</span>, 117012 (Oct 2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Henschel, L., Kügler, D., Reuter, M.: Fastsurfervinn: Building resolution-independence into deep learning segmentation methods—a solution for highres brain mri. NeuroImage <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">251</span>, 118933 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ho, T.C., Gutman, B., Pozzi, E., Grabe, H.J., Hosten, N., Wittfeld, K., Völzke, H., Baune, B., Dannlowski, U., Förster, K., et al.: Subcortical shape alterations in major depressive disorder: Findings from the enigma major depressive disorder working group. Human Brain Mapping <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">43</span>(1), 341–351 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Liu, W., Wei, D., Chen, Q., Yang, W., Meng, J., Wu, G., Bi, T., Zhang, Q., Zuo, X.N., Qiu, J.: Longitudinal test-retest neuroimaging data from healthy young adults in southwest china. Scientific data <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">4</span>(1),  1–9 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012–10022 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Marcus, D.S., Fotenos, A.F., Csernansky, J.G., Morris, J.C., Buckner, R.L.: Open access series of imaging studies: longitudinal mri data in nondemented and demented older adults. Journal of cognitive neuroscience <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">22</span>(12), 2677–2684 (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Marcus, D.S., Wang, T.H., Parker, J., Csernansky, J.G., Morris, J.C., Buckner, R.L.: Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults. Journal of cognitive neuroscience <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">19</span>(9), 1498–1507 (2007)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Marek, K., Jennings, D., Lasch, S., Siderowf, A., Tanner, C., Simuni, T., Coffey, C., Kieburtz, K., Flagg, E., Chowdhury, S., et al.: The parkinson progression marker initiative (ppmi). Progress in neurobiology <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">95</span>(4), 629–635 (2011)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Rao, V.M., Wan, Z., Arabshahi, S., Ma, D.J., Lee, P.Y., Tian, Y., Zhang, X., Laine, A.F., Guo, J.: Improving across-dataset brain tissue segmentation for mri imaging using transformer. Frontiers in Neuroimaging <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">1</span> (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. pp. 234–241. Springer (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Roy, A.G., Conjeti, S., Sheet, D., Katouzian, A., Navab, N., Wachinger, C.: Error corrective boosting for learning fully convolutional networks with limited data. In: Descoteaux, M., Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne, S. (eds.) Medical Image Computing and Computer Assisted Intervention - MICCAI 2017. pp. 231–239. Springer International Publishing, Cham (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Taha, A.A., Hanbury, A.: Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool. BMC Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">15</span>(1),  29 (Aug 2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">30</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
van der Velpen, I.F., Vlasov, V., Evans, T.E., Ikram, M.K., Gutman, B.A., Roshchupkin, G.V., Adams, H.H., Vernooij, M.W., Ikram, M.A.: Subcortical brain structures and the risk of dementia in the rotterdam study. Alzheimer’s &amp; Dementia <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">19</span>(2), 646–657 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Wang, L., Alpert, K.I., Calhoun, V.D., Cobia, D.J., Keator, D.B., King, M.D., Kogan, A., Landis, D., Tallis, M., Turner, M.D., et al.: Schizconnect: Mediating neuroimaging databases on schizophrenia and related disorders for large-scale integration. Neuroimage <span class="ltx_text ltx_font_bold" id="bib.bib29.1.1">124</span>, 1155–1167 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wang, W., Chen, C., Ding, M., Yu, H., Zha, S., Li, J.: Transbts: Multimodal brain tumor segmentation using transformer. In: Medical Image Computing and Computer Assisted Intervention – MICCAI 2021. pp. 109–119. Springer (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Wei, D., Zhuang, K., Ai, L., Chen, Q., Yang, W., Liu, W., Wang, K., Sun, J., Qiu, J.: Structural and functional brain scans from the cross-sectional southwest university adult lifespan dataset. Scientific data <span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">5</span>(1), 1–10 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Yeghiazaryan, V., Voiculescu, I.: Family of boundary overlap metrics for the evaluation of medical image segmentation. Journal of Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib33.1.1">5</span>,  1 (02 2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zuo, X.N., Anderson, J.S., Bellec, P., Birn, R.M., Biswal, B.B., Blautzik, J., Breitner, J., Buckner, R.L., Calhoun, V.D., Castellanos, F.X., et al.: An open science resource for establishing reliability and reproducibility in functional connectomics. Scientific data <span class="ltx_text ltx_font_bold" id="bib.bib35.1.1">1</span>(1), 1–13 (2014)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 20:15:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
