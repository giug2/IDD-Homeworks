<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.12091] Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions</title><meta property="og:description" content="Internet of Things (IoT) interconnects a massive amount of devices, generating heterogeneous data with diverse characteristics. IoT data emerges as a vital asset for data-intensive IoT applications, such as healthcare,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.12091">

<!--Generated on Tue Feb 27 12:43:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  Internet of things,  deep learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Boyu Fan,
Siyang Jiang,
Xiang Su, 
and Pan Hui, 


</span><span class="ltx_author_notes">Boyu Fan is with the Department of Computer Science, University of Helsinki, Helsinki 00100, Finland. E-mail: boyu.fan@helsinki.fi
Siyang Jiang is with the School of Mathematics and Statistics, Huizhou University, and also with the Department of Information Engineering, The Chinese University of Hong Kong.
E-mail: syjiang@hzu.edu.cn
Xiang Su is with the Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway. E-mail: xiang.su@ntnu.no
Pan Hui is with the Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and also with the Department of Computer Science, University of Helsinki, Helsinki 00100, Finland.
E-mail: panhui@cse.ust.hk
Manuscript received December 7, 2023</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Internet of Things (IoT) interconnects a massive amount of devices, generating heterogeneous data with diverse characteristics. IoT data emerges as a vital asset for data-intensive IoT applications, such as healthcare, smart city and predictive maintenance, harnessing the vast volume of heterogeneous data to its maximum advantage.
These applications leverage different Artificial Intelligence (AI) algorithms to discover new insights. While machine learning effectively uncovers implicit patterns through model training, centralizing IoT data for training poses significant privacy and security concerns. Federated Learning (FL) offers an promising solution, allowing IoT devices to conduct local learning without sharing raw data with third parties. Model-heterogeneous FL empowers clients to train models with varying complexities based on their hardware capabilities, aligning with heterogeneity of devices in real-world IoT environments.
In this article, we review the state-of-the-art model-heterogeneous FL methods and provide insights into their merits and limitations. Moreover, we showcase their applicability to IoT and identify the open problems and future directions. To the best of our knowledge, this is the first article that focuses on the topic of model-heterogeneous FL for IoT.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, Internet of things, deep learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past decade, significant advancements in Artificial Intelligence (AI) have led to widespread integration into various Internet of Things (IoT) applications. AI has empowered various IoT domains, ranging from smart city and environmental monitoring to manufacturing and healthcare. Machine learning (ML) plays a crucial role, enabling the development of robust models by allowing algorithms to learn patterns from data. Traditionally, data is collected from various IoT sources and stored in centralized servers, thereby supporting data analysis and model development for researchers and scientists. These powerful servers offer ample computational resources, facilitating efficient and reliable model development. However, with the rising privacy concerns, it is evident that centralized data collection poses inherent risks to individual privacy.
To address these concerns, a series of regulatory laws and policies, such as the General Data Protection Regulation (GDPR), are enacted by governments to regulate the collection and handling of data, which motivates researchers to explore alternative ML paradigms that adhere to these privacy guidelines.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) emerges as a promising solution for training models in a privacy-preserving manner for IoT applications. FL obviates the need for centralized data collection by local storage of data. Each client trains the model with its local data and contributes to the global model by sharing only model parameters or gradients with the central server. With this privacy-preserving feature, FL is widely used across diverse domains, such as health, banking, and transportation. It is imperative to highlight the growing adoption of FL within the context of the Internet of Things (IoT), where billions of sensors and edge devices are continuously generating vast amounts of data on a daily basis. Characteristics of FL in the IoT context underscore its adaptability and effectiveness in handling the unique data challenges in IoT ecosystems.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One limitation of FL is that the server can only perform aggregation once it has received updates from all participating clients. Training a neural network is computationally expensive, given a huge number of model parameters and the necessity for multiple iteration rounds.
Consequently, if some clients fail to complete the local training within the expected times due to the limited computational resources, both the server and other clients must wait for them, leading to low learning efficiency and resource utilization inefficiency. However, the real-world IoT environments naturally include diverse devices, contributing to a high level of resource heterogeneity. Moreover, the dynamic operating environments, e.g., network traffic fluctuations and temperature variations, also have the potential to affect the training times. Therefore, traditional model-homogeneous FL approaches like FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> cannot be effectively utilized in such IoT scenarios.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To this end, model-heterogeneous FL has been investigated to solve the aforementioned challenge, alleviating the requirement imposed by traditional FL where all clients are expected to share a universal global model. Instead, it enables clients to train different types of models based on their respective capabilities, which offers two significant advantages, i.e., mitigating the straggler issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and decreasing of resource wastage. For instance, consider a scenario where a Raspberry Pi can efficiently train a two-layer convolutional neural network (CNN), while a ResNet18 model would pose a significant burden on it. Lengthy training times result in waiting periods for other clients, leading to idle periods for other clients and inefficient resource utilization.
In contrast, model-heterogeneous FL allows a Raspberry Pi to train a small model, while a server machine can simultaneously train a ResNet18 model within the same FL round.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">While model-heterogeneous FL holds great potential to mitigate the affect of resource heterogeneity, a core challenge should be addressed. Since clients train different neural networks, the server cannot directly conduct averaging operations on the received model parameters due to variations in their architectures.
Novel approaches should be proposed to enable the server to conduct aggregation even if the model parameters have different shapes. The advantages of model-heterogeneous FL and its great potential in IoT motivate us to perform a comprehensive literature review to understand the state-of-the-art solutions.
In this article, we comprehensively review the state-of-the-art model-heterogeneous FL approaches. Beyond enumerating the advantages and disadvantages of each method, we also summarize the mainstream solutions and propose potential refinements.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of relative surveys</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Ref</span></th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S1.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Scope</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></td>
<td id="S1.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S1.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">FL for general IoT</td>
</tr>
<tr id="S1.T1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></td>
<td id="S1.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2022</td>
<td id="S1.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">The fusion of FL and IoT</td>
</tr>
<tr id="S1.T1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></td>
<td id="S1.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2022</td>
<td id="S1.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">FL on resource-constrained IoT devices</td>
</tr>
<tr id="S1.T1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></td>
<td id="S1.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2023</td>
<td id="S1.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">FL on heterogeneous devices</td>
</tr>
<tr id="S1.T1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Our work</td>
<td id="S1.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2023</td>
<td id="S1.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Model-heterogeneous FL for IoT</td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Table <a href="#S1.T1" title="TABLE I ‣ I Introduction ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> compares our survey with existing surveys related to FL and IoT. To the best of our knowledge, our survey marks the pioneering efforts in addressing the domain of model-heterogeneous FL, underscoring its novelty and significance within the research landscape.
Our contributions are twofold:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We review the state-of-the-art approaches to model-heterogeneous FL and highlight the insights of each approach.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We provide the future directions of model-heterogeneous FL research for IoT based on our findings.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of this article is organized as follows. Section <a href="#S2" title="II Preliminaries and challenges ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> introduces the background of FL and our motivation for this survey. Section <a href="#S3" title="III State-of-the-art Model-heterogeneous FL approaches ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the state-of-the-art methods for model-heterogeneous FL and conducts comparative analyses to show the strengths and weaknesses of each method.
Section <a href="#S4" title="IV Applications of model-heterogeneous FL in IoT ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> details the applications of model-heterogeneous FL on IoT. Section <a href="#S5" title="V Open problems and future directions ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> discusses the open questions and future research directions based on our findings. Finally, Section <a href="#S6" title="VI Conclusion ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes this survey.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Preliminaries and challenges</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Overview of FL</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">FL was first proposed by Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which becomes a paradigm to train ML models without the central collection of data. Since then, ongoing research has developed lots of new applications for FL. The driving force behind the continued research is the emergence of new and strict privacy regulations and laws, exemplified by GDPR, which was enacted in 2016 and enforced from 2018 onward.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ II-A Overview of FL ‣ II Preliminaries and challenges ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the general FL architecture, consisting of two main entities, i.e., a central server for conducting aggregation and multiple clients for performing local training. Each client trains models with its local dataset without sharing with others, thereby ensuring the protection of their privacy. During the training process, instead of collecting raw data from different sources, the server only receives model parameters from clients and sends the aggregated model parameters back to the clients to update the local models.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2312.12091/assets/img/FL.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A standard FL architecture.</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.2" class="ltx_p">In general, the FL training process includes three main steps. 1) Server initialization. As the first step of FL training, the server initializes a global model tailored to the specific training task and subsequently distributes the model to the selected clients. 2) Local Update. Each selected client <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">k</annotation></semantics></math> conducts model training with its local data <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="D_{k}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msub id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">𝐷</ci><ci id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">D_{k}</annotation></semantics></math> based on the received global model. Once the training is completed, the clients send the updated models back to the server.
3) Model aggregation. The server receives the updated models from clients and then conducts model averaging aggregation to update the model. The aggregated model will become the new global model and continually be updated in the subsequent rounds until convergence.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">The Challenges of System Heterogeneity</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">System heterogeneity refers to the diversity in hardware configurations, software platforms, communication protocols, and even run-time environments among different participants and devices, which exists in common IoT scenarios. The traditional FL workflow typically involves the server waiting to receive updated parameters from the selected clients before performing aggregation. However, due to the existence of system heterogeneity, different clients may present varying training efficiency, leading to two potential problems.
First, due to the fact that all clients perform local updates based on a unified global model, the required training times vary based on their computation capabilities. Consequently, both the server and the clients with more abundant computation resources need to wait for slower clients, resulting in resource wastage. Second, some resource-constrained devices are not capable of training complex neural networks, e.g., ResNet. Extremely long processing time can cause the server to mistakenly assume that the clients are offline, thus never selecting them for the distributed training. As the local data owned by these devices cannot join the training, the contribution to the global model can be disproportionately small, leading to imbalanced models. In conclusion, system heterogeneity poses challenges to the performance, generalization, coordination and efficiency of the FL system.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Model-heterogeneous FL</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The optimal approach for addressing system heterogeneity lies in empowering devices to learn diverse models with varying complexities based on their available resources.
For instance, training a CNN with two convolutional layers requires significantly fewer computing resources compared to training a ResNet18. Therefore, model-heterogeneous FL, allowing clients to train heterogeneous models, naturally emerges as a promising technique for mitigating system heterogeneity issues. However, in model-heterogeneous FL, clients may possess different model architectures, hyperparameter settings, or optimization techniques, thereby complicating the aggregation process on the server side. Conversely, in model-homogeneous FL, there exists only one global model architecture that is shared by all clients, facilitating straightforward element-to-element averaging operations on multiple model parameters. Hence, how to achieve effective model aggregation for models with different architectures becomes the core challenge of model-heterogeneous FL.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">State-of-the-art Model-heterogeneous FL approaches</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we investigate the existing approaches for model-heterogeneous FL, which can be classified into two main solutions, i.e., KD-based approaches and PT-based approaches. We also present several alternative approaches that fall outside of these two categories.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">KD-based Approaches</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Knowledge Distillation</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Knowledge Distillation (KD) was proposed by Hinton <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which aims to distill knowledge from a larger model or an ensemble of models and transfer this understanding to a smaller, more compact model. Despite its reduced size, this distilled model, often referred to as the “student” model, can exhibit performance metrics remarkably similar to its larger “teacher” counterpart.
During the KD process, three types of knowledge play pivotal roles, including response-based, feature-based, and relationship-based knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In response-based knowledge, information is extracted from the output layer of the teacher model, capturing the class probabilities. Feature-based knowledge, on the other hand, does not restrict itself to the output of the last layer, it also gathers insights from the outputs of intermediate layers of a neural network. Contrasting with the prior two types, relationship-based knowledge leverages the relationships either between different layers of a neural network or between individual data samples. Various techniques, such as similarity matrices, instance relationship graphs, or mutual information flows, have been proposed to define these relationships.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Knowledge Distillation in FL</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">The output of the last layer of a model, known as logits in response-based knowledge, has a shape determined by the number of classes in the dataset, which is independent of the model’s architecture. For instance, in the CIFAR10 dataset, the logits’ shape remains at <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="10\times 1" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mrow id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">10</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p1.1.m1.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><times id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">10\times 1</annotation></semantics></math>, whereas in the CIFAR100 dataset, it is <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="100\times 1" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mrow id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">100</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p1.2.m2.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><times id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">100</cn><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">100\times 1</annotation></semantics></math>. This characteristic allows the utilization of KD in FL, enabling aggregation at the server even when received models have varying architectures.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2312.12091/assets/img/kd.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The basic idea of KD in FL.</figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">In recent years, several approaches have been developed to apply KD to achieve model-heterogeneous FL. The basic idea is presented in Figure <a href="#S3.F2" title="Figure 2 ‣ III-A2 Knowledge Distillation in FL ‣ III-A KD-based Approaches ‣ III State-of-the-art Model-heterogeneous FL approaches ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Clients use their local data to conduct hard predictions and leverage public data to serve as soft predictions. FedMD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is a pioneering work to achieve model aggregation using KD rather than averaging the model parameters. The training process of FedMD consists of four steps: First, each client computes the class scores on a public dataset and transfers them to the central server. Second, the server computes the average logits to serve as a consensus and sends it to all clients. Third, each client trains its model to approach the consensus on the public dataset. Finally, each client trains its model based on the local data to achieve local updates. Compared to conventional FL approaches, FedMD does not need to exchange model parameters between server and client, allowing models to have different architectures. Clients contribute to the global model by only sharing their logits that are computed on the public dataset.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">Although FedMD bypasses model parameter aggregation, its global logits construction by simple averaging at the server may not optimally harness the collective knowledge of clients. KT-pFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> introduces a knowledge coefficient matrix in the FL training process. After training the clients’ personalized models over the local data, the local model logits is generated to the public data set and shared with the server. Then, using the linear combination of the aggregated soft predictions and the knowledge coefficient matrix, the server calculates the local soft prediction for each client. After each iteration, the knowledge coefficient matrix is updated on the server. With the help of a knowledge coefficient matrix, each client can adaptively aggregate all local soft predictions to form a personalized one instead of being restricted to a global soft prediction.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.1" class="ltx_p">A shared limitation of the aforementioned approaches is their dependency on a public dataset. In practical applications, this kind of public dataset requires careful deliberation and even prior knowledge of clients’ private data. To eliminate this dependency, several approaches for data-free KD have been proposed. Compared to FedMD, FedDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> relieves the requirement of labeled public data. Instead, unlabeled or artificially generated data from arbitrary domains is used to perform ensemble distillation on the server. This involves training local models with unlabeled or generated data and using the resulting output logits to construct the global model. In contrast to FedMD, distillation is carried out on the server side, leaving the local training unaffected.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p">Similarly, FedGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> achieves data-free learning by training a generative model based on the prediction rules of the local clients. Through this approach, the generator effectively models the global data distribution. With this distilled joint knowledge about data distribution from the generator, local models are then updated. This methodology allows the local models to mutually benefit from each other without the necessity of employing an external public data source. A notable distinction from earlier frameworks is that the distilled knowledge is not passed on to a global model, but directly to the local models. However, a potential drawback of FedGen pertains to its tendency to lose information from the local models, due to the fact that the approach sometimes overlooks the incompatibility of localized knowledge during the distillation process.</p>
</div>
<div id="S3.SS1.SSS2.p6" class="ltx_para">
<p id="S3.SS1.SSS2.p6.1" class="ltx_p">FedZKT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> presents an alternative approach to data-free KD for FL. In contrast to FedGen, FedZKT keeps the computation-heavy generator on the server side to relieve devices with low computational capacity. The on-device parameters are not shared with the server to preserve privacy. The generative model provides inputs for global and on-device training, which are utilized to detect and maximize the disagreements between the global and private on-device models. Moreover, the pre-trained generative model is also utilized to distill the on-device model and update its parameters with global knowledge. The approach of alleviating clients from some computation-heavy tasks results in an increase in the number of classes per client in comparison to FedMD.</p>
</div>
<div id="S3.SS1.SSS2.p7" class="ltx_para">
<p id="S3.SS1.SSS2.p7.1" class="ltx_p">FedFTG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> offers a solution by fine-tuning the global model through a process of hard sample mining. This method harnesses the robust computational capacities of the server effectively. FedFTG employs a generator to extract knowledge from the input spaces of individual local models. Central to this process is the utilization of a hard sample mining scheme, prompting the generator to produce challenging samples within the data distribution that highlight disparities between the local and global models. This approach ensures the global model is finely attuned to the overall data distribution. In addition, the framework integrates customized label sampling and a class-level ensemble strategy to offset variances in label distributions among local clients resulting from class imbalances and varying importance levels of knowledge for the classes.</p>
</div>
<div id="S3.SS1.SSS2.p8" class="ltx_para">
<p id="S3.SS1.SSS2.p8.1" class="ltx_p">In FedHKD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the hyper knowledge is introduced as the average of the data representation of a class in combination with the respective average soft prediction. The iterative process unfolds as follows: 1) Each client sends the local hyper-knowledge of each class to the server. To safeguard the client’s privacy, noise is intentionally added to the data representation. 2) The global hyper knowledge is aggregated from the collected hyper knowledge of the clients. The added noise is compensated during the aggregation. 3) The global hyper knowledge is sent back to the clients, who adjusts their loss function accordingly for the subsequent iteration. In comparison to FedMD, FedHDK is competitive in local and global test accuracy for heterogeneous data.</p>
</div>
<div id="S3.SS1.SSS2.p9" class="ltx_para">
<p id="S3.SS1.SSS2.p9.1" class="ltx_p">Building on the success of using response-based KD on FL, researchers believe the logits implicitly possess the label information of each class, serving as a channel for knowledge transfer among different parties. FedProto <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> capitalizes on this idea by employing class prototypes as a mean for knowledge exchange. Specifically, a class prototype is a representation of a class, derived from the embedding vectors of class features and subsequently averaged. These prototypes are sent to the server, where they are aggregated to create a global knowledge base. The global knowledge is then sent back to the clients, motivating them to refine their local prototypes, which in turn bolsters the efficacy of local training. In this way, no global model is maintained, allowing clients to employ different model architectures.</p>
</div>
<div id="S3.SS1.SSS2.p10" class="ltx_para">
<p id="S3.SS1.SSS2.p10.1" class="ltx_p">FedHeNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> presents an architecture-agnostic FL approach for heterogeneous neural networks. In FedHeNN, each client trains a personalized model with their local dataset, which is allowed to have a different architecture from the global model. In each iteration, a Representation Alignment Dataset (RAD) is generated by the server and distributed to the clients. The clients then use the RAD to align their representations. Instead of sending model parameters, the clients share their generated representation for RAD with the server to achieve aggregation. It has been empirically proven that the method is not vulnerable to clients with smaller data footprints. The test accuracy experiences only minimal degradation compared to the proportion of clients with smaller local data sets.</p>
</div>
<div id="S3.SS1.SSS2.p11" class="ltx_para">
<p id="S3.SS1.SSS2.p11.1" class="ltx_p">Regatti <span id="S3.SS1.SSS2.p11.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> investigate a solution to preserve the model privacy of clients in addition to data privacy. The Fed-CMA algorithm allows clients to maintain individual model architectures while participating in FL with other clients. This approach does not require a public dataset or maintain a global model. Instead, the feature weights of the feature extractor and the classification weights are distilled client-side. The server then aggregates the weights from all clients and sends the aggregated feature and classification weights back to the clients to update the local model. To achieve this, a new algorithm based on conditional moment alignment and generalization for aggregating the weights is introduced, since conventional algorithms such as FedAvg and FedProx cannot be used due to the different client model architectures.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.4.1.1" class="ltx_text">III-A</span>3 </span>Limitations</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">The KD-based concept facilitates various kinds of new FL approaches to solve the model-heterogeneous FL problem. However, several challenges exist. First, despite several claims of achieving data-free KD, they still require some kind of data that the server and client can jointly access to achieve knowledge alignment. How to achieve truly data-free KD remains an interesting yet challenging question. Second, most of the existing works focus on the response-based KD, the model performance with feature-based and relationship-based KD needs to be further investigated. Third, compared to conventional model parameter-shared FL approaches, KD-based approaches solely rely on exchanging logits, resulting in a much smaller amount of information exchange. Therefore, despite they achieve model-heterogeneity and require much less communication cost, the model performance may decrease. How to balance the model performance and the ability to accommodate model-heterogeneity is a challenge. Finally,
privacy concerns related to knowledge transmission need further exploration, especially given the ease with which some types of knowledge can be decoded and potentially accessed by unintended parties.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">PT-Based Approaches</span>
</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Partial Training</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Partial training refers to the strategy that allows clients to train specific segments of neural networks based on their computational capabilities. This methodology primarily divides into two branches, i.e., dynamic partial training and post-partial training. Dynamic partial training focuses on training only a subset of the complete network, with the objective of generating diverse models in various FL scenarios while significantly reducing the computational demands. On the other hand, post-partial training approaches aim at pruning the original full-size model on the server. The pruned model is subsequently fine-tuned on devices using their local data, a process initiated after acquiring the aggregated model. This approach strategically leverages local data to refine the model, thereby enhancing its applicability and effectiveness.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>Partial Training in FL</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ III-B2 Partial Training in FL ‣ III-B PT-Based Approaches ‣ III State-of-the-art Model-heterogeneous FL approaches ‣ Model-Heterogeneous Federated Learning for Internet of Things: Enabling Technologies and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the basic idea of PT in FL. Each client trains a part of the global model and only aggregates the corresponding parts during the aggregation step.
HeteroFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, as a typical dynamic partial training method, allows clients to train specific parts of the neural network based on their computational capacity and an adaptive communication compression scheme that adjusts model update compression according to clients’ communication resources. In particular, the clients are organized into multiple groups based on their computation capabilities and communication resources. Each group independently trains its model locally using the partial training technique, and the local models are then aggregated to form a global model. This method is suitable for HeteroFL since every communication round is independent. HeteroFL achieves significant improvements in terms of computation and communication efficiency on a benchmark dataset, making FL feasible for a wider range of devices and use cases.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2312.12091/assets/img/pt.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The basic idea of PT in FL.</figcaption>
</figure>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">Compared to the static sub-model extraction strategy in HeteroFL, FedRolex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposes a rolling sub-model extraction method. In this scheme, the sub-model is derived from the full-sized model using a progressively advancing rolling window with each communication round. This innovative strategy ensures that different sections of the global server model receive uniform training, thereby alleviating the client drift caused by model heterogeneity. In addition, while HeteroFL confines the global model’s size to match that of its largest client model, FedRolex permits global models to surpass the complexity of any individual client model, leading to enhanced performance</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">In contrast to HeteroFL and FedRolex dividing the clients into multiple groups, Hermes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> develops a two-stage technique that includes client selection and adaptive model compression, aiming to improve the efficiency of FL on diverse mobile clients. In the client selection stage, the authors employ a reinforcement learning algorithm to select the most suitable clients for participating in the training process based on their data quality, computational capabilities, and communication bandwidth. The selected clients then train their models locally using partial training, and the updated model weights are sent to the central server for aggregation. In the adaptive model compression stage, the authors propose a novel approach that uses a combination of model pruning and quantization to compress the model size without sacrificing accuracy. This approach facilitates efficient transmission of model updates from the clients to the server, resulting in reduced communication costs.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">As a typical dynamic partial training method, PriSM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> leverages the spatial structure of neutral networks, i.e., low-rank structure and networks kernel orthogonality to train sub-networks in the orthogonal kernel space. In particular, PriSM uses singular value decomposition on the original kernels to extract a set of principal orthogonal kernels.
The significance of these kernels is determined by their singular values. Following this, it also introduces a unique sampling strategy that independently selects varying subsets of the principal kernels. This strategy is used to create client-specific sub-models, effectively reducing computational and communication demands. It is worth noting that kernels with larger singular values are given a higher likelihood of being selected during the sampling process. As a result, each sub-model represents a low-rank approximation of the comprehensive server model.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p">Compared to dynamic partial training approaches, post-partial training approaches finetune the model by leveraging their local data. For example, LotteryFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> generates heterogeneous models, which is inspired by the Lottery Ticket hypothesis, presenting a method for identifying Lottery Ticket Networks (LTNs) – sparse subnetworks within larger models. In this context, LotteryFL aims to find each client’s LTN during communication rounds in FL and only exchanges LTN parameters between clients and the server, thereby reducing communication overhead. In particular, LotteryFL employs client-specific pruning strategies to the updated weights, resulting in sparse subnetworks (winning tickets). In summary, the LotteryFL framework leverages the lottery ticket hypothesis to improve personalization and communication efficiency in FL for non-IID datasets.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p id="S3.SS2.SSS2.p6.1" class="ltx_p">PruneFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> directly employs model pruning to create various local models to fit model-heterogeneous requirements. In particular, PruneFL proposes distributed pruning which involves initial pruning at a chosen client, followed by additional distributed pruning integrated within the standard FL process, enabling more efficient model compression across the network. Subsequently, PruneFL dynamically maintains a compact model with efficient transmission, computation, and reduced memory footprint, while preserving essential connections for convergence to an accuracy similar to the original model. In addition, PruneFL continuously updates the parameters to retain and adjust the model size, aiming to minimize the time taken to reach convergence. Experimental results demonstrate that this approach significantly reduces model size and communication costs while maintaining high accuracy on benchmark datasets. The PruneFL approach also shows improved efficiency in terms of training time and energy consumption, making it suitable for FL on resource-constrained edge devices.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Limitations</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">The partial training methodology enables clients to tailor the training of specific sectors within neural networks, but it comes with several challenges. First, it is assumed that each local model is a subset of the global model, meaning they should share common elements. This requirement limits the types of local models, reducing flexibility in designing models for different local data. Second, most of the existing works employ either random or fixed strategies to obtain these partial neural networks. However, the optimal method to select the most crucial components for enhanced learning efficiency remains ambiguous. Third, while much of the existing research is grounded in empirical evidence, there is a notable lack of rigorous theoretical analysis.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Alternative approaches</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In addition to the KD and PT-based methodologies, several innovative techniques have been developed to enable model-heterogeneous FL.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">HAFL-GHN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> handles heterogeneous client architectures by employing a Graph HyperNetwork (GHN). The framework does not require any public data and the client architectures remain private. GHNs are highly adaptable for accommodating any architecture, with their nodes embodying the layers of the client model and the edges of its computational workflow. Each local client initiates by training a copy of the GHN with its private data and then passes the resulting weights of the graph to the server. These weights are aggregated on the servervvside and distributed to local models for further optimization. HAFL-GHN outperforms the alternative frameworks based on KD or non-graph hypernetworks, especially when clients possess smaller local datasets. However, when the clients have homogeneous architectures, the performance results of the presented framework do not yet match that of other state-of-the-art methods. Another notable advantage of this method pertains to flexibility, i.e. allowing for changes in local clients or new clients can be added with minimal effort.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">DISTREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> presents a Distributed Resource-Aware Learning pattern to address systems with various resources, allowing clients to adapt independently of the server. Such adaptation involves omitting dynamical filters of the convolutional layers to reduce the computational complexity of the training. This dropout mechanism is applied during runtime and uses a dropout rate, which is determined once before training and is defined by a Pareto-optimal vector. The rate is based on the convergence speed and the resource requirements and is determined by an automated design space exploration in the form of a genetic algorithm. Experimental results with different datasets have demonstrated that DISTREAL achieves significantly faster convergence speed in the FL system without sacrificing/compromising accuracy. This dynamic dropout approach maximizes resource utilization, allowing devices of varying specifications to participate seamlessly.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Yao <span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> propose FL for Heterogeneous Models (FEDHM) as a method to address the problem of heterogeneous models. Low-rank factorization is used to decompose the model parameters for different clients and divide them into shared and individual components. The shared components capture the common features across different models, while the individual components capture the unique features of each model. FEDHM applies FL to update the shared and individual components separately. This update approach for both shared and unique components reduces both communication costs and device training overhead, boosting performance across diverse settings. Empirical evidence highlights FEDHM’s superior efficacy compared to the state-of-the-art methods. However, a notable drawback remains in the steep computational overhead linked to matrix composition.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Applications of model-heterogeneous FL in IoT</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Healthcare</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The integration of AI and smart devices into healthcare is transforming medical services from being confined to hospital settings to becoming a part of our everyday lives. The IoT in healthcare involves a wide variety of applications and devices, where wearable devices play a key role. These wearables, such as heart and blood pressure monitors, as well as wristbands and smartwatches, are equipped with multiple monitoring functions. They monitor real-time physiological data and transmit this information for further analysis. While the data collected is invaluable for the development of sophisticated analytical models, it also raises significant concerns regarding user privacy. Model-heterogeneous FL emerges as a promising solution to this dilemma. This approach ensures that private health data remains stored on the user’s own device, thereby eliminating the need for data to be transmitted externally. Detection models are updated using local data. Thanks to the model-heterogeneous feature, different models can be trained on various types of smartphones. This eliminates the unrealistic requirement for all users to possess identical devices, thereby enhancing the accessibility and applicability of this technology in diverse real-world scenarios.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Automobile</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In the evolving landscape of the automobile industry, vehicles are increasingly integrating with the IoT. This connectivity allows for real-time feedback about driving conditions, enhancing both safety and the overall driving experience. However, the integration of a vast number of vehicles with the IoT introduces certain challenges. For instance, cloud servers may incur substantial communication costs when processing vast amounts of data. Furthermore, data privacy risks arise as vehicles, in theory, have the capacity to interconnect through the IoT. FL-based vehicle IoT mitigates these challenges. Specifically, FL enables vehicles to process and train models without transmitting local data to the centralized system, thereby mitigating privacy concerns. Moreover, model-heterogeneous FL supports vehicles with varied hardware configurations, offering increased flexibility and significantly reducing the costs associated with deploying automotive IoT solutions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Smart City</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The development and deployment of IoT technologies are revolutionizing urban landscapes worldwide by connecting billions of devices and making informed decisions based on massive amounts of heterogeneous IoT data generated from these devices. Challenges of processing IoT data from smart cities pertain to efficiently analyzing generated data, developing novel services for citizens and stakeholders, and data privacy and consent. For example, city stakeholders make decisions, such as to ensure optimal city planning and enhance city management, which requires a deep understanding of the massive amounts of heterogeneous IoT data.
Achieving this level of understanding necessitates deep analytics across various sectors of smart city data. However, the sharing of data between these sectors can often be restricted. In this context, model-heterogeneous FL in smart cities offers a promising solution to this challenge by training diverse ML models tailored to the specific infrastructures of different sectors, while keeping the data localized. Moreover, model-heterogeneous FL facilitates collaboration between cities. This is particularly beneficial for cities lacking advanced data centers, as they can still participate by training smaller-scale models. Such collaborative efforts enhance the overall efficacy and intelligence of smart city initiatives, paving the way for more responsive and adaptive urban environments.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Industrial Automation</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">IoT-empowered industrial automation facilitates efficient production and finer management. This automation process generates vast quantities of data, encompassing sensor readings, image captures, and more. With the assistance of AI technology, this data holds the potential for the development of enhanced processing models. However, concerns regarding data privacy and security deter many enterprises and factories from sharing their sensitive data across industrial networks, potentially stalling advancements in automation systems. Integrating model-heterogeneous FL with industrial automation offers a solution to this challenge. First, data integrity is assured as it remains stored locally throughout the process. Second, given that different factories may have distinct infrastructural and equipment setups, IoT service providers can optimize system efficiency across diverse factories through model-heterogeneous FL. This is achieved by collecting only model parameters, rather than raw production data, ensuring both privacy and system enhancements.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Energy Networks</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">With the flourishing of AI technology and IoT, energy networks are also constantly evolving and developing. The IoT serves as a foundational element to connect different energy infrastructures, while AI technology plays the role of a planner, determining how to distribute and transmit energy reasonably and efficiently. A representative example is the smart electricity meters installed in every household. They monitor and record the energy consumption of each family and send the data to the server of the energy manufacturer on a regular basis. The volume of this data enables energy manufacturers to train robust AI models, predicting future electricity usage of individual households. They can then adjust both power supply and pricing, optimizing benefits for both power supply companies and their consumers. However, from the perspective of privacy protection, attackers or malicious actors might deduce a household’s living patterns and daily routines by analyzing energy consumption data. With model-heterogeneous FL, this data can be kept on the user’s side without being sent to the energy manufacturer. Meanwhile, model-heterogeneous FL allows households to install different smart meters, reducing the overhead incurred by standardization. Model training is performed locally using real-time energy consumption data, and only the model parameters are sent to the energy manufacturer to improve the overall energy prediction performance.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Open problems and future directions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Model-heterogeneous FL is an emerging yet practical distributed learning paradigm. Despite the considerable research on this topic, we identify several open problems and future directions that demand further research.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Aggregation strategy</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In model-heterogeneous FL scenarios, element-wise weight averaging for aggregation is impractical due to differing model architectures, affecting the efficacy of most homogeneous FL techniques. To address this, leading methods either use latent representation to standardize the models, like employing KD for logit processing, or employ pruning to ensure models have overlapping sections for aggregation. However, each approach has limitations. KD-based methods presuppose aligned logits across clients, either from an existing dataset or synthetic data. Without this alignment, logit aggregation might lose relevance, as they could represent entirely different features. In pruning-based methods, focusing solely on overlapping components may neglect essential, non-overlapping model features, resulting in suboptimal model performance.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We envision promising future research directions toward model aggregation strategy. First, While KD strategies often rely on logits, this is constrained by data alignment. A more promising direction might be to investigate relation-based knowledge, which is calculated by the inner products between features across layers. Such knowledge contains the implicit learning logic of a model, which is irreverent to the input data, eliminating the reliance on public datasets. Regarding pruning-based strategy, an interesting direction is to identify the principal components, in other words, identify the layers and weights that most impact model performance, akin to principal component analysis (PCA). With a clear understanding of the principal components, efficient pruning techniques can be employed to modify model architectures, while preserving the most important features of a model.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Privacy and security</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Despite FL allowing data to remain local, sharing model parameters can potentially reveal information about the private data. An adversary may conduct reverse engineering to induce the raw data from the model information, as both weights and gradients are calculated by the input data with specific matrix operations<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In model-heterogeneous FL, the transferred data is either output logits or a subset of the full model parameters. It remains ambiguous whether transferring this information can mitigate reverse engineer risks. Model poisoning is another security concern for FL systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The malicious party can manipulate the updated model before it is sent to the server for aggregation, thereby affecting the performance of the global model. In model-heterogeneous FL, where clients are allowed to have different types of models, directly modifying the model has limited impact on the global model as the update strategy does not rely on the model parameter aggregations. However, quantifying this impact is crucial for designing more efficient model-heterogeneous methods.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Resource-constrained IoT devices</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Model-heterogeneous FL presents a promising solution to IoT applications, which connects to large-scale heterogeneous IoT devices, compared to the conventional model-homogeneous FL. However, many devices in IoT are resource-constrained, such as robots, embedding controllers, cameras, and electric meters, making them unsuitable for training even the simplest neural networks. The computation cost of local training also contributes to heightened power consumption, potentially affecting the core functions of the devices. Therefore, exploring the effective deployment of model-heterogeneous FL frameworks on IoT is an open issue. One potential direction is offloading a part of training tasks from resource-constrained devices to their trusted edge nodes through split learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Another direction lies in investigating more advanced FL training strategies. For instance, a system might allow clients with weak computation capacities to train linear models, while permitting others to train neural networks.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This article presents a thorough review of model-heterogeneous FL methods for IoT. We start with discussing the challenges posed by conventional FL, setting the background for understanding the motivations behind model-heterogeneous FL within the IoT context. Subsequently, we dive into the state-of-the-art model-heterogeneous FL strategies, classifying them into two primary directions, i.e. KD-based and PT-based approaches. We also highlight other notable works that defy simple classification. Drawing from comparative insights into these methods, we spotlight the applications of model-heterogeneous FL within IoT, culminating in discussions about unresolved challenges and potential directions for future research. This article serves as a structured guide to model-heterogeneous FL and underscores its promising future within the IoT landscape.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Reisizadeh, I. Tziotis, H. Hassani, A. Mokhtari, and R. Pedarsani, “Straggler-resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogeneity,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE JASC</em>, vol. 3, no. 2, pp. 197–205, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, and H. Vincent Poor, “Federated learning for internet of things: A comprehensive survey,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Commun Surv Tutor</em>, vol. 23, no. 3, pp. 1622–1658, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P. Boobalan, S. P. Ramu, Q.-V. Pham, K. Dev, S. Pandya, P. K. R. Maddikunta, T. R. Gadekallu, and T. Huynh-The, “Fusion of federated learning and industrial internet of things: A survey,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Networks</em>, vol. 212, p. 109048, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Imteaj, U. Thakker, S. Wang, J. Li, and M. H. Amini, “A survey on federated learning for resource-constrained iot devices,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE IoT-J</em>, vol. 9, no. 1, pp. 1–24, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
K. Pfeiffer, M. Rapp, R. Khalili, and J. Henkel, “Federated learning for computationally constrained heterogeneous devices: A survey,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, vol. 55, no. 14s, jul 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em>, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A survey,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, vol. 129, pp. 1789–1819, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. Li and J. Wang, “Fedmd: Heterogenous federated learning via model distillation,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03581</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Zhang, S. Guo, X. Ma, H. Wang, W. Xu, and F. Wu, “Parameterized knowledge transfer for personalized federated learning,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 34, pp. 10 092–10 104, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for robust model fusion in federated learning,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 33, pp. 2351–2363, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Z. Zhu, J. Hong, and J. Zhou, “Data-free knowledge distillation for heterogeneous federated learning,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ICML</em>, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139.   PMLR, 18–24 Jul 2021, pp. 12 878–12 889.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Zhang, D. Wu, and X. Yuan, “Fedzkt: Zero-shot knowledge transfer towards resource-constrained federated learning with heterogeneous on-device models,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE ICDCS 2024</em>, 2022, pp. 928–938.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. Zhang, L. Shen, L. Ding, D. Tao, and L.-Y. Duan, “Fine-tuning global model via data-free knowledge distillation for non-iid federated learning,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, June 2022, pp. 10 174–10 183.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Chen, H. Vikalo <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The best of both worlds: Accurate global and personalized models through federated learning with data-free hyper-knowledge distillation,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.08968</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Tan, G. Long, L. Liu, T. Zhou, Q. Lu, J. Jiang, and C. Zhang, “Fedproto: Federated prototype learning across heterogeneous clients,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 36, no. 8, 2022, pp. 8432–8440.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. Makhija, X. Han, N. Ho, and J. Ghosh, “Architecture agnostic federated learning for neural networks,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICML</em>.   PMLR, 2022, pp. 14 860–14 870.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. R. Regatti, S. Lu, A. Gupta, and N. Shroff, “Conditional moment alignment for improved generalization in federated learning,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">FL Workshop in NeurIPS 2022)</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
E. Diao, J. Ding, and V. Tarokh, “Heterofl: Computation and communication efficient federated learning for heterogeneous clients,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Alam, L. Liu, M. Yan, and M. Zhang, “Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 35, pp. 29 677–29 690, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Li, J. Sun, P. Li, Y. Pu, H. Li, and Y. Chen, “Hermes: An efficient federated learning framework for heterogeneous mobile clients,” ser. MobiCom ’21.   New York, NY, USA: Association for Computing Machinery, 2021, p. 420–437.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Niu, S. Prakash, S. Kundu, S. Lee, and S. Avestimehr, “Federated learning of large models at the edge via principal sub-model training,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.13141</em>, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Li, J. Sun, B. Wang, L. Duan, S. Li, Y. Chen, and H. Li, “Lotteryfl: Empower edge intelligence with personalized and communication-efficient federated learning,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/ACM SEC</em>, 2021, pp. 68–79.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Jiang, S. Wang, V. Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and L. Tassiulas, “Model pruning enables efficient federated learning on edge devices,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Neural Netw. Learn. Syst.</em>, pp. 1–13, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
O. Litany, H. Maron, D. Acuna, J. Kautz, G. Chechik, and S. Fidler, “Federated learning with heterogeneous architectures using graph hypernetworks,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08459</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Rapp, R. Khalili, K. Pfeiffer, and J. Henkel, “Distreal: Distributed resource-aware learning in heterogeneous systems,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 36, no. 7, 2022, pp. 8062–8071.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. Yao, W. Pan, M. J. O’Neill, Y. Dai, Y. Wan, H. Jin, and L. Sun, “Fedhm: Efficient federated learning for heterogeneous models via low-rank factorization,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.14655</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C. Ma, J. Li, M. Ding, H. H. Yang, F. Shu, T. Q. S. Quek, and H. V. Poor, “On safeguarding privacy and security in the framework of federated learning,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 34, no. 4, pp. 242–248, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks to Byzantine-Robust federated learning,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">USENIX Security 20</em>, Aug. 2020, pp. 1605–1622.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C. Thapa, P. C. M. Arachchige, S. Camtepe, and L. Sun, “Splitfed: When federated learning meets split learning,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 36, no. 8, 2022, pp. 8485–8493.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.12090" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.12091" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.12091">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.12091" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.12092" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 12:43:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
