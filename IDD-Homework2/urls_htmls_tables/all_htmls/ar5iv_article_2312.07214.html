<article class="ltx_document ltx_authors_1line">
 <div class="ltx_para" id="p1">
  <span class="ltx_ERROR undefined" id="p1.1" lang="en">
   \correspondance
  </span>
 </div>
 <div class="ltx_para" id="p2">
  <span class="ltx_ERROR undefined" id="p2.1" lang="en">
   \extraAuth
  </span>
 </div>
 <h1 class="ltx_title ltx_title_document" lang="en">
  Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author" lang="en">
   <span class="ltx_personname">
    Younes Lakhnati
    <sup class="ltx_sup" id="id8.4.id1">
     <span class="ltx_text ltx_font_italic" id="id8.4.id1.1">
      1,∗
     </span>
    </sup>
    , Max Pascher
    <sup class="ltx_sup" id="id9.5.id2">
     <span class="ltx_text ltx_font_italic" id="id9.5.id2.1">
      1,2,∗
     </span>
    </sup>
    , and Jens Gerken
    <sup class="ltx_sup" id="id10.6.id3">
     <span class="ltx_text ltx_font_italic" id="id10.6.id3.1">
      1,∗
     </span>
    </sup>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_address">
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract" lang="en">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <section class="ltx_section" id="S1">
   <h2 class="ltx_title ltx_title_section">
    <span class="ltx_tag ltx_tag_section">
     1
    </span>
   </h2>
   <div class="ltx_para" id="S1.p1">
    <p class="ltx_p" id="S1.p1.1">
     In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of
     <span class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_long-plural">
       Large Language Models
      </span>
     </span>
     like
     <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">
      <span class="ltx_glossaryref" title="Generative pre-trained transformer">
       <span class="ltx_text ltx_glossary_long">
        Generative pre-trained transformer
       </span>
      </span>
      (
      <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
       <span class="ltx_text ltx_glossary_short">
        GPT
       </span>
      </abbr>
      )
     </span>
     into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication.
In this paper, we introduce a novel framework for such a
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     -powered multi-robot testbed environment, based on a Unity
     <span class="ltx_glossaryref" title="Virtual Reality">
      <span class="ltx_text ltx_glossary_long">
       Virtual Reality
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="Virtual Reality">
      <span class="ltx_text ltx_glossary_short">
       VR
      </span>
     </abbr>
     ) setting. This system allows users to interact with robot agents through natural language, each powered by individual
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     cores. By means of OpenAI’s function calling, we bridge the gap between unstructured natural language input and structure robot actions.
A user study with 12 participants explores the effectiveness of
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     -4 and, more importantly, user strategies when being given the opportunity to converse in natural language within a multi-robot environment. Our findings suggest that users may have preconceived expectations on how to converse with robots and seldom try to explore the actual language and cognitive capabilities of their robot collaborators. Still, those users who did explore where able to benefit from a much more natural flow of communication and human-like back-and-forth. We provide a set of lessons learned for future research and technical implementations of similar systems.
    </p>
   </div>
   <div class="ltx_para" id="S1.p2">
    <span class="ltx_ERROR undefined" id="S1.p2.1">
     \helveticabold
    </span>
   </div>
   <section class="ltx_section" id="S2">
    <h2 class="ltx_title ltx_title_section" style="font-size:50%;">
     <span class="ltx_tag ltx_tag_section">
      2
     </span>
     Keywords:
    </h2>
    <div class="ltx_para" id="S2.p1">
     <p class="ltx_p" id="S2.p1.1">
      <span class="ltx_text" id="S2.p1.1.1" style="font-size:50%;">
       assistive robots, virtual reality, evaluation, shared control, variable autonomy
      </span>
     </p>
    </div>
   </section>
  </section>
 </div>
 <section class="ltx_section" id="S3" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Mankind’s success is predicated upon our ability to communicate and cooperate. This fundamental principle, evident throughout the history of human civilization, has now reached a pivotal point. The tools we have created can think and act on their own. This requires us to rethink past approaches to human-machine interaction, which had relied mainly on humans operating machines, not collaborating with them
    <cite class="ltx_cite ltx_citemacro_citep">
     (Grudin,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2022
     </a>
     )
    </cite>
    . However, for humans to cooperate seamlessly with autonomous tools, such as robots, a delicate balance must be struck between both a user’s need for autonomy
    <cite class="ltx_cite ltx_citemacro_citet">
     Deci and Ryan (
     <a class="ltx_ref" href="#bib.bib14" title="">
      2012
     </a>
     )
    </cite>
    and the robot’s capabilities and level of autonomous behavior. In this context, concepts such as variable autonomy and shared control emerge as a vital research area. Shared control, in particular, aims to find optimal configurations of how control can be shared between an autonomous machine such as a robot and human so that the overall system is as effective and efficient as possible
    <cite class="ltx_cite ltx_citemacro_citep">
     (Erdogan and Argall,
     <a class="ltx_ref" href="#bib.bib16" title="">
      2017
     </a>
     )
    </cite>
    . It requires, at its core, an effective way for communication between parties to negotiate and adapt the sharing of control
    <cite class="ltx_cite ltx_citemacro_citep">
     (Flemisch et al.,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2019
     </a>
     ; Abbink et al.,
     <a class="ltx_ref" href="#bib.bib1" title="">
      2018
     </a>
     ; Pascher et al.,
     <a class="ltx_ref" href="#bib.bib48" title="">
      2023a
     </a>
     )
    </cite>
    . While such communication between human and machines can rely on a variety of input and output modalities, spoken language, as a vehicle of thought, has played an essential role for human-human communication and collaboration
    <cite class="ltx_cite ltx_citemacro_citep">
     (Clark,
     <a class="ltx_ref" href="#bib.bib12" title="">
      1996
     </a>
     , pp. 17–18)
    </cite>
    . The exploration of natural language in fields like human-machine cooperation and AI has a long and storied history.
    <span class="ltx_glossaryref" title="Natural Language Processing">
     <span class="ltx_text ltx_glossary_long">
      Natural Language Processing
     </span>
    </span>
    (
    <abbr class="ltx_glossaryref" title="Natural Language Processing">
     <span class="ltx_text ltx_glossary_short">
      NLP
     </span>
    </abbr>
    ) has been an incredibly significant branch of robotics and
    <span class="ltx_glossaryref" title="Artificial Intelligence">
     <span class="ltx_text ltx_glossary_long">
      Artificial Intelligence
     </span>
    </span>
    (
    <abbr class="ltx_glossaryref" title="Artificial Intelligence">
     <span class="ltx_text ltx_glossary_short">
      AI
     </span>
    </abbr>
    ) research since its infancy, spearheaded by foundational works like Winograd’s
    <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">
     SHRDLU
    </em>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Winograd,
     <a class="ltx_ref" href="#bib.bib67" title="">
      1972
     </a>
     )
    </cite>
    and Nilsson et al.’s
    <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">
     Shakey the Robot
    </em>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Nilsson et al.,
     <a class="ltx_ref" href="#bib.bib43" title="">
      1984
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    Conceptually, however, for language to be an effective means for collaboration, research has suggested the overarching concept of
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.1">
     Joint Actions
    </em>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Clark,
     <a class="ltx_ref" href="#bib.bib12" title="">
      1996
     </a>
     , p. 59)
    </cite>
    , which has become a popular approach to understanding human-human cooperation
    <cite class="ltx_cite ltx_citemacro_citep">
     (Sebanz et al.,
     <a class="ltx_ref" href="#bib.bib57" title="">
      2006
     </a>
     ; Garrod and Pickering,
     <a class="ltx_ref" href="#bib.bib21" title="">
      2004
     </a>
     ; Vesper et al.,
     <a class="ltx_ref" href="#bib.bib65" title="">
      2010
     </a>
     )
    </cite>
    . This concept has the potential to be extrapolated to the human-and-machine side of cooperation, as suggested by
    <cite class="ltx_cite ltx_citemacro_citet">
     Flemisch et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2019
     </a>
     , p. 2)
    </cite>
    . This conceptual perspective is joined by recent advances in the domain of
    <abbr class="ltx_glossaryref" title="Natural Language Processing">
     <span class="ltx_text ltx_glossary_short">
      NLP
     </span>
    </abbr>
    , following Vaswani et al.’s groundbreaking
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.2">
     Transformer
    </em>
    architecture
    <cite class="ltx_cite ltx_citemacro_citep">
     (Vaswani et al.,
     <a class="ltx_ref" href="#bib.bib64" title="">
      2017
     </a>
     )
    </cite>
    , which gave rise to so-called
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    , such as
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    and
    <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">
     <span class="ltx_glossaryref" title="Bidirectional Encoder Representations from Transformers">
      <span class="ltx_text ltx_glossary_long">
       Bidirectional Encoder Representations from Transformers
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="Bidirectional Encoder Representations from Transformers">
      <span class="ltx_text ltx_glossary_short">
       BERT
      </span>
     </abbr>
     )
    </span>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Radford et al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2018
     </a>
     , p. 6)
    </cite>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Devlin et al.,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2018
     </a>
     , p. 6)
    </cite>
    . These have demonstrated competences in fields such as translation
    <cite class="ltx_cite ltx_citemacro_citep">
     (Brown et al.,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2020
     </a>
     , p. 6–7)
    </cite>
    , question-answering
    <cite class="ltx_cite ltx_citemacro_citep">
     (Nakano et al.,
     <a class="ltx_ref" href="#bib.bib41" title="">
      2021
     </a>
     )
    </cite>
    , creative writing
    <cite class="ltx_cite ltx_citemacro_citep">
     (Swanson et al.,
     <a class="ltx_ref" href="#bib.bib60" title="">
      2021
     </a>
     )
    </cite>
    and even medical diagnostics
    <cite class="ltx_cite ltx_citemacro_citep">
     (Hirosawa et al.,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023
     </a>
     ; Nori et al.,
     <a class="ltx_ref" href="#bib.bib44" title="">
      2023
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    With an
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    ’s ability to parse unstructured natural language and to adapt and learn on the fly based on
    <em class="ltx_emph ltx_font_italic" id="S3.p3.1.1">
     prompts
    </em>
    , we see significant potential in their application within the field of shared control and variable autonomy.
Not only do they present modes of interaction that align with our inherent tendencies to use language (like we would in inter-human cooperation), these systems are also very adept at producing believable output that can mirror a conversation.
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    could facilitate the bi-directional flow of information between humans and autonomous agents in a shared control system by understanding nuances, asking for clarifications and providing feedback. Additionally, the degree to which an
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    is autonomous can be influenced by the prompts it is given.
Of course, there are certain inherent challenges in play as well. The use cases where
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    excel do not require to map spoken commands to a very specific output that is bound by the physical capabilities of an autonomous agent. Still, existing work has shown that available
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    such as
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    might already allow the communication and coordination based on natural language with a robot
    <cite class="ltx_cite ltx_citemacro_citep">
     (Koubaa,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023b
     </a>
     )
    </cite>
    .
This paper aims to explore this up and coming research area both from a feasibility and a user perspective. To this end, we have designed an
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    -powered multi-agent system as part of a
    <abbr class="ltx_glossaryref" title="Virtual Reality">
     <span class="ltx_text ltx_glossary_short">
      VR
     </span>
    </abbr>
    -based framework. In the virtual game world, a user can fulfill simple tasks solely by conversing with three simulated robot agents, which are controlled by their own
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    cores. Utilizing the prototype, a user study spanning 12 participants was conducted to investigate the ways in which humans engaged with the robots, how natural it felt to them and how well
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    was suited for this use case.
Consequently, the contribution of this paper consists of
   </p>
  </div>
  <div class="ltx_para" id="S3.p4">
   <ul class="ltx_itemize" id="S3.I1">
    <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S3.I1.i1.p1">
      <p class="ltx_p" id="S3.I1.i1.p1.1">
       The design and architecture of a Unity-based framework for a voice-controlled multi-agent system in VR, with which the interaction and control dynamics between a single user and a scalable amount of
       <abbr class="ltx_glossaryref" title="Large Language Model">
        <span class="ltx_text ltx_glossary_short">
         LLM
        </span>
       </abbr>
       -based robots can be explored
       <span class="ltx_note ltx_role_footnote" id="footnote1">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_note_outer">
         <span class="ltx_note_content">
          <sup class="ltx_note_mark">
           1
          </sup>
          <span class="ltx_tag ltx_tag_note">
           1
          </span>
          We plan to make the framework available as Open-Source on GitHub upon acceptance and are happy to provide it to reviewers on request in the meantime
         </span>
        </span>
       </span>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S3.I1.i2.p1">
      <p class="ltx_p" id="S3.I1.i2.p1.1">
       Based on our study, an improved understanding of user’s strategies and behaviors when being confronted with an
       <abbr class="ltx_glossaryref" title="Large Language Model">
        <span class="ltx_text ltx_glossary_short">
         LLM
        </span>
       </abbr>
       -based multi-robot envionrment.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S3.I1.i3.p1">
      <p class="ltx_p" id="S3.I1.i3.p1.1">
       A set of lessons learned on the feasibility and practicability of adapting
       <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
        <span class="ltx_text ltx_glossary_short">
         GPT
        </span>
       </abbr>
       -based
       <abbr class="ltx_glossaryref" title="Large Language Model">
        <span class="ltx_text ltx_glossary_short-plural">
         LLMs
        </span>
       </abbr>
       to interact with autonomous multi-robot agents.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S4" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Background
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Shared Control &amp; Variable Autonomy
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     While research in autonomous robotics is trending
     <cite class="ltx_cite ltx_citemacro_citep">
      (Canal et al.,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2016
      </a>
      ; Lauretti et al.,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2017
      </a>
      ; Gallenberger et al.,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2019
      </a>
      ; Rakhimkul et al.,
      <a class="ltx_ref" href="#bib.bib55" title="">
       2019
      </a>
      )
     </cite>
     , there are numerous indications that an autonomous robot does not help to satisfy basic psychological needs such as self-autonomy and competence. For example,
     <cite class="ltx_cite ltx_citemacro_citet">
      Kim et al. (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2012
      </a>
      )
     </cite>
     reported that the sporadic and supervisory nature of working with an autonomous robot results in the users experiencing them as
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p1.1.1">
      “one more external agent that does their work for them”
     </span>
     . These findings are corroborated by
     <cite class="ltx_cite ltx_citemacro_citet">
      Pollak et al. (
      <a class="ltx_ref" href="#bib.bib52" title="">
       2020
      </a>
      )
     </cite>
     who reported that manual control compared to autonomous behavior led to significantly reduced symptoms of stress and a lower heart rate. Similarly,
     <cite class="ltx_cite ltx_citemacro_citet">
      Złotowski et al. (
      <a class="ltx_ref" href="#bib.bib70" title="">
       2017
      </a>
      )
     </cite>
     found that autonomous robots generally evoke a more negative attitude than non-autonomous robots and are experienced as more threatening.
     <cite class="ltx_cite ltx_citemacro_citet">
      Latikka et al. (
      <a class="ltx_ref" href="#bib.bib35" title="">
       2021
      </a>
      )
     </cite>
     reported, that in particular in the workplace, users preferred to see robots as equipment and as a result preferred non-autonomous robots. In addition
     <cite class="ltx_cite ltx_citemacro_citet">
      Park et al. (
      <a class="ltx_ref" href="#bib.bib46" title="">
       2020
      </a>
      )
     </cite>
     showed that fully autonomous robots may discourage people with motor impairments from using their remaining physical capabilities.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     To counter that, shared-control (or traded-control) systems aim to strike a balance between autonomous robot behavior and manual user control
     <cite class="ltx_cite ltx_citemacro_citep">
      (Erdogan and Argall,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2017
      </a>
      ; Pascher et al.,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2023b
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib47" title="">
       2024
      </a>
      )
     </cite>
     . Due to the lack of clear definitions of these terms,
     <cite class="ltx_cite ltx_citemacro_citet">
      Abbink et al. (
      <a class="ltx_ref" href="#bib.bib1" title="">
       2018
      </a>
      )
     </cite>
     introduced a topology of shared control systems and axioms for the design and evaluation thereof, unifying varying shared control concepts and definitions under one common framework.
     <cite class="ltx_cite ltx_citemacro_citet">
      Flemisch et al. (
      <a class="ltx_ref" href="#bib.bib18" title="">
       2019
      </a>
      )
     </cite>
     later expanded upon
     <cite class="ltx_cite ltx_citemacro_citeauthor">
      <a class="ltx_ref" href="#bib.bib1" title="">
       Abbink et al.
      </a>
     </cite>
     ’s framework and explored shared-control in the context of human-robot cooperation, emphasizing the importance of cooperation on multiple layers of interaction, such as a strategical, tactical and operational layer.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     <cite class="ltx_cite ltx_citemacro_citet">
      Abbink et al. (
      <a class="ltx_ref" href="#bib.bib1" title="">
       2018
      </a>
      )
     </cite>
     argue, that robot functionality, and as a result users preferences and abilities, are situational and depend on context. As a result, concepts such as variable autonomy or adjustable autonomy have emerged, which build upon the principle of shared control by adding a dynamic and situated component
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chiou et al.,
      <a class="ltx_ref" href="#bib.bib11" title="">
       2023
      </a>
      ; Bustamante et al.,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2021
      </a>
      )
     </cite>
     . They often provide the user with some means to adjust the degree and level of involvement or control in a task. While typically, such means are realized through user interface elements, e.g. control elements such as buttons, research has long aimed to establish language as means of collaborating with a machine.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Natural Language Interface systems and Large Language Models
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     The domain of
     <span class="ltx_glossaryref" title="Natural language interface">
      <span class="ltx_text ltx_glossary_long-plural">
       Natural language interfaces
      </span>
     </span>
     represents an important reference point for our research. Historical works such as
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">
      SHRDLU
     </span>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Winograd,
      <a class="ltx_ref" href="#bib.bib67" title="">
       1972
      </a>
      )
     </cite>
     and
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">
      LUNAR system
     </span>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Woods,
      <a class="ltx_ref" href="#bib.bib68" title="">
       1973
      </a>
      )
     </cite>
     are early examples of natural language being used to control complex systems. Both Woods and Winograd identified that the biggest challenges stemmed from semantic barriers that would need to be overcome by advancements in the field of
     <abbr class="ltx_glossaryref" title="Natural language interface">
      <span class="ltx_text ltx_glossary_short">
       NLI
      </span>
     </abbr>
     and
     <abbr class="ltx_glossaryref" title="Natural Language Processing">
      <span class="ltx_text ltx_glossary_short">
       NLP
      </span>
     </abbr>
     . While still battling with the challenges of human linguistics, commercial product developments in the form of
     <span class="ltx_glossaryref" title="Virtual Personal Assistant">
      <span class="ltx_text ltx_glossary_long-plural">
       Virtual Personal Assistants
      </span>
     </span>
     or
     <span class="ltx_glossaryref" title="Intelligent Personal Assistant">
      <span class="ltx_text ltx_glossary_long-plural">
       Intelligent Personal Assistants
      </span>
     </span>
     , such as Microsoft’s Cortana, Apple’s Siri, Amazon Alexa, Google Assistant, and so on were able to create a public awareness, establish a user base and provide a better understanding of the potential of such systems
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kepuska and Bohouta,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2018
      </a>
      )
     </cite>
     . Related approaches were also introduced into the field of robotics – here
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu and Zhang (
      <a class="ltx_ref" href="#bib.bib39" title="">
       2019
      </a>
      )
     </cite>
     offer an extensive overview related to NLI-controlled robots.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     While these approaches were certainly impressive, they still lacked a fundamental understanding that transcends their confined domains.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     Following
     <cite class="ltx_cite ltx_citemacro_citet">
      Vaswani et al. (
      <a class="ltx_ref" href="#bib.bib64" title="">
       2017
      </a>
      )
     </cite>
     ’s breakthrough work on self-attention, the development of transformer models has reshaped the landscape of
     <abbr class="ltx_glossaryref" title="Artificial Intelligence">
      <span class="ltx_text ltx_glossary_short">
       AI
      </span>
     </abbr>
     as well as
     <abbr class="ltx_glossaryref" title="Natural Language Processing">
      <span class="ltx_text ltx_glossary_short">
       NLP
      </span>
     </abbr>
     and gave rise to
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     : AI systems, which are trained on massive amounts of textual data using deep learning techniques. Building upon this concept,
     <abbr class="ltx_glossaryref" title="Bidirectional Encoder Representations from Transformers">
      <span class="ltx_text ltx_glossary_short">
       BERT
      </span>
     </abbr>
     , revolutionized many
     <abbr class="ltx_glossaryref" title="Natural Language Processing">
      <span class="ltx_text ltx_glossary_short">
       NLP
      </span>
     </abbr>
     tasks by training bidirectionally, meaning it considers both the left and the right context in all layers, capturing information from the entire passage
     <cite class="ltx_cite ltx_citemacro_citep">
      (Devlin et al.,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2018
      </a>
      )
     </cite>
     . This made BERT particularly suited for tasks that require understanding context. On the other hand,
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     is a unidirectional model (every token can only attend to previous tokens) trained to predict the next word in a sequence
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford et al.,
      <a class="ltx_ref" href="#bib.bib54" title="">
       2018
      </a>
      )
     </cite>
     . Despite this, it excels in various
     <abbr class="ltx_glossaryref" title="Natural Language Processing">
      <span class="ltx_text ltx_glossary_short">
       NLP
      </span>
     </abbr>
     tasks by leveraging its transformer architecture and a massive amount of data for training.
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     is especially noteworthy for its capability to generate coherent and contextually relevant text over long passages. Both,
     <abbr class="ltx_glossaryref" title="Bidirectional Encoder Representations from Transformers">
      <span class="ltx_text ltx_glossary_short">
       BERT
      </span>
     </abbr>
     and
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     have not only been released for research collaboration, but have been integrated into commercial products and provide
     <span class="ltx_glossaryref" title="Application Programming Interface">
      <span class="ltx_text ltx_glossary_long">
       Application Programming Interface
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="Application Programming Interface">
      <span class="ltx_text ltx_glossary_short">
       API
      </span>
     </abbr>
     ) access for software developers. Noteworthy, there is a sheer infinite number of further
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     and several significant extensions to these existing approaches
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yang et al.,
      <a class="ltx_ref" href="#bib.bib69" title="">
       2019
      </a>
      ; Brown et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2020
      </a>
      ; Chen et al.,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2021
      </a>
      ; Neelakantan et al.,
      <a class="ltx_ref" href="#bib.bib42" title="">
       2022
      </a>
      ; Stiennon et al.,
      <a class="ltx_ref" href="#bib.bib59" title="">
       2020
      </a>
      ; Ouyang et al.,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2022
      </a>
      )
     </cite>
     . Thanks to the generality of language, these models have been applied in various applications, ranging from search engines and chat bots
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kelly et al.,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2023
      </a>
      )
     </cite>
     , general problem-solving and knowledge extraction
     <cite class="ltx_cite ltx_citemacro_citep">
      (Petroni et al.,
      <a class="ltx_ref" href="#bib.bib51" title="">
       2019
      </a>
      )
     </cite>
     , medical diagnosis
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hirosawa et al.,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
      ; Nori et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023
      </a>
      ; Waisberg et al.,
      <a class="ltx_ref" href="#bib.bib66" title="">
       2023
      </a>
      ; Shea et al.,
      <a class="ltx_ref" href="#bib.bib58" title="">
       2023
      </a>
      ; Rao et al.,
      <a class="ltx_ref" href="#bib.bib56" title="">
       2023
      </a>
      )
     </cite>
     , education
     <cite class="ltx_cite ltx_citemacro_citep">
      (Tack and Piech,
      <a class="ltx_ref" href="#bib.bib61" title="">
       2022
      </a>
      ; Firat,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023
      </a>
      ; Ausat et al.,
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      )
     </cite>
     , law
     <cite class="ltx_cite ltx_citemacro_citep">
      (Cyphert,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2021
      </a>
      ; Perlman,
      <a class="ltx_ref" href="#bib.bib50" title="">
       2022
      </a>
      ; Trozze et al.,
      <a class="ltx_ref" href="#bib.bib63" title="">
       2023
      </a>
      )
     </cite>
     and robotics, as we will discuss in the next section.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    LLMs and Robots
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     have opened up new possibilities in the field of robotics and human-robot teaming, most apparently for social robots and
     <span class="ltx_glossaryref" title="Socially Assistive Robot">
      <span class="ltx_text ltx_glossary_long-plural">
       Socially Assistive Robots
      </span>
     </span>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Lee et al.,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023
      </a>
      ; Alessa and Al-Khalifa,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2023
      </a>
      ; Kahambing,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2023
      </a>
      ; Lekova et al.,
      <a class="ltx_ref" href="#bib.bib38" title="">
       2023
      </a>
      ; Irfan et al.,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023
      </a>
      )
     </cite>
     . However, the ability of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     to converse is not only beneficial in social settings but their ability to process all kinds of textual input seems extremely promising in environments like speech-controlled robotics, which have historically struggled with unstructured input. Recent research indicates that
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     work well in this environment and that users generally prefer unstructured modes of communication in comparison to structured ones
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kodur et al.,
      <a class="ltx_ref" href="#bib.bib32" title="">
       2023
      </a>
      , p. 10)
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     The issue at hand is the mapping between unstructured voice input and structured robot actions, sometimes referred to as language grounding or as part of the alignment problem.
     <cite class="ltx_cite ltx_citemacro_citet">
      Trott et al. (
      <a class="ltx_ref" href="#bib.bib62" title="">
       2015
      </a>
      )
     </cite>
     , while not yet being able to rely on a LLM, presented a promising architectural layout for verbal communication with multiple robots. They introduced a boss-agent, which routes the communication between multiple robots and the user via N-tuple, which is translated into a pattern response. While the approach was limited through the need for an explicit grammar and lexicon, similar to the grammar based approach by
     <cite class="ltx_cite ltx_citemacro_citet">
      Misra et al. (
      <a class="ltx_ref" href="#bib.bib40" title="">
       2016
      </a>
      )
     </cite>
     , we adapted this approach for our work, as we will discuss in the next chapter.
     <cite class="ltx_cite ltx_citemacro_citet">
      Arumugam et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2019
      </a>
      )
     </cite>
     analyzed the issue of language grounding more closely and proposed an approach to achieve a high
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p2.1.1">
      “accuracy, efficiency and generalization through the construction of simple, semantic goal representations within Markov decision processes”
     </span>
     .
     <cite class="ltx_cite ltx_citemacro_citet">
      Ahn et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2022
      </a>
      )
     </cite>
     propose a method called
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p2.1.2">
      “SayCan”
     </span>
     , that integrates the capabilities of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     with robotic affordances (learned mappings that quantify the likelihood of a robot successfully executing specific actions given its current state) to better interpret and execute high-level textual instructions. Their approach seeks to bridge the gap between the knowledge embedded within
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     and the practical constraints of physical environments. Similar to our work,
     <cite class="ltx_cite ltx_citemacro_citet">
      Koubaa (
      <a class="ltx_ref" href="#bib.bib33" title="">
       2023a
      </a>
      )
     </cite>
     integrated a
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     model and a parser to operate a robot – in this case a real robot through
     <span class="ltx_glossaryref" title="Robot Operating System">
      <span class="ltx_text ltx_glossary_long">
       Robot Operating System
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="Robot Operating System">
      <span class="ltx_text ltx_glossary_short">
       ROS
      </span>
     </abbr>
     ). They employed an ontology approach to map unstructured language input into structured robotic instructions, but encountered issues with the model unexpectedly straying from the ontology at times. We initially chose the same approach and encountered similar issues of GPT not adhering to the ontology.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p3">
    <p class="ltx_p" id="S4.SS3.p3.1">
     Also just recently,
     <cite class="ltx_cite ltx_citemacro_citeauthor">
      <a class="ltx_ref" href="#bib.bib23" title="">
       Händler
      </a>
     </cite>
     introduced a multi-dimensional taxonomy, specifically designed to evaluate and categorize the way autonomous
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -powered multi-agent systems manage the relationship between autonomy and alignment
     <cite class="ltx_cite ltx_citemacro_cite">
      Händler (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      )
     </cite>
     . This analysis covers various architectural perspectives, focusing on their intrinsic characteristics and operational mechanisms. Händler’s approach is motivated by the observation that existing taxonomies fall short to categorize and understand the complexity of these new systems.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p4">
    <p class="ltx_p" id="S4.SS3.p4.1">
     In 2023, OpenAI introduced function calls
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        OpenAI Function calling.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/function-calling" target="_blank" title="">
         https://platform.openai.com/docs/guides/function-calling
        </a>
        , last retrieved February 27, 2024.
       </span>
      </span>
     </span>
     as a way to bridge the gap between unstructured text input and structured system operation that is much less prone to unexpected model behaviors. We see this as promising alternative to the existing approaches that have been discussed here. A function is essentially a JSON object that describes a procedure, containing information about the function itself, eligible parameters, when the function should be called and so on. Based on this description, a GPT module, which would usually respond to a user in form of a chat response, can decide to not answer at all but return a function call instead. This decision is up to the LLM’s discretion and is heavily influenced by previous messages and the descriptions of its available functions. For example, if a GPT module is fed with function descriptions containing a
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.1">
      “pick up”
     </span>
     function that defines
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.2">
      “apple”
     </span>
     as an eligible parameter, a user asking it anything close enough to
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.3">
      “pick up the apple”
     </span>
     would cause it to return a
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.4">
      “pick up”
     </span>
     function call with the parameter
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.5">
      “apple”
     </span>
     . Conversely, if a user asked the LLM to pick up an object that is not listed as an eligible parameter in the
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.6">
      “pick up”
     </span>
     function description, the LLM would not execute the function call but ask the user for clarification instead. As a result, function calls also provide an additional grounding of a GPT module. In addition, function calls can be employed as GPT’s close connection to the program’s backend. Since function calls (unlike an
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     ’s regular answers) are clearly defined and predictable, procedures in the code can be called as a direct consequence of the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     ’s decisions. For instance, if the GPT module returns the example function call from above, the extracted information can be used to start whatever routine is defined as the
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.7">
      “pick up”
     </span>
     procedure, with
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS3.p4.1.8">
      “apple”
     </span>
     as its parameter.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p5">
    <p class="ltx_p" id="S4.SS3.p5.1">
     Aiming to understand how users interact with machines using language, the work of Porcheron and colleagues is notable. Starting with
     <abbr class="ltx_glossaryref" title="Intelligent Personal Assistant">
      <span class="ltx_text ltx_glossary_short-plural">
       IPAs
      </span>
     </abbr>
     ,
     <cite class="ltx_cite ltx_citemacro_citeauthor">
      <a class="ltx_ref" href="#bib.bib53" title="">
       Porcheron et al.
      </a>
     </cite>
     examined the integration of IPAs within human conversation, focusing on linguistic aspects. They explore how IPAs such as Siri and Alexa influence conversational dynamics and shed light on the linguistic interplay between human users and AI agents
     <cite class="ltx_cite ltx_citemacro_citep">
      (Porcheron et al.,
      <a class="ltx_ref" href="#bib.bib53" title="">
       2017
      </a>
      )
     </cite>
     . Expanding on this theme,
     <cite class="ltx_cite ltx_citemacro_citeauthor">
      <a class="ltx_ref" href="#bib.bib19" title="">
       Fuentes et al.
      </a>
     </cite>
     explore linguistic challenges faced in human-robot interactions in low-resource environments
     <cite class="ltx_cite ltx_citemacro_citep">
      (Fuentes et al.,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     . Their research emphasized the importance of robots possessing a deep contextual understanding to accurately interpret user instructions. They identified natural language challenges such as referential expression resolution and the dynamic nature of language, which can pose hurdles in human-robot communication. Their findings accentuate the need for improved linguistic reasoning in robots, especially in specialized environments where context plays a key role.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Take-away
   </h3>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     In our work, we add to this emerging area of research by addressing two perspectives in particular. First, we realized that while the utilization and integration of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     has become much easier, prototyping
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -powered robots is not straightforward which limits our ability to explore the inherent trade-offs and limitations. Therefore, we explored and developed the integration of a
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     -4 model within a Unity-based simulation environment, relying on the just introduced function calls capabilities to map unstructured speech input to structured robot actions. Second, there is currently very little research which helps to understand how the combination of speech input and robots influences and changes the way users talk and interact with robots or virtual agents. In particular, the non-deterministic nature of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     makes it essential to explore this in a working implementation of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -powered robot agents. Wizard-of-Oz like studies, such as
     <cite class="ltx_cite ltx_citemacro_citet">
      Fuentes et al. (
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     , mostly try to simulate a perfectly functioning
     <abbr class="ltx_glossaryref" title="Artificial Intelligence">
      <span class="ltx_text ltx_glossary_short">
       AI
      </span>
     </abbr>
     , while human-robot conversation may be most interesting in situations where there is miscommunication.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   <abbr class="ltx_glossaryref" title="Large Language Model">
    <span class="ltx_text ltx_glossary_short">
     LLM
    </span>
   </abbr>
   Framework Concept and Architecture
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    As discussed, the application of
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    to the field of robotics is still in its infancy, but could be in particular promising to utilize the inherent capability of language-based communication to support variable autonomy in a human-robot teaming environment.
Still, a range of challenges must be addressed to effectively develop a flexible
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    -based framework for the study of human-robot collaboration, which is the focus of this chapter.
   </p>
  </div>
  <div class="ltx_para" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    To give an overview, these are:
   </p>
   <ul class="ltx_itemize" id="S5.I1">
    <li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I1.i1.p1">
      <p class="ltx_p" id="S5.I1.i1.p1.1">
       Decide and select between an existing generic
       <abbr class="ltx_glossaryref" title="Large Language Model">
        <span class="ltx_text ltx_glossary_short">
         LLM
        </span>
       </abbr>
       compared to either fine-tuned or custom-trained models.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I1.i2.p1">
      <p class="ltx_p" id="S5.I1.i2.p1.1">
       Mapping unstructured verbal input to actionable robot agent behavior.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I1.i3.p1">
      <p class="ltx_p" id="S5.I1.i3.p1.1">
       Provide a flexible testbed-framework, which allows the technical exploration as well as the conduction of user studies.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I1.i4.p1">
      <p class="ltx_p" id="S5.I1.i4.p1.1">
       Provide a scalable architecture for the framework, which supports multiple robot agents.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    <em class="ltx_emph ltx_font_italic" id="S5.p3.1.1">
     LLM selection
    </em>
    : The motivation for our research is grounded in the idea that advanced
    <abbr class="ltx_glossaryref" title="Artificial Intelligence">
     <span class="ltx_text ltx_glossary_short">
      AI
     </span>
    </abbr>
    technologies, such as
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    , have reached a level of maturity that they can be applied by non-AI experts and thereby be leveraged for a variety of contexts, such as human-robot teaming. Therefore, we decided to select an existing general purpose
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    for our framework and explore the capabilities and limitations this may pose.
   </p>
  </div>
  <div class="ltx_para" id="S5.p4">
   <p class="ltx_p" id="S5.p4.1">
    <em class="ltx_emph ltx_font_italic" id="S5.p4.1.1">
     Mapping problem
    </em>
    : Based on the second challenge, mapping unstructured verbal input to actionable robot agent behavior, we eventually decided to utilize OpenAI’s GPT-4 model, as the already described function calls capability provides a promising approach to overcome this problem. One additional reason why function calls are worth exploring in this context is that they provide a degree of agency to the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    . Instead of just answering in chat messages, when the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    deems it probable enough that the user wants it to execute an action which is within its abilities, it can decide to
    <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.p4.1.2">
     “seize control”
    </span>
    and initiate a function. However, when the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    is unsure and requires further information, it can
    <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.p4.1.3">
     “relinquish control”
    </span>
    again. For example, if a user asked the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    to pick up an object that is not listed as an eligible parameter in a
    <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.p4.1.4">
     “pick up”
    </span>
    function description, the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    would not execute the function call but ask the user for clarification instead.
   </p>
  </div>
  <div class="ltx_para" id="S5.p5">
   <p class="ltx_p" id="S5.p5.1">
    In this way, the function descriptions also provide a scaffolding to the model of what it can and cannot do. The
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    can then share this information with the user, contributing to a shared understanding of what the agents are capable of and to which degree they can cooperate with the user. Function descriptions are also highly extensible and modifiable, making them well-suited for an experimental framework that might need to be adjusted to investigate specific situations and contexts.
   </p>
  </div>
  <div class="ltx_para" id="S5.p6">
   <p class="ltx_p" id="S5.p6.1">
    <em class="ltx_emph ltx_font_italic" id="S5.p6.1.1">
     Testbed framework
    </em>
    :
We decided for a
    <abbr class="ltx_glossaryref" title="Virtual Reality">
     <span class="ltx_text ltx_glossary_short">
      VR
     </span>
    </abbr>
    -based testbed framework to both explore technical exploration of
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    -based human-robot teaming and also study human-robot interaction and communication in a flexible and adaptable environment.
    <abbr class="ltx_glossaryref" title="Virtual Reality">
     <span class="ltx_text ltx_glossary_short">
      VR
     </span>
    </abbr>
    was chosen over physical robots to simplify complexity and increase flexibility, while also maintaining an immersive environment to facilitate realistic interaction dynamics.
   </p>
  </div>
  <figure class="ltx_figure" id="S5.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S5.F1.g1" src="/html/2312.07214/assets/figures/LLMUser.jpg" width="538"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S5.F1.2.1.1" style="font-size:90%;">
      Figure 1
     </span>
     :
    </span>
    <span class="ltx_text" id="S5.F1.3.2" style="font-size:90%;">
     A mock-up image portraying a user in the gameworld, interacting with the three robots, Neptune (in blue), Jupiter (in yellow) and Pluto (in red).
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S5.p7">
   <p class="ltx_p" id="S5.p7.1">
    In our initial framework setting, we feature three heterogeneous robot agents named
    <span class="ltx_text ltx_font_italic" id="S5.p7.1.1">
     Jupiter
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S5.p7.1.2">
     Pluto
    </span>
    , and
    <span class="ltx_text ltx_font_italic" id="S5.p7.1.3">
     Neptune
    </span>
    (see
    <a class="ltx_ref ltx_refmacro_autoref" href="#S5.F1" title="Figure 1 ‣ 5 Framework Concept and Architecture ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
     <span class="ltx_text ltx_ref_tag">
      Figure 1
     </span>
    </a>
    ). Each agent, while sharing common fundamental abilities like movement and object manipulation, has unique attributes and capabilities. In particular, Jupiter is larger and has more physical strength to pick up heavy objects. Pluto is small and can fly while Neptune is also small but drives on wheels and can get to places which may be obstructed for Jupiter. The physical environment is a abstract combination of multiple rooms that are connected through doors. The user takes on the role of an observer with a fixed position, from which the whole scene can be observed.
Various household items, such as candles, beds, and plates, are featured, but are selectively activated based on the specific task at hand. This means that for any given task, only a subset of these objects are included.
   </p>
  </div>
  <div class="ltx_para" id="S5.p8">
   <p class="ltx_p" id="S5.p8.1">
    To make the interaction seamless and emulate the conversational nature of human-human cooperation, our framework allows the user to talk to the agents in a completely unstructured way. The framework includes a
    <span class="ltx_text ltx_font_italic" id="S5.p8.1.1">
     speech-to-text-transcription
    </span>
    of the microphone input to provide the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    with text-input and uses
    <span class="ltx_text ltx_font_italic" id="S5.p8.1.2">
     text-to-speech-synthesis
    </span>
    as output to interact with the user.
   </p>
  </div>
  <div class="ltx_para" id="S5.p9">
   <p class="ltx_p" id="S5.p9.1">
    <em class="ltx_emph ltx_font_italic" id="S5.p9.1.1">
     Scalable architecture
    </em>
    :
    <a class="ltx_ref ltx_refmacro_autoref" href="#S5.F2" title="Figure 2 ‣ 5 Framework Concept and Architecture ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
     <span class="ltx_text ltx_ref_tag">
      Figure 2
     </span>
    </a>
    illustrates the overall architecture of our framework, which we will explain here in more detail. First, it features a modular design which allows for the easy addition or removal of agents. Naturally, in a situation involving multiple agents, additional challenges arise, such as deciding which agent the user is currently talking to, how to deal with commands that are directed at multiple agents and how to handle continuous conversations where no new recipient is decided. We bypassed traditional methods (like specific buttons for each agent) in favor of a central
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    controller. In addition, each agent is powered by it’s own
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    instance.
This controller interprets user inputs and distributes them to relevant agents (0…n) based on conversation history and context. This decentralized approach is scalable and computationally efficient, as it limits the need for all agents to process every input.
   </p>
  </div>
  <div class="ltx_para" id="S5.p10">
   <p class="ltx_p" id="S5.p10.1">
    The controller and agents are initialized with a respective starting prompt. A typical starting prompt is constructed as follows:
   </p>
  </div>
  <div class="ltx_para" id="S5.p11">
   <ul class="ltx_itemize" id="S5.I2">
    <li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I2.i1.p1">
      <p class="ltx_p" id="S5.I2.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">
        Briefing:
       </span>
       The model is told which role it embodies and what the current situation is, e.g.,
       <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.I2.i1.p1.1.2">
        “You are the yellow robot agent Jupiter…”
       </span>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I2.i2.p1">
      <p class="ltx_p" id="S5.I2.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">
        Restraints:
       </span>
       The model is instructed to stay in character and not slip back into unwanted
       <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
        <span class="ltx_text ltx_glossary_short">
         GPT
        </span>
       </abbr>
       standard behaviors, e.g.,
       <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.I2.i2.p1.1.2">
        “Don’t ever break character. Don’t ever mention that you are an
        <abbr class="ltx_glossaryref" title="Artificial Intelligence">
         <span class="ltx_text ltx_glossary_short">
          AI
         </span>
        </abbr>
        model”
       </span>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S5.I2.i3.p1">
      <p class="ltx_p" id="S5.I2.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.1">
        Few-shot examples:
       </span>
       Providing the model with a few examples of how to
       <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.I2.i3.p1.1.2">
        “correctly”
       </span>
       behave as part of its prompt has been proven to be an effective way to enhance its performance
       <cite class="ltx_cite ltx_citemacro_citep">
        (Brown et al.,
        <a class="ltx_ref" href="#bib.bib7" title="">
         2020
        </a>
        )
       </cite>
       .
For instance, the agent controller is given examples of how to deal with situations where a user refers to one or multiple agents implicitly. As an example, the controller is instructed that, when continuing a conversation with a single agent,
       <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.I2.i3.p1.1.3">
        “you”
       </span>
       probably refers to that agent but when engaged in a conversation with multiple agents,
       <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.I2.i3.p1.1.4">
        “you”
       </span>
       can refer to all of them.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S5.p12">
   <p class="ltx_p" id="S5.p12.1">
    Another important part of the agent’s initialization are the function descriptions, which are not technically part of the initial prompt but are described to the model in a similar way.
   </p>
  </div>
  <figure class="ltx_figure" id="S5.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S5.F2.g1" src="/html/2312.07214/assets/figures/LLM.jpg" width="538"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S5.F2.2.1.1" style="font-size:90%;">
      Figure 2
     </span>
     :
    </span>
    <span class="ltx_text" id="S5.F2.3.2" style="font-size:90%;">
     An illustration of the architecture of the introduced framework. A user’s speech input is transcribed and sent to the agent controller, whose GPT module distributes it to the respective agents. The agent’s GPT module, based on its function descriptions and the state of its robot representation, interprets the command and decides to execute a function and then responds. The response is synthesized into audio that is received by the user. Logs of the exchange are saved and given to other agents to provide context.
    </span>
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Implementation
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     For our testbed framework we used a
     <abbr class="ltx_glossaryref" title="Virtual Reality">
      <span class="ltx_text ltx_glossary_short">
       VR
      </span>
     </abbr>
     environment, which was created using the
     <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.1">
      Unity 2022.3.4f1
     </em>
     editor and optimized for usage with a
     <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.2">
      Meta Quest 2
     </em>
     <abbr class="ltx_glossaryref" title="Virtual Reality">
      <span class="ltx_text ltx_glossary_short">
       VR
      </span>
     </abbr>
     <span class="ltx_glossaryref" title="Head-Mounted Display">
      <span class="ltx_text ltx_glossary_long">
       Head-Mounted Display
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="Head-Mounted Display">
      <span class="ltx_text ltx_glossary_short">
       HMD
      </span>
     </abbr>
     ). Srcnalt’s OpenAI-Unity package
     <span class="ltx_note ltx_role_footnote" id="footnote3">
      <sup class="ltx_note_mark">
       3
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         3
        </sup>
        <span class="ltx_tag ltx_tag_note">
         3
        </span>
        OpenAI-Unity Package.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/srcnalt/OpenAI-Unity" target="_blank" title="">
         https://github.com/srcnalt/OpenAI-Unity
        </a>
        , last retrieved February 27, 2024.
       </span>
      </span>
     </span>
     was used to connect Unity to the OpenAI endpoints.
During the study, user behavior was recorded with appropriate software on a
     <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.3">
      Schenker XMG Key 17
     </em>
     laptop with
     <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.4">
      Windows 10 64-bit
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.5">
      Oculus Link
     </em>
     connected to the
     <abbr class="ltx_glossaryref" title="Virtual Reality">
      <span class="ltx_text ltx_glossary_short">
       VR
      </span>
     </abbr>
     headset.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     The user communicates with the robot agents via speech input, which is then transcribed by
     <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">
      OpenAI’s Whisper v2
     </span>
     .
     <span class="ltx_note ltx_role_footnote" id="footnote4">
      <sup class="ltx_note_mark">
       4
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         4
        </sup>
        <span class="ltx_tag ltx_tag_note">
         4
        </span>
        OpenAI Speech to text.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/speech-to-text" target="_blank" title="">
         https://platform.openai.com/docs/guides/speech-to-text
        </a>
        , last retrieved February 27, 2024.
       </span>
      </span>
     </span>
     The transcription is subsequently sent to the agent controller, whose
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     <span class="ltx_note ltx_role_footnote" id="footnote5">
      <sup class="ltx_note_mark">
       5
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         5
        </sup>
        <span class="ltx_tag ltx_tag_note">
         5
        </span>
        OpenAI API Reference.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/api-reference/chat" target="_blank" title="">
         https://platform.openai.com/docs/api-reference/chat
        </a>
        , last retrieved February 27, 2024.
       </span>
      </span>
     </span>
     module decides which agent the user is addressing, based on previous messages and context information.
Lastly, their output is transformed into sound via
     <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">
      Amazon Polly
     </span>
     .
     <span class="ltx_note ltx_role_footnote" id="footnote6">
      <sup class="ltx_note_mark">
       6
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         6
        </sup>
        <span class="ltx_tag ltx_tag_note">
         6
        </span>
        Amazon Polly.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/polly/" target="_blank" title="">
         https://aws.amazon.com/polly/
        </a>
        , last retrieved February 27, 2024.
       </span>
      </span>
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p3">
    <p class="ltx_p" id="S5.SS1.p3.1">
     All agents employ the
     <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.1">
      gpt-4-0613
     </span>
     model
     <span class="ltx_note ltx_role_footnote" id="footnote7">
      <sup class="ltx_note_mark">
       7
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         7
        </sup>
        <span class="ltx_tag ltx_tag_note">
         7
        </span>
        OpenAI GPT-4 and GPT-4 Turbo.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" target="_blank" title="">
         https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo
        </a>
        , last retrieved February 27, 2024
       </span>
      </span>
     </span>
     , which was up until November 2023 the latest
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     model optimized for function calling. Our framework exclusively uses chat completions
     <span class="ltx_note ltx_role_footnote" id="footnote8">
      <sup class="ltx_note_mark">
       8
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         8
        </sup>
        <span class="ltx_tag ltx_tag_note">
         8
        </span>
        OpenAI Chat Completions API.
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/text-generation/chat-completions-api" target="_blank" title="">
         https://platform.openai.com/docs/guides/text-generation/chat-completions-api
        </a>
        , last retrieved February 27, 2024
       </span>
      </span>
     </span>
     , meaning that the
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     models receive a growing list of messages from user interactions. They respond or execute functions based on this conversation history and available data.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p4">
    <p class="ltx_p" id="S5.SS1.p4.1">
     Function outputs are validated by the system, and the
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     modules are updated with context, mimicking a robot’s sensory input (like cameras and sensors) in a real-world scenario. For instance, if an agent attempts to pick up an object, the system evaluates the distance between the agent and the object, updating the model with textual feedback to facilitate the task execution. This approach simplifies the simulation’s validation process.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Study Method and Materials
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    To observe interaction between participants and the multi-robot systems we conducted an exploratory within-subjects study with 12 participants. There were 14 participants in total but the system went through significant changes after the first two sessions. Consequently, participant 1 (P1) and P2 were excluded from further data analysis and the remaining participants were labeled P3 through P14 to reflect this decision.
   </p>
  </div>
  <div class="ltx_para" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    The participants’ ages varied between 20 and 68 years. The average age (mean, M) was 36.45 years, with a standard deviation (SD) of 14.57 years. Out of the 12 participants, 9 of them self-identified as female and 3 of them self-identified as male. All except one of them were students or employees of the
    <span class="ltx_text ltx_font_italic" id="S6.p2.1.1">
     TU Dortmund University
    </span>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S6.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.1
    </span>
    Procedure
   </h3>
   <div class="ltx_para" id="S6.SS1.p1">
    <p class="ltx_p" id="S6.SS1.p1.1">
     The study was conducted face-to-face in a small room, with the participants sitting on a chair at the far side of a table. Before starting, participants were briefed on the study’s objectives and the mode by which they would communicate with the application. They willingly gave their consent for participation, as well as for audio and video recording of the session. During the study, participants used a Quest 2
     <abbr class="ltx_glossaryref" title="Virtual Reality">
      <span class="ltx_text ltx_glossary_short">
       VR
      </span>
     </abbr>
     headset and controlled the application with a Quest 2 controller in their right hand, while the left controller was used by a researcher to toggle between the different tasks.
The Quest 2 footage was streamed to a laptop for recording purposes. The laptop’s speakers were used to play back the sound of the application, like the synthesized voice replies of the virtual robots. The participants were free to ask questions during the experiment but were only given answers of a pre-defined nature or encouraged to ask the virtual robots for help.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS1.p2">
    <p class="ltx_p" id="S6.SS1.p2.1">
     All participants underwent the same condition, with the tasks building on one another. The experiment spanned seven tasks in total, all sharing the same starting position. In order to avoid any bias due to verbal or written task descriptions, participants were given a virtual task goal screen. By comparing that to the starting position, participants were able to derive the necessary task instructions they had to give the robots. During the task, participants could check back with the task goal screen at all times, which was placed virtually below the scene. All task goals are depicted in
     <a class="ltx_ref ltx_refmacro_autoref" href="#S6.SS1.1" title="6.1 Procedure ‣ 6 Study Method and Materials ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
      <span class="ltx_text ltx_ref_tag">
       Section 6.1
      </span>
     </a>
     :
    </p>
   </div>
   <figure class="ltx_figure" id="S6.SS1.1">
    <p class="ltx_p" id="S6.SS1.1.1">
     minipage0.24
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S6.SS1.1.1.g1" src="/html/2312.07214/assets/figures/environmentscreenshot.jpg" width="598"/>
    </p>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S6.SS1.1.2.1.1" style="font-size:90%;">
       (a)
      </span>
     </span>
     <span class="ltx_text" id="S6.SS1.1.3.2" style="font-size:90%;">
      Start position
      <br class="ltx_break"/>
     </span>
    </figcaption>
   </figure>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.3" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.3.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.2.g1" src="/html/2312.07214/assets/figures/Task1.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.3.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.3.p1.1.1">
       Task goal 1
       <br class="ltx_break"/>
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.5" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.5.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.4.g1" src="/html/2312.07214/assets/figures/Task2.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.5.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.5.p1.1.1">
       Task goal 2
       <br class="ltx_break"/>
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.7" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.7.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.6.g1" src="/html/2312.07214/assets/figures/Task3.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.7.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.7.p1.1.1">
       Task goal 3
       <br class="ltx_break"/>
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.9" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.9.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.8.g1" src="/html/2312.07214/assets/figures/Task4.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.9.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.9.p1.1.1">
       Task goal 4
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.11" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.11.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.10.g1" src="/html/2312.07214/assets/figures/Task5.2.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.11.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.11.p1.1.1">
       Task goal 5
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.13" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.13.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.12.g1" src="/html/2312.07214/assets/figures/Task6.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.13.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.13.p1.1.1">
       Task goal 6
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S6.SS1.15" style="width:104.1pt;">
    <div class="ltx_para" id="S6.SS1.15.p1">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="449" id="S6.SS1.14.g1" src="/html/2312.07214/assets/figures/Task7.jpg" width="598"/>
     <p class="ltx_p ltx_align_center" id="S6.SS1.15.p1.1">
      <span class="ltx_text ltx_caption" id="S6.SS1.15.p1.1.1">
       Task goal 7
      </span>
     </p>
    </div>
   </div>
   <div class="ltx_para" id="S6.SS1.p3">
    <p class="ltx_p" id="S6.SS1.p3.3">
     <span class="ltx_text ltx_caption" id="S6.SS1.p3.3.1">
      The preview images of the goal conditions that the participants needed to reach during the study. For all tasks, the starting position is the same. Objects not relevant to the current task are hidden.
     </span>
    </p>
    <ul class="ltx_itemize" id="S6.I1">
     <li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i1.p1">
       <p class="ltx_p" id="S6.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">
         Task 1:
        </span>
        Neptune needs to move to the candle.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i2.p1">
       <p class="ltx_p" id="S6.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">
         Task 2:
        </span>
        All agents need to move to the candle.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i3.p1">
       <p class="ltx_p" id="S6.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">
         Task 3:
        </span>
        Jupiter needs to move to the dumbbell and pick it up, Neptune and Pluto have to move to the fridge.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i4.p1">
       <p class="ltx_p" id="S6.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i4.p1.1.1">
         Task 4:
        </span>
        Pluto needs to fly over to the red key, pick it up, fly to the red door and open it. With the door open, Neptune needs to move to the yellow key, pick it up and bring it to the user.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i5.p1">
       <p class="ltx_p" id="S6.I1.i5.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i5.p1.1.1">
         Task 5:
        </span>
        Pluto needs to move to the candle. Jupiter needs to move onto the pressure plate, opening a glass door at the back of the room for a few seconds. While the door is open, Neptune needs to move to the chair behind the glass door.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S6.I1.i6" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i6.p1">
       <p class="ltx_p" id="S6.I1.i6.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i6.p1.1.1">
         Task 6:
        </span>
        Three dinner plates have to be put into the trash and all agents need to end up next to the garbage bin.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S6.I1.i7" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S6.I1.i7.p1">
       <p class="ltx_p" id="S6.I1.i7.p1.1">
        <span class="ltx_text ltx_font_bold" id="S6.I1.i7.p1.1.1">
         Task 7:
        </span>
        The robots have to flip a bed with a vase on it. The vase needs to be picked before the bed is flipped, so it does not break. Jupiter and Neptune must flip the bed together.
       </p>
      </div>
     </li>
    </ul>
    <p class="ltx_p" id="S6.SS1.p3.2">
     The tasks were designed to get more complex as the experiment progressed to elicit different interaction dynamics and to promote varied communication patterns. For example, for the first task, the participant would likely only speak to Neptune but for the second task, they might speak to all virtual robots at once. Task 5, for instance, required simultaneous commanding of both Jupiter and Neptune, as the timer on the door was purposefully low enough to where sequential commands did not seem feasible due to latency.
After the fifth task, the participants were asked to take a five-minute break. After the seventh task, the participants were asked to fill out a questionnaire and a subsequent semi-structured interview would be conducted.
The mean time of the experiment was 27 minutes and 28 seconds (SD
     <math alttext="=" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1">
      <semantics id="S6.SS1.p3.1.m1.1a">
       <mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">
        =
       </mo>
       <annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b">
        <eq id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">
        </eq>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">
        =
       </annotation>
      </semantics>
     </math>
     05:14 minutes) with the subsequent interview mean time 18 minutes and 51 seconds (SD
     <math alttext="=" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1">
      <semantics id="S6.SS1.p3.2.m2.1a">
       <mo id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">
        =
       </mo>
       <annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b">
        <eq id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">
        </eq>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">
        =
       </annotation>
      </semantics>
     </math>
     06:32 minutes). In total, five hours and 28 minutes of experiment data and three hours and 46 minutes of interview data were captured. Participants were given a 15€ voucher as compensation.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.2
    </span>
    Materials &amp; Data Analysis
   </h3>
   <div class="ltx_para" id="S6.SS2.p1">
    <p class="ltx_p" id="S6.SS2.p1.1">
     To analyze the qualitative data from the interviews, the audio recordings were transcribed using OpenAI’s Whisper transcription tool. Likewise, the transcriptions created as part of the study and the corresponding agent dialog were saved as text files and extracted after the studies were over. The transcribed records were then manually checked and corrected. Subsequently, the transcriptions were prepared for analysis. A thematic analysis approach was employed to identify and analyze patterns and themes within the data
     <cite class="ltx_cite ltx_citemacro_cite">
      Braun and Clarke (
      <a class="ltx_ref" href="#bib.bib6" title="">
       2006
      </a>
      )
     </cite>
     .
We employed an iterative process in which first one of the researchers thoroughly read and analyzed the data separately, generating initial codes that captured the general themes and experiences of the participants. Subsequently, two additional researchers independently analyzed the data under an analytical lens of variable autonomy. In this step, additional codes were generated and grouped by all three researchers to refine and investigate overarching themes that capture the general sentiment of the participants’ experiences. Lastly, the researcher from the first step double-checked the transcripts of all the participants according to the refined codes.
For the questionnaire, a German version of Kate Hone and Robert Graham’s
     <span class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
      <span class="ltx_text ltx_glossary_long">
       Subjective Assessment of Speech System Interfaces
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
      <span class="ltx_text ltx_glossary_short">
       SASSI
      </span>
     </abbr>
     ) questionnaire
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hone and Graham,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2000
      </a>
      )
     </cite>
     was selected. Designed specifically for speech input systems, the
     <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
      <span class="ltx_text ltx_glossary_short">
       SASSI
      </span>
     </abbr>
     aims to measure usability and gauges the user’s perception across six factors:
     <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">
      Accuracy
     </span>
     ,
     <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">
      Likeability
     </span>
     ,
     <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.3">
      Cognitive Demand
     </span>
     ,
     <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.4">
      Annoyance
     </span>
     ,
     <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.5">
      Habitability
     </span>
     , and
     <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.6">
      Speed
     </span>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hone and Graham,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2000
      </a>
      , p.2)
     </cite>
     .
This questionnaire was selected based on recommendations from Kocabalil et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kocabalil et al.,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2018
      </a>
      )
     </cite>
     . In this overview, Kocabalil et al. compare six questionnaires for conversational interfaces and describe in detail which
     <span class="ltx_glossaryref" title="user experience">
      <span class="ltx_text ltx_glossary_long">
       user experience
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="user experience">
      <span class="ltx_text ltx_glossary_short">
       UX
      </span>
     </abbr>
     ) dimensions they cover. As a standalone questionnaire for testing a system with conversational agents, the authors recommend the
     <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
      <span class="ltx_text ltx_glossary_short">
       SASSI
      </span>
     </abbr>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kocabalil et al.,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2018
      </a>
      , p. 8)
     </cite>
     .
We also deemed the high amount of questions related to the pragmatic qualities of the
     <abbr class="ltx_glossaryref" title="user experience">
      <span class="ltx_text ltx_glossary_short">
       UX
      </span>
     </abbr>
     compared to other questionnaires
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kocabalil et al.,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2018
      </a>
      , p. 7)
     </cite>
     a good fit for the study, as we were interested in how usable and controllable the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -powered architecture would end up being.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Results
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    Our study provides numerous insights into understanding how human operators interact with multiple
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    -based robots. In this section, we present our results mainly through the lens of variable autonomy. This means, we paid particular attention towards verbal interactions between participants and the robots that can help to understand how and to what extend a voice user interface based on a
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    supports or hinders variable autonomy.
The footage of the study, as well as the three hours and 45 minutes of interview data, were
thematically analyzed, as described in the previous section. In the following, we will present the themes we have found, supported by quotes from the interview or specific interactions between participants and robots that we observed. In total, we found four themes, see an overview below. In addition, we will report overall aspects of user experience:
   </p>
   <ul class="ltx_itemize" id="S7.I1">
    <li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S7.I1.i1.p1">
      <p class="ltx_p" id="S7.I1.i1.p1.1">
       Towards a shared world model between user and robot
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S7.I1.i2.p1">
      <p class="ltx_p" id="S7.I1.i2.p1.1">
       Instruction-based dialog
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S7.I1.i3.p1">
      <p class="ltx_p" id="S7.I1.i3.p1.1">
       Dialog engagement vs. Command optimization
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S7.I1.i4.p1">
      <p class="ltx_p" id="S7.I1.i4.p1.1">
       Holistic coordination vs. task decomposition
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S7.I1.i5" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S7.I1.i5.p1">
      <p class="ltx_p" id="S7.I1.i5.p1.1">
       Overall aspects of User Experience
      </p>
     </div>
    </li>
   </ul>
  </div>
  <section class="ltx_subsection" id="S7.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.1
    </span>
    Towards a shared world model between user and robot
   </h3>
   <div class="ltx_para" id="S7.SS1.p1">
    <p class="ltx_p" id="S7.SS1.p1.1">
     Throughout the study, an overarching process that could be observed was an exchange between the participants and the robots about how they see the world. These exchanges would often arise in conflict situations where the perceptions of the participants and robots diverged. Such conflicts were sometimes caused by user error, as shown in the following conversation P7 had with Neptune, where there are two keys present in the scene but P7 had only noticed one of them:
     <span class="ltx_tabular ltx_align_middle" id="S7.SS1.p1.1.1">
      <span class="ltx_tbody">
       <span class="ltx_tr" id="S7.SS1.p1.1.1.1.1">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.1.1.1.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.1.1.1.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.1.1.1.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1.1.1.1.1.1.1" style="font-size:90%;">
            P7:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.1.1.1.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.1.1.1.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.1.1.1.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1.1.1.2.1.1.1" style="font-size:90%;">
            Neptune, please get the key and then come to me.
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.1.2.2">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.1.2.2.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.1.2.2.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.1.2.2.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1.2.2.1.1.1.1" style="font-size:90%;">
            Neptune:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.1.2.2.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.1.2.2.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.1.2.2.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1.2.2.2.1.1.1" style="font-size:90%;">
            Could you please specify which key you would like me to get? There is a yellow key and a red key.
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.1.3.3">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.1.3.3.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.1.3.3.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.1.3.3.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1.3.3.1.1.1.1" style="font-size:90%;">
            P7:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.1.3.3.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.1.3.3.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.1.3.3.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.1.3.3.2.1.1.1" style="font-size:90%;">
            The yellow key, please.
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     Another common case was participants using words for certain objects that differed from the descriptions the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     was familiar with, such as P14 and P9 using the word
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS1.p1.1.2">
      “disc”
     </span>
     and then being informed by the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     that the object is a
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS1.p1.1.3">
      “plate”
     </span>
     .
Interestingly, there were also some conflicts in which the participants had to convince the robots that their perception of the world was incorrect, as shown in the following interaction between P12 and Neptune, in which Neptune assumes that the yellow key is still unreachable until P12 convinces them otherwise:
     <span class="ltx_tabular ltx_align_middle" id="S7.SS1.p1.1.4">
      <span class="ltx_tbody">
       <span class="ltx_tr" id="S7.SS1.p1.1.4.1.1">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.1.1.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.1.1.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.1.1.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.1.1.1.1.1.1" style="font-size:90%;">
            P12:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.1.1.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.1.1.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.1.1.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.1.1.2.1.1.1" style="font-size:90%;">
            Neptune, please get the yellow key.
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.4.2.2">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.2.2.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.2.2.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.2.2.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.2.2.1.1.1.1" style="font-size:90%;">
            Neptune:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.2.2.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.2.2.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.2.2.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.2.2.2.1.1.1" style="font-size:90%;">
            I’m sorry, but I currently can’t reach the yellow key. It’s behind a glass pane that I can’t cross. Is there another task I can do for you?
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.4.3.3">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.3.3.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.3.3.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.3.3.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.3.3.1.1.1.1" style="font-size:90%;">
            P12:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.3.3.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.3.3.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.3.3.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.3.3.2.1.1.1" style="font-size:90%;">
            ((Opens a door through the pane with the help of Pluto))
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.4.4.4">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.4.4.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.4.4.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.4.4.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.4.4.1.1.1.1" style="font-size:90%;">
            P12:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.4.4.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.4.4.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.4.4.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.4.4.2.1.1.1" style="font-size:90%;">
            Neptune, please get the yellow key.
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.4.5.5">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.5.5.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.5.5.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.5.5.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.5.5.1.1.1.1" style="font-size:90%;">
            Neptune:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.5.5.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.5.5.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.5.5.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.5.5.2.1.1.1" style="font-size:90%;">
            Sorry, but I have already tried to reach the yellow key, and I can’t. The key is behind a glass pane that I can’t cross. Is there another task you would like?
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.4.6.6">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.6.6.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.6.6.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.6.6.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.6.6.1.1.1.1" style="font-size:90%;">
            P12:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.6.6.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.6.6.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.6.6.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.6.6.2.1.1.1" style="font-size:90%;">
            I have had the glass wall opened. You can go through now.
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="S7.SS1.p1.1.4.7.7">
        <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.7.7.1">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.7.7.1.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.7.7.1.1.1" style="width:56.9pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.7.7.1.1.1.1" style="font-size:90%;">
            Neptune:
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS1.p1.1.4.7.7.2">
         <span class="ltx_inline-block ltx_align_top" id="S7.SS1.p1.1.4.7.7.2.1">
          <span class="ltx_p" id="S7.SS1.p1.1.4.7.7.2.1.1" style="width:435.3pt;">
           <span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4.7.7.2.1.1.1" style="font-size:90%;">
            ((Moves to the yellow key and picks it up)) I have successfully picked up the yellow key. What should I do next?
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     In the interviews, many participants expressed that the robots’ explanation of their world view was helpful and made their actions more understandable, as shown by quotes like
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS1.p1.1.5">
      “the feedback was helpful. They informed me about their understanding of commands and their perception of the environment, allowing me to align it with my own”
     </span>
     by P11. However, at times, the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     would hallucinate and convince the participants of things that were not true about the world and engage in repeating or oversharing of information that was not immediately relevant to the task, as P7 expresses here:
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS1.p1.1.6">
      “When the robots couldn’t do something, they always explained, ’I can’t do that’, and then they always added, ’I can only do this and that.’ And I mean, I know by now what you can do, it’s enough if you just say, ’I can’t do that right now,’ then I’ll think of something else.”
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.2
    </span>
    Instruction-based dialog
   </h3>
   <div class="ltx_para" id="S7.SS2.p1">
    <p class="ltx_p" id="S7.SS2.p1.1">
     While we did not instruct participants to engage with robots in a specific way, all of them considered the robots as recipients of instructions, which they in turn had to provide. Interestingly, however, some participants applied this in a very command-like style. For example, P14 said
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.1">
      “Pluto, Jupiter, Neptune all of you move towards the candle”
     </span>
     . Others instead opted for a more humane and conversation-like language, e.g., P7 asking Neptune
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.2">
      “can you move next to the candle?”
     </span>
     and further politely advising Jupiter to do the same
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.3">
      “Jupiter,
      <span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.3.1">
       please
      </span>
      move next to the candle”
     </span>
     .
Interestingly, this also had an impact on the way users engaged in the conversation. While the command-style participants aimed to formulate the command in the best way straight away, the conversation-like participants sometimes started talking and figuring out the exact instruction on the fly, resulting also in situations where they needed to correct them, e.g., P11, when asking Jupiter to pick up the plates
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.4">
      “Jupiter, please drive to the plates in the room and take, [thinking aloud] I think there are three, [resumes instruction language] and take three plates and throw them in the garbage bin.”
     </span>
     This second group of participants in general was more likely to ask the robots for advice, again P11 after a robot had picked up a key
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.5">
      “Can you do anything with the key? Can you place it on the key sign?”
     </span>
     . But overall, such questions remained the exception, as participants did not seem to regard the robots as equal conversational partners.
This was also a point made in the interviews, where participants mentioned that it felt like talking to young children and that they felt the need to be very specific and precise in what they want from them, as evidenced by statements like
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.6">
      “it’s similar to interacting with a young child, where you have to give very specific instructions and keep it concise”
     </span>
     by P4 and
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.7">
      “the experience felt somewhat like talking to young children, where you try to keep things simple in your communication”
     </span>
     by P5. We believe this may have been amplified by the fact that the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -based robots were happy to provide lengthy explanations and asked for clarifications whenever they were not sure what to do. This also happened in situations, where participants did not expect the robots to have any problems understanding what they were told to do, e.g., P9 being surprised at the robot’s response when being asked to pick up an object it was currently not next to:
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS2.p1.1.8">
      “When the system mentioned that an object was out of its reach and asked for permission to approach it first, I found it unintuitive. I expected it to act on the instruction directly.”
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.3
    </span>
    Dialog engagement vs. Command optimization
   </h3>
   <div class="ltx_para" id="S7.SS3.p1">
    <p class="ltx_p" id="S7.SS3.p1.1">
     We observed two main ways in which the participants reacted to the robots’ answers: Often, the robots would offer a dialogue by ending their sentences with questions like
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS3.p1.1.1">
      “[…] Should I do that?”
     </span>
     . While some users accepted the offer and replied, others simply scanned the robots’ answer for relevant information to come up with a new disconnected and optimized command.
For instance, when P6 and P9 asked Jupiter to place the vase next to the bed and Jupiter responded with
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS3.p1.1.2">
      “I can only place the vase on specific surfaces, like the bed or the ground. Can you specify where I should place it?”
     </span>
     , P6 responded with
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS3.p1.1.3">
      “on the ground”
     </span>
     while P9 responded with
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS3.p1.1.4">
      “Jupiter, please place the vase on the ground”
     </span>
     . While P6 engaged directly with the robot’s question, P9 issued a standalone command, independent of the prior dialogue, effectively resetting the interaction.
Based on information from the interviews, a big reason as to why participants did not respond to the robots’ questions was a lack of trust in their ability to understand them. P14 alludes to this, while also mentioning the aspect of delay as a possible cause:
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS3.p1.1.5">
      “I perceived that they were offering a dialogue by asking, but I didn’t believe they would understand me if I replied. I think it was also partly because, between their question and my answer, time had passed, making me think ’their memory was wiped’”
     </span>
     .
P6 also explicitly mentioned that their initial idea was to always say the names of the robots and the commands each time they spoke. This idea was reinforced, as the system’s inability to decide the right recipient (which is very unlikely if the names are mentioned), can be a costly mistake.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.4
    </span>
    Holistic coordination vs. task decomposition
   </h3>
   <div class="ltx_para" id="S7.SS4.p1">
    <p class="ltx_p" id="S7.SS4.p1.1">
     We observed significant differences in the complexity of the participants’ requests, especially when it comes to tasks that can be completed with multiple robots. Some participants seemed to prefer breaking down tasks into simple steps, and wait for the robots to complete one before moving on to the next step. On the other hand, some participants issued strategic multi-robot commands, giving each robot one or multiple steps to execute simultaneously. Generally, the participants who used more complex language seemed more inclined to try more elaborate instructions.
P11, for instance, issued complex commands like
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS4.p1.1.1">
      “first to you, Pluto. Put down the weight and fly to the refrigerator. To you, Neptune. Move in front of the refrigerator. To you, Jupiter. Pick up the weight after Pluto has put it down”
     </span>
     and remarked in the interview that their initial impression that each robot had to be addressed individually had been subverted, and the instructions could be surprisingly elaborate and target multiple robots at once. Conversely, other participants broke down the tasks into the smallest possible steps and issued commands to each robot separately, such as P6 and P13.
One reason for this seemed to be a lack of faith in the robots’ abilities to understand longer commands, or a preconceived notion that the robots would perform better when given simple orders.
This coincides with the interviews, in which P6 likened their default mode of communicating to how they would talk to a dog and P13 mentioned that they tried to be
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS4.p1.1.2">
      “clear and concise”
     </span>
     from the beginning. A similar interpretation was expressed by P4, who stated that
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS4.p1.1.3">
      “I always had to think beforehand about what to say, to say it briefly and precisely, and yes, very clearly. I had to put myself in the system’s shoes, guessing what it probably expected in the way I spoke.”
     </span>
     P4 was one of the participants who used very simple language from the beginning and intuitively broke down tasks into simple steps, likely expecting that the system would not be able to handle longer sequences directed at multiple robots.
These patterns were exacerbated by the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     ’s tendency to mirror the user’s way of talking, responding more
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS4.p1.1.4">
      “robotically”
     </span>
     when given
     <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS4.p1.1.5">
      “robotic”
     </span>
     instructions. Additionally, in response to failure situations, it is not unreasonable to assume that participants would tone down the complexity of their commands. However, our data suggests that the participants generally decided on the complexity of their requests from the start and did not adjust its overall nature during the study, even in response to robot failure.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     7.5
    </span>
    Overall aspects of User Experience
   </h3>
   <div class="ltx_para" id="S7.SS5.p1">
    <p class="ltx_p" id="S7.SS5.p1.1">
     In this section, we will present a collection of sub-themes describing the user’s experiences with the system and their positive and negative impressions of it according to the interviews. These sub-themes range from the degree to which the system was lenient with input, to system feedback and responsiveness. At the end, we will present an overview of the
     <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
      <span class="ltx_text ltx_glossary_short">
       SASSI
      </span>
     </abbr>
     factors to corroborate these findings.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S7.SS5.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      7.5.1
     </span>
     Semantic flexibility
    </h4>
    <div class="ltx_para" id="S7.SS5.SSS1.p1">
     <p class="ltx_p" id="S7.SS5.SSS1.p1.1">
      Participants appreciated that there was a high degree of flexibility to the words they were able to use. e.g., P14 said
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS1.p1.1.1">
       “I found the system’s understanding impressive. I used various words like ’move’, ’go there’, ’fly there’, ’proceed there’, and I believe they were all understood. I didn’t feel tied to a specific word, which made the interaction feel more natural.”
      </span>
      P3 echoed this statement, while also appreciating that the system did not have any issues with pauses during a request or exchanging pleasantries like
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS1.p1.1.2">
       “please”
      </span>
      and
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS1.p1.1.3">
       “thank you”
      </span>
      .
It was expected that the
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short">
        LLM
       </span>
      </abbr>
      , with its wealth of linguistic knowledge, would be fairly flexible when it comes understanding variations of words and commands. However, the results indicate that the
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short">
        LLM
       </span>
      </abbr>
      was rather meticulous when it came to words that are part of its function descriptions, sometimes refusing to do something if, for example, the name of an object was slightly off, as shown in the following exchange between P11 and Pluto:
      <span class="ltx_tabular ltx_align_middle" id="S7.SS5.SSS1.p1.1.4">
       <span class="ltx_tbody">
        <span class="ltx_tr" id="S7.SS5.SSS1.p1.1.4.1.1">
         <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS5.SSS1.p1.1.4.1.1.1">
          <span class="ltx_inline-block ltx_align_top" id="S7.SS5.SSS1.p1.1.4.1.1.1.1">
           <span class="ltx_p" id="S7.SS5.SSS1.p1.1.4.1.1.1.1.1" style="width:56.9pt;">
            <span class="ltx_text ltx_font_typewriter" id="S7.SS5.SSS1.p1.1.4.1.1.1.1.1.1" style="font-size:90%;">
             P11:
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS5.SSS1.p1.1.4.1.1.2">
          <span class="ltx_inline-block ltx_align_top" id="S7.SS5.SSS1.p1.1.4.1.1.2.1">
           <span class="ltx_p" id="S7.SS5.SSS1.p1.1.4.1.1.2.1.1" style="width:435.3pt;">
            <span class="ltx_text ltx_font_typewriter" id="S7.SS5.SSS1.p1.1.4.1.1.2.1.1.1" style="font-size:90%;">
             Pluto, please go to the blue trash can.
            </span>
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_tr" id="S7.SS5.SSS1.p1.1.4.2.2">
         <span class="ltx_td ltx_align_justify ltx_align_top" id="S7.SS5.SSS1.p1.1.4.2.2.1">
          <span class="ltx_inline-block ltx_align_top" id="S7.SS5.SSS1.p1.1.4.2.2.1.1">
           <span class="ltx_p" id="S7.SS5.SSS1.p1.1.4.2.2.1.1.1" style="width:56.9pt;">
            <span class="ltx_text ltx_font_typewriter" id="S7.SS5.SSS1.p1.1.4.2.2.1.1.1.1" style="font-size:90%;">
             Pluto:
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S7.SS5.SSS1.p1.1.4.2.2.2">
          <span class="ltx_inline-block ltx_align_top" id="S7.SS5.SSS1.p1.1.4.2.2.2.1">
           <span class="ltx_p" id="S7.SS5.SSS1.p1.1.4.2.2.2.1.1" style="width:435.3pt;">
            <span class="ltx_text ltx_font_typewriter" id="S7.SS5.SSS1.p1.1.4.2.2.2.1.1.1" style="font-size:90%;">
             Sorry, I can only go to locations that are defined in my functions. A blue trash can is unfortunately not on my list. However, I can move to the trash bin, if that helps. Should I do that?
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
      This is further substantiated by the interview data. P9 states that
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS1.p1.1.5">
       “they did execute the commands, but sometimes they were very meticulous”
      </span>
      . While the example above is an outlier and the robots could generally make that connection, it was apparent that they paid special attention to terms outlined by their functions. This phenomenon and the fact that participants were not provided with any guidelines as to which words to choose is also reflected in the
      <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
       <span class="ltx_text ltx_glossary_short">
        SASSI
       </span>
      </abbr>
      results. Question 29, which states
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS1.p1.1.6">
       “I sometimes wondered if I was using the right word”
      </span>
      , received an average score of 5.92 (M = 5.92, SD = 1.73) out of 7, where 7 indicates ’strongly agree’.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S7.SS5.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      7.5.2
     </span>
     Verbal robot feedback
    </h4>
    <div class="ltx_para" id="S7.SS5.SSS2.p1">
     <p class="ltx_p" id="S7.SS5.SSS2.p1.1">
      The verbal feedback by the robots was generally regarded as helpful. Participants mentioned that the clarifications provided by the robots made tasks easier and the robots’ decisions more understandable. For example, P9 stated in their interview that
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS2.p1.1.1">
       “the robots behaved very understandably. Like with the key handover, Neptune couldn’t directly take the key from Pluto but could pick it up from the ground. He explained that, so it was clear what I had to do.”
      </span>
      In a similar vein, P6 called the feedback helpful and appreciated that the robots corrected their nomenclature in cases where P6 could not visually discern what type of object was in front of them. However, P6 also remarked that the feedback was too long for their taste, which aligns with statements made by P13 and P14.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S7.SS5.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      7.5.3
     </span>
     Mismatch regarding expected robot autonomy
    </h4>
    <div class="ltx_para" id="S7.SS5.SSS3.p1">
     <p class="ltx_p" id="S7.SS5.SSS3.p1.1">
      Participants reported that they sometimes expected more autonomous behavior by the robots, especially in situations that required a sequence of tasks to be executed. As it was up to the discretion of the non-deterministic
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short">
        LLM
       </span>
      </abbr>
      to decide between calling a function or asking a user for clarification before proceeding, there was a certain amount of variance to the autonomy with which the robots executed given tasks. For example, P3 expressed:
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS3.p1.1.1">
       “In certain situations, I didn’t realize I had to explicitly instruct the system to approach an object before picking it up. With humans, if I say ’pick up the key’, I’d expect them to walk over to the key and then pick it up.”
      </span>
      This was referring to a situation in which P3 had asked Jupiter to pick up the key, to which Jupiter responded by asking for permission to approach the key first. P6 and P9 echoed this sentiment, expressing a similar surprise about the robots asking before executing a pre-requisite task. However, there were also occurrences where a robot would by itself decompose a task into necessary sub-tasks and perform them autonomously. For example, when P8 asked Jupiter to
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS3.p1.1.2">
       “take three plates and throw them in the trash bin”
      </span>
      , the robot executed the correct function-triplet (consisting of movement, pick up, and throw away) three times in succession.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S7.SS5.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      7.5.4
     </span>
     Inherent response-lag
    </h4>
    <div class="ltx_para" id="S7.SS5.SSS4.p1">
     <p class="ltx_p" id="S7.SS5.SSS4.p1.1">
      A common complaint lodged by the participants was that the system took too long to answer. In the application, there was a visual cue to indicate the status of the transcription and which robots were currently processing it to let the users know that they had been heard. However, there was an amount of delay before the robots’ responses that was unlike an inter-human conversation, where feedback is usually instant.
This led to cases in which participants started talking without waiting for the responses, as described by P9:
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS4.p1.1.1">
       “In terms of naturalness, the robots’ feedback always came late. There was a lot of narration, and by the time they responded, I had already continued speaking”
      </span>
      .
This is reflected by the
      <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S7.SS5.SSS4.p1.1.2">
       “Speed”
      </span>
      dimension of the
      <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
       <span class="ltx_text ltx_glossary_short">
        SASSI
       </span>
      </abbr>
      , where the system received an average score of 5.25 (M = 5.25, SD = 1.57) (see Table
      <a class="ltx_ref" href="#S7.T1" title="Table 1 ‣ 7.5.5 SASSI factors ‣ 7.5 Overall aspects of User Experience ‣ 7 Results ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      ), with 7 being the slowest.
The reason for this behavior are due to the technical setup and we will discuss them in more detail in the discussion section.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S7.SS5.SSS5">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      7.5.5
     </span>
     SASSI factors
    </h4>
    <div class="ltx_para" id="S7.SS5.SSS5.p1">
     <p class="ltx_p" id="S7.SS5.SSS5.p1.1">
      For an overview,
      <a class="ltx_ref ltx_refmacro_autoref" href="#S7.T1" title="Table 1 ‣ 7.5.5 SASSI factors ‣ 7.5 Overall aspects of User Experience ‣ 7 Results ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
       <span class="ltx_text ltx_ref_tag">
        Table 1
       </span>
      </a>
      shows the different dimensions of the
      <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
       <span class="ltx_text ltx_glossary_short">
        SASSI
       </span>
      </abbr>
      questionnaire, each on a scale from 1 to 7, averaged across the 12 participants with standard deviation.
     </p>
    </div>
    <figure class="ltx_table" id="S7.T1">
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       <span class="ltx_text" id="S7.T1.4.1.1" style="font-size:90%;">
        Table 1
       </span>
       :
      </span>
      <span class="ltx_text" id="S7.T1.5.2" style="font-size:90%;">
       Descriptive statistics for individual
       <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
        <span class="ltx_text ltx_glossary_short">
         SASSI
        </span>
       </abbr>
       dimensions.
      </span>
     </figcaption>
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T1.2">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S7.T1.2.3.1">
        <th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S7.T1.2.3.1.1">
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T1.2.3.1.2">
         <span class="ltx_text ltx_font_bold" id="S7.T1.2.3.1.2.1" style="font-size:90%;">
          System Response Accuracy
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T1.2.3.1.3">
         <span class="ltx_text ltx_font_bold" id="S7.T1.2.3.1.3.1" style="font-size:90%;">
          Likeability
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T1.2.3.1.4">
         <span class="ltx_text ltx_font_bold" id="S7.T1.2.3.1.4.1" style="font-size:90%;">
          Cognitive Demand
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T1.2.3.1.5">
         <span class="ltx_text ltx_font_bold" id="S7.T1.2.3.1.5.1" style="font-size:90%;">
          Annoyance
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T1.2.3.1.6">
         <span class="ltx_text ltx_font_bold" id="S7.T1.2.3.1.6.1" style="font-size:90%;">
          Habitability
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T1.2.3.1.7">
         <span class="ltx_text ltx_font_bold" id="S7.T1.2.3.1.7.1" style="font-size:90%;">
          Speed
         </span>
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S7.T1.1.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T1.1.1.1">
         <math alttext="M" class="ltx_Math" display="inline" id="S7.T1.1.1.1.m1.1">
          <semantics id="S7.T1.1.1.1.m1.1a">
           <mi id="S7.T1.1.1.1.m1.1.1" mathsize="90%" xref="S7.T1.1.1.1.m1.1.1.cmml">
            M
           </mi>
           <annotation-xml encoding="MathML-Content" id="S7.T1.1.1.1.m1.1b">
            <ci id="S7.T1.1.1.1.m1.1.1.cmml" xref="S7.T1.1.1.1.m1.1.1">
             𝑀
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S7.T1.1.1.1.m1.1c">
            M
           </annotation>
          </semantics>
         </math>
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S7.T1.1.1.2">
         <span class="ltx_text" id="S7.T1.1.1.2.1" style="font-size:90%;">
          4.43
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S7.T1.1.1.3">
         <span class="ltx_text" id="S7.T1.1.1.3.1" style="font-size:90%;">
          5.36
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S7.T1.1.1.4">
         <span class="ltx_text" id="S7.T1.1.1.4.1" style="font-size:90%;">
          4.88
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S7.T1.1.1.5">
         <span class="ltx_text" id="S7.T1.1.1.5.1" style="font-size:90%;">
          3.80
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S7.T1.1.1.6">
         <span class="ltx_text" id="S7.T1.1.1.6.1" style="font-size:90%;">
          4.44
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S7.T1.1.1.7">
         <span class="ltx_text" id="S7.T1.1.1.7.1" style="font-size:90%;">
          5.25
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S7.T1.2.2">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S7.T1.2.2.1">
         <math alttext="SD" class="ltx_Math" display="inline" id="S7.T1.2.2.1.m1.1">
          <semantics id="S7.T1.2.2.1.m1.1a">
           <mrow id="S7.T1.2.2.1.m1.1.1" xref="S7.T1.2.2.1.m1.1.1.cmml">
            <mi id="S7.T1.2.2.1.m1.1.1.2" mathsize="90%" xref="S7.T1.2.2.1.m1.1.1.2.cmml">
             S
            </mi>
            <mo id="S7.T1.2.2.1.m1.1.1.1" lspace="0em" rspace="0em" xref="S7.T1.2.2.1.m1.1.1.1.cmml">
             ​
            </mo>
            <mi id="S7.T1.2.2.1.m1.1.1.3" mathsize="90%" xref="S7.T1.2.2.1.m1.1.1.3.cmml">
             D
            </mi>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S7.T1.2.2.1.m1.1b">
            <apply id="S7.T1.2.2.1.m1.1.1.cmml" xref="S7.T1.2.2.1.m1.1.1">
             <times id="S7.T1.2.2.1.m1.1.1.1.cmml" xref="S7.T1.2.2.1.m1.1.1.1">
             </times>
             <ci id="S7.T1.2.2.1.m1.1.1.2.cmml" xref="S7.T1.2.2.1.m1.1.1.2">
              𝑆
             </ci>
             <ci id="S7.T1.2.2.1.m1.1.1.3.cmml" xref="S7.T1.2.2.1.m1.1.1.3">
              𝐷
             </ci>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S7.T1.2.2.1.m1.1c">
            SD
           </annotation>
          </semantics>
         </math>
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T1.2.2.2">
         <span class="ltx_text" id="S7.T1.2.2.2.1" style="font-size:90%;">
          0.95
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T1.2.2.3">
         <span class="ltx_text" id="S7.T1.2.2.3.1" style="font-size:90%;">
          1.01
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T1.2.2.4">
         <span class="ltx_text" id="S7.T1.2.2.4.1" style="font-size:90%;">
          1.02
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T1.2.2.5">
         <span class="ltx_text" id="S7.T1.2.2.5.1" style="font-size:90%;">
          1.44
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T1.2.2.6">
         <span class="ltx_text" id="S7.T1.2.2.6.1" style="font-size:90%;">
          1.22
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T1.2.2.7">
         <span class="ltx_text" id="S7.T1.2.2.7.1" style="font-size:90%;">
          1.57
         </span>
        </td>
       </tr>
      </tbody>
     </table>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S8" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    8
   </span>
   Discussion
  </h2>
  <div class="ltx_para" id="S8.p1">
   <p class="ltx_p" id="S8.p1.1">
    Based on our findings, it is apparent that users generally do not perceive robots as equal conversation partners. Conversations between the participants and the robots often boiled down to simple instructions and some users effectively ignored the robot’s part in the conversation, despite the much more elaborate conversational capabilities of
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    . Robot error and the overly meticulous nature of the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    in some cases could have been contributing factors to a user’s low expectations of the robot’s abilities. We believe that to allow for smooth cooperation, the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    must be aligned in such a way that its decisions are as understandable to the user as another human’s. While our data shows attempts on the side of the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    to stay as aligned as possible by frequently communicating and asking questions, the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    was almost
    <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S8.p1.1.1">
     “too communicative”
    </span>
    , leading to a mismatch regarding the expected robot autonomy. It is clear that a deeper, intuitive understanding is required on the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    ’s side so that the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    ’s and the human’s conceptual models can be matched closely.
   </p>
  </div>
  <section class="ltx_subsection" id="S8.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     8.1
    </span>
    Lessons Learned
   </h3>
   <div class="ltx_para" id="S8.SS1.p1">
    <p class="ltx_p" id="S8.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S8.SS1.p1.1.1">
      Function calls are a double-edged sword:
     </span>
     Function calls, with their reliable formatting and consistent nature, turned out to be very effective at connecting the language-based
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     -model to coded robot processes. Functions are highly modular and they are very adjustable, making them a good fit for an experimental framework that can hold multiple agents with vastly different abilities. This feature shows promise, even when applied to more elaborate scenarios, as function descriptions can be complex with multiple nested properties. The function feature also displayed the ability to confine the LLM more effectively than a textual ontology approach. However, the feature created other issues in the model’s behaviors. For instance, the cases in which the model was overly meticulous illustrated in
     <a class="ltx_ref ltx_refmacro_autoref" href="#S7.SS5.SSS1" title="7.5.1 Semantic flexibility ‣ 7.5 Overall aspects of User Experience ‣ 7 Results ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
      <span class="ltx_text ltx_ref_tag">
       Section 7.5.1
      </span>
     </a>
     were not conducive to intuitive cooperation. Likewise, the model’s refusal to execute a function and ask the user for permission needlessly, as shown in
     <a class="ltx_ref ltx_refmacro_autoref" href="#S7.SS5.SSS3" title="7.5.3 Mismatch regarding expected robot autonomy ‣ 7.5 Overall aspects of User Experience ‣ 7 Results ‣ Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming">
      <span class="ltx_text ltx_ref_tag">
       Section 7.5.3
      </span>
     </a>
     could have been a result of the function feature. Due to the opaque nature of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     it is difficult to say what exactly caused these particular problems.
Overall, we believe that functions have proven to be an effective way for prototyping and testing
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     ’s abilities to make decisions that can be translated into robot actions.
     <span class="ltx_text ltx_font_bold" id="S8.SS1.p1.1.2">
      GPT displays flaws as a solitary controller:
     </span>
     Our results suggest that
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     as a solitary controller for an actual robot is not yet feasible, especially when it comes to usability concerns.
Perhaps the most glaring issue of
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     as a solitary controller is its inherently non-deterministic nature. Even when the model is given a low temperature (a parameter which controls the randomness and creativity of the responses), the outputs are too inconsistent to act as a governing element for a real robot. Additional modules would have to be developed to support it, if consistency is to be ensured.
One of
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     ’s greatest advantages, its massive parameter count which endows it with a wealth of linguistic knowledge, also limits its deployment to a physical robot. Being a cloud-based service, the
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     <abbr class="ltx_glossaryref" title="Application Programming Interface">
      <span class="ltx_text ltx_glossary_short">
       API
      </span>
     </abbr>
     introduces latency which is exacerbated by problems with the internet connection.
The system’s slowness is reflected within the
     <abbr class="ltx_glossaryref" title="Subjective Assessment of Speech System Interfaces">
      <span class="ltx_text ltx_glossary_short">
       SASSI
      </span>
     </abbr>
     ’s factor 7, as well as the statements made by the participants during the interviews. Aside from negatively impacting the user experience, instant feedback is essential to language-based communication, especially during tasks where time-critical cooperation is required.
     <span class="ltx_text ltx_font_bold" id="S8.SS1.p1.1.3">
      Calibration could be very beneficial:
     </span>
     We believe that a calibration process, in which a user and the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     clarify what their preferred way of communicating is, could be helpful. Our data suggests that, intuitively, humans do not expect that they could influence a robot with this kind of meta-communication. A calibration process could not only help to improve the user’s initial expectations of the robot, but also prevent unwanted behaviors like overly long explanations.
     <span class="ltx_text ltx_font_bold" id="S8.SS1.p1.1.4">
      A competent sensory system for the robots is essential:
     </span>
     During communication, certain assumptions arise about what the conversation partner knows, sees and feels. What we have seen in our study is the importance of the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     ’s conceptual model to be aligned to the user’s. For one, this will necessitate the inclusion of non-verbal communication, such as pointing. Additionally, an improved sensory system would allow for more varied and flexible interpretation of user requests. The more data the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     has access to when deciding whether to execute an action, the more informed its decision will be. Supplying the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     with sensor and camera information from a physical robot therefore shows a lot of potential but will bring a new set of challenges.
     <span class="ltx_text ltx_font_bold" id="S8.SS1.p1.1.5">
      Additional avenues for control are necessary:
     </span>
     Intervention is very important when interacting with a robot that can act autonomously and perhaps even more so when interacting with multiple. Our study indicated an additional need for intervention as well. The issue with intervention in this context is that, due to the processing time of the transcription and
     <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     modules, the regular channels of communication between the user and the robot are unfit for intervening. Therefore, additional possibilities of intervention will have to be explored. An intermediary solution would be the introduction of a button that immediately stops the robot regardless of the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     ’s current state.
Another addition to improve the controllability of the system would be to make the processes of the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     less opaque and find ways to visualize its current state. This will be especially important as task complexity and robot complexity grow.
     <span class="ltx_text ltx_font_bold" id="S8.SS1.p1.1.6">
      Inter-robot communication is needed:
     </span>
     While the framework shows promise in allowing a user to interact with multiple robots simultaneously in an unstructured way, it is clear now that additional communication between the robots themselves is required. While the robots share a general sense of what their fellow robots are doing, textual representation and a unified log of conversations is insufficient, especially during tasks where the robots have to collaborate. Their responses should also be clustered in a way that the response to the user is streamlined and free of repetitions.
The nature of our framework would support such approaches, with the agent controller potentially taking a more elaborate part in controlling agent-to-agent communication and interaction. The agents themselves could potentially exchange function calls with each other, allowing for synchronized and planned cooperation towards certain goals.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S8.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     8.2
    </span>
    Limitations
   </h3>
   <div class="ltx_para" id="S8.SS2.p1">
    <p class="ltx_p" id="S8.SS2.p1.1">
     There are a few noteworthy implementations that pertain to the framework, as well as the study the framework was used in.
Utilizing an
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     comes with certain
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -inherent limitations. First off, due to their opaque nature, programs utilizing
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     are notoriously hard to debug. While there were some unforeseen consequences caused by unexpected
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     behavior, resulting bugs during the study only caused minor inconveniences and all participants were able to solve the tasks successfully. The use of German language could have been a possible cause for some of these unexpected behaviors, especially in regards to functions - although the objects described in the function descriptions were deliberately given German names to prevent the
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     from disambiguating.
Additionally, the framework inherits other
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     -related limitations, such as possible biases in training data and errors being propagated forward, as false output becomes part of future input.
Another limitation arose from the University-based internet connection used for the study. While the cloud services employed in this application inherently involve some latency, the sometimes unreliable internet connection further compounded the issue. While there were no disconnects or complete outages, the delays in transcriptions and model responses were at times noticeably longer, in contrast to quicker performance observed with a stable internet connection.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S9" lang="en">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    9
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S9.p1">
   <p class="ltx_p" id="S9.p1.1">
    This study makes a significant contribution to understanding the integration of
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    such as
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    in human-robot teaming and the potential of language as means to facilitate variable autonomy. Our findings indicate that despite the advanced capabilities of
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    , user perception and interaction with robots present challenges. The study highlights that users do not perceive robots as equal conversational partners, leading to a communication limited to simple instructions. Still, the study also showcased more elaborate conversation for those users who did not expect preconceived limitations in the conversational abilities of the robot counterparts.
A critical aspect is the alignment of the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    to make decisions understandable and predictable for the user. The over-communication by the
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    and the resulting ambiguity about robot autonomy demonstrate the need for a more balanced approach. Furthermore, the study underscores the importance of a calibration process to align communication preferences between users and
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    .
The study also reveals that using
    <abbr class="ltx_glossaryref" title="Generative pre-trained transformer">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    as a sole controller for robots has limitations, primarily due to its non-deterministic nature and latency issues from cloud connectivity. This underscores the necessity of considering
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    as part of a broader system that incorporates both user feedback and sensor information.
In conclusion, our research provides valuable insights into the dynamics of human-robot teaming, indicating a significant need for further research and development to effectively deploy
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    in such systems. Our findings suggest that a multidisciplinary approach, encompassing technology, user experience, and psychological aspects of interaction, is required to fully realize the potentials of
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    for variable autonomy in human-robot teaming.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib" lang="en">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Abbink et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Abbink, D. A., Carlson, T., Mulder, M., de Winter, J. C. F., Aminravan, F.,
Gibo, T. L., et al. (2018).
    </span>
    <span class="ltx_bibblock">
     A topology of shared control systems—finding common
ground in diversity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      IEEE Transactions on Human-Machine Systems
     </em>
     48, 509–525.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1109/thms.2018.2791570" target="_blank" title="">
      10.1109/thms.2018.2791570
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahn et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., et al.
(2022).
    </span>
    <span class="ltx_bibblock">
     Do as i can, not as i say: Grounding language in robotic affordances.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      arXiv preprint arXiv:2204.01691
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alessa and Al-Khalifa (2023)
    </span>
    <span class="ltx_bibblock">
     Alessa, A. and Al-Khalifa, H. (2023).
    </span>
    <span class="ltx_bibblock">
     Towards designing a chatgpt conversational companion for elderly
people.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2304.09866
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Arumugam et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Arumugam, D., Karamcheti, S., Gopalan, N., Williams, E. C., Rhee, M., Wong, L.
L. S., et al. (2019).
    </span>
    <span class="ltx_bibblock">
     Grounding natural language instructions to semantic goal
representations for abstraction and generalization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      Autonomous Robots
     </em>
     43, 449–468.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1007/s10514-018-9792-8" target="_blank" title="">
      10.1007/s10514-018-9792-8
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ausat et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ausat, A. M. A., Massang, B., Efendi, M., Nofirman, N., and Riady, Y. (2023).
    </span>
    <span class="ltx_bibblock">
     Can chat gpt replace the role of the teacher in the classroom: A
fundamental analysis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      Journal on Education
     </em>
     5, 16100–16106
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Braun and Clarke (2006)
    </span>
    <span class="ltx_bibblock">
     Braun, V. and Clarke, V. (2006).
    </span>
    <span class="ltx_bibblock">
     Using thematic analysis in psychology.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      Qualitative research in psychology
     </em>
     3, 77–101
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
et al. (2020).
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      Advances in neural information processing systems
     </em>
     33,
1877–1901
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bustamante et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Bustamante, S., Quere, G., Hagmann, K., Wu, X., Schmaus, P., Vogel, J., et al.
(2021).
    </span>
    <span class="ltx_bibblock">
     Toward seamless transitions between shared control and supervised
autonomy in robotic assistance.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      IEEE Robotics and Automation Letters
     </em>
     6, 3833–3840.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1109/LRA.2021.3064449" target="_blank" title="">
      10.1109/LRA.2021.3064449
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Canal et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Canal, G., Alenyà, G., and Torras, C. (2016).
    </span>
    <span class="ltx_bibblock">
     Personalization framework for adaptive robotic feeding assistance.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Social Robotics
     </em>
     (Cham: Springer International
Publishing), vol. 9979. 22–31.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1007/978-3-319-47437-3_3" target="_blank" title="">
      10.1007/978-3-319-47437-3_3
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., et al.
(2021).
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2107.03374
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chiou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chiou, M., Booth, S., Lacerda, B., Theodorou, A., and Rothfuß, S. (2023).
    </span>
    <span class="ltx_bibblock">
     Variable autonomy for human-robot teaming (vat).
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Companion of the 2023 ACM/IEEE International Conference on
Human-Robot Interaction
     </em>
     (New York, NY, USA: Association for Computing
Machinery), HRI ’23, 932–934.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1145/3568294.3579957" target="_blank" title="">
      10.1145/3568294.3579957
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Clark (1996)
    </span>
    <span class="ltx_bibblock">
     Clark, H. H. (1996).
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Using language
     </em>
     (Cambridge university press)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cyphert (2021)
    </span>
    <span class="ltx_bibblock">
     Cyphert, A. B. (2021).
    </span>
    <span class="ltx_bibblock">
     A human being wrote this law review article: Gpt-3 and the practice
of law.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      UC Davis L. Rev.
     </em>
     55, 401
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deci and Ryan (2012)
    </span>
    <span class="ltx_bibblock">
     Deci, E. L. and Ryan, R. M. (2012).
    </span>
    <span class="ltx_bibblock">
     Self-determination theory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Handbook of theories of social psychology
     </em>
     1, 416–436
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Devlin et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
    </span>
    <span class="ltx_bibblock">
     Bert: Pre-training of deep bidirectional transformers for language
understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:1810.04805
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Erdogan and Argall (2017)
    </span>
    <span class="ltx_bibblock">
     Erdogan, A. and Argall, B. (2017).
    </span>
    <span class="ltx_bibblock">
     The effect of robotic wheelchair control paradigm and interface on
user performance, effort and preference: An experimental assessment.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Robotics and Autonomous Systems
     </em>
     94, 282–297.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.robot.2017.04.013" target="_blank" title="">
      https://doi.org/10.1016/j.robot.2017.04.013
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Firat (2023)
    </span>
    <span class="ltx_bibblock">
     Firat, M. (2023).
    </span>
    <span class="ltx_bibblock">
     How chat gpt can transform autodidactic experiences and open
education.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      Department of Distance Education, Open Education Faculty,
Anadolu Unive
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Flemisch et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Flemisch, F., Abbink, D. A., Itoh, M., Pacaux-Lemoine, M.-P., and Wessel, G.
(2019).
    </span>
    <span class="ltx_bibblock">
     Joining the blunt and the pointy end of the spear: towards a common
framework of joint action, human–machine cooperation, cooperative guidance
and control, shared, traded and supervisory control.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      Cognition, Technology &amp; Work
     </em>
     21, 555–568
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fuentes et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Fuentes, C., Porcheron, M., and Fischer, J. E. (2023).
    </span>
    <span class="ltx_bibblock">
     Roboclean: Contextual language grounding for human-robot interactions
in specialised low-resource environments
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gallenberger et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Gallenberger, D., Bhattacharjee, T., Kim, Y., and Srinivasa, S. S.
(2019).
    </span>
    <span class="ltx_bibblock">
     Transfer depends on acquisition: Analyzing manipulation strategies
for robotic feeding.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      2019 14th ACM/IEEE International Conference on Human-Robot
Interaction (HRI)
     </em>
     . 267–276.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1109/HRI.2019.8673309" target="_blank" title="">
      10.1109/HRI.2019.8673309
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Garrod and Pickering (2004)
    </span>
    <span class="ltx_bibblock">
     Garrod, S. and Pickering, M. J. (2004).
    </span>
    <span class="ltx_bibblock">
     Why is conversation so easy?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      Trends in cognitive sciences
     </em>
     8, 8–11
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Grudin (2022)
    </span>
    <span class="ltx_bibblock">
     Grudin, J. (2022).
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      From tool to partner: The evolution of human-computer
interaction
     </em>
     (Springer Nature)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Händler (2023)
    </span>
    <span class="ltx_bibblock">
     Händler, T. (2023).
    </span>
    <span class="ltx_bibblock">
     Balancing autonomy and alignment: A multi-dimensional taxonomy for
autonomous llm-powered multi-agent architectures.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2310.03659
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hirosawa et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hirosawa, T., Harada, Y., Yokose, M., Sakamoto, T., Kawamura, R., and Shimizu,
T. (2023).
    </span>
    <span class="ltx_bibblock">
     Diagnostic accuracy of differential-diagnosis lists generated by
generative pretrained transformer 3 chatbot for clinical vignettes with
common chief complaints: A pilot study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      International journal of environmental research and public
health
     </em>
     20, 3378
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hone and Graham (2000)
    </span>
    <span class="ltx_bibblock">
     Hone, K. S. and Graham, R. (2000).
    </span>
    <span class="ltx_bibblock">
     Towards a tool for the subjective assessment of speech system
interfaces (sassi).
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      Natural Language Engineering
     </em>
     6, 287–303
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Irfan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Irfan, B., Kuoppamäki, S.-M., and Skantze, G. (2023).
    </span>
    <span class="ltx_bibblock">
     Between reality and delusion: Challenges of applying large language
models to companion robots for open-domain dialogues with older adults
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kahambing (2023)
    </span>
    <span class="ltx_bibblock">
     Kahambing, J. G. (2023).
    </span>
    <span class="ltx_bibblock">
     Chatgpt, public health communication and ‘intelligent patient
companionship’.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Journal of public health
     </em>
     , fdad028
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kelly et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kelly, D., Chen, Y., Cornwell, S. E., Delellis, N. S., Mayhew, A., Onaolapo,
S., et al. (2023).
    </span>
    <span class="ltx_bibblock">
     Bing chat: The future of search engines?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      Proceedings of the Association for Information Science and
Technology
     </em>
     60, 1007–1009
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kepuska and Bohouta (2018)
    </span>
    <span class="ltx_bibblock">
     Kepuska, V. and Bohouta, G. (2018).
    </span>
    <span class="ltx_bibblock">
     Next-generation of virtual personal assistants (microsoft cortana,
apple siri, amazon alexa and google home).
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      2018 IEEE 8th annual computing and communication workshop
and conference (CCWC)
     </em>
     (IEEE), 99–103
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. (2012)
    </span>
    <span class="ltx_bibblock">
     Kim, D.-J., Hazlett-Knudsen, R., Culver-Godfrey, H., Rucks, G., Cunningham, T.,
Portee, D., et al. (2012).
    </span>
    <span class="ltx_bibblock">
     How autonomy impacts performance and satisfaction: Results from a
study with spinal cord injured subjects using an assistive robot.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      IEEE Transactions on Systems, Man, and Cybernetics - Part A:
Systems and Humans
     </em>
     42, 2–14.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1109/TSMCA.2011.2159589" target="_blank" title="">
      10.1109/TSMCA.2011.2159589
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kocabalil et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Kocabalil, A. B., Laranjo, L., and Coiera, E. (2018).
    </span>
    <span class="ltx_bibblock">
     Measuring user experience in conversational interfaces: a comparison
of six questionnaires.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      Proceedings of the 32nd International BCS Human Computer
Interaction Conference 32
     </em>
     . 1–12
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kodur et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kodur, K., Zand, M., Tognotti, M., Jauregui, C., and Kyrarini, M. (2023).
    </span>
    <span class="ltx_bibblock">
     Structured and unstructured speech2action frameworks for human-robot
collaboration: A user study
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Koubaa (2023a)
    </span>
    <span class="ltx_bibblock">
     Koubaa, A. (2023a).
    </span>
    <span class="ltx_bibblock">
     Gpt-4 vs. gpt-3.5: A concise showdown
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Koubaa (2023b)
    </span>
    <span class="ltx_bibblock">
     Koubaa, A. (2023b).
    </span>
    <span class="ltx_bibblock">
     Rosgpt: Next-generation human-robot interaction with chatgpt and ros.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      Preprints
     </em>
     <a class="ltx_ref" href="https:/doi.org/10.20944/preprints202304.0827.v2" target="_blank" title="">
      10.20944/preprints202304.0827.v2
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Latikka et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Latikka, R., Savela, N., Koivula, A., and Oksanen, A. (2021).
    </span>
    <span class="ltx_bibblock">
     Attitudes toward robots as equipment and coworkers and the impact of
robot autonomy level.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      International Journal of Social Robotics
     </em>
     13, 1747–1759
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lauretti et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Lauretti, C., Cordella, F., Guglielmelli, E., and Zollo, L. (2017).
    </span>
    <span class="ltx_bibblock">
     Learning by demonstration for planning activities of daily living in
rehabilitation and assistive robotics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      IEEE Robotics and Automation Letters
     </em>
     2, 1375–1382
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lee et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lee, Y. K., Jung, Y., Kang, G., and Hahn, S. (2023).
    </span>
    <span class="ltx_bibblock">
     Developing social robots with empathetic non-verbal cues using large
language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      arXiv preprint arXiv:2308.16529
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lekova et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lekova, A., Tsvetkova, P., and Andreeva, A. (2023).
    </span>
    <span class="ltx_bibblock">
     System software architecture for enhancing human-robot interaction by
conversational ai.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      2023 International Conference on Information Technologies
(InfoTech)
     </em>
     (IEEE), 1–6
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu and Zhang (2019)
    </span>
    <span class="ltx_bibblock">
     Liu, R. and Zhang, X. (2019).
    </span>
    <span class="ltx_bibblock">
     A review of methodologies for natural-language-facilitated
human–robot cooperation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      International Journal of Advanced Robotic Systems
     </em>
     16,
1729881419851402
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Misra et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Misra, D. K., Sung, J., Lee, K., and Saxena, A. (2016).
    </span>
    <span class="ltx_bibblock">
     Tell me dave: Context-sensitive grounding of natural language to
manipulation instructions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      The International Journal of Robotics Research
     </em>
     35, 281–300.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1177/0278364915602060" target="_blank" title="">
      10.1177/0278364915602060
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., et al. (2021).
    </span>
    <span class="ltx_bibblock">
     Webgpt: Browser-assisted question-answering with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      arXiv preprint arXiv:2112.09332
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Neelakantan et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M., Tworek, J., et al.
(2022).
    </span>
    <span class="ltx_bibblock">
     Text and code embeddings by contrastive pre-training.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2201.10005
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nilsson et al. (1984)
    </span>
    <span class="ltx_bibblock">
     Nilsson, N. J. et al. (1984).
    </span>
    <span class="ltx_bibblock">
     Shakey the robot
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nori et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Nori, H., King, N., McKinney, S. M., Carignan, D., and Horvitz, E. (2023).
    </span>
    <span class="ltx_bibblock">
     Capabilities of gpt-4 on medical challenge problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      arXiv preprint arXiv:2303.13375
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ouyang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., et al.
(2022).
    </span>
    <span class="ltx_bibblock">
     Training language models to follow instructions with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      Advances in Neural Information Processing Systems
     </em>
     35,
27730–27744
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Park, D., Hoshi, Y., Mahajan, H. P., Kim, H. K., Erickson, Z., Rogers, W. A.,
et al. (2020).
    </span>
    <span class="ltx_bibblock">
     Active robot-assisted feeding with a general-purpose mobile
manipulator: Design, evaluation, and lessons learned.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      Robotics and Autonomous Systems
     </em>
     124, 103344.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1016/j.robot.2019.103344" target="_blank" title="">
      10.1016/j.robot.2019.103344
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pascher et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Pascher, M., Goldau, F. F., Kronhardt, K., Frese, U., and Gerken, J. (2024).
    </span>
    <span class="ltx_bibblock">
     AdaptiX – A Transitional XR Framework for Development and
Evaluation of Shared Control Applications in Assistive Robotics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      Proc. ACM Hum.-Comput. Interact.
     </em>
     8.
    </span>
    <span class="ltx_bibblock">
     Preprint on arXiv:
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.15887" target="_blank" title="">
      https://arxiv.org/abs/2310.15887
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pascher et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Pascher, M., Gruenefeld, U., Schneegass, S., and Gerken, J.
(2023a).
    </span>
    <span class="ltx_bibblock">
     How to communicate robot motion intent: A scoping review.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems - CHI ’23
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1145/3544548.3580857" target="_blank" title="">
      10.1145/3544548.3580857
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pascher et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Pascher, M., Kronhardt, K., Goldau, F. F., Frese, U., and Gerken, J.
(2023b).
    </span>
    <span class="ltx_bibblock">
     In Time and Space: Towards Usable Adaptive Control for Assistive
Robotic Arms.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      2023 32nd IEEE International Conference on Robot and Human
Interactive Communication (RO-MAN)
     </em>
     (IEEE), 2300–2307.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1109/RO-MAN571019.2023.10309381" target="_blank" title="">
      10.1109/RO-MAN571019.2023.10309381
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Perlman (2022)
    </span>
    <span class="ltx_bibblock">
     Perlman, A. (2022).
    </span>
    <span class="ltx_bibblock">
     The implications of chatgpt for legal services and society.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      Available at SSRN 4294197
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Petroni et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller,
A. H., et al. (2019).
    </span>
    <span class="ltx_bibblock">
     Language models as knowledge bases?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      arXiv preprint arXiv:1909.01066
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pollak et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Pollak, A., Paliga, M., Pulopulos, M. M., Kozusznik, B., and Kozusznik, M. W.
(2020).
    </span>
    <span class="ltx_bibblock">
     Stress in manual and autonomous modes of collaboration with a cobot.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">
      Computers in Human Behavior
     </em>
     112, 106469.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.chb.2020.106469" target="_blank" title="">
      https://doi.org/10.1016/j.chb.2020.106469
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Porcheron et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Porcheron, M., Fischer, J. E., and Sharples, S. (2017).
    </span>
    <span class="ltx_bibblock">
     ” do animals have accents?” talking with agents in multi-party
conversation.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      Proceedings of the 2017 ACM conference on computer supported
cooperative work and social computing
     </em>
     . 207–219
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018).
    </span>
    <span class="ltx_bibblock">
     Improving language understanding by generative pre-training
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rakhimkul et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Rakhimkul, S., Kim, A., Pazylbekov, A., and Shintemirov, A. (2019).
    </span>
    <span class="ltx_bibblock">
     Autonomous object detection and grasping using deep learning for
design of an intelligent assistive robot manipulation system.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      2019 IEEE International Conference on Systems, Man and
Cybernetics (SMC)
     </em>
     (IEEE), 3962–3968
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Rao, A., Kim, J., Kamineni, M., Pang, M., Lie, W., and Succi, M. D. (2023).
    </span>
    <span class="ltx_bibblock">
     Evaluating chatgpt as an adjunct for radiologic decision-making.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      medRxiv
     </em>
     , 2023–02
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sebanz et al. (2006)
    </span>
    <span class="ltx_bibblock">
     Sebanz, N., Bekkering, H., and Knoblich, G. (2006).
    </span>
    <span class="ltx_bibblock">
     Joint action: bodies and minds moving together.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      Trends in cognitive sciences
     </em>
     10, 70–76
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shea et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shea, Y.-F., Lee, C. M. Y., Ip, W. C. T., Luk, D. W. A., and Wong, S. S. W.
(2023).
    </span>
    <span class="ltx_bibblock">
     Use of gpt-4 to analyze medical records of patients with extensive
investigations and delayed diagnosis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">
      JAMA Network Open
     </em>
     6, e2325000–e2325000
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stiennon et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., et al.
(2020).
    </span>
    <span class="ltx_bibblock">
     Learning to summarize with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">
      Advances in Neural Information Processing Systems
     </em>
     33,
3008–3021
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Swanson et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Swanson, B., Mathewson, K., Pietrzak, B., Chen, S., and Dinalescu, M. (2021).
    </span>
    <span class="ltx_bibblock">
     Story centaur: Large language model few shot learning as a creative
writing tool.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">
      Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: System Demonstrations
     </em>
     ,
eds. D. Gkatzia and D. Seddah (Stroudsburg, PA, USA: Association for
Computational Linguistics), 244–256.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.eacl-demos.29" target="_blank" title="">
      10.18653/v1/2021.eacl-demos.29
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tack and Piech (2022)
    </span>
    <span class="ltx_bibblock">
     Tack, A. and Piech, C. (2022).
    </span>
    <span class="ltx_bibblock">
     The ai teacher test: Measuring the pedagogical ability of blender and
gpt-3 in educational dialogues.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">
      arXiv preprint arXiv:2205.07540
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Trott et al. (2015)
    </span>
    <span class="ltx_bibblock">
     Trott, S., Appriou, A., Feldman, J., and Janin, A. (2015).
    </span>
    <span class="ltx_bibblock">
     Natural language understanding and communication for multi-agent
systems.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">
      2015 AAAI Fall Symposium Series
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Trozze et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Trozze, A., Davies, T., and Kleinberg, B. (2023).
    </span>
    <span class="ltx_bibblock">
     Large language models in cryptocurrency securities cases: Can chatgpt
replace lawyers?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">
      arXiv preprint arXiv:2308.06032
     </em>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaswani et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
et al. (2017).
    </span>
    <span class="ltx_bibblock">
     Attention is all you need.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">
      Advances in neural information processing systems
     </em>
     30
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib65">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vesper et al. (2010)
    </span>
    <span class="ltx_bibblock">
     Vesper, C., Butterfill, S., Knoblich, G., and Sebanz, N. (2010).
    </span>
    <span class="ltx_bibblock">
     A minimal architecture for joint action.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">
      Neural Networks
     </em>
     23, 998–1003
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib66">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Waisberg et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Waisberg, E., Ong, J., Masalkhi, M., Kamran, S. A., Zaman, N., Sarker, P.,
et al. (2023).
    </span>
    <span class="ltx_bibblock">
     Gpt-4: a new era of artificial intelligence in medicine.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">
      Irish Journal of Medical Science (1971-)
     </em>
     , 1–4
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib67">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Winograd (1972)
    </span>
    <span class="ltx_bibblock">
     Winograd, T. (1972).
    </span>
    <span class="ltx_bibblock">
     Understanding natural language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">
      Cognitive Psychology
     </em>
     3, 1–191.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1016/0010-0285(72)90002-3" target="_blank" title="">
      10.1016/0010-0285(72)90002-3
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib68">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Woods (1973)
    </span>
    <span class="ltx_bibblock">
     Woods, W. A. (1973).
    </span>
    <span class="ltx_bibblock">
     Progress in natural language understanding.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">
      Proceedings of the June 4-8, 1973, national computer
conference and exposition on - AFIPS ’73
     </em>
     (New York, New York, USA: ACM
Press).
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref" href="https:/doi.org/10.1145/1499586.1499695" target="_blank" title="">
      10.1145/1499586.1499695
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib69">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V.
(2019).
    </span>
    <span class="ltx_bibblock">
     Xlnet: Generalized autoregressive pretraining for language
understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">
      Advances in neural information processing systems
     </em>
     32
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib70">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Złotowski et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Złotowski, J., Yogeeswaran, K., and Bartneck, C. (2017).
    </span>
    <span class="ltx_bibblock">
     Can we control it? Autonomous robots threaten human identity,
uniqueness, safety, and resources.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">
      International Journal of Human-Computer Studies
     </em>
     100, 48–54
    </span>
   </li>
  </ul>
 </section>
</article>
