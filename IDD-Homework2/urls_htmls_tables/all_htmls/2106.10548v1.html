<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.10548] VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis</title><meta property="og:description" content="Visual Question Answering system integrated with Unmanned Aerial Vehicle (UAV) has a lot of potentials to advance the post-disaster damage assessment purpose. Providing assistance to affected areas is highly dependent …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.10548">

<!--Generated on Tue Mar 19 12:20:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering system integrated with Unmanned Aerial Vehicle (UAV) has a lot of potentials to advance the post-disaster damage assessment purpose. Providing assistance to affected areas is highly dependent on real-time data assessment and analysis. Scope of the Visual Question Answering is to understand the scene and provide query related answer which certainly faster the recovery process after any disaster. In this work, we address the importance of <span id="id1.id1.1" class="ltx_text ltx_font_italic">visual question answering (VQA)</span> task for post-disaster damage assessment by presenting our recently developed VQA dataset called <span id="id1.id1.2" class="ltx_text ltx_font_italic">HurMic-VQA</span> collected during hurricane Michael, and comparing the performances of baseline VQA models.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Visual Question Answering, Post-Disaster Damage Assessment, Hurricane Michael</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2106.10548/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="196" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_italic">VQA-Aid</span>: At first, a UAV with built-in VQA system captures the images from the affected region and an observer asks a question relevant to the scenario. Finally, after the analysis, the device produces responses.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2106.10548/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="315" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.10.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>For all the images, <span id="S1.F2.11.2" class="ltx_text ltx_font_italic">Q1</span> represents the <span id="S1.F2.12.3" class="ltx_text ltx_font_italic">simple counting</span> question.<span id="S1.F2.13.4" class="ltx_text ltx_font_italic"> Q2</span> and <span id="S1.F2.14.5" class="ltx_text ltx_font_italic">Q3</span> are the reflection of <span id="S1.F2.15.6" class="ltx_text ltx_font_italic">complex counting</span> and <span id="S1.F2.16.7" class="ltx_text ltx_font_italic">yes/no</span> type of questions. We aim to count the object of a particular attribute in <span id="S1.F2.17.8" class="ltx_text ltx_font_italic">complex counting</span> questions (e.g. number of <span id="S1.F2.18.9" class="ltx_text ltx_font_italic">majorly/minorly</span> damaged buildings instead of total number of building). </figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is a complicated multimodal research problem in which the aim is to address an image-specified question. Thus, to find the right answer, VQA systems need to model the question and image (visual content). Visual Question Answering is regarded as a cognitive activity that separates it from other perceptual activities, such as the classification of images. For providing answers based on questions in natural language, a VQA model needs to identify the relevant objects from the images, recognize the attributes and find out the interactive relationships among several objects. This high-level scene understanding has the potential to advance the decision support systems for post-disaster damage assessment. Answers from the questions such as “What is the condition of the road?”, “How many buildings are damaged?” provides vital information that assists and faster the recovery process which could save many lives. Additionally, the management and the distribution of limited resources can be allocated optimally with the information from the VQA system. However, the success of any VQA model depends on the task-specific data. As the collection of the data is laborious as well as risky due to difficulties to enter the affected areas because of many adverse conditions such as damaged roads, flooded areas, etc., an automated system such as UAV integrated with the VQA module, trained on disaster specific dataset, can be implemented for damage assessment purpose. Understanding the scarcity of VQA datasets for post-disaster damage assessment, we develop a VQA dataset namely <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">HurMic-VQA</span> collected after the <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">Hurricane Michael</span>.
Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> represents the <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">VQA-Aid</span> framework in which we showed how the VQA task can be introduced as an assistant tool for disaster assessment that enables us to make the right decision at any time.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although several datasets are provided for post-disaster damage assessment purposes. Most of those datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> contain satellite images and images collected from social media. However, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> authors provide high resolution UAV images. Satellite images are usually captured from high altitudes therefore they have low resolution. Our <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">HurMic-VQA</span> dataset contains high resolution UAV images. In most cases, tasks related to the available datasets for natural disaster are limited to classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Visual Question Answering practice has not been considered much for the post-disaster damage assessment under climate change issue. To the best of our knowledge, this is the first work addressing VQA tasks based on UAV imagery in climate issues. Substantial research efforts have been made on the development of VQA algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> in the computer vision and natural language processing communities on many datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In these methods, different approaches for the fined-grained fusion between semantic image and question features have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. However, the implementation of VQA algorithms for UAV imagery is complex compare to the other datasets. The representation of UAV images is vertical which is opposite from the everyday images. Differentiating among several objects from a high altitude makes it difficult even for a human. The scenario of the affected areas after a disaster makes it more complicated as there exist many noises such as debris from structural damage compare to the pre-disaster condition. No benchmark results of the well-established VQA algorithms have been provided regarding post-disaster damage assessment based on UAV imagery. To address this issue, we compare the baseline VQA models, in this work, on our dataset.
Our work is unique for two reasons. Firstly, we introduce the VQA dataset for post-disaster damage assessment based on UAV imageries, and finally, we conduct a comprehensive study of the performances of baseline VQA algorithms on our dataset.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset Description</h2>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Accuracy Results from Baseline VQA Models</figcaption>
<div id="S2.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:502.6pt;height:255.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-107.7pt,54.8pt) scale(0.7,0.7) ;">
<table id="S2.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.1.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Mode of Feature Combination</th>
<th id="S2.T1.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Loss Function</th>
<th id="S2.T1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data Type</th>
<th id="S2.T1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Overall Accuracy</th>
<th id="S2.T1.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T1.3.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Accuracy for</td>
</tr>
<tr id="S2.T1.3.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">“Simple Counting”</td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T1.3.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.1.6.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Accuracy for</td>
</tr>
<tr id="S2.T1.3.1.1.1.6.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">“Complex Counting”</td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S2.T1.3.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.1.7.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Accuracy for</td>
</tr>
<tr id="S2.T1.3.1.1.1.7.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">“Yes/No”</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.1.2.1" class="ltx_tr">
<td id="S2.T1.3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S2.T1.3.1.2.1.1.1" class="ltx_text">Concatenation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span></td>
<td id="S2.T1.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.3.1.2.1.2.1" class="ltx_text">Cross Entropy</span></td>
<td id="S2.T1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training</td>
<td id="S2.T1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
<td id="S2.T1.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.65</td>
</tr>
<tr id="S2.T1.3.1.3.2" class="ltx_tr">
<td id="S2.T1.3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation</td>
<td id="S2.T1.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57</td>
<td id="S2.T1.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58</td>
<td id="S2.T1.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S2.T1.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.60</td>
</tr>
<tr id="S2.T1.3.1.4.3" class="ltx_tr">
<td id="S2.T1.3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testing</td>
<td id="S2.T1.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.55</td>
<td id="S2.T1.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.53</td>
<td id="S2.T1.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
</tr>
<tr id="S2.T1.3.1.5.4" class="ltx_tr">
<td id="S2.T1.3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.3.1.5.4.1.1" class="ltx_text">KL Divergence</span></td>
<td id="S2.T1.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training</td>
<td id="S2.T1.3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.67</td>
</tr>
<tr id="S2.T1.3.1.6.5" class="ltx_tr">
<td id="S2.T1.3.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation</td>
<td id="S2.T1.3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58</td>
<td id="S2.T1.3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S2.T1.3.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.61</td>
</tr>
<tr id="S2.T1.3.1.7.6" class="ltx_tr">
<td id="S2.T1.3.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testing</td>
<td id="S2.T1.3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.55</td>
<td id="S2.T1.3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.53</td>
<td id="S2.T1.3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
</tr>
<tr id="S2.T1.3.1.8.7" class="ltx_tr">
<td id="S2.T1.3.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S2.T1.3.1.8.7.1.1" class="ltx_text">Point-wise Multiplication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span></td>
<td id="S2.T1.3.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.3.1.8.7.2.1" class="ltx_text">Cross Entropy</span></td>
<td id="S2.T1.3.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training</td>
<td id="S2.T1.3.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
<td id="S2.T1.3.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.55</td>
<td id="S2.T1.3.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.64</td>
</tr>
<tr id="S2.T1.3.1.9.8" class="ltx_tr">
<td id="S2.T1.3.1.9.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation</td>
<td id="S2.T1.3.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58</td>
<td id="S2.T1.3.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.61</td>
<td id="S2.T1.3.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S2.T1.3.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.61</td>
</tr>
<tr id="S2.T1.3.1.10.9" class="ltx_tr">
<td id="S2.T1.3.1.10.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testing</td>
<td id="S2.T1.3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57</td>
<td id="S2.T1.3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.53</td>
<td id="S2.T1.3.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.65</td>
</tr>
<tr id="S2.T1.3.1.11.10" class="ltx_tr">
<td id="S2.T1.3.1.11.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.3.1.11.10.1.1" class="ltx_text">KL Divergence</span></td>
<td id="S2.T1.3.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training</td>
<td id="S2.T1.3.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.67</td>
</tr>
<tr id="S2.T1.3.1.12.11" class="ltx_tr">
<td id="S2.T1.3.1.12.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation</td>
<td id="S2.T1.3.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
<td id="S2.T1.3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S2.T1.3.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66</td>
</tr>
<tr id="S2.T1.3.1.13.12" class="ltx_tr">
<td id="S2.T1.3.1.13.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testing</td>
<td id="S2.T1.3.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58</td>
<td id="S2.T1.3.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.53</td>
<td id="S2.T1.3.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.68</td>
</tr>
<tr id="S2.T1.3.1.14.13" class="ltx_tr">
<td id="S2.T1.3.1.14.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S2.T1.3.1.14.13.1.1" class="ltx_text">MFB Module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span></td>
<td id="S2.T1.3.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.3.1.14.13.2.1" class="ltx_text">Cross Entropy</span></td>
<td id="S2.T1.3.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training</td>
<td id="S2.T1.3.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
<td id="S2.T1.3.1.14.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.64</td>
</tr>
<tr id="S2.T1.3.1.15.14" class="ltx_tr">
<td id="S2.T1.3.1.15.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation</td>
<td id="S2.T1.3.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58</td>
<td id="S2.T1.3.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S2.T1.3.1.15.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.63</td>
</tr>
<tr id="S2.T1.3.1.16.15" class="ltx_tr">
<td id="S2.T1.3.1.16.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testing</td>
<td id="S2.T1.3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.53</td>
<td id="S2.T1.3.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.63</td>
</tr>
<tr id="S2.T1.3.1.17.16" class="ltx_tr">
<td id="S2.T1.3.1.17.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.3.1.17.16.1.1" class="ltx_text">KL Divergence</span></td>
<td id="S2.T1.3.1.17.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training</td>
<td id="S2.T1.3.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59</td>
<td id="S2.T1.3.1.17.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.17.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.17.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.63</td>
</tr>
<tr id="S2.T1.3.1.18.17" class="ltx_tr">
<td id="S2.T1.3.1.18.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Validation</td>
<td id="S2.T1.3.1.18.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57</td>
<td id="S2.T1.3.1.18.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S2.T1.3.1.18.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S2.T1.3.1.18.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.60</td>
</tr>
<tr id="S2.T1.3.1.19.18" class="ltx_tr">
<td id="S2.T1.3.1.19.18.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Testing</td>
<td id="S2.T1.3.1.19.18.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.57</td>
<td id="S2.T1.3.1.19.18.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.56</td>
<td id="S2.T1.3.1.19.18.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.53</td>
<td id="S2.T1.3.1.19.18.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.64</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Collection Process</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The dataset is collected with a small UAV platform, DJI Mavic Pro quadcopters, after <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Hurricane Michael</span>. The dataset consists of video and imagery taken from several flights at Ford Bend County in Texas and other directly impacted areas. All the images are high in resolution, i.e., <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="4000\times 3000" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">4000</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">4000</cn><cn type="integer" id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">4000\times 3000</annotation></semantics></math>. The damage and debris situation after the hurricane is presented in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Though several objects are present, most of the images include debris and buildings. The buildings include both residential and non-residential structures. Table <a href="#S2.T2" title="Table 2 ‣ 2.1 Data Collection Process ‣ 2 Dataset Description ‣ VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the object types with different attributes. While generating the questions. these attributes are considered. In this work, we are interested in investigating the structural damage condition for buildings by asking questions for given images.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Object with associated Attributes</figcaption>
<div id="S2.T2.3" class="ltx_inline-block ltx_transformed_outer" style="width:274.6pt;height:67.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.8pt,11.3pt) scale(0.75,0.75) ;">
<table id="S2.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.3.1.1.1" class="ltx_tr">
<th id="S2.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Object</span></th>
<th id="S2.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T2.3.1.1.1.2.1" class="ltx_text ltx_font_bold">Associated Attribute</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.3.1.2.1" class="ltx_tr">
<td id="S2.T2.3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Building</td>
<td id="S2.T2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Total Destructive, Majorly Damaged, Minorly Damaged, No damage</td>
</tr>
<tr id="S2.T2.3.1.3.2" class="ltx_tr">
<td id="S2.T2.3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Road</td>
<td id="S2.T2.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Covered with Debris, Flooded, Undamaged</td>
</tr>
<tr id="S2.T2.3.1.4.3" class="ltx_tr">
<td id="S2.T2.3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Water</td>
<td id="S2.T2.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Covered with Debris, Flood Water, Clean Water</td>
</tr>
<tr id="S2.T2.3.1.5.4" class="ltx_tr">
<td id="S2.T2.3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Pools</td>
<td id="S2.T2.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Damaged, Undamaged</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Question Type</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Questions are grouped into a three-way category of questions, namely <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">“Simple Counting”</span>, <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">“Complex Counting”</span>, and <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">“Yes / No”</span>. We mainly ask the number of presence of an object in <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">“Simple Counting”</span> problem regardless of the associated attribute (e.g. <span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">How many buildings are in the images?</span>). <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">Yes / No</span> type questions concentrate on examining whether an object’s particular attribute is present. Finally, <span id="S2.SS2.p1.1.7" class="ltx_text ltx_font_italic">complex counting</span> type of query is explicitly intended to count the existence of a specific attribute of an object (e.g. <span id="S2.SS2.p1.1.8" class="ltx_text ltx_font_italic">How many <span id="S2.SS2.p1.1.8.1" class="ltx_text ltx_font_bold">majorly damaged</span> buildings are in the images?</span>). A total of 3197 images are available and each image is connected to all of the 3 types of questions. Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> represents these three types of questions.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">The baseline models: <span id="S3.p1.4.1" class="ltx_text ltx_font_italic">simple baseline</span> and <span id="S3.p1.4.2" class="ltx_text ltx_font_italic">Multimodal Factorized Bilinear (MFB) baseline</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have been considered for this task and all of these models are configured according to our <span id="S3.p1.4.3" class="ltx_text ltx_font_italic">HurMic-VQA</span> dataset. The main pipeline for the aforementioned baseline VQA models consists of image feature extraction, semantic representation of question, and fine-grained combination of these two features. For image and question feature extraction, respectively, VGGNet (VGG 16) and Two-Layer LSTM are taken into account. Image feature vector <span id="S3.p1.4.4" class="ltx_text ltx_font_italic">I</span>, <span id="S3.p1.4.5" class="ltx_text ltx_font_italic">I</span> <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><in id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\in</annotation></semantics></math> <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathbb{R}^{m}" display="inline"><semantics id="S3.p1.2.m2.1a"><msup id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">ℝ</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">m</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">superscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ℝ</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathbb{R}^{m}</annotation></semantics></math> where m represents the dimension of image vector and semantic question feature <span id="S3.p1.4.6" class="ltx_text ltx_font_italic">Q</span>, <span id="S3.p1.4.7" class="ltx_text ltx_font_italic">Q</span> <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S3.p1.3.m3.1a"><mo id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><in id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\in</annotation></semantics></math> <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="\mathbb{R}^{n}" display="inline"><semantics id="S3.p1.4.m4.1a"><msup id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">ℝ</mi><mi id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">superscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ℝ</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathbb{R}^{n}</annotation></semantics></math> where n represents dimension of question vector, are combined in a <span id="S3.p1.4.8" class="ltx_text ltx_font_italic">simple baseline</span> method by both concatenation and point-wise multiplication. 1024-D image feature vector (from last pooling layer) and 1024-D question vector (from the last word of Two-Layer LSTM) are considered for our study.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For the <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">MFB baseline</span> approach, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> proposed the MFB module for a fine-grained combination between image and question feature. The MFB module consists of two phases: expanding and squeezing. Image and question feature vector are multiplied point-wise in the expanding process, followed by a dropout layer. In the squeezing step, sum pooling is considered, followed by power and <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">L-2</span> normalization layers.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Fully-connected and softmax layers are taken into account after the fine-grained combination of the two features in all approaches to model the answers. Given a question and an image, the models will predict the answer to the question by formulating the problem as a classification task (for a given set of answers).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment and Result</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">After the image and question feature extraction from VGGNet and LSTM layer respectively, three modes of feature combination criteria are considered. 1024 dimensional image and question feature vector are combined by concatenation, point-wise multiplication , or MFB module. By considering both cross-entropy and KL divergence loss, all the models are optimized by stochastic gradient descent ( SGD) with batch size 16. The dataset is split into three sets namely training, validation and testing with 60%, 20%, 20% ratio respectively. In the training phase, models are validated by validation dataset via <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">early stopping</span> criterion with patience 30.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Our motivation for developing an efficient UAV imagery-based VQA model for post-disaster damage assessment comes from the performances of baseline models in Table <a href="#S2.T1" title="Table 1 ‣ 2 Dataset Description ‣ VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Accuracy is the performance metric that we consider for the VQA task to compare the baseline models. We consider top-1 accuracy for the comparison purpose. If the ground-truth matches the output (which has the highest probability) from a model, the accuracy for any image is 1, otherwise it is 0. Overall accuracy for all these models lies between .54 and .60 which indicates that models hardly understand the scenario for a given question. <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">Yes/No</span> type question has higher accuracy compared to other question types. Furthermore, we highlight the difficulties in dealing with the <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">complex counting</span> by comparing the accuracy between <span id="S4.p2.1.3" class="ltx_text ltx_font_italic">simple counting</span> and <span id="S4.p2.1.4" class="ltx_text ltx_font_italic">complex counting</span> problems. Accuracy for the <span id="S4.p2.1.5" class="ltx_text ltx_font_italic">complex counting</span> problem is lower among all the question types. This result highlight the performances of baseline VQA models on our dataset.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our aim for this analysis is to study the baseline VQA frameworks by presenting our <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">HurMic-VQA</span> dataset for post-disaster damage assessment. This study also upholds the importance of implementing Visual Question Answering task for post-disaster damage assessment. We only consider a subset of our dataset that targets only one type of object for this work. From the baseline results, we can understand the importance of developing an effective VQA algorithm for post-disaster damage assessment.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Benjamin Bischke, Patrick Helber, Christian Schulze, Venkat Srinivasan, Andreas
Dengel, and Damian Borth,

</span>
<span class="ltx_bibblock">“The multimedia satellite task at mediaeval 2017.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">MediaEval</span>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bischke Benjamin, Helber Patrick, Zhao Zhengyu, Borth Damian, et al.,

</span>
<span class="ltx_bibblock">“The multimedia satellite task at mediaeval 2018: Emergency response
for flooding events,”

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar
Doshi, Eric Heim, Howie Choset, and Matthew Gaston,

</span>
<span class="ltx_bibblock">“xbd: A dataset for assessing building damage from satellite
imagery,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.09296</span>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud
Yari, and Robin Murphy,

</span>
<span class="ltx_bibblock">“Floodnet: A high resolution aerial imagery dataset for post flood
scene understanding,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.02951</span>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ritwik Gupta, Bryce Goodman, Nirav Patel, Ricky Hosfelt, Sandra Sajeev, Eric
Heim, Jigar Doshi, Keane Lucas, Howie Choset, and Matthew Gaston,

</span>
<span class="ltx_bibblock">“Creating xbd: A dataset for assessing building damage from
satellite imagery,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span>, 2019, pp. 10–17.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Christos Kyrkou and Theocharis Theocharides,

</span>
<span class="ltx_bibblock">“Deep-learning-based aerial image classification for emergency
response applications using unmanned aerial vehicles.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">CVPR Workshops</span>, 2019, pp. 517–525.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Tim GJ Rudner, Marc Rußwurm, Jakub Fil, Ramona Pelich, Benjamin Bischke,
Veronika Kopačková, and Piotr Biliński,

</span>
<span class="ltx_bibblock">“Multi3net: segmenting flooded buildings via fusion of
multiresolution, multisensor, and multitemporal satellite imagery,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, 2019, vol. 33, pp. 702–709.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Tashnim Chowdhury, Maryam Rahnemoonfar, Robin Murphy, and Odair Fernandes,

</span>
<span class="ltx_bibblock">“Comprehensive semantic segmentation on high resolution uav imagery
for natural disaster damage assessment,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Conference on Big Data (Big Data)</span>,
2020, pp. 3904–3913.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus,

</span>
<span class="ltx_bibblock">“Simple baseline for visual question answering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span>, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh,

</span>
<span class="ltx_bibblock">“Visual question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia,

</span>
<span class="ltx_bibblock">“Abc-cnn: An attention based convolutional neural network for visual
question answering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05960</span>, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kevin J Shih, Saurabh Singh, and Derek Hoiem,

</span>
<span class="ltx_bibblock">“Where to look: Focus regions for visual question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2016, pp. 4613–4621.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola,

</span>
<span class="ltx_bibblock">“Stacked attention networks for image question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2016, pp. 21–29.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach,

</span>
<span class="ltx_bibblock">“Multimodal compact bilinear pooling for visual question answering
and visual grounding,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang,

</span>
<span class="ltx_bibblock">“Hadamard product for low-rank bilinear pooling,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.04325</span>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao,

</span>
<span class="ltx_bibblock">“Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, 2017, pp. 1821–1830.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh,

</span>
<span class="ltx_bibblock">“Hierarchical question-image co-attention for visual question
answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus,

</span>
<span class="ltx_bibblock">“Indoor segmentation and support inference from rgbd images,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Computer Vision – ECCV 2012</span>, Andrew Fitzgibbon, Svetlana
Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid, Eds., Berlin,
Heidelberg, 2012, pp. 746–760, Springer Berlin Heidelberg.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.10546" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.10548" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.10548">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.10548" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.10549" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 12:20:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
