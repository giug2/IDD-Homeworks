<article class="ltx_document ltx_authors_1line ltx_leqno">
 <h1 class="ltx_title ltx_title_document">
  Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Shuyuan Liu
    <sup class="ltx_sup" id="id9.9.id1">
     <span class="ltx_text ltx_font_italic" id="id9.9.id1.1">
      1
     </span>
    </sup>
    , Jiawei Chen
    <sup class="ltx_sup" id="id10.10.id2">
     <span class="ltx_text ltx_font_italic" id="id10.10.id2.1">
      1
     </span>
    </sup>
    , Shouwei Ruan
    <sup class="ltx_sup" id="id11.11.id3">
     <span class="ltx_text ltx_font_italic" id="id11.11.id3.1">
      2
     </span>
    </sup>
    , Hang Su
    <sup class="ltx_sup" id="id12.12.id4">
     <span class="ltx_text ltx_font_italic" id="id12.12.id4.1">
      3
     </span>
    </sup>
    , Zhaoxia Yin
    <sup class="ltx_sup" id="id13.13.id5">
     <span class="ltx_text ltx_font_italic" id="id13.13.id5.1">
      1
     </span>
    </sup>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id14.14.id6">
     <span class="ltx_text ltx_font_italic" id="id14.14.id6.1">
      1
     </span>
    </sup>
    East China Normal University
    <sup class="ltx_sup" id="id15.15.id7">
     <span class="ltx_text ltx_font_italic" id="id15.15.id7.1">
      2
     </span>
    </sup>
    Beihang University
    <sup class="ltx_sup" id="id16.16.id8">
     <span class="ltx_text ltx_font_italic" id="id16.16.id8.1">
      3
     </span>
    </sup>
    Tsinghua University
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract.
  </h6>
  <p class="ltx_p" id="id17.id1">
   Embodied intelligence empowers agents with a profound sense of perception, enabling them to respond in a manner closely aligned with real-world situations. Large Language Models (LLMs) delve into language instructions with depth, serving a crucial role in generating plans for intricate tasks. Thus, LLM-based embodied models further enhance the agent’s capacity to comprehend and process information. However, this amalgamation also ushers in new challenges in the pursuit of heightened intelligence. Specifically, attackers can manipulate LLMs to produce irrelevant or even malicious outputs by altering their prompts. Confronted with this challenge, we observe a notable absence of multi-modal datasets essential for comprehensively evaluating the robustness of LLM-based embodied models. Consequently, we construct the Embodied Intelligent Robot Attack Dataset (EIRAD), tailored specifically for robustness evaluation. Additionally, two attack strategies are devised, including untargeted attacks and targeted attacks, to effectively simulate a range of diverse attack scenarios. At the same time, during the attack process, to more accurately ascertain whether our method is successful in attacking the LLM-based embodied model, we devise a new attack success evaluation method utilizing the BLIP2 model. Recognizing the time and cost-intensive nature of the GCG algorithm in attacks, we devise a scheme for prompt suffix initialization based on various target tasks, thus expediting the convergence process. Experimental results demonstrate that our method exhibits a superior attack success rate when targeting LLM-based embodied models, indicating a lower level of decision-level robustness in these models.
  </p>
 </div>
 <div class="ltx_keywords">
  Embodied task planning, Adversarial attack, Large language model
 </div>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Security and privacy Social aspects of security and privacy
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Security and privacy Spoofing attacks
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Security and privacy Spoofing attacks
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id4">
  <sup class="ltx_note_mark">
   †
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     †
    </sup>
    <span class="ltx_note_type">
     ccs:
    </span>
    Security and privacy Spoofing attacks
   </span>
  </span>
 </span>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1.
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    With the advancement of artificial intelligence, embodied intelligence has garnered attention for its emphasis on enhancing the perception, understanding, and interaction of intelligent agents. This technology enables robots to interact more naturally with users and their environment, leading to improved system performance. Recent studies suggest that the fusion of an embodied intelligence robot with a LLM can further augment the system’s intelligence level
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2023b
     </a>
     ; Song et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib26" title="">
      2023
     </a>
     ; Li et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023a
     </a>
     ; Brohan et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     ; Lynch et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     ; Huang et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2022
     </a>
     ; Schumann et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      2024
     </a>
     )
    </cite>
    . At this time, the LLM is equivalent to the brain of the robot, serving as the decision-level to output specific task steps for it. However, this integration presents new challenges, particularly the risk of adversarial attacks
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2024
     </a>
     ; Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     ; Liu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     ; Ding et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023
     </a>
     ; Li et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023b
     </a>
     )
    </cite>
    . Attackers can manipulate text prompts of LLMs to generate irrelevant or malicious outputs, raising concerns about the security and reliability of the system
    <cite class="ltx_cite ltx_citemacro_citep">
     (Liu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     ; Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     )
    </cite>
    . Such attacks can lead agents to perform actions irrelevant to intended tasks or even exhibit unsafe behaviors, as illustrated in Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    . Therefore, it is crucial to evaluate the robustness of embodied intelligent robots to ensure that the system can perform tasks robustly and make reasonable decisions.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="290" id="S1.F1.g1" src="/html/2405.19802/assets/x1.png" width="368"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1.
    </span>
    Illustration of embodied intelligence attack. Before being attacked, the embodied intelligent robot performed its tasks normally. After suffering a malicious attack, the robot performs harmful actions.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Traditional LLM text attacks or jailbreak attacks mainly focus on the security issues of the model in text generation, especially on the LLM value alignment level. For instance, Zou et al.
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     )
    </cite>
    proposed the GCG algorithm to circumvent LLM’s value alignment, inducing it to generate harmful content by appending an adversarial suffix to prompts. Additionally, Zhu et al.
    <cite class="ltx_cite ltx_citemacro_citep">
     (Liu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    introduced a jailbreak attack tailored for aligned LLMs, which automates the generation of cryptic prompts using a hierarchical genetic algorithm to bypass value alignment. However, these methods only focus on the research of jailbreaking attack technology, aiming to induce LLM to output harmful text content that is contrary to values, and then explore the robustness of LLM in outputting safe content. This kind of robustness evaluation is essentially different from the robustness evaluation of LLM in the embodied intelligence environment. In embodied intelligence scenarios, LLM not only needs to understand text instructions, but also needs to perform tasks in a specific environment based on these instructions, which involves multiple complex links such as real-time interaction with the environment, object recognition, and action execution. Therefore, this requires us to consider attacks at the level of LLM values as well as attacks related to the actual task execution of the robot in the adversarial robustness evaluation, thereby ensuring that the embodied intelligent robot can maintain stable performance and security in the face of various attacks. It is precisely because of these differences that traditional LLM attack methods cannot be applied to the robustness evaluation of LLM in embodied environments.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Secondly, a key problem is the lack of multi-modal dataset suitable for LLM robustness evaluation of embodied intelligent robots. The AdvBench dataset proposed by Zou et al.
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     )
    </cite>
    includes a wide range of harmful content such as profanity, graphic descriptions, threatening behaviors, misinformation, discrimination, cybercrime, and dangerous or illegal advice. However, in LLM-based embodied model, the required data not only needs to cover harmful text, but also needs to input images, and it also needs to involve in-depth interaction and fusion between text and images. This complexity makes it difficult for existing datasets to directly adapt to this specific scenario, thus limiting the further development and application of embodied intelligence LLM.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    To address this challenge, we propose a multi-modal dataset in embodied scenes to fill this research gap. The interactive relationship between text and images is fully considered during the production process of this dataset. All text information in the dataset is designed based on the objects contained in the pictures in order to more comprehensively evaluate the performance of embodied intelligent robots. The dataset is divided into targeted attack data and untargeted attack data. Each type of data contains 500 pairs of image and text information. Targeted attack data simulates a situation where the attacker has a clear target and is intended to examine the system’s defense and confrontation capabilities in this situation; untargeted attack data does not limit the specific output target and is intended to make the system output inconsistent with expected, random or meaningless content. At the same time, according to the characteristics of the LLM-based embodied model that output content according to the structure of step 1 to step n, we improve the text matching algorithm in the GCG
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     )
    </cite>
    and slice the output content of LLM according to each step, aiming to reduce the occurrence of missed and wrong judgments, making attack assessment more accurate and reliable. Moreover, we use the CLIP model to encode the content of each step and the target task, and compute the cosine similarity between them, enabling a more robust assessment of attack success and enhancing the method’s adaptability across diverse embodied intelligence scenarios. In addition, we observe that when performing a targeted attack, the prompt suffix of a successful attack contains certain keywords of the target task. Therefore, we use certain keywords in the target task to initialize the prompt suffix, thereby improving the attack success rate and shortening the attack time.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Experimental results show that compared with GCG
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     )
    </cite>
    and AutoDAN
    <cite class="ltx_cite ltx_citemacro_citep">
     (Liu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    , using our method to attack LLM-based embodied model has a higher success rate and takes less time and cost. Our contributions are summarized as follows:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       As far as we know, this work represents the first experiment in exploring the robustness of LLM-based embodied model decision-level processes.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We design a multi-modal dataset consisting of 500 instances of untargeted attack data and 500 instances of targeted attack data to fill the gaps in datasets for robustness evaluation in embodied scenarios.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       Extensive experiments show that our method improves attack success rate and attack efficiency.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2.
   </span>
   Related Works
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1.
    </span>
    Embodied task planning
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     As an emerging and significant research direction, embodied task planning has garnered attention from numerous researchers in domains such as domestic service
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023a
      </a>
      )
     </cite>
     , medical treatment
     <cite class="ltx_cite ltx_citemacro_citep">
      (Sun et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2023
      </a>
      ; Zhao et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib41" title="">
       2023
      </a>
      ; Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2024
      </a>
      )
     </cite>
     , and agricultural harvesting
     <cite class="ltx_cite ltx_citemacro_citep">
      (Vemprala et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib32" title="">
       2023
      </a>
      ; Stella et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2023
      </a>
      )
     </cite>
     . Existing works primarily delve into harnessing NLP technology to aid robots in planning and executing intricate tasks. On one front, some studies utilize domain-specific datasets to train conventional deep learning models for generating robot mission plans
     <cite class="ltx_cite ltx_citemacro_citep">
      (Sun et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2022
      </a>
      )
     </cite>
     . On the other hand, the emergence of LLM has brought forth enhanced semantic comprehension and natural language processing capabilities, further empowering embodied intelligence. The LLM-based embodied model empowers the system to better grasp and execute tasks based on natural language. Wu et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023b
      </a>
      )
     </cite>
     introduced the TAsk planning Agent, which aligns LLM with a visual perception model to generate executable plans based on scene objects, grounding planning with physical constraints. Li et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023a
      </a>
      )
     </cite>
     created a multi-modal dataset and fine-tuned LLM using it, allowing the robot to execute new instructions with minimal context learning. Song et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Song et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023
      </a>
      )
     </cite>
     proposed the LLM-Planner framework, facilitating the interaction between planning and the environment by amalgamating high-level planning instructions from LLM with the environmental state mapped by low-level planners. However, despite the strides made in embodied task planning by the aforementioned research
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023a
      </a>
      ; Wu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023b
      </a>
      ; Song et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023
      </a>
      ; Lynch et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      ; Sharan et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2024
      </a>
      ; Yang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib38" title="">
       2023
      </a>
      ; Nottingham et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2023
      </a>
      ; Dorbala et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib11" title="">
       2023
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2024
      </a>
      ; Zheng et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib42" title="">
       2023
      </a>
      ; Qiao et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      ; Szot et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      ; Dagan et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      ; Zhang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2023
      </a>
      )
     </cite>
     , the measurement and assurance of robot safety and robustness during task execution post the integration of LLM remain prominent challenges. Thus, this paper aims to introduce new perspectives and methodologies for the evaluation of security and robustness in the field of embodied task planning combined with LLM, through the design of attack algorithms.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2.
    </span>
    Jailbreak attack based on LLM
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     LLM has received a lot of attention because of its powerful generative ability, but recent studies
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      ; Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      ; Ding et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
      ; Wei et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2024
      </a>
      ; Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023b
      </a>
      ; Dong et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2023
      </a>
      ; Wei et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib33" title="">
       2023
      </a>
      ; Chao et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2023
      </a>
      ; Shah et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
      ; Carlini et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      ; Huang et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2023
      </a>
      ; Yu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib39" title="">
       2023
      </a>
      ; Qi et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023
      </a>
      )
     </cite>
     have shown that LLM is vulnerable to jailbreak attacks to bypass its own value alignment mechanism. Zou et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     proposed a adversarial jailbreak attack algorithm that allows malicious questions to induce their aligned language models to produce harmful content by adding adversarial suffixes. Ding et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ding et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
      )
     </cite>
     proposed the ReNeLLM framework, which uses dual design to conceal the harmful prompt and bypass the value alignment strategy of LLM. Zhu et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     proposed the AutoDAN framework, which automatically generates secret jailbreak hints through a carefully designed hierarchical genetic algorithm, enabling LLM to bypass value alignment and generate responses to the malicious prompt. However, the above-mentioned studies only focus on adversarial content at the text level, ignoring the impact of multi-modal information such as vision and action in embodied intelligence, and cannot be directly applied to embodied scenarios. Therefore, this paper considers combining a multi-modal dataset with embodied environments to more comprehensively evaluate the performance of embodied intelligent robots. At the same time, LLM in the embodied scenario is attacked according to the values-aligned attack strategy, so that it outputs content unrelated to prompts or even malicious content, and then explores the security risks caused by the introduction of LLM in embodied intelligence technology.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3.
   </span>
   Method
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this section, we first describe the format distribution and construction of the EIRAD used to evaluate the robustness of the LLM decision-level in embodied scenarios. In Section
    <a class="ltx_ref" href="#S3.SS1.SSS1" title="3.1.1. Data types and statistics. ‣ 3.1. Dataset analysis and creation process ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      3.1.1
     </span>
    </a>
    we present the data types and statistics of the EIRAD. In Section
    <a class="ltx_ref" href="#S3.SS1.SSS2" title="3.1.2. Description of the dataset creation process. ‣ 3.1. Dataset analysis and creation process ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      3.1.2
     </span>
    </a>
    , we outline the process of generating the EIRAD. Subsequently, we delve into the details of attacking the LLM-based embodied models. Specifically, in Section
    <a class="ltx_ref" href="#S3.SS2.SSS1" title="3.2.1. Initialize prompt suffix ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      3.2.1
     </span>
    </a>
    , we elaborate on the details of prompt suffix initialization, and discuss the implementation of the attack algorithm in Section
    <a class="ltx_ref" href="#S3.SS2.SSS2" title="3.2.2. Optimize adversarial suffixes ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      3.2.2
     </span>
    </a>
    . Furthermore, we outline the evaluation method for determining the success of the attack in Section
    <a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3. Judgment of attack success ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      3.2.3
     </span>
    </a>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1.
    </span>
    Dataset analysis and creation process
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     The AdvBench dataset proposed by Zou et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     encompasses a broad spectrum of harmful content, ranging from profanity and graphic descriptions to threatening behaviors, misinformation, discrimination, cybercrime, and dangerous or illegal advice. However, in the embodied scenario, the requisite data not only entails encompassing harmful text but also necessitates the inclusion of images as inputs, alongside requiring a deep interaction and fusion between text and images. This intricate nature poses challenges for existing datasets to directly cater to this specific scenario. Hence, we propose the EIRAD dataset to assess the robustness of LLM in embodied intelligent robotics.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS1.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.1.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.1.1">
      Data types and statistics.
     </span>
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS1.p1">
     <p class="ltx_p" id="S3.SS1.SSS1.p1.1">
      The dataset types are illustrated in Figure
      <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.1.1. Data types and statistics. ‣ 3.1. Dataset analysis and creation process ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      , which is divided into two main categories: targeted attack data and untargeted attack data. In untargeted attack data, we do not set a specific output target, and aim to make the system output unexpected, random or meaningless content. Such attacks may exhibit more randomness and covert characteristics, necessitating a highly robust system to handle them effectively. In contrast, in the targeted attack data, specific attack targets are set, such as ”creating chaos”, ”making harmful suggestions”, and so on. This configuration aims to simulate scenarios where attackers have clear goals or expected outputs, thereby evaluating the system’s defense and countermeasure capabilities under such circumstances. Additionally, the targeted attack data is further subdivided into harmful attack data and harmless attack data. The goal of setting the harmful attack data is to prompt the LLM to produce harmful, dangerous, or inappropriate content. This enables the evaluation of the system’s response to malicious inputs and assesses whether the system adheres to ethical and legal standards. Conversely, the goal of setting harmless attack data is to prompt the LLM to generate harmless but invalid content, providing insights into the system’s stability against harmless inputs. Additionally, we supplement the dataset by incorporating output responses generated by GPT3.5 for each attack data, thereby enhancing the completeness of the dataset. By utilizing this data classification and setup to simulate various attack scenarios, the security and robustness of the LLM’s decision-level capabilities in specific contexts can be thoroughly assessed. This process aids researchers in identifying potential security vulnerabilities and improving the system’s defense mechanisms.
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F2">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S3.F2.g1" src="/html/2405.19802/assets/x2.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 2.
      </span>
      Data type distribution in EIRAD
     </figcaption>
    </figure>
    <figure class="ltx_figure" id="S3.F3">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_3">
       <figure class="ltx_figure ltx_figure_panel" id="S3.F3.sf1">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="158" id="S3.F3.sf1.g1" src="/html/2405.19802/assets/data-harmless-prompt.png" width="198"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (a)
         </span>
         prompt in harmless data
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_3">
       <figure class="ltx_figure ltx_figure_panel" id="S3.F3.sf2">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="158" id="S3.F3.sf2.g1" src="/html/2405.19802/assets/data-harmless-target.png" width="198"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (b)
         </span>
         target in harmless data
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_3">
       <figure class="ltx_figure ltx_figure_panel" id="S3.F3.sf3">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="158" id="S3.F3.sf3.g1" src="/html/2405.19802/assets/data-harmful-target.png" width="198"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (c)
         </span>
         target in harmful data
        </figcaption>
       </figure>
      </div>
     </div>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       Figure 3.
      </span>
      The data statistics of multi-modal.
(a) The 10 most frequently prompted verbs in harmless data along with their corresponding noun objects.
(b) The 10 most frequently targeted verbs in harmless data along with their corresponding noun objects.
(c) The 10 most frequently targeted verbs in harmful data along with their corresponding noun objects.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S3.SS1.SSS1.p2">
     <p class="ltx_p" id="S3.SS1.SSS1.p2.1">
      In order to better evaluate the performance of embodied intelligent robots in terms of security and robustness, we simulate as much as possible the various behaviors and actions that attackers might take when making EIRAD. In figure
      <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.1.1. Data types and statistics. ‣ 3.1. Dataset analysis and creation process ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      , we illustrate the 10 most frequently occurring verbs in the prompt and target instructions of the targeted attack data, along with all the corresponding noun objects. In the harmless data, both prompt and target instructions exhibit rich and diverse characteristics, encompassing various actions such as ”heating,” ”using,” ”adjustment,” and more. The noun objects associated with these actions also vary, including items like ”microwave oven,” ”spatula,” ”lampshade,” and others. This diversity is designed to simulate the myriad challenges that robots may encounter in embodied environments. On the other hand, the harmful data presents a range of high-frequency verb and noun object combinations, such as ”cut off fingers,” ”break mirrors,” and so on. These combinations reflect the diversity and complexity of attacks that malicious actors might employ, leveraging the robot’s physical and sensory capabilities. For instance, ”cutting off fingers” implies scenarios where the robot could potentially cause harm to human bodies, while ”breaking mirrors” might result in damage to objects within the environment. Overall, EIRAD has important reference significance for evaluating the performance of embodied intelligent robots in terms of safety and robustness, and designing effective defense strategies and mechanisms. Additionally, it provides researchers with a comprehensive and detailed dataset to delve into and address the safety challenges and potential risks inherent in embodied intelligent robot systems.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS1.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.2.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.1.1">
      Description of the dataset creation process.
     </span>
    </h4>
    <figure class="ltx_figure" id="S3.F4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="211" id="S3.F4.g1" src="/html/2405.19802/assets/x3.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 4.
      </span>
      The creation process of multi-modal dataset
     </figcaption>
    </figure>
    <div class="ltx_para" id="S3.SS1.SSS2.p1">
     <p class="ltx_p" id="S3.SS1.SSS2.p1.1">
      The creation process of the multi-modal dataset EIRAD is depicted in Figure
      <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.1.2. Description of the dataset creation process. ‣ 3.1. Dataset analysis and creation process ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      . The key distinction between targeted attack data and untargeted attack data lies in the presence of an additional ”target” instruction within the targeted attack data. Consequently, step 1 to 3 in Figure 4 represent the Co-production process for both untargeted and targeted attack data, while step 4 is the distinct production process for targeted attack data.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS1.SSS2.p2">
     <p class="ltx_p" id="S3.SS1.SSS2.p2.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p2.1.1">
       Collect image.
      </span>
      100 scene images from varying perspectives are chosen from the AI2-THOR simulator
      <cite class="ltx_cite ltx_citemacro_citep">
       (Kolve et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib14" title="">
        2017
       </a>
       )
      </cite>
      to serve as the embodied scene for the robotic entity.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS1.SSS2.p3">
     <p class="ltx_p" id="S3.SS1.SSS2.p3.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p3.1.1">
       Detect object list.
      </span>
      A methodology akin to TAPA
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wu et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib37" title="">
        2023b
       </a>
       )
      </cite>
      is employed to precisely discern object information within the scene using an open vocabulary detector. Any redundant object names within the scene are then expunged, thereby furnishing pertinent scene details for the LLM, such as the input = [chair, table, bowl, microwave…].
     </p>
    </div>
    <div class="ltx_para" id="S3.SS1.SSS2.p4">
     <p class="ltx_p" id="S3.SS1.SSS2.p4.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p4.1.1">
       Generate prompt.
      </span>
      In the ALFRED benchmark
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zou et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023
       </a>
       )
      </cite>
      , a straightforward approach for generating multi-modal instructions pertinent to embodied tasks involves crafting a series of instructions tailored to the prevailing environment. Nonetheless, devised designs necessitate considerable effort, especially when crafting targeted attack data. Each data instance mandates the separate formulation of both prompt and target instructions. To enhance production efficiency and curtail costs, we devise a mechanism utilizing GPT-3.5 to simulate specific task planning scenarios, thereby automatically generating prompt instructions based on the provided object name list input, as depicted in step 3 of Figure 4. Consequently, the production of untargeted attack data is completed at this juncture.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS1.SSS2.p5">
     <p class="ltx_p" id="S3.SS1.SSS2.p5.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p5.1.1">
       Generate target.
      </span>
      In order to reduce production costs and improve efficiency, GPT-3.5 is also used in this step to generate target instructions in the targeted attack data. The GPT3.5 prompt is shown in step 4. In this prompt, It is pivotal to emphasize that the generated ”target” should bear no relation to the prompt, yet simultaneously encompass the listed input objects.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2.
    </span>
    Embodied scenario attack algorithm
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Our objective is to investigate the robustness of the LLM-based embodied model’s decision-level processes. The algorithmic framework, as shown in Figure
     <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , unfolds in several steps. Initially, in step 1, we initialize a prompt suffix. Subsequently, in step 2, we optimize the prompt suffix using the greedy gradient descent algorithm
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     , aiming to prompt the LLM to output content unrelated to the prompt. Following this optimization process, in step 3, we slice the output content according to each step of output and calculate its similarity with the target to determine the success of the attack. If the attack is unsuccessful, we return to step 2 and continue optimizing the prompt suffix. In the subsequent sections, we will elaborate on these three steps in detail.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="196" id="S3.F5.g1" src="/html/2405.19802/assets/x4.png" width="368"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5.
     </span>
     The framework of the attack algorithm. Attack algorithms are categorized into two main types: untargeted attacks and targeted attacks. The targeted approach builds upon the foundation of non-targeted methods, showcasing differences in keyword initialization (step 1), selection of optimal suffixes (step 2), and selection of evaluation objects (step 3).
    </figcaption>
   </figure>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.1.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.1.1">
      Initialize prompt suffix
     </span>
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.1">
      To guide the LLM in generating content unrelated to the prompts, we initialize a prompt suffix, as illustrated in step 1 of Figure
      <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      . In untargeted attacks, the suffix is optimized to ensure that the LLM outputs content that is unrelated to the original prompt. However, In targeted attacks, the suffix is optimized to prompt the LLM to output content relevant to the target task. As depicted in Figure
      <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.2.1. Initialize prompt suffix ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      , through experiments on targeted attacks, it is discovered that successful adversarial suffixes often contain keywords related to the target task. In order to enhance the iteration speed and success rate of the attack, we devise a strategy to design the initial content of the adversarial suffix based on the specific target task in the targeted attack scenario. Building upon the original ”!!!”, we replace a portion of the ”!!!” with keywords pertinent to the targeted task, as demonstrated in Figure
      <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.2.1. Initialize prompt suffix ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F6">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S3.F6.g1" src="/html/2405.19802/assets/x5.png" width="438"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 6.
      </span>
      Prompt suffix display and initialization strategy.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.2.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.1.1">
      Optimize adversarial suffixes
     </span>
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      As depicted in Figure
      <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , optimizing adversarial suffixes enables LLM to generate prompt-independent content. Following a methodology similar to GCG
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zou et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023
       </a>
       )
      </cite>
      , we conceptualize the generation phase of the LLM as predicting the subsequent token given the current token sequence. Building on this concept, we allow the LLM to use an input sequence of length n to generate a response of length H which can be represented as:
     </p>
     <table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx1">
      <tbody id="S3.E1">
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_left">
          (1)
         </span>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_td ltx_align_right ltx_eqn_cell">
         <math alttext="\displaystyle p\left(x_{n+1:n+H}\mid x_{1:n}\right)=\prod_{i=1}^{H}p\left(x_{n+i}\mid x_{1:n+i-1}\right)" class="ltx_Math" display="inline" id="S3.E1.m1.2">
          <semantics id="S3.E1.m1.2a">
           <mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">
            <mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">
             <mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">
              p
             </mi>
             <mo id="S3.E1.m1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.2.cmml">
              ​
             </mo>
             <mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">
              <mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">
               (
              </mo>
              <mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">
               <msub id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">
                <mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">
                 x
                </mi>
                <mrow id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">
                 <mrow id="S3.E1.m1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.cmml">
                  <mi id="S3.E1.m1.1.1.1.1.1.1.2.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.2.cmml">
                   n
                  </mi>
                  <mo id="S3.E1.m1.1.1.1.1.1.1.2.3.2.1" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.1.cmml">
                   +
                  </mo>
                  <mn id="S3.E1.m1.1.1.1.1.1.1.2.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.3.cmml">
                   1
                  </mn>
                 </mrow>
                 <mo id="S3.E1.m1.1.1.1.1.1.1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml">
                  :
                 </mo>
                 <mrow id="S3.E1.m1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.cmml">
                  <mi id="S3.E1.m1.1.1.1.1.1.1.2.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.2.cmml">
                   n
                  </mi>
                  <mo id="S3.E1.m1.1.1.1.1.1.1.2.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.1.cmml">
                   +
                  </mo>
                  <mi id="S3.E1.m1.1.1.1.1.1.1.2.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.3.cmml">
                   H
                  </mi>
                 </mrow>
                </mrow>
               </msub>
               <mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">
                ∣
               </mo>
               <msub id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">
                <mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">
                 x
                </mi>
                <mrow id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">
                 <mn id="S3.E1.m1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.3.2.cmml">
                  1
                 </mn>
                 <mo id="S3.E1.m1.1.1.1.1.1.1.3.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml">
                  :
                 </mo>
                 <mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3.cmml">
                  n
                 </mi>
                </mrow>
               </msub>
              </mrow>
              <mo id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">
             =
            </mo>
            <mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">
             <mstyle displaystyle="true" id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">
              <munderover id="S3.E1.m1.2.2.2.2a" xref="S3.E1.m1.2.2.2.2.cmml">
               <mo id="S3.E1.m1.2.2.2.2.2.2" movablelimits="false" xref="S3.E1.m1.2.2.2.2.2.2.cmml">
                ∏
               </mo>
               <mrow id="S3.E1.m1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.3.cmml">
                <mi id="S3.E1.m1.2.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.2.3.2.cmml">
                 i
                </mi>
                <mo id="S3.E1.m1.2.2.2.2.2.3.1" xref="S3.E1.m1.2.2.2.2.2.3.1.cmml">
                 =
                </mo>
                <mn id="S3.E1.m1.2.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.2.3.3.cmml">
                 1
                </mn>
               </mrow>
               <mi id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml">
                H
               </mi>
              </munderover>
             </mstyle>
             <mrow id="S3.E1.m1.2.2.2.1" xref="S3.E1.m1.2.2.2.1.cmml">
              <mi id="S3.E1.m1.2.2.2.1.3" xref="S3.E1.m1.2.2.2.1.3.cmml">
               p
              </mi>
              <mo id="S3.E1.m1.2.2.2.1.2" lspace="0em" rspace="0em" xref="S3.E1.m1.2.2.2.1.2.cmml">
               ​
              </mo>
              <mrow id="S3.E1.m1.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
               <mo id="S3.E1.m1.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
                (
               </mo>
               <mrow id="S3.E1.m1.2.2.2.1.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
                <msub id="S3.E1.m1.2.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.1.2.cmml">
                 <mi id="S3.E1.m1.2.2.2.1.1.1.1.2.2" xref="S3.E1.m1.2.2.2.1.1.1.1.2.2.cmml">
                  x
                 </mi>
                 <mrow id="S3.E1.m1.2.2.2.1.1.1.1.2.3" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.cmml">
                  <mi id="S3.E1.m1.2.2.2.1.1.1.1.2.3.2" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.2.cmml">
                   n
                  </mi>
                  <mo id="S3.E1.m1.2.2.2.1.1.1.1.2.3.1" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.1.cmml">
                   +
                  </mo>
                  <mi id="S3.E1.m1.2.2.2.1.1.1.1.2.3.3" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.3.cmml">
                   i
                  </mi>
                 </mrow>
                </msub>
                <mo id="S3.E1.m1.2.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.1.cmml">
                 ∣
                </mo>
                <msub id="S3.E1.m1.2.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.cmml">
                 <mi id="S3.E1.m1.2.2.2.1.1.1.1.3.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2.cmml">
                  x
                 </mi>
                 <mrow id="S3.E1.m1.2.2.2.1.1.1.1.3.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.cmml">
                  <mn id="S3.E1.m1.2.2.2.1.1.1.1.3.3.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.2.cmml">
                   1
                  </mn>
                  <mo id="S3.E1.m1.2.2.2.1.1.1.1.3.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.1.cmml">
                   :
                  </mo>
                  <mrow id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.cmml">
                   <mrow id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.cmml">
                    <mi id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.2.cmml">
                     n
                    </mi>
                    <mo id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.1" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.1.cmml">
                     +
                    </mo>
                    <mi id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.3.cmml">
                     i
                    </mi>
                   </mrow>
                   <mo id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.1" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.1.cmml">
                    −
                   </mo>
                   <mn id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.3.cmml">
                    1
                   </mn>
                  </mrow>
                 </mrow>
                </msub>
               </mrow>
               <mo id="S3.E1.m1.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b">
            <apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">
             <eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3">
             </eq>
             <apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1">
              <times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2">
              </times>
              <ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">
               𝑝
              </ci>
              <apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1">
               <csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">
                conditional
               </csymbol>
               <apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">
                <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">
                 subscript
                </csymbol>
                <ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">
                 𝑥
                </ci>
                <apply id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">
                 <ci id="S3.E1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.1">
                  :
                 </ci>
                 <apply id="S3.E1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2">
                  <plus id="S3.E1.m1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.1">
                  </plus>
                  <ci id="S3.E1.m1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.2">
                   𝑛
                  </ci>
                  <cn id="S3.E1.m1.1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.3">
                   1
                  </cn>
                 </apply>
                 <apply id="S3.E1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3">
                  <plus id="S3.E1.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.1">
                  </plus>
                  <ci id="S3.E1.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.2">
                   𝑛
                  </ci>
                  <ci id="S3.E1.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.3.3">
                   𝐻
                  </ci>
                 </apply>
                </apply>
               </apply>
               <apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">
                <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">
                 subscript
                </csymbol>
                <ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">
                 𝑥
                </ci>
                <apply id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">
                 <ci id="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.1">
                  :
                 </ci>
                 <cn id="S3.E1.m1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.3.3.2">
                  1
                 </cn>
                 <ci id="S3.E1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3">
                  𝑛
                 </ci>
                </apply>
               </apply>
              </apply>
             </apply>
             <apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2">
              <apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2">
                superscript
               </csymbol>
               <apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">
                <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2">
                 subscript
                </csymbol>
                <csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">
                 product
                </csymbol>
                <apply id="S3.E1.m1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.3">
                 <eq id="S3.E1.m1.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.3.1">
                 </eq>
                 <ci id="S3.E1.m1.2.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.3.2">
                  𝑖
                 </ci>
                 <cn id="S3.E1.m1.2.2.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.3.3">
                  1
                 </cn>
                </apply>
               </apply>
               <ci id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3">
                𝐻
               </ci>
              </apply>
              <apply id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.1">
               <times id="S3.E1.m1.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.1.2">
               </times>
               <ci id="S3.E1.m1.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.1.3">
                𝑝
               </ci>
               <apply id="S3.E1.m1.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1">
                <csymbol cd="latexml" id="S3.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.1">
                 conditional
                </csymbol>
                <apply id="S3.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.2">
                  𝑥
                 </ci>
                 <apply id="S3.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3">
                  <plus id="S3.E1.m1.2.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.1">
                  </plus>
                  <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.2">
                   𝑛
                  </ci>
                  <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.3">
                   𝑖
                  </ci>
                 </apply>
                </apply>
                <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2">
                  𝑥
                 </ci>
                 <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3">
                  <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.1">
                   :
                  </ci>
                  <cn id="S3.E1.m1.2.2.2.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.2">
                   1
                  </cn>
                  <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3">
                   <minus id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.1">
                   </minus>
                   <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2">
                    <plus id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.1">
                    </plus>
                    <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.2">
                     𝑛
                    </ci>
                    <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.2.3">
                     𝑖
                    </ci>
                   </apply>
                   <cn id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.3">
                    1
                   </cn>
                  </apply>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.E1.m1.2c">
            \displaystyle p\left(x_{n+1:n+H}\mid x_{1:n}\right)=\prod_{i=1}^{H}p\left(x_{n+i}\mid x_{1:n+i-1}\right)
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_left ltx_eqn_cell">
         <math alttext="\displaystyle=0" class="ltx_Math" display="inline" id="S3.E1.m2.1">
          <semantics id="S3.E1.m2.1a">
           <mrow id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">
            <mi id="S3.E1.m2.1.1.2" xref="S3.E1.m2.1.1.2.cmml">
            </mi>
            <mo id="S3.E1.m2.1.1.1" xref="S3.E1.m2.1.1.1.cmml">
             =
            </mo>
            <mn id="S3.E1.m2.1.1.3" xref="S3.E1.m2.1.1.3.cmml">
             0
            </mn>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b">
            <apply id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">
             <eq id="S3.E1.m2.1.1.1.cmml" xref="S3.E1.m2.1.1.1">
             </eq>
             <csymbol cd="latexml" id="S3.E1.m2.1.1.2.cmml" xref="S3.E1.m2.1.1.2">
              absent
             </csymbol>
             <cn id="S3.E1.m2.1.1.3.cmml" type="integer" xref="S3.E1.m2.1.1.3">
              0
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.E1.m2.1c">
            \displaystyle=0
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p2">
     <p class="ltx_p" id="S3.SS2.SSS2.p2.2">
      In untargeted attacks, our aim is for
      <math alttext="x_{n+1:n+H}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1.1">
       <semantics id="S3.SS2.SSS2.p2.1.m1.1a">
        <msub id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">
         <mi id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">
          x
         </mi>
         <mrow id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml">
          <mrow id="S3.SS2.SSS2.p2.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.cmml">
           <mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.2.cmml">
            n
           </mi>
           <mo id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.1.cmml">
            +
           </mo>
           <mn id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.3.cmml">
            1
           </mn>
          </mrow>
          <mo id="S3.SS2.SSS2.p2.1.m1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.1.cmml">
           :
          </mo>
          <mrow id="S3.SS2.SSS2.p2.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.cmml">
           <mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2.cmml">
            n
           </mi>
           <mo id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1.cmml">
            +
           </mo>
           <mi id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3.cmml">
            H
           </mi>
          </mrow>
         </mrow>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b">
         <apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">
           𝑥
          </ci>
          <apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">
           <ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.1">
            :
           </ci>
           <apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2">
            <plus id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.1">
            </plus>
            <ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.2">
             𝑛
            </ci>
            <cn id="S3.SS2.SSS2.p2.1.m1.1.1.3.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.2.3">
             1
            </cn>
           </apply>
           <apply id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3">
            <plus id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.1">
            </plus>
            <ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.2">
             𝑛
            </ci>
            <ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.3.3">
             𝐻
            </ci>
           </apply>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">
         x_{n+1:n+H}
        </annotation>
       </semantics>
      </math>
      to generate content that is as unrelated to the prompts as possible. Conversely, in targeted attacks, our goal is for
      <math alttext="x_{n+1:n+H}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.2.m2.1">
       <semantics id="S3.SS2.SSS2.p2.2.m2.1a">
        <msub id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">
         <mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">
          x
         </mi>
         <mrow id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">
          <mrow id="S3.SS2.SSS2.p2.2.m2.1.1.3.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.cmml">
           <mi id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.2.cmml">
            n
           </mi>
           <mo id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.1.cmml">
            +
           </mo>
           <mn id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.3.cmml">
            1
           </mn>
          </mrow>
          <mo id="S3.SS2.SSS2.p2.2.m2.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.1.cmml">
           :
          </mo>
          <mrow id="S3.SS2.SSS2.p2.2.m2.1.1.3.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.cmml">
           <mi id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.2.cmml">
            n
           </mi>
           <mo id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.1.cmml">
            +
           </mo>
           <mi id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.3.cmml">
            H
           </mi>
          </mrow>
         </mrow>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b">
         <apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">
           𝑥
          </ci>
          <apply id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">
           <ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.1">
            :
           </ci>
           <apply id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2">
            <plus id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.1">
            </plus>
            <ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.2">
             𝑛
            </ci>
            <cn id="S3.SS2.SSS2.p2.2.m2.1.1.3.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.2.3">
             1
            </cn>
           </apply>
           <apply id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3">
            <plus id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.1">
            </plus>
            <ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.2">
             𝑛
            </ci>
            <ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.3.3">
             𝐻
            </ci>
           </apply>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">
         x_{n+1:n+H}
        </annotation>
       </semantics>
      </math>
      to generate content that fulfills the target requirements to the fullest extent. Therefore, the loss function can be formulated to minimize the probability of these key target sequences under untargeted attack conditions, and to maximize the probability of these key target sequences under targeted attack conditions.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p3">
     <table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx2">
      <tbody id="S3.E2">
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_left">
          (2)
         </span>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_td ltx_align_right ltx_eqn_cell">
         <math alttext="\displaystyle\mathcal{L}\left(x_{1:n}\right)=-\log p\left(x_{n+1:n+H}^{\prime}\mid x_{1:n}\right)" class="ltx_Math" display="inline" id="S3.E2.m1.2">
          <semantics id="S3.E2.m1.2a">
           <mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">
            <mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">
             <mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">
              ℒ
             </mi>
             <mo id="S3.E2.m1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E2.m1.1.1.1.2.cmml">
              ​
             </mo>
             <mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
              <mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
               (
              </mo>
              <msub id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
               <mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">
                x
               </mi>
               <mrow id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">
                <mn id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">
                 1
                </mn>
                <mo id="S3.E2.m1.1.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml">
                 :
                </mo>
                <mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">
                 n
                </mi>
               </mrow>
              </msub>
              <mo id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml">
             =
            </mo>
            <mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">
             <mo id="S3.E2.m1.2.2.2a" rspace="0.167em" xref="S3.E2.m1.2.2.2.cmml">
              −
             </mo>
             <mrow id="S3.E2.m1.2.2.2.1" xref="S3.E2.m1.2.2.2.1.cmml">
              <mrow id="S3.E2.m1.2.2.2.1.3" xref="S3.E2.m1.2.2.2.1.3.cmml">
               <mi id="S3.E2.m1.2.2.2.1.3.1" xref="S3.E2.m1.2.2.2.1.3.1.cmml">
                log
               </mi>
               <mo id="S3.E2.m1.2.2.2.1.3a" lspace="0.167em" xref="S3.E2.m1.2.2.2.1.3.cmml">
                ⁡
               </mo>
               <mi id="S3.E2.m1.2.2.2.1.3.2" xref="S3.E2.m1.2.2.2.1.3.2.cmml">
                p
               </mi>
              </mrow>
              <mo id="S3.E2.m1.2.2.2.1.2" lspace="0em" rspace="0em" xref="S3.E2.m1.2.2.2.1.2.cmml">
               ​
              </mo>
              <mrow id="S3.E2.m1.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.1.1.1.1.cmml">
               <mo id="S3.E2.m1.2.2.2.1.1.1.2" xref="S3.E2.m1.2.2.2.1.1.1.1.cmml">
                (
               </mo>
               <mrow id="S3.E2.m1.2.2.2.1.1.1.1" xref="S3.E2.m1.2.2.2.1.1.1.1.cmml">
                <msubsup id="S3.E2.m1.2.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.2.1.1.1.1.2.cmml">
                 <mi id="S3.E2.m1.2.2.2.1.1.1.1.2.2.2" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.2.cmml">
                  x
                 </mi>
                 <mrow id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.cmml">
                  <mrow id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.cmml">
                   <mi id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.2" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.2.cmml">
                    n
                   </mi>
                   <mo id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.1" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.1.cmml">
                    +
                   </mo>
                   <mn id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.3" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.3.cmml">
                    1
                   </mn>
                  </mrow>
                  <mo id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.1.cmml">
                   :
                  </mo>
                  <mrow id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.cmml">
                   <mi id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.2" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.2.cmml">
                    n
                   </mi>
                   <mo id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.1" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.1.cmml">
                    +
                   </mo>
                   <mi id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.3" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.3.cmml">
                    H
                   </mi>
                  </mrow>
                 </mrow>
                 <mo id="S3.E2.m1.2.2.2.1.1.1.1.2.3" xref="S3.E2.m1.2.2.2.1.1.1.1.2.3.cmml">
                  ′
                 </mo>
                </msubsup>
                <mo id="S3.E2.m1.2.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.2.1.1.1.1.1.cmml">
                 ∣
                </mo>
                <msub id="S3.E2.m1.2.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.2.1.1.1.1.3.cmml">
                 <mi id="S3.E2.m1.2.2.2.1.1.1.1.3.2" xref="S3.E2.m1.2.2.2.1.1.1.1.3.2.cmml">
                  x
                 </mi>
                 <mrow id="S3.E2.m1.2.2.2.1.1.1.1.3.3" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.cmml">
                  <mn id="S3.E2.m1.2.2.2.1.1.1.1.3.3.2" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.2.cmml">
                   1
                  </mn>
                  <mo id="S3.E2.m1.2.2.2.1.1.1.1.3.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.1.cmml">
                   :
                  </mo>
                  <mi id="S3.E2.m1.2.2.2.1.1.1.1.3.3.3" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.3.cmml">
                   n
                  </mi>
                 </mrow>
                </msub>
               </mrow>
               <mo id="S3.E2.m1.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b">
            <apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">
             <eq id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3">
             </eq>
             <apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">
              <times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2">
              </times>
              <ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">
               ℒ
              </ci>
              <apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1">
               <csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1">
                subscript
               </csymbol>
               <ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">
                𝑥
               </ci>
               <apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">
                <ci id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1">
                 :
                </ci>
                <cn id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.1.1.3.2">
                 1
                </cn>
                <ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">
                 𝑛
                </ci>
               </apply>
              </apply>
             </apply>
             <apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2">
              <minus id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2">
              </minus>
              <apply id="S3.E2.m1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.1">
               <times id="S3.E2.m1.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.1.2">
               </times>
               <apply id="S3.E2.m1.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.1.3">
                <log id="S3.E2.m1.2.2.2.1.3.1.cmml" xref="S3.E2.m1.2.2.2.1.3.1">
                </log>
                <ci id="S3.E2.m1.2.2.2.1.3.2.cmml" xref="S3.E2.m1.2.2.2.1.3.2">
                 𝑝
                </ci>
               </apply>
               <apply id="S3.E2.m1.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1">
                <csymbol cd="latexml" id="S3.E2.m1.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.1">
                 conditional
                </csymbol>
                <apply id="S3.E2.m1.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2">
                 <csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2">
                  superscript
                 </csymbol>
                 <apply id="S3.E2.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2">
                  <csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2">
                   subscript
                  </csymbol>
                  <ci id="S3.E2.m1.2.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.2">
                   𝑥
                  </ci>
                  <apply id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3">
                   <ci id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.1">
                    :
                   </ci>
                   <apply id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2">
                    <plus id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.1">
                    </plus>
                    <ci id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.2">
                     𝑛
                    </ci>
                    <cn id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.3.cmml" type="integer" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.2.3">
                     1
                    </cn>
                   </apply>
                   <apply id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3">
                    <plus id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.1">
                    </plus>
                    <ci id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.2">
                     𝑛
                    </ci>
                    <ci id="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.2.3.3.3">
                     𝐻
                    </ci>
                   </apply>
                  </apply>
                 </apply>
                 <ci id="S3.E2.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.2.3">
                  ′
                 </ci>
                </apply>
                <apply id="S3.E2.m1.2.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.3">
                 <csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.3">
                  subscript
                 </csymbol>
                 <ci id="S3.E2.m1.2.2.2.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.3.2">
                  𝑥
                 </ci>
                 <apply id="S3.E2.m1.2.2.2.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3">
                  <ci id="S3.E2.m1.2.2.2.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.1">
                   :
                  </ci>
                  <cn id="S3.E2.m1.2.2.2.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.2">
                   1
                  </cn>
                  <ci id="S3.E2.m1.2.2.2.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.2.2.2.1.1.1.1.3.3.3">
                   𝑛
                  </ci>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.E2.m1.2c">
            \displaystyle\mathcal{L}\left(x_{1:n}\right)=-\log p\left(x_{n+1:n+H}^{\prime}\mid x_{1:n}\right)
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_left ltx_eqn_cell">
         <math alttext="\displaystyle=0" class="ltx_Math" display="inline" id="S3.E2.m2.1">
          <semantics id="S3.E2.m2.1a">
           <mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">
            <mi id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml">
            </mi>
            <mo id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.cmml">
             =
            </mo>
            <mn id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml">
             0
            </mn>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b">
            <apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1">
             <eq id="S3.E2.m2.1.1.1.cmml" xref="S3.E2.m2.1.1.1">
             </eq>
             <csymbol cd="latexml" id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2">
              absent
             </csymbol>
             <cn id="S3.E2.m2.1.1.3.cmml" type="integer" xref="S3.E2.m2.1.1.3">
              0
             </cn>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.E2.m2.1c">
            \displaystyle=0
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p4">
     <p class="ltx_p" id="S3.SS2.SSS2.p4.1">
      Furthermore, the untargeted attack task is transformed into maximizing the loss function’s negative log probability, while the targeted attack task is transformed into minimizing the loss function’s negative log probability.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p5">
     <table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx3">
      <tbody id="S3.E3">
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_left">
          (3)
         </span>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_td ltx_align_right ltx_eqn_cell">
         <math alttext="\displaystyle\underset{x\in\{1,\ldots,V\}}{\operatorname{minimize}}\mathcal{L}\left(x_{1:n}\right)=0,~{}~{}\underset{x\in\{1,\ldots,V\}}{\operatorname{maximize}}\mathcal{L}\left(x_{1:n}\right)=0" class="ltx_Math" display="inline" id="S3.E3.m1.8">
          <semantics id="S3.E3.m1.8a">
           <mrow id="S3.E3.m1.8.8.2" xref="S3.E3.m1.8.8.3.cmml">
            <mrow id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml">
             <mrow id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.1.cmml">
              <munder accentunder="true" id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">
               <mi id="S3.E3.m1.3.3.4" xref="S3.E3.m1.3.3.4.cmml">
                minimize
               </mi>
               <mrow id="S3.E3.m1.3.3.3" xref="S3.E3.m1.3.3.3.cmml">
                <mi id="S3.E3.m1.3.3.3.5" xref="S3.E3.m1.3.3.3.5.cmml">
                 x
                </mi>
                <mo id="S3.E3.m1.3.3.3.4" xref="S3.E3.m1.3.3.3.4.cmml">
                 ∈
                </mo>
                <mrow id="S3.E3.m1.3.3.3.6.2" xref="S3.E3.m1.3.3.3.6.1.cmml">
                 <mo id="S3.E3.m1.3.3.3.6.2.1" stretchy="false" xref="S3.E3.m1.3.3.3.6.1.cmml">
                  {
                 </mo>
                 <mn id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">
                  1
                 </mn>
                 <mo id="S3.E3.m1.3.3.3.6.2.2" xref="S3.E3.m1.3.3.3.6.1.cmml">
                  ,
                 </mo>
                 <mi id="S3.E3.m1.2.2.2.2" mathvariant="normal" xref="S3.E3.m1.2.2.2.2.cmml">
                  …
                 </mi>
                 <mo id="S3.E3.m1.3.3.3.6.2.3" xref="S3.E3.m1.3.3.3.6.1.cmml">
                  ,
                 </mo>
                 <mi id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">
                  V
                 </mi>
                 <mo id="S3.E3.m1.3.3.3.6.2.4" stretchy="false" xref="S3.E3.m1.3.3.3.6.1.cmml">
                  }
                 </mo>
                </mrow>
               </mrow>
              </munder>
              <mo id="S3.E3.m1.7.7.1.1.1.2" lspace="0.167em" rspace="0em" xref="S3.E3.m1.7.7.1.1.1.2.cmml">
               ​
              </mo>
              <mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.7.7.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.3.cmml">
               ℒ
              </mi>
              <mo id="S3.E3.m1.7.7.1.1.1.2a" lspace="0em" rspace="0em" xref="S3.E3.m1.7.7.1.1.1.2.cmml">
               ​
              </mo>
              <mrow id="S3.E3.m1.7.7.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml">
               <mo id="S3.E3.m1.7.7.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml">
                (
               </mo>
               <msub id="S3.E3.m1.7.7.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml">
                <mi id="S3.E3.m1.7.7.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml">
                 x
                </mi>
                <mrow id="S3.E3.m1.7.7.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml">
                 <mn id="S3.E3.m1.7.7.1.1.1.1.1.1.3.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.2.cmml">
                  1
                 </mn>
                 <mo id="S3.E3.m1.7.7.1.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.1.cmml">
                  :
                 </mo>
                 <mi id="S3.E3.m1.7.7.1.1.1.1.1.1.3.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.3.cmml">
                  n
                 </mi>
                </mrow>
               </msub>
               <mo id="S3.E3.m1.7.7.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
             <mo id="S3.E3.m1.7.7.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml">
              =
             </mo>
             <mn id="S3.E3.m1.7.7.1.1.3" xref="S3.E3.m1.7.7.1.1.3.cmml">
              0
             </mn>
            </mrow>
            <mo id="S3.E3.m1.8.8.2.3" rspace="0.827em" xref="S3.E3.m1.8.8.3a.cmml">
             ,
            </mo>
            <mrow id="S3.E3.m1.8.8.2.2" xref="S3.E3.m1.8.8.2.2.cmml">
             <mrow id="S3.E3.m1.8.8.2.2.1" xref="S3.E3.m1.8.8.2.2.1.cmml">
              <munder accentunder="true" id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml">
               <mi id="S3.E3.m1.6.6.4" xref="S3.E3.m1.6.6.4.cmml">
                maximize
               </mi>
               <mrow id="S3.E3.m1.6.6.3" xref="S3.E3.m1.6.6.3.cmml">
                <mi id="S3.E3.m1.6.6.3.5" xref="S3.E3.m1.6.6.3.5.cmml">
                 x
                </mi>
                <mo id="S3.E3.m1.6.6.3.4" xref="S3.E3.m1.6.6.3.4.cmml">
                 ∈
                </mo>
                <mrow id="S3.E3.m1.6.6.3.6.2" xref="S3.E3.m1.6.6.3.6.1.cmml">
                 <mo id="S3.E3.m1.6.6.3.6.2.1" stretchy="false" xref="S3.E3.m1.6.6.3.6.1.cmml">
                  {
                 </mo>
                 <mn id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml">
                  1
                 </mn>
                 <mo id="S3.E3.m1.6.6.3.6.2.2" xref="S3.E3.m1.6.6.3.6.1.cmml">
                  ,
                 </mo>
                 <mi id="S3.E3.m1.5.5.2.2" mathvariant="normal" xref="S3.E3.m1.5.5.2.2.cmml">
                  …
                 </mi>
                 <mo id="S3.E3.m1.6.6.3.6.2.3" xref="S3.E3.m1.6.6.3.6.1.cmml">
                  ,
                 </mo>
                 <mi id="S3.E3.m1.6.6.3.3" xref="S3.E3.m1.6.6.3.3.cmml">
                  V
                 </mi>
                 <mo id="S3.E3.m1.6.6.3.6.2.4" stretchy="false" xref="S3.E3.m1.6.6.3.6.1.cmml">
                  }
                 </mo>
                </mrow>
               </mrow>
              </munder>
              <mo id="S3.E3.m1.8.8.2.2.1.2" lspace="0.167em" rspace="0em" xref="S3.E3.m1.8.8.2.2.1.2.cmml">
               ​
              </mo>
              <mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.8.8.2.2.1.3" xref="S3.E3.m1.8.8.2.2.1.3.cmml">
               ℒ
              </mi>
              <mo id="S3.E3.m1.8.8.2.2.1.2a" lspace="0em" rspace="0em" xref="S3.E3.m1.8.8.2.2.1.2.cmml">
               ​
              </mo>
              <mrow id="S3.E3.m1.8.8.2.2.1.1.1" xref="S3.E3.m1.8.8.2.2.1.1.1.1.cmml">
               <mo id="S3.E3.m1.8.8.2.2.1.1.1.2" xref="S3.E3.m1.8.8.2.2.1.1.1.1.cmml">
                (
               </mo>
               <msub id="S3.E3.m1.8.8.2.2.1.1.1.1" xref="S3.E3.m1.8.8.2.2.1.1.1.1.cmml">
                <mi id="S3.E3.m1.8.8.2.2.1.1.1.1.2" xref="S3.E3.m1.8.8.2.2.1.1.1.1.2.cmml">
                 x
                </mi>
                <mrow id="S3.E3.m1.8.8.2.2.1.1.1.1.3" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.cmml">
                 <mn id="S3.E3.m1.8.8.2.2.1.1.1.1.3.2" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.2.cmml">
                  1
                 </mn>
                 <mo id="S3.E3.m1.8.8.2.2.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.1.cmml">
                  :
                 </mo>
                 <mi id="S3.E3.m1.8.8.2.2.1.1.1.1.3.3" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.3.cmml">
                  n
                 </mi>
                </mrow>
               </msub>
               <mo id="S3.E3.m1.8.8.2.2.1.1.1.3" xref="S3.E3.m1.8.8.2.2.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
             <mo id="S3.E3.m1.8.8.2.2.2" xref="S3.E3.m1.8.8.2.2.2.cmml">
              =
             </mo>
             <mn id="S3.E3.m1.8.8.2.2.3" xref="S3.E3.m1.8.8.2.2.3.cmml">
              0
             </mn>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S3.E3.m1.8b">
            <apply id="S3.E3.m1.8.8.3.cmml" xref="S3.E3.m1.8.8.2">
             <csymbol cd="ambiguous" id="S3.E3.m1.8.8.3a.cmml" xref="S3.E3.m1.8.8.2.3">
              formulae-sequence
             </csymbol>
             <apply id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1.1">
              <eq id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2">
              </eq>
              <apply id="S3.E3.m1.7.7.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1">
               <times id="S3.E3.m1.7.7.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.2">
               </times>
               <apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">
                <apply id="S3.E3.m1.3.3.3.cmml" xref="S3.E3.m1.3.3.3">
                 <in id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.4">
                 </in>
                 <ci id="S3.E3.m1.3.3.3.5.cmml" xref="S3.E3.m1.3.3.3.5">
                  𝑥
                 </ci>
                 <set id="S3.E3.m1.3.3.3.6.1.cmml" xref="S3.E3.m1.3.3.3.6.2">
                  <cn id="S3.E3.m1.1.1.1.1.cmml" type="integer" xref="S3.E3.m1.1.1.1.1">
                   1
                  </cn>
                  <ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">
                   …
                  </ci>
                  <ci id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">
                   𝑉
                  </ci>
                 </set>
                </apply>
                <ci id="S3.E3.m1.3.3.4.cmml" xref="S3.E3.m1.3.3.4">
                 minimize
                </ci>
               </apply>
               <ci id="S3.E3.m1.7.7.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.3">
                ℒ
               </ci>
               <apply id="S3.E3.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1">
                <csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1">
                 subscript
                </csymbol>
                <ci id="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2">
                 𝑥
                </ci>
                <apply id="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3">
                 <ci id="S3.E3.m1.7.7.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.1">
                  :
                 </ci>
                 <cn id="S3.E3.m1.7.7.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.2">
                  1
                 </cn>
                 <ci id="S3.E3.m1.7.7.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.3">
                  𝑛
                 </ci>
                </apply>
               </apply>
              </apply>
              <cn id="S3.E3.m1.7.7.1.1.3.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.3">
               0
              </cn>
             </apply>
             <apply id="S3.E3.m1.8.8.2.2.cmml" xref="S3.E3.m1.8.8.2.2">
              <eq id="S3.E3.m1.8.8.2.2.2.cmml" xref="S3.E3.m1.8.8.2.2.2">
              </eq>
              <apply id="S3.E3.m1.8.8.2.2.1.cmml" xref="S3.E3.m1.8.8.2.2.1">
               <times id="S3.E3.m1.8.8.2.2.1.2.cmml" xref="S3.E3.m1.8.8.2.2.1.2">
               </times>
               <apply id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6">
                <apply id="S3.E3.m1.6.6.3.cmml" xref="S3.E3.m1.6.6.3">
                 <in id="S3.E3.m1.6.6.3.4.cmml" xref="S3.E3.m1.6.6.3.4">
                 </in>
                 <ci id="S3.E3.m1.6.6.3.5.cmml" xref="S3.E3.m1.6.6.3.5">
                  𝑥
                 </ci>
                 <set id="S3.E3.m1.6.6.3.6.1.cmml" xref="S3.E3.m1.6.6.3.6.2">
                  <cn id="S3.E3.m1.4.4.1.1.cmml" type="integer" xref="S3.E3.m1.4.4.1.1">
                   1
                  </cn>
                  <ci id="S3.E3.m1.5.5.2.2.cmml" xref="S3.E3.m1.5.5.2.2">
                   …
                  </ci>
                  <ci id="S3.E3.m1.6.6.3.3.cmml" xref="S3.E3.m1.6.6.3.3">
                   𝑉
                  </ci>
                 </set>
                </apply>
                <ci id="S3.E3.m1.6.6.4.cmml" xref="S3.E3.m1.6.6.4">
                 maximize
                </ci>
               </apply>
               <ci id="S3.E3.m1.8.8.2.2.1.3.cmml" xref="S3.E3.m1.8.8.2.2.1.3">
                ℒ
               </ci>
               <apply id="S3.E3.m1.8.8.2.2.1.1.1.1.cmml" xref="S3.E3.m1.8.8.2.2.1.1.1">
                <csymbol cd="ambiguous" id="S3.E3.m1.8.8.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.8.8.2.2.1.1.1">
                 subscript
                </csymbol>
                <ci id="S3.E3.m1.8.8.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.8.8.2.2.1.1.1.1.2">
                 𝑥
                </ci>
                <apply id="S3.E3.m1.8.8.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3">
                 <ci id="S3.E3.m1.8.8.2.2.1.1.1.1.3.1.cmml" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.1">
                  :
                 </ci>
                 <cn id="S3.E3.m1.8.8.2.2.1.1.1.1.3.2.cmml" type="integer" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.2">
                  1
                 </cn>
                 <ci id="S3.E3.m1.8.8.2.2.1.1.1.1.3.3.cmml" xref="S3.E3.m1.8.8.2.2.1.1.1.1.3.3">
                  𝑛
                 </ci>
                </apply>
               </apply>
              </apply>
              <cn id="S3.E3.m1.8.8.2.2.3.cmml" type="integer" xref="S3.E3.m1.8.8.2.2.3">
               0
              </cn>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.E3.m1.8c">
            \displaystyle\underset{x\in\{1,\ldots,V\}}{\operatorname{minimize}}\mathcal{L}\left(x_{1:n}\right)=0,~{}~{}\underset{x\in\{1,\ldots,V\}}{\operatorname{maximize}}\mathcal{L}\left(x_{1:n}\right)=0
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p6">
     <p class="ltx_p" id="S3.SS2.SSS2.p6.1">
      After determining the optimization target, the subsequent step involves optimizing this set of discrete inputs. Specifically, we utilize the one-hot token indicator to identify a set of promising candidate replacement tokens at each position. Subsequently, through forward propagation, we assess these replacements. Then, by calculating the top-k candidates for token replacement, we select the replacement words that maximize the loss in untargeted attacks and minimize the loss in targeted attacks. This computation is executed for each candidate position, yielding the final result—the adversarial suffix that optimizes the loss function.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.3.
     </span>
     <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.1.1">
      Judgment of attack success
     </span>
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS3.p1">
     <p class="ltx_p" id="S3.SS2.SSS3.p1.1">
      As illustrated in step 3 of Figure
      <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , the updated suffix is incorporated into LLM alongside the original prompt to obtain the resulting output content. Subsequently, the output content is sliced and the similarity is computed to ascertain the success of the attack. In both GCG
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zou et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023
       </a>
       )
      </cite>
      and AutoDAN
      <cite class="ltx_cite ltx_citemacro_citep">
       (Liu et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib18" title="">
        2023
       </a>
       )
      </cite>
      , the determination of attack success hinges on whether the output content aligns with the predefined list. However, this method heavily relies on the quality and comprehensiveness of the predefined list. Particularly in targeted attack, the predefined list needs constant updates corresponding to changes in the target task, which may introduce errors and affect the accuracy of experimental outcomes. Therefore, we devise a novel set of evaluation criteria and employ slicing operations to partition the output content of LLM based on the characteristics of embodied intelligence. It includes slicing operations and calculating similarity operations, which we will introduce in detail below.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS3.p2">
     <p class="ltx_p" id="S3.SS2.SSS3.p2.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p2.1.1">
       Slice operation.
      </span>
      To mitigate the occurrence of misjudgments and oversights, we implement a slicing operation on the LLM’s response. In an embodied intelligence environment, the response format of the LLM typically aligns with the structure depicted in Figure
      <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.2.1. Initialize prompt suffix ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      . Herein, the number of steps within the output content varies with different tasks, and their respective correlations fluctuate accordingly. Consequently, establishing a singular, standardized threshold to gauge the strong correlation between them for subsequent similarity assessments proves challenging. This challenge could potentially lead to misjudgments and overlooked details. To address this challenge, we employ a slicing operation, treating each step within the output content as an individual entity. We calculate the similarity of each step with the target task independently and select the step with the highest similarity as the basis for measurement. If the computed similarity exceeds the predetermined threshold, it indicates that the LLM has indeed produced the target statement, signifying the success of the attack.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS3.p3">
     <p class="ltx_p" id="S3.SS2.SSS3.p3.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.1">
       Similarity calculation.
      </span>
      Given the non-uniqueness of the output content from LLM-based embodied model, a similarity calculation approach is employed to assess the alignment with the target task. Common methods for calculating this similarity include those based on the bag-of-words model
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wu et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib36" title="">
        2010
       </a>
       )
      </cite>
      , TF-IDF weighted word vectors
      <cite class="ltx_cite ltx_citemacro_citep">
       (Aizawa,
       <a class="ltx_ref" href="#bib.bib2" title="">
        2003
       </a>
       )
      </cite>
      , Bert-score
      <cite class="ltx_cite ltx_citemacro_citep">
       (Unanue et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib31" title="">
        2021
       </a>
       )
      </cite>
      , among others. These methods are predominantly utilized in machine translation and text matching tasks. However, they possess limitations as they only capture the surface meaning of the statements, lack flexibility, and are unable to identify variations in expressions that convey the same meaning. To enhance the judgment of whether the output content aligns with the target task, we utilize the text encoding method from blip2-image-text-matching to obtain feature representations of both the output content and the target task. Subsequently, we calculate the cosine similarity between these representations to determine the degree of alignment with the target task.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4.
   </span>
   Experiments
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In this section, we demonstrate the experimental impact of our method on attacking LLM-based embodied model to assess the robustness of
embodied system. Firstly, in Section
    <a class="ltx_ref" href="#S4.SS1" title="4.1. Settings ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      4.1
     </span>
    </a>
    , we introduce the experimental setup. Following that, in Section
    <a class="ltx_ref" href="#S4.SS2" title="4.2. Main results ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      4.2
     </span>
    </a>
    , we present the comparison of attack results between our method and the most advanced white-box jailbreak attack technologies, including GCG
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zou et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023
     </a>
     )
    </cite>
    and AutoDAN
    <cite class="ltx_cite ltx_citemacro_citep">
     (Liu et al
     <span class="ltx_text">
      .
     </span>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    , applied to three different LLM-based embodied models. Subsequently, in Section
    <a class="ltx_ref" href="#S4.SS3" title="4.3. Execution success rate ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      4.3
     </span>
    </a>
    , we analyze the execution success rate by user study to evaluate whether the output of LLM can be executed in the current environment. Furthermore, in Section
    <a class="ltx_ref" href="#S4.SS4" title="4.4. The impact of keyword initialization on loss ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      4.4
     </span>
    </a>
    , we delve into the reasons why the initialization of prompt suffix keywords can significantly reduce the attack success time. Finally, ablation experiments are conducted in Section
    <a class="ltx_ref" href="#S4.SS5" title="4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
     <span class="ltx_text ltx_ref_tag">
      4.5
     </span>
    </a>
    to assess the importance of the two modules proposed by ours.
   </p>
  </div>
  <figure class="ltx_table" id="S4.T1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_table">
     Table 1.
    </span>
    Main results. We report the ASR and Epoch cost of our method for targeted and untargeted attacks on three models on the EIRAD dataset. Compared with the baseline, our method can effectively attack the LLM-based embodied model and greatly shorten the attack time.
   </figcaption>
   <div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1" style="width:390.3pt;height:137.5pt;vertical-align:-0.0pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-85.9pt,30.3pt) scale(0.694218484561075,0.694218484561075) ;">
     <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T1.1.1.1.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.1.1.1.1.1" style="padding-left:16.5pt;padding-right:16.5pt;">
         Experiment
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.1.1.1.1.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         Targeted attack-unharmful
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.1.1.1.1.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         Targeted attack-harmful
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.1.1.1.1.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         Untargeted attack
        </th>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.2.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.1" style="padding-left:16.5pt;padding-right:16.5pt;">
         Model
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         Method
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         ASR
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         Epoch cost
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         ASR
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         Epoch cost
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         ASR
        </th>
        <th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         Epoch cost
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T1.1.1.3.1">
        <td class="ltx_td ltx_border_t" id="S4.T1.1.1.3.1.1" style="padding-left:16.5pt;padding-right:16.5pt;">
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         AutoDAN
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         500
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         500
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.4.2">
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.1" style="padding-left:16.5pt;padding-right:16.5pt;">
         Tapa
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         GCG
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.32
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         148
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.02
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         74
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.4.2.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.5.3">
        <td class="ltx_td" id="S4.T1.1.1.5.3.1" style="padding-left:16.5pt;padding-right:16.5pt;">
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         Ours
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.3.1">
          0.72
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.4.1">
          84
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.5.1">
          0.22
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         124
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         1
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.5.3.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         9
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.6.4">
        <td class="ltx_td ltx_border_t" id="S4.T1.1.1.6.4.1" style="padding-left:16.5pt;padding-right:16.5pt;">
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         AutoDAN
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         500
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         500
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.1.1.6.4.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.7.5">
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.1" style="padding-left:16.5pt;padding-right:16.5pt;">
         Otter
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         GCG
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.81
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         101
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.64
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         180
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.7.5.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.8.6">
        <td class="ltx_td" id="S4.T1.1.1.8.6.1" style="padding-left:16.5pt;padding-right:16.5pt;">
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         Ours
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.8.6.3.1">
          0.95
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.8.6.4.1">
          56
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.8.6.5.1">
          0.86
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.8.6.6.1">
          120
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.80
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.8.6.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         67
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.9.7">
        <td class="ltx_td ltx_border_t" id="S4.T1.1.1.9.7.1" style="padding-left:16.5pt;padding-right:16.5pt;">
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         AutoDAN
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         500
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         500
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.1.1.9.7.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.10.8">
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.1" style="padding-left:16.5pt;padding-right:16.5pt;">
         Llama-2-chat
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         GCG
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.97
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         142
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.82
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         207
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.10.8.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.1.11.9">
        <td class="ltx_td ltx_border_bb" id="S4.T1.1.1.11.9.1" style="padding-left:16.5pt;padding-right:16.5pt;">
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.2" style="padding-left:16.5pt;padding-right:16.5pt;">
         Ours
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.3" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.11.9.3.1">
          0.97
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.4" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.11.9.4.1">
          60
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.5" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.11.9.5.1">
          0.92
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.6" style="padding-left:16.5pt;padding-right:16.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.11.9.6.1">
          127
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.7" style="padding-left:16.5pt;padding-right:16.5pt;">
         0.57
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.1.1.11.9.8" style="padding-left:16.5pt;padding-right:16.5pt;">
         104
        </td>
       </tr>
      </tbody>
     </table>
    </span>
   </div>
  </figure>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1.
    </span>
    Settings
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">
      Datasets.
     </span>
     We employ the multi-modal dataset EIRAD to assess the LLM robustness of embodied intelligent robots. This dataset comprises a total of 500 instances of untargeted attack data and 500 instances of targeted attack data.
Additionally, the targeted attack data is further categorized into 450 instances of harmless attack data and 50 instances of harmful attack data.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">
      Models.
     </span>
     To ensure the generality of the attack method, we assess two fine-tuned open-source models (TaPA and Otter) and one un-fine-tuned open-source model (Llama-2-7b-chat) in embodied scenarios. The TaPA model, developed by Wu et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023b
      </a>
      )
     </cite>
     , was fine-tuned using a dataset comprising 15K instruction-task data pairs to refine the Llama model. The Otter model was generated by Li et al.
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023a
      </a>
      )
     </cite>
     using the MIMIC-IT dataset containing 2.8 million multi-modal instruction-response pairs to fine-tune the OpenFlamingo
     <cite class="ltx_cite ltx_citemacro_citep">
      (Awadalla et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2023
      </a>
      )
     </cite>
     model. Additionally, the Llama-2-7b-chat model is utilized for decision-making in embodied tasks.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">
      Baselines.
     </span>
     Considering the related work on LLM adversarial jailbreaking
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      ; Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      ; Ding et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
      ; Wei et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2024
      </a>
      ; Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023b
      </a>
      )
     </cite>
     , we compare our method with some representative baseline methods, such as GCG
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     and AutoDAN
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     , to assess the robustness of the LLM-based embodied model under white-box attack scenarios. As evident from Section
     <a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3. Judgment of attack success ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       3.2.3
      </span>
     </a>
     , the text matching list methods employed by GCG and AutoDAN are not suitable for tasks involving embodied attacks. Hence, we replace the text matching algorithms in these two methods with the slicing and similarity calculation methods proposed in our paper as the criteria for judging attack success. Finally, we compare the method proposed in this paper with the GCG
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     and AutoDAN
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     algorithms to demonstrate the advantages of our method in terms of attack success rate and efficiency. It is important to note that our method has the same initial parameters as the original GCG
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     , epoch is 500, top-k is 256, and batchsize is 512.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">
      Evaluation metrics.
     </span>
     We evaluate the algorithm from three key aspects: Attack Success Rate (ASR), Execution Success Rate (ESR), and Epoch Cost. ASR indicates whether the LLM-based embodied model successfully outputs decision content related to the target task in targeted attacks or outputs decision content unrelated to the prompt in untargeted attacks. ESR reflects whether, upon a successful attack, the system can execute the output content under the prevailing environmental conditions. Epoch Cost denotes the average number of iterations required for an attack to succeed.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2.
    </span>
    Main results
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     illustrates the white-box attack evaluation results of our method and other baselines
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      ; Liu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     . In targeted attacks, we conduct these assessments by generating prompt suffixes for each targeted request in the EIRAD dataset and examining whether the final response from the LLM-based embodied model aligns with the targeted task. In untargeted attacks, we conducted similar evaluations by generating a prompt suffix to examine whether the final response of the LLM-based embodied model remains independent of the prompt task. We noted that in targeted attacks, our method effectively produces prompt suffixes and achieves a superior attack success rate compared to baseline methods. For the fine-tuned LLM-based embodied model TaPA
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wu et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023b
      </a>
      )
     </cite>
     and Otter
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023a
      </a>
      )
     </cite>
     , our method enhances the attack success rate by over 10%, and even surpasses 20% in harmful attack. Regarding the native model Llama-2-chat used for embodied tasks, our method demonstrates a comparable attack success rate to GCG
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     in harmless attack, while significantly reducing the Epoch cost. In untargeted attacks, our method also exhibits varying degrees of success across the three models, leading them to output content unrelated to the prompt. The AutoDAN algorithm’s attack success rate, as indicated by the data, is 0. This is due to its core concept of using a semantic prompt framework to guide LLMs to circumvent the value alignment mechanism, which falls short in generating specified content for particular tasks. In summary, our approach proves effective when embodied intelligent robots encounter diverse attack scenarios, enhancing the attack success rate while mitigating training costs. These results suggest that LLM-based embodied models display diminished robustness at the decision-level when subjected to adversarial attacks, offering insights for robustness research on embodied intelligent robots.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3.
    </span>
    Execution success rate
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     In the context of attacking a LLM-based embodied model, it is crucial to assess whether the LLM’s output aligns with both the task requirements and the embodied constraints, ensuring its successful execution within the current environment. As depicted in Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.3. Execution success rate ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , user study serves as a means to evaluate the ESR of the LLM’s output in the given environment. Experimental results demonstrate that in targeted attack, where the target task is designed with a thorough consideration of current environmental factors, the resulting output task steps largely adhere to the embodied requirements. In addition, due to the heightened precision in our attack success assessment, our method exhibits a superior ESR when juxtaposed with the GCG
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zou et al
      <span class="ltx_text">
       .
      </span>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023
      </a>
      )
     </cite>
     . However, in untargeted attack, where no specific target task is specified, the objective is to guide the LLM-based embodied model to generate content that is unrelated to the original prompt. Consequently, for a successful untargeted attack, the primary criterion is to ensure that the output content is disconnected from the prompt, without considering its feasibility for execution within the current scenario,resulting in a low ESR value.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T2">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 2.
     </span>
     ESR based on user study.
    </figcaption>
    <div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.1" style="width:390.3pt;height:151.3pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(9.4pt,-3.6pt) scale(1.05064070506271,1.05064070506271) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S4.T2.1.1.1.1">
         <td class="ltx_td ltx_border_t" id="S4.T2.1.1.1.1.1">
         </td>
         <th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.2">
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.3">
          Harmful attack
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.4">
          Harmless attack
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.5">
          Untargeted attack
         </th>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.2.2">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.1">
          Model
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.2">
          Method
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.3">
          ESR
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.4">
          ESR
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.5">
          ESR
         </th>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.3.3">
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.1" rowspan="2">
          <span class="ltx_text" id="S4.T2.1.1.3.3.1.1">
           Tapa
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.2">
          GCG
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.3">
          0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.4">
          0.67
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.5">
          -
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.4.4">
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.4.1">
          Ours
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.4.2">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.4.2.1">
           0.72
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.4.3">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.4.3.1">
           0.81
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.4.4">
          0.48
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.5.5">
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.1" rowspan="2">
          <span class="ltx_text" id="S4.T2.1.1.5.5.1.1">
           Otter
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.2">
          GCG
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.3">
          0.74
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.4">
          0.91
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.5">
          -
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.6.6">
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.6.1">
          Ours
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.6.2">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.6.2.1">
           0.84
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.6.3">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.6.3.1">
           0.95
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.6.4">
          0.48
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.7.7">
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.7.7.1" rowspan="2">
          <span class="ltx_text" id="S4.T2.1.1.7.7.1.1">
           Llama-2-chat
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.2">
          GCG
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.3">
          0.73
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.4">
          0.87
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.5">
          -
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.8.8">
         <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.8.8.1">
          Ours
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.8.8.2">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.8.8.2.1">
           0.78
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.8.8.3">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.8.8.3.1">
           0.88
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.8.8.4">
          0.45
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4.
    </span>
    The impact of keyword initialization on loss
   </h3>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     To delve into the reason behind the significant reduction in epoch cost due to prompt suffix keyword initialization, we conduct an analysis on the change trend of the loss value throughout the attack process. We compare the effects of prompt suffix keyword initialization with 2 keywords versus no keyword initialization on the three models individually. The variations in the loss value are visualized in Figure
     <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.4. The impact of keyword initialization on loss ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     . It is evident that the suffix initialized with keywords exhibits a notably lower loss value in the initial stages of the attack process. Consequently, during the iterative optimization of the suffixes, the model tends to swiftly identify the most suitable prompt suffix. This implies that in the early phases of an attack, the model can swiftly pinpoint an effective attack direction, thereby advancing towards a successful attack status more rapidly. This strategic advantage leads to quicker convergence towards successful attack outcomes, thereby enhancing the overall effectiveness and reliability of the targeted attack process.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F7">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F7.sf1.g1" src="/html/2405.19802/assets/OTTER-loss_vs_steps_harmless.png" width="138"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        Harmless attack in Otter
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F7.sf2.g1" src="/html/2405.19802/assets/OTTER-loss_vs_steps_harmful.png" width="138"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        Harmful attack in Otter
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf3">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F7.sf3.g1" src="/html/2405.19802/assets/VELMA-loss_vs_steps_harmless.png" width="138"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (c)
        </span>
        Harmless attack in Llama-2-chat
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf4">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F7.sf4.g1" src="/html/2405.19802/assets/VELMA-loss_vs_steps_harmful.png" width="138"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (d)
        </span>
        Harmful attack in Llama-2-chat
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf5">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F7.sf5.g1" src="/html/2405.19802/assets/TAPA-loss_vs_steps_harmless.png" width="138"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (e)
        </span>
        Harmless attack in TAPA
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf6">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F7.sf6.g1" src="/html/2405.19802/assets/TAPA-loss_vs_steps_harmful.png" width="138"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (f)
        </span>
        Harmful attack in TAPA
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7.
     </span>
     Suffix keyword initialization loss comparison
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.5.
    </span>
    Ablation Study
   </h3>
   <div class="ltx_para" id="S4.SS5.p1">
    <p class="ltx_p" id="S4.SS5.p1.1">
     In order to evaluate the importance of the two modules proposed in this paper, we conduct ablation experiments on the prompt suffix keyword initialization and the evaluation method to determine the success of the attack. In Section
     <a class="ltx_ref" href="#S4.SS5.SSS1" title="4.5.1. The impact of initializing the number of keywords ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       4.5.1
      </span>
     </a>
     , we examined how varying the number of prompt suffix keywords affects the ASR and epoch cost. In Section
     <a class="ltx_ref" href="#S4.SS5.SSS2" title="4.5.2. Validity of Assessment Methods ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
      <span class="ltx_text ltx_ref_tag">
       4.5.2
      </span>
     </a>
     , we assess the effectiveness of our chosen evaluation method for determining attack success.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S4.SS5.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.5.1.
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.SS5.SSS1.1.1">
      The impact of initializing the number of keywords
     </span>
    </h4>
    <div class="ltx_para" id="S4.SS5.SSS1.p1">
     <p class="ltx_p" id="S4.SS5.SSS1.p1.1">
      In order to analyze the impact of prompt suffix keyword initialization on ASR and Epoch cost, we conduct an ablation experiment on the number of keywords initialized by prompt suffix and explore its impact on the Otter model attack process. The attack results of other LLM-based embodied models are shown in the appendix. As depicted in Figure
      <a class="ltx_ref" href="#S4.F8.sf1" title="In Figure 8 ‣ 4.5.1. The impact of initializing the number of keywords ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        8a
       </span>
      </a>
      and Figure
      <a class="ltx_ref" href="#S4.F8.sf2" title="In Figure 8 ‣ 4.5.1. The impact of initializing the number of keywords ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        8b
       </span>
      </a>
      , we set various numbers of keyword initializations for both harmful and harmless attack data in the targeted attack scenario. The experimental findings reveal a consistent trend: as the number of initialized keywords increases, the ASR value steadily rises, while the epoch cost value decreases. This trend suggests that augmenting the number of keywords offers improved guidance to the model in identifying the optimal attack direction, thereby expediting the discovery of effective attack suffixes. However, during the actual attack execution, it becomes imperative to carefully balance the trade-offs among the increased workload due to additional initial suffix keywords, the resultant attack success rate, and the time taken for the attack. Consequently, selecting the optimal number of keywords for prompt suffix initialization becomes a crucial consideration in the attack strategy. Furthermore, our analysis of the evolving trend of loss values under varying initialization keyword numbers, as illustrated in Figure
      <a class="ltx_ref" href="#S4.F8.sf3" title="In Figure 8 ‣ 4.5.1. The impact of initializing the number of keywords ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        8c
       </span>
      </a>
      and Figure
      <a class="ltx_ref" href="#S4.F8.sf4" title="In Figure 8 ‣ 4.5.1. The impact of initializing the number of keywords ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        8d
       </span>
      </a>
      , reveals a compelling relationship: a higher number of initialized keywords corresponds to a lower initial loss value, resulting in a quicker attainment of attack success. This underscores the significance of judiciously optimizing the number of keywords for prompt suffix initialization to enhance the efficiency and effectiveness of the attack process.
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F8">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel" id="S4.F8.sf1">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="104" id="S4.F8.sf1.g1" src="/html/2405.19802/assets/Otter-ASR-keywords.png" width="138"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (a)
         </span>
         ASR in different keywords
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel" id="S4.F8.sf2">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="104" id="S4.F8.sf2.g1" src="/html/2405.19802/assets/OTTER-Epoch-cost-keywords.png" width="138"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (b)
         </span>
         Epoch cost in different keywords
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_break">
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel" id="S4.F8.sf3">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F8.sf3.g1" src="/html/2405.19802/assets/keywords-init-harmful.png" width="138"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (c)
         </span>
         Loss in harmful attack
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel" id="S4.F8.sf4">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="83" id="S4.F8.sf4.g1" src="/html/2405.19802/assets/keywords-init-harmless.png" width="138"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          (d)
         </span>
         Loss in harmless attack
        </figcaption>
       </figure>
      </div>
     </div>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       Figure 8.
      </span>
      The impact of initializing the number of keywords.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS5.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.5.2.
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.SS5.SSS2.1.1">
      Validity of Assessment Methods
     </span>
    </h4>
    <div class="ltx_para" id="S4.SS5.SSS2.p1">
     <p class="ltx_p" id="S4.SS5.SSS2.p1.1">
      To validate the effectiveness of the evaluation method proposed in this paper to determine the success of an attack, we conduct an ablation experiment of the step 3 in Figure
      <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.2. Embodied scenario attack algorithm ‣ 3. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      . Table
      <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.5.2. Validity of Assessment Methods ‣ 4.5. Ablation Study ‣ 4. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      illustrates that under the same attack conditions, the combination of slicing and similarity calculation proposed in this paper is compared with the matching list method in GCG
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zou et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023
       </a>
       )
      </cite>
      and with only the similarity calculation method to evaluate the correlation between the response of LLM-based embodied model and the target task upon a successful attack. Green markers indicate a strong correlation between the output step and the target task, while red markers signify deviations from the target task. The results indicate that when using our method for evaluation, the response of LLM-based embodied model closely aligns with the set attack goals. In contrast, under the other two evaluation methods, the output task content deviates from the intended goals. This underscores the effectiveness of our evaluation method in reducing the occurrences of false detections and missed detections observed in GCG
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zou et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2023
       </a>
       )
      </cite>
      and AutoDAN
      <cite class="ltx_cite ltx_citemacro_citep">
       (Liu et al
       <span class="ltx_text">
        .
       </span>
       ,
       <a class="ltx_ref" href="#bib.bib18" title="">
        2023
       </a>
       )
      </cite>
      , thus enhancing the accuracy of attack results.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T3">
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_table">
       Table 3.
      </span>
      Responce under three Judgment methods.
     </figcaption>
     <div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1" style="width:390.3pt;height:244.2pt;vertical-align:-0.0pt;">
      <span class="ltx_transformed_inner" style="transform:translate(5.3pt,-3.3pt) scale(1.02773717406749,1.02773717406749) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S4.T3.1.1.1.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2" id="S4.T3.1.1.1.1.1">
           <span class="ltx_text" id="S4.T3.1.1.1.1.1.1">
            <span class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.1.1.1.1.1">
             <span class="ltx_tr" id="S4.T3.1.1.1.1.1.1.1.1">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.1.1.1.1.1.1.1">
               prompt : Please pass me the chair.
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.1.1.1.1.1.2">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.1.1.1.1.1.2.1">
               target :
               <span class="ltx_text" id="S4.T3.1.1.1.1.1.1.1.2.1.1" style="color:#FF00FF;">
                Open the toaster and place it on the table
               </span>
              </span>
             </span>
            </span>
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S4.T3.1.1.2.1">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.2.1.1">
           Judgment-method
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.2">
           Response
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.3.2">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.3.2.1">
           <span class="ltx_text" id="S4.T3.1.1.3.2.1.1">
            GCG
           </span>
          </th>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.2.2">
           <span class="ltx_text" id="S4.T3.1.1.3.2.2.1">
            <span class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.3.2.2.1.1">
             <span class="ltx_tr" id="S4.T3.1.1.3.2.2.1.1.1">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.2.2.1.1.1.1">
               Step 1:
               <span class="ltx_text" id="S4.T3.1.1.3.2.2.1.1.1.1.1" style="color:#00FF00;">
                Open the toaster
               </span>
               directory and find the toast.
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.3.2.2.1.1.2">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.2.2.1.1.2.1">
               Step 2: Click on the toast to open it.
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.3.2.2.1.1.3">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.2.2.1.1.3.1">
               Step 3:
               <span class="ltx_text" id="S4.T3.1.1.3.2.2.1.1.3.1.1" style="color:#FF0000;">
                Inside the toast.
               </span>
              </span>
             </span>
            </span>
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.4.3">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.4.3.1">
           <span class="ltx_text" id="S4.T3.1.1.4.3.1.1">
            Only calculate similarity
           </span>
          </th>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.3.2">
           <span class="ltx_text" id="S4.T3.1.1.4.3.2.1">
            <span class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.4.3.2.1.1">
             <span class="ltx_tr" id="S4.T3.1.1.4.3.2.1.1.1">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.3.2.1.1.1.1">
               Step 1:
               <span class="ltx_text" id="S4.T3.1.1.4.3.2.1.1.1.1.1" style="color:#00FF00;">
                Open the toaster
               </span>
               and
               <span class="ltx_text" id="S4.T3.1.1.4.3.2.1.1.1.1.2" style="color:#FF0000;">
                insert the bread.
               </span>
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.4.3.2.1.1.2">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.3.2.1.1.2.1">
               Step 2: Close the toaster and
               <span class="ltx_text" id="S4.T3.1.1.4.3.2.1.1.2.1.1" style="color:#FF0000;">
                press the button.
               </span>
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.4.3.2.1.1.3">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.3.2.1.1.3.1">
               Step 3:
               <span class="ltx_text" id="S4.T3.1.1.4.3.2.1.1.3.1.1" style="color:#FF0000;">
                Wait for the toast.
               </span>
              </span>
             </span>
            </span>
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.5.4">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.1.5.4.1">
           <span class="ltx_text" id="S4.T3.1.1.5.4.1.1">
            Ours
           </span>
          </th>
          <td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T3.1.1.5.4.2">
           <span class="ltx_text" id="S4.T3.1.1.5.4.2.1">
            <span class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.5.4.2.1.1">
             <span class="ltx_tr" id="S4.T3.1.1.5.4.2.1.1.1">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.4.2.1.1.1.1">
               Step 1:
               <span class="ltx_text" id="S4.T3.1.1.5.4.2.1.1.1.1.1" style="color:#00FF00;">
                Open the toaster.
               </span>
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.5.4.2.1.1.2">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.4.2.1.1.2.1">
               Step 2:
               <span class="ltx_text" id="S4.T3.1.1.5.4.2.1.1.2.1.1" style="color:#00FF00;">
                Place the toast on the table.
               </span>
              </span>
             </span>
             <span class="ltx_tr" id="S4.T3.1.1.5.4.2.1.1.3">
              <span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.4.2.1.1.3.1">
               Step 3: Close the toaster.
              </span>
             </span>
            </span>
           </span>
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5.
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we introduce the creation of a multi-modal dataset named EIRAD, which aims to assess the robustness of LLM decision-level in embodied scenarios. We devise two attack strategies, untargeted attack and targeted attack, to simulate diverse attack scenarios. Throughout these attacks, we implement prompt suffix keyword initialization tailored to specific target tasks, enhancing the convergence speed during the attack process. Additionally, we develop an attack success assessment method based on BLIP2 model to provide a more precise evaluation of the conditions for attack success. The experimental outcomes validate the effectiveness of our approach, while also underscoring the challenge of robustness in LLM decision-level within embodied scenarios. We aspire that our study will shed further light on the vulnerabilities of LLMs in embodied settings and furnish them with advanced defense mechanisms for ensuring secure utilizatio.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     (1)
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Aizawa (2003)
    </span>
    <span class="ltx_bibblock">
     Akiko Aizawa. 2003.
    </span>
    <span class="ltx_bibblock">
     An information-theoretic perspective of tf–idf measures.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Information Processing &amp; Management
     </em>
     39, 1 (2003), 45–65.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Awadalla et al
     <span class="ltx_text" id="bib.bib3.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al
     <span class="ltx_text" id="bib.bib3.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Openflamingo: An open-source framework for training large autoregressive vision-language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.1">
      arXiv preprint arXiv:2308.01390
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brohan et al
     <span class="ltx_text" id="bib.bib4.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al
     <span class="ltx_text" id="bib.bib4.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Do as i can, not as i say: Grounding language in robotic affordances. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.1">
      Conference on robot learning
     </em>
     . PMLR, 287–318.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Carlini et al
     <span class="ltx_text" id="bib.bib5.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, and Ludwig Schmidt. 2023.
    </span>
    <span class="ltx_bibblock">
     Are aligned neural networks adversarially aligned?
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2306.15447 [cs.CL]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chao et al
     <span class="ltx_text" id="bib.bib6.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. 2023.
    </span>
    <span class="ltx_bibblock">
     Jailbreaking Black Box Large Language Models in Twenty Queries.
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2310.08419 [cs.LG]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dagan et al
     <span class="ltx_text" id="bib.bib7.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Gautier Dagan, Frank Keller, and Alex Lascarides. 2023.
    </span>
    <span class="ltx_bibblock">
     Dynamic planning with a llm.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">
      arXiv preprint arXiv:2308.06391
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ding et al
     <span class="ltx_text" id="bib.bib8.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang. 2023.
    </span>
    <span class="ltx_bibblock">
     A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">
      arXiv preprint arXiv:2311.08268
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dong et al
     <span class="ltx_text" id="bib.bib9.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, and Jun Zhu. 2023.
    </span>
    <span class="ltx_bibblock">
     How Robust is Google’s Bard to Adversarial Image Attacks?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">
      arXiv preprint arXiv:2309.11751
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dorbala et al
     <span class="ltx_text" id="bib.bib10.2.2.1">
      .
     </span>
     (2024)
    </span>
    <span class="ltx_bibblock">
     Vishnu Sashank Dorbala, Sanjoy Chowdhury, and Dinesh Manocha. 2024.
    </span>
    <span class="ltx_bibblock">
     Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">
      arXiv preprint arXiv:2403.11487
     </em>
     (2024).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dorbala et al
     <span class="ltx_text" id="bib.bib11.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Vishnu Sashank Dorbala, James F Mullen Jr, and Dinesh Manocha. 2023.
    </span>
    <span class="ltx_bibblock">
     Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">
      IEEE Robotics and Automation Letters
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al
     <span class="ltx_text" id="bib.bib12.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al
     <span class="ltx_text" id="bib.bib12.3.1">
      .
     </span>
     2022.
    </span>
    <span class="ltx_bibblock">
     Inner monologue: Embodied reasoning through planning with language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">
      arXiv preprint arXiv:2207.05608
     </em>
     (2022).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al
     <span class="ltx_text" id="bib.bib13.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023.
    </span>
    <span class="ltx_bibblock">
     Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation.
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2310.06987 [cs.CL]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kolve et al
     <span class="ltx_text" id="bib.bib14.2.2.1">
      .
     </span>
     (2017)
    </span>
    <span class="ltx_bibblock">
     Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al
     <span class="ltx_text" id="bib.bib14.3.1">
      .
     </span>
     2017.
    </span>
    <span class="ltx_bibblock">
     Ai2-thor: An interactive 3d environment for visual ai.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">
      arXiv preprint arXiv:1712.05474
     </em>
     (2017).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib15.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023a.
    </span>
    <span class="ltx_bibblock">
     Mimic-it: Multi-modal in-context instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">
      arXiv preprint arXiv:2306.05425
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib16.2.2.1">
      .
     </span>
     (2024)
    </span>
    <span class="ltx_bibblock">
     Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2024.
    </span>
    <span class="ltx_bibblock">
     Llava-med: Training a large language-and-vision assistant for biomedicine in one day.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">
      Advances in Neural Information Processing Systems
     </em>
     36 (2024).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al
     <span class="ltx_text" id="bib.bib17.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. 2023b.
    </span>
    <span class="ltx_bibblock">
     Deepinception: Hypnotize large language model to be jailbreaker.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">
      arXiv preprint arXiv:2311.03191
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al
     <span class="ltx_text" id="bib.bib18.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023.
    </span>
    <span class="ltx_bibblock">
     Autodan: Generating stealthy jailbreak prompts on aligned large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">
      arXiv preprint arXiv:2310.04451
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lynch et al
     <span class="ltx_text" id="bib.bib19.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. 2023.
    </span>
    <span class="ltx_bibblock">
     Interactive language: Talking to robots in real time.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">
      IEEE Robotics and Automation Letters
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nottingham et al
     <span class="ltx_text" id="bib.bib20.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. 2023.
    </span>
    <span class="ltx_bibblock">
     Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">
      International Conference on Machine Learning
     </em>
     . PMLR, 26311–26325.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qi et al
     <span class="ltx_text" id="bib.bib21.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.
    </span>
    <span class="ltx_bibblock">
     Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2310.03693 [cs.CL]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qiao et al
     <span class="ltx_text" id="bib.bib22.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu. 2023.
    </span>
    <span class="ltx_bibblock">
     March in chat: Interactive prompting for remote embodied referring expression. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     . 15758–15767.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schumann et al
     <span class="ltx_text" id="bib.bib23.2.2.1">
      .
     </span>
     (2024)
    </span>
    <span class="ltx_bibblock">
     Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and William Yang Wang. 2024.
    </span>
    <span class="ltx_bibblock">
     Velma: Verbalization embodiment of llm agents for vision and language navigation in street view. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , Vol. 38. 18924–18933.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shah et al
     <span class="ltx_text" id="bib.bib24.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando. 2023.
    </span>
    <span class="ltx_bibblock">
     Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation.
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2311.03348 [cs.CL]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sharan et al
     <span class="ltx_text" id="bib.bib25.2.2.1">
      .
     </span>
     (2024)
    </span>
    <span class="ltx_bibblock">
     SP Sharan, Ruihan Zhao, Zhangyang Wang, Sandeep P Chinchali, et al
     <span class="ltx_text" id="bib.bib25.3.1">
      .
     </span>
     2024.
    </span>
    <span class="ltx_bibblock">
     Plan Diffuser: Grounding LLM Planners with Diffusion Models for Robotic Manipulation. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.4.1">
      Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Song et al
     <span class="ltx_text" id="bib.bib26.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023.
    </span>
    <span class="ltx_bibblock">
     Llm-planner: Few-shot grounded planning for embodied agents with large language models. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     . 2998–3009.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stella et al
     <span class="ltx_text" id="bib.bib27.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Francesco Stella, Cosimo Della Santina, and Josie Hughes. 2023.
    </span>
    <span class="ltx_bibblock">
     How can LLMs transform the robotic design process?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">
      Nature Machine Intelligence
     </em>
     5, 6 (2023), 561–564.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al
     <span class="ltx_text" id="bib.bib28.2.2.1">
      .
     </span>
     (2022)
    </span>
    <span class="ltx_bibblock">
     Jiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou, and Animesh Garg. 2022.
    </span>
    <span class="ltx_bibblock">
     Plate: Visually-grounded planning with transformers in procedural tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">
      IEEE Robotics and Automation Letters
     </em>
     7, 2 (2022), 4924–4930.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al
     <span class="ltx_text" id="bib.bib29.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, et al
     <span class="ltx_text" id="bib.bib29.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Pathasst: Redefining pathology through generative foundation ai assistant for pathology.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.4.1">
      arXiv preprint arXiv:2305.15072
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Szot et al
     <span class="ltx_text" id="bib.bib30.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, R Devon Hjelm, and Alexander T Toshev. 2023.
    </span>
    <span class="ltx_bibblock">
     Large language models as generalizable policies for embodied tasks. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Unanue et al
     <span class="ltx_text" id="bib.bib31.2.2.1">
      .
     </span>
     (2021)
    </span>
    <span class="ltx_bibblock">
     Inigo Jauregi Unanue, Jacob Parnell, and Massimo Piccardi. 2021.
    </span>
    <span class="ltx_bibblock">
     BERTTune: Fine-tuning neural machine translation with BERTScore.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">
      arXiv preprint arXiv:2106.02208
     </em>
     (2021).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vemprala et al
     <span class="ltx_text" id="bib.bib32.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. 2023.
    </span>
    <span class="ltx_bibblock">
     Chatgpt for robotics: Design principles and model abilities.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">
      arXiv preprint arXiv:2306.17582
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al
     <span class="ltx_text" id="bib.bib33.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023.
    </span>
    <span class="ltx_bibblock">
     Jailbroken: How Does LLM Safety Training Fail?
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2307.02483 [cs.LG]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al
     <span class="ltx_text" id="bib.bib34.2.2.1">
      .
     </span>
     (2024)
    </span>
    <span class="ltx_bibblock">
     Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024.
    </span>
    <span class="ltx_bibblock">
     Jailbroken: How does llm safety training fail?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">
      Advances in Neural Information Processing Systems
     </em>
     36 (2024).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al
     <span class="ltx_text" id="bib.bib35.2.2.1">
      .
     </span>
     (2023a)
    </span>
    <span class="ltx_bibblock">
     Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023a.
    </span>
    <span class="ltx_bibblock">
     Tidybot: Personalized robot assistance with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">
      Autonomous Robots
     </em>
     47, 8 (2023), 1087–1102.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al
     <span class="ltx_text" id="bib.bib36.2.2.1">
      .
     </span>
     (2010)
    </span>
    <span class="ltx_bibblock">
     Lei Wu, Steven CH Hoi, and Nenghai Yu. 2010.
    </span>
    <span class="ltx_bibblock">
     Semantics-preserving bag-of-words models and applications.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">
      IEEE Transactions on Image Processing
     </em>
     19, 7 (2010), 1908–1920.
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al
     <span class="ltx_text" id="bib.bib37.2.2.1">
      .
     </span>
     (2023b)
    </span>
    <span class="ltx_bibblock">
     Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. 2023b.
    </span>
    <span class="ltx_bibblock">
     Embodied task planning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">
      arXiv preprint arXiv:2307.01848
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al
     <span class="ltx_text" id="bib.bib38.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. 2023.
    </span>
    <span class="ltx_bibblock">
     Embodied multi-modal agent trained by an llm from a parallel textworld.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">
      arXiv preprint arXiv:2311.16714
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al
     <span class="ltx_text" id="bib.bib39.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. 2023.
    </span>
    <span class="ltx_bibblock">
     GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.
    </span>
    <span class="ltx_bibblock">
    </span>
    <span class="ltx_bibblock">
     arXiv:2309.10253 [cs.AI]
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al
     <span class="ltx_text" id="bib.bib40.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. 2023.
    </span>
    <span class="ltx_bibblock">
     Building cooperative embodied agents modularly with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">
      arXiv preprint arXiv:2307.02485
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al
     <span class="ltx_text" id="bib.bib41.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, and Dinggang Shen. 2023.
    </span>
    <span class="ltx_bibblock">
     Chatcad+: Towards a universal and reliable interactive cad using llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">
      arXiv preprint arXiv:2305.15964
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al
     <span class="ltx_text" id="bib.bib42.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Sipeng Zheng, Yicheng Feng, Zongqing Lu, et al
     <span class="ltx_text" id="bib.bib42.3.1">
      .
     </span>
     2023.
    </span>
    <span class="ltx_bibblock">
     Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds. In
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.4.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zou et al
     <span class="ltx_text" id="bib.bib43.2.2.1">
      .
     </span>
     (2023)
    </span>
    <span class="ltx_bibblock">
     Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.
    </span>
    <span class="ltx_bibblock">
     Universal and transferable adversarial attacks on aligned language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">
      arXiv preprint arXiv:2307.15043
     </em>
     (2023).
    </span>
    <span class="ltx_bibblock">
    </span>
   </li>
  </ul>
 </section>
</article>
