<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.00055] Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors</title><meta property="og:description" content="RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of tâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.00055">

<!--Generated on Wed Mar  6 10:23:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
radar,  micro-Doppler,  sign language,  ASL,  adversarial learning,  kinematics
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">M.M. Rahman, , E. Malaia, A.C. Gurbuz, , D.J. Griffin, C. Crawford and S.Z. Gurbuz
</span><span class="ltx_author_notes">This work was supported in part by the National Science Foundation under Grants 1932547, 1931861 and 1734938. Human studies research was conducted under University of Alabama Institutional Review Board (IRB) Protocol #18-06-1271. M.M. Rahman and S.Z. Gurbuz are with the University of Alabama, Department of Electrical and Computer Engineering, Tuscaloosa, AL 35487 (email: mrahman17@crimson.ua.edu, szgurbuz@ua.edu).E. Malaia is with the University of Alabama, Department of Communication Disorders, Tuscaloosa, AL 35487 (e-mail: eamalaia@ua.edu).A.C. Gurbuz is with Mississippi State University, Department of Electrical and Computer Engineering (e-mail: gurbuz@ece.msstate.edu).D.J. Griffin is with the University of Alabama, Department of Communication Studies, Tuscaloosa, AL 35487 (e-mail: djgriffin1@ua.edu).C. Crawford is with the University of Alabama, Department of Computer Science, Tuscaloosa, AL 35487 (e-mail: crawford@ua.edu).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
radar, micro-Doppler, sign language, ASL, adversarial learning, kinematics

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">According to the World Federation for the Deaf (WFD), an estimated 74 million people world-wide communicate using sign language. American Sign Language (ASL) is estimated to be the primary mode of communication for over a million people in North America and Canada, based on statistics provided by Gallaudet University (the worldâ€™s only university designed to be barrier-free for deaf and hard of hearing students located in Washington, D.C.). Although much research in ASL recognition has focused on translation (e.g. sign to speech) as a means to bridge the communication gap between Deaf and hearing individuals, this work aims at the development of RF sensing-enabled sign language processing (SLP) technologies in an ambient, non-invasive fashion for human-computer interaction (HCI) applications, such as assistive robots <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and smart environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. RF sensors have been successfully used for remote health monitoring of vital signs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, fall detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, gait analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and detection of sleep apnea <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or sudden infant death syndrome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The addition of ASL recognition capability to such systems would extend their use potential to Deaf populations, and enhance the quality of life for those who use ASL.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">RF sensors offer unique advantages in that they are non-contact, not restrictive or invasive, operate at a distance, protect the privacy of the user and personal spaces, and are effective in the dark, regardless of what the individual is wearing. Thus, RF sensors can recognize signing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> in situations where other sensors, such as wearables <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> or cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, are either undesireable or ineffective. RF sensors cannot perceive hand shapes or facial expressions, but they can provide a direct measure of distance and velocity as a function of time. Velocity can be obtained via the Doppler effect; namely, the principle that the frequency shift incurred in the received RF signal is proportional to the radial velocity of an object in motion. While translational motion results in a central Doppler frequency, <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">micro-Doppler</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> refers to the frequency modulations generated about the center frequency that result from vibrations or rotations. As such, the micro-Doppler signature is comprised of unique patterns directly related to the kinematics of the underlying motion, and can serve as a biometric to identify individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, various activities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and gestures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Deep learning has enabled great advances in the recognition capabilities for many sensing modalities, including RF sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. However, deep neural networks (DNNs) require large amounts of data to learn complex underlying data representations. In biomedical applications for human motion recognition, acquisition of adequate sample sizes can be challenging; targeted populations may be mobility-impaired or reluctant to participate. Given that the Deaf are a minority community, its members are highly sought after for involvement in a variety of research studies, and thus may be wary of frequent requests. Inclusion of Deaf researchers is critical for hearing researchers to understand Deaf cultural perspectives and incorporate Deaf experiences and knowledge of the language into all aspects of SLP technology design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Our research team includes a Child-of-Deaf-Adults (CODA), who is fluent in ASL, and benefits from the involvement of community partnerships with Gallaudet University and the Alabama Institute for the Deaf and Blind (AIDB), who have provided feedback on Deaf-centric design and aided in the recruitment of Deaf participants.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Nevertheless, reliance on extensive amounts of human subject data for training deep models can result in an undue burden on Deaf participants, even if well-intentioned and for the benefit of the community. One way some researchers have addressed the need for signing data has been to utilize ASL learners or imitation signers, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, who are more easily recruitable. Imitation signing refers to the process of asking hearing sign-naive participants to replicate the signs shown in any video. However, it can take learners of sign language at least 3 years to produce signs in a manner that is perceived as fluent by fluent signers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Thus, even with â€trainingâ€ sessions to teach participants how to articulate the signs prior to conducting the experiment with imitation signing videos, the production of hearing imitation signers is not comparable to that of fluent signing and may indeed contain significant errors in temporal dynamics and repetitions (which RF sensors easily perceive), as well as hand shape and place of articulation, i.e. position of the sign in space. In our prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we found that imitation signing and fluent signing occupy different regions in the feature space, enabling machine learning to
effectively distinguish between imitation signing and fluent signing.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">But does this mean that imitation signing data has no value in the training of DNNs for fluent ASL recognition? Adversarial domain adaptation is an approach that has been utilized in the computer vision community for style transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and image-to-image translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. One approach to the design of generative adversarial networks (GANs) for domain adaptation is to use Pix2Pix GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> for image-to-image translation based on the conditional GAN, where a target image is generated, conditional on a given input image. In this case, the Pix2Pix GAN changes the loss function so that the generated image is both plausible in the content of the target domain, and is a plausible translation of the input image. Another approach is to minimize the <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">cycle-consistency loss</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which enforces two mappings to be the reverse of each other: <math id="S1.p5.1.m1.2" class="ltx_Math" alttext="F(G(x))=x" display="inline"><semantics id="S1.p5.1.m1.2a"><mrow id="S1.p5.1.m1.2.2" xref="S1.p5.1.m1.2.2.cmml"><mrow id="S1.p5.1.m1.2.2.1" xref="S1.p5.1.m1.2.2.1.cmml"><mi id="S1.p5.1.m1.2.2.1.3" xref="S1.p5.1.m1.2.2.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.p5.1.m1.2.2.1.2" xref="S1.p5.1.m1.2.2.1.2.cmml">â€‹</mo><mrow id="S1.p5.1.m1.2.2.1.1.1" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S1.p5.1.m1.2.2.1.1.1.2" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S1.p5.1.m1.2.2.1.1.1.1" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml"><mi id="S1.p5.1.m1.2.2.1.1.1.1.2" xref="S1.p5.1.m1.2.2.1.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S1.p5.1.m1.2.2.1.1.1.1.1" xref="S1.p5.1.m1.2.2.1.1.1.1.1.cmml">â€‹</mo><mrow id="S1.p5.1.m1.2.2.1.1.1.1.3.2" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S1.p5.1.m1.2.2.1.1.1.1.3.2.1" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml">(</mo><mi id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S1.p5.1.m1.2.2.1.1.1.1.3.2.2" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S1.p5.1.m1.2.2.1.1.1.3" xref="S1.p5.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S1.p5.1.m1.2.2.2" xref="S1.p5.1.m1.2.2.2.cmml">=</mo><mi id="S1.p5.1.m1.2.2.3" xref="S1.p5.1.m1.2.2.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.2b"><apply id="S1.p5.1.m1.2.2.cmml" xref="S1.p5.1.m1.2.2"><eq id="S1.p5.1.m1.2.2.2.cmml" xref="S1.p5.1.m1.2.2.2"></eq><apply id="S1.p5.1.m1.2.2.1.cmml" xref="S1.p5.1.m1.2.2.1"><times id="S1.p5.1.m1.2.2.1.2.cmml" xref="S1.p5.1.m1.2.2.1.2"></times><ci id="S1.p5.1.m1.2.2.1.3.cmml" xref="S1.p5.1.m1.2.2.1.3">ğ¹</ci><apply id="S1.p5.1.m1.2.2.1.1.1.1.cmml" xref="S1.p5.1.m1.2.2.1.1.1"><times id="S1.p5.1.m1.2.2.1.1.1.1.1.cmml" xref="S1.p5.1.m1.2.2.1.1.1.1.1"></times><ci id="S1.p5.1.m1.2.2.1.1.1.1.2.cmml" xref="S1.p5.1.m1.2.2.1.1.1.1.2">ğº</ci><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">ğ‘¥</ci></apply></apply><ci id="S1.p5.1.m1.2.2.3.cmml" xref="S1.p5.1.m1.2.2.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.2c">F(G(x))=x</annotation></semantics></math>. As an alternative to the cycle-consistency proposed with CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, TravelGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> has also been proposed, which instead utilizes an additional Siamese network to guide the generator in generating shared semantics, and thus learn mappings between more complex domains that have large differences beyond that of just style or texture. Thus, one approach to training deep models for ASL recognition could be to utilize adversarial domain adaptation to transform the imitation signing signatures to have greater resemblance to fluent signing signatures.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">A significant challenge to the application of adversarial learning to RF datasets, however, is that the data supplied to the DNNs are not optical images, but computed images, derived from time-frequency transform of the raw complex received RF signal. Thus, the pixels in an RF micro-Doppler signature bear no relation to geometry, lighting, or perspective. In contrast, it is the kinematics of the human skeleton that determines the frequency profiles revealed in the RF signature. In prior work, we have shown that consideration of kinematics can lead to great gains in DNN training for human activity recognition:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Data augmentation via temporal and spatial scaling of the underlying skeletal animation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> yields much more effective, diversified and statistically independent samples than image-based data augmentation techniques, which can corrupt the signatures and result in physically impossible samples.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The classification accuracy for eight different daily human activities was boosted by 10% simply by discarding 9,000 kinematically impossible samples, which were identified as outliers relative to real data samples using Principal Component Analysis (PCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, from a synthetic dataset of 40,000 samples generated using an Auxilliary Conditional GAN (ACGAN).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Use of the signature envelope in addition to the signature itself in a multi-branch GAN (MBGAN) architecture was shown <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> to shift the distribution of synthetic human activity data so as to have greater overlap with that of real data, as visualized using t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Studies of gross motor motion recognition showed that MBGAN offered improved classification accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
</li>
</ol>
<p id="S1.p6.2" class="ltx_p">Thus, an alternative approach to the transformation of imitation signing signatures could be to directly synthesize ASL signatures for training using adversarial learning.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this work, we investigate the kinematic properties of sign production by fluent signers versus hearing imitation signers using RF sensors, as well as the impact of fluency and sign kinematics (i.e. components of sign phonology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>) on training DNNs for classification of fluent signing using synthetic signatures that are (a) transformed from imitation signing data, versus (b) directly generated from a small set of real signatures from fluent signers. In Section II, the experimental procedure for acquiring the 100-sign ASL datasets for imitation signers and fluent ASL users is presented. The kinematic and linguistic properties of these signs, as listed in the ASL-LEX2 database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, are described. In Section III, techniques for estimating these properties from the RF micro-Doppler signature for each sign are presented. In Section IV, the adversarial networks for domain adaptation and design of a 3-branch MBGAN for ASL training data synthesis are detailed. In particular, using the estimators developed in Section III, the degree to which different adversarial networks preserve the salient properties of each sign are quantitatively evaluated and the advantages of embedding kinematics into the GAN architecture are demonstrated. The similarity of transformed imitation signing and synthesized ASL is compared with that of fluent ASL signatures for different database sizes. In Section IV, kinematically deviant signatures are sifted out from the generated data, and the resulting synthetic datasets are used to train DNNs. In this way, the efficacy of the proposed approaches to classyfing a large number of ASL signs while minimizing real human subject data requirements is demonstrated. The paper concludes in Section V with a discussion of results and future research directions.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Listing of the 100 ASL signs utilized in experiments.</figcaption><img src="/html/2201.00055/assets/100_Asl_words.png" id="S1.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="695" height="314" alt="[Uncaptioned image]">
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Experimental RF Datasets</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">RF Sensors and Test Environment</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.7" class="ltx_p">The RF sensor used in this work is a TI IWR1443BOOST 77 GHz - 81 GHz automotive short-range radar (SRR) sensor, which transmits a pulsed, linear frequency modulated continuous wave (FMCW) signal (a.k.a â€chirpâ€ signal). The normalized transmitted signal of the FMCW
radar is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="x_{tr}(t)=exp\bigg{\{}j2\pi(f_{c}t+\frac{k}{2}t^{2})\bigg{\}}" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml"><msub id="S2.E1.m1.2.2.3.2" xref="S2.E1.m1.2.2.3.2.cmml"><mi id="S2.E1.m1.2.2.3.2.2" xref="S2.E1.m1.2.2.3.2.2.cmml">x</mi><mrow id="S2.E1.m1.2.2.3.2.3" xref="S2.E1.m1.2.2.3.2.3.cmml"><mi id="S2.E1.m1.2.2.3.2.3.2" xref="S2.E1.m1.2.2.3.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.3.2.3.1" xref="S2.E1.m1.2.2.3.2.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.3.2.3.3" xref="S2.E1.m1.2.2.3.2.3.3.cmml">r</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.3.1" xref="S2.E1.m1.2.2.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.3.3.2" xref="S2.E1.m1.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.3.3.2.1" xref="S2.E1.m1.2.2.3.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S2.E1.m1.2.2.3.3.2.2" xref="S2.E1.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">=</mo><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml"><mi id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.2.2.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.1.4" xref="S2.E1.m1.2.2.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.2a" xref="S2.E1.m1.2.2.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.1.5" xref="S2.E1.m1.2.2.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.2b" xref="S2.E1.m1.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.2.cmml"><mo maxsize="210%" minsize="210%" id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.2.cmml">{</mo><mrow id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mn id="S2.E1.m1.2.2.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.4.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.2a" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.1.1.1.1.5" xref="S2.E1.m1.2.2.1.1.1.1.5.cmml">Ï€</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.2b" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml">f</mi><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">t</mi></mrow><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mfrac id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml">k</mi><mn id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><msup id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mn id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">2</mn></msup></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="210%" minsize="210%" id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.2.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></eq><apply id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"><times id="S2.E1.m1.2.2.3.1.cmml" xref="S2.E1.m1.2.2.3.1"></times><apply id="S2.E1.m1.2.2.3.2.cmml" xref="S2.E1.m1.2.2.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.3.2.1.cmml" xref="S2.E1.m1.2.2.3.2">subscript</csymbol><ci id="S2.E1.m1.2.2.3.2.2.cmml" xref="S2.E1.m1.2.2.3.2.2">ğ‘¥</ci><apply id="S2.E1.m1.2.2.3.2.3.cmml" xref="S2.E1.m1.2.2.3.2.3"><times id="S2.E1.m1.2.2.3.2.3.1.cmml" xref="S2.E1.m1.2.2.3.2.3.1"></times><ci id="S2.E1.m1.2.2.3.2.3.2.cmml" xref="S2.E1.m1.2.2.3.2.3.2">ğ‘¡</ci><ci id="S2.E1.m1.2.2.3.2.3.3.cmml" xref="S2.E1.m1.2.2.3.2.3.3">ğ‘Ÿ</ci></apply></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘¡</ci></apply><apply id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1"><times id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1.2"></times><ci id="S2.E1.m1.2.2.1.3.cmml" xref="S2.E1.m1.2.2.1.3">ğ‘’</ci><ci id="S2.E1.m1.2.2.1.4.cmml" xref="S2.E1.m1.2.2.1.4">ğ‘¥</ci><ci id="S2.E1.m1.2.2.1.5.cmml" xref="S2.E1.m1.2.2.1.5">ğ‘</ci><set id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1"><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2"></times><ci id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3">ğ‘—</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.4">2</cn><ci id="S2.E1.m1.2.2.1.1.1.1.5.cmml" xref="S2.E1.m1.2.2.1.1.1.1.5">ğœ‹</ci><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1"><plus id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1"></plus><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.1"></times><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2">ğ‘“</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.3">ğ‘</ci></apply><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3">ğ‘¡</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.1"></times><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2"><divide id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2"></divide><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2">ğ‘˜</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3">2</cn></apply><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3">superscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2">ğ‘¡</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3">2</cn></apply></apply></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">x_{tr}(t)=exp\bigg{\{}j2\pi(f_{c}t+\frac{k}{2}t^{2})\bigg{\}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.6" class="ltx_p">where <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">t</annotation></semantics></math> denotes the fast time within a chirp (a frequency
modulation period), <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="T_{s}/2\leq t\leq T_{s}/2" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mrow id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><msub id="S2.SS1.p1.2.m2.1.1.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.2.cmml">T</mi><mi id="S2.SS1.p1.2.m2.1.1.2.2.3" xref="S2.SS1.p1.2.m2.1.1.2.2.3.cmml">s</mi></msub><mo id="S2.SS1.p1.2.m2.1.1.2.1" xref="S2.SS1.p1.2.m2.1.1.2.1.cmml">/</mo><mn id="S2.SS1.p1.2.m2.1.1.2.3" xref="S2.SS1.p1.2.m2.1.1.2.3.cmml">2</mn></mrow><mo id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">â‰¤</mo><mi id="S2.SS1.p1.2.m2.1.1.4" xref="S2.SS1.p1.2.m2.1.1.4.cmml">t</mi><mo id="S2.SS1.p1.2.m2.1.1.5" xref="S2.SS1.p1.2.m2.1.1.5.cmml">â‰¤</mo><mrow id="S2.SS1.p1.2.m2.1.1.6" xref="S2.SS1.p1.2.m2.1.1.6.cmml"><msub id="S2.SS1.p1.2.m2.1.1.6.2" xref="S2.SS1.p1.2.m2.1.1.6.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.6.2.2" xref="S2.SS1.p1.2.m2.1.1.6.2.2.cmml">T</mi><mi id="S2.SS1.p1.2.m2.1.1.6.2.3" xref="S2.SS1.p1.2.m2.1.1.6.2.3.cmml">s</mi></msub><mo id="S2.SS1.p1.2.m2.1.1.6.1" xref="S2.SS1.p1.2.m2.1.1.6.1.cmml">/</mo><mn id="S2.SS1.p1.2.m2.1.1.6.3" xref="S2.SS1.p1.2.m2.1.1.6.3.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><and id="S2.SS1.p1.2.m2.1.1a.cmml" xref="S2.SS1.p1.2.m2.1.1"></and><apply id="S2.SS1.p1.2.m2.1.1b.cmml" xref="S2.SS1.p1.2.m2.1.1"><leq id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"></leq><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><divide id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2.1"></divide><apply id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.2.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2.2">ğ‘‡</ci><ci id="S2.SS1.p1.2.m2.1.1.2.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2.3">ğ‘ </ci></apply><cn type="integer" id="S2.SS1.p1.2.m2.1.1.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.2.3">2</cn></apply><ci id="S2.SS1.p1.2.m2.1.1.4.cmml" xref="S2.SS1.p1.2.m2.1.1.4">ğ‘¡</ci></apply><apply id="S2.SS1.p1.2.m2.1.1c.cmml" xref="S2.SS1.p1.2.m2.1.1"><leq id="S2.SS1.p1.2.m2.1.1.5.cmml" xref="S2.SS1.p1.2.m2.1.1.5"></leq><share href="#S2.SS1.p1.2.m2.1.1.4.cmml" id="S2.SS1.p1.2.m2.1.1d.cmml" xref="S2.SS1.p1.2.m2.1.1"></share><apply id="S2.SS1.p1.2.m2.1.1.6.cmml" xref="S2.SS1.p1.2.m2.1.1.6"><divide id="S2.SS1.p1.2.m2.1.1.6.1.cmml" xref="S2.SS1.p1.2.m2.1.1.6.1"></divide><apply id="S2.SS1.p1.2.m2.1.1.6.2.cmml" xref="S2.SS1.p1.2.m2.1.1.6.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.6.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.6.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.6.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.6.2.2">ğ‘‡</ci><ci id="S2.SS1.p1.2.m2.1.1.6.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.6.2.3">ğ‘ </ci></apply><cn type="integer" id="S2.SS1.p1.2.m2.1.1.6.3.cmml" xref="S2.SS1.p1.2.m2.1.1.6.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">T_{s}/2\leq t\leq T_{s}/2</annotation></semantics></math>, <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="f_{c}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><msub id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">f</mi><mi id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">ğ‘“</ci><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">f_{c}</annotation></semantics></math> and <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="k=B/T_{s}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">k</mi><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml">B</mi><mo id="S2.SS1.p1.4.m4.1.1.3.1" xref="S2.SS1.p1.4.m4.1.1.3.1.cmml">/</mo><msub id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.3.2" xref="S2.SS1.p1.4.m4.1.1.3.3.2.cmml">T</mi><mi id="S2.SS1.p1.4.m4.1.1.3.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><eq id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></eq><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">ğ‘˜</ci><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><divide id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3.1"></divide><ci id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">ğµ</ci><apply id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.2">ğ‘‡</ci><ci id="S2.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.3">ğ‘ </ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">k=B/T_{s}</annotation></semantics></math> denote the center frequency and the frequency slope of the
chirp, and <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">B</annotation></semantics></math> and <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="T_{s}" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><msub id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">T</mi><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">ğ‘‡</ci><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">T_{s}</annotation></semantics></math> denote the bandwidth and the time
duration of the chirp, respectively.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The transmitted signal illuminates an ASL signer who sits 1.5 meters in front of the sensor and signs in ASL. The radar receives backscatter from the moving arms and hands, as well as reflection from static parts of the body and environment. According to geometric diffraction theory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, when
the wavelength of the incident wave is much smaller than the target size, the backscattered returns from the target can
be expressed as the superposition of a set of independent scattering centers. Thus, the signal received by the receiver is a weighted summation of time-delayed, frequency-shifted versions of the transmitted signal given by the the superposition of returns from <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">M</annotation></semantics></math> points on the body <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Thus,</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="x_{rec}(t)=\sum_{i=1}^{M}{a_{i}exp\Bigg{\{}-j\frac{4\pi f_{c}}{c}R_{t,i}\Bigg{\}}}," display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml"><msub id="S2.E2.m1.4.4.1.1.3.2" xref="S2.E2.m1.4.4.1.1.3.2.cmml"><mi id="S2.E2.m1.4.4.1.1.3.2.2" xref="S2.E2.m1.4.4.1.1.3.2.2.cmml">x</mi><mrow id="S2.E2.m1.4.4.1.1.3.2.3" xref="S2.E2.m1.4.4.1.1.3.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.3.2.3.2" xref="S2.E2.m1.4.4.1.1.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.2.3.1" xref="S2.E2.m1.4.4.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.3.2.3.3" xref="S2.E2.m1.4.4.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.2.3.1a" xref="S2.E2.m1.4.4.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.3.2.3.4" xref="S2.E2.m1.4.4.1.1.3.2.3.4.cmml">c</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.1" xref="S2.E2.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.4.4.1.1.3.3.2" xref="S2.E2.m1.4.4.1.1.3.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.3.2.1" xref="S2.E2.m1.4.4.1.1.3.cmml">(</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">t</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.3.2.2" xref="S2.E2.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.E2.m1.4.4.1.1.2" xref="S2.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml"><munderover id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E2.m1.4.4.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.E2.m1.4.4.1.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.2.2.3.2" xref="S2.E2.m1.4.4.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E2.m1.4.4.1.1.1.2.2.3.1" xref="S2.E2.m1.4.4.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.4.4.1.1.1.2.2.3.3" xref="S2.E2.m1.4.4.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.4.4.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.2.3.cmml">M</mi></munderover><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.3.2.cmml">a</mi><mi id="S2.E2.m1.4.4.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.2a" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.1.1.5" xref="S2.E2.m1.4.4.1.1.1.1.5.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.2b" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.1.1.6" xref="S2.E2.m1.4.4.1.1.1.1.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.2c" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><mo maxsize="260%" minsize="260%" id="S2.E2.m1.4.4.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">{</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1a" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mfrac id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.cmml"><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.1.cmml">â€‹</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.3.cmml">Ï€</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.1a" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.1.cmml">â€‹</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.2.cmml">f</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.3.cmml">c</mi></msub></mrow><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.3.cmml">c</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.1a" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.2.cmml">R</mi><mrow id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">t</mi><mo id="S2.E2.m1.2.2.2.4.1" xref="S2.E2.m1.2.2.2.3.cmml">,</mo><mi id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml">i</mi></mrow></msub></mrow></mrow><mo maxsize="260%" minsize="260%" id="S2.E2.m1.4.4.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1"><eq id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.2"></eq><apply id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3"><times id="S2.E2.m1.4.4.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.1"></times><apply id="S2.E2.m1.4.4.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2.2">ğ‘¥</ci><apply id="S2.E2.m1.4.4.1.1.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.3.2.3"><times id="S2.E2.m1.4.4.1.1.3.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.2.3.1"></times><ci id="S2.E2.m1.4.4.1.1.3.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2.3.2">ğ‘Ÿ</ci><ci id="S2.E2.m1.4.4.1.1.3.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.3.2.3.3">ğ‘’</ci><ci id="S2.E2.m1.4.4.1.1.3.2.3.4.cmml" xref="S2.E2.m1.4.4.1.1.3.2.3.4">ğ‘</ci></apply></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ğ‘¡</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><apply id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.4.4.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.4.4.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.4.4.1.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.3"><eq id="S2.E2.m1.4.4.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.4.4.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.1.2.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.4.4.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.3">ğ‘€</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2"></times><apply id="S2.E2.m1.4.4.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.3.2">ğ‘</ci><ci id="S2.E2.m1.4.4.1.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.3.3">ğ‘–</ci></apply><ci id="S2.E2.m1.4.4.1.1.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.4">ğ‘’</ci><ci id="S2.E2.m1.4.4.1.1.1.1.5.cmml" xref="S2.E2.m1.4.4.1.1.1.1.5">ğ‘¥</ci><ci id="S2.E2.m1.4.4.1.1.1.1.6.cmml" xref="S2.E2.m1.4.4.1.1.1.1.6">ğ‘</ci><set id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1"></minus><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2"><times id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.1"></times><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.2">ğ‘—</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3"><divide id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3"></divide><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2"><times id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.1"></times><cn type="integer" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.2">4</cn><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.3">ğœ‹</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.2">ğ‘“</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.2.4.3">ğ‘</ci></apply></apply><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.3.3">ğ‘</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.4.2">ğ‘…</ci><list id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.4"><ci id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1">ğ‘¡</ci><ci id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2">ğ‘–</ci></list></apply></apply></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">x_{rec}(t)=\sum_{i=1}^{M}{a_{i}exp\Bigg{\{}-j\frac{4\pi f_{c}}{c}R_{t,i}\Bigg{\}}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.8" class="ltx_p">where <math id="S2.SS1.p2.2.m1.2" class="ltx_Math" alttext="R_{t,i}" display="inline"><semantics id="S2.SS1.p2.2.m1.2a"><msub id="S2.SS1.p2.2.m1.2.3" xref="S2.SS1.p2.2.m1.2.3.cmml"><mi id="S2.SS1.p2.2.m1.2.3.2" xref="S2.SS1.p2.2.m1.2.3.2.cmml">R</mi><mrow id="S2.SS1.p2.2.m1.2.2.2.4" xref="S2.SS1.p2.2.m1.2.2.2.3.cmml"><mi id="S2.SS1.p2.2.m1.1.1.1.1" xref="S2.SS1.p2.2.m1.1.1.1.1.cmml">t</mi><mo id="S2.SS1.p2.2.m1.2.2.2.4.1" xref="S2.SS1.p2.2.m1.2.2.2.3.cmml">,</mo><mi id="S2.SS1.p2.2.m1.2.2.2.2" xref="S2.SS1.p2.2.m1.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m1.2b"><apply id="S2.SS1.p2.2.m1.2.3.cmml" xref="S2.SS1.p2.2.m1.2.3"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m1.2.3.1.cmml" xref="S2.SS1.p2.2.m1.2.3">subscript</csymbol><ci id="S2.SS1.p2.2.m1.2.3.2.cmml" xref="S2.SS1.p2.2.m1.2.3.2">ğ‘…</ci><list id="S2.SS1.p2.2.m1.2.2.2.3.cmml" xref="S2.SS1.p2.2.m1.2.2.2.4"><ci id="S2.SS1.p2.2.m1.1.1.1.1.cmml" xref="S2.SS1.p2.2.m1.1.1.1.1">ğ‘¡</ci><ci id="S2.SS1.p2.2.m1.2.2.2.2.cmml" xref="S2.SS1.p2.2.m1.2.2.2.2">ğ‘–</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m1.2c">R_{t,i}</annotation></semantics></math> is the range to the <math id="S2.SS1.p2.3.m2.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S2.SS1.p2.3.m2.1a"><msup id="S2.SS1.p2.3.m2.1.1" xref="S2.SS1.p2.3.m2.1.1.cmml"><mi id="S2.SS1.p2.3.m2.1.1.2" xref="S2.SS1.p2.3.m2.1.1.2.cmml">i</mi><mrow id="S2.SS1.p2.3.m2.1.1.3" xref="S2.SS1.p2.3.m2.1.1.3.cmml"><mi id="S2.SS1.p2.3.m2.1.1.3.2" xref="S2.SS1.p2.3.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.3.m2.1.1.3.1" xref="S2.SS1.p2.3.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p2.3.m2.1.1.3.3" xref="S2.SS1.p2.3.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m2.1b"><apply id="S2.SS1.p2.3.m2.1.1.cmml" xref="S2.SS1.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m2.1.1.1.cmml" xref="S2.SS1.p2.3.m2.1.1">superscript</csymbol><ci id="S2.SS1.p2.3.m2.1.1.2.cmml" xref="S2.SS1.p2.3.m2.1.1.2">ğ‘–</ci><apply id="S2.SS1.p2.3.m2.1.1.3.cmml" xref="S2.SS1.p2.3.m2.1.1.3"><times id="S2.SS1.p2.3.m2.1.1.3.1.cmml" xref="S2.SS1.p2.3.m2.1.1.3.1"></times><ci id="S2.SS1.p2.3.m2.1.1.3.2.cmml" xref="S2.SS1.p2.3.m2.1.1.3.2">ğ‘¡</ci><ci id="S2.SS1.p2.3.m2.1.1.3.3.cmml" xref="S2.SS1.p2.3.m2.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m2.1c">i^{th}</annotation></semantics></math> body part at time <math id="S2.SS1.p2.4.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p2.4.m3.1a"><mi id="S2.SS1.p2.4.m3.1.1" xref="S2.SS1.p2.4.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m3.1b"><ci id="S2.SS1.p2.4.m3.1.1.cmml" xref="S2.SS1.p2.4.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m3.1c">t</annotation></semantics></math>, <math id="S2.SS1.p2.5.m4.1" class="ltx_Math" alttext="f_{c}" display="inline"><semantics id="S2.SS1.p2.5.m4.1a"><msub id="S2.SS1.p2.5.m4.1.1" xref="S2.SS1.p2.5.m4.1.1.cmml"><mi id="S2.SS1.p2.5.m4.1.1.2" xref="S2.SS1.p2.5.m4.1.1.2.cmml">f</mi><mi id="S2.SS1.p2.5.m4.1.1.3" xref="S2.SS1.p2.5.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m4.1b"><apply id="S2.SS1.p2.5.m4.1.1.cmml" xref="S2.SS1.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m4.1.1.1.cmml" xref="S2.SS1.p2.5.m4.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.m4.1.1.2.cmml" xref="S2.SS1.p2.5.m4.1.1.2">ğ‘“</ci><ci id="S2.SS1.p2.5.m4.1.1.3.cmml" xref="S2.SS1.p2.5.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m4.1c">f_{c}</annotation></semantics></math> is the transmit center frequency, <math id="S2.SS1.p2.6.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p2.6.m5.1a"><mi id="S2.SS1.p2.6.m5.1.1" xref="S2.SS1.p2.6.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m5.1b"><ci id="S2.SS1.p2.6.m5.1.1.cmml" xref="S2.SS1.p2.6.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m5.1c">c</annotation></semantics></math> is the speed of light, and the amplitude <math id="S2.SS1.p2.7.m6.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.SS1.p2.7.m6.1a"><msub id="S2.SS1.p2.7.m6.1.1" xref="S2.SS1.p2.7.m6.1.1.cmml"><mi id="S2.SS1.p2.7.m6.1.1.2" xref="S2.SS1.p2.7.m6.1.1.2.cmml">a</mi><mi id="S2.SS1.p2.7.m6.1.1.3" xref="S2.SS1.p2.7.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m6.1b"><apply id="S2.SS1.p2.7.m6.1.1.cmml" xref="S2.SS1.p2.7.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.7.m6.1.1.1.cmml" xref="S2.SS1.p2.7.m6.1.1">subscript</csymbol><ci id="S2.SS1.p2.7.m6.1.1.2.cmml" xref="S2.SS1.p2.7.m6.1.1.2">ğ‘</ci><ci id="S2.SS1.p2.7.m6.1.1.3.cmml" xref="S2.SS1.p2.7.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m6.1c">a_{i}</annotation></semantics></math> is the square root of the power of the received signal as given by the radar range equation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Thus, RF sensors provide a complex-time series of measurements in the form <math id="S2.SS1.p2.8.m7.3" class="ltx_Math" alttext="x[t]=I[t]+jQ[t]" display="inline"><semantics id="S2.SS1.p2.8.m7.3a"><mrow id="S2.SS1.p2.8.m7.3.4" xref="S2.SS1.p2.8.m7.3.4.cmml"><mrow id="S2.SS1.p2.8.m7.3.4.2" xref="S2.SS1.p2.8.m7.3.4.2.cmml"><mi id="S2.SS1.p2.8.m7.3.4.2.2" xref="S2.SS1.p2.8.m7.3.4.2.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.8.m7.3.4.2.1" xref="S2.SS1.p2.8.m7.3.4.2.1.cmml">â€‹</mo><mrow id="S2.SS1.p2.8.m7.3.4.2.3.2" xref="S2.SS1.p2.8.m7.3.4.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.p2.8.m7.3.4.2.3.2.1" xref="S2.SS1.p2.8.m7.3.4.2.3.1.1.cmml">[</mo><mi id="S2.SS1.p2.8.m7.1.1" xref="S2.SS1.p2.8.m7.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p2.8.m7.3.4.2.3.2.2" xref="S2.SS1.p2.8.m7.3.4.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S2.SS1.p2.8.m7.3.4.1" xref="S2.SS1.p2.8.m7.3.4.1.cmml">=</mo><mrow id="S2.SS1.p2.8.m7.3.4.3" xref="S2.SS1.p2.8.m7.3.4.3.cmml"><mrow id="S2.SS1.p2.8.m7.3.4.3.2" xref="S2.SS1.p2.8.m7.3.4.3.2.cmml"><mi id="S2.SS1.p2.8.m7.3.4.3.2.2" xref="S2.SS1.p2.8.m7.3.4.3.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.8.m7.3.4.3.2.1" xref="S2.SS1.p2.8.m7.3.4.3.2.1.cmml">â€‹</mo><mrow id="S2.SS1.p2.8.m7.3.4.3.2.3.2" xref="S2.SS1.p2.8.m7.3.4.3.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.p2.8.m7.3.4.3.2.3.2.1" xref="S2.SS1.p2.8.m7.3.4.3.2.3.1.1.cmml">[</mo><mi id="S2.SS1.p2.8.m7.2.2" xref="S2.SS1.p2.8.m7.2.2.cmml">t</mi><mo stretchy="false" id="S2.SS1.p2.8.m7.3.4.3.2.3.2.2" xref="S2.SS1.p2.8.m7.3.4.3.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S2.SS1.p2.8.m7.3.4.3.1" xref="S2.SS1.p2.8.m7.3.4.3.1.cmml">+</mo><mrow id="S2.SS1.p2.8.m7.3.4.3.3" xref="S2.SS1.p2.8.m7.3.4.3.3.cmml"><mi id="S2.SS1.p2.8.m7.3.4.3.3.2" xref="S2.SS1.p2.8.m7.3.4.3.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.8.m7.3.4.3.3.1" xref="S2.SS1.p2.8.m7.3.4.3.3.1.cmml">â€‹</mo><mi id="S2.SS1.p2.8.m7.3.4.3.3.3" xref="S2.SS1.p2.8.m7.3.4.3.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.8.m7.3.4.3.3.1a" xref="S2.SS1.p2.8.m7.3.4.3.3.1.cmml">â€‹</mo><mrow id="S2.SS1.p2.8.m7.3.4.3.3.4.2" xref="S2.SS1.p2.8.m7.3.4.3.3.4.1.cmml"><mo stretchy="false" id="S2.SS1.p2.8.m7.3.4.3.3.4.2.1" xref="S2.SS1.p2.8.m7.3.4.3.3.4.1.1.cmml">[</mo><mi id="S2.SS1.p2.8.m7.3.3" xref="S2.SS1.p2.8.m7.3.3.cmml">t</mi><mo stretchy="false" id="S2.SS1.p2.8.m7.3.4.3.3.4.2.2" xref="S2.SS1.p2.8.m7.3.4.3.3.4.1.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m7.3b"><apply id="S2.SS1.p2.8.m7.3.4.cmml" xref="S2.SS1.p2.8.m7.3.4"><eq id="S2.SS1.p2.8.m7.3.4.1.cmml" xref="S2.SS1.p2.8.m7.3.4.1"></eq><apply id="S2.SS1.p2.8.m7.3.4.2.cmml" xref="S2.SS1.p2.8.m7.3.4.2"><times id="S2.SS1.p2.8.m7.3.4.2.1.cmml" xref="S2.SS1.p2.8.m7.3.4.2.1"></times><ci id="S2.SS1.p2.8.m7.3.4.2.2.cmml" xref="S2.SS1.p2.8.m7.3.4.2.2">ğ‘¥</ci><apply id="S2.SS1.p2.8.m7.3.4.2.3.1.cmml" xref="S2.SS1.p2.8.m7.3.4.2.3.2"><csymbol cd="latexml" id="S2.SS1.p2.8.m7.3.4.2.3.1.1.cmml" xref="S2.SS1.p2.8.m7.3.4.2.3.2.1">delimited-[]</csymbol><ci id="S2.SS1.p2.8.m7.1.1.cmml" xref="S2.SS1.p2.8.m7.1.1">ğ‘¡</ci></apply></apply><apply id="S2.SS1.p2.8.m7.3.4.3.cmml" xref="S2.SS1.p2.8.m7.3.4.3"><plus id="S2.SS1.p2.8.m7.3.4.3.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.1"></plus><apply id="S2.SS1.p2.8.m7.3.4.3.2.cmml" xref="S2.SS1.p2.8.m7.3.4.3.2"><times id="S2.SS1.p2.8.m7.3.4.3.2.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.2.1"></times><ci id="S2.SS1.p2.8.m7.3.4.3.2.2.cmml" xref="S2.SS1.p2.8.m7.3.4.3.2.2">ğ¼</ci><apply id="S2.SS1.p2.8.m7.3.4.3.2.3.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.2.3.2"><csymbol cd="latexml" id="S2.SS1.p2.8.m7.3.4.3.2.3.1.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.2.3.2.1">delimited-[]</csymbol><ci id="S2.SS1.p2.8.m7.2.2.cmml" xref="S2.SS1.p2.8.m7.2.2">ğ‘¡</ci></apply></apply><apply id="S2.SS1.p2.8.m7.3.4.3.3.cmml" xref="S2.SS1.p2.8.m7.3.4.3.3"><times id="S2.SS1.p2.8.m7.3.4.3.3.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.3.1"></times><ci id="S2.SS1.p2.8.m7.3.4.3.3.2.cmml" xref="S2.SS1.p2.8.m7.3.4.3.3.2">ğ‘—</ci><ci id="S2.SS1.p2.8.m7.3.4.3.3.3.cmml" xref="S2.SS1.p2.8.m7.3.4.3.3.3">ğ‘„</ci><apply id="S2.SS1.p2.8.m7.3.4.3.3.4.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.3.4.2"><csymbol cd="latexml" id="S2.SS1.p2.8.m7.3.4.3.3.4.1.1.cmml" xref="S2.SS1.p2.8.m7.3.4.3.3.4.2.1">delimited-[]</csymbol><ci id="S2.SS1.p2.8.m7.3.3.cmml" xref="S2.SS1.p2.8.m7.3.3">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m7.3c">x[t]=I[t]+jQ[t]</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.11" class="ltx_p">Typically, this data stream is re-shaped into a 2D matrix for each RF receive channel, so that the columns represent <span id="S2.SS1.p3.11.1" class="ltx_text ltx_font_italic">fast-time</span>, e.g. the analog-to-digital converter samples, and the rows represent <span id="S2.SS1.p3.11.2" class="ltx_text ltx_font_italic">slow-time</span>, e.g. pulse number. The range (<math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">R</annotation></semantics></math>) between the radar and any scattering point is found from the round-trip travel time (<math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="t_{d}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msub id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">t</mi><mi id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">ğ‘¡</ci><ci id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">t_{d}</annotation></semantics></math>) as
<math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="R=ct_{d}/2" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mrow id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1.2" xref="S2.SS1.p3.3.m3.1.1.2.cmml">R</mi><mo id="S2.SS1.p3.3.m3.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.cmml">=</mo><mrow id="S2.SS1.p3.3.m3.1.1.3" xref="S2.SS1.p3.3.m3.1.1.3.cmml"><mrow id="S2.SS1.p3.3.m3.1.1.3.2" xref="S2.SS1.p3.3.m3.1.1.3.2.cmml"><mi id="S2.SS1.p3.3.m3.1.1.3.2.2" xref="S2.SS1.p3.3.m3.1.1.3.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.3.m3.1.1.3.2.1" xref="S2.SS1.p3.3.m3.1.1.3.2.1.cmml">â€‹</mo><msub id="S2.SS1.p3.3.m3.1.1.3.2.3" xref="S2.SS1.p3.3.m3.1.1.3.2.3.cmml"><mi id="S2.SS1.p3.3.m3.1.1.3.2.3.2" xref="S2.SS1.p3.3.m3.1.1.3.2.3.2.cmml">t</mi><mi id="S2.SS1.p3.3.m3.1.1.3.2.3.3" xref="S2.SS1.p3.3.m3.1.1.3.2.3.3.cmml">d</mi></msub></mrow><mo id="S2.SS1.p3.3.m3.1.1.3.1" xref="S2.SS1.p3.3.m3.1.1.3.1.cmml">/</mo><mn id="S2.SS1.p3.3.m3.1.1.3.3" xref="S2.SS1.p3.3.m3.1.1.3.3.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><apply id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1"><eq id="S2.SS1.p3.3.m3.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1.1"></eq><ci id="S2.SS1.p3.3.m3.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.2">ğ‘…</ci><apply id="S2.SS1.p3.3.m3.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3"><divide id="S2.SS1.p3.3.m3.1.1.3.1.cmml" xref="S2.SS1.p3.3.m3.1.1.3.1"></divide><apply id="S2.SS1.p3.3.m3.1.1.3.2.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2"><times id="S2.SS1.p3.3.m3.1.1.3.2.1.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2.1"></times><ci id="S2.SS1.p3.3.m3.1.1.3.2.2.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2.2">ğ‘</ci><apply id="S2.SS1.p3.3.m3.1.1.3.2.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m3.1.1.3.2.3.1.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2.3">subscript</csymbol><ci id="S2.SS1.p3.3.m3.1.1.3.2.3.2.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2.3.2">ğ‘¡</ci><ci id="S2.SS1.p3.3.m3.1.1.3.2.3.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3.2.3.3">ğ‘‘</ci></apply></apply><cn type="integer" id="S2.SS1.p3.3.m3.1.1.3.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">R=ct_{d}/2</annotation></semantics></math>.
In an FMCW radar system, the travel time can be found by mixing the received signal with the transmitted signal and filtering out high frequency components to obtain the beat frequency, <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="f_{b}=f_{t}-f_{r}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mrow id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml"><msub id="S2.SS1.p3.4.m4.1.1.2" xref="S2.SS1.p3.4.m4.1.1.2.cmml"><mi id="S2.SS1.p3.4.m4.1.1.2.2" xref="S2.SS1.p3.4.m4.1.1.2.2.cmml">f</mi><mi id="S2.SS1.p3.4.m4.1.1.2.3" xref="S2.SS1.p3.4.m4.1.1.2.3.cmml">b</mi></msub><mo id="S2.SS1.p3.4.m4.1.1.1" xref="S2.SS1.p3.4.m4.1.1.1.cmml">=</mo><mrow id="S2.SS1.p3.4.m4.1.1.3" xref="S2.SS1.p3.4.m4.1.1.3.cmml"><msub id="S2.SS1.p3.4.m4.1.1.3.2" xref="S2.SS1.p3.4.m4.1.1.3.2.cmml"><mi id="S2.SS1.p3.4.m4.1.1.3.2.2" xref="S2.SS1.p3.4.m4.1.1.3.2.2.cmml">f</mi><mi id="S2.SS1.p3.4.m4.1.1.3.2.3" xref="S2.SS1.p3.4.m4.1.1.3.2.3.cmml">t</mi></msub><mo id="S2.SS1.p3.4.m4.1.1.3.1" xref="S2.SS1.p3.4.m4.1.1.3.1.cmml">âˆ’</mo><msub id="S2.SS1.p3.4.m4.1.1.3.3" xref="S2.SS1.p3.4.m4.1.1.3.3.cmml"><mi id="S2.SS1.p3.4.m4.1.1.3.3.2" xref="S2.SS1.p3.4.m4.1.1.3.3.2.cmml">f</mi><mi id="S2.SS1.p3.4.m4.1.1.3.3.3" xref="S2.SS1.p3.4.m4.1.1.3.3.3.cmml">r</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><apply id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1"><eq id="S2.SS1.p3.4.m4.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1.1"></eq><apply id="S2.SS1.p3.4.m4.1.1.2.cmml" xref="S2.SS1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.2.1.cmml" xref="S2.SS1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.2.2.cmml" xref="S2.SS1.p3.4.m4.1.1.2.2">ğ‘“</ci><ci id="S2.SS1.p3.4.m4.1.1.2.3.cmml" xref="S2.SS1.p3.4.m4.1.1.2.3">ğ‘</ci></apply><apply id="S2.SS1.p3.4.m4.1.1.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3"><minus id="S2.SS1.p3.4.m4.1.1.3.1.cmml" xref="S2.SS1.p3.4.m4.1.1.3.1"></minus><apply id="S2.SS1.p3.4.m4.1.1.3.2.cmml" xref="S2.SS1.p3.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.3.2.1.cmml" xref="S2.SS1.p3.4.m4.1.1.3.2">subscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.3.2.2.cmml" xref="S2.SS1.p3.4.m4.1.1.3.2.2">ğ‘“</ci><ci id="S2.SS1.p3.4.m4.1.1.3.2.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3.2.3">ğ‘¡</ci></apply><apply id="S2.SS1.p3.4.m4.1.1.3.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.p3.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.p3.4.m4.1.1.3.3.2">ğ‘“</ci><ci id="S2.SS1.p3.4.m4.1.1.3.3.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3.3.3">ğ‘Ÿ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">f_{b}=f_{t}-f_{r}</annotation></semantics></math>, which is the difference in the instantaneous frequencies of transmit and receive signals, <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="f_{t}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msub id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">f</mi><mi id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">ğ‘“</ci><ci id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">f_{t}</annotation></semantics></math> and <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="f_{r}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msub id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">f</mi><mi id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">ğ‘“</ci><ci id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">f_{r}</annotation></semantics></math>, respectively. Since the chirp rate is
<math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="\gamma=B/\tau=f_{b}/t_{d}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><mrow id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml">Î³</mi><mo id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml">=</mo><mrow id="S2.SS1.p3.7.m7.1.1.4" xref="S2.SS1.p3.7.m7.1.1.4.cmml"><mi id="S2.SS1.p3.7.m7.1.1.4.2" xref="S2.SS1.p3.7.m7.1.1.4.2.cmml">B</mi><mo id="S2.SS1.p3.7.m7.1.1.4.1" xref="S2.SS1.p3.7.m7.1.1.4.1.cmml">/</mo><mi id="S2.SS1.p3.7.m7.1.1.4.3" xref="S2.SS1.p3.7.m7.1.1.4.3.cmml">Ï„</mi></mrow><mo id="S2.SS1.p3.7.m7.1.1.5" xref="S2.SS1.p3.7.m7.1.1.5.cmml">=</mo><mrow id="S2.SS1.p3.7.m7.1.1.6" xref="S2.SS1.p3.7.m7.1.1.6.cmml"><msub id="S2.SS1.p3.7.m7.1.1.6.2" xref="S2.SS1.p3.7.m7.1.1.6.2.cmml"><mi id="S2.SS1.p3.7.m7.1.1.6.2.2" xref="S2.SS1.p3.7.m7.1.1.6.2.2.cmml">f</mi><mi id="S2.SS1.p3.7.m7.1.1.6.2.3" xref="S2.SS1.p3.7.m7.1.1.6.2.3.cmml">b</mi></msub><mo id="S2.SS1.p3.7.m7.1.1.6.1" xref="S2.SS1.p3.7.m7.1.1.6.1.cmml">/</mo><msub id="S2.SS1.p3.7.m7.1.1.6.3" xref="S2.SS1.p3.7.m7.1.1.6.3.cmml"><mi id="S2.SS1.p3.7.m7.1.1.6.3.2" xref="S2.SS1.p3.7.m7.1.1.6.3.2.cmml">t</mi><mi id="S2.SS1.p3.7.m7.1.1.6.3.3" xref="S2.SS1.p3.7.m7.1.1.6.3.3.cmml">d</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><and id="S2.SS1.p3.7.m7.1.1a.cmml" xref="S2.SS1.p3.7.m7.1.1"></and><apply id="S2.SS1.p3.7.m7.1.1b.cmml" xref="S2.SS1.p3.7.m7.1.1"><eq id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3"></eq><ci id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2">ğ›¾</ci><apply id="S2.SS1.p3.7.m7.1.1.4.cmml" xref="S2.SS1.p3.7.m7.1.1.4"><divide id="S2.SS1.p3.7.m7.1.1.4.1.cmml" xref="S2.SS1.p3.7.m7.1.1.4.1"></divide><ci id="S2.SS1.p3.7.m7.1.1.4.2.cmml" xref="S2.SS1.p3.7.m7.1.1.4.2">ğµ</ci><ci id="S2.SS1.p3.7.m7.1.1.4.3.cmml" xref="S2.SS1.p3.7.m7.1.1.4.3">ğœ</ci></apply></apply><apply id="S2.SS1.p3.7.m7.1.1c.cmml" xref="S2.SS1.p3.7.m7.1.1"><eq id="S2.SS1.p3.7.m7.1.1.5.cmml" xref="S2.SS1.p3.7.m7.1.1.5"></eq><share href="#S2.SS1.p3.7.m7.1.1.4.cmml" id="S2.SS1.p3.7.m7.1.1d.cmml" xref="S2.SS1.p3.7.m7.1.1"></share><apply id="S2.SS1.p3.7.m7.1.1.6.cmml" xref="S2.SS1.p3.7.m7.1.1.6"><divide id="S2.SS1.p3.7.m7.1.1.6.1.cmml" xref="S2.SS1.p3.7.m7.1.1.6.1"></divide><apply id="S2.SS1.p3.7.m7.1.1.6.2.cmml" xref="S2.SS1.p3.7.m7.1.1.6.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.6.2.1.cmml" xref="S2.SS1.p3.7.m7.1.1.6.2">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.6.2.2.cmml" xref="S2.SS1.p3.7.m7.1.1.6.2.2">ğ‘“</ci><ci id="S2.SS1.p3.7.m7.1.1.6.2.3.cmml" xref="S2.SS1.p3.7.m7.1.1.6.2.3">ğ‘</ci></apply><apply id="S2.SS1.p3.7.m7.1.1.6.3.cmml" xref="S2.SS1.p3.7.m7.1.1.6.3"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.6.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.6.3">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.6.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.6.3.2">ğ‘¡</ci><ci id="S2.SS1.p3.7.m7.1.1.6.3.3.cmml" xref="S2.SS1.p3.7.m7.1.1.6.3.3">ğ‘‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">\gamma=B/\tau=f_{b}/t_{d}</annotation></semantics></math>, the range is found as
<math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="R=c\tau f_{b}/2B" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><mrow id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml"><mi id="S2.SS1.p3.8.m8.1.1.2" xref="S2.SS1.p3.8.m8.1.1.2.cmml">R</mi><mo id="S2.SS1.p3.8.m8.1.1.1" xref="S2.SS1.p3.8.m8.1.1.1.cmml">=</mo><mrow id="S2.SS1.p3.8.m8.1.1.3" xref="S2.SS1.p3.8.m8.1.1.3.cmml"><mrow id="S2.SS1.p3.8.m8.1.1.3.2" xref="S2.SS1.p3.8.m8.1.1.3.2.cmml"><mrow id="S2.SS1.p3.8.m8.1.1.3.2.2" xref="S2.SS1.p3.8.m8.1.1.3.2.2.cmml"><mi id="S2.SS1.p3.8.m8.1.1.3.2.2.2" xref="S2.SS1.p3.8.m8.1.1.3.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.1.1.3.2.2.1" xref="S2.SS1.p3.8.m8.1.1.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p3.8.m8.1.1.3.2.2.3" xref="S2.SS1.p3.8.m8.1.1.3.2.2.3.cmml">Ï„</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.1.1.3.2.2.1a" xref="S2.SS1.p3.8.m8.1.1.3.2.2.1.cmml">â€‹</mo><msub id="S2.SS1.p3.8.m8.1.1.3.2.2.4" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4.cmml"><mi id="S2.SS1.p3.8.m8.1.1.3.2.2.4.2" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4.2.cmml">f</mi><mi id="S2.SS1.p3.8.m8.1.1.3.2.2.4.3" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4.3.cmml">b</mi></msub></mrow><mo id="S2.SS1.p3.8.m8.1.1.3.2.1" xref="S2.SS1.p3.8.m8.1.1.3.2.1.cmml">/</mo><mn id="S2.SS1.p3.8.m8.1.1.3.2.3" xref="S2.SS1.p3.8.m8.1.1.3.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.1.1.3.1" xref="S2.SS1.p3.8.m8.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p3.8.m8.1.1.3.3" xref="S2.SS1.p3.8.m8.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><apply id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1"><eq id="S2.SS1.p3.8.m8.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1.1"></eq><ci id="S2.SS1.p3.8.m8.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1.2">ğ‘…</ci><apply id="S2.SS1.p3.8.m8.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3"><times id="S2.SS1.p3.8.m8.1.1.3.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3.1"></times><apply id="S2.SS1.p3.8.m8.1.1.3.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2"><divide id="S2.SS1.p3.8.m8.1.1.3.2.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.1"></divide><apply id="S2.SS1.p3.8.m8.1.1.3.2.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2"><times id="S2.SS1.p3.8.m8.1.1.3.2.2.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.1"></times><ci id="S2.SS1.p3.8.m8.1.1.3.2.2.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.2">ğ‘</ci><ci id="S2.SS1.p3.8.m8.1.1.3.2.2.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.3">ğœ</ci><apply id="S2.SS1.p3.8.m8.1.1.3.2.2.4.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.3.2.2.4.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4">subscript</csymbol><ci id="S2.SS1.p3.8.m8.1.1.3.2.2.4.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4.2">ğ‘“</ci><ci id="S2.SS1.p3.8.m8.1.1.3.2.2.4.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2.4.3">ğ‘</ci></apply></apply><cn type="integer" id="S2.SS1.p3.8.m8.1.1.3.2.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.3">2</cn></apply><ci id="S2.SS1.p3.8.m8.1.1.3.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.3">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">R=c\tau f_{b}/2B</annotation></semantics></math>,
where <math id="S2.SS1.p3.9.m9.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S2.SS1.p3.9.m9.1a"><mi id="S2.SS1.p3.9.m9.1.1" xref="S2.SS1.p3.9.m9.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m9.1b"><ci id="S2.SS1.p3.9.m9.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m9.1c">\tau</annotation></semantics></math> is the pulse width.
The radial velocity of motion, <math id="S2.SS1.p3.10.m10.1" class="ltx_Math" alttext="v_{r}" display="inline"><semantics id="S2.SS1.p3.10.m10.1a"><msub id="S2.SS1.p3.10.m10.1.1" xref="S2.SS1.p3.10.m10.1.1.cmml"><mi id="S2.SS1.p3.10.m10.1.1.2" xref="S2.SS1.p3.10.m10.1.1.2.cmml">v</mi><mi id="S2.SS1.p3.10.m10.1.1.3" xref="S2.SS1.p3.10.m10.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.10.m10.1b"><apply id="S2.SS1.p3.10.m10.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.10.m10.1.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p3.10.m10.1.1.2.cmml" xref="S2.SS1.p3.10.m10.1.1.2">ğ‘£</ci><ci id="S2.SS1.p3.10.m10.1.1.3.cmml" xref="S2.SS1.p3.10.m10.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.10.m10.1c">v_{r}</annotation></semantics></math>, is given by the Doppler shift,
<math id="S2.SS1.p3.11.m11.1" class="ltx_Math" alttext="f_{D}=2v_{r}f_{t}/c" display="inline"><semantics id="S2.SS1.p3.11.m11.1a"><mrow id="S2.SS1.p3.11.m11.1.1" xref="S2.SS1.p3.11.m11.1.1.cmml"><msub id="S2.SS1.p3.11.m11.1.1.2" xref="S2.SS1.p3.11.m11.1.1.2.cmml"><mi id="S2.SS1.p3.11.m11.1.1.2.2" xref="S2.SS1.p3.11.m11.1.1.2.2.cmml">f</mi><mi id="S2.SS1.p3.11.m11.1.1.2.3" xref="S2.SS1.p3.11.m11.1.1.2.3.cmml">D</mi></msub><mo id="S2.SS1.p3.11.m11.1.1.1" xref="S2.SS1.p3.11.m11.1.1.1.cmml">=</mo><mrow id="S2.SS1.p3.11.m11.1.1.3" xref="S2.SS1.p3.11.m11.1.1.3.cmml"><mrow id="S2.SS1.p3.11.m11.1.1.3.2" xref="S2.SS1.p3.11.m11.1.1.3.2.cmml"><mn id="S2.SS1.p3.11.m11.1.1.3.2.2" xref="S2.SS1.p3.11.m11.1.1.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p3.11.m11.1.1.3.2.1" xref="S2.SS1.p3.11.m11.1.1.3.2.1.cmml">â€‹</mo><msub id="S2.SS1.p3.11.m11.1.1.3.2.3" xref="S2.SS1.p3.11.m11.1.1.3.2.3.cmml"><mi id="S2.SS1.p3.11.m11.1.1.3.2.3.2" xref="S2.SS1.p3.11.m11.1.1.3.2.3.2.cmml">v</mi><mi id="S2.SS1.p3.11.m11.1.1.3.2.3.3" xref="S2.SS1.p3.11.m11.1.1.3.2.3.3.cmml">r</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p3.11.m11.1.1.3.2.1a" xref="S2.SS1.p3.11.m11.1.1.3.2.1.cmml">â€‹</mo><msub id="S2.SS1.p3.11.m11.1.1.3.2.4" xref="S2.SS1.p3.11.m11.1.1.3.2.4.cmml"><mi id="S2.SS1.p3.11.m11.1.1.3.2.4.2" xref="S2.SS1.p3.11.m11.1.1.3.2.4.2.cmml">f</mi><mi id="S2.SS1.p3.11.m11.1.1.3.2.4.3" xref="S2.SS1.p3.11.m11.1.1.3.2.4.3.cmml">t</mi></msub></mrow><mo id="S2.SS1.p3.11.m11.1.1.3.1" xref="S2.SS1.p3.11.m11.1.1.3.1.cmml">/</mo><mi id="S2.SS1.p3.11.m11.1.1.3.3" xref="S2.SS1.p3.11.m11.1.1.3.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.11.m11.1b"><apply id="S2.SS1.p3.11.m11.1.1.cmml" xref="S2.SS1.p3.11.m11.1.1"><eq id="S2.SS1.p3.11.m11.1.1.1.cmml" xref="S2.SS1.p3.11.m11.1.1.1"></eq><apply id="S2.SS1.p3.11.m11.1.1.2.cmml" xref="S2.SS1.p3.11.m11.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.1.1.2.1.cmml" xref="S2.SS1.p3.11.m11.1.1.2">subscript</csymbol><ci id="S2.SS1.p3.11.m11.1.1.2.2.cmml" xref="S2.SS1.p3.11.m11.1.1.2.2">ğ‘“</ci><ci id="S2.SS1.p3.11.m11.1.1.2.3.cmml" xref="S2.SS1.p3.11.m11.1.1.2.3">ğ·</ci></apply><apply id="S2.SS1.p3.11.m11.1.1.3.cmml" xref="S2.SS1.p3.11.m11.1.1.3"><divide id="S2.SS1.p3.11.m11.1.1.3.1.cmml" xref="S2.SS1.p3.11.m11.1.1.3.1"></divide><apply id="S2.SS1.p3.11.m11.1.1.3.2.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2"><times id="S2.SS1.p3.11.m11.1.1.3.2.1.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.1"></times><cn type="integer" id="S2.SS1.p3.11.m11.1.1.3.2.2.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.2">2</cn><apply id="S2.SS1.p3.11.m11.1.1.3.2.3.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.1.1.3.2.3.1.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.3">subscript</csymbol><ci id="S2.SS1.p3.11.m11.1.1.3.2.3.2.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.3.2">ğ‘£</ci><ci id="S2.SS1.p3.11.m11.1.1.3.2.3.3.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.3.3">ğ‘Ÿ</ci></apply><apply id="S2.SS1.p3.11.m11.1.1.3.2.4.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.4"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.1.1.3.2.4.1.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.4">subscript</csymbol><ci id="S2.SS1.p3.11.m11.1.1.3.2.4.2.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.4.2">ğ‘“</ci><ci id="S2.SS1.p3.11.m11.1.1.3.2.4.3.cmml" xref="S2.SS1.p3.11.m11.1.1.3.2.4.3">ğ‘¡</ci></apply></apply><ci id="S2.SS1.p3.11.m11.1.1.3.3.cmml" xref="S2.SS1.p3.11.m11.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.11.m11.1c">f_{D}=2v_{r}f_{t}/c</annotation></semantics></math>,
which may be found by taking the Fast Fourier Transform (FFT) across pulses for a specific range bin.
The significance of these relations is that the range and velocity estimates obtained from RF sensors are independent measurements.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.6" class="ltx_p">To enable fine grain motion recognition, it is also important for the radar to have sufficient resolution so as to distinguish the motion of the left and right hands, as well as fingers. The minimum interval between two adjacent targets that can be discriminated by the radar in the radial direction is defined as the range resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, and is given by <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\Delta r=c/2B" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mrow id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mrow id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S2.SS1.p4.1.m1.1.1.2.2" xref="S2.SS1.p4.1.m1.1.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.1.m1.1.1.2.1" xref="S2.SS1.p4.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS1.p4.1.m1.1.1.2.3" xref="S2.SS1.p4.1.m1.1.1.2.3.cmml">r</mi></mrow><mo id="S2.SS1.p4.1.m1.1.1.1" xref="S2.SS1.p4.1.m1.1.1.1.cmml">=</mo><mrow id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml"><mrow id="S2.SS1.p4.1.m1.1.1.3.2" xref="S2.SS1.p4.1.m1.1.1.3.2.cmml"><mi id="S2.SS1.p4.1.m1.1.1.3.2.2" xref="S2.SS1.p4.1.m1.1.1.3.2.2.cmml">c</mi><mo id="S2.SS1.p4.1.m1.1.1.3.2.1" xref="S2.SS1.p4.1.m1.1.1.3.2.1.cmml">/</mo><mn id="S2.SS1.p4.1.m1.1.1.3.2.3" xref="S2.SS1.p4.1.m1.1.1.3.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p4.1.m1.1.1.3.1" xref="S2.SS1.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p4.1.m1.1.1.3.3" xref="S2.SS1.p4.1.m1.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><eq id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1.1"></eq><apply id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2"><times id="S2.SS1.p4.1.m1.1.1.2.1.cmml" xref="S2.SS1.p4.1.m1.1.1.2.1"></times><ci id="S2.SS1.p4.1.m1.1.1.2.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2.2">Î”</ci><ci id="S2.SS1.p4.1.m1.1.1.2.3.cmml" xref="S2.SS1.p4.1.m1.1.1.2.3">ğ‘Ÿ</ci></apply><apply id="S2.SS1.p4.1.m1.1.1.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3"><times id="S2.SS1.p4.1.m1.1.1.3.1.cmml" xref="S2.SS1.p4.1.m1.1.1.3.1"></times><apply id="S2.SS1.p4.1.m1.1.1.3.2.cmml" xref="S2.SS1.p4.1.m1.1.1.3.2"><divide id="S2.SS1.p4.1.m1.1.1.3.2.1.cmml" xref="S2.SS1.p4.1.m1.1.1.3.2.1"></divide><ci id="S2.SS1.p4.1.m1.1.1.3.2.2.cmml" xref="S2.SS1.p4.1.m1.1.1.3.2.2">ğ‘</ci><cn type="integer" id="S2.SS1.p4.1.m1.1.1.3.2.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3.2.3">2</cn></apply><ci id="S2.SS1.p4.1.m1.1.1.3.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3.3">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\Delta r=c/2B</annotation></semantics></math>.
The velocity resolution determines the minimum difference in velocity that can be perceived by the RF sensor, and, mathematically, is inversely proportional to the coherent processing interval or CPI, during which the target is illuminated. If <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="T_{f}" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><msub id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi id="S2.SS1.p4.2.m2.1.1.2" xref="S2.SS1.p4.2.m2.1.1.2.cmml">T</mi><mi id="S2.SS1.p4.2.m2.1.1.3" xref="S2.SS1.p4.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">ğ‘‡</ci><ci id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">T_{f}</annotation></semantics></math> is the CPI and <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\lambda</annotation></semantics></math> is the wavelength, then the velocity resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, <math id="S2.SS1.p4.4.m4.1" class="ltx_Math" alttext="v_{res}" display="inline"><semantics id="S2.SS1.p4.4.m4.1a"><msub id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml"><mi id="S2.SS1.p4.4.m4.1.1.2" xref="S2.SS1.p4.4.m4.1.1.2.cmml">v</mi><mrow id="S2.SS1.p4.4.m4.1.1.3" xref="S2.SS1.p4.4.m4.1.1.3.cmml"><mi id="S2.SS1.p4.4.m4.1.1.3.2" xref="S2.SS1.p4.4.m4.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.4.m4.1.1.3.1" xref="S2.SS1.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p4.4.m4.1.1.3.3" xref="S2.SS1.p4.4.m4.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.4.m4.1.1.3.1a" xref="S2.SS1.p4.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p4.4.m4.1.1.3.4" xref="S2.SS1.p4.4.m4.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><apply id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.4.m4.1.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p4.4.m4.1.1.2.cmml" xref="S2.SS1.p4.4.m4.1.1.2">ğ‘£</ci><apply id="S2.SS1.p4.4.m4.1.1.3.cmml" xref="S2.SS1.p4.4.m4.1.1.3"><times id="S2.SS1.p4.4.m4.1.1.3.1.cmml" xref="S2.SS1.p4.4.m4.1.1.3.1"></times><ci id="S2.SS1.p4.4.m4.1.1.3.2.cmml" xref="S2.SS1.p4.4.m4.1.1.3.2">ğ‘Ÿ</ci><ci id="S2.SS1.p4.4.m4.1.1.3.3.cmml" xref="S2.SS1.p4.4.m4.1.1.3.3">ğ‘’</ci><ci id="S2.SS1.p4.4.m4.1.1.3.4.cmml" xref="S2.SS1.p4.4.m4.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">v_{res}</annotation></semantics></math> is given by
<math id="S2.SS1.p4.5.m5.1" class="ltx_Math" alttext="v_{res}=\lambda/2T_{f}=\lambda/2N_{p}\tau" display="inline"><semantics id="S2.SS1.p4.5.m5.1a"><mrow id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml"><msub id="S2.SS1.p4.5.m5.1.1.2" xref="S2.SS1.p4.5.m5.1.1.2.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2.2" xref="S2.SS1.p4.5.m5.1.1.2.2.cmml">v</mi><mrow id="S2.SS1.p4.5.m5.1.1.2.3" xref="S2.SS1.p4.5.m5.1.1.2.3.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2.3.2" xref="S2.SS1.p4.5.m5.1.1.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.5.m5.1.1.2.3.1" xref="S2.SS1.p4.5.m5.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.SS1.p4.5.m5.1.1.2.3.3" xref="S2.SS1.p4.5.m5.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.5.m5.1.1.2.3.1a" xref="S2.SS1.p4.5.m5.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.SS1.p4.5.m5.1.1.2.3.4" xref="S2.SS1.p4.5.m5.1.1.2.3.4.cmml">s</mi></mrow></msub><mo id="S2.SS1.p4.5.m5.1.1.3" xref="S2.SS1.p4.5.m5.1.1.3.cmml">=</mo><mrow id="S2.SS1.p4.5.m5.1.1.4" xref="S2.SS1.p4.5.m5.1.1.4.cmml"><mrow id="S2.SS1.p4.5.m5.1.1.4.2" xref="S2.SS1.p4.5.m5.1.1.4.2.cmml"><mi id="S2.SS1.p4.5.m5.1.1.4.2.2" xref="S2.SS1.p4.5.m5.1.1.4.2.2.cmml">Î»</mi><mo id="S2.SS1.p4.5.m5.1.1.4.2.1" xref="S2.SS1.p4.5.m5.1.1.4.2.1.cmml">/</mo><mn id="S2.SS1.p4.5.m5.1.1.4.2.3" xref="S2.SS1.p4.5.m5.1.1.4.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p4.5.m5.1.1.4.1" xref="S2.SS1.p4.5.m5.1.1.4.1.cmml">â€‹</mo><msub id="S2.SS1.p4.5.m5.1.1.4.3" xref="S2.SS1.p4.5.m5.1.1.4.3.cmml"><mi id="S2.SS1.p4.5.m5.1.1.4.3.2" xref="S2.SS1.p4.5.m5.1.1.4.3.2.cmml">T</mi><mi id="S2.SS1.p4.5.m5.1.1.4.3.3" xref="S2.SS1.p4.5.m5.1.1.4.3.3.cmml">f</mi></msub></mrow><mo id="S2.SS1.p4.5.m5.1.1.5" xref="S2.SS1.p4.5.m5.1.1.5.cmml">=</mo><mrow id="S2.SS1.p4.5.m5.1.1.6" xref="S2.SS1.p4.5.m5.1.1.6.cmml"><mrow id="S2.SS1.p4.5.m5.1.1.6.2" xref="S2.SS1.p4.5.m5.1.1.6.2.cmml"><mi id="S2.SS1.p4.5.m5.1.1.6.2.2" xref="S2.SS1.p4.5.m5.1.1.6.2.2.cmml">Î»</mi><mo id="S2.SS1.p4.5.m5.1.1.6.2.1" xref="S2.SS1.p4.5.m5.1.1.6.2.1.cmml">/</mo><mn id="S2.SS1.p4.5.m5.1.1.6.2.3" xref="S2.SS1.p4.5.m5.1.1.6.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p4.5.m5.1.1.6.1" xref="S2.SS1.p4.5.m5.1.1.6.1.cmml">â€‹</mo><msub id="S2.SS1.p4.5.m5.1.1.6.3" xref="S2.SS1.p4.5.m5.1.1.6.3.cmml"><mi id="S2.SS1.p4.5.m5.1.1.6.3.2" xref="S2.SS1.p4.5.m5.1.1.6.3.2.cmml">N</mi><mi id="S2.SS1.p4.5.m5.1.1.6.3.3" xref="S2.SS1.p4.5.m5.1.1.6.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p4.5.m5.1.1.6.1a" xref="S2.SS1.p4.5.m5.1.1.6.1.cmml">â€‹</mo><mi id="S2.SS1.p4.5.m5.1.1.6.4" xref="S2.SS1.p4.5.m5.1.1.6.4.cmml">Ï„</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1"><and id="S2.SS1.p4.5.m5.1.1a.cmml" xref="S2.SS1.p4.5.m5.1.1"></and><apply id="S2.SS1.p4.5.m5.1.1b.cmml" xref="S2.SS1.p4.5.m5.1.1"><eq id="S2.SS1.p4.5.m5.1.1.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3"></eq><apply id="S2.SS1.p4.5.m5.1.1.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.2.1.cmml" xref="S2.SS1.p4.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.2.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2.2">ğ‘£</ci><apply id="S2.SS1.p4.5.m5.1.1.2.3.cmml" xref="S2.SS1.p4.5.m5.1.1.2.3"><times id="S2.SS1.p4.5.m5.1.1.2.3.1.cmml" xref="S2.SS1.p4.5.m5.1.1.2.3.1"></times><ci id="S2.SS1.p4.5.m5.1.1.2.3.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2.3.2">ğ‘Ÿ</ci><ci id="S2.SS1.p4.5.m5.1.1.2.3.3.cmml" xref="S2.SS1.p4.5.m5.1.1.2.3.3">ğ‘’</ci><ci id="S2.SS1.p4.5.m5.1.1.2.3.4.cmml" xref="S2.SS1.p4.5.m5.1.1.2.3.4">ğ‘ </ci></apply></apply><apply id="S2.SS1.p4.5.m5.1.1.4.cmml" xref="S2.SS1.p4.5.m5.1.1.4"><times id="S2.SS1.p4.5.m5.1.1.4.1.cmml" xref="S2.SS1.p4.5.m5.1.1.4.1"></times><apply id="S2.SS1.p4.5.m5.1.1.4.2.cmml" xref="S2.SS1.p4.5.m5.1.1.4.2"><divide id="S2.SS1.p4.5.m5.1.1.4.2.1.cmml" xref="S2.SS1.p4.5.m5.1.1.4.2.1"></divide><ci id="S2.SS1.p4.5.m5.1.1.4.2.2.cmml" xref="S2.SS1.p4.5.m5.1.1.4.2.2">ğœ†</ci><cn type="integer" id="S2.SS1.p4.5.m5.1.1.4.2.3.cmml" xref="S2.SS1.p4.5.m5.1.1.4.2.3">2</cn></apply><apply id="S2.SS1.p4.5.m5.1.1.4.3.cmml" xref="S2.SS1.p4.5.m5.1.1.4.3"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.4.3.1.cmml" xref="S2.SS1.p4.5.m5.1.1.4.3">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.4.3.2.cmml" xref="S2.SS1.p4.5.m5.1.1.4.3.2">ğ‘‡</ci><ci id="S2.SS1.p4.5.m5.1.1.4.3.3.cmml" xref="S2.SS1.p4.5.m5.1.1.4.3.3">ğ‘“</ci></apply></apply></apply><apply id="S2.SS1.p4.5.m5.1.1c.cmml" xref="S2.SS1.p4.5.m5.1.1"><eq id="S2.SS1.p4.5.m5.1.1.5.cmml" xref="S2.SS1.p4.5.m5.1.1.5"></eq><share href="#S2.SS1.p4.5.m5.1.1.4.cmml" id="S2.SS1.p4.5.m5.1.1d.cmml" xref="S2.SS1.p4.5.m5.1.1"></share><apply id="S2.SS1.p4.5.m5.1.1.6.cmml" xref="S2.SS1.p4.5.m5.1.1.6"><times id="S2.SS1.p4.5.m5.1.1.6.1.cmml" xref="S2.SS1.p4.5.m5.1.1.6.1"></times><apply id="S2.SS1.p4.5.m5.1.1.6.2.cmml" xref="S2.SS1.p4.5.m5.1.1.6.2"><divide id="S2.SS1.p4.5.m5.1.1.6.2.1.cmml" xref="S2.SS1.p4.5.m5.1.1.6.2.1"></divide><ci id="S2.SS1.p4.5.m5.1.1.6.2.2.cmml" xref="S2.SS1.p4.5.m5.1.1.6.2.2">ğœ†</ci><cn type="integer" id="S2.SS1.p4.5.m5.1.1.6.2.3.cmml" xref="S2.SS1.p4.5.m5.1.1.6.2.3">2</cn></apply><apply id="S2.SS1.p4.5.m5.1.1.6.3.cmml" xref="S2.SS1.p4.5.m5.1.1.6.3"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.6.3.1.cmml" xref="S2.SS1.p4.5.m5.1.1.6.3">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.6.3.2.cmml" xref="S2.SS1.p4.5.m5.1.1.6.3.2">ğ‘</ci><ci id="S2.SS1.p4.5.m5.1.1.6.3.3.cmml" xref="S2.SS1.p4.5.m5.1.1.6.3.3">ğ‘</ci></apply><ci id="S2.SS1.p4.5.m5.1.1.6.4.cmml" xref="S2.SS1.p4.5.m5.1.1.6.4">ğœ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">v_{res}=\lambda/2T_{f}=\lambda/2N_{p}\tau</annotation></semantics></math>,
where <math id="S2.SS1.p4.6.m6.1" class="ltx_Math" alttext="N_{p}" display="inline"><semantics id="S2.SS1.p4.6.m6.1a"><msub id="S2.SS1.p4.6.m6.1.1" xref="S2.SS1.p4.6.m6.1.1.cmml"><mi id="S2.SS1.p4.6.m6.1.1.2" xref="S2.SS1.p4.6.m6.1.1.2.cmml">N</mi><mi id="S2.SS1.p4.6.m6.1.1.3" xref="S2.SS1.p4.6.m6.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.6.m6.1b"><apply id="S2.SS1.p4.6.m6.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.6.m6.1.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p4.6.m6.1.1.2.cmml" xref="S2.SS1.p4.6.m6.1.1.2">ğ‘</ci><ci id="S2.SS1.p4.6.m6.1.1.3.cmml" xref="S2.SS1.p4.6.m6.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.6.m6.1c">N_{p}</annotation></semantics></math> is the number of pulses transmitted during a CPI.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.5" class="ltx_p">With a bandwidth of <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="4GHz" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><mrow id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml"><mn id="S2.SS1.p5.1.m1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.1" xref="S2.SS1.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.1.m1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.1a" xref="S2.SS1.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.1.m1.1.1.4" xref="S2.SS1.p5.1.m1.1.1.4.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.1b" xref="S2.SS1.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.1.m1.1.1.5" xref="S2.SS1.p5.1.m1.1.1.5.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><apply id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1"><times id="S2.SS1.p5.1.m1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p5.1.m1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2">4</cn><ci id="S2.SS1.p5.1.m1.1.1.3.cmml" xref="S2.SS1.p5.1.m1.1.1.3">ğº</ci><ci id="S2.SS1.p5.1.m1.1.1.4.cmml" xref="S2.SS1.p5.1.m1.1.1.4">ğ»</ci><ci id="S2.SS1.p5.1.m1.1.1.5.cmml" xref="S2.SS1.p5.1.m1.1.1.5">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">4GHz</annotation></semantics></math>, center frequency of <math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="77GHz" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><mrow id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml"><mn id="S2.SS1.p5.2.m2.1.1.2" xref="S2.SS1.p5.2.m2.1.1.2.cmml">77</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p5.2.m2.1.1.1" xref="S2.SS1.p5.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.2.m2.1.1.3" xref="S2.SS1.p5.2.m2.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.2.m2.1.1.1a" xref="S2.SS1.p5.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.2.m2.1.1.4" xref="S2.SS1.p5.2.m2.1.1.4.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.2.m2.1.1.1b" xref="S2.SS1.p5.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.2.m2.1.1.5" xref="S2.SS1.p5.2.m2.1.1.5.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><apply id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1"><times id="S2.SS1.p5.2.m2.1.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1.1"></times><cn type="integer" id="S2.SS1.p5.2.m2.1.1.2.cmml" xref="S2.SS1.p5.2.m2.1.1.2">77</cn><ci id="S2.SS1.p5.2.m2.1.1.3.cmml" xref="S2.SS1.p5.2.m2.1.1.3">ğº</ci><ci id="S2.SS1.p5.2.m2.1.1.4.cmml" xref="S2.SS1.p5.2.m2.1.1.4">ğ»</ci><ci id="S2.SS1.p5.2.m2.1.1.5.cmml" xref="S2.SS1.p5.2.m2.1.1.5">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">77GHz</annotation></semantics></math> and a CPI of <math id="S2.SS1.p5.3.m3.1" class="ltx_Math" alttext="40ms" display="inline"><semantics id="S2.SS1.p5.3.m3.1a"><mrow id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml"><mn id="S2.SS1.p5.3.m3.1.1.2" xref="S2.SS1.p5.3.m3.1.1.2.cmml">40</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p5.3.m3.1.1.1" xref="S2.SS1.p5.3.m3.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.3.m3.1.1.3" xref="S2.SS1.p5.3.m3.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.3.m3.1.1.1a" xref="S2.SS1.p5.3.m3.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.3.m3.1.1.4" xref="S2.SS1.p5.3.m3.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.1b"><apply id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1"><times id="S2.SS1.p5.3.m3.1.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1.1"></times><cn type="integer" id="S2.SS1.p5.3.m3.1.1.2.cmml" xref="S2.SS1.p5.3.m3.1.1.2">40</cn><ci id="S2.SS1.p5.3.m3.1.1.3.cmml" xref="S2.SS1.p5.3.m3.1.1.3">ğ‘š</ci><ci id="S2.SS1.p5.3.m3.1.1.4.cmml" xref="S2.SS1.p5.3.m3.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.1c">40ms</annotation></semantics></math>, the RF transmit waveform offers a range resolution of <math id="S2.SS1.p5.4.m4.1" class="ltx_Math" alttext="0.0375m" display="inline"><semantics id="S2.SS1.p5.4.m4.1a"><mrow id="S2.SS1.p5.4.m4.1.1" xref="S2.SS1.p5.4.m4.1.1.cmml"><mn id="S2.SS1.p5.4.m4.1.1.2" xref="S2.SS1.p5.4.m4.1.1.2.cmml">0.0375</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p5.4.m4.1.1.1" xref="S2.SS1.p5.4.m4.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p5.4.m4.1.1.3" xref="S2.SS1.p5.4.m4.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.4.m4.1b"><apply id="S2.SS1.p5.4.m4.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1"><times id="S2.SS1.p5.4.m4.1.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1.1"></times><cn type="float" id="S2.SS1.p5.4.m4.1.1.2.cmml" xref="S2.SS1.p5.4.m4.1.1.2">0.0375</cn><ci id="S2.SS1.p5.4.m4.1.1.3.cmml" xref="S2.SS1.p5.4.m4.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.4.m4.1c">0.0375m</annotation></semantics></math> and a velocity resolution of <math id="S2.SS1.p5.5.m5.1" class="ltx_Math" alttext="0.0487m/s" display="inline"><semantics id="S2.SS1.p5.5.m5.1a"><mrow id="S2.SS1.p5.5.m5.1.1" xref="S2.SS1.p5.5.m5.1.1.cmml"><mrow id="S2.SS1.p5.5.m5.1.1.2" xref="S2.SS1.p5.5.m5.1.1.2.cmml"><mn id="S2.SS1.p5.5.m5.1.1.2.2" xref="S2.SS1.p5.5.m5.1.1.2.2.cmml">0.0487</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p5.5.m5.1.1.2.1" xref="S2.SS1.p5.5.m5.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS1.p5.5.m5.1.1.2.3" xref="S2.SS1.p5.5.m5.1.1.2.3.cmml">m</mi></mrow><mo id="S2.SS1.p5.5.m5.1.1.1" xref="S2.SS1.p5.5.m5.1.1.1.cmml">/</mo><mi id="S2.SS1.p5.5.m5.1.1.3" xref="S2.SS1.p5.5.m5.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.5.m5.1b"><apply id="S2.SS1.p5.5.m5.1.1.cmml" xref="S2.SS1.p5.5.m5.1.1"><divide id="S2.SS1.p5.5.m5.1.1.1.cmml" xref="S2.SS1.p5.5.m5.1.1.1"></divide><apply id="S2.SS1.p5.5.m5.1.1.2.cmml" xref="S2.SS1.p5.5.m5.1.1.2"><times id="S2.SS1.p5.5.m5.1.1.2.1.cmml" xref="S2.SS1.p5.5.m5.1.1.2.1"></times><cn type="float" id="S2.SS1.p5.5.m5.1.1.2.2.cmml" xref="S2.SS1.p5.5.m5.1.1.2.2">0.0487</cn><ci id="S2.SS1.p5.5.m5.1.1.2.3.cmml" xref="S2.SS1.p5.5.m5.1.1.2.3">ğ‘š</ci></apply><ci id="S2.SS1.p5.5.m5.1.1.3.cmml" xref="S2.SS1.p5.5.m5.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.5.m5.1c">0.0487m/s</annotation></semantics></math>. These numbers indicate that this sensor is capable of recognizing fine-grained motion characteristic of ASL.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Experimental Design</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The data were collected in a laboratory setting, where the sensor was placed on a table at an elevation of 0.91 m from the ground. Participants sat on a chair directly facing a computer monitor, which was placed immediately behind the radar system. The monitor was used to relay prompts indicating the signs to be articulated. The radar system was positioned at a distance of 1.5 meters in front of the participant.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">4 fluent ASL signers took part in the IRB-approved study, of whom 2 were Deaf and 2 were CODAs. The experiments included 100 ASL signs, as shown in Table 1, which were selected from the ASL-LEX2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> database to include signs of high frequency,
but not phonologically related to ensure a diverse dataset in terms of both handshapes and sign kinematics. The participants repeated each sign 5 times.
The same experiment was repeated with 12 hearing participants, who did not know sign language. These participants were shown the signs prior to the experiment to familiarize them with the task. During the experiment, immediately prior to recording, the participants were prompted with a video of each sign in isolation, and asked to repeat it. Participants were presented with a random ordering of single signs minimize coarticulation during sign production. A total of 2000 fluent sign samples and 6000 imitation signing samples were collected.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2201.00055/assets/spect_native_imit.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="275" height="382" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Micro-Doppler signatures acquired from fluent (row 1) and imitation (row 2) signers.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">RF Data Pre-Processing</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.3" class="ltx_p">The kinematic properties of signing are captured by the frequency modulations in the phase of the received signal.
Micro-motions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, e.g. rotations and vibrations, result in micro-Doppler (<math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mu D" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">Î¼</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">ğœ‡</ci><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\mu D</annotation></semantics></math>) frequency modulations centered about the main Doppler shift, which is caused by translational motion. Signing results in a time-varying pattern of micro-Doppler frequencies. Each sign generates its own unique patterns, which can be revealed through time-frequency analysis.
The <span id="S2.SS3.p1.3.1" class="ltx_text ltx_font_italic">micro-Doppler signature</span>, or spectrogram, is found from the square modulus of the Short-Time Fourier Transform (STFT) of the continuous-time input signal <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="x(t)" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mrow id="S2.SS3.p1.2.m2.1.2" xref="S2.SS3.p1.2.m2.1.2.cmml"><mi id="S2.SS3.p1.2.m2.1.2.2" xref="S2.SS3.p1.2.m2.1.2.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.2.m2.1.2.1" xref="S2.SS3.p1.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S2.SS3.p1.2.m2.1.2.3.2" xref="S2.SS3.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S2.SS3.p1.2.m2.1.2.3.2.1" xref="S2.SS3.p1.2.m2.1.2.cmml">(</mo><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS3.p1.2.m2.1.2.3.2.2" xref="S2.SS3.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.2.cmml" xref="S2.SS3.p1.2.m2.1.2"><times id="S2.SS3.p1.2.m2.1.2.1.cmml" xref="S2.SS3.p1.2.m2.1.2.1"></times><ci id="S2.SS3.p1.2.m2.1.2.2.cmml" xref="S2.SS3.p1.2.m2.1.2.2">ğ‘¥</ci><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">x(t)</annotation></semantics></math> and can be expressed in terms of the window function, <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="h(t)" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.2" xref="S2.SS3.p1.3.m3.1.2.cmml"><mi id="S2.SS3.p1.3.m3.1.2.2" xref="S2.SS3.p1.3.m3.1.2.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.3.m3.1.2.1" xref="S2.SS3.p1.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S2.SS3.p1.3.m3.1.2.3.2" xref="S2.SS3.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS3.p1.3.m3.1.2.3.2.1" xref="S2.SS3.p1.3.m3.1.2.cmml">(</mo><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS3.p1.3.m3.1.2.3.2.2" xref="S2.SS3.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.2.cmml" xref="S2.SS3.p1.3.m3.1.2"><times id="S2.SS3.p1.3.m3.1.2.1.cmml" xref="S2.SS3.p1.3.m3.1.2.1"></times><ci id="S2.SS3.p1.3.m3.1.2.2.cmml" xref="S2.SS3.p1.3.m3.1.2.2">â„</ci><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">h(t)</annotation></semantics></math>, as</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.4" class="ltx_Math" alttext="S(t,\omega)=\Big{|}\int^{\infty}_{-\infty}h(t-u)x(u)e^{-j\omega t}du\Big{|}^{2}." display="block"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.4.1" xref="S2.E3.m1.4.4.1.1.cmml"><mrow id="S2.E3.m1.4.4.1.1" xref="S2.E3.m1.4.4.1.1.cmml"><mrow id="S2.E3.m1.4.4.1.1.3" xref="S2.E3.m1.4.4.1.1.3.cmml"><mi id="S2.E3.m1.4.4.1.1.3.2" xref="S2.E3.m1.4.4.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.3.1" xref="S2.E3.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E3.m1.4.4.1.1.3.3.2" xref="S2.E3.m1.4.4.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.E3.m1.4.4.1.1.3.3.2.1" xref="S2.E3.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">t</mi><mo id="S2.E3.m1.4.4.1.1.3.3.2.2" xref="S2.E3.m1.4.4.1.1.3.3.1.cmml">,</mo><mi id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">Ï‰</mi><mo stretchy="false" id="S2.E3.m1.4.4.1.1.3.3.2.3" xref="S2.E3.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.4.1.1.2" xref="S2.E3.m1.4.4.1.1.2.cmml">=</mo><msup id="S2.E3.m1.4.4.1.1.1" xref="S2.E3.m1.4.4.1.1.1.cmml"><mrow id="S2.E3.m1.4.4.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.2.cmml"><mo maxsize="160%" minsize="160%" id="S2.E3.m1.4.4.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.2.1.cmml">|</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.cmml"><msubsup id="S2.E3.m1.4.4.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.cmml"><mo lspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">âˆ«</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.2.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.cmml"><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.2.3a" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.cmml">âˆ’</mo><mi mathvariant="normal" id="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.2.cmml">âˆ</mi></mrow><mi mathvariant="normal" id="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.3.cmml">âˆ</mi></msubsup><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">t</mi><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">u</mi></mrow><mo stretchy="false" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2a" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2b" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.5.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.5.2.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml">u</mi><mo stretchy="false" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.5.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2c" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">â€‹</mo><msup id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.2.cmml">e</mi><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.cmml"><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3a" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.cmml">âˆ’</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.1.cmml">â€‹</mo><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.3.cmml">Ï‰</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.1a" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.1.cmml">â€‹</mo><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.4" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.4.cmml">t</mi></mrow></mrow></msup><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2d" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.7" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.cmml"><mo rspace="0em" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.1.cmml">ğ‘‘</mo><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.2.cmml">u</mi></mrow></mrow></mrow><mo maxsize="160%" minsize="160%" id="S2.E3.m1.4.4.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.2.1.cmml">|</mo></mrow><mn id="S2.E3.m1.4.4.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.3.cmml">2</mn></msup></mrow><mo lspace="0em" id="S2.E3.m1.4.4.1.2" xref="S2.E3.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.4.1.1.cmml" xref="S2.E3.m1.4.4.1"><eq id="S2.E3.m1.4.4.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.2"></eq><apply id="S2.E3.m1.4.4.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.3"><times id="S2.E3.m1.4.4.1.1.3.1.cmml" xref="S2.E3.m1.4.4.1.1.3.1"></times><ci id="S2.E3.m1.4.4.1.1.3.2.cmml" xref="S2.E3.m1.4.4.1.1.3.2">ğ‘†</ci><interval closure="open" id="S2.E3.m1.4.4.1.1.3.3.1.cmml" xref="S2.E3.m1.4.4.1.1.3.3.2"><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">ğ‘¡</ci><ci id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">ğœ”</ci></interval></apply><apply id="S2.E3.m1.4.4.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1">superscript</csymbol><apply id="S2.E3.m1.4.4.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1"><abs id="S2.E3.m1.4.4.1.1.1.1.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2"></abs><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1"><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2">superscript</csymbol><int id="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.2"></int><infinity id="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.2.3"></infinity></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.3"><minus id="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.3"></minus><infinity id="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.3.2"></infinity></apply></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1"><times id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2"></times><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.3">â„</ci><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1"><minus id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">ğ‘¢</ci></apply><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4">ğ‘¥</ci><ci id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3">ğ‘¢</ci><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6">superscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.2">ğ‘’</ci><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3"><minus id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3"></minus><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2"><times id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.1"></times><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.2">ğ‘—</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.3">ğœ”</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.4.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.6.3.2.4">ğ‘¡</ci></apply></apply></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.7"><csymbol cd="latexml" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.1">differential-d</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.7.2">ğ‘¢</ci></apply></apply></apply></apply><cn type="integer" id="S2.E3.m1.4.4.1.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">S(t,\omega)=\Big{|}\int^{\infty}_{-\infty}h(t-u)x(u)e^{-j\omega t}du\Big{|}^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Ground clutter from stationary objects, such as furniture and the walls, will appear in the micro-Doppler signature as a band centered around 0 Hz. At 77 GHz, elimination of low-speed signal components during clutter filtering results in performance degradation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, therefore no filtering was applied on the data. Samples of the micro-Doppler signatures for both fluent and imitation ASL users are shown in Fig. <a href="#S2.F1" title="Figure 1 â€£ II-B Experimental Design â€£ II Experimental RF Datasets â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Estimation of Signing Kinematics</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The most relevant kinematic information of a sign can be extracted from the motion associated with the arms and hands. Thus, the prosody of ASL is encoded in the velocity trace of the sign, since this is the simplest interpretation of the motion of the hands and arms of the signer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. In RF sensing, the micro-Doppler signature captures this velocity trace since the Doppler frequency is proportional to velocity. In this section, we describe the processing steps taken to extract three kinematic properties of ASL: hand speed, type of signs (one-handed vs. two-handed), and the number of directionally isolatable components of the sign with the motion toward the radar, termed strokes (including transitions toward and from initial and final handshapes).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Hand Speed</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The speed of signing is measured by tracing the Doppler velocity of the upper and lower envelopes of the micro-Doppler signature. The upper envelope represents the radial velocity of the fastest point moving towards the radar, while the lower envelope gives the speed of the fastest point on the body moving away from the radar. Thus, envelopes provide a means for learning the speed of the hands during signing. Envelopes are extracted by using an energy-based thresholding method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. First, the energy corresponding to each slow time index is computed, for which the first frequency bin whose corresponding
spectrogram amplitude is greater than or equal to a threshold is tagged as an envelope pixel. The threshold is computed as the product of a pre-determined scaling factor and the energy at that slow-time index.
Figure <a href="#S3.F2" title="Figure 2 â€£ III-A Hand Speed â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of spectrograms for fluent and imitation signers, as well as their corresponding upper and lower envelopes.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2201.00055/assets/velocity_extract.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Spectrograms with upper and lower velocity envelope.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2201.00055/assets/average_velocity_native.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="579" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average velocity with standard deviation for one-handed and two-handed fluent ASL signing.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2201.00055/assets/average_velocity_imitation.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="600" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Average velocity with standard deviation for one-handed and two-handed imitation ASL signing.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The average hand speed during articulation of a sign is calculated by taking an average over each upper and lower envelope extracted from each spectrograms. Figure <a href="#S3.F3" title="Figure 3 â€£ III-A Hand Speed â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="Figure 4 â€£ III-A Hand Speed â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show the average hand speed and its standard deviation for fluent and imitations signers for 100 signs. The RF measurements show that the hand speed of imitation signers, on average (0.45 m/s), is greater than that of fluent signers (0.36 m/s), while the standard deviation is much greater. This is reflective of the inconsistency between hearing participants in sign articulations. The greater speed in hand movements of imitation signers may on the one hand seem surprising, as one might think someone less fluent would be more hesitant. But, perhaps in part because hearing imitation signers perceive signing more akin to gesturing, than talking, their articulations are more rushed and sweeping. In contrast, fluent signers articulate the sign within a tighter space, i.e. traverse less distance, but with calculated, precise expression. This results in, on average, slower hand speeds.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Moreover, the RF measurements show that the average hand speed of two-handed signs are greater than that of one-handed signs. This may be in part because two-handed signs typically involve larger movements, while one-handed signs have finer-scale finger movements or hand shapes.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Number of Strokes</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The number of strokes corresponds to the number of times the hands move towards the radar throughout the duration of the isolated sign (i.e. including transition to the initial handshape, and transition after the final handshape). In other words, number of positive peaks in the micro-Doppler signature correspond to the number of strokes. The number of strokes for a sign can be measured by applying peak detection algorithm to the upper envelope. For repetitive motion (reduplicated signs), such as in signs <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">walk</span>, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">water</span>, and <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">shop</span>, imitation signers were likely to err in production kinematics, producing an incorrect number of strokes (a typical error of early sign language learners).
From Figs. <a href="#S3.F3" title="Figure 3 â€£ III-A Hand Speed â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="Figure 4 â€£ III-A Hand Speed â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, it may be observed that as the number of strokes in a sign increases, hand speed also increases.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2201.00055/assets/energy_bsed_sign_type.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="279" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Energy-based sign type identification.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">One-Handed vs. Two-Handed Signs</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Signs in ASL can be one-handed or two-handed. One-handed signs result in less backscatter than two-handed ones. Thus, the average received signal energy for a sign can be indicative of whether the sign involves one or two hands. The total energy of a spectrogram is computed by summing the energy corresponding to each slow-time index.
This process is repeated for each spectrogram and then divided by the number of samples to find the algebraic mean. In this way, the total average energy for all 100 words is calculated. Figure <a href="#S3.F5" title="Figure 5 â€£ III-B Number of Strokes â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the average total energy for one-handed and two-handed signs. Note that energy of two-handed signs is distinctly higher than that of one-handed signs. Thus, a threshold can then be designated for categorizing whether a sign is one-handed or two-handed. we found that a threshold of 0.674 yielded a classification accuracy of 81% for one-handed versus two-handed signs.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">The kinematic errors of real imitation signers can be quantitatively compared with that of fluent signers using the metrics of average speed (<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="V_{h}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">V</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ‘‰</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">V_{h}</annotation></semantics></math>), number of strokes (<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="N_{str}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">N</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1a" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.4" xref="S3.SS3.p2.2.m2.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ğ‘</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><times id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">ğ‘ </ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ğ‘¡</ci><ci id="S3.SS3.p2.2.m2.1.1.3.4.cmml" xref="S3.SS3.p2.2.m2.1.1.3.4">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">N_{str}</annotation></semantics></math>), and handedness detection (<math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="N_{h12}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">N</mi><mrow id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.1" xref="S3.SS3.p2.3.m3.1.1.3.1.cmml">â€‹</mo><mn id="S3.SS3.p2.3.m3.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.cmml">12</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">ğ‘</ci><apply id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"><times id="S3.SS3.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2">â„</ci><cn type="integer" id="S3.SS3.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3">12</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">N_{h12}</annotation></semantics></math>), as shown in Table <a href="#S3.T2" title="TABLE II â€£ III-C One-Handed vs. Two-Handed Signs â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Notice that the deviation in average speeds of imitation signers is greater than that of fluent signers. In addition, the errors in repetitions during the articulation of signs, as indicated by the number of strokes, is also significantly higher for imitation signers. This is consistent with the visual observations of fluent and hearing participants during experiments.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Statistics of real fluent and imitations signer data.</figcaption><img src="/html/2201.00055/assets/real_samples_states.png" id="S3.T2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="335" height="87" alt="[Uncaptioned image]">
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2201.00055/assets/pix2pix_and_cycleGAN.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> Imitation to fluent sign transformation using (a) Pix2Pix and (b) CycleGAN. (c) Examples of transformed signatures.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Adversarial Learning Approaches</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Compilation of large datasets for training state-of-the-art DNNs is difficult when human subjects are involved, due to the time spent in measuring numerous iterations of each class. In previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, ASL recognition using conventional supervised machine learning was explored due to the small amount of available real data: 9 samples per class per sensor, using a total of 5 sensors. The minimum-redundancy maximum-relevance (mRMR) method was used to select 150 handcrafted features for input to a random forest classifier, resulting in a classification accuracy of 72.5% for 20 ASL signs.
Later, a slightly larger dataset was acquired (on average of 40 samples per class per sensor for 3 sensors at different frequencies) to train a DNN for fusion of multi-frequency sensor data to achieve an accuracy of 95% for the same 20 ASL signs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. The limitations in the amount of available real training data also limited the depth and accuracy of the DNNs utilized.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">One approach that has been used in some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is to instead use imitation signing data for both training and testing of algorithms. However, this can lead to over-optimistic results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> due to the differences in production between imitation and fluent signers, which are also captured by the RF sensor measurements as presented in Section III. This is further evidenced by the ability to distinguish between the RF data from fluent versus imitation signers using a support vector machine classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Thus, we wish to emphasize that in this work, all DNNs have been tested on ASL signs articulated only by fluent signers.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The question of whether imitation signing data can be leveraged in any way to train DNNs for ASL recognition of fluent signers is an interesting avenue to explore. Due to the differences in data distribution, direct use of imitation signing data as training data is not effective: when a convolutional neural network (CNN) is trained on imitation signing data and tested on fluent ASL-R dataset, only 24% accuracy is attained <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. One possible remedy could be to use domain adaptation techniques to transform imitation signing data into signatures that better match the distribution of fluent ASL data, as discussed next.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Transformation of Imitation Signing Signatures</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Image translation is a class of computer vision techniques where the goal is to learn a mapping between an input and an output image. A number of image-to-image translation techniques such as Pix2Pix<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, CycleGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and TravelGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> have been proposed in the literature. As CycleGAN has been shown to outperform TravelGAN on RF signatures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, in this work we consider the efficacy of both Pix2Pix and CycleGAN for transformation of imitation signing data. The architectures of both techniques are illustrated in Figure <a href="#S3.F6" title="Figure 6 â€£ III-C One-Handed vs. Two-Handed Signs â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Pix2Pix</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Pix2Pix is a type of conditional GAN (cGAN), where the generation of the output image is conditioned on the input; in this case, a source image. The generator of Pix2Pix uses the U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> architecture. In general, image synthesis architectures take in a random vector as input, project it onto a much higher dimensional vector via a fully connected layer, reshape it, and then apply a series of de-convolutional operations until the desired spatial resolution is achieved. In contrast, the generator of Pix2Pix resembles an auto-encoder. The generator takes in the image to be translated, compresses it into a low-dimensional vector representation, and then learns how to upsample it into the output image.
The generator is trained via adversarial loss, which encourages it to generate plausible images in the target domain. The generator is also updated via an <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">â„“</mi><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">â„“</ci><cn type="integer" id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\ell_{1}</annotation></semantics></math>-loss measured between the generated image and the expected output image. This additional loss encourages the generator model to create plausible translations of the source image.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The architecture of the discriminator is a PatchGAN / Markovian discriminator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> that works by classifying individual (<math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mrow id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p2.1.m1.1.1.2" xref="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.p2.1.m1.1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mi id="S4.SS1.SSS1.p2.1.m1.1.1.3" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><apply id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1"><times id="S4.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.1"></times><ci id="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.2">ğ‘</ci><ci id="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">N\times N</annotation></semantics></math>) patches in the image as â€œreal vs. fake,â€ as opposed to classifying the entire image. This enforces more constraints that encourage sharp high-frequency detail in the output images.
The discriminator is provided both with a source image (in this case, an imitation signing signature) and the target image (fluent signing signature) and must determine whether the target is a plausible transformation of the source image.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">One limitation of Pix2Pix is that since it is a paired image-to-image translation method, the total number of synthetic samples generated is identical to the number of real imitation signing signatures acquired. In this work, a total of 6,000 transformed signatures are synthesized using Pix2Pix.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>CycleGAN</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.8" class="ltx_p">In constrast to Pix2Pix, CycleGAN is a GAN for unpaired image-to-image translation. Thus, a greater amount of synthetic data can be generated than the real imitation samples used at the input of the network. For two domains <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mi id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><ci id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">A</annotation></semantics></math> and <math id="S4.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mi id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><ci id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">B</annotation></semantics></math>, CycleGAN learns two mappings: <math id="S4.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mi id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><ci id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">G</annotation></semantics></math><span id="S4.SS1.SSS2.p1.5.2" class="ltx_text ltx_font_bold ltx_font_italic">:<math id="S4.SS1.SSS2.p1.4.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS1.SSS2.p1.4.1.m1.1a"><mi mathvariant="normal" id="S4.SS1.SSS2.p1.4.1.m1.1.1" xref="S4.SS1.SSS2.p1.4.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.1.m1.1b"><ci id="S4.SS1.SSS2.p1.4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.4.1.m1.1.1">A</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.1.m1.1c">A</annotation></semantics></math>â†’<math id="S4.SS1.SSS2.p1.5.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.SSS2.p1.5.2.m2.1a"><mi mathvariant="normal" id="S4.SS1.SSS2.p1.5.2.m2.1.1" xref="S4.SS1.SSS2.p1.5.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.5.2.m2.1b"><ci id="S4.SS1.SSS2.p1.5.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.5.2.m2.1.1">B</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.5.2.m2.1c">B</annotation></semantics></math></span> and <math id="S4.SS1.SSS2.p1.6.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S4.SS1.SSS2.p1.6.m4.1a"><mi id="S4.SS1.SSS2.p1.6.m4.1.1" xref="S4.SS1.SSS2.p1.6.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.6.m4.1b"><ci id="S4.SS1.SSS2.p1.6.m4.1.1.cmml" xref="S4.SS1.SSS2.p1.6.m4.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.6.m4.1c">F</annotation></semantics></math><span id="S4.SS1.SSS2.p1.8.4" class="ltx_text ltx_font_bold ltx_font_italic">:<math id="S4.SS1.SSS2.p1.7.3.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.SSS2.p1.7.3.m1.1a"><mi mathvariant="normal" id="S4.SS1.SSS2.p1.7.3.m1.1.1" xref="S4.SS1.SSS2.p1.7.3.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.7.3.m1.1b"><ci id="S4.SS1.SSS2.p1.7.3.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.7.3.m1.1.1">B</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.7.3.m1.1c">B</annotation></semantics></math>â†’<math id="S4.SS1.SSS2.p1.8.4.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS1.SSS2.p1.8.4.m2.1a"><mi mathvariant="normal" id="S4.SS1.SSS2.p1.8.4.m2.1.1" xref="S4.SS1.SSS2.p1.8.4.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.8.4.m2.1b"><ci id="S4.SS1.SSS2.p1.8.4.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.8.4.m2.1.1">A</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.8.4.m2.1c">A</annotation></semantics></math></span>. CycleGAN translates an image from a source domain A to a target domain B by forming a series connection between two GANs to form a â€œcycleâ€: the first GAN tries to synthesize â€œfake fluentâ€ ASL data from the imitation signing data, while the second GAN works to reconstruct the original sample, synthesizing â€œfake imitationâ€ ASL samples. Thus, the network tries to minimize the cycle consistency loss, i.e. the difference between the input of the first GAN and the output of second GAN.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Each CycleGAN generator is comprised of three sections: an encoder, a transformer, and a decoder. The input image is fed directly into the encoder, which shrinks the representation size while increasing the number of channels. The encoder is composed of three convolution layers. The resulting activation is passed to the transformer, a series of six residual blocks. It is then expanded again by the decoder, which uses two transpose convolutions to enlarge the representation size, and one output layer to produce the final transformed image.
The discriminators are comprised of PatchGANs - fully convolutional neural networks that look at a â€œpatchâ€ of the input image, and output the probability of the patch being â€œreal.â€ This is both more computationally efficient than trying to look at the entire input image, and is also more effective since it allows the discriminator to focus on more localized features, like texture.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.4.1.1" class="ltx_text">IV-A</span>3 </span>Comparison of Pix2Pix and CycleGAN</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.4" class="ltx_p">Samples of Pix2Pix and CycleGAN transformed signatures are shown in Figure <a href="#S3.F6" title="Figure 6 â€£ III-C One-Handed vs. Two-Handed Signs â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (c). Although the general trends in the signatures are consistent, the Pix2Pix signatures have greater visual resemblance to the signature from fluent ASL users. CycleGAN signatures appear more faded and blurry, especially in regions outside the 0 Hz ground clutter returns. These differences can be quantitatively compared via the kinematic properties of ASL, which can be extracted from RF data as described in Section <a href="#S3" title="III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Table <a href="#S4.T3" title="TABLE III â€£ IV-A3 Comparison of Pix2Pix and CycleGAN â€£ IV-A Transformation of Imitation Signing Signatures â€£ IV Adversarial Learning Approaches â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> lists the mean error and standard deviation of hand speed, <math id="S4.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="V_{h}" display="inline"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><msub id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p1.1.m1.1.1.2" xref="S4.SS1.SSS3.p1.1.m1.1.1.2.cmml">V</mi><mi id="S4.SS1.SSS3.p1.1.m1.1.1.3" xref="S4.SS1.SSS3.p1.1.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><apply id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.2">ğ‘‰</ci><ci id="S4.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">V_{h}</annotation></semantics></math>, as well as the percentage of erroneous samples of strokes, <math id="S4.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="N_{str}" display="inline"><semantics id="S4.SS1.SSS3.p1.2.m2.1a"><msub id="S4.SS1.SSS3.p1.2.m2.1.1" xref="S4.SS1.SSS3.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS3.p1.2.m2.1.1.2" xref="S4.SS1.SSS3.p1.2.m2.1.1.2.cmml">N</mi><mrow id="S4.SS1.SSS3.p1.2.m2.1.1.3" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS3.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS3.p1.2.m2.1.1.3.1" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.SSS3.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS3.p1.2.m2.1.1.3.1a" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.SSS3.p1.2.m2.1.1.3.4" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.2.m2.1b"><apply id="S4.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.2">ğ‘</ci><apply id="S4.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3"><times id="S4.SS1.SSS3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.1"></times><ci id="S4.SS1.SSS3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.2">ğ‘ </ci><ci id="S4.SS1.SSS3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.3">ğ‘¡</ci><ci id="S4.SS1.SSS3.p1.2.m2.1.1.3.4.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1.3.4">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.2.m2.1c">N_{str}</annotation></semantics></math>, and handedness detections, <math id="S4.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="N_{h}" display="inline"><semantics id="S4.SS1.SSS3.p1.3.m3.1a"><msub id="S4.SS1.SSS3.p1.3.m3.1.1" xref="S4.SS1.SSS3.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS3.p1.3.m3.1.1.2" xref="S4.SS1.SSS3.p1.3.m3.1.1.2.cmml">N</mi><mi id="S4.SS1.SSS3.p1.3.m3.1.1.3" xref="S4.SS1.SSS3.p1.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.3.m3.1b"><apply id="S4.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1.2">ğ‘</ci><ci id="S4.SS1.SSS3.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.3.m3.1c">N_{h}</annotation></semantics></math>, for the number of synthetic samples, <math id="S4.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="N_{s}" display="inline"><semantics id="S4.SS1.SSS3.p1.4.m4.1a"><msub id="S4.SS1.SSS3.p1.4.m4.1.1" xref="S4.SS1.SSS3.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS3.p1.4.m4.1.1.2" xref="S4.SS1.SSS3.p1.4.m4.1.1.2.cmml">N</mi><mi id="S4.SS1.SSS3.p1.4.m4.1.1.3" xref="S4.SS1.SSS3.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.4.m4.1b"><apply id="S4.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1.2">ğ‘</ci><ci id="S4.SS1.SSS3.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS3.p1.4.m4.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.4.m4.1c">N_{s}</annotation></semantics></math>. While Pix2Pix can only transform 6,000 samples, CycleGAN is used to generate both 6,000 and 50,000 samples.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of Kinematic Errors in Pix2Pix and CycleGAN Signatures.</figcaption><img src="/html/2201.00055/assets/transformed_signatures_error.png" id="S4.T3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="341" height="106" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">Pix2Pix signatures show better adherence to the kinematic properties of fluent signing than CycleGAN.
Notice that on average, the CycleGAN signatures exhibit more error in hand speed, number of strokes and detection of handedness relative to those generated by Pix2Pix. Increasing the number of generated CycleGAN samples has only a slight detrimental effect on hand speed, while more significant errors are induced in the number of strokes and handedness.</p>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p">Note that there are two different causes for kinematic errors: first, the lack of fluency in the language, and second, the network itself. Let us first consider the reasons for why Pix2Pix significantly outperforms CycleGAN. In prior work, we showed that the generative process creates synthetic data with significant kinematic errors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, which are described more in the next section. Networks generate kinematic errors because RF data is not naturally an image, but converted into a 2D format via time-frequency analysis (Section II-C). Hence, spatial correlations are not based on physical proximity (as in optical images), but on the distribution of velocity across the human body and the constraints imposed by the skeleton. However, the GAN architectures are not supplied with any information or metric pertaining to these constraints, resulting in synthetic samples that bear spatial resemblance, but in fact correspond to physically impossible movement. The CycleGAN architecture includes two generators, in contrast to the single generator of Pix2Pix; hence, the greater the amount of kinematic errors exhibited in the CycleGAN synthesized samples.</p>
</div>
<div id="S4.SS1.SSS3.p4" class="ltx_para">
<p id="S4.SS1.SSS3.p4.1" class="ltx_p">Moreover, imitation signing data itself has significantly more error in average speed as well as the number of strokes, as was shown in Table <a href="#S3.T2" title="TABLE II â€£ III-C One-Handed vs. Two-Handed Signs â€£ III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. That these errors persist in the domain adapted signatures can be seen by observing that the average errors in Pix2Pix and CycleGAN synthesized data remain significantly greater than the levels observed in fluent signing data. In fact, the error in average speed of Pix2Pix data exceeds that observed even in real imitation signing data.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Direct Synthesis of ASL Sign Signatures</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">An alternative to transformation of imitation signatures is to instead use a small amount of real, fluent ASL data as input to a GAN, which generates a larger number of synthetic samples for training.
In our prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, several different types of architectures have been explored for synthetic data synthesis, including auxiliary-conditional GANs (ACGANs), conditional variational autoencoders (CVAE) and WGANs, but all were found to generate data that exhibits significant discrepancies from that of real RF signatures. Examples include</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Disjoint components micro-Doppler:</span> Real micro-Doppler signatures are connected and continuous, because all points on the human body are connected with each other, forming a continuous spread of velocities. This prevents human RF signatures from having disjoint components or regions in the signature.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Leakage between target and non-target components:</span> A benefit of GANs is that sensor-artifacts can also be synthesized, but sometimes this results in leakage (connected segments) between target movements and sensor artifacts or noise, which are not physically possible.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Incorrect shape of signature:</span> When the shape of the micro-Doppler is distorted, with additional peaks, or symmetric reflections about the x-axis, these components correspond to physically impossible movements; e.g., a person whose hand simultaneous moves towards and away from the radar, additional repetitions, or sudden motion back and forth that are not normally part of the sign.</p>
</div>
</li>
</ul>
<p id="S4.SS2.p1.2" class="ltx_p">While these erroneous components may not seem significant visually, they ultimately correspond to kinematically impossible articulations, which, when used as training data, incorrectly trains the DNN and significantly degrades classification accuracy.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">One way to mitigate such problems is to design the GAN so as to enable greater emphasis on preservation of the shape of the envelope. The envelopes correspond to the maximum velocity towards/away from the radar; so, from the standpoint of hand kinematics, the synthetic signatures should conform to, and not exceed the envelope profiles of source data. In prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, a multi-branch GAN (MBGAN) architecture with an additional auxiliary branch in the WGAN discriminator, which took as input the upper envelope, was proposed as a means of ensuring kinematic accuracy when synthesizing micro-Doppler signatures of different ambulatory gaits, such as walking, limping, or taking short steps. However, during production of sign language, the hands may move towards or away from the radar, so both the upper and lower envelopes are important for maintaining critical kinematic features. Hence, in this work, we incorporated two additional auxiliary branches in the discriminator: one that takes the upper envelope as input, and a second that takes the lower envelope as input. The resulting MBGAN with 3-branch discriminator is shown in Figure <a href="#S4.F7" title="Figure 7 â€£ IV-B Direct Synthesis of ASL Sign Signatures â€£ IV Adversarial Learning Approaches â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The generator is comprised of 10 convolutional layers; each layer is followed by batch normalization
with 0.9 momentum and a Rectified Linear Unit (ReLU) activation function. The main branch of the discriminator is
an 8-layer CNN, where each layer is followed by a Leaky-ReLU activation function. Each auxiliary branch is comprised of three 1D-convolutional layers.
The outputs of the dense layers are concatenated with the flattened output of the main discriminator.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2201.00055/assets/MBGAN.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="336" height="463" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Proposed 3-branch discriminator MBGAN.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The kinematic errors incurred in the synthetic signatures generated by WGAN and MBGAN are compared in Table <a href="#S4.T4" title="TABLE IV â€£ IV-B Direct Synthesis of ASL Sign Signatures â€£ IV Adversarial Learning Approaches â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
The both networks used 75% of the fluent signing data as input during training, and the remaining are used for testing.
Notice that the synthetic training data generated by WGAN or MBGAN outperform both Pix2Pix and CycleGAN with respect to generating signatures that have greater kinematic fidelity to fluent signing data. While the kinematic errors in WGAN generated signatures increase as the network generates a greater amount of synthetic samples, the errors in MBGAN signatures are fewer, and constant over sample size - only a slight drop in the accuracy in replicating the correct number of strokes is incurred, from %99.5 to %98. Visually, MBGAN signatures may be observed to have greater resemblance to fluent ASL samples in comparison with the WGAN samples, as shown in Figure <a href="#S4.F8" title="Figure 8 â€£ IV-C Kinematic Sifting â€£ IV Adversarial Learning Approaches â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Note that in comparison, peaks in WGAN signatures are not as clearly constructed, slightly faded, and have envelopes whose shape has some differences from the envelope of the fluent ASL signature.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of Kinematic Errors in WGAN and MBGAN Signatures.</figcaption><img src="/html/2201.00055/assets/kinematics_WGAN_MBGAN.png" id="S4.T4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="330" height="122" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Signatures with kinematic errors may divert the classifier in the wrong direction during the feature extraction, and thereby result in poor recognition performance. Hence, it is important to identify and exclude the incorrect kinematic signatures generated in GANs synthesis. In the next section, several kinematic rules are defined and the synthesized data are sifted by these constraints.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Kinematic Sifting</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Although the 3-branch discriminator MBGAN does have the intended effect of generating signatures with greater kinematic fidelity to fluent ASL, relative to the other networks considered, it is possible that it still generates kinematically unrealistic synthetic samples. Ideally, we wish to generate training data that is statistically independent, diverse, and representative of the range of potential variations within possible articulations of each sign. The presence of kinematically erroneous samples can have a corrupting effect that leads to confusion between different signs. Thus, we seek to remove such samples.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">One way of removing outliers is to determine a boundary in feature space based on the measured, fluent ASL data acquired. Using Principal Component Analysis (PCA) or t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, each sample can be projected to a 3-D feature space and a convex hull encompassing all samples computed. The convex hull thus forms a boundary; any synthetic samples lying beyond this boundary could be excluded from the training dataset as â€œerroneous.â€ However, it is still possible for samples within the convex hull boundary to be kinematically flawed. Instead of relying on the PCA-based convex hull, instead we identify and sift flawed synthetic data based on the following kinematic properties:</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2201.00055/assets/spect_GAN.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="306" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Example of WGAN and MBGAN generated spectrograms.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Rule 1 - Number of strokes:</span>
The number of strokes estimated using the peak detection algorithm described in Section <a href="#S3" title="III Estimation of Signing Kinematics â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> is compared with the number of strokes listed for each sign in Table <a href="#S1.T1" title="TABLE I â€£ I Introduction â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, as given by ASL-LEX. If the detected number of strokes for a synthetically generated signature is incorrect, then this synthetic sample is removed from the dataset. For example, this rule will preclude signatures corresponding to a signer utilizing an incorrect number of repetitions.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.2" class="ltx_p"><span id="S4.I2.i2.p1.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Rule 2 - Total Energy:</span> As energy is related to whether a sign is one handed or two handed, ensuring the synthetic data lies within reasonable energy bounds, given that we know whether the sign is one or two handed, can be an effective criterion. The rule is tested by first finding the average total energy and its standard deviation from the real, fluent ASL data. Then, for each synthetic signature, the total energy is calculated and checked to see whether it falls within <math id="S4.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.I2.i2.p1.1.m1.1a"><mo id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><csymbol cd="latexml" id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">\pm</annotation></semantics></math> <math id="S4.I2.i2.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.I2.i2.p1.2.m2.1a"><mn id="S4.I2.i2.p1.2.m2.1.1" xref="S4.I2.i2.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.2.m2.1b"><cn type="integer" id="S4.I2.i2.p1.2.m2.1.1.cmml" xref="S4.I2.i2.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.2.m2.1c">1</annotation></semantics></math> standard deviation of the average total energy of the real signatures. This is tested on a class-by-class basis. If the criterion holds true then the sample is regarded as kinematically valid, otherwise it is sifted out.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Rule 3 - Envelope Matching:</span> The envelope of the spectograms is a time-series curve and the similarity between curves can be measured by taking into account both the location and ordering of the points along the curve <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Dynamic Time Warping (DTW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> is a commonly used curve matching technique that measures the similarity between two temporal sequences. To apply envelope matching as a kinematic metric , first, for each class, the average DTW distance and standard deviation are calculated from the combinations of all real samples. Then the DTW distance for each synthetic samples are computed with respect to each real samples, from which the average distance is found. Then this average distance is examined whether it falls within <math id="S4.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.I2.i3.p1.1.m1.1a"><mo id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><csymbol cd="latexml" id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">\pm</annotation></semantics></math>1 Standard deviation of the average DTW distance for that class. If it is within the limit then the sample is kinematically valid; otherwise sifted out as kinematically invalid.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Kinematic sifting provides for tighter constraints than the convex hull boundary. Consider synthetic samples for the ASL sign <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_smallcaps">walk</span> projected to a 3D space using t-SNE, as shown in Figure <a href="#S4.F9" title="Figure 9 â€£ IV-C Kinematic Sifting â€£ IV Adversarial Learning Approaches â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The boundary found based on the convex hull derived from the real fluent ASL samples is shown with solid black lines. The synthetic samples are shown as dots. Notice that most fall within the convex hull boundary, while some are outliers. With statistical sifting, only the outliers outside this boundary would be sifted out of the training dataset. However, if we apply the kinematic rules described above, we may see that there are many kinematically invalid samples (shown in red) that remain within the hull. The valid samples (shown in green) form a tight nucleus within the convex hull. Hence, the kinematic rules form a more stringent constraint.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2201.00055/assets/kinematic_vs_statistical_sifting.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="283" height="358" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Kinematically sifted synthetic samples projected on real sampleâ€™s feature space boundary.</figcaption>
</figure>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">Table <a href="#S4.T5" title="TABLE V â€£ IV-C Kinematic Sifting â€£ IV Adversarial Learning Approaches â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> provides a listing of the number of synthetic samples sifted out, <math id="S4.SS3.p5.1.m1.1" class="ltx_Math" alttext="N_{sft}" display="inline"><semantics id="S4.SS3.p5.1.m1.1a"><msub id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml"><mi id="S4.SS3.p5.1.m1.1.1.2" xref="S4.SS3.p5.1.m1.1.1.2.cmml">N</mi><mrow id="S4.SS3.p5.1.m1.1.1.3" xref="S4.SS3.p5.1.m1.1.1.3.cmml"><mi id="S4.SS3.p5.1.m1.1.1.3.2" xref="S4.SS3.p5.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p5.1.m1.1.1.3.1" xref="S4.SS3.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS3.p5.1.m1.1.1.3.3" xref="S4.SS3.p5.1.m1.1.1.3.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p5.1.m1.1.1.3.1a" xref="S4.SS3.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS3.p5.1.m1.1.1.3.4" xref="S4.SS3.p5.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><apply id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p5.1.m1.1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p5.1.m1.1.1.2.cmml" xref="S4.SS3.p5.1.m1.1.1.2">ğ‘</ci><apply id="S4.SS3.p5.1.m1.1.1.3.cmml" xref="S4.SS3.p5.1.m1.1.1.3"><times id="S4.SS3.p5.1.m1.1.1.3.1.cmml" xref="S4.SS3.p5.1.m1.1.1.3.1"></times><ci id="S4.SS3.p5.1.m1.1.1.3.2.cmml" xref="S4.SS3.p5.1.m1.1.1.3.2">ğ‘ </ci><ci id="S4.SS3.p5.1.m1.1.1.3.3.cmml" xref="S4.SS3.p5.1.m1.1.1.3.3">ğ‘“</ci><ci id="S4.SS3.p5.1.m1.1.1.3.4.cmml" xref="S4.SS3.p5.1.m1.1.1.3.4">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">N_{sft}</annotation></semantics></math>, by kinematic rules for the Pix2Pix, CycleGAN, WGAN, and MBGAN networks, and the kinematic errors based on the sifted synthetic datasets. For all synthesis methods, comparison with error metrics reported in Tables II and III
shows that the sifting process reduced the average error in the remaining data. As the number of samples generated increases, kinematic errors only slightly increase. CycleGAN appeared to be the network most prone to errors, with the greatest number of samples failing the kinematic rules, and, hence, was excluded from the final synthetic dataset. In contrast, even after sifting, the proposed MBGAN remains the network that results in synthetic signatures that exhibit the greatest kinematic fidelity to fluent signing data.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Comparison of Kinematic Errors in Signatures After Sifting.</figcaption><img src="/html/2201.00055/assets/kineamtics_after_sifting.png" id="S4.T5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="348" height="153" alt="[Uncaptioned image]">
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">ASL Recognition Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, the resulting classification accuracies obtained using the various methods for synthesizing training data are compared.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">DNN Architectures</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">In previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, Convolutional Auto-encoders (CAEs) were shown to be effective when small, yet reasonable, amounts of real data are available for training, outperforming transfer learning from weights pre-trained using ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> for VGG<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> and Resnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Consequently, in this work, a four-layer convolutional autoencoder (CAE) has been utilized to classify the 100-sign fluent ASL dataset.
CAEs use unsupervised pre-training to initialize the network near a good local minima. In each layer, a filter concatenation technique is employed, in which a filter size of <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">3\times 3</annotation></semantics></math> and <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="9\times 9" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">9</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><times id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">9</cn><cn type="integer" id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">9\times 9</annotation></semantics></math> were concatenated to take advantage of multilevel feature extraction. After training the CAE model, the decoder was removed, and two fully connected
layers with 256 neurons followed by a dropout of 0.55 were added after flattening the output of the encoder. At the output, a softmax layer with 100 nodes was employed for classification. During training, an ADAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> optimizer was utilized, along with a batch size of 16, learning rate of 0.0005 and 30 epochs.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Classification Accuracy</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The classification accuracies obtained using the CAE trained on the various sources of synthetic data are compared in Table <a href="#S5.T6" title="TABLE VI â€£ V-B Classification Accuracy â€£ V ASL Recognition Results â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, while the best performing techniques are compared in Figure <a href="#S5.F10" title="Figure 10 â€£ V-B Classification Accuracy â€£ V ASL Recognition Results â€£ Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> based on the Top-1, Top-3, and Top-5 accuracies. The proposed approach of direct training data synthesis with MBGAN surpasses other conventional approaches by achieving a 77% top-1 accuracy, 89% top-3 accuracy, and 93% top-5 accuracy.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2201.00055/assets/BAR_diag_CAE.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="320" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Comparison of accuracies attained with Pix2Pix, WGAN, and proposed MBGAN methods for synthesizing training data. </figcaption>
</figure>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>100 ASL Signs Recognition Using CAE.</figcaption><img src="/html/2201.00055/assets/CAE_classifier_results.png" id="S5.T6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="338" height="151" alt="[Uncaptioned image]">
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Implications and Discussion</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The most important conclusion we may draw from these results is based on the observation that the resulting classification accuracies are inversely related to the amount of kinematic errors in the synthetic data. The greater the error, the lower the classification accuracy. For all methods, sifting out samples that fail the kinematic rules results in performance improvement.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.2" class="ltx_p">As mentioned earlier in the paper, there are two sources of errors: namely, kinematic errors generated by DNNs used for adaptation and synthesis, and kinematic errors inherent to the original source data. The direct synthesis approach with GANs have the benefit of utilizing fluent signer data in the synthesis process. In contrast, the data synthesized via domain adaptation contains both sources of errors.
Note that the error in average speed reflected in Pix2Pix synthesized samples is <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="0.19m/s" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mrow id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml"><mn id="S5.SS3.p2.1.m1.1.1.2.2" xref="S5.SS3.p2.1.m1.1.1.2.2.cmml">0.19</mn><mo lspace="0em" rspace="0em" id="S5.SS3.p2.1.m1.1.1.2.1" xref="S5.SS3.p2.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S5.SS3.p2.1.m1.1.1.2.3" xref="S5.SS3.p2.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">/</mo><mi id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><divide id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></divide><apply id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2"><times id="S5.SS3.p2.1.m1.1.1.2.1.cmml" xref="S5.SS3.p2.1.m1.1.1.2.1"></times><cn type="float" id="S5.SS3.p2.1.m1.1.1.2.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2.2">0.19</cn><ci id="S5.SS3.p2.1.m1.1.1.2.3.cmml" xref="S5.SS3.p2.1.m1.1.1.2.3">ğ‘š</ci></apply><ci id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">0.19m/s</annotation></semantics></math>, which is greater than that computed from the real data from imitation signers (<math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="0.09m/s" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mrow id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mrow id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml"><mn id="S5.SS3.p2.2.m2.1.1.2.2" xref="S5.SS3.p2.2.m2.1.1.2.2.cmml">0.09</mn><mo lspace="0em" rspace="0em" id="S5.SS3.p2.2.m2.1.1.2.1" xref="S5.SS3.p2.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S5.SS3.p2.2.m2.1.1.2.3" xref="S5.SS3.p2.2.m2.1.1.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p2.2.m2.1.1.1" xref="S5.SS3.p2.2.m2.1.1.1.cmml">/</mo><mi id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><divide id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1"></divide><apply id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2"><times id="S5.SS3.p2.2.m2.1.1.2.1.cmml" xref="S5.SS3.p2.2.m2.1.1.2.1"></times><cn type="float" id="S5.SS3.p2.2.m2.1.1.2.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2.2">0.09</cn><ci id="S5.SS3.p2.2.m2.1.1.2.3.cmml" xref="S5.SS3.p2.2.m2.1.1.2.3">ğ‘š</ci></apply><ci id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">0.09m/s</annotation></semantics></math>), Pix2Pixâ€™s source data. In other words, Pix2Pix cannot compensate for the imitation signing errors, and exhibits additional model-based errors as well.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">In comparing Pix2Pix results with that of WGAN, readers should be reminded that domain adaptation methods predominantly utilize a PatchGAN architecture in the discriminator, which operates on localized patches in the image, while WGAN and MBGAN discriminators operate on the entire image. Both results in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and comparisons we conducted on radar micro-Doppler signatures reveal that operations on patches are more effective than that on the entire image. For example, Pix2Pix with a PatchGAN discriminator generates much crisper and textured synthetic signatures than Pix2Pix with a discriminator operating on the entire image. This is likely because discriminators operating on the entire image cannot model the sharpness of high frequency components in the image as effectively. Modeling high frequencies requires restricting attention to the structure in local image patches through the application of penalties at a patch-scale. Despite utilization of the entire image, rather than patches, in the discriminator, WGAN synthesized signatures exhibited fewer kinematic errors than the domain adaptation networks we considered.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Although the error in average speed in WGAN signatures (<math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="0.13m/s" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mrow id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mrow id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml"><mn id="S5.SS3.p4.1.m1.1.1.2.2" xref="S5.SS3.p4.1.m1.1.1.2.2.cmml">0.13</mn><mo lspace="0em" rspace="0em" id="S5.SS3.p4.1.m1.1.1.2.1" xref="S5.SS3.p4.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S5.SS3.p4.1.m1.1.1.2.3" xref="S5.SS3.p4.1.m1.1.1.2.3.cmml">m</mi></mrow><mo id="S5.SS3.p4.1.m1.1.1.1" xref="S5.SS3.p4.1.m1.1.1.1.cmml">/</mo><mi id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><divide id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1.1"></divide><apply id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2"><times id="S5.SS3.p4.1.m1.1.1.2.1.cmml" xref="S5.SS3.p4.1.m1.1.1.2.1"></times><cn type="float" id="S5.SS3.p4.1.m1.1.1.2.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2.2">0.13</cn><ci id="S5.SS3.p4.1.m1.1.1.2.3.cmml" xref="S5.SS3.p4.1.m1.1.1.2.3">ğ‘š</ci></apply><ci id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">0.13m/s</annotation></semantics></math>) is lower than that of Pix2Pix, it is greater than that of imitation signing. These errors are due to the generation process itself, and can be mitigated through modification of GAN architecture, such as done in the proposed MBGAN. In future work, we plan to explore extensions of the proposed approach (e.g. modifications of GAN architecture and inclusion of envelopes as an auxiliary input, as well as modifications to the loss function to include physics-based loss regularization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>) to adversarial domain adaptation to improve the resulting classification accuracy when imitation signing data is leveraged for model training.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">Another important open question for future work that relates to Explainable AI is to better understand the physical interpretation, i.e. underlying kinematic model, and nature of the diversity seen in GAN-synthesized micro-Doppler signatures. For example, do the variations between synthetic samples correspond to plausible variations within a certain subject profile (physical or linguistic), or span all probable articulations within a class? Improvements to the generation of synthetic data for training will require a better understanding of not just the statistical properties, but also the physical and linguistic properties of the synthetic samples to ensure good model generalization.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although imitation signing has been used in some studies of sign language recognition, imitation signers exhibit significant differences in kinematics of sign production as compared with fluent signers. This results in substantial statistical differences between imitation and fluent ASL data, which has rendered imitation data ineffective when used to train DNNs for fluent signing recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. This work investigates the use of domain adaptation to transform imitation signing samples to have greater resemblance to fluent signing data, and compares the efficacy of this approach with direct generation of synthetic data from fluent signing data. A novel approach to synthetic RF signature generation is proposed, which is shown to generate samples with greater kinematic fidelity than conventional GANs for transformation of imitation signing samples. Proposed kinematic metrics are extracted from RF ASL signatures and used to evaluate GAN-generated synthetic data from a kinematic perspective. The classification results obtained using a CAE were found to be directly proportionate to the kinematic fidelity of the synthetic data. The proposed methods were used to achieve 77% top-1 accuracy, 89% top-3 accuracy, and 93% top-5 accuracy for the recognition of 100 ASL signs.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank Dr. Caroline Kobek-Pezzarossi from Gallaudet University, Washington D.C. and Dr. Dennis Gilliam from AIDB for their support of this research.</p>
</div>
<figure id="Sx1.1" class="ltx_float biography">
<table id="Sx1.1.1" class="ltx_tabular">
<tr id="Sx1.1.1.1" class="ltx_tr">
<td id="Sx1.1.1.1.1" class="ltx_td"><img src="/html/2201.00055/assets/mahbub.jpg" id="Sx1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="[Uncaptioned image]"></td>
<td id="Sx1.1.1.1.2" class="ltx_td">
<span id="Sx1.1.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.1.1.1.2.1.1" class="ltx_p"><span id="Sx1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">M. Mahbubur Rahman</span>  received the B.S. degree in Electronics and Communication Engineering from Khulna University of Engineering and Technology (KUET), Bangladesh, in 2016. He is currently a Ph.D. student in Electrical and Computer Engineering at the University of Alabama (UA), Tuscaloosa, AL, USA, and a research assistant in the UA Laboratory of Computational Intelligence for Radar (CI4R). His research interests include radar signal processing, machine learning, and multi-modal sensing for fall detection and gait analysis, vehicular autonomy and human-computer interaction.</span>
<span id="Sx1.1.1.1.2.1.2" class="ltx_p">M.M. Rahman is a recipient of the UA Graduate Council Fellowship in September 2019 and 3rd place in the Best Student Paper Competition of the IEEE Radar Conference in April 2021.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.2" class="ltx_float biography">
<table id="Sx1.2.1" class="ltx_tabular">
<tr id="Sx1.2.1.1" class="ltx_tr">
<td id="Sx1.2.1.1.1" class="ltx_td"><img src="/html/2201.00055/assets/EvieMalaia.jpg" id="Sx1.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="[Uncaptioned image]"></td>
<td id="Sx1.2.1.1.2" class="ltx_td">
<span id="Sx1.2.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.2.1.1.2.1.1" class="ltx_p"><span id="Sx1.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Evie A. Malaia</span>  received her Ph.D. degree in Computational Linguistics from Purdue University, West Lafayette, in 2004.</span>
<span id="Sx1.2.1.1.2.1.2" class="ltx_p">Formerly a Research Scientist at Indiana University and Purdue University, and an Assistant Professor at the University of Texas at Arlington, she is currently an Associate Professor at the University of Alabama at Tuscaloosa, Department of Communicative Disorders. Her current research interests include neural and physical bases of sign language communication, classification of higher cognitive states, and neural bases of autism spectrum disorders.</span>
<span id="Sx1.2.1.1.2.1.3" class="ltx_p">Dr. Malaia is a recipient of the Ralph E. Powe Award from DOE/ORAU, EurIAS Research Fellowship, EU Marie Curie Senior Research Fellowship, and the APS Award for Teaching and Public Understanding of Psychological Science.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.3" class="ltx_float biography">
<table id="Sx1.3.1" class="ltx_tabular">
<tr id="Sx1.3.1.1" class="ltx_tr">
<td id="Sx1.3.1.1.1" class="ltx_td"><img src="/html/2201.00055/assets/Gurbuz.jpg" id="Sx1.3.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="Sx1.3.1.1.2" class="ltx_td">
<span id="Sx1.3.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.3.1.1.2.1.1" class="ltx_p"><span id="Sx1.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Ali Cafer Gurbuz</span>  received B.S. degree from Bilkent University, Ankara, Turkey, in 2003, in Electrical Engineering, and the M.S. and Ph.D. degrees from Georgia Institute of Technology, Atlanta, GA, USA, in 2005 and 2008, both in Electrical and Computer Engineering. From 2003 to 2009, he researched compressive sensing based computational imaging problems at Georgia Tech. He held faculty positions at TOBB University and University of Alabama between 2009 and 2017 where he pursued an active research program on the development of sparse signal representations, compressive sensing theory and applications, radar and sensor array signal processing, and machine learning. Currently, he is an Assistant Professor at Mississippi State University, Department of Electrical and Computer Engineering, where he is co-director of Information Processing and Sensing (IMPRESS) Lab.</span>
<span id="Sx1.3.1.1.2.1.2" class="ltx_p">Dr. Gurbuz is the recipient of The Best Paper Award for Signal Processing Journal in 2013 and the Turkish Academy of Sciences Best Young Scholar Award in Electrical Engineering in 2014. He has served as an associate editor for several journals such as Digital Signal Processing, EURASIP Journal on Advances in Signal Processing and Physical Communications.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.4" class="ltx_float biography">
<table id="Sx1.4.1" class="ltx_tabular">
<tr id="Sx1.4.1.1" class="ltx_tr">
<td id="Sx1.4.1.1.1" class="ltx_td"><img src="/html/2201.00055/assets/djgriffin.jpg" id="Sx1.4.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="84" height="125" alt="[Uncaptioned image]"></td>
<td id="Sx1.4.1.1.2" class="ltx_td">
<span id="Sx1.4.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.4.1.1.2.1.1" class="ltx_p"><span id="Sx1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Darrin J. Griffin</span>  received the B.S. degree in communication sciences and disorders with a focus on deaf education and the M.A. degree in communication studies in 2004 and 2007, respectively, from The University of Texas at Austin. The Ph.D. degree was completed at The University at Buffalo, SUNY in 2010 in communication with a focus on deceptive communication.</span>
<span id="Sx1.4.1.1.2.1.2" class="ltx_p">From August 2010 to current he has served as a faculty member at The University of Alabama, Department of Communication Studies where he currently teaches and conducts research as an associate professor on topics related to nonverbal communication, deceptive communication, and deafness. Dr. Griffin is fluent in American Sign Language and participates in various forms of community engagement with the Deaf community.</span>
<span id="Sx1.4.1.1.2.1.3" class="ltx_p">Dr. Griffin is recipient of the 2020 College of Communication and Information Sciences Board of Visitors Research Excellence Award; the 2018 Presidentâ€™s Faculty Research Award at The University of Alabama; and a 2018 Premiere Award from The University of Alabama Council on Community-Based Partnerships for research that raised weather awareness and preparedness for the Deaf &amp; hard of hearing community.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.5" class="ltx_float biography">
<table id="Sx1.5.1" class="ltx_tabular">
<tr id="Sx1.5.1.1" class="ltx_tr">
<td id="Sx1.5.1.1.1" class="ltx_td"><img src="/html/2201.00055/assets/crawford4.png" id="Sx1.5.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="80" height="125" alt="[Uncaptioned image]"></td>
<td id="Sx1.5.1.1.2" class="ltx_td">
<span id="Sx1.5.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.5.1.1.2.1.1" class="ltx_p"><span id="Sx1.5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Chris S. Crawford</span>  received the Ph.D. degree in human-centered computing from the University of Florida, Gainesville, FL, USA. He is currently an Assistant Professor at the University of Alabamaâ€™s Department of Computer Science. He directs the Human-Technology Interaction Lab (HTIL). He has investigated multiple systems that provide computer applications and robots with information about a userâ€™s cognitive state. In 2016, he lead the development of a BCI application that was featured in the worldâ€™s first multiparty brain-drone racing event. His current research focuses on computer science education, human-robot interaction, and brain-computer interfaces.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.6" class="ltx_float biography">
<table id="Sx1.6.1" class="ltx_tabular">
<tr id="Sx1.6.1.1" class="ltx_tr">
<td id="Sx1.6.1.1.1" class="ltx_td"><img src="/html/2201.00055/assets/SZGURBUZ.jpg" id="Sx1.6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="124" alt="[Uncaptioned image]"></td>
<td id="Sx1.6.1.1.2" class="ltx_td">
<span id="Sx1.6.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.6.1.1.2.1.1" class="ltx_p"><span id="Sx1.6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Sevgi Z. Gurbuz</span>  (Sâ€™01â€“Mâ€™10â€“SMâ€™17) received the B.S. degree in electrical engineering with minor in mechanical engineering and the M.Eng. degree in electrical engineering and computer science from the Massachusetts Institute of Technology, Cambridge, MA, USA, in 1998 and 2000, respectively, and the Ph.D. degree in electrical and computer engineering from Georgia Institute of Technology, Atlanta, GA, USA, in 2009.</span>
<span id="Sx1.6.1.1.2.1.2" class="ltx_p">From February 2000 to January 2004, she worked as a Radar Signal Processing Research Engineer with the U.S. Air Force Research Laboratory, Sensors Directorate, Rome, NY, USA. Formerly an Assistant Professor in the Department of Electrical-Electronics Engineering at TOBB University, Ankara, Turkey and Senior Research Scientist with the TUBITAK Space Technologies Research Institute, Ankara, Turkey, she is currently an Assistant Professor in the University of Alabama at Tuscaloosa, Department of Electrical and Computer Engineering. Her current research interests include physics-aware machine learning, RF sensor-enabled cyber-physical systems, radar signal processing, sensor networks, human motion recognition for biomedical, automotive autonomy, and human-computer interaction (HCI) applications.</span>
<span id="Sx1.6.1.1.2.1.3" class="ltx_p">Dr. Gurbuz is a recipient of the IEEE Harry Rowe Mimno Award for 2019, 2020 SPIE Rising Researcher Award, EU Marie Curie Research Fellowship, and the 2010 IEEE Radar Conference Best Student Paper Award.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.Â Fasola and M.Â J. Mataric, â€œUsing socially assistive humanâ€“robot
interaction to motivate physical exercise for older adults,â€
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 100, no.Â 8, pp. 2512â€“2526, 2012.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
N.Â CÃ©spedes, M.Â MÃºnera, C.Â GÃ³mez, and C.Â A. Cifuentes, â€œSocial
human-robot interaction for gait rehabilitation,â€ <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Trans on Neural
Systems and Rehabilitation Engg.</em>, vol.Â 28, no.Â 6, pp. 1299â€“1307, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R.Â Zhang, S.Â He, X.Â Yang, X.Â Wang, K.Â Li, Q.Â Huang, Z.Â Yu,
X.Â Zhang, D.Â Tang, and Y.Â Li, â€œAn EOG-Based humanâ€“machine
interface to control a smart home environment for patients with severe spinal
cord injuries,â€ <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical Engineering</em>, vol.Â 66,
no.Â 1, pp. 89â€“100, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.Â MuÃ±oz-Ferreras, Z.Â Peng, R.Â GÃ³mez-GarcÃ­a, and C.Â Li, â€œReview on
advanced short-range multimode continuous-wave radar architectures for
healthcare applications,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Electromagnetics, RF and
Microwaves in Medicine and Biology</em>, vol.Â 1, no.Â 1, pp. 14â€“25, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M.Â G. Amin, Y.Â D. Zhang, F.Â Ahmad, and K.Â C.Â D. Ho, â€œRadar signal
processing for elderly fall detection: The future for in-home monitoring,â€
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, vol.Â 33, no.Â 2, pp. 71â€“80, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.Â K. Seifert, M.Â G. Amin, and A.Â M. Zoubir, â€œToward unobtrusive in-home
gait analysis based on radar micro-doppler signatures,â€ <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Biomedical Engineering</em>, vol.Â 66, no.Â 9, pp. 2629â€“2640,
2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.Â S. Lee, P.Â N. Pathirana, C.Â L. Steinfort, and T.Â Caelli,
â€œMonitoring and analysis of respiratory patterns using microwave doppler
radar,â€ <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Translational Engineering in Health and
Medicine</em>, vol.Â 2, pp. 1â€“12, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E.Â G. Ziganshin, M.Â A. Numerov, and S.Â A. Vygolov, â€œUWB baby
monitor,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2010 5th International Confernce on Ultrawideband and
Ultrashort Impulse Signals</em>, 2010, pp. 159â€“161.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.Â Gurbuz, A.Â Gurbuz, C.Â Crawford, and D.Â Griffin, â€œRadar-based methods and
apparatus for communication and interpretation of sign languages,â€ in
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">U.S. Patent Application No. US2020/0334452 (Invention Disclosure filed
Feb. 2018; Provisional Patent App. filed Apr. 2019.)</em>, October 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S.Â Z. Gurbuz, A.Â C. Gurbuz, E.Â A. Malaia, D.Â J. Griffin, C.Â S.
Crawford, M.Â M. Rahman, E.Â Kurtoglu, R.Â Aksu, T.Â Macks, and
R.Â Mdrafi, â€œAmerican sign language recognition using rf sensing,â€
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, vol.Â 21, no.Â 3, pp. 3763â€“3775, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
V.Â E. Kosmidou and L.Â J. Hadjileontiadis, â€œSign language recognition using
intrinsic-mode sample entropy on semg and accelerometer data,â€ <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Biomedical Engineering</em>, vol.Â 56, no.Â 12, pp. 2879â€“2890,
2009.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.Â Li, X.Â Chen, X.Â Zhang, K.Â Wang, and Z.Â J. Wang, â€œA
sign-component-based framework for chinese sign language recognition using
accelerometer and semg data,â€ <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical
Engineering</em>, vol.Â 59, no.Â 10, pp. 2695â€“2704, 2012.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
O.Â Koller, N.Â C. Camgoz, H.Â Ney, and R.Â Bowden, â€œWeakly supervised
learning with multi-stream cnn-lstm-hmms to discover sequential parallelism
in sign language videos,â€ <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em>, vol.Â 42, no.Â 9, pp. 2306â€“2320, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C.Â Sun, T.Â Zhang, B.Â Bao, C.Â Xu, and T.Â Mei, â€œDiscriminative
exemplar coding for sign language recognition with kinect,â€ <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Cybernetics</em>, vol.Â 43, no.Â 5, pp. 1418â€“1428, 2013.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â Mittal, P.Â Kumar, P.Â P. Roy, R.Â Balasubramanian, and B.Â B.
Chaudhuri, â€œA modified lstm model for continuous sign language recognition
using leap motion,â€ <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, vol.Â 19, no.Â 16, pp.
7056â€“7063, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
V.Â Chen, <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">The Micro-Doppler Effect in Radar, 2nd Ed.</em>Â Â Â Boston: Artech House, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B.Â Vandersmissen, N.Â Knudde, A.Â Jalalvand, I.Â Couckuyt, A.Â Bourdoux,
W.Â DeÂ Neve, and T.Â Dhaene, â€œIndoor person identification using a low-power
FMCW radar,â€ <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. on Geoscience and Remote Sensing</em>, vol.Â PP,
pp. 1â€“12, 04 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A.Â Sevgi Z. Gurbuz, Carmine Clemente and John J. Soraghan,
â€œMicro-doppler-based in-home aided and unaided walking recognition with
multiple radar and sonar systems,â€ <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IET Radar, Sonar &amp; Navigation</em>,
vol.Â 11, pp. 107â€“115(8), January 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A.Â Arbabian, S.Â Callender, S.Â Kang, M.Â Rangwala, and A.Â Niknejad, â€œA 94 GHz
mm-wave-to-baseband pulsed-radar transceiver with applications in imaging and
gesture recognition,â€ <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Solid-State Circuits, IEEE Journal of</em>, vol.Â 48,
pp. 1055â€“1071, 04 2013.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z.Â Wang, Z.Â Yu, X.Â Lou, B.Â Guo, and L.Â Chen, â€œGesture-radar: A dual
doppler radar based system for robust recognition and quantitative profiling
of human gestures,â€ <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Trans on Human-Machine Systems</em>, vol.Â 51,
no.Â 1, pp. 32â€“43, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S.Â Z. Gurbuz, A.Â C. Gurbuz, E.Â A. Malaia, D.Â J. Griffin, C.Â S. Crawford, M.Â M.
Rahman, E.Â Kurtoglu, R.Â Aksu, T.Â Macks, and R.Â Mdrafi, â€œAmerican sign
language recognition using rf sensing,â€ <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>,
vol.Â 21, no.Â 3, pp. 3763â€“3775, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S.Â Z. Gurbuz and M.Â G. Amin, â€œRadar-based human-motion recognition with
deep learning: Promising applications for indoor monitoring,â€ <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE
Signal Processing Magazine</em>, vol.Â 36, no.Â 4, pp. 16â€“28, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D.Â Bragg and e.Â Koller, â€œSign language recognition, generation, and
translation: An interdisciplinary perspective,â€ in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">The 21st Int. ACM
SIGACCESS Conference on Computers and Accessibility</em>, 2019, p. 16â€“31.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B.Â Fang, J.Â Co, and M.Â Zhang, â€œDeepasl: Enabling ubiquitous and non-intrusive
word and sentence-level sign language translation,â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 15th ACM Conference on Embedded Network Sensor Systems</em>, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y.Â Ma, G.Â Zhou, S.Â Wang, H.Â Zhao, and W.Â Jung, â€œSignfi: Sign language
recognition using wifi,â€ <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Interact. Mob. Wearable Ubiquitous
Technol.</em>, vol.Â 2, no.Â 1, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.Â S. Beal and K.Â Faniel, â€œHearing l2 sign language learners: How do they
perform on asl phonological fluency?â€ <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Sign Language Studies</em>, vol.Â 19,
no.Â 2, pp. 204â€“224, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S.Â Z. Gurbuz, A.Â C. Gurbuz, E.Â A. Malaia, D.Â J. Griffin, C.Â Crawford,
M.Â M. Rahman, R.Â Aksu, E.Â Kurtoglu, R.Â Mdrafi, A.Â Anbuselvam,
T.Â Macks, and E.Â Ozcelik, â€œA linguistic perspective on radar
micro-doppler analysis of american sign language,â€ in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Radar Conference (RADAR)</em>, 2020, pp. 232â€“237.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R.Â Li, C.Â H. Wu, S.Â Liu, J.Â Wang, G.Â Wang, G.Â Liu, and B.Â Zeng,
â€œSdp-gan: Saliency detail preservation generative adversarial networks for
high perceptual quality style transfer,â€ <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. on Image
Processing</em>, vol.Â 30, pp. 374â€“385, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X.Â Gao, Y.Â Tian, and Z.Â Qi, â€œRpd-gan: Learning to draw realistic
paintings with generative adversarial network,â€ <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Trans on Image
Processing</em>, vol.Â 29, pp. 8706â€“8720, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X.Â Pan, M.Â Zhang, D.Â Ding, and M.Â Yang, â€œA geometrical perspective on
image style transfer with adversarial learning,â€ <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. on PAMI</em>,
pp. 1â€“1, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.-Y. Liu, T.Â Breuel, and J.Â Kautz, â€œUnsupervised image-to-image translation
networks,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
P.Â Isola, J.-Y. Zhu, T.Â Zhou, and A.Â A. Efros, â€œImage-to-image translation
with conditional adversarial networks,â€ <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE CVPR</em>, pp. 5967â€“5976,
2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T.Â Park, P.Â Isola, and A.Â A. Efros, â€œUnpaired image-to-image
translation using cycle-consistent adversarial networks,â€ <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE ICCV</em>,
pp. 2242â€“2251, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T.Â Kim, M.Â Cha, H.Â Kim, J.Â Lee, and J.Â Kim, â€œLearning to discover cross-domain
relations with generative adversarial networks,â€ in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z.Â Yi, H.Â Zhang, P.Â Tan, and M.Â Gong, â€œDualgan: Unsupervised dual learning for
image-to-image translation,â€ <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE ICCV</em>, pp. 2868â€“2876, 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
M.Â Amodio and S.Â Krishnaswamy, â€œTravelgan: Image-to-image translation by
transformation vector learning,â€ <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE CVPR)</em>, pp. 8975â€“8984, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M.Â S. Seyfioglu, B.Â Erol, S.Â Z. Gurbuz, and M.Â G. Amin, â€œDNN
transfer learning from diversified micro-doppler for motion classification,â€
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Trans on AES</em>, vol.Â 55, no.Â 5, pp. 2164â€“2180, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
B.Â Erol, S.Â Z. Gurbuz, and M.Â G. Amin, â€œMotion classification using
kinematically sifted acgan-synthesized radar micro-doppler signatures,â€
<em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IEEE Trans on AES</em>, vol.Â 56, no.Â 4, pp. 3197â€“3213, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
â€”â€”, â€œSynthesis of micro-doppler signatures for abnormal gait using
multi-branch discriminator with embedded kinematics,â€ in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE Radar
Conf.</em>, 2020, pp. 175â€“179.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
L.Â vanÂ der Maaten and G.Â Hinton, â€œVisualizing data using t-sne,â€
<em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, vol.Â 9, no.Â 86, pp. 2579â€“2605,
2008.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M.Â M. Rahman, S.Â Z. Gurbuz, and M.Â G. Amin, â€œPhysics-aware design of
multi-branch gan for human rf micro-doppler signature synthesis,â€ in
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE Radar Conf.</em>, 2021, pp. 1â€“6.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
E.Â Malaia and R.Â B. Wilbur, â€œKinematic signatures of telic and atelic events
in asl predicates,â€ <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Language and speech</em>, vol.Â 55, no.Â 3, pp.
407â€“421, 2012.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
E.Â A. Malaia and R.Â B. Wilbur, â€œSyllable as a unit of information transfer in
linguistic communication: The entropy syllable parsing model,â€ <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Wiley
Interdisciplinary Reviews: Cognitive Science</em>, vol.Â 11, no.Â 1, p. e1518,
2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
N.Â Caselli, Z.Â Sehyr, A.Â Cohen-Goldberg, and K.Â Emmorey, â€œAsl-lex: A lexical
database of american sign language,â€ <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Behavior Research Methods</em>,
vol.Â 49, 05 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M.Â Jankiraman, B.Â J. Wessels, and P.Â van Genderen, â€œDesign of a
multifrequency fmcw radar,â€ in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">1998 28th European Microwave
Conference</em>, vol.Â 1, 1998, pp. 584â€“589.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J.Â B. Keller, â€œGeometrical theory of diffraction<math id="bib.bib46.1.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="bib.bib46.1.m1.1a"><mo id="bib.bib46.1.m1.1.1" xref="bib.bib46.1.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="bib.bib46.1.m1.1b"><ci id="bib.bib46.1.m1.1.1.cmml" xref="bib.bib46.1.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib46.1.m1.1c">\ast</annotation></semantics></math>,â€ <em id="bib.bib46.2.1" class="ltx_emph ltx_font_italic">J. Opt. Soc.
Am.</em>, vol.Â 52, no.Â 2, pp. 116â€“130, 1962.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
P.Â van Dorp and F.Â Groen, â€œHuman walking estimation with radar,â€ <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">IET
Radar, Sonar and Navigation</em>, vol. 150, pp. 356â€“365(9), 2003.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
M.Â Richards.Â Â Â McGraw-Hill Education,
2014.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
R.Â Wilbur and A.Â M. Martnez, â€œPhysical correlates of prosodic structure in
american sign language,â€ <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Chicago Linguistic Society</em>, vol.Â 38, pp.
693â€“704, April 2002.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S.Â Z. Gurbuz, M.Â MahbuburÂ Rahman, E.Â Kurtoglu, E.Â Malaia, A.Â C. Gurbuz, D.Â J.
Griffin, and C.Â Crawford, â€œMulti-frequency rf sensor fusion for word-level
fluent asl recognition,â€ <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, pp. 1â€“1, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M.Â Rahman, R.Â Mdrafi, A.Â Gurbuz, E.Â Malaia, C.Â Crawford, D.Â Griffin, and
S.Â Gurbuz, â€œWord-level sign language recognition using linguistic adaptation
of 77 GHz FMCW radar data,â€ in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Radar Conference</em>, May
2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
O.Â Ronneberger, P.Â Fischer, and T.Â Brox, â€œU-net: Convolutional networks for
biomedical image segmentation,â€ 2015.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
C.Â Li and M.Â Wand, â€œPrecomputed real-time texture synthesis with markovian
generative adversarial networks,â€ <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1604.04382, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
M.Â G. Amin, Z.Â Zeng, and T.Â Shan, â€œArm motion classification using curve
matching of maximum instantaneous doppler frequency signatures,â€ in
<em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">IEEE International Radar Conference</em>, 2020, pp. 303â€“308.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
D.Â J. Berndt and J.Â Clifford, â€œUsing dynamic time warping to find patterns
in time serie,â€ in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">in KDD workshop, Seattle, WA</em>, 1994.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M.Â S. SeyfioÄŸlu and S.Â Z. GÃ¼rbÃ¼z, â€œDeep neural network initialization
methods for micro-doppler classification with low training sample support,â€
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IGERS Letters</em>, vol.Â 14, no.Â 12, pp. 2462â€“2466, 2017.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
J.Â Deng, W.Â Dong, R.Â Socher, L.-J. Li, K.Â Li, and L.Â Fei-Fei, â€œImagenet: A
large-scale hierarchical image database,â€ in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE CVPR</em>, 2009, pp.
248â€“255.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
K.Â Simonyan and A.Â Zisserman, â€œVery deep convolutional networks for
large-scale image recognition,â€ in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">3rd Int Conf on Learning
Representations, ICLR 2015</em>, Y.Â Bengio and Y.Â LeCun, Eds.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image
recognition,â€ <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1512.03385, 2015.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
H.Â Sun, L.Â Gu, and B.Â Sun, â€œAdathm: Adaptive gradient method based on
estimates of third-order moments,â€ in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Fourth International
Conference on Data Science in Cyberspace (DSC)</em>, 2019, pp. 361â€“366.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.00054" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.00055" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.00055">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.00055" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.00056" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 10:23:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
