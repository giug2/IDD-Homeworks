<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.15932] NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data</title><meta property="og:description" content="Federated learning (FL), a privacy-preserving distributed machine learning, has been rapidly applied in wireless communication networks. FL enables Internet of Things (IoT) clients to obtain well-trained models while p‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.15932">

<!--Generated on Wed Feb 28 22:29:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Kangning Yin<sup id="id11.2.id1" class="ltx_sup"><span id="id11.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhen Ding<sup id="id12.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhihua Dong<sup id="id13.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dongsheng Chen<sup id="id14.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Fu<sup id="id15.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinhui Ji<sup id="id16.2.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guangqiang Yin<sup id="id17.2.id1" class="ltx_sup"><span id="id17.2.id1.1" class="ltx_text ltx_font_italic">1,‚àó</span></sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiguo Wang<sup id="id18.4.id1" class="ltx_sup"><span id="id18.4.id1.1" class="ltx_text ltx_font_italic">1,‚àó</span></sup>
<sup id="id19.5.id2" class="ltx_sup">1</sup>School of Information and Software Engineering, University of Electronic Science and Technology of China, China.
<br class="ltx_break"><sup id="id20.6.id3" class="ltx_sup">2</sup>Institute of Public Security, Kash Institute of Electronics and Information Industry, China.
<br class="ltx_break">knyin@std.uestc.edu.cn,
zhending@std.uestc.edu.cn,
zhdong@std.uestc.edu.cn,
chendongsheng@std.uestc.edu.cn,
fujie@std.uestc.edu.cn,
xinhj9981@163.com,
yingq@uestc.edu.cn,
zgwang@uestc.edu.cn
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id21.id1" class="ltx_p">Federated learning (FL), a privacy-preserving distributed machine learning, has been rapidly applied in wireless communication networks. FL enables Internet of Things (IoT) clients to obtain well-trained models while preventing privacy leakage. Person detection can be deployed on edge devices with limited computing power if combined with FL to process the video data directly at the edge. However, due to the different hardware and deployment scenarios of different cameras, the data collected by the camera present non-independent and identically distributed (non-IID), and the global model derived from FL aggregation is less effective. Meanwhile, existing research lacks public data set for real-world FL object detection, which is not conducive to studying the non-IID problem on IoT cameras. Therefore, we open source a non-IID IoT person detection (NIPD) data set, which is collected from five different cameras. To our knowledge, this is the first true device-based non-IID person detection data set. Based on this data set, we explain how to establish a FL experimental platform and provide a benchmark for non-IID person detection. NIPD<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/ShenSuanZiZhen/NIID_Person_Detection</span></span></span> is expected to promote the application of FL and the security of smart city.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As one of the big data analysis methods, machine learning (ML) has surpassed human performance in many fields, such as communication, security, and industrial manufacture. In wireless communications and networks, it is a forward-looking concept to combine Internet of Things (IoT) devices with artificial intelligence to form Artificial Intelligence Internet of Things (AIoT). AIoT can collect a large amount of data, and IoT is data-driven and intelligently connected through the analysis and processing of data. However, while IoT devices are capable of collecting large amounts of data, strict data privacy acts allow for stricter protection of data in the IoT and form isolated islands of data from device to device. The inaccessibility of private data and the vast communication overhead require to transfer the raw data to a central ML processor, which is difficult to apply traditional ML algorithms in wireless communications <cite class="ltx_cite ltx_citemacro_cite">Niknam <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address the issues of feature distribution and data privacy, federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> has been proposed. FL is a distributed collaborative computing framework, as shown in Fig. 1. This framework provides a valuable solution for implementing artificial intelligence algorithms on these devices and enabling network edge intelligence in future sixth generation (6G) systems <cite class="ltx_cite ltx_citemacro_cite">Zhao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. The framework keeps data on edge devices that can locally implement image acquisition and training tasks, then transfers training parameters to the central server for aggregation. Replacing raw data transfer with shared model parameters significantly reduces communication overhead.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With the rapid development of deep learning and computing power, person detection has been widely used in real-world security. Person detection mainly relies on a large number of person image data to learn the characteristics of people. Due to the single environment of data collected by a single monitoring device, the trained model effect is not necessarily available in other real-world scenarios. FL can help train models to effectively adapt to changes in these systems while maintaining user privacy <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. Therefore, there has been a general trend to introduce FL in computer vision for IoT. Usually, the underlying assumption in FL is based on independent and identically distributed (IID) data. However, the assumption is difficult to hold in the real world. Data in the real world is often collected from different devices, which are not uniformly distributed. Due to the highly dynamic environment and person behavior in wireless communication networks, the collected data is non-IID. The minimum empirical error model obtained on the non-IID training set does not necessarily perform well on the testing set.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Moreover, there is a lack of real available data sets in the IoT, and most of the existing experiments <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> for non-IID data are based on MNIST and CIFAR-10. A non-IID classification data set named NICO, containing animals and vehicles, is provided in <cite class="ltx_cite ltx_citemacro_cite">He <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>, but is not practical in the computer vision domain of IoT. The object detection data sets Street_5 and Street_20 <cite class="ltx_cite ltx_citemacro_cite">Luo <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> are somewhat segmented by device, but the amount of data is too small to validate the non-IID pervasiveness laws well. <cite class="ltx_cite ltx_citemacro_cite">Chiu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Amit and Mohan (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite> conducted studies related to non-IID object detection, but did not open their data sets for other researchers to study. Therefore, data sets that can be applied for real-world IoT FL are urgently needed.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we propose a non-IID person detection data set: non-IID IoT person detection (NIPD). NIPD is generated from multiple cameras, and the labels of the data are first automatically filtered and labeled by an algorithm, and then manually filled in and accurately labeled. We evaluate the person distribution in the data set and there was significant variability in the distribution of data under each camera. Based on the FedVision <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, we validate this data set using two classical two-stage and one-stage object detection algorithms, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">Ren <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2015</a>)</cite> and YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">Redmon and Farhadi (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>. The results show that our NIPD is suitable for the IoT FL, providing a benchmark for non-IID person detection research in the monitoring field. The data set will facilitate research on non-IID IoT FL issues and promote computer vision security for smart city.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.15932/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated learning in the wireless network IoT.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The rest of the paper is structured as follows. In Section II, we describe the basic concepts behind the proposed experimental design, and in Section III, we detail the experimental platform‚Äôs practical implementation and experimental specification. Section IV describes NIPD and provides two experimental setups. Finally, Section V concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Experimental platform design</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first introduce the classification and impact of non-IID data. Then, we focus on how to design and build an experimental platform for IoT FL with non-IID data.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Classification and impact of non-independently identically distributed data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">In a real-world FL environment, the data between participants is characterized by a high degree of heterogeneity and large gaps in data volume. Therefore, it is possible that the data between participants in different contexts are entirely different. Suppose the data sample is <math id="S2.SS1.p1.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S2.SS1.p1.1.m1.2a"><mrow id="S2.SS1.p1.1.m1.2.3.2" xref="S2.SS1.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.2.3.2.1" xref="S2.SS1.p1.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">x</mi><mo id="S2.SS1.p1.1.m1.2.3.2.2" xref="S2.SS1.p1.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.p1.1.m1.2.3.2.3" xref="S2.SS1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.2b"><interval closure="open" id="S2.SS1.p1.1.m1.2.3.1.cmml" xref="S2.SS1.p1.1.m1.2.3.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ùë•</ci><ci id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.2c">(x,y)</annotation></semantics></math>, where <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">x</annotation></semantics></math> is the input attribute or feature and <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">y</annotation></semantics></math> is the label. For non-IID data, the local data distribution of terminal device <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">k</annotation></semantics></math> is assumed to be <math id="S2.SS1.p1.5.m5.2" class="ltx_Math" alttext="p_{\text{k}}(x,y)" display="inline"><semantics id="S2.SS1.p1.5.m5.2a"><mrow id="S2.SS1.p1.5.m5.2.3" xref="S2.SS1.p1.5.m5.2.3.cmml"><msub id="S2.SS1.p1.5.m5.2.3.2" xref="S2.SS1.p1.5.m5.2.3.2.cmml"><mi id="S2.SS1.p1.5.m5.2.3.2.2" xref="S2.SS1.p1.5.m5.2.3.2.2.cmml">p</mi><mtext id="S2.SS1.p1.5.m5.2.3.2.3" xref="S2.SS1.p1.5.m5.2.3.2.3a.cmml">k</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.2.3.1" xref="S2.SS1.p1.5.m5.2.3.1.cmml">‚Äã</mo><mrow id="S2.SS1.p1.5.m5.2.3.3.2" xref="S2.SS1.p1.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.5.m5.2.3.3.2.1" xref="S2.SS1.p1.5.m5.2.3.3.1.cmml">(</mo><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">x</mi><mo id="S2.SS1.p1.5.m5.2.3.3.2.2" xref="S2.SS1.p1.5.m5.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p1.5.m5.2.2" xref="S2.SS1.p1.5.m5.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.p1.5.m5.2.3.3.2.3" xref="S2.SS1.p1.5.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.2b"><apply id="S2.SS1.p1.5.m5.2.3.cmml" xref="S2.SS1.p1.5.m5.2.3"><times id="S2.SS1.p1.5.m5.2.3.1.cmml" xref="S2.SS1.p1.5.m5.2.3.1"></times><apply id="S2.SS1.p1.5.m5.2.3.2.cmml" xref="S2.SS1.p1.5.m5.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.2.3.2.1.cmml" xref="S2.SS1.p1.5.m5.2.3.2">subscript</csymbol><ci id="S2.SS1.p1.5.m5.2.3.2.2.cmml" xref="S2.SS1.p1.5.m5.2.3.2.2">ùëù</ci><ci id="S2.SS1.p1.5.m5.2.3.2.3a.cmml" xref="S2.SS1.p1.5.m5.2.3.2.3"><mtext mathsize="70%" id="S2.SS1.p1.5.m5.2.3.2.3.cmml" xref="S2.SS1.p1.5.m5.2.3.2.3">k</mtext></ci></apply><interval closure="open" id="S2.SS1.p1.5.m5.2.3.3.1.cmml" xref="S2.SS1.p1.5.m5.2.3.3.2"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">ùë•</ci><ci id="S2.SS1.p1.5.m5.2.2.cmml" xref="S2.SS1.p1.5.m5.2.2">ùë¶</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.2c">p_{\text{k}}(x,y)</annotation></semantics></math>, <math id="S2.SS1.p1.6.m6.2" class="ltx_Math" alttext="p_{\text{k}}(x,y)" display="inline"><semantics id="S2.SS1.p1.6.m6.2a"><mrow id="S2.SS1.p1.6.m6.2.3" xref="S2.SS1.p1.6.m6.2.3.cmml"><msub id="S2.SS1.p1.6.m6.2.3.2" xref="S2.SS1.p1.6.m6.2.3.2.cmml"><mi id="S2.SS1.p1.6.m6.2.3.2.2" xref="S2.SS1.p1.6.m6.2.3.2.2.cmml">p</mi><mtext id="S2.SS1.p1.6.m6.2.3.2.3" xref="S2.SS1.p1.6.m6.2.3.2.3a.cmml">k</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.2.3.1" xref="S2.SS1.p1.6.m6.2.3.1.cmml">‚Äã</mo><mrow id="S2.SS1.p1.6.m6.2.3.3.2" xref="S2.SS1.p1.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.2.3.3.2.1" xref="S2.SS1.p1.6.m6.2.3.3.1.cmml">(</mo><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">x</mi><mo id="S2.SS1.p1.6.m6.2.3.3.2.2" xref="S2.SS1.p1.6.m6.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p1.6.m6.2.2" xref="S2.SS1.p1.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.p1.6.m6.2.3.3.2.3" xref="S2.SS1.p1.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.2b"><apply id="S2.SS1.p1.6.m6.2.3.cmml" xref="S2.SS1.p1.6.m6.2.3"><times id="S2.SS1.p1.6.m6.2.3.1.cmml" xref="S2.SS1.p1.6.m6.2.3.1"></times><apply id="S2.SS1.p1.6.m6.2.3.2.cmml" xref="S2.SS1.p1.6.m6.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.2.3.2.1.cmml" xref="S2.SS1.p1.6.m6.2.3.2">subscript</csymbol><ci id="S2.SS1.p1.6.m6.2.3.2.2.cmml" xref="S2.SS1.p1.6.m6.2.3.2.2">ùëù</ci><ci id="S2.SS1.p1.6.m6.2.3.2.3a.cmml" xref="S2.SS1.p1.6.m6.2.3.2.3"><mtext mathsize="70%" id="S2.SS1.p1.6.m6.2.3.2.3.cmml" xref="S2.SS1.p1.6.m6.2.3.2.3">k</mtext></ci></apply><interval closure="open" id="S2.SS1.p1.6.m6.2.3.3.1.cmml" xref="S2.SS1.p1.6.m6.2.3.3.2"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">ùë•</ci><ci id="S2.SS1.p1.6.m6.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2">ùë¶</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.2c">p_{\text{k}}(x,y)</annotation></semantics></math> is different from that of other terminal devices. Non-IID data are classified in <cite class="ltx_cite ltx_citemacro_cite">Kairouz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> into the following five types:</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Feature distribution skew-covariate shift</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">The same feature is expressed differently by different clients. For example, the same number is written differently by different people. This is the most common problem faced in FL.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Label distribution skew-prior probability shift</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The same label, whose distribution depends on the client. For example, kangaroos are only found in Australia or zoos, which can cause different clients to have different preferences for prediction models.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Same label, different features-concept drift</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">For different clients, the same label has different features. For example, buildings differ significantly from one region to another. We believe that this problem is common in the field of video surveillance.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Same features, different label-concept shift</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">The same feature vector in the training data can have different labels due to personal preferences. For example, the label reflecting the emotions or predicting the next word differs according to the individual and the region. This problem is complex and makes personalized FL a big challenge.</p>
</div>
</section>
<section id="S2.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Data skewing</h4>

<div id="S2.SS1.SSS5.p1" class="ltx_para">
<p id="S2.SS1.SSS5.p1.1" class="ltx_p">Different clients can save different data, which can cause unfairness in FL.</p>
</div>
<div id="S2.SS1.SSS5.p2" class="ltx_para">
<p id="S2.SS1.SSS5.p2.1" class="ltx_p">In either case of non-IID, the data distribution in each device is not representative of the global data distribution. The aggregated model does not perform well enough or fairly enough for the participating clients. Our NIPD considers the three types of non-IID described above: feature distribution skew-covariate shift, same label but different features-concept drift and data skewing.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Introduction to the IoT FL experiment platform for non-IID data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The experiment platform is designed to include data annotation, device communication, client-side training, server aggregation of models and distribution of the updated models.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Data annotation</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">This process converts unprocessed speech, images, text, video and other data into machine-recognized information, which requires the staff to take the position of the object of interest (e.g., person, bicycle) in the given image. The finished data can be used as training and testing samples for the object detection algorithm. Meanwhile, new image data is obtained from the camera and staff continue to annotate it, which can update the training samples and achieve online learning.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Device communication</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Communication between the server and the client takes place during the FL process. First, the server initiates a FL task to the clients, and the client participating in the training respond to the request of the server. The server then selects the clients to participate in the training, issues the initial model, and the clients train the model on the local data and upload it to the servers. Finally, the server aggregates the models and sends them to the clients for further training until convergence. Therefore, in the design of the experimental platform, the stability of communication and the cost of communication are taken into account.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Client training</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The client is trained locally after receiving the FL task. Each client should have a configuration information file, including the number of local training epoch, batch size, optimizer, learning rate, the number of reconnections, etc. The training process is similar to traditional machine learning algorithms, where communication with the server is required, as well as updating the training model each round.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Model aggregation</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">After the clients have been trained on the local model, the model parameters for each client are transferred to the FL server. The server stores the updated model parameters. In terms of aggregation algorithms, there are several options for dealing with the adverse non-IID problems <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. In addition, security and privacy need to be considered, which prevents attack methods such as backdoor attacks and inference <cite class="ltx_cite ltx_citemacro_cite">Gao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Platform Implementation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we explain how to implement the design of the experimental platform for non-IID data in IoT devices described in the previous section. First, we describe the challenges of the implementation, including the client-side data annotation of the experimental platform and how it was communicated, the further partitioning of the partitioned existing data set with a quantitative imbalance, and the aggregation algorithm used. Then, we present the exact experimental specifications of the experiment and give examples of the experimental setting and its results.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Challenges in the implementation process</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Non-IID data poses a huge challenge to the learning accuracy of FL algorithms. Since the data distribution of users in different regions and environments is non-IID, the models of users are also different. If they are directly considered as a class for uploading and aggregating, the generated models are bound to deviate from the target models. The existing FL algorithms cannot adapt to all cases. The experimental platform described in this paper addresses the non-IID data phenomenon prevalent in IoT and is designed in four aspects: data annotation, device communication, number imbalance division, and aggregation algorithms.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Data annotation</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">After the image data is captured by the IoT camera, it needs to be manually annotated before it can be sent to the FL client for learning. The data annotation software uses labelImg<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/HumanSignal/labelImg</span></span></span> and the annotation schematic is shown in Fig. 2. Manual image annotation is first performed, requiring accurate annotation of each object, which will result in a good global model of the effect. Then, after the client receives the task initiated by FL, the corresponding client reads the annotated data for training. Finally, after the training is completed, it is sent to the server side for model aggregation, model updating and opening the next round of training.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.15932/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="499" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data annotation process in wireless IoT FL.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Device communication</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The communication part of the experiment platform is based on the Flask-SocketIO framework, and the communication process between the server and client is shown in Fig. 3. Firstly, each client waits for the server to initiate a federal learning request after collecting and manually annotating image data. Secondly, after the server initiates a training job request, the client makes a participating training response, and the server selects the clients to participate in this round of training. Thirdly, each participating client downloads the training request and receives the initial model from the server, while configuring the local training epoch, batch size, data set metrics, etc. for each client for local training. Fourthly, after the local training epoch requirement is met, the local model is uploaded to the server side. The server waits for all the participating clients‚Äô models to be uploaded in this round, then performs model aggregation and generates a global model. Finally, the server sends the global model to each client, and each client determines whether convergence is achieved based on the metrics measured on the test data.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.15932/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="269" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Communication process in wireless IoT FL.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Quantity imbalance division</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.2" class="ltx_p">We design a hyperparameter <math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="{\alpha}" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mi id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><ci id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">{\alpha}</annotation></semantics></math> to further divide the data set so that the amount of data held under each client is imbalanced and more relevant to the real scene. <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="{\alpha}" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mi id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><ci id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">{\alpha}</annotation></semantics></math> takes a range of 0 - 0.2, the larger it is, the more drastic the data imbalance is.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Aggregation algorithm</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Our aggregation algorithm uses the classical aggregation algorithm Federated Averaging (FedAvg) <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. The server only passes the current new parameters to the selected model each time, aggregates the model parameters of the selected client, and selects the model aggregation weight according to the number of samples of the client.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p">Our person detection code is based on the architecture of FedVision <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, and related experiments are performed on the Ubuntu 20.04 Linux operating system. The PyTorch version is 1.7.0, the CUDA version is 11.2, and the Python version is 3.8.3. The training is performed on a GPU server with two Intel Xeon Gold 6226R CPUs and eight Tesla T4 GPUs. The RAM of the server is 256 GB.
The experimental environment of our person detection algorithm is shown in Table 1.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental environment settings </figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Settings</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Details</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Servers</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Intel Xeon Gold 6226R CPU * 2</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td"></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left">Tesla T4 GPU * 8</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left">Operating System</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left">Ubuntu 20.04</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left">FL Framework</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_left">FedVision</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left">Python</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_left">3.8.3</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left">Pytorch</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_left">1.7.0</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<td id="S3.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_bb">CUDA</td>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_bb">11.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.SSS4.p3" class="ltx_para">
<p id="S3.SS1.SSS4.p3.1" class="ltx_p">The number of participating clients can be fixed by the server, and the participating clients per round can be selected by configuring the value of the parameter NUM_CLIENTS_CONTACTED_PER_ROUND. The MAX_NUM_ROUNDS parameter is used to adjust the training rounds, with 50 rounds for default setting. We use two classic object detection networks: YOLOv3 and Faster R-CNN. We adopts Adam optimization method to train YOLOv3, the initial learning rate is 1e-3, the batch size of Camera_1 to Camera_4 is set to 6, and the batch size of Camera_5 is set to 10. Faster R-CNN is trained using the Adam optimization method. The fixed learning rate is 1e-4, the momentum is 0.9, and the batch size from Camera_1 to Camera_5 is set to 1. It should be noted that the pre-trained model is loaded to speed up the convergence of the model, where the pre-trained Darknet-53 model is used for YOLOv3 and the pre-trained VGG16. In particular, in the data preprocessing part, the image input of the YOLOv3 algorithm is uniformly scaled to 416 * 416.</p>
</div>
<div id="S3.SS1.SSS4.p4" class="ltx_para">
<p id="S3.SS1.SSS4.p4.1" class="ltx_p">The local epoch of each client is set to 1 by default for both the Faster R-CNN and YOLOv3 object detection models.</p>
</div>
<div id="S3.SS1.SSS4.p5" class="ltx_para">
<p id="S3.SS1.SSS4.p5.1" class="ltx_p">The evaluation criterion we adopted is the general index mean Average Precision (mAP) in the field of object detection, and the mean value of the Average Precision (AP) value of each class is calculated. Intersection Over Union (IoU) represents the area of overlap between the predicted bounding box and the ground truth bounding box to determine whether the prediction is true positive.The specific formulas are as follows:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle mAP=\frac{{\sum\nolimits_{k=1}^{k}{A{P_{i}}}}}{k}" display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.1a" xref="S3.E1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.2.4" xref="S3.E1.m1.1.1.2.4.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mfrac id="S3.E1.m1.1.1.3a" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><msubsup id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml"><mo id="S3.E1.m1.1.1.3.2.1.2.2" xref="S3.E1.m1.1.1.3.2.1.2.2.cmml">‚àë</mo><mrow id="S3.E1.m1.1.1.3.2.1.2.3" xref="S3.E1.m1.1.1.3.2.1.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.1.2.3.2" xref="S3.E1.m1.1.1.3.2.1.2.3.2.cmml">k</mi><mo id="S3.E1.m1.1.1.3.2.1.2.3.1" xref="S3.E1.m1.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.3.2.1.2.3.3" xref="S3.E1.m1.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.3.2.1.3" xref="S3.E1.m1.1.1.3.2.1.3.cmml">k</mi></msubsup><mrow id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.1" xref="S3.E1.m1.1.1.3.2.2.1.cmml">‚Äã</mo><msub id="S3.E1.m1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.3.2.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.2.3.2" xref="S3.E1.m1.1.1.3.2.2.3.2.cmml">P</mi><mi id="S3.E1.m1.1.1.3.2.2.3.3" xref="S3.E1.m1.1.1.3.2.2.3.3.cmml">i</mi></msub></mrow></mrow><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">k</mi></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">ùëö</ci><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">ùê¥</ci><ci id="S3.E1.m1.1.1.2.4.cmml" xref="S3.E1.m1.1.1.2.4">ùëÉ</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><divide id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3"></divide><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><apply id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.1.cmml" xref="S3.E1.m1.1.1.3.2.1">superscript</csymbol><apply id="S3.E1.m1.1.1.3.2.1.2.cmml" xref="S3.E1.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1">subscript</csymbol><sum id="S3.E1.m1.1.1.3.2.1.2.2.cmml" xref="S3.E1.m1.1.1.3.2.1.2.2"></sum><apply id="S3.E1.m1.1.1.3.2.1.2.3.cmml" xref="S3.E1.m1.1.1.3.2.1.2.3"><eq id="S3.E1.m1.1.1.3.2.1.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.1.2.3.1"></eq><ci id="S3.E1.m1.1.1.3.2.1.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.1.2.3.2">ùëò</ci><cn type="integer" id="S3.E1.m1.1.1.3.2.1.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.3.2.1.3.cmml" xref="S3.E1.m1.1.1.3.2.1.3">ùëò</ci></apply><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><times id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.1"></times><ci id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2">ùê¥</ci><apply id="S3.E1.m1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2">ùëÉ</ci><ci id="S3.E1.m1.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3.3">ùëñ</ci></apply></apply></apply><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">ùëò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle mAP=\frac{{\sum\nolimits_{k=1}^{k}{A{P_{i}}}}}{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS4.p6" class="ltx_para">
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle AP=\int_{0}^{q}{p\left(r\right)}dr" display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.2" xref="S3.E2.m1.1.2.cmml"><mrow id="S3.E2.m1.1.2.2" xref="S3.E2.m1.1.2.2.cmml"><mi id="S3.E2.m1.1.2.2.2" xref="S3.E2.m1.1.2.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.2.1" xref="S3.E2.m1.1.2.2.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.2.2.3" xref="S3.E2.m1.1.2.2.3.cmml">P</mi></mrow><mo id="S3.E2.m1.1.2.1" xref="S3.E2.m1.1.2.1.cmml">=</mo><mrow id="S3.E2.m1.1.2.3" xref="S3.E2.m1.1.2.3.cmml"><mstyle displaystyle="true" id="S3.E2.m1.1.2.3.1" xref="S3.E2.m1.1.2.3.1.cmml"><msubsup id="S3.E2.m1.1.2.3.1a" xref="S3.E2.m1.1.2.3.1.cmml"><mo id="S3.E2.m1.1.2.3.1.2.2" xref="S3.E2.m1.1.2.3.1.2.2.cmml">‚à´</mo><mn id="S3.E2.m1.1.2.3.1.2.3" xref="S3.E2.m1.1.2.3.1.2.3.cmml">0</mn><mi id="S3.E2.m1.1.2.3.1.3" xref="S3.E2.m1.1.2.3.1.3.cmml">q</mi></msubsup></mstyle><mrow id="S3.E2.m1.1.2.3.2" xref="S3.E2.m1.1.2.3.2.cmml"><mi id="S3.E2.m1.1.2.3.2.2" xref="S3.E2.m1.1.2.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.3.2.1" xref="S3.E2.m1.1.2.3.2.1.cmml">‚Äã</mo><mrow id="S3.E2.m1.1.2.3.2.3.2" xref="S3.E2.m1.1.2.3.2.cmml"><mo id="S3.E2.m1.1.2.3.2.3.2.1" xref="S3.E2.m1.1.2.3.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">r</mi><mo id="S3.E2.m1.1.2.3.2.3.2.2" xref="S3.E2.m1.1.2.3.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.3.2.1a" xref="S3.E2.m1.1.2.3.2.1.cmml">‚Äã</mo><mrow id="S3.E2.m1.1.2.3.2.4" xref="S3.E2.m1.1.2.3.2.4.cmml"><mo rspace="0em" id="S3.E2.m1.1.2.3.2.4.1" xref="S3.E2.m1.1.2.3.2.4.1.cmml">ùëë</mo><mi id="S3.E2.m1.1.2.3.2.4.2" xref="S3.E2.m1.1.2.3.2.4.2.cmml">r</mi></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.2.cmml" xref="S3.E2.m1.1.2"><eq id="S3.E2.m1.1.2.1.cmml" xref="S3.E2.m1.1.2.1"></eq><apply id="S3.E2.m1.1.2.2.cmml" xref="S3.E2.m1.1.2.2"><times id="S3.E2.m1.1.2.2.1.cmml" xref="S3.E2.m1.1.2.2.1"></times><ci id="S3.E2.m1.1.2.2.2.cmml" xref="S3.E2.m1.1.2.2.2">ùê¥</ci><ci id="S3.E2.m1.1.2.2.3.cmml" xref="S3.E2.m1.1.2.2.3">ùëÉ</ci></apply><apply id="S3.E2.m1.1.2.3.cmml" xref="S3.E2.m1.1.2.3"><apply id="S3.E2.m1.1.2.3.1.cmml" xref="S3.E2.m1.1.2.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.3.1.1.cmml" xref="S3.E2.m1.1.2.3.1">superscript</csymbol><apply id="S3.E2.m1.1.2.3.1.2.cmml" xref="S3.E2.m1.1.2.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.3.1.2.1.cmml" xref="S3.E2.m1.1.2.3.1">subscript</csymbol><int id="S3.E2.m1.1.2.3.1.2.2.cmml" xref="S3.E2.m1.1.2.3.1.2.2"></int><cn type="integer" id="S3.E2.m1.1.2.3.1.2.3.cmml" xref="S3.E2.m1.1.2.3.1.2.3">0</cn></apply><ci id="S3.E2.m1.1.2.3.1.3.cmml" xref="S3.E2.m1.1.2.3.1.3">ùëû</ci></apply><apply id="S3.E2.m1.1.2.3.2.cmml" xref="S3.E2.m1.1.2.3.2"><times id="S3.E2.m1.1.2.3.2.1.cmml" xref="S3.E2.m1.1.2.3.2.1"></times><ci id="S3.E2.m1.1.2.3.2.2.cmml" xref="S3.E2.m1.1.2.3.2.2">ùëù</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ùëü</ci><apply id="S3.E2.m1.1.2.3.2.4.cmml" xref="S3.E2.m1.1.2.3.2.4"><csymbol cd="latexml" id="S3.E2.m1.1.2.3.2.4.1.cmml" xref="S3.E2.m1.1.2.3.2.4.1">differential-d</csymbol><ci id="S3.E2.m1.1.2.3.2.4.2.cmml" xref="S3.E2.m1.1.2.3.2.4.2">ùëü</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle AP=\int_{0}^{q}{p\left(r\right)}dr</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS4.p7" class="ltx_para">
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\displaystyle IoU=\frac{{area\left({{B_{gt}}\cap{B_{pred}}}\right)}}{{area\left({{B_{gt}}\cup{B_{pred}}}\right)}}" display="inline"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.3" xref="S3.E3.m1.2.3.cmml"><mrow id="S3.E3.m1.2.3.2" xref="S3.E3.m1.2.3.2.cmml"><mi id="S3.E3.m1.2.3.2.2" xref="S3.E3.m1.2.3.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.2.1" xref="S3.E3.m1.2.3.2.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.3.2.3" xref="S3.E3.m1.2.3.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.2.1a" xref="S3.E3.m1.2.3.2.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.3.2.4" xref="S3.E3.m1.2.3.2.4.cmml">U</mi></mrow><mo id="S3.E3.m1.2.3.1" xref="S3.E3.m1.2.3.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mfrac id="S3.E3.m1.2.2a" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.4" xref="S3.E3.m1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2a" xref="S3.E3.m1.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.5" xref="S3.E3.m1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2b" xref="S3.E3.m1.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.6" xref="S3.E3.m1.1.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2c" xref="S3.E3.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">B</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.3.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">‚à©</mo><msub id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">B</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.4" xref="S3.E3.m1.1.1.1.1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.3.1b" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.5" xref="S3.E3.m1.1.1.1.1.1.1.3.3.5.cmml">d</mi></mrow></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2a" xref="S3.E3.m1.2.2.2.2.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.5" xref="S3.E3.m1.2.2.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2b" xref="S3.E3.m1.2.2.2.2.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.6" xref="S3.E3.m1.2.2.2.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2c" xref="S3.E3.m1.2.2.2.2.cmml">‚Äã</mo><mrow id="S3.E3.m1.2.2.2.1.1" xref="S3.E3.m1.2.2.2.1.1.1.cmml"><mo id="S3.E3.m1.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.cmml"><msub id="S3.E3.m1.2.2.2.1.1.1.2" xref="S3.E3.m1.2.2.2.1.1.1.2.cmml"><mi id="S3.E3.m1.2.2.2.1.1.1.2.2" xref="S3.E3.m1.2.2.2.1.1.1.2.2.cmml">B</mi><mrow id="S3.E3.m1.2.2.2.1.1.1.2.3" xref="S3.E3.m1.2.2.2.1.1.1.2.3.cmml"><mi id="S3.E3.m1.2.2.2.1.1.1.2.3.2" xref="S3.E3.m1.2.2.2.1.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.1.1.1.2.3.1" xref="S3.E3.m1.2.2.2.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.1.1.1.2.3.3" xref="S3.E3.m1.2.2.2.1.1.1.2.3.3.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.2.2.2.1.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml">‚à™</mo><msub id="S3.E3.m1.2.2.2.1.1.1.3" xref="S3.E3.m1.2.2.2.1.1.1.3.cmml"><mi id="S3.E3.m1.2.2.2.1.1.1.3.2" xref="S3.E3.m1.2.2.2.1.1.1.3.2.cmml">B</mi><mrow id="S3.E3.m1.2.2.2.1.1.1.3.3" xref="S3.E3.m1.2.2.2.1.1.1.3.3.cmml"><mi id="S3.E3.m1.2.2.2.1.1.1.3.3.2" xref="S3.E3.m1.2.2.2.1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.1.1.1.3.3.1" xref="S3.E3.m1.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.1.1.1.3.3.3" xref="S3.E3.m1.2.2.2.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.1.1.1.3.3.1a" xref="S3.E3.m1.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.1.1.1.3.3.4" xref="S3.E3.m1.2.2.2.1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.1.1.1.3.3.1b" xref="S3.E3.m1.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.2.2.2.1.1.1.3.3.5" xref="S3.E3.m1.2.2.2.1.1.1.3.3.5.cmml">d</mi></mrow></msub></mrow><mo id="S3.E3.m1.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.3.cmml" xref="S3.E3.m1.2.3"><eq id="S3.E3.m1.2.3.1.cmml" xref="S3.E3.m1.2.3.1"></eq><apply id="S3.E3.m1.2.3.2.cmml" xref="S3.E3.m1.2.3.2"><times id="S3.E3.m1.2.3.2.1.cmml" xref="S3.E3.m1.2.3.2.1"></times><ci id="S3.E3.m1.2.3.2.2.cmml" xref="S3.E3.m1.2.3.2.2">ùêº</ci><ci id="S3.E3.m1.2.3.2.3.cmml" xref="S3.E3.m1.2.3.2.3">ùëú</ci><ci id="S3.E3.m1.2.3.2.4.cmml" xref="S3.E3.m1.2.3.2.4">ùëà</ci></apply><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">ùëé</ci><ci id="S3.E3.m1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.4">ùëü</ci><ci id="S3.E3.m1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.5">ùëí</ci><ci id="S3.E3.m1.1.1.1.6.cmml" xref="S3.E3.m1.1.1.1.6">ùëé</ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><intersect id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"></intersect><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">ùêµ</ci><apply id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3"><times id="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.2">ùëî</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.3">ùë°</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">ùêµ</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.2">ùëù</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3">ùëü</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.4">ùëí</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.5">ùëë</ci></apply></apply></apply></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2"></times><ci id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3">ùëé</ci><ci id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">ùëü</ci><ci id="S3.E3.m1.2.2.2.5.cmml" xref="S3.E3.m1.2.2.2.5">ùëí</ci><ci id="S3.E3.m1.2.2.2.6.cmml" xref="S3.E3.m1.2.2.2.6">ùëé</ci><apply id="S3.E3.m1.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.1.1"><union id="S3.E3.m1.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1"></union><apply id="S3.E3.m1.2.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2.2">ùêµ</ci><apply id="S3.E3.m1.2.2.2.1.1.1.2.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2.3"><times id="S3.E3.m1.2.2.2.1.1.1.2.3.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2.3.1"></times><ci id="S3.E3.m1.2.2.2.1.1.1.2.3.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2.3.2">ùëî</ci><ci id="S3.E3.m1.2.2.2.1.1.1.2.3.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.2.3.3">ùë°</ci></apply></apply><apply id="S3.E3.m1.2.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.2.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.2">ùêµ</ci><apply id="S3.E3.m1.2.2.2.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.3"><times id="S3.E3.m1.2.2.2.1.1.1.3.3.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.3.1"></times><ci id="S3.E3.m1.2.2.2.1.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.3.2">ùëù</ci><ci id="S3.E3.m1.2.2.2.1.1.1.3.3.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.3.3">ùëü</ci><ci id="S3.E3.m1.2.2.2.1.1.1.3.3.4.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.3.4">ùëí</ci><ci id="S3.E3.m1.2.2.2.1.1.1.3.3.5.cmml" xref="S3.E3.m1.2.2.2.1.1.1.3.3.5">ùëë</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\displaystyle IoU=\frac{{area\left({{B_{gt}}\cap{B_{pred}}}\right)}}{{area\left({{B_{gt}}\cup{B_{pred}}}\right)}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS4.p8" class="ltx_para">
<p id="S3.SS1.SSS4.p8.1" class="ltx_p">where AP is calculated for each class separately and k is the number of classes (in our dataset, k is 1). And the IoU threshold is set to 0.5 to calculate the mAP.</p>
</div>
<div id="S3.SS1.SSS4.p9" class="ltx_para">
<p id="S3.SS1.SSS4.p9.1" class="ltx_p">In the example program, we perform the following two setups to give concrete examples of the experiments.</p>
</div>
<div id="S3.SS1.SSS4.p10" class="ltx_para">
<p id="S3.SS1.SSS4.p10.1" class="ltx_p"><span id="S3.SS1.SSS4.p10.1.1" class="ltx_text ltx_font_bold">Setting 1: </span>Standard Non-IID data. The amount of data held by each client is basically the same, around 1600. From Camera_1 to Camera_5, the number of training data is 1590, 1574, 1581, 1583 and 1607, and the number of test data is 410, 426, 419, 417 and 393.</p>
</div>
<div id="S3.SS1.SSS4.p11" class="ltx_para">
<p id="S3.SS1.SSS4.p11.1" class="ltx_p"><span id="S3.SS1.SSS4.p11.1.1" class="ltx_text ltx_font_bold">Setting 2: </span>The number of data is imbalanced, as reflected by the fact that there is a certain difference in the number of images under each camera. We set the size of the above hyperparameter <math id="S3.SS1.SSS4.p11.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.SSS4.p11.1.m1.1a"><mi id="S3.SS1.SSS4.p11.1.m1.1.1" xref="S3.SS1.SSS4.p11.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p11.1.m1.1b"><ci id="S3.SS1.SSS4.p11.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p11.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p11.1.m1.1c">\alpha</annotation></semantics></math> to 0.1, reclassifying the training data in Setting 1 as imbalanced. From Camera_1 to Camera_5, the number of training data is 1590, 1412, 1284, 1117, and 937, and the number of test data is the same as Setting 1.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2306.15932/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="246" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Person detection results for YOLOv3 and Faster R-CNN.</figcaption>
</figure>
<div id="S3.SS1.SSS4.p12" class="ltx_para">
<p id="S3.SS1.SSS4.p12.2" class="ltx_p">After completing the above configuration, we conduct our experiments. We use the methods Faster R-CNN and YOLOv3 to conduct 50 rounds of training for standard and imbalanced data sets respectively. The test mAP results are shown in Fig. 4. In the case of 50 rounds of training, the benchmark values we provide are as follows: The mAP of Faster R-CNN on the NIPD standard dataset is 82.2 percent, and the mAP at <math id="S3.SS1.SSS4.p12.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.SSS4.p12.1.m1.1a"><mi id="S3.SS1.SSS4.p12.1.m1.1.1" xref="S3.SS1.SSS4.p12.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p12.1.m1.1b"><ci id="S3.SS1.SSS4.p12.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p12.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p12.1.m1.1c">\alpha</annotation></semantics></math> = 0.1 is 81.5 percent. The mAP of YOLOv3 on the NIPD standard dataset is 82.0 percent, and the mAP at <math id="S3.SS1.SSS4.p12.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.SSS4.p12.2.m2.1a"><mi id="S3.SS1.SSS4.p12.2.m2.1.1" xref="S3.SS1.SSS4.p12.2.m2.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p12.2.m2.1b"><ci id="S3.SS1.SSS4.p12.2.m2.1.1.cmml" xref="S3.SS1.SSS4.p12.2.m2.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p12.2.m2.1c">\alpha</annotation></semantics></math> = 0.1 is 80.4 percent.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Set</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we provide an exhaustive introduction to the data set, with person sizes defined in the context of the scenario. The data clusters are introduced including camera locations, data annotation formats, object distribution, data partitioning, and some challenges in NIPD applications.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Resource Description</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Current real-world non-IID data lack corresponding dedicated data sets in IoT-related devices. The data sets used by most researchers are base data sets for object detection domain data, but these data sets do not accurately model real device situations in IoT-related studies.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To solve this problem, we randomly capture different scenes from the cameras in the front plaza of the Innovation Centre of the University of Electronic Science and Technology of China (UESTC) at similar periods, and delete the pictures of people that do not exist within the field of view and the night pictures. We select the data of five representative cameras located around the square, named Camera_1 to Camera_5. These cameras‚Äô view angle, chromatic aberration, resolution, and mounting heights all differ, and the scene is shown in Fig. 5, which contains the camera resolutions and mounting heights. Camera_1 to Camera_4 have a resolution of 1920 * 1080 and Camera_5 has a resolution of 3072 * 2048. Two thousand images are acquired from each camera, for a total of 10000 images, all of which are identified with a person class and ensure that at least one person exists within each image. The total number of all people in the data set is 55,160, with an average of 5.516 persons per image.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2306.15932/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="261" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Camera data acquisition scene in UESTC Innovation Center.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data format</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.7" class="ltx_p">All people are labeled using rectangular bounding boxes. To accommodate the need for pervasiveness of person detection, two types of labels are used to standardize the data set, the VOC format and the YOLO format. For the VOC format labeling, a line represents the person bounding box information in the form of
<math id="S4.SS2.p1.1.m1.4" class="ltx_Math" alttext="\left\{{x_{\text{min}},x_{\text{max}},y_{\text{min}},y_{\text{max}}}\right\}" display="inline"><semantics id="S4.SS2.p1.1.m1.4a"><mrow id="S4.SS2.p1.1.m1.4.4.4" xref="S4.SS2.p1.1.m1.4.4.5.cmml"><mo id="S4.SS2.p1.1.m1.4.4.4.5" xref="S4.SS2.p1.1.m1.4.4.5.cmml">{</mo><msub id="S4.SS2.p1.1.m1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.2.cmml">x</mi><mtext id="S4.SS2.p1.1.m1.1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.1.1.3a.cmml">min</mtext></msub><mo id="S4.SS2.p1.1.m1.4.4.4.6" xref="S4.SS2.p1.1.m1.4.4.5.cmml">,</mo><msub id="S4.SS2.p1.1.m1.2.2.2.2" xref="S4.SS2.p1.1.m1.2.2.2.2.cmml"><mi id="S4.SS2.p1.1.m1.2.2.2.2.2" xref="S4.SS2.p1.1.m1.2.2.2.2.2.cmml">x</mi><mtext id="S4.SS2.p1.1.m1.2.2.2.2.3" xref="S4.SS2.p1.1.m1.2.2.2.2.3a.cmml">max</mtext></msub><mo id="S4.SS2.p1.1.m1.4.4.4.7" xref="S4.SS2.p1.1.m1.4.4.5.cmml">,</mo><msub id="S4.SS2.p1.1.m1.3.3.3.3" xref="S4.SS2.p1.1.m1.3.3.3.3.cmml"><mi id="S4.SS2.p1.1.m1.3.3.3.3.2" xref="S4.SS2.p1.1.m1.3.3.3.3.2.cmml">y</mi><mtext id="S4.SS2.p1.1.m1.3.3.3.3.3" xref="S4.SS2.p1.1.m1.3.3.3.3.3a.cmml">min</mtext></msub><mo id="S4.SS2.p1.1.m1.4.4.4.8" xref="S4.SS2.p1.1.m1.4.4.5.cmml">,</mo><msub id="S4.SS2.p1.1.m1.4.4.4.4" xref="S4.SS2.p1.1.m1.4.4.4.4.cmml"><mi id="S4.SS2.p1.1.m1.4.4.4.4.2" xref="S4.SS2.p1.1.m1.4.4.4.4.2.cmml">y</mi><mtext id="S4.SS2.p1.1.m1.4.4.4.4.3" xref="S4.SS2.p1.1.m1.4.4.4.4.3a.cmml">max</mtext></msub><mo id="S4.SS2.p1.1.m1.4.4.4.9" xref="S4.SS2.p1.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.4b"><set id="S4.SS2.p1.1.m1.4.4.5.cmml" xref="S4.SS2.p1.1.m1.4.4.4"><apply id="S4.SS2.p1.1.m1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2">ùë•</ci><ci id="S4.SS2.p1.1.m1.1.1.1.1.3a.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.3">min</mtext></ci></apply><apply id="S4.SS2.p1.1.m1.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.2.2.2.2.1.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.SS2.p1.1.m1.2.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2">ùë•</ci><ci id="S4.SS2.p1.1.m1.2.2.2.2.3a.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.3"><mtext mathsize="70%" id="S4.SS2.p1.1.m1.2.2.2.2.3.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.3">max</mtext></ci></apply><apply id="S4.SS2.p1.1.m1.3.3.3.3.cmml" xref="S4.SS2.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.3.3.3.3.1.cmml" xref="S4.SS2.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.SS2.p1.1.m1.3.3.3.3.2.cmml" xref="S4.SS2.p1.1.m1.3.3.3.3.2">ùë¶</ci><ci id="S4.SS2.p1.1.m1.3.3.3.3.3a.cmml" xref="S4.SS2.p1.1.m1.3.3.3.3.3"><mtext mathsize="70%" id="S4.SS2.p1.1.m1.3.3.3.3.3.cmml" xref="S4.SS2.p1.1.m1.3.3.3.3.3">min</mtext></ci></apply><apply id="S4.SS2.p1.1.m1.4.4.4.4.cmml" xref="S4.SS2.p1.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.4.4.4.4.1.cmml" xref="S4.SS2.p1.1.m1.4.4.4.4">subscript</csymbol><ci id="S4.SS2.p1.1.m1.4.4.4.4.2.cmml" xref="S4.SS2.p1.1.m1.4.4.4.4.2">ùë¶</ci><ci id="S4.SS2.p1.1.m1.4.4.4.4.3a.cmml" xref="S4.SS2.p1.1.m1.4.4.4.4.3"><mtext mathsize="70%" id="S4.SS2.p1.1.m1.4.4.4.4.3.cmml" xref="S4.SS2.p1.1.m1.4.4.4.4.3">max</mtext></ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.4c">\left\{{x_{\text{min}},x_{\text{max}},y_{\text{min}},y_{\text{max}}}\right\}</annotation></semantics></math>, where the coordinates <math id="S4.SS2.p1.2.m2.2" class="ltx_Math" alttext="x_{\text{min}},y_{\text{min}}" display="inline"><semantics id="S4.SS2.p1.2.m2.2a"><mrow id="S4.SS2.p1.2.m2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.3.cmml"><msub id="S4.SS2.p1.2.m2.1.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.1.1.2" xref="S4.SS2.p1.2.m2.1.1.1.1.2.cmml">x</mi><mtext id="S4.SS2.p1.2.m2.1.1.1.1.3" xref="S4.SS2.p1.2.m2.1.1.1.1.3a.cmml">min</mtext></msub><mo id="S4.SS2.p1.2.m2.2.2.2.3" xref="S4.SS2.p1.2.m2.2.2.3.cmml">,</mo><msub id="S4.SS2.p1.2.m2.2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.2.2.cmml"><mi id="S4.SS2.p1.2.m2.2.2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.2.2.2.cmml">y</mi><mtext id="S4.SS2.p1.2.m2.2.2.2.2.3" xref="S4.SS2.p1.2.m2.2.2.2.2.3a.cmml">min</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.2b"><list id="S4.SS2.p1.2.m2.2.2.3.cmml" xref="S4.SS2.p1.2.m2.2.2.2"><apply id="S4.SS2.p1.2.m2.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.2">ùë•</ci><ci id="S4.SS2.p1.2.m2.1.1.1.1.3a.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.3">min</mtext></ci></apply><apply id="S4.SS2.p1.2.m2.2.2.2.2.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.2.2.2.2.1.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S4.SS2.p1.2.m2.2.2.2.2.2.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.2">ùë¶</ci><ci id="S4.SS2.p1.2.m2.2.2.2.2.3a.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.3"><mtext mathsize="70%" id="S4.SS2.p1.2.m2.2.2.2.2.3.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.3">min</mtext></ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.2c">x_{\text{min}},y_{\text{min}}</annotation></semantics></math> are the top left corner of the bounding box and the coordinates <math id="S4.SS2.p1.3.m3.2" class="ltx_Math" alttext="x_{\text{max}},y_{\text{max}}" display="inline"><semantics id="S4.SS2.p1.3.m3.2a"><mrow id="S4.SS2.p1.3.m3.2.2.2" xref="S4.SS2.p1.3.m3.2.2.3.cmml"><msub id="S4.SS2.p1.3.m3.1.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.1.1.2" xref="S4.SS2.p1.3.m3.1.1.1.1.2.cmml">x</mi><mtext id="S4.SS2.p1.3.m3.1.1.1.1.3" xref="S4.SS2.p1.3.m3.1.1.1.1.3a.cmml">max</mtext></msub><mo id="S4.SS2.p1.3.m3.2.2.2.3" xref="S4.SS2.p1.3.m3.2.2.3.cmml">,</mo><msub id="S4.SS2.p1.3.m3.2.2.2.2" xref="S4.SS2.p1.3.m3.2.2.2.2.cmml"><mi id="S4.SS2.p1.3.m3.2.2.2.2.2" xref="S4.SS2.p1.3.m3.2.2.2.2.2.cmml">y</mi><mtext id="S4.SS2.p1.3.m3.2.2.2.2.3" xref="S4.SS2.p1.3.m3.2.2.2.2.3a.cmml">max</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.2b"><list id="S4.SS2.p1.3.m3.2.2.3.cmml" xref="S4.SS2.p1.3.m3.2.2.2"><apply id="S4.SS2.p1.3.m3.1.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.2">ùë•</ci><ci id="S4.SS2.p1.3.m3.1.1.1.1.3a.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.3.m3.1.1.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1.3">max</mtext></ci></apply><apply id="S4.SS2.p1.3.m3.2.2.2.2.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.2.2.2.2.1.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S4.SS2.p1.3.m3.2.2.2.2.2.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.2">ùë¶</ci><ci id="S4.SS2.p1.3.m3.2.2.2.2.3a.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.3"><mtext mathsize="70%" id="S4.SS2.p1.3.m3.2.2.2.2.3.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.3">max</mtext></ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.2c">x_{\text{max}},y_{\text{max}}</annotation></semantics></math> are the bottom right corner of the bounding box.
For the YOLO format of the label, each line describes the rectangular range information of the object in the form of <math id="S4.SS2.p1.4.m4.5" class="ltx_Math" alttext="\left\{{label,x,y,w,h}\right\}" display="inline"><semantics id="S4.SS2.p1.4.m4.5a"><mrow id="S4.SS2.p1.4.m4.5.5.1" xref="S4.SS2.p1.4.m4.5.5.2.cmml"><mo id="S4.SS2.p1.4.m4.5.5.1.2" xref="S4.SS2.p1.4.m4.5.5.2.cmml">{</mo><mrow id="S4.SS2.p1.4.m4.5.5.1.1" xref="S4.SS2.p1.4.m4.5.5.1.1.cmml"><mi id="S4.SS2.p1.4.m4.5.5.1.1.2" xref="S4.SS2.p1.4.m4.5.5.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.5.5.1.1.1" xref="S4.SS2.p1.4.m4.5.5.1.1.1.cmml">‚Äã</mo><mi id="S4.SS2.p1.4.m4.5.5.1.1.3" xref="S4.SS2.p1.4.m4.5.5.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.5.5.1.1.1a" xref="S4.SS2.p1.4.m4.5.5.1.1.1.cmml">‚Äã</mo><mi id="S4.SS2.p1.4.m4.5.5.1.1.4" xref="S4.SS2.p1.4.m4.5.5.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.5.5.1.1.1b" xref="S4.SS2.p1.4.m4.5.5.1.1.1.cmml">‚Äã</mo><mi id="S4.SS2.p1.4.m4.5.5.1.1.5" xref="S4.SS2.p1.4.m4.5.5.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.5.5.1.1.1c" xref="S4.SS2.p1.4.m4.5.5.1.1.1.cmml">‚Äã</mo><mi id="S4.SS2.p1.4.m4.5.5.1.1.6" xref="S4.SS2.p1.4.m4.5.5.1.1.6.cmml">l</mi></mrow><mo id="S4.SS2.p1.4.m4.5.5.1.3" xref="S4.SS2.p1.4.m4.5.5.2.cmml">,</mo><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">x</mi><mo id="S4.SS2.p1.4.m4.5.5.1.4" xref="S4.SS2.p1.4.m4.5.5.2.cmml">,</mo><mi id="S4.SS2.p1.4.m4.2.2" xref="S4.SS2.p1.4.m4.2.2.cmml">y</mi><mo id="S4.SS2.p1.4.m4.5.5.1.5" xref="S4.SS2.p1.4.m4.5.5.2.cmml">,</mo><mi id="S4.SS2.p1.4.m4.3.3" xref="S4.SS2.p1.4.m4.3.3.cmml">w</mi><mo id="S4.SS2.p1.4.m4.5.5.1.6" xref="S4.SS2.p1.4.m4.5.5.2.cmml">,</mo><mi id="S4.SS2.p1.4.m4.4.4" xref="S4.SS2.p1.4.m4.4.4.cmml">h</mi><mo id="S4.SS2.p1.4.m4.5.5.1.7" xref="S4.SS2.p1.4.m4.5.5.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.5b"><set id="S4.SS2.p1.4.m4.5.5.2.cmml" xref="S4.SS2.p1.4.m4.5.5.1"><apply id="S4.SS2.p1.4.m4.5.5.1.1.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1"><times id="S4.SS2.p1.4.m4.5.5.1.1.1.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1.1"></times><ci id="S4.SS2.p1.4.m4.5.5.1.1.2.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1.2">ùëô</ci><ci id="S4.SS2.p1.4.m4.5.5.1.1.3.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1.3">ùëé</ci><ci id="S4.SS2.p1.4.m4.5.5.1.1.4.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1.4">ùëè</ci><ci id="S4.SS2.p1.4.m4.5.5.1.1.5.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1.5">ùëí</ci><ci id="S4.SS2.p1.4.m4.5.5.1.1.6.cmml" xref="S4.SS2.p1.4.m4.5.5.1.1.6">ùëô</ci></apply><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">ùë•</ci><ci id="S4.SS2.p1.4.m4.2.2.cmml" xref="S4.SS2.p1.4.m4.2.2">ùë¶</ci><ci id="S4.SS2.p1.4.m4.3.3.cmml" xref="S4.SS2.p1.4.m4.3.3">ùë§</ci><ci id="S4.SS2.p1.4.m4.4.4.cmml" xref="S4.SS2.p1.4.m4.4.4">‚Ñé</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.5c">\left\{{label,x,y,w,h}\right\}</annotation></semantics></math>,where label indicates the class of the object (in this data set only person), <math id="S4.SS2.p1.5.m5.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S4.SS2.p1.5.m5.2a"><mrow id="S4.SS2.p1.5.m5.2.3.2" xref="S4.SS2.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p1.5.m5.2.3.2.1" xref="S4.SS2.p1.5.m5.2.3.1.cmml">(</mo><mi id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">x</mi><mo id="S4.SS2.p1.5.m5.2.3.2.2" xref="S4.SS2.p1.5.m5.2.3.1.cmml">,</mo><mi id="S4.SS2.p1.5.m5.2.2" xref="S4.SS2.p1.5.m5.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS2.p1.5.m5.2.3.2.3" xref="S4.SS2.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.2b"><interval closure="open" id="S4.SS2.p1.5.m5.2.3.1.cmml" xref="S4.SS2.p1.5.m5.2.3.2"><ci id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">ùë•</ci><ci id="S4.SS2.p1.5.m5.2.2.cmml" xref="S4.SS2.p1.5.m5.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.2c">(x,y)</annotation></semantics></math> the center of the bounding box, <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mi id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><ci id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">ùë§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">w</annotation></semantics></math> the width of the bounding box and <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mi id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><ci id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">h</annotation></semantics></math> the height of the bounding box.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In the standard data set, we use approximately 80 percent of the data set (7,935 images) for training and the remaining approximately 20 percent (2,065 images) for testing. The number and size distribution of persons these images are also distributed with the images. The data set splits are all randomly selected to ensure the reliability and generalizability of the overall data set during use.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Sample statistics of NIPD data set</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.2.1.1.1" class="ltx_text ltx_font_bold">Type</span></th>
<th id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">Camera</span></th>
<th id="S4.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.3.1" class="ltx_text ltx_font_bold">Images</span></th>
<th id="S4.T2.1.2.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.4.1" class="ltx_text ltx_font_bold">Total objects</span></th>
<th id="S4.T2.1.2.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.5.1" class="ltx_text ltx_font_bold">Large objects</span></th>
<th id="S4.T2.1.2.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.6.1" class="ltx_text ltx_font_bold">Medium objects</span></th>
<th id="S4.T2.1.2.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.7.1" class="ltx_text ltx_font_bold">Small objects</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.3.1" class="ltx_tr">
<th id="S4.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T2.1.3.1.1.1" class="ltx_text">All</span></th>
<th id="S4.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Camera_1</th>
<td id="S4.T2.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">2000</td>
<td id="S4.T2.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">10497</td>
<td id="S4.T2.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">676</td>
<td id="S4.T2.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">9497</td>
<td id="S4.T2.1.3.1.7" class="ltx_td ltx_align_right ltx_border_t">324</td>
</tr>
<tr id="S4.T2.1.4.2" class="ltx_tr">
<th id="S4.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_2</th>
<td id="S4.T2.1.4.2.2" class="ltx_td ltx_align_right">2000</td>
<td id="S4.T2.1.4.2.3" class="ltx_td ltx_align_right">7573</td>
<td id="S4.T2.1.4.2.4" class="ltx_td ltx_align_right">894</td>
<td id="S4.T2.1.4.2.5" class="ltx_td ltx_align_right">6417</td>
<td id="S4.T2.1.4.2.6" class="ltx_td ltx_align_right">262</td>
</tr>
<tr id="S4.T2.1.5.3" class="ltx_tr">
<th id="S4.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_3</th>
<td id="S4.T2.1.5.3.2" class="ltx_td ltx_align_right">2000</td>
<td id="S4.T2.1.5.3.3" class="ltx_td ltx_align_right">12857</td>
<td id="S4.T2.1.5.3.4" class="ltx_td ltx_align_right">2059</td>
<td id="S4.T2.1.5.3.5" class="ltx_td ltx_align_right">9660</td>
<td id="S4.T2.1.5.3.6" class="ltx_td ltx_align_right">1138</td>
</tr>
<tr id="S4.T2.1.6.4" class="ltx_tr">
<th id="S4.T2.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_4</th>
<td id="S4.T2.1.6.4.2" class="ltx_td ltx_align_right">2000</td>
<td id="S4.T2.1.6.4.3" class="ltx_td ltx_align_right">13434</td>
<td id="S4.T2.1.6.4.4" class="ltx_td ltx_align_right">2453</td>
<td id="S4.T2.1.6.4.5" class="ltx_td ltx_align_right">8125</td>
<td id="S4.T2.1.6.4.6" class="ltx_td ltx_align_right">2856</td>
</tr>
<tr id="S4.T2.1.7.5" class="ltx_tr">
<th id="S4.T2.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_5</th>
<td id="S4.T2.1.7.5.2" class="ltx_td ltx_align_right">2000</td>
<td id="S4.T2.1.7.5.3" class="ltx_td ltx_align_right">10799</td>
<td id="S4.T2.1.7.5.4" class="ltx_td ltx_align_right">4430</td>
<td id="S4.T2.1.7.5.5" class="ltx_td ltx_align_right">6205</td>
<td id="S4.T2.1.7.5.6" class="ltx_td ltx_align_right">164</td>
</tr>
<tr id="S4.T2.1.8.6" class="ltx_tr">
<th id="S4.T2.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T2.1.8.6.1.1" class="ltx_text">Training</span></th>
<th id="S4.T2.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Camera_1</th>
<td id="S4.T2.1.8.6.3" class="ltx_td ltx_align_right ltx_border_t">1590</td>
<td id="S4.T2.1.8.6.4" class="ltx_td ltx_align_right ltx_border_t">8303</td>
<td id="S4.T2.1.8.6.5" class="ltx_td ltx_align_right ltx_border_t">548</td>
<td id="S4.T2.1.8.6.6" class="ltx_td ltx_align_right ltx_border_t">7499</td>
<td id="S4.T2.1.8.6.7" class="ltx_td ltx_align_right ltx_border_t">256</td>
</tr>
<tr id="S4.T2.1.9.7" class="ltx_tr">
<th id="S4.T2.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_2</th>
<td id="S4.T2.1.9.7.2" class="ltx_td ltx_align_right">1574</td>
<td id="S4.T2.1.9.7.3" class="ltx_td ltx_align_right">5979</td>
<td id="S4.T2.1.9.7.4" class="ltx_td ltx_align_right">698</td>
<td id="S4.T2.1.9.7.5" class="ltx_td ltx_align_right">5082</td>
<td id="S4.T2.1.9.7.6" class="ltx_td ltx_align_right">199</td>
</tr>
<tr id="S4.T2.1.10.8" class="ltx_tr">
<th id="S4.T2.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_3</th>
<td id="S4.T2.1.10.8.2" class="ltx_td ltx_align_right">1581</td>
<td id="S4.T2.1.10.8.3" class="ltx_td ltx_align_right">10121</td>
<td id="S4.T2.1.10.8.4" class="ltx_td ltx_align_right">1574</td>
<td id="S4.T2.1.10.8.5" class="ltx_td ltx_align_right">7656</td>
<td id="S4.T2.1.10.8.6" class="ltx_td ltx_align_right">891</td>
</tr>
<tr id="S4.T2.1.11.9" class="ltx_tr">
<th id="S4.T2.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_4</th>
<td id="S4.T2.1.11.9.2" class="ltx_td ltx_align_right">1583</td>
<td id="S4.T2.1.11.9.3" class="ltx_td ltx_align_right">10555</td>
<td id="S4.T2.1.11.9.4" class="ltx_td ltx_align_right">1950</td>
<td id="S4.T2.1.11.9.5" class="ltx_td ltx_align_right">6370</td>
<td id="S4.T2.1.11.9.6" class="ltx_td ltx_align_right">2235</td>
</tr>
<tr id="S4.T2.1.12.10" class="ltx_tr">
<th id="S4.T2.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_5</th>
<td id="S4.T2.1.12.10.2" class="ltx_td ltx_align_right">1607</td>
<td id="S4.T2.1.12.10.3" class="ltx_td ltx_align_right">8572</td>
<td id="S4.T2.1.12.10.4" class="ltx_td ltx_align_right">3493</td>
<td id="S4.T2.1.12.10.5" class="ltx_td ltx_align_right">4946</td>
<td id="S4.T2.1.12.10.6" class="ltx_td ltx_align_right">133</td>
</tr>
<tr id="S4.T2.1.13.11" class="ltx_tr">
<th id="S4.T2.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T2.1.13.11.1.1" class="ltx_text">Testing</span></th>
<th id="S4.T2.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Camera_1</th>
<td id="S4.T2.1.13.11.3" class="ltx_td ltx_align_right ltx_border_t">410</td>
<td id="S4.T2.1.13.11.4" class="ltx_td ltx_align_right ltx_border_t">2194</td>
<td id="S4.T2.1.13.11.5" class="ltx_td ltx_align_right ltx_border_t">128</td>
<td id="S4.T2.1.13.11.6" class="ltx_td ltx_align_right ltx_border_t">1998</td>
<td id="S4.T2.1.13.11.7" class="ltx_td ltx_align_right ltx_border_t">68</td>
</tr>
<tr id="S4.T2.1.14.12" class="ltx_tr">
<th id="S4.T2.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_2</th>
<td id="S4.T2.1.14.12.2" class="ltx_td ltx_align_right">426</td>
<td id="S4.T2.1.14.12.3" class="ltx_td ltx_align_right">1594</td>
<td id="S4.T2.1.14.12.4" class="ltx_td ltx_align_right">196</td>
<td id="S4.T2.1.14.12.5" class="ltx_td ltx_align_right">1335</td>
<td id="S4.T2.1.14.12.6" class="ltx_td ltx_align_right">63</td>
</tr>
<tr id="S4.T2.1.15.13" class="ltx_tr">
<th id="S4.T2.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_3</th>
<td id="S4.T2.1.15.13.2" class="ltx_td ltx_align_right">419</td>
<td id="S4.T2.1.15.13.3" class="ltx_td ltx_align_right">2736</td>
<td id="S4.T2.1.15.13.4" class="ltx_td ltx_align_right">485</td>
<td id="S4.T2.1.15.13.5" class="ltx_td ltx_align_right">2004</td>
<td id="S4.T2.1.15.13.6" class="ltx_td ltx_align_right">247</td>
</tr>
<tr id="S4.T2.1.16.14" class="ltx_tr">
<th id="S4.T2.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_4</th>
<td id="S4.T2.1.16.14.2" class="ltx_td ltx_align_right">417</td>
<td id="S4.T2.1.16.14.3" class="ltx_td ltx_align_right">2879</td>
<td id="S4.T2.1.16.14.4" class="ltx_td ltx_align_right">503</td>
<td id="S4.T2.1.16.14.5" class="ltx_td ltx_align_right">1755</td>
<td id="S4.T2.1.16.14.6" class="ltx_td ltx_align_right">621</td>
</tr>
<tr id="S4.T2.1.17.15" class="ltx_tr">
<th id="S4.T2.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_5</th>
<td id="S4.T2.1.17.15.2" class="ltx_td ltx_align_right">393</td>
<td id="S4.T2.1.17.15.3" class="ltx_td ltx_align_right">2227</td>
<td id="S4.T2.1.17.15.4" class="ltx_td ltx_align_right">937</td>
<td id="S4.T2.1.17.15.5" class="ltx_td ltx_align_right">1259</td>
<td id="S4.T2.1.17.15.6" class="ltx_td ltx_align_right">31</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="5"><span id="S4.T2.1.1.1.1" class="ltx_text"><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\alpha=0.1" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">Œ±</mi><mo id="S4.T2.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><eq id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></eq><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">ùõº</ci><cn type="float" id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\alpha=0.1</annotation></semantics></math></span></th>
<th id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Camera_1</th>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_right ltx_border_t">1590</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_right ltx_border_t">8303</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_right ltx_border_t">548</td>
<td id="S4.T2.1.1.6" class="ltx_td ltx_align_right ltx_border_t">7499</td>
<td id="S4.T2.1.1.7" class="ltx_td ltx_align_right ltx_border_t">256</td>
</tr>
<tr id="S4.T2.1.18.16" class="ltx_tr">
<th id="S4.T2.1.18.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_2</th>
<td id="S4.T2.1.18.16.2" class="ltx_td ltx_align_right">1411</td>
<td id="S4.T2.1.18.16.3" class="ltx_td ltx_align_right">5332</td>
<td id="S4.T2.1.18.16.4" class="ltx_td ltx_align_right">623</td>
<td id="S4.T2.1.18.16.5" class="ltx_td ltx_align_right">4527</td>
<td id="S4.T2.1.18.16.6" class="ltx_td ltx_align_right">182</td>
</tr>
<tr id="S4.T2.1.19.17" class="ltx_tr">
<th id="S4.T2.1.19.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_3</th>
<td id="S4.T2.1.19.17.2" class="ltx_td ltx_align_right">1284</td>
<td id="S4.T2.1.19.17.3" class="ltx_td ltx_align_right">8201</td>
<td id="S4.T2.1.19.17.4" class="ltx_td ltx_align_right">1302</td>
<td id="S4.T2.1.19.17.5" class="ltx_td ltx_align_right">6184</td>
<td id="S4.T2.1.19.17.6" class="ltx_td ltx_align_right">715</td>
</tr>
<tr id="S4.T2.1.20.18" class="ltx_tr">
<th id="S4.T2.1.20.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Camera_4</th>
<td id="S4.T2.1.20.18.2" class="ltx_td ltx_align_right">1117</td>
<td id="S4.T2.1.20.18.3" class="ltx_td ltx_align_right">7430</td>
<td id="S4.T2.1.20.18.4" class="ltx_td ltx_align_right">1361</td>
<td id="S4.T2.1.20.18.5" class="ltx_td ltx_align_right">4479</td>
<td id="S4.T2.1.20.18.6" class="ltx_td ltx_align_right">1590</td>
</tr>
<tr id="S4.T2.1.21.19" class="ltx_tr">
<th id="S4.T2.1.21.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Camera_5</th>
<td id="S4.T2.1.21.19.2" class="ltx_td ltx_align_right ltx_border_bb">937</td>
<td id="S4.T2.1.21.19.3" class="ltx_td ltx_align_right ltx_border_bb">4915</td>
<td id="S4.T2.1.21.19.4" class="ltx_td ltx_align_right ltx_border_bb">2050</td>
<td id="S4.T2.1.21.19.5" class="ltx_td ltx_align_right ltx_border_bb">2789</td>
<td id="S4.T2.1.21.19.6" class="ltx_td ltx_align_right ltx_border_bb">76</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Data division</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We aim to simulate real-world IoT FL with different regions and performance differences on surveillance cameras. This data partition is based on the real situation of the cameras and the data set suffers from non-IID data distribution problems. Combining the definition of object size in the Society of Photo-Optical Instrumentation Engineers (SPIE) and COCO data set, we take a relative size approach to the definition based on the characteristics of NIPD, where less than 0.12 percent of the image area is a small object, between 0.12 percent and 1.08 percent of the image area is a medium object, and greater than 1.08 percent of the image area is a large object.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Table 2 shows the detailed distribution of data and persons in different cameras according to the full data, the standard training data, the testing data, and the training data at <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\alpha</annotation></semantics></math> = 0.1. From this, we can see the bias of the data per client in this case, and the subsequent resulting bias in client training. In terms of the total number of persons, Camera_4 has the highest number of total personss, 77.4 percent higher than Camera_2, which has the lowest number of total persons. The different training number will lead to differences in the convergence degree of the model for different clients, and the training effect of clients with fewer persons will be weaker than those with more persons. In terms of large, medium and small persons, Camera_5 has the highest number of large objects, which is 6.55 times higher than Camera_1 with the lowest number of large persons; Camera_3 has the highest number of medium persons, which is 1.56 times higher than Camera_5 with the lowest number of medium persons; Camera_4 has the highest number of small persons, which is 17.41 times higher than Camera_5 with the lowest number of small persons. The networks trained by different clients have different scale preferences, which poses a great challenge for the aggregated object detection network to be able to identify persons at different scales simultaneously.
Thus, NIPD can effectively test the ability of IoT camera devices to solve real-world problems through FL, while serving as a benchmark to solve the non-IID data distribution problem in real-world applications.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Challenges in NIPD applications</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We have reflected on non-IID FL in light of some meaningful observations in the proposed data set and suggest some possible guidelines for the design of subsequent experimental use.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Object consistency</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Due to different geographical locations, different devices have different object scale distributions, which may cause their local models to be biased towards the size of a particular object. Therefore, in subsequent experiments, the aggregation algorithm can be reconsidered in conjunction with the scale of objects in the model device to enhance the generalization capability of the final model.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Selection of hyperparameter <math id="S4.SS4.SSS2.1.m1.1" class="ltx_Math" alttext="\boldsymbol{\alpha}" display="inline"><semantics id="S4.SS4.SSS2.1.m1.1b"><mi id="S4.SS4.SSS2.1.m1.1.1" xref="S4.SS4.SSS2.1.m1.1.1.cmml">ùú∂</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.1.m1.1c"><ci id="S4.SS4.SSS2.1.m1.1.1.cmml" xref="S4.SS4.SSS2.1.m1.1.1">ùú∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.1.m1.1d">\boldsymbol{\alpha}</annotation></semantics></math>
</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">To ensure imbalance in the amount of data, <math id="S4.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="{\alpha}" display="inline"><semantics id="S4.SS4.SSS2.p1.1.m1.1a"><mi id="S4.SS4.SSS2.p1.1.m1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.1.m1.1b"><ci id="S4.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.1.m1.1c">{\alpha}</annotation></semantics></math> can be selected from 0 - 0.2 in the subsequent experimental design according to the actual. The more severe the client data imbalance, the worse the aggregation model performs.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Group Normalization</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Batch Normalization (BN) is more likely to exacerbate the mismatch between the global mean and variance during non-IID training, thus affecting the validation accuracy. Therefore, <cite class="ltx_cite ltx_citemacro_cite">Hsieh <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> proposed that Group Normalization (GN) can be used to replace the BN structure in the network, and GN can overcome the shortcomings of BN and Layer Normalization (LN) in the non-IID case to a certain extent, providing more accurate mean and variance. However, GN is not widely used at present, whose adoption in practical experiments is still debatable in the context of specific situations.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Model Lightweighting</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">Since FL involves data exchange between multiple devices and model parameter update, communication efficiency is one of the important factors affecting the performance of FL. The model lightweight can reduce the parameter quantity and computational complexity of the model, and can reduce the communication bandwidth required by the device to transmit the model parameters, thereby improving the communication efficiency. For the YOLOv3 model, the number of model parameters can be reduced by using more lightweight network structure and model lightweight technologies such as pruning, thereby reducing communication cost. Taking Setting 1 as the experimental data set, we conducted some experiments, as shown in Table 3. Firstly, before training, the convolutional layer in YOLOv3 is replaced by depthwise seperable convolution (DS Conv). Compared with YOLOv3, the number of parameters is reduced by nearly half, and the number of parameters of the model uploaded and distributed by the client and server is reduced. When the model training meets certain mAP, the network slimming (NS) pruning <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> is used to prune the server side. Under the condition of basically maintaining the mAP, the number of parameters is reduced by more than half again, which reduces the number of parameters issued by the server to the client.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental environment settings </figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Number</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Number of</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T3.1.2.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column">parameters</th>
<td id="S4.T3.1.2.1.4" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">0</th>
<th id="S4.T3.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YOLOv3</th>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_right ltx_border_t">61,523,734</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_right ltx_border_t">82.0</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1</th>
<th id="S4.T3.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ DS Conv</th>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_right">34,322,198</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_right">81.2</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">2</th>
<th id="S4.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">+ DS Conv + NS</th>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb">16,342,727</td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb">80.9</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSION</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we set up an IoT camera experimental platform for the lack of non-IID person detection data sets in the IoT field, presenting the initial concept to the practical implementation. We propose a non-IID person detection dataset named NIPD, which is the first data set proposed for the non-IID person detection problem with three non-IID types: feature distribution skew-covariate shift, same label but different features, and data skewing. Moreover, we have conducted experiments on this dataset using YOLOv3 and Faster R-CNN to provide a non-IID person detection benchmark for others to conduct subsequent studies. We hope to carry out the next work in two ways. On FL, we will improve the communication policy and aggregation algorithms to reduce the communication cost in the wireless communication, and generate more personalized models for devices. On the data set, we will subsequently improve on the data set by labeling more classes, such as bicycle, electric vehicle, and person attributes, to satisfy the class imbalance of the non-IID data set.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported in part by the Natural Science Foundation of Xinjiang Uygur Autonomous Region (No.2022D01B187).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amit and Mohan [2022]</span>
<span class="ltx_bibblock">
Rasna¬†A Amit and C¬†Krishna Mohan.

</span>
<span class="ltx_bibblock">Federated learning: Dataset management for airport object
representations using remote sensing images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">2022 IEEE Aerospace Conference (AERO)</span>, pages 1‚Äì14. IEEE,
2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiu <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Te-Chuan Chiu, Yuan-Yao Shih, Ai-Chun Pang, Chieh-Sheng Wang, Wei Weng, and
Chun-Ting Chou.

</span>
<span class="ltx_bibblock">Semisupervised distributed learning with non-iid data for aiot
service platform.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 7(10):9266‚Äì9277, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Jiqiang Gao, Baolei Zhang, Xiaojie Guo, Thar Baker, Min Li, and Zheli Liu.

</span>
<span class="ltx_bibblock">Secure partial aggregation: Making federated learning more robust for
industry 4.0 applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He <span id="bib.bib4.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Yue He, Zheyan Shen, and Peng Cui.

</span>
<span class="ltx_bibblock">Towards non-iid image classification: A dataset and baselines.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, 110:107383, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh <span id="bib.bib5.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons.

</span>
<span class="ltx_bibblock">The non-iid data quagmire of decentralized machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4387‚Äì4398. PMLR, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz <span id="bib.bib6.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Peter Kairouz, H¬†Brendan McMahan, Brendan Avent, Aur√©lien Bellet, Mehdi
Bennis, Arjun¬†Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, et¬†al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">Foundations and Trends¬Æ in Machine Learning</span>,
14(1‚Äì2):1‚Äì210, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib7.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Tian Li, Anit¬†Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50‚Äì60, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">2022 IEEE 38th International Conference on Data Engineering
(ICDE)</span>, pages 965‚Äì978. IEEE, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu <span id="bib.bib9.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
Zhang.

</span>
<span class="ltx_bibblock">Learning efficient convolutional networks through network slimming.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 2736‚Äì2744, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Yang Liu, Anbu Huang, Yun Luo, He¬†Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng,
Tianjian Chen, Han Yu, and Qiang Yang.

</span>
<span class="ltx_bibblock">Fedvision: An online visual object detection platform powered by
federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume¬†34, pages 13172‚Äì13179, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo <span id="bib.bib11.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, and
Qiang Yang.

</span>
<span class="ltx_bibblock">Real-world image datasets for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.11089</span>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise¬†Aguera
y¬†Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273‚Äì1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niknam <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Solmaz Niknam, Harpreet¬†S Dhillon, and Jeffrey¬†H Reed.

</span>
<span class="ltx_bibblock">Federated learning for wireless communications: Motivation,
opportunities, and challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">IEEE Communications Magazine</span>, 58(6):46‚Äì51, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redmon and Farhadi [2018]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">Yolov3: An incremental improvement.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.02767</span>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren <span id="bib.bib15.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Zhongyuan Zhao, Chenyuan Feng, Wei Hong, Jiamo Jiang, Chao Jia, Tony¬†QS Quek,
and Mugen Peng.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data in wireless networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Wireless Communications</span>, 21(3):1927‚Äì1942,
2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.15931" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.15932" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.15932">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.15932" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.15933" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 22:29:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
