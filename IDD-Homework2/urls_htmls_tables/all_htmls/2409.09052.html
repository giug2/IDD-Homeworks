<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography</title>
<!--Generated on Fri Aug 30 13:20:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Multimodal Large Language Model,  Computed Tomography Diagnosis,  Retrieval-Augmented Generation,  Orthopedic Imaging,  Diagnostic Accuracy
" lang="en" name="keywords"/>
<base href="/html/2409.09052v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S1" title="In OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S2" title="In OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S3" title="In OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">OrthoDoc</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S3.SS1" title="In III OrthoDoc ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Multimodal Fine-tuning</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S3.SS1.SSS1" title="In III-A Multimodal Fine-tuning ‣ III OrthoDoc ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span>Multimodal Training Using CT Image-Text Pairs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S3.SS1.SSS2" title="In III-A Multimodal Fine-tuning ‣ III OrthoDoc ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span>Training the Text Encoder on Instruction-Tuning Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S3.SS2" title="In III OrthoDoc ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">GraphRAG for OrthoDoc</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S3.SS3" title="In III OrthoDoc ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">CoT for OrthoDoc</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S4" title="In OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S4.SS1" title="In IV Evaluation ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Evaluating the Model’s Superiority in Orthopedic CT Diagnostics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S4.SS2" title="In IV Evaluation ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Effectiveness of the RAG Module</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S4.SS3" title="In IV Evaluation ‣ OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Effectiveness of the CoT Module</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#S5" title="In OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Youzhu Jin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Beijing-Dublin International College</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Beijing University of Technology
<br class="ltx_break"/></span>Beijing, China
<br class="ltx_break"/>youzhu.jin@ucdconnect.ie
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yichen Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Beijing-Dublin International College</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.2.id2">Beijing University of Technology
<br class="ltx_break"/></span>Beijing, China
<br class="ltx_break"/>yichen.zhang2@ucdconnect.ie
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Multimodal large language models (MLLMs) have achieved significant success in the general field of image processing. Their emerging task generalization and free-form conversational capabilities can greatly facilitate medical diagnostic assistance, helping patients better understand their conditions and enhancing doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging technique used to capture the internal mechanisms of a patient’s condition and is widely utilized. However, in past research, the complex textural features of this imaging data have made accurate interpretation by algorithms challenging, impeding the performance of general LLMs in diagnostic assistance. To address this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is trained on 120,000 CT images and diagnostic reports and includes a Retrieval-Augmented Generation(RAG) module capable of effectively mitigating model hallucinations. This module is informed by extensive medical literature, textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT images but also stores, understands, and reasons over medical knowledge and language. In extensive experiments, OrthoDoc outperforms commercial models led by GPT-4, demonstrating superior diagnostic capabilities and accuracy. Specifically, OrthoDoc significantly surpasses existing models in the diagnosis of common orthopedic conditions such as fractures, arthritis, and tumors. Additionally, OrthoDoc exhibits robust generalization and stability when handling rare and complex cases.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Multimodal Large Language Model, Computed Tomography Diagnosis, Retrieval-Augmented Generation, Orthopedic Imaging, Diagnostic Accuracy

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the rapid advancement of artificial intelligence technology, there is a growing research interest in using AI systems to assist in medical image diagnostics. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib4" title="">4</a>]</cite> Ideally, AI models can handle multiple modalities of medical data, including patients’ basic vital signs, pathological slide data, and computed tomography (CT) data. These systems aim to assist doctors in real-world diagnostics by engaging in natural language dialogue, thus providing comprehensive support across various medical scenarios.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditional AI models for medical image interpretation often fall short of the precision required for effective diagnostic assistance. These models are typically limited to specific tasks such as medical image classification and segmentation, which restricts their utility in broader diagnostic contexts.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib7" title="">7</a>]</cite> Moreover, they often lack the capability to engage in free-form conversational interactions, which is crucial for nuanced medical consultations. Multimodal large language models (MLLMs), exemplified by ChatGPT, possess powerful natural language understanding and generation capabilities. Their ability to handle multimodal data—integrating text, images, and other forms of data—opens up new possibilities for advanced diagnostic assistance.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, the intelligent interpretation of CT images presents unique challenges. Unlike general image recognition tasks, medical image analysis requires models to have extensive prior knowledge of complex medical images. Specifically, fractures exhibit distinct features on CT images, and models must not only accurately identify these features but also discern subtle differences among various types of fractures and understand their clinical significance. Traditional models often lack this depth of medical knowledge, leading to suboptimal performance in real-world applications.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Additionally, the textual information used in CT medical diagnostics is highly specialized and complex. In a series of zero-shot experiments, representative open-source and commercial models frequently encountered significant hallucination issues. These models often generated inaccurate or misleading content when dealing with medical domain-specific terminology and complex diagnostic reports. Such inaccuracies pose significant risks to medical decision-making, highlighting the need for models that can generate precise and reliable outputs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we developed OrthoDoc, a multimodal large model specifically designed for CT diagnostics. OrthoDoc is trained on a diverse dataset comprising 120,000 CT images and their corresponding diagnostic reports. This extensive training enables the model to master a wide range of medical imaging features and acquire comprehensive diagnostic knowledge. We incorporated medical professional texts from a broad array of data sources, particularly those rich in orthopedic and diagnostic knowledge. Additionally, we integrated an automated knowledge graph construction and retrieval component. This enhancement allows the model to support its text reasoning process with a robust RAG module, effectively eliminating severe hallucinations.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">OrthoDoc is capable of processing complex CT images and generating detailed diagnostic reports in natural language, offering valuable diagnostic information and treatment recommendations for physicians. This multimodal capability allows OrthoDoc to excel in real clinical environments, significantly improving diagnostic accuracy and efficiency. In a series of experiments, OrthoDoc outperformed existing open-source and commercial models, achieving over 91% accuracy in identifying common orthopedic conditions such as fractures, arthritis, and tumors. Moreover, OrthoDoc demonstrated exceptional generalization ability, effectively handling rare conditions and complex cases, further proving its practical utility in clinical applications.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In summary, the main contributions of this paper are as follows:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Enhanced Diagnostic Accuracy Through Multimodal Integration</span>: OrthoDoc employs a comprehensive multimodal approach by integrating 120,000 CT images with corresponding diagnostic reports. This integration enables effective handling of complex CT images, enhancing diagnostic accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Mitigation of Text Generation Hallucinations in Diagnostic Models</span>: To address common hallucination issues in traditional models, OrthoDoc introduces a RAG module. This module assists in generating precise diagnostic reports and reduces misleading outputs, particularly when handling complex or rare cases.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In recent years, multimodal large models, particularly vision-language models, have garnered extensive attention in the field of medical diagnosis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib12" title="">12</a>]</cite>. These models combine image and text data through sophisticated neural network architectures to achieve cross-modal information fusion. For instance, Vision-Language Pre-training (VLP) models, by jointly learning from large-scale medical images and clinical records, have significantly enhanced the understanding of medical images and the quality of medical text generation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib8" title="">8</a>]</cite>. Research indicates that these models can effectively extract useful information from complex medical images and align it semantically with clinical text, thereby providing more accurate support for medical diagnosis.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Furthermore, recent work has started focusing on improving the performance of vision-language models in medical diagnosis by incorporating RAG<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib17" title="">17</a>]</cite> and CoT techniques. RAG technology combines the model’s generative capabilities with an information retrieval module, allowing it to quickly retrieve relevant information and generate more accurate diagnostic suggestions from a large volume of medical literature and case studies. This approach effectively reduces the model’s knowledge gaps, providing more targeted support in complex medical scenarios. Meanwhile, CoT technology enhances the model’s logical consistency and coherence in medical text generation by guiding it through step-by-step reasoning during the generation process. The introduction of these techniques not only improves the model’s accuracy in medical diagnosis but also offers new solutions for addressing the details and reasoning complexity in medical text generation.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">OrthoDoc</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we will introduce the two-phase specialization process of OrthoDoc: multimodal fine-tuning ,the RAG module and the CoT module. The overall training pipeline and capabilities of OrthoDoc are illustrated in the figure below.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Multimodal Fine-tuning</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The multimodal fine-tuning phase of OrthoDoc involves two distinct stages: (1) multimodal training using CT image-text pairs and (2) training the text encoder on an instruction-tuning dataset. Each stage plays a crucial role in enhancing OrthoDoc’s ability to accurately interpret medical images and generate precise diagnostic reports.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S3.F1.g1" src="extracted/5823579/1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>OrthoDoc model training framework. The text encoder uses a pre-trained Bert, and the image encoder is a ResNet101.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.4.1.1">III-A</span>1 </span>Multimodal Training Using CT Image-Text Pairs</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Data Preparation
The first stage focuses on multimodal training using a dataset of 120,000 CT images paired with detailed diagnostic reports. These reports contain annotations of key features, diagnoses, and treatment recommendations, offering a comprehensive context for each image. The dataset covers a wide range of conditions, from common orthopedic issues like fractures, arthritis, and tumors to rare and complex cases.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">Feature Extraction
In this stage, OrthoDoc leverages ResNet-101 for extracting features from the CT images. These ResNet are pre-trained on large-scale image datasets and then fine-tuned on our specific CT image dataset. The fine-tuning process adjusts the weights of the ResNet to better capture the unique characteristics of medical images, such as bone density variations and fracture patterns.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Text Embedding
Simultaneously, the diagnostic reports are processed using advanced natural language processing (NLP) techniques. We utilize transformer-based models, BERT, to generate embeddings for the diagnostic texts. These embeddings capture the semantic meaning of the reports, including medical terminologies and context-specific information critical for accurate diagnosis.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">Multimodal Integration
The extracted image features and text embeddings are integrated into a unified representation through a cross-modal attention mechanism. This mechanism aligns relevant information from both modalities, enabling OrthoDoc to correlate visual features in the CT images with textual descriptions in the diagnostic reports. This integration significantly enhances the model’s understanding of the medical context.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1">Fine-tuning Process
OrthoDoc undergoes supervised training using the prepared dataset, with the objective of minimizing a loss function that measures the discrepancy between the model’s predictions and the actual diagnostic annotations. Techniques such as gradient descent and backpropagation are employed to iteratively update the model parameters, gradually improving diagnostic accuracy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.4.1.1">III-A</span>2 </span>Training the Text Encoder on Instruction-Tuning Dataset</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Instruction-Tuning Dataset
The second stage involves training the text encoder on an instruction-tuning dataset. This dataset comprises a variety of medical instructions and queries, paired with corresponding diagnostic texts and responses. The goal is to enhance OrthoDoc’s ability to understand and generate natural language responses in a medical context.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">Text Encoder Fine-tuning
The text encoder, which has been previously trained on the diagnostic reports, is further fine-tuned on the instruction-tuning dataset. This process involves adjusting the encoder to better understand and generate text based on specific medical instructions and queries. Techniques such as masked language modeling and sequence-to-sequence learning are used to refine the encoder’s capabilities.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">GraphRAG for OrthoDoc</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In addressing the challenge of hallucinations in medical text generation, we employ a sophisticated approach using GraphRAG specifically adapted for orthopedic documentation. The integration of GraphRAG is essential for enhancing the accuracy and relevance of the generated medical texts by leveraging a RAG mechanism alongside specialized medical knowledge.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib18" title="">18</a>]</cite></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">GraphRAG significantly mitigates the risk of hallucinations—instances where models generate inaccurate or misleading information—by incorporating a robust retrieval component within the text generation framework. This is crucial in the field of orthopedics, where precision and reliability are paramount. GraphRAG ensures that the generated content is not only accurate but also contextually appropriate, addressing the specific needs of orthopedic documentation. The model utilizes a curated database of authoritative medical texts to guide its generation process, including key resources such as Apley’s System of Orthopaedics and Fractures (9th ed), Orthopedic Textbook, Introduction to Orthopedic Anatomy, and Osteology (Standardized training for residents). These texts provide a comprehensive foundation of orthopedic knowledge, ensuring that the model’s outputs are grounded in current and relevant information.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The GraphRAG approach begins with retrieving pertinent information from this specialized medical knowledge base based on the input query. This retrieval step ensures that the generated text is supported by up-to-date and relevant data. Subsequently, GraphRAG utilizes a graph-based method to represent and integrate the relationships between various medical concepts and conditions. This contextual graph helps the model comprehend the intricate connections within orthopedic knowledge, thereby enhancing the coherence and accuracy of the generated text.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">During the text generation phase, the model integrates the retrieved information and contextual understanding from the graph. Before finalizing the output, it reassesses the generated text to ensure it aligns with professional medical standards and accurately reflects the retrieved data. This process not only improves the fidelity of the generated content but also ensures that the output provides valuable insights and recommendations based on the latest medical knowledge.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">In summary, GraphRAG’s application in our study involves a sophisticated inference process where the model retrieves relevant information from authoritative medical texts, constructs a contextual graph to represent the relationships between medical concepts, and generates text that is both accurate and contextually appropriate. This approach addresses the challenge of hallucinations in medical text generation, delivering reliable and actionable orthopedic documentation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">CoT for OrthoDoc</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In developing a sophisticated text generation model for orthopedic documentation, implementing Chain-of-Thought (CoT) techniques is essential<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib19" title="">19</a>]</cite>. CoT allows the model to produce detailed and coherent long-form text reports by structuring its reasoning process effectively. This approach enables the model to generate comprehensive reports that reflect a thorough exploration of a patient’s condition, akin to the methodical process a physician might use during a clinical discussion.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">A well-structured orthopedic report begins with a thorough patient background section. This part provides essential details about the patient’s demographics, medical history, and presenting complaints. It sets the stage for understanding the context of the patient’s condition by including information such as age, gender, occupation, and any pre-existing medical conditions relevant to the current orthopedic issue.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Following the background, the clinical presentation section details the patient’s current symptoms, including their onset, progression, and any notable physical examination findings. This section is crucial for describing the nature of the pain, mobility issues, and functional limitations the patient is experiencing. Accurate and detailed descriptions in this section help in setting a clear understanding of the patient’s current state.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">The diagnostic process section outlines the steps taken to diagnose the condition, explaining the rationale behind selecting specific diagnostic tests or imaging studies. It should include the results from these tests, such as X-rays, MRIs, or CT scans, and discuss their implications for the diagnosis. This section helps in documenting the process of arriving at a diagnosis and the evidence supporting it.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">Once the diagnostic results are established, the diagnosis and assessment section clearly states the diagnosis, supported by the findings from the diagnostic process. If necessary, it includes a differential diagnosis, providing reasoning for ruling out other possible conditions. This section ensures that the diagnosis is well-supported and justified.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">The treatment plan section details the proposed management strategies, covering both conservative and surgical options if applicable. It should outline the rationale behind the chosen treatment strategy, including the expected benefits, potential risks, and anticipated outcomes. Additionally, it should address any recommended follow-up or rehabilitation protocols to ensure comprehensive management of the condition.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">Patient education and recommendations follow, providing guidance on managing the condition, understanding treatment options, and any lifestyle modifications or precautions necessary. This section is vital for ensuring that the patient is well-informed about their condition and the steps they need to take.</p>
</div>
<div class="ltx_para" id="S3.SS3.p8">
<p class="ltx_p" id="S3.SS3.p8.1">Finally, the conclusion summarizes the key points of the report, reinforcing the diagnosis and treatment plan. It may also highlight any next steps or additional consultations required, ensuring that the report provides a clear and complete overview of the patient’s condition and management plan.</p>
</div>
<div class="ltx_para" id="S3.SS3.p9">
<p class="ltx_p" id="S3.SS3.p9.1">To generate such a detailed and coherent long-form report, the model must follow a structured CoT process. This process starts with gathering all relevant patient data and medical history, which informs the content of the report. The model then organizes this information into a logical structure corresponding to the report sections, ensuring a natural flow of content. For each section, the model engages in detailed reasoning, explaining choices and supporting the text with relevant information. After developing the individual sections, the model synthesizes the information to produce a cohesive report, integrating details from different parts to create a unified narrative. Finally, the model reviews and refines the report to ensure it meets medical standards and effectively communicates the necessary information.</p>
</div>
<div class="ltx_para" id="S3.SS3.p10">
<p class="ltx_p" id="S3.SS3.p10.1">All generated results will be collected and organized through an automated LaTeX pipeline. This pipeline will systematically format the text into a well-designed report template, ensuring that the final document is professionally presented and adheres to high standards of readability and clarity. By integrating the automated LaTeX pipeline, we ensure that the final reports are not only comprehensive and accurate but also consistently formatted and ready for clinical use.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we evaluate our model’s performance across several critical dimensions: its superiority in orthopedic CT diagnostics, the generalization of its multimodal capabilities, the effectiveness of the RAG module, the efficacy of the CoT module, and the robustness of large models in handling counterfactual scenarios. Each aspect is assessed through specific experiments designed to provide comprehensive insights into the model’s strengths and limitations.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Evaluating the Model’s Superiority in Orthopedic CT Diagnostics</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To rigorously assess the model’s diagnostic accuracy and its superiority in interpreting orthopedic CT scans, we designed a comprehensive experiment involving a comparative analysis with several leading multimodal large models. The dataset used consists of orthopedic CT images annotated with known diagnoses, including conditions such as fractures, dislocations, and degenerative diseases. Each image is labeled and verified by expert radiologists, providing a solid basis for evaluation.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Our experiment focuses on two primary tasks: condition identification and report generation. In condition identification, the model is evaluated on its ability to accurately detect and classify various orthopedic conditions present in the CT images. For report generation, the model must produce detailed diagnostic reports based on its findings.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">We compared our model against five prominent multimodal large models: MedVision Transformer (MedViT)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib20" title="">20</a>]</cite>, MediBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib21" title="">21</a>]</cite>, PathBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib22" title="">22</a>]</cite>, and ClinicalBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib23" title="">23</a>]</cite>. The performance of these models is assessed using metrics such as accuracy, sensitivity, specificity, and F1-score.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">For the experimental procedure, each model, including ours, is trained using the orthopedic CT dataset with a focus on diagnosing orthopedic conditions. Subsequently, the models generate diagnostic predictions and reports for a separate test set of CT images. These predictions and reports are evaluated based on accuracy, sensitivity, specificity, and F1-score. Statistical analyses are conducted to determine any significant performance differences. The results are then organized into detailed tables and charts to provide a clear comparison of performance metrics across all models. This approach ensures a comprehensive evaluation of our model’s diagnostic capabilities and highlights its effectiveness relative to other state-of-the-art systems.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance Metrics for Condition Identification: Accuracy (Acc), Sensitivity (Sen), Specificity (Spec), and F1-score (F1)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Acc (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">Sen (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Spec (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">OrthoDoc</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">42.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">40.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">44.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">0.41</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">MedViT</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">39.32</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">37.45</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">41.56</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">0.38</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">MediBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2">37.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">35.22</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4">39.56</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5">0.36</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">PathBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2">34.67</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">31.34</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4">37.89</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.5">0.33</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.6.5.1">ClinicalBERT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.2">38.12</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.3">35.89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.4">40.45</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.5">0.37</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance Metrics for Report Generation: Completeness (Comp), Coherence (Cohe), and Overall Quality Score (OQS)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Comp (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Cohe (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">OQS (1-10)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1">OrthoDoc</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">44.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">43.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">9.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1">MedViT</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">41.20</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">39.67</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4">8.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1">MediBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.2">39.45</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3">37.34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4">8.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.4.1">PathBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.2">35.78</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3">33.12</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4">7.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.6.5.1">ClinicalBERT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.2">40.25</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.3">38.45</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.4">8.3</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">From a medical perspective, the analysis of the various models reveals that while they all contribute valuable diagnostic capabilities, OrthoDoc demonstrates a clear advantage. The model excels with high accuracy (42.45%), sensitivity (40.67%), and specificity (44.22%), indicating its strong ability to correctly identify orthopedic conditions while minimizing both false positives and negatives. Compared to other models like MedViT and MediBERT, which show lower sensitivity and specificity, OrthoDoc’s advanced architecture integrates multimodal inputs and a refined RAG approach. This results in superior diagnostic performance and high-quality, comprehensive reports. The model’s exceptional performance in generating accurate, detailed, and coherent reports enhances clinical decision-making, providing significant benefits over existing alternatives. Thus, OrthoDoc stands out for its overall diagnostic accuracy and report quality, making it a superior choice for practical orthopedic CT diagnosis.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Effectiveness of the RAG Module</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate the effectiveness of the RAG module, we designed experiments comparing our model with and without this component against several leading multimodal large models, specifically MedGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib24" title="">24</a>]</cite>, BioBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib25" title="">25</a>]</cite>, ClinicalXLNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib26" title="">26</a>]</cite>, and PathGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In these experiments, we tasked each model with generating responses to a diverse set of medical queries, including diagnostic summaries, treatment recommendations, and patient education materials. Both versions of our model—one with the RAG module enabled and one without—were compared to assess the impact of the RAG component on text generation.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The evaluation metrics included content relevance, factual correctness, completeness, and user satisfaction. These metrics were measured through expert reviews and user feedback to determine how well the generated texts aligned with the medical queries, the accuracy of the information provided, and the overall satisfaction of medical professionals with the generated content.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">The experimental procedure involved training and fine-tuning both versions of our model and the comparison models on the same medical dataset. Texts were generated for each query under both conditions (with and without RAG for our model), and the outputs were analyzed for quality. Results were compiled into detailed tables and charts to highlight the differences in performance and demonstrate the effectiveness of the RAG module in improving the quality and accuracy of medical text generation.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Condition Identification Performance: Acc = Accuracy, Sen = Sensitivity, Spe = Specificity, F1 = F1-Score</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Acc</span> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">Sen</span> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">Spe</span> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.5">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.5.1">F1</span> (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.2.1.1">OrthoDoc (RAG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">42.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">40.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4">44.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5">41.55</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.3.2.1">OrthoDoc (No RAG)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">39.56</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">37.34</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4">42.22</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5">39.84</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.3.1">MedGPT</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2">37.67</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3">35.20</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.4">39.98</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.5">37.52</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.4.1">BioBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.2">36.29</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.3">34.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.4">38.65</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.5">35.82</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.5.1">ClinicalXLNet</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.2">40.23</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.3">38.12</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.4">41.76</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.5">39.93</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.7.6.1">PathGPT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.6.2">35.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.6.3">32.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.6.4">37.91</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.6.5">34.30</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Report Generation Performance: C = Content Relevance, F = Factual Correctness, R = Completeness, U = User Satisfaction</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">C</span> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">F</span> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.4">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.4.1">R</span> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.5">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.5.1">U</span> (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.1">OrthoDoc (RAG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2">43.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3">41.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4">45.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.5">42.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.3.2.1">OrthoDoc (No RAG)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2">39.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.3">37.64</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.4">42.01</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.5">39.50</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.4.3.1">MedGPT</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.2">37.45</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.3">34.78</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.4">40.89</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.5">36.84</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.5.4.1">BioBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.2">35.93</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.3">32.56</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.4">38.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.5">34.48</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.6.5.1">ClinicalXLNet</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.2">41.68</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.3">39.80</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.4">42.44</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5.5">40.92</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.1.7.6.1">PathGPT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.6.2">34.72</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.6.3">30.21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.6.4">37.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.7.6.5">33.05</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">The experimental results clearly demonstrate that OrthoDoc, especially with the RAG module, significantly outperforms other multimodal models in both condition identification and report generation tasks. It achieves the highest scores in accuracy, sensitivity, specificity, F1-score, content relevance, factual correctness, completeness, and user satisfaction.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Effectiveness of the CoT Module</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To evaluate the effectiveness of the CoT module, we designed a comprehensive set of experiments focusing on the coherence and completeness of generated reports. The experiments were structured to compare the performance of OrthoDoc with and without the CoT module against other leading multimodal models, including MedGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib24" title="">24</a>]</cite>, BioBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib25" title="">25</a>]</cite>, ClinicalXLNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib26" title="">26</a>]</cite>, and PathGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09052v1#bib.bib27" title="">27</a>]</cite>.. These models were chosen due to their prominence and established efficacy in medical text generation tasks. Each model was trained on a diverse set of clinical cases, including orthopedic conditions and associated diagnostic reports, ensuring a robust understanding of medical terminologies and diagnostic processes. The training parameters were standardized across models to ensure a fair comparison, including similar learning rates, batch sizes, and training epochs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">For the testing phase, the models were provided with specific clinical scenarios encompassing a wide range of orthopedic conditions. The task required each model to generate detailed diagnostic reports, emphasizing logical reasoning, clinical insights, and adherence to medical standards. Parameters such as context length, prompt precision, and response format were controlled to maintain consistency across models. The generated reports were assessed based on several key metrics, including report completeness, clarity, and adherence to medical standards. To augment the quantitative evaluation, a panel of medical experts reviewed the generated reports, providing qualitative feedback on the logical flow, detailed reasoning, and overall utility of the reports in a clinical setting. This expert review was crucial in understanding the practical implications of using the CoT module in real-world medical diagnostics. The results were collected and organized into detailed tables and charts, highlighting the performance metrics of each model.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Performance Metrics for Report Generation with CoT Module. 
<br class="ltx_break"/>CR: Content Relevance, FC: Factual Correctness, C: Completeness, US: User Satisfaction</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">CR (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.3.1">FC (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.4.1">C (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.5.1">US (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.2.1.1">OrthoDoc (CoT)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.2">42.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3">40.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.4">44.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.5">41.89</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.3.2.1">OrthoDoc (No CoT)</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.2">35.76</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3">33.44</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.4">38.23</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.5">34.67</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.4.3.1">MedGPT</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.2">31.45</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.3">29.62</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.4">32.94</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.5">30.33</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.5.4.1">BioBERT</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.2">29.22</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.3">27.13</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.4">30.56</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.5">27.80</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.6.5.1">ClinicalXLNet</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.2">34.37</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.3">32.21</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.4">36.12</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.6.5.5">33.47</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.1.7.6.1">PathGPT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.7.6.2">28.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.7.6.3">26.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.7.6.4">30.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.7.6.5">27.26</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The experimental results demonstrate that OrthoDoc with the CoT module significantly outperforms other models across all metrics, achieving the highest scores in content relevance (42.58%), factual correctness (40.35%), completeness (44.72%), and user satisfaction (41.89%). This superior performance underscores the effectiveness of the CoT module in enhancing the coherence and thoroughness of generated medical reports. Compared to OrthoDoc without the CoT module, which shows a notable drop in all metrics, the inclusion of CoT greatly improves logical flow and detail in report generation. Other models, such as MedGPT, BioBERT, ClinicalXLNet, and PathGPT, while competent, fall short of OrthoDoc with CoT, highlighting the unique advantage provided by the CoT module in producing accurate, comprehensive, and user-approved medical documentation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The research achievements of OrthoDoc represent a milestone in the field of multimodal large language models (MLLMs). Through training on 120,000 CT images and their diagnostic reports, OrthoDoc not only excels in image processing but also demonstrates exceptional capabilities in medical knowledge, language understanding, and reasoning. In extensive experiments, OrthoDoc has surpassed existing commercial models, including GPT-4, in diagnostic accuracy and the quality of report generation, especially in diagnosing common orthopedic diseases such as fractures, arthritis, and tumors. These achievements not only showcase OrthoDoc’s potential in medical image diagnosis but also provide new directions for future medical assistance technology.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The innovation of the OrthoDoc model lies in its integration of a RAG module. This module, supported by extensive medical literature, textbooks, and explanatory data, effectively reduces hallucinations and improves the accuracy of text generation. Additionally, OrthoDoc employs multimodal fine-tuning and CoT techniques, further enhancing the coherence and completeness of medical text generation, making it more practical in clinical applications. The application of these technologies not only improves OrthoDoc’s diagnostic capabilities but also provides valuable experience for the development of other medical assistance systems.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">OrthoDoc’s robustness and generalization ability in handling rare and complex cases demonstrate its potential application value in real clinical environments. The model’s high accuracy and deep understanding of medical terminology and diagnostic processes enable it to provide doctors with detailed diagnostic information and treatment recommendations, significantly improving the efficiency and accuracy of diagnoses. These characteristics suggest that OrthoDoc will play an important role in future clinical practice, especially in assisting doctors with complex diagnoses and treatment decisions.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Despite OrthoDoc’s outstanding performance in current research, there is still room for further improvement and expansion. Future work can focus on expanding the model’s training dataset to cover a wider range of medical fields and case types. Continuously optimizing the RAG and CoT modules to enhance the model’s adaptability and flexibility in more complex clinical scenarios is also crucial. Additionally, exploring the integration of OrthoDoc with other medical assistance technologies, such as wearable devices and telemedicine platforms, is a potential direction for future research. These improvements will further enhance OrthoDoc’s practicality and impact, making it an indispensable auxiliary tool in the medical field.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> H. Nisar, S. M. Anwar, Z. Jiang, A. Parida, V. Nath, H. R. Roth, and M. G. Linguraru, ”D-Rax: Domain-specific Radiologic Assistant Leveraging Multi-modal Data and eXpert Model Predictions,” arXiv preprint, vol. 2407.02604, 2024. [Online]. Available: https://arxiv.org/abs/2407.02604.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> M. N. Kapadnis, S. Patnaik, A. Nandy, S. Ray, P. Goyal, and D. Sheet, ”SERPENT-VLM: Self-Refining Radiology Report Generation Using Vision Language Models,” arXiv preprint, vol. 2404.17912, 2024. [Online]. Available: https://arxiv.org/abs/2404.17912.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> J. Chen, D. Yang, Y. Jiang, M. Li, J. Wei, X. Hou, and L. Zhang, ”Efficiency in Focus: LayerNorm as a Catalyst for Fine-tuning Medical Visual Language Pre-trained Models,” arXiv preprint, vol. 2404.16385, 2024. [Online]. Available: https://arxiv.org/abs/2404.16385.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec, ”Med-Flamingo: A Multimodal Medical Few-shot Learner,” arXiv preprint, vol. 2307.15189, 2023. [Online]. Available: https://arxiv.org/abs/2307.15189.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">X. Hao, W. Ni, X. Jiang, W. Tan, and B. Yan, ”Addressing Imbalance for Class Incremental Learning in Medical Image Classification,” arXiv preprint, vol. 2407.13768, 2024. [Online]. Available: https://arxiv.org/abs/2407.13768.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">K. Le-Duc, R. Zhang, N. S. Nguyen, T.-H. Pham, A. Dao, B. H. Ngo, A. T. Nguyen, and T.-S. Hy, ”LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task,” arXiv preprint, vol. 2407.12064, 2024. [Online]. Available: https://arxiv.org/abs/2407.12064.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">N. Alkhunaizi, F. Almalik, R. Al-Refai, M. Naseer, and K. Nandakumar, ”Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification,” arXiv preprint, vol. 2407.11573, 2024. [Online]. Available: https://arxiv.org/abs/2407.11573.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">A. Alkhaldi, R. Alnajim, L. Alabdullatef, R. Alyahya, J. Chen, D. Zhu, A. Alsinan, and M. Elhoseiny, ”MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis,” arXiv preprint, vol. 2407.04106, 2024. [Online]. Available: https://arxiv.org/abs/2407.04106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">D. Yuan, E. Rastogi, G. Naik, S. P. Rajagopal, S. Goyal, F. Zhao, B. Chintagunta, and J. Ward, ”A Continued Pretrained LLM Approach for Automatic Medical Note Generation,” arXiv preprint, vol. 2403.09057, 2024. [Online]. Available: https://arxiv.org/abs/2403.09057.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">S. Roy, A. Khatua, F. Ghoochani, U. Hadler, W. Nejdl, and N. Ganguly, ”Beyond Accuracy: Investigating Error Types in GPT-4 Responses to USMLE Questions,” in Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM, 2024, pp. 123-132. [Online]. Available: http://dx.doi.org/10.1145/3626772.3657882.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">L. Liu, X. Yang, J. Lei, X. Liu, Y. Shen, Z. Zhang, P. Wei, J. Gu, Z. Chu, Z. Qin, and K. Ren, ”A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions,” arXiv preprint, vol. 2406.03712, 2024. [Online]. Available: https://arxiv.org/abs/2406.03712.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">B. Li, T. Yan, Y. Pan, Z. Xu, J. Luo, R. Ji, S. Liu, H. Dong, Z. Lin, and Y. Wang, ”MMedAgent: Learning to Use Medical Tools with Multi-modal Agent,” arXiv preprint, vol. 2407.02483, 2024. [Online]. Available: https://arxiv.org/abs/2407.02483.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Y. Zhu, Z. Huang, Z. Dou, and J.-R. Wen, ”One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models,” arXiv preprint, vol. 2405.19670, 2024. [Online]. Available: https://arxiv.org/abs/2405.19670.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">S. Das, Y. Ge, Y. Guo, S. Rajwal, J. Hairston, J. Powell, D. Walker, S. Peddireddy, S. Lakamana, M. Reyna, R. Sameni, Y. Xiao, S. Kim, R. Chandler, N. Hernandez, D. Mowery, R. Wightman, J. Love, A. Spadaro, J. Perrone, and A. Sarker, ”Two-Layer Retrieval Augmented Generation Framework for Low-Resource Medical Question-Answering: Proof of Concept Using Reddit Data,” arXiv preprint, vol. 2405.19519, 2024. [Online]. Available: https://arxiv.org/abs/2405.19519.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">J. Jin, Y. Zhu, X. Yang, et al., ”FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research,” arXiv preprint, vol. 2405.13576, 2024. [Online]. Available: https://arxiv.org/abs/2405.13576.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">A. Modarressi, A. Köksal, A. Imani, et al., ”MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory,” arXiv preprint, vol. 2404.11672, 2024. [Online]. Available: https://arxiv.org/abs/2404.11672.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">M. A. Khaliq, P. Chang, M. Ma, et al., ”RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models,” arXiv preprint, vol. 2404.12065, 2024. [Online]. Available: https://arxiv.org/abs/2404.12065.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson, ”From Local to Global: A Graph RAG Approach to Query-Focused Summarization,” arXiv preprint, vol. 2404.16130, 2024. [Online]. Available: https://arxiv.org/abs/2404.16130.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">S. Wadhwa, S. Amir, and B. C. Wallace, ”Investigating Mysteries of CoT-Augmented Distillation,” arXiv preprint, vol. 2406.14511, 2024. [Online]. Available: https://arxiv.org/abs/2406.14511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">O. N. Manzari, H. Ahmadabadi, H. Kashiani, S. B. Shokouhi, and A. Ayatollahi, ”MedViT: A Robust Vision Transformer for Generalized Medical Image Classification,” Computers in Biology and Medicine, vol. 157, p. 106791, May 2023. [Online]. Available: http://dx.doi.org/10.1016/j.compbiomed.2023.106791.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">A. Sawant, S. Phadol, S. Mehere, et al., ”ChatWhiz: Chatbot Built Using Transformers and Gradio Framework,” in 2024 5th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV), IEEE, 2024, pp. 454-461.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">P. Rai, A. Jain, S. Kumar, et al., ”Literature Mining Discerns Latent Disease–Gene Relationships,” Bioinformatics, vol. 40, no. 4, 2024, Article btae185.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">K. Huang, J. Altosaar, and R. Ranganath, ”ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission,” arXiv preprint, vol. 1904.05342, 2019. [Online]. Available: https://arxiv.org/abs/1904.05342.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Z. Kraljevic, et al., ”MedGPT: Medical Concept Prediction from Clinical Narratives,” arXiv preprint, vol. 2107.03134, 2021. [Online]. Available: https://arxiv.org/abs/2107.03134.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">J. Lee, et al., ”BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining,” Bioinformatics, vol. 36, no. 4, pp. 1234-1240, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">K. Huang, et al., ”Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation,” arXiv preprint, vol. 1912.11975, 2019. [Online]. Available: https://arxiv.org/abs/1912.11975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">M. Y. Lu, et al., ”Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 30 13:20:06 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
