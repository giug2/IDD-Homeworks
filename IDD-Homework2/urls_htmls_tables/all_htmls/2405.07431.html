<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.07431] Packing Peanuts: The Role Synthetic Data Can Play in Enhancing Conventional Economic Prediction Models</title><meta property="og:description" content="Packing peanuts, as defined by Wikipedia, is a common loose-fill packaging and cushioning material that helps prevent damage to fragile items. In this paper, I propose that synthetic data, akin to packing peanuts, can …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Packing Peanuts: The Role Synthetic Data Can Play in Enhancing Conventional Economic Prediction Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Packing Peanuts: The Role Synthetic Data Can Play in Enhancing Conventional Economic Prediction Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.07431">

<!--Generated on Wed Jun  5 13:11:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Packing Peanuts: 
<br class="ltx_break">The Role Synthetic Data Can Play in Enhancing Conventional Economic Prediction Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vansh Murad Kalia

<br class="ltx_break">
<br class="ltx_break">Candidate for Master’s of Arts in
<br class="ltx_break">Quantitative Methods for the Social Sciences

<br class="ltx_break">
<br class="ltx_break">Columbia University

<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">Thesis Advisor: Prof. Gregory M. Eirich
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Packing peanuts, as defined by Wikipedia, is a common loose-fill packaging and cushioning material that helps prevent damage to fragile items. In this paper, I propose that synthetic data, akin to packing peanuts, can serve as a valuable asset for economic prediction models, enhancing their performance and robustness when integrated with real data. This hybrid approach proves particularly beneficial in scenarios where data is either missing or limited in availability. Through the utilization of Affinity credit card spending and Womply small business datasets, this study demonstrates the substantial performance improvements achieved by employing a hybrid data approach, surpassing the capabilities of traditional economic modeling techniques.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Index</h2>

<figure id="Sx1.tab1" class="ltx_table">
<table id="Sx1.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx1.tab1.1.1.1" class="ltx_tr">
<th id="Sx1.tab1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.1.1.1.1.1" class="ltx_p" style="width:260.2pt;"><span id="Sx1.tab1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Section</span></span>
</span>
</th>
<th id="Sx1.tab1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.1.1.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="Sx1.tab1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Page</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.tab1.1.2.1" class="ltx_tr">
<td id="Sx1.tab1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.2.1.1.1.1" class="ltx_p" style="width:260.2pt;">1. Introduction</span>
</span>
</td>
<td id="Sx1.tab1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.2.1.2.1.1" class="ltx_p" style="width:130.1pt;">3</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.3.2" class="ltx_tr">
<td id="Sx1.tab1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.3.2.1.1.1" class="ltx_p" style="width:260.2pt;">2. Literature Review</span>
</span>
</td>
<td id="Sx1.tab1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.3.2.2.1.1" class="ltx_p" style="width:130.1pt;">3 - 5</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.4.3" class="ltx_tr">
<td id="Sx1.tab1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.4.3.1.1.1" class="ltx_p" style="width:260.2pt;">3. Data</span>
</span>
</td>
<td id="Sx1.tab1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.4.3.2.1.1" class="ltx_p" style="width:130.1pt;">5</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.5.4" class="ltx_tr">
<td id="Sx1.tab1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.5.4.1.1.1" class="ltx_p" style="width:260.2pt;">4. Methodology</span>
</span>
</td>
<td id="Sx1.tab1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.5.4.2.1.1" class="ltx_p" style="width:130.1pt;">6 - 12</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.6.5" class="ltx_tr">
<td id="Sx1.tab1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.6.5.1.1.1" class="ltx_p" style="width:260.2pt;">    4.1 Exploratory Data Analysis</span>
</span>
</td>
<td id="Sx1.tab1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.6.5.2.1.1" class="ltx_p" style="width:130.1pt;">6 - 7</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.7.6" class="ltx_tr">
<td id="Sx1.tab1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.7.6.1.1.1" class="ltx_p" style="width:260.2pt;">    4.2 Data Pre-processing</span>
</span>
</td>
<td id="Sx1.tab1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.7.6.2.1.1" class="ltx_p" style="width:130.1pt;">8</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.8.7" class="ltx_tr">
<td id="Sx1.tab1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.8.7.1.1.1" class="ltx_p" style="width:260.2pt;">    4.3 Model Selection</span>
</span>
</td>
<td id="Sx1.tab1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.8.7.2.1.1" class="ltx_p" style="width:130.1pt;">8 - 10</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.9.8" class="ltx_tr">
<td id="Sx1.tab1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.9.8.1.1.1" class="ltx_p" style="width:260.2pt;">    4.4 Model Testing Results</span>
</span>
</td>
<td id="Sx1.tab1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.9.8.2.1.1" class="ltx_p" style="width:130.1pt;">10 - 12</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.10.9" class="ltx_tr">
<td id="Sx1.tab1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.10.9.1.1.1" class="ltx_p" style="width:260.2pt;">5. Conclusion</span>
</span>
</td>
<td id="Sx1.tab1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.10.9.2.1.1" class="ltx_p" style="width:130.1pt;">12</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.11.10" class="ltx_tr">
<td id="Sx1.tab1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.11.10.1.1.1" class="ltx_p" style="width:260.2pt;">6. Limitations</span>
</span>
</td>
<td id="Sx1.tab1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.11.10.2.1.1" class="ltx_p" style="width:130.1pt;">13</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.12.11" class="ltx_tr">
<td id="Sx1.tab1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.12.11.1.1.1" class="ltx_p" style="width:260.2pt;">7. Next Steps</span>
</span>
</td>
<td id="Sx1.tab1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.12.11.2.1.1" class="ltx_p" style="width:130.1pt;">13</span>
</span>
</td>
</tr>
<tr id="Sx1.tab1.1.13.12" class="ltx_tr">
<td id="Sx1.tab1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.13.12.1.1.1" class="ltx_p" style="width:260.2pt;">References</span>
</span>
</td>
<td id="Sx1.tab1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="Sx1.tab1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx1.tab1.1.13.12.2.1.1" class="ltx_p" style="width:130.1pt;">14</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, the use of machine learning models for economic prediction has gained significant traction. While the adoption of these techniques has expedited the process of synthesizing vast amounts of data, one of the main challenges that remains is obtaining the data itself (or enough of it, at least!). Traditional approaches to data collection in the field of economics can be time-consuming, expensive, and limited in scope. There are countless cases where data is available but spotty, with missing samples. In such cases, synthetic data has emerged as a promising candidate to help fill that gap, but what is synthetic data?
<br class="ltx_break"></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">On the highest level, synthetic data can be categorized into three main types:</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Derived from real datasets, inheriting their statistical properties.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Generated independently of real data, without using any existing datasets.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Hybrid in nature, combining aspects of the first two types.</p>
</div>
</li>
</ul>
<p id="S1.p3.1" class="ltx_p">This paper focuses on the Hybrid type, exploring its potential applications in enhancing economic prediction models.
<br class="ltx_break">
<br class="ltx_break">Utilizing data from Affinity and Womply, this study aims to investigate whether the integration of synthetic data can improve model performance and robustness in scenarios characterized by limited data availability, potentially outperforming models reliant solely on real data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Given the nascent nature of the academic intersection of economic prediction models and synthetic data, there is not a lot of academic research that focuses directly on this topic. As such, for this research, I leverage some academic literature on synthetic data in relational fields like computer science, to formulate my hypothesis.

<br class="ltx_break">
<br class="ltx_break"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Data Generation for Economists:
<span id="S2.p1.1.1.1" class="ltx_ERROR undefined">\footfullcite</span></span>koenecke2020synthetic

<br class="ltx_break">
<br class="ltx_break">In my search for academic literature at the intersection of synthetic data and economics, this paper stands out as one of the most important contemporary pieces. In this study, the authors address synthetic data generation within the field of economics by recognizing the challenges associated with accessing and handling sensitive or limited datasets. Koenecke and Varian discuss the methodologies and implications of generating synthetic data, providing economists with a valuable resource for exploring and testing hypotheses in situations where real data availability is constrained. The authors propose the use of synthetic data as an alternative for economic researchers:</p>
</div>
<div id="S2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Assist with privacy issues related to the use of data.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Increase the number of samples available for a certain type of data.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Test the robustness of existing models.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">The paper contributes as an important piece to my research by offering insights into the potential benefits of synthetic data and helping formulate my hypothesis that using the hybrid of synthetic and real data should improve the performance of an economic prediction model.

<br class="ltx_break">
<br class="ltx_break"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Macroeconomic Predictions using Payments Data and Machine Learning:
<span id="S2.p3.1.1.1" class="ltx_ERROR undefined">\footfullcite</span></span>chapman2022macroeconomic

<br class="ltx_break">
<br class="ltx_break">In this study, the authors focus on predicting the economy’s short-term dynamics and delve into economic forecasting by leveraging payments data and machine learning techniques. This paper aims to demonstrate that non-traditional and timely data such as retail and wholesale payments, with the aid of nonlinear machine learning approaches, can provide policymakers with sophisticated models to accurately estimate key macroeconomic indicators in near real-time. By incorporating advanced machine learning algorithms and non-linear learning approaches, Chapman and Desai show over 40 percent improved accuracy in macroeconomic nowcasting. As a deeply quantitative study, this paper helped me structure the quantitative analysis for my research and nudged me towards the data I use as well.

<br class="ltx_break">
<br class="ltx_break"><span id="S2.p3.1.2" class="ltx_text ltx_font_bold">Augmentation Techniques in Time Series Domain: A Survey and Taxonomy<span id="S2.p3.1.2.1" class="ltx_ERROR undefined">\footfullcite</span></span>Iglesias_2023

<br class="ltx_break">
<br class="ltx_break">This study offers a comprehensive overview of various data augmentation methods specifically tailored for time series data. In this paper, the authors delve into a systematic classification of different augmentation techniques, categorizing them based on their underlying principles and applications. The authors explore a wide array of augmentation approaches including traditional methods such as linear interpolation and synthetic data generation techniques like Generative Adversarial Networks (GANs). They discuss the advantages, limitations, and potential applications of each technique, providing insights into their effectiveness in addressing various challenges encountered in time series analysis. Additionally, the paper examines the implications of data augmentation on model generalization, robustness, and interpretability. Overall, this survey and taxonomy have helped me navigate the landscape of data augmentation techniques in the context of the time series analysis pertinent to my research.

<br class="ltx_break">
<br class="ltx_break"><span id="S2.p3.1.3" class="ltx_text ltx_font_bold">K-Nearest Neighbor (k-NN) based Missing Data Imputation<span id="S2.p3.1.3.1" class="ltx_ERROR undefined">\footfullcite</span></span>inproceedings

<br class="ltx_break">
<br class="ltx_break">The authors of this paper explore the application of the K-Nearest Neighbor (k-NN) algorithm for imputing missing data. They investigate the use of the k-NN method as a means to address missing data in datasets and through their research, they propose a framework that leverages the k-NN algorithm to predict missing values based on the values of neighboring data points. This approach aims to improve data completeness and accuracy in datasets affected by missing information. While this paper contributes to the field of data imputation by offering a novel method that utilizes machine learning techniques to handle missing data effectively, it’s not necessarily most suitable for my research as the distance between the missing data points is too much to be able to efficiently use the k-NN algorithm.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For this research, I’ve opted to use the Affinity credit card spending datasets and Womply small business datasets from the Economic Tracker database<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/OpportunityInsights/EconomicTracker</span></span></span>. These datasets offer diverse features, but to narrow the focus for hypothesis testing, attention is given to the <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">daily_spend_19_all</span> variable from Affinity and the <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">merchants_all</span> variable from Womply. These variables can be described as follows:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">daily_spend_19_all</span>: Daily spending in all merchant category codes (MCCs).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">merchants_all</span>: Percent change in the number of small businesses open, calculated as a seven-day moving average, seasonally adjusted, and indexed to January 4 to 31, 2020.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The <span id="S3.p3.1.1" class="ltx_text ltx_font_typewriter">daily_spend_19_all</span> variable comes from the Affinity dataset, as all spending features are measured relative to January 6 to February 2, 2020, seasonally adjusted, and calculated as a seven-day moving average. There are additional quartile features that are subdivisions by income using the median income of the ZIP codes; <span id="S3.p3.1.2" class="ltx_text ltx_font_typewriter">q1</span> is the quartile with the lowest median income and <span id="S3.p3.1.3" class="ltx_text ltx_font_typewriter">q4</span> is the quartile with the highest median income. I selected these variables on the highest level as they are ideal to test my hypothesis where there is missing data for the <span id="S3.p3.1.4" class="ltx_text ltx_font_typewriter">merchants_all</span> that I would look to impute.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To test my hypothesis, I will create a real-life example using this dataset and my aim will be to create the best possible model to predict spending (<span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">daily_spend_19_all</span>) using the independent variable (<span id="S4.p1.1.2" class="ltx_text ltx_font_typewriter">merchants_all</span>)</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Exploratory Data Analysis</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As an initial step in exploratory data analysis, I examine the descriptive statistics of the original dataset:</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Descriptive Statistics</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">daily_spend_19_all</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">merchants_all</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">count</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">1253.000</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">109.000</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mean</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">0.280</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center">-0.056</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">std</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center">0.267</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center">0.067</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">min</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center">-0.643</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center">-0.302</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">25%</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center">0.124</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center">-0.066</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">50%</th>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center">0.243</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center">-0.049</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">75%</th>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_center">0.455</td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_center">-0.021</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<th id="S4.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">max</th>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb">1.200</td>
<td id="S4.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb">0.086</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The descriptive statistics provide basic insights into the dataset. However, they do not offer much relevant information for addressing the research question. Thus, I proceed towards exploratory data analysis to examine the temporal distribution of the two variables.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2405.07431/assets/hist1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Data Distribution for Daily Spend</figcaption>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2405.07431/assets/hist2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data Distribution for All Merchants</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2405.07431/assets/missing_data_matrix.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Missing Data Across Variables</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">Both Figure 1 and Figure 2 reveal notable trends, particularly a substantial drop in both variables during 2020, coinciding with the onset of the COVID-19 pandemic. Additionally, there are intriguing outliers, such as the end of end-of-year data points in Figure 1. Figure 3 reveals that there are no missing variables for Daily Spend features but there are a lot of missing data for <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">merchants_all</span>. Other than this, not much can be derived from an Exploratory Data Analysis that is relevant to my research question. The missing data will be addressed during preprocessing to ensure the success of this real-life example.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Pre-processing</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Despite the well-organized and clean nature of the data, several challenges exist, including datatype mismatches, DateTime information, filtering, and missing variables. To address these issues, I implement various data pre-processing techniques, such as handling different data types, managing DateTime information, and applying filtering. However, due to the density of the data, a careful selection of features is necessary for visualization purposes. Most notably, Womply’s business data is provided on a weekly basis, in contrast to the daily spending data. This missing data could potentially affect the accuracy of my model’s predictions and to address this discrepancy, I plan to generate synthetic data to fill this gap and facilitate meaningful comparisons with non-synthetic data models.
<br class="ltx_break">
<br class="ltx_break">I create four base datasets that cover conventional methods of missing data imputation used in Economics which will then be compared with the fifth model trained on the hybrid dataset built from generated synthetic data and real data. The techniques I follow are removing missing rows, global mean imputation, and Monte Carlo simulations and the base models for testing will be generated on the below datasets:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Original dataset with no imputations</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Original dataset with missing rows removed</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Mean-imputed dataset that fills missing values using global mean</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Monte Carlo simulations imputed dataset</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">As mentioned in the literature review, I also considered the k-Nearest Neighbors (k-NN) technique for another base model. However, it proved unsuitable for the data I’m dealing with due to the lack of neighboring data points for proper imputation. These base datasets will facilitate the creation of the first four base models for testing and evaluation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Selection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">As I begin the model selection process for testing and evaluation, it is important to recognize that this research involves a variety of complications with using time-series economic data, including missing data for a variable of interest and endogeneity concerns. As such, choosing the right model and evaluation techniques is of immense importance.
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">OLS Regression:

<br class="ltx_break">
<br class="ltx_break"></span>In the initial stage of analysis, I utilize Ordinary Least Squares (OLS) regression to investigate the linear relationship between the variables of interest. OLS regression is a widely used statistical method for estimating the relationship between a dependent variable and one or more independent variables by minimizing the sum of the squared differences between observed and predicted values<span id="S4.SS3.p1.1.2" class="ltx_ERROR undefined">\footfullcite</span>Zdaniuk2014. By fitting a linear regression model to the data, I aim to identify any significant linear associations and quantify the strength and direction of these relationships. Additionally, OLS regression provides insights into the relative importance of each independent variable in explaining the variation observed in the dependent variable. This initial analysis will help inform subsequent modeling approaches and provide valuable insights into the underlying factors influencing the target variable’s behavior.
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_bold">Random Forest Model:

<br class="ltx_break">
<br class="ltx_break"></span>Beyond capturing linear relationships, I employ a Random Forest model as a secondary economic prediction model. Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees<span id="S4.SS3.p1.1.4" class="ltx_ERROR undefined">\footfullcite</span>Tripp2023DiabetIA. It is a powerful tool for capturing nonlinear relationships and has been widely applied in economic research for variable selection, forecasting, and causal inference<span id="S4.SS3.p1.1.5" class="ltx_ERROR undefined">\footfullcite</span>coulombe2020macroeconomy. The Random Forest algorithm is well-suited for handling complex relationships and interactions within the data, providing a more comprehensive understanding of the factors influencing the outcome. As such, I also use a Random Forest Model to compare the model performance across all the datasets.
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS3.p1.1.6" class="ltx_text ltx_font_bold">Synthetic Data Generation:</span>

<br class="ltx_break">
<br class="ltx_break">To complete the Model Selection process, I will now outline the unique approach I take to generate synthetic data to fill data gaps within the Womply business dataset. I train a Random Forest Model on the second dataset mentioned in the pre-processing section (original dataset with missing rows removed) as it represents the cleanest form of real data. While the original dataset contains missing daily values for <span id="S4.SS3.p1.1.7" class="ltx_text ltx_font_typewriter">merchants_all</span> variable, it still has enough weekly values for me to be able to use it as a target variable to train the random forest model. Then, I use the trained model to predict (or impute) the missing values within the original dataset.

<br class="ltx_break">
<br class="ltx_break">This approach leverages Affinity data’s <span id="S4.SS3.p1.1.8" class="ltx_text ltx_font_typewriter">daily_spend_19_all</span> features as inputs and learns any non-linear relationships between the target variable and the features. By doing so, it enhances the accuracy of filling the gaps in the <span id="S4.SS3.p1.1.9" class="ltx_text ltx_font_typewriter">merchants_all</span> column, leading to a more robust model. This leads to the creation of the fifth dataset for comparison against all the base models, and any model trained on this hybrid dataset will be referred to as Model 5 from here onwards.

<br class="ltx_break">
<br class="ltx_break">Below is a scatter plot of the distribution of this hybrid dataset:</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2405.07431/assets/Hybrid.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Hybrid Data Distribution for All Merchants</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">In comparison, below is a scatter plot of the distribution of just the real data:</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2405.07431/assets/hist3.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Hybrid Data Distribution for All Merchants</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Model Testing Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Now that all of the models are ready, let’s take a look at how Model 5 (the comparison model) performs against all the base models across OLS Regression and Random Forest Models.</p>
<p id="S4.SS4.p1.2" class="ltx_p ltx_align_center">Base Models OLS Regression Results</p>
</div>
<div id="S4.SS4.2" class="ltx_para">
<table id="S4.SS4.2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS4.2.2.2" class="ltx_tr">
<th id="S4.SS4.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.SS4.2.2.2.3.1" class="ltx_text ltx_font_bold">Model 1</span></th>
<th id="S4.SS4.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.2.2.2.4.1" class="ltx_text ltx_font_bold">coef</span></th>
<th id="S4.SS4.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.2.2.2.5.1" class="ltx_text ltx_font_bold">std err</span></th>
<th id="S4.SS4.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.2.2.2.6.1" class="ltx_text ltx_font_bold">t</span></th>
<th id="S4.SS4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.2.2.2.2.2" class="ltx_text ltx_font_bold">P<math id="S4.SS4.1.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="&gt;|" display="inline"><semantics id="S4.SS4.1.1.1.1.1.m1.1a"><mrow id="S4.SS4.1.1.1.1.1.m1.1b"><mo rspace="0em" id="S4.SS4.1.1.1.1.1.m1.1.1">&gt;</mo><mo fence="false" stretchy="false" id="S4.SS4.1.1.1.1.1.m1.1.2">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.1.1.1.1.1.m1.1c">&gt;|</annotation></semantics></math>t<math id="S4.SS4.2.2.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.SS4.2.2.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="S4.SS4.2.2.2.2.2.m2.1.1" xref="S4.SS4.2.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.2.2.2.2.2.m2.1b"><ci id="S4.SS4.2.2.2.2.2.m2.1.1.cmml" xref="S4.SS4.2.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.2.2.2.2.2.m2.1c">|</annotation></semantics></math></span></th>
<th id="S4.SS4.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.2.2.2.7.1" class="ltx_text ltx_font_bold">[0.025</span></th>
<th id="S4.SS4.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.2.2.2.8.1" class="ltx_text ltx_font_bold">0.975]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS4.2.2.3.1" class="ltx_tr">
<th id="S4.SS4.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.SS4.2.2.3.1.1.1" class="ltx_text ltx_font_bold">const</span></th>
<td id="S4.SS4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.0582***</td>
<td id="S4.SS4.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.017</td>
<td id="S4.SS4.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">3.369</td>
<td id="S4.SS4.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.001</td>
<td id="S4.SS4.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.024</td>
<td id="S4.SS4.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.092</td>
</tr>
<tr id="S4.SS4.2.2.4.2" class="ltx_tr">
<th id="S4.SS4.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.SS4.2.2.4.2.1.1" class="ltx_text ltx_font_bold">merchants_all</span></th>
<td id="S4.SS4.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">1.6710***</td>
<td id="S4.SS4.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.198</td>
<td id="S4.SS4.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">8.430</td>
<td id="S4.SS4.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.000</td>
<td id="S4.SS4.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">1.278</td>
<td id="S4.SS4.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">2.064</td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS4.4" class="ltx_para">
<table id="S4.SS4.4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS4.4.2.2" class="ltx_tr">
<th id="S4.SS4.4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.SS4.4.2.2.3.1" class="ltx_text ltx_font_bold">Model 2</span></th>
<th id="S4.SS4.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.4.2.2.4.1" class="ltx_text ltx_font_bold">coef</span></th>
<th id="S4.SS4.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.4.2.2.5.1" class="ltx_text ltx_font_bold">std err</span></th>
<th id="S4.SS4.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.4.2.2.6.1" class="ltx_text ltx_font_bold">t</span></th>
<th id="S4.SS4.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.4.2.2.2.2" class="ltx_text ltx_font_bold">P<math id="S4.SS4.3.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="&gt;|" display="inline"><semantics id="S4.SS4.3.1.1.1.1.m1.1a"><mrow id="S4.SS4.3.1.1.1.1.m1.1b"><mo rspace="0em" id="S4.SS4.3.1.1.1.1.m1.1.1">&gt;</mo><mo fence="false" stretchy="false" id="S4.SS4.3.1.1.1.1.m1.1.2">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.3.1.1.1.1.m1.1c">&gt;|</annotation></semantics></math>t<math id="S4.SS4.4.2.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.SS4.4.2.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="S4.SS4.4.2.2.2.2.m2.1.1" xref="S4.SS4.4.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.4.2.2.2.2.m2.1b"><ci id="S4.SS4.4.2.2.2.2.m2.1.1.cmml" xref="S4.SS4.4.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.4.2.2.2.2.m2.1c">|</annotation></semantics></math></span></th>
<th id="S4.SS4.4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.4.2.2.7.1" class="ltx_text ltx_font_bold">[0.025</span></th>
<th id="S4.SS4.4.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.4.2.2.8.1" class="ltx_text ltx_font_bold">0.975]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS4.4.2.3.1" class="ltx_tr">
<th id="S4.SS4.4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.SS4.4.2.3.1.1.1" class="ltx_text ltx_font_bold">const</span></th>
<td id="S4.SS4.4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.0582***</td>
<td id="S4.SS4.4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.017</td>
<td id="S4.SS4.4.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">3.369</td>
<td id="S4.SS4.4.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.001</td>
<td id="S4.SS4.4.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.024</td>
<td id="S4.SS4.4.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.092</td>
</tr>
<tr id="S4.SS4.4.2.4.2" class="ltx_tr">
<th id="S4.SS4.4.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.SS4.4.2.4.2.1.1" class="ltx_text ltx_font_bold">merchants_all</span></th>
<td id="S4.SS4.4.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">1.6710***</td>
<td id="S4.SS4.4.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.198</td>
<td id="S4.SS4.4.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">8.430</td>
<td id="S4.SS4.4.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.000</td>
<td id="S4.SS4.4.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">1.278</td>
<td id="S4.SS4.4.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">2.064</td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS4.6" class="ltx_para">
<table id="S4.SS4.6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS4.6.2.2" class="ltx_tr">
<th id="S4.SS4.6.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.SS4.6.2.2.3.1" class="ltx_text ltx_font_bold">Model 3</span></th>
<th id="S4.SS4.6.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.6.2.2.4.1" class="ltx_text ltx_font_bold">coef</span></th>
<th id="S4.SS4.6.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.6.2.2.5.1" class="ltx_text ltx_font_bold">std err</span></th>
<th id="S4.SS4.6.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.6.2.2.6.1" class="ltx_text ltx_font_bold">t</span></th>
<th id="S4.SS4.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.6.2.2.2.2" class="ltx_text ltx_font_bold">P<math id="S4.SS4.5.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="&gt;|" display="inline"><semantics id="S4.SS4.5.1.1.1.1.m1.1a"><mrow id="S4.SS4.5.1.1.1.1.m1.1b"><mo rspace="0em" id="S4.SS4.5.1.1.1.1.m1.1.1">&gt;</mo><mo fence="false" stretchy="false" id="S4.SS4.5.1.1.1.1.m1.1.2">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.5.1.1.1.1.m1.1c">&gt;|</annotation></semantics></math>t<math id="S4.SS4.6.2.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.SS4.6.2.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="S4.SS4.6.2.2.2.2.m2.1.1" xref="S4.SS4.6.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.6.2.2.2.2.m2.1b"><ci id="S4.SS4.6.2.2.2.2.m2.1.1.cmml" xref="S4.SS4.6.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.6.2.2.2.2.m2.1c">|</annotation></semantics></math></span></th>
<th id="S4.SS4.6.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.6.2.2.7.1" class="ltx_text ltx_font_bold">[0.025</span></th>
<th id="S4.SS4.6.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.6.2.2.8.1" class="ltx_text ltx_font_bold">0.975]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS4.6.2.3.1" class="ltx_tr">
<th id="S4.SS4.6.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.SS4.6.2.3.1.1.1" class="ltx_text ltx_font_bold">const</span></th>
<td id="S4.SS4.6.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.3737***</td>
<td id="S4.SS4.6.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.023</td>
<td id="S4.SS4.6.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">16.473</td>
<td id="S4.SS4.6.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.000</td>
<td id="S4.SS4.6.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.329</td>
<td id="S4.SS4.6.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.418</td>
</tr>
<tr id="S4.SS4.6.2.4.2" class="ltx_tr">
<th id="S4.SS4.6.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.SS4.6.2.4.2.1.1" class="ltx_text ltx_font_bold">merchants_all</span></th>
<td id="S4.SS4.6.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">1.6710***</td>
<td id="S4.SS4.6.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.381</td>
<td id="S4.SS4.6.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">4.382</td>
<td id="S4.SS4.6.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.000</td>
<td id="S4.SS4.6.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">0.923</td>
<td id="S4.SS4.6.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">2.419</td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS4.8" class="ltx_para">
<table id="S4.SS4.8.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS4.8.2.2" class="ltx_tr">
<th id="S4.SS4.8.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.SS4.8.2.2.3.1" class="ltx_text ltx_font_bold">Model 4</span></th>
<th id="S4.SS4.8.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.8.2.2.4.1" class="ltx_text ltx_font_bold">coef</span></th>
<th id="S4.SS4.8.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.8.2.2.5.1" class="ltx_text ltx_font_bold">std err</span></th>
<th id="S4.SS4.8.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.8.2.2.6.1" class="ltx_text ltx_font_bold">t</span></th>
<th id="S4.SS4.8.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.8.2.2.2.2" class="ltx_text ltx_font_bold">P<math id="S4.SS4.7.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="&gt;|" display="inline"><semantics id="S4.SS4.7.1.1.1.1.m1.1a"><mrow id="S4.SS4.7.1.1.1.1.m1.1b"><mo rspace="0em" id="S4.SS4.7.1.1.1.1.m1.1.1">&gt;</mo><mo fence="false" stretchy="false" id="S4.SS4.7.1.1.1.1.m1.1.2">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.7.1.1.1.1.m1.1c">&gt;|</annotation></semantics></math>t<math id="S4.SS4.8.2.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.SS4.8.2.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="S4.SS4.8.2.2.2.2.m2.1.1" xref="S4.SS4.8.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.8.2.2.2.2.m2.1b"><ci id="S4.SS4.8.2.2.2.2.m2.1.1.cmml" xref="S4.SS4.8.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.8.2.2.2.2.m2.1c">|</annotation></semantics></math></span></th>
<th id="S4.SS4.8.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.8.2.2.7.1" class="ltx_text ltx_font_bold">[0.025</span></th>
<th id="S4.SS4.8.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.8.2.2.8.1" class="ltx_text ltx_font_bold">0.975]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS4.8.2.3.1" class="ltx_tr">
<th id="S4.SS4.8.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.SS4.8.2.3.1.1.1" class="ltx_text ltx_font_bold">const</span></th>
<td id="S4.SS4.8.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.0582***</td>
<td id="S4.SS4.8.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.017</td>
<td id="S4.SS4.8.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">3.369</td>
<td id="S4.SS4.8.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.001</td>
<td id="S4.SS4.8.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.024</td>
<td id="S4.SS4.8.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.092</td>
</tr>
<tr id="S4.SS4.8.2.4.2" class="ltx_tr">
<th id="S4.SS4.8.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.SS4.8.2.4.2.1.1" class="ltx_text ltx_font_bold">merchants_all</span></th>
<td id="S4.SS4.8.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">1.6710***</td>
<td id="S4.SS4.8.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.198</td>
<td id="S4.SS4.8.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">8.430</td>
<td id="S4.SS4.8.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.000</td>
<td id="S4.SS4.8.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">1.278</td>
<td id="S4.SS4.8.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">2.064</td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS4.10" class="ltx_para">
<table id="S4.SS4.10.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS4.10.2.2" class="ltx_tr">
<th id="S4.SS4.10.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.SS4.10.2.2.3.1" class="ltx_text ltx_font_bold">Model 5</span></th>
<th id="S4.SS4.10.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.10.2.2.4.1" class="ltx_text ltx_font_bold">coef</span></th>
<th id="S4.SS4.10.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.10.2.2.5.1" class="ltx_text ltx_font_bold">std err</span></th>
<th id="S4.SS4.10.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.10.2.2.6.1" class="ltx_text ltx_font_bold">t</span></th>
<th id="S4.SS4.10.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.10.2.2.2.2" class="ltx_text ltx_font_bold">P<math id="S4.SS4.9.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="&gt;|" display="inline"><semantics id="S4.SS4.9.1.1.1.1.m1.1a"><mrow id="S4.SS4.9.1.1.1.1.m1.1b"><mo rspace="0em" id="S4.SS4.9.1.1.1.1.m1.1.1">&gt;</mo><mo fence="false" stretchy="false" id="S4.SS4.9.1.1.1.1.m1.1.2">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.9.1.1.1.1.m1.1c">&gt;|</annotation></semantics></math>t<math id="S4.SS4.10.2.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.SS4.10.2.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="S4.SS4.10.2.2.2.2.m2.1.1" xref="S4.SS4.10.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.10.2.2.2.2.m2.1b"><ci id="S4.SS4.10.2.2.2.2.m2.1.1.cmml" xref="S4.SS4.10.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.10.2.2.2.2.m2.1c">|</annotation></semantics></math></span></th>
<th id="S4.SS4.10.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.10.2.2.7.1" class="ltx_text ltx_font_bold">[0.025</span></th>
<th id="S4.SS4.10.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.SS4.10.2.2.8.1" class="ltx_text ltx_font_bold">0.975]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS4.10.2.3.1" class="ltx_tr">
<th id="S4.SS4.10.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.SS4.10.2.3.1.1.1" class="ltx_text ltx_font_bold">const</span></th>
<td id="S4.SS4.10.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.3302***</td>
<td id="S4.SS4.10.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.006</td>
<td id="S4.SS4.10.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">51.379</td>
<td id="S4.SS4.10.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.000</td>
<td id="S4.SS4.10.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.318</td>
<td id="S4.SS4.10.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.343</td>
</tr>
<tr id="S4.SS4.10.2.4.2" class="ltx_tr">
<th id="S4.SS4.10.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.SS4.10.2.4.2.1.1" class="ltx_text ltx_font_bold">merchants_all</span></th>
<td id="S4.SS4.10.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">4.2133***</td>
<td id="S4.SS4.10.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.165</td>
<td id="S4.SS4.10.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">25.588</td>
<td id="S4.SS4.10.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.000</td>
<td id="S4.SS4.10.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">3.890</td>
<td id="S4.SS4.10.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">4.536</td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">From the above table, it’s clear that while all the models have statistically significant coefficients and constants, Model 5 stands out in comparison to the baseline models with its notably higher coefficient value (4.2133) for the variable <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">merchants_all</span>, indicating a stronger impact on the dependent variable. Moreover, Model 5 exhibits lower standard errors for both the constant and <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">merchants_all</span>, suggesting greater precision in the coefficient estimates. The high t-values (51.379 and 25.588 respectively) and extremely low p-values indicate high significance, further supporting the robustness of Model 5. Overall, Model 5 appears to offer a more accurate and statistically significant representation of the relationship between the variables compared to the other models. This result confirms my hypothesis, however, I will still look to substantiate it using Random Forest Models and see how Model 5 compares to the baseline models.
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break"></p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Random Forest Model Results</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Average MAE</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Average MSE</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Average R-squared</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">NA</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">NA</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">NA</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_left">0.162</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_left">0.042</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_left">-5.92</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_left">0.217</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_left">0.077</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_left">-0.75</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">4</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_left">0.232</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_left">0.088</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_nopad_r ltx_align_left">-1.06</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">5</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_left ltx_border_bb">0.092</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_left ltx_border_bb">0.017</td>
<td id="S4.T2.1.6.5.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">0.55</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p">Similarly to the OLS Regression results, Table 2 demonstrates Model 5’s superior performance as it exhibits the lowest average Mean Absolute Error (MAE) of 0.092 and the lowest average Mean Squared Error (MSE) of 0.017, indicating the closest proximity of predicted values to the actual values compared to other models. Additionally, Model 5 achieves the highest average R-squared value of 0.55, suggesting that it explains a higher proportion of the variance in the dependent variable. It should be noted that Model 1 is marked NA as the dataset for Model 1 has missing values and is not suitable for a Random Forest analysis. Models 3 and 4 display poorer performance across all metrics while Model 2, although showing a low average MAE and MSE, has a substantially negative R-squared value, indicating poor model fit or potential overfitting. Therefore, Model 5 emerges as the most favorable choice among the presented Random Forest models, demonstrating superior predictive accuracy and model robustness.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">I started this research to explore whether the integration of Synthetic Data could enhance model performance and robustness in scenarios characterized by limited data availability. Based on the literature review, I hypothesized that employing the hybrid approach of synthetic and real data should improve the performance of an economic prediction model, surpassing the efficacy of utilizing only real data. To test this hypothesis, I set up a real-life example using the Affinity and Womply datasets and created four different baseline models covering the conventional data-handling techniques used in economic prediction modeling. These techniques covered using the original dataset with no imputations, the original dataset removing rows with missing data, imputing missing values with global mean, and Monte Carlo simulations. My comparison model was trained on a dataset created using an advanced data augmentation technique that leverages Random Forest Models to generate Synthetic Data and use it in conjunction with real data. The comparison model outperformed all the baseline models across both OLS Regression testing and Random Forest Modeling, giving me strong conviction that my hypothesis is correct.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In terms of limitations of this paper, there were quite a few that I faced throughout the course of the research:</p>
</div>
<div id="S6.p2" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">As highlighted in the literature review, the nascent nature of this topic meant that there was not a lot of reliable academic research I could leverage that focused directly on the intersection of economics and synthetic data. I consider this a huge limitation, as a lot more literature on the topic could have changed the structure of my research.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">The dataset I have been working with had a very high number of missing values for the target variable which could’ve easily led to an imbalanced dataset, adding bias in the generated data. If I had access to more data, or data with more frequency, I potentially could’ve used other baseline modeling techniques like k-NN and seen different results.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Another limitation would be lacking the skills required to build more sophisticated models like Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). Based on the literature review, it is very likely that synthetic data generated through either of these models would perform better than synthetic data generated by a Random Forest Model.</p>
</div>
</li>
</ul>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Even considering these limitations, I believe this research is well grounded in both qualitative and quantitative logic proving valid grounds for my hypothesis to be correct.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Next Steps</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In terms of next steps, I would like to create three more hybrid datasets using the following techniques that I discovered through my literature review <span id="S7.p1.1.1" class="ltx_ERROR undefined">\footfullcite</span>Iglesias_2023 and include them as comparison models to the existing tests:</p>
</div>
<div id="S7.p2" class="ltx_para">
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">A library like Datawig to leverage Deep Learning Neural Networks.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Generative Adversarial Network (GAN) to generate more accurate synthetic data.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">Variational Auto-Encoder (VAE) to generate more accurate synthetic data by accounting for variance in time-series data.</p>
</div>
</li>
</ul>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p">All of these are more sophisticated modeling techniques that have the potential to produce more robust results compared to Random Forest Models and are something I can look forward to implementing in economic prediction models.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">James T.. Chapman and Ajit Desai
</span>
<span class="ltx_bibblock">“Macroeconomic Predictions using Payments Data and Machine Learning”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2209.00948" title="" class="ltx_ref ltx_href">2209.00948 [econ.GN]</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Philippe Goulet Coulombe
</span>
<span class="ltx_bibblock">“The Macroeconomy as a Random Forest”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.12724</em>, 2020
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Guillermo Iglesias et al.
</span>
<span class="ltx_bibblock">“Data Augmentation techniques in time series domain: a survey and taxonomy”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">35.14</span>
</span>
<span class="ltx_bibblock">Springer ScienceBusiness Media LLC, 2023, pp. 10123–10145
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1007/s00521-023-08459-3" title="" class="ltx_ref ltx_href">10.1007/s00521-023-08459-3</a>
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Allison Koenecke and Hal Varian
</span>
<span class="ltx_bibblock">“Synthetic Data Generation for Economists”, 2020
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2011.01374" title="" class="ltx_ref ltx_href">2011.01374 [econ.GN]</a>
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Della Murti, Utomo Pujianto, Aji Wibawa and Muhammad Akbar
</span>
<span class="ltx_bibblock">“K-Nearest Neighbor (K-NN) based Missing Data Imputation”, 2019, pp. 83–88
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICSITech46713.2019.8987530" title="" class="ltx_ref ltx_href">10.1109/ICSITech46713.2019.8987530</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Hector M. Tripp et al.
</span>
<span class="ltx_bibblock">“DiabetIA: a real-world research database to predict de novo diabetic complications using artificial intelligence”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Nature Diabetes</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">4.3</span>, 2023, pp. 201–211
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1038/s42255-023-00712-9" title="" class="ltx_ref ltx_href">10.1038/s42255-023-00712-9</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Bartosz Zdaniuk
</span>
<span class="ltx_bibblock">“Ordinary Least-Squares (OLS) Model”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Encyclopedia of Quality of Life and Well-Being Research</em>
</span>
<span class="ltx_bibblock">Dordrecht: Springer, 2014
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1007/978-94-007-0753-5%5C_2008" title="" class="ltx_ref ltx_href">10.1007/978-94-007-0753-5“˙2008</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.07430" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.07431" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.07431">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.07431" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.07432" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 13:11:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
