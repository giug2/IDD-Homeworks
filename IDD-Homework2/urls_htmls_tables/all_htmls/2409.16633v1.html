<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences</title>
<!--Generated on Wed Sep 25 05:16:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Recommendation System,  Compute eXpress Link,  Software-Hardware
Co-design,  Memory Pooling.
" lang="en" name="keywords"/>
<base href="/html/2409.16633v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S1" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background and Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.SS1" title="In II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Deep Learning Recommendation Model (DLRM)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.SS2" title="In II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">CXL Overview</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.SS2.SSS1" title="In II-B CXL Overview ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>Conventional CXL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.SS2.SSS2" title="In II-B CXL Overview ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Fabric Switch</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.SS3" title="In II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Related Works</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S3" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Characterization Study and Motivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">System Design</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1" title="In IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Hardware Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS1" title="In IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>System Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS2" title="In IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Process Flow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS3" title="In IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>Instruction Modification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS4" title="In IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>4 </span>On-Switch Buffer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS5" title="In IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>5 </span>Out-of-Order Accumulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2" title="In IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Software Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS1" title="In IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>Page Granular Access</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS2" title="In IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span>Global Hotness Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS3" title="In IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>3 </span>Embedding Spreading for Bandwidth Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS4" title="In IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>4 </span>Optimization in Page Migration</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS3" title="In IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Fabric Switch at Scale</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS3.SSS1" title="In IV-C Fabric Switch at Scale ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>1 </span>Multi-layer Instruction Forwarding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS3.SSS2" title="In IV-C Fabric Switch at Scale ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>2 </span>Versatility</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS4" title="In IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Programming-Related Aspects</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S5" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Implementation and Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS1" title="In VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS2" title="In VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Baselines</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3" title="In VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS1" title="In VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span>1 </span>HW/SW Co-Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS2" title="In VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span>2 </span>Generality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS3" title="In VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span>3 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS4" title="In VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span>4 </span>Scalability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS5" title="In VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span>5 </span>On Switch Buffer Capacity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS6" title="In VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span>6 </span>Page Management</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS4" title="In VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Hardware Overheads</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS5" title="In VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-E</span> </span><span class="ltx_text ltx_font_italic">Cost and Performance Analysis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S7" title="In PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Concluding Remarks</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pingyi Huo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">The Pennsylvania State University</span>
<br class="ltx_break"/>University Park, PA, USA 
<br class="ltx_break"/>pqh5140@psu.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anusha Devulapally
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">The Pennsylvania State University</span>
<br class="ltx_break"/>University Park, PA, USA 
<br class="ltx_break"/>akd5994@psu.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hasan Al Maruf
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id5.1.id1">AMD, Inc</span>
<br class="ltx_break"/>Austin, TX, USA 
<br class="ltx_break"/>hasan.maruf@amd.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minseo Park
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.1.id1">AMD, Inc</span>
<br class="ltx_break"/>Austin, TX, USA 
<br class="ltx_break"/>minseo.park@amd.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Krishnakumar Nair
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id7.1.id1">AMD, Inc</span>
<br class="ltx_break"/>Austin, TX, USA 
<br class="ltx_break"/>krishnakumar.nair@amd.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meena Arunachalam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.1.id1">AMD, Inc</span>
<br class="ltx_break"/>Austin, TX, USA 
<br class="ltx_break"/>meena.arunachalam@amd.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gulsum Gudukbay Akbulut
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id9.1.id1">The Pennsylvania State University</span>
<br class="ltx_break"/>University Park, PA, USA 
<br class="ltx_break"/>gulsum@psu.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahmut Taylan Kandemir
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id10.1.id1">The Pennsylvania State University</span>
<br class="ltx_break"/>University Park, PA, USA 
<br class="ltx_break"/>mtk2@psu.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vijaykrishnan Narayanan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id11.1.id1">The Pennsylvania State University</span>
<br class="ltx_break"/>University Park, PA, USA 
<br class="ltx_break"/>vxn9@psu.edu
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.2">Deep Learning Recommendation Models (DLRMs) have become increasingly popular and prevalent in today’s datacenters, consuming most of the AI inference cycles. The performance of DLRMs is heavily influenced by available bandwidth due to their large vector sizes in embedding tables and concurrent accesses. To achieve substantial improvements over existing solutions, novel approaches towards DLRM optimization are needed, especially, in the context of emerging interconnect technologies like CXL. This study delves into exploring CXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to accelerate DLRMs while optimizing their memory and bandwidth scalability. We present an in-depth characterization of industry-scale DLRM workloads running on CXL-ready systems, identifying the predominant bottlenecks in existing CXL systems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements near-data processing through downstream ports of the fabric switch. PIFS-Rec achieves a latency that is 3.89<math alttext="\times" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">×</annotation></semantics></math> lower than Pond, an industry-standard CXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by 2.03<math alttext="\times" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">×</annotation></semantics></math>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Recommendation System, Compute eXpress Link, Software-Hardware
Co-design, Memory Pooling.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Personalized recommendation systems have emerged as a cornerstone in the interface between users and technology, spanning various application domains from e-commerce to social networking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib1" title="">1</a>]</cite>. These systems, powered by deep learning techniques, sift through vast user data to deliver tailored content. This personalized approach boosts user engagement and significantly enhances overall satisfaction. As these systems become increasingly essential components of our digital experiences, datacenters worldwide are scaling up their capabilities, dedicating extensive resources to the AI inference tasks that underpin recommendation models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Among various deployed models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib4" title="">4</a>]</cite>, Deep Learning Recommendation Models (DLRMs) stand out due to their unique characteristics: unlike their compute-heavy counterparts, DLRMs are predominantly bandwidth-intensive due to their large embedding table accumulations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib8" title="">8</a>]</cite>. This distinction shifts the performance bottleneck from compute capacity to data bandwidth and transfer efficiency. The research community and industry have proposed several hardware-based solutions, such as Process-near-Memory (PNM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib12" title="">12</a>]</cite> and ASIC designs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib13" title="">13</a>]</cite> to address these challenges. However, these solutions introduce new challenges. Firstly, the ever-expanding size of industrial DLRM models, now surpassing even the most significant Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib16" title="">16</a>]</cite>, poses a significant challenge to the scalability of PNM and ASIC solutions, due to the limited physical interface on boards.
Secondly, the PNM-based solutions, by their nature, diverge from standard DRAM protocols <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib17" title="">17</a>]</cite>. Consequently, adapting to these memory technologies may demand extensive hardware and software stack modifications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib20" title="">20</a>]</cite>, elevating development costs and extending the product development cycle. Thirdly, they also introduce resource inefficiency – with shared memory capabilities at the board level, the PNM solutions can serve limited sockets per board. This limitation may cause redundant data copies within a rack or even across racks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib23" title="">23</a>]</cite> to facilitate multi-host access, leading to inefficient memory usage and increased latency, despite the advancements like RDMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Compute Express Link (CXL) technology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite> is rapidly gaining traction in the contemporary datacenter landscape, setting a new standard in the industry. It ensures cache coherence over the PCIe physical layer and introduces memory pooling by using fabric switch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite>. This advancement offers enhanced memory scalability and utilization, leading towards a new data processing and management era. Furthermore, recent studies emphasize CXL’s capability to operate as a separate and independent memory bandwidth source <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>]</cite>, significantly enhancing the system’s overall bandwidth availability. Together, these features provide a robust foundation for accelerating DLRMs at the datacenter scale.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Motivated by these observations, this study leverages the capabilities of the CXL standard (bandwidth/memory expander) and its interconnects to accelerate DLRMs. We present <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">PIFS-Rec</span> (<span class="ltx_text ltx_font_bold" id="S1.p4.1.2">Process-In-Fabric-Switch for Recommendation Systems</span>), a scalable, near-data processing capability tailored for fabric switch hardware. Focusing on large-scale industrial DLRM inference systems, PIFS-Rec utilizes the scalability of downstream ports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite> and proximity to memory within the CXL fabric switch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite> to accelerate the embedding table operations. Through minimal hardware and software optimizations within the fabric switch, we extend its capabilities beyond current implementations, including DRAM-based Type 3 memory expanders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib32" title="">32</a>]</cite>. Our design (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4" title="IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">IV</span></a>) enhances existing CXL memory systems by leveraging the “scalable bandwidth” of fabric switches to address bandwidth bottlenecks in embedding table accesses of DLRM. This boosts performance through enhanced device-level I/O utilization and parallelization. Additionally, we explore integrating a process-on-a-fabric-switch framework to reduce data movement costs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our main <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">contributions</span> in this work are as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">∙</annotation></semantics></math> We present results from a characterization study that analyzes recommendation models using real-industrial access traces, production-scale DLRM models, and CXL-ready system. We quantitatively assess the bottlenecks of CXL-enabled memory pooling as well as the potential opportunities it brings.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">∙</annotation></semantics></math> We introduce <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">PIFS-Rec</span>, a scalable near-data processing approach customized for fabric switch hardware. Our optimizations include hardware and software enhancements such as data repacking, snooping mechanisms, on-switch buffer implementation, and optimized compute logic. Additionally, we explore software-assisted page management strategies to enhance the efficiency of the DLRM processing pipeline.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">∙</annotation></semantics></math> We use open-sourced industrial DLRM traces to quantify the effectiveness of our optimizations. We find that PIFS-Rec outperforms an existing CXL-based system, Pond <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite> by 3.89<math alttext="\times" class="ltx_Math" display="inline" id="S1.p8.2.m2.1"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><times id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p8.2.m2.1d">×</annotation></semantics></math> and a state-of-the-art comparable design, BEACON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite> by 2.03<math alttext="\times" class="ltx_Math" display="inline" id="S1.p8.3.m3.1"><semantics id="S1.p8.3.m3.1a"><mo id="S1.p8.3.m3.1.1" xref="S1.p8.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p8.3.m3.1b"><times id="S1.p8.3.m3.1.1.cmml" xref="S1.p8.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p8.3.m3.1d">×</annotation></semantics></math> in terms of latency.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background and Related Works</span>
</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="313" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S2.F1.2.1">End-to-end DLRM pipeline for inference.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Deep Learning Recommendation Model (DLRM)</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The end-to-end inference in DLRM involves several stages. Initially, the recommendation model is loaded into DRAM. Incoming input queries are grouped into batches, and the necessary dense and sparse features are organized for input into the DLRM. The inference step then processes these batches to generate predictions. In a DLRM architecture, four key stages can be identified: the bottom fully-connected layer (Bottom MLP), embedding lookup, feature interaction, and top fully-connected layer (Top MLP), as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F1" title="Figure 1 ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">1</span></a>. This architecture handles two types of input features: “dense features” which are continuous personal variables (e.g., age, gender), and “sparse features” which are categorical (e.g., product IDs, music genres). During the inference process, dense features undergo processing by the Bottom MLP. On the other hand, sparse features are transformed into “dense latent representation” in the embedding lookup stage. Each feature’s value or indices are used to retrieve the corresponding “embedding vector” from a large table. Embedding vectors can be of different dimensions (e.g., 16B, 32B, etc.) in different setups; high embedding dimensions and the number of embeddings lead to large memory footprints. The outputs from the Bottom MLP and the embedding lookup are combined in the feature interaction layer to calculate interactions before being passed to the Top MLP for determining the click-through rate (CTR).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">CXL Overview</span>
</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.5.1.1">II-B</span>1 </span>Conventional CXL</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">CXL operates as a transaction layer designed for rack-level memory pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite>, building upon the physical layer of PCIe. PCIe 5.0 supports a data transfer rate of 32 GT/s per lane, translating to approximately 64GB/s when utilizing <math alttext="16\times" class="ltx_math_unparsed" display="inline" id="S2.SS2.SSS1.p1.1.m1.1"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mrow id="S2.SS2.SSS1.p1.1.m1.1b"><mn id="S2.SS2.SSS1.p1.1.m1.1.1">16</mn><mo id="S2.SS2.SSS1.p1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">16\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.1.m1.1d">16 ×</annotation></semantics></math> lanes. As CXL uses PCIe’s physical layer, unlike RDMA, it does not require a device’s DMA engine or Network Interface Card (NIC). CXL encompasses three protocols: “CXL.io”, “CXL.cache”, and “CXL.mem”. The CXL.io protocol configures and establishes connections between CPUs and CXL devices. In contrast, the CXL.cache (resp. CXL.mem) protocol enables a device to access the host CPU cache (resp. memory) and vice versa. These protocols enable three types of CXL devices: <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p1.1.1">Type 1</span> (only cache, e.g., NIC), <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p1.1.2">Type 2</span> (both cache and memory, e.g., GPU, accelerator, etc.), and <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p1.1.3">Type 3</span> (only memory, e.g., memory expander).</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S2.F2.2.1">Architecture of a CXL-based system. The devices use Flexbus to communicate with the host. The fabric manager configures the Virtual PCI-to-PCI Bridge (VPPB) to control the FM endpoints in the fabric switches. These switches connect all devices within the system. Data leaves fabric switch through PCI-to-PCI Bridge (PPB). </span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.1">Memory Expander.</span>
Type 3 devices also known as CXL memory expanders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib31" title="">31</a>]</cite>, are designed to increase <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p2.1.2">both</span> memory capacity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib34" title="">34</a>]</cite> and bandwidth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib27" title="">27</a>]</cite>. These conventional expanders include DDR memory and rely on the CXL.mem protocol for data storage and retrieval, as well as the CXL.io protocol for establishing the connections. This protocol enables CPU-to-device memory access, coordinated by a Home Agent (HA) and a CXL controller within the host and device. The HA manages the CXL.mem protocol, presenting memory to the CPU as if it were on a remote NUMA node, allowing direct access via standard load/store commands.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p3.1.1">Memory Pooling.</span>
CXL enables memory pooling where each host and end device can access any shared, unified memory space connected through CXL. This addresses memory stranding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite> and redundancy issues.
To manage the data access flow within a memory pool, CXL employs an asymmetric cache protocol and introduces a “Bias Table” (4KB per table). This table operates in two modes: “host-bias” and “device-bias”. In the host-bias mode, devices accessing addresses within CXL memory need control instructions to ensure data coherence, adding extra overhead. Conversely, in the device-bias mode, the region is locked for the device’s exclusive use, preventing access by other hosts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.5.1.1">II-B</span>2 </span>Fabric Switch</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F2" title="Figure 2 ‣ II-B1 Conventional CXL ‣ II-B CXL Overview ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">2</span></a>, In the advancement from version 1.0 to 3.1, CXL introduces the “fabric switch” in CXL 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite>. Note that the fabric switch is a compulsory component and non-bypass hardware in a multi-node CXL interconnect. Unlike the one-to-one communication of earlier CXL versions (1.0/1.1), CXL 2.0 and later versions facilitate multiple-device communication and interconnectivity. The fabric switch functions as both a memory request dispatcher and a connected device manager. Each device is assigned a <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p1.1.1">cacheID</span> when recognized by the FM endpoint in the fabric switch.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p2.1.1">Process-In-Fabric-Switch.</span> To the best of our knowledge, in the context of CXL, BEACON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite> is the first work that adds compute capability to a fabric switch. Specifically, BEACON integrates “compute units” within the fabric switch to harness the processing capabilities close to the data source and the high bandwidth of the downstream port for accelerating genome workloads. Note that BEACON has been developed to accelerate “genome analysis”. Our analysis reveals several areas where BEACON may not fully maximize the potential benefits of in-switch computation. From a hardware perspective, BEACON’s design relies on custom DIMM instructions for CXL memory management, diverging from the established CXL standards. It also needs an additional memory translation logic in the fabric switch which can introduce performance overheads. Moreover, the system is not scalable as it does not support fabric switch scaling. It also does not take advantage of data locality. On the software front, existing work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib34" title="">34</a>]</cite> suggests that directly accessing CXL memory without careful management can detrimentally affect performance across various workloads, primarily due to higher data access latency compared to local DRAM. BEACON’s standalone use of CXL memory, without integrating address interleaving with local DRAM, might result in suboptimal performance. Additionally, it focuses solely on single-host configurations, neglecting the complexities and opportunities brought by multi-host environments.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1">To address BEACON’s shortcomings and maximize the potential of in-switch compute capability in the context of DLRM workloads, we undertake a comprehensive redesign encompassing <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p3.1.1">both</span> hardware and software support, establishing a new workflow and a new instruction flow. This effort results in the development of PIFS-Rec, which extends beyond DLRM workloads to cater to a broad range of applications with a focus on enhancing scalability, efficiency, and performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Related Works</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Numerous works address the acceleration (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib37" title="">37</a>]</cite>) and optimization (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib40" title="">40</a>]</cite>) of DLRMs.
Software-based approaches focus on techniques like feature-based resource allocation (CAFE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib41" title="">41</a>]</cite>) and CPU optimizations for pre-fetching and overlapping computation with memory access ( <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib8" title="">8</a>]</cite>). Hercules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib42" title="">42</a>]</cite> provides an adaptive scheduler to deploy various DLRM models across the datacenters with heterogeneous devices, considering multiple factors such as power budget, latency requirement, and throughput. In comparison, DisaggRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib43" title="">43</a>]</cite> explores the deployment of DLRM using hardware resource disaggregation to improve cost efficiency. Note that both of these solutions explore effective scheduling strategies targeting existing servers, while PIFS-Rec is a new hardware acceleration-based solution that targets scalability.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Hardware-based solutions leverage technologies like PIM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib12" title="">12</a>]</cite> for faster data processing within memory and specialized ASICs designed for DLRMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib13" title="">13</a>]</cite>. Furthermore, recent research also explores CXL for both characterization and optimization purposes. For example, Pond <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite> focuses on memory pooling with CXL to increase scalability; TPP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>]</cite> improves CXL system performance with tiered memory page management; and studies like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib45" title="">45</a>]</cite> explore the potential performance gains from CXL. Our research is centered on leveraging fabric switches in the context of memory disaggregation over CXL.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="313" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S2.F3.2.1">A simplified illustration of the production-ready CXL-enabled experiment platform.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="S2.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S2.F4.2.1">(a) Batch Threading – each batch is assigned to a CPU core to be processed. (b) Table Threading – each embedding table is accessed by a CPU core to be processed.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="139" id="S2.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S2.F5.2.1">The X-axis indicates the embedding table size and Y-axis indicates the normalized application bandwidth. (a)-(b) The addition of CPU sockets can address the scale-up issue of memory-bound embedding table lookup operations at the cost of high-performance overhead. (c)-(d) CXL memory can provide better performance over remote CPU sockets. However, simply replacing CPU-attached memory with CXL memory causes performance overheads during high memory traffic over CXL. (e)-(f) Software interleaving during page allocation improves performance through CXL’s bandwidth expansion.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="289" id="S2.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S2.F6.2.1">The CXL bandwidth contribution to the system with different workload configurations.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Characterization Study and Motivation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The DLRM inference process is both memory capacity and bandwidth bound – the working set size increases with different parameters (e.g., the number of embeddings, embedding dimension, batch size, and number of tables) while the parallel computation demands high memory bandwidth. To get more memory bandwidth along with larger capacity, one possible solution in today’s server system is to add more CPU sockets and populate memory channels in the power of two. However, this restricts flexibility and results in stranded memory resources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite>.
As DRAM is a significant driver of infrastructure cost and power consumption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>]</cite>, its excessive underutilization also leads to high total cost of ownership (TCO). Further, the interconnect between the CPU sockets can be a bottleneck and significantly impact the performance.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.2">To illustrate this, we run the embedding table lookup phase of the DLRM inference process on a dual-socket AMD Genoa system (each socket having 96 physical CPU cores and 12 channels of DDR5 memory that populates a capacity of 768GB), with a representative DLRM trace. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F3" title="Figure 3 ‣ II-C Related Works ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">3</span></a>,
besides adding more CPU sockets, we can also increase the overall system’s memory and bandwidth capacity by populating memory channels through CXL interconnects.
Here, CXL memory is enabled through four channels of DDR4 memory resulting in memory capacity of <math alttext="256" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><cn id="S3.p2.1.m1.1.1.cmml" type="integer" xref="S3.p2.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">256</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">256</annotation></semantics></math>GB. Including the CXL memory, the server has a total memory capacity of <math alttext="1.8" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mn id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">1.8</mn><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><cn id="S3.p2.2.m2.1.1.cmml" type="float" xref="S3.p2.2.m2.1.1">1.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">1.8</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">1.8</annotation></semantics></math>TB.
Here, we configure the DLRM trace to use 192 tables with a batch size of 1024, and run embedding table look-up operation with different parallelization methods, namely, “batch threading” and “table threading” (detailed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F4" title="Figure 4 ‣ II-C Related Works ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">CXL allows flexibility in server memory capacity population, which is restricted during remote CPU socket-based memory capacity expansion. To consume the full memory bandwidth, we must populate and access all the memory channels in the remote socket. While accessing remote socket memory partially, the effective memory bandwidth might be low. For example, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F5" title="Figure 5 ‣ II-C Related Works ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">5</span></a> (a)-(b) illustrates that accessing only 20% of the whole working set size from a remote CPU socket with DDR5 DIMMs invariably reduces the application bandwidth consumption. This significantly impacts performance, particularly with large embedding dimensions and large numbers of embeddings, where we observe up to 95% degraded performance.
In contrast, instead of remote CPU memory, while accessing the same amount of memory (i.e., 20% of the working set size) over CXL interconnects with DDR4 DIMMs, we can have an enhanced performance of 5–30% (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F5" title="Figure 5 ‣ II-C Related Works ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">5</span></a> (c)-(d)). Note that CXL-attached DDR4 memory has a low refresh rate over CPU-attached DDR5 memory. Also, CXL memory is CPU-less – we do not need additional CPUs to expand memory capacity. Consequently, CXL memory can consume less power than remote CPU sockets. Moreover, re-purposing earlier generations of DDR DIMMs can save datacenter TCO while augmenting performance.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">As CXL adds bandwidth to the overall system, it can act as a “bandwidth expander” when CPU-attached memory channels are saturated. For DLRMs with large thread counts, batch sizes, embedding dimensions and sizes, both the working set size and memory bandwidth increase significantly.
At some point, the CPU-attached memory channels get saturated and become the performance bottleneck. In such cases, extra bandwidth from CXL can enhance the application throughput. For example, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F6" title="Figure 6 ‣ II-C Related Works ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">6</span></a>, when we increase the thread count from 16 to 32 and embedding table dimensions from 64 to 128, system bandwidth increases by 43%. Here, DDR4-based CXL memory improves application throughput by 28.5–38.9%, compared to the standalone CPU-attached DDR5 memory system.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.3">The potential of CXL to improve system performance and efficiency is evident. Our findings also highlight several “limitations” with the current CXL interconnects. These include the risk of flex bus congestion under heavy memory traffic, increased data access latency (by over 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><mo id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><times id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">×</annotation></semantics></math> compared to local DRAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>]</cite>), and limited bandwidth expansion capabilities. Such constraints can lead to substantial performance degradation in specific configurations, with the observed impact reaching up to 90% when the bandwidth is saturated. Our analysis of the software front suggests a promising strategy involving the <span class="ltx_text ltx_font_italic" id="S3.p5.3.1">distribution</span> of embedding tables across available memory tiers. We tried with different interleave ratios and empirically found that, when we allocate 20% of the total working set size to CXL memory and the remaining 80% to local DRAM, i.e., we allocate over a 4:1 interleave policy, we get a significant performance improvement (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S2.F5" title="Figure 5 ‣ II-C Related Works ‣ II Background and Related Works ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">5</span></a> (e)-(f)). We can achieve up to a <math alttext="9\times" class="ltx_math_unparsed" display="inline" id="S3.p5.2.m2.1"><semantics id="S3.p5.2.m2.1a"><mrow id="S3.p5.2.m2.1b"><mn id="S3.p5.2.m2.1.1">9</mn><mo id="S3.p5.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">9\times</annotation><annotation encoding="application/x-llamapun" id="S3.p5.2.m2.1d">9 ×</annotation></semantics></math> performance increase over configurations where all memory is allocated to the CXL. Table threading scenarios can offer up to a <math alttext="1.73\times" class="ltx_math_unparsed" display="inline" id="S3.p5.3.m3.1"><semantics id="S3.p5.3.m3.1a"><mrow id="S3.p5.3.m3.1b"><mn id="S3.p5.3.m3.1.1">1.73</mn><mo id="S3.p5.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">1.73\times</annotation><annotation encoding="application/x-llamapun" id="S3.p5.3.m3.1d">1.73 ×</annotation></semantics></math> performance boost compared to operations running solely on local DRAM. These results reveal that any method that relies solely on CXL memory (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite>) may restrict performance. Our experiments underscore the significant potential of CXL technology in enhancing scalability and performance.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Key takeaways from our characterization experiments are:</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.p7.1.m1.1"><semantics id="S3.p7.1.m1.1a"><mo id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><ci id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.p7.1.1">Key Takeaway 1:</span> While CXL memory enhances system scalability by offering more flexible memory configurations, its data retrieval latency is higher than DRAM. This adversely affects performance. To mitigate this, computation should happen close to memory to minimize the data transfer latency.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.p8.1.m1.1"><semantics id="S3.p8.1.m1.1a"><mo id="S3.p8.1.m1.1.1" xref="S3.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.p8.1.m1.1b"><ci id="S3.p8.1.m1.1.1.cmml" xref="S3.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.p8.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.p8.1.1">Key Takeaway 2:</span> The CXL memory can outperform remote CPU socket configurations but requires memory management strategies. Specifically, spreading memory between DRAM and CXL, coupled with careful page management, can significantly reduce performance degradation.</p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">We believe, from a hardware perspective, PIFS is a suitable solution to address these issues. First, placing a fabric switch closer to the memory reduces data movement significantly compared to traditional host-centric models. Second, unlike the PNM/PIM solutions, PIFS does <span class="ltx_text ltx_font_italic" id="S3.p9.1.1">not</span> require modifications to existing CXL devices, thus maintaining compatibility. We can also optimize <span class="ltx_text ltx_font_italic" id="S3.p9.1.2">both</span> memory usage and system performance by integrating PIFS hardware with effective page management with DRAMs. Based on these considerations and observed characteristics, we propose <span class="ltx_text ltx_font_bold" id="S3.p9.1.3">PIFS-Rec</span>.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="215" id="S3.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S3.F7.2.1">Overview of Process-In-Fabric-Switch (PIFS) for DLRMs architecture, which includes Process Core (PC), accumulate configuration logic, accumulate configuration register, and a mechanism for instruction ingress registry.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">System Design</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">PIFS-Rec scales the embedding tables and accelerates the embedding operations, known as SparseLengthSum (SLS), by leveraging device I/O level parallelism, and facilitates processing near memory by executing computations within a fabric switch. PIFS-Rec features a minimalist hardware architecture with specialized computation tailored to support the SLS family of inference operators. This special-purpose computation logic is localized to the Process Core (PC). Additionally, PIFS-Rec includes an enhanced memory controller (FM endpoint Extension) that extends the functionality of the Fabric Manager (FM) endpoint within the fabric switch. Throughout this paper, we refer to this component as the “memory controller”.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S3.F7" title="Figure 7 ‣ III Characterization Study and Motivation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">7</span></a> shows the process flow (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS2" title="IV-A2 Process Flow ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>2</span></a>) and a new instruction (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS3" title="IV-A3 Instruction Modification ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>3</span></a>) to minimize modifications to standard CXL-based DRAM devices (Type 3) and mitigate additional overhead. Considering the “localized nature” of embedding table access patterns, our framework explores employing an on-switch buffer (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS4" title="IV-A4 On-Switch Buffer ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>4</span></a>) to enhance overall system efficiency. We also implement an “out-of-order engine” (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS5" title="IV-A5 Out-of-Order Accumulation ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>5</span></a>) to prevent pipeline stalling during DLRM row access accumulation. Additionally, we enhance the software architecture (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2" title="IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>) through an optimized page management and migration process to complement our hardware design. Furthermore, we discuss scaling up multiple PIFS-Rec interconnections (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS3" title="IV-C Fabric Switch at Scale ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>) by introducing multi-layer forwarding and necessary modifications to support this growth. These optimizations draw upon empirical insights from our workload characterization in a CXL hardware-ready memory system and prior research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib10" title="">10</a>]</cite>, ensuring a grounded and practical approach to tackle the complexities of modern recommendation systems.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Hardware Architecture</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">One of our objectives is to keep the fabric switch lightweight with minimal hardware and software modifications, ensuring cost-effective deployment and compatibility. We make several changes to the conventional fabric switch – from a hardware perspective, we design the processing core to passively receive instructions from the host and operate exclusively on physical addresses, eliminating the need for a softcore or host presence within the fabric switch. With this, we do <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">not</span> need to modify the hardware or software on CXL end devices, which facilitates seamless integration to existing Type 3 devices.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>System Overview</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">PIFS-Rec is integrated within the fabric switch, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S3.F7" title="Figure 7 ‣ III Characterization Study and Motivation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">7</span></a>. The PC (Process Core), a hardware component within the fabric switch, facilitates this integration. The memory controller for PIFS-Rec is an FM endpoint extension with an enhanced memory indexing unit. Communication between the host-side CXL controller HA (Home Agent) and PIFS-Rec occurs via CXL-based instructions through the CXL interface using PCIE PHY. PIFS-Rec returns the accumulated embedding table row access results to the host. Regular CXL-based instructions are decoded by the FM endpoint extension and forwarded to the corresponding CXL devices with the modified instructions. By locating the logic within the fabric switch, PIFS-Rec can issue “concurrent requests” to <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.1">parallel CXL devices</span> and efficiently utilize bandwidth across <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.2">multiple memory channels</span>. The embedding table region is designated as a device-bias region.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>Process Flow</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite> introduces customized DIMM-based instructions and an independent CXL workflow, diverging from the current CXL protocol standard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite>. To avoid complete hardware and software stack changes, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">we design the fabric switch from scratch, maintaining compatibility with CXL memory and avoiding major modifications to the CXL host-device control protocol</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S4.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="S4.F8.2.1">(a) Instruction flow of PIFS-Rec. The valid signal indicates a successful retrieval of data. (b) The host gets the result from the fabric switch.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.4.1">Instruction Flow.</span>
In BEACON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite>, the computational logic within the fabric switch initiates memory requests. However, bypassing the host in a cloud-based inference system presents challenges, as each query might access different row candidates. Hence, the host must relay essential memory address information to the fabric switch for accurate memory access. The PC decodes instructions from the host and issues memory fetch request to specific CXL memory devices. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.F8" title="Figure 8 ‣ IV-A2 Process Flow ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">8</span></a>, during row accumulation access, the host issues a standard CXL.mem {M2S} request <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite> to the fabric switch
 <svg class="ltx_picture" height="19.62" id="S4.SS1.SSS2.p2.1.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><g fill="#000000"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject color="#FFFFFF" height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS1.SSS2.p2.1.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg>, while reserving a memory address in the LLC or specific CXL cache region and transmitting it to the fabric switch’s process core register. Upon receiving a memory request from the host, the memopcode checker examines the instruction’s memory operation (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.4.2">memOpcode</span>) field. If the instruction is standard, it bypasses the processing core and is sent directly to the VCS. Otherwise, it is routed to the process core for further handling. After receiving CXL instructions via the interface, the process core decodes the instruction and proceeds with <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.4.3">instruction repacking</span>.
This repacking modifies two instruction fields:
Firstly, for the requests initiated by the host as read requests, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.4.4">memopcode</span> is modified to transform them into standard read requests with data directed toward the CXL memory
 <svg class="ltx_picture" height="19.62" id="S4.SS1.SSS2.p2.2.pic2" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><g fill="#000000"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject color="#FFFFFF" height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS1.SSS2.p2.2.pic2.1.1.1.1.1">2</span></foreignobject></g></g></svg>. From the host (CPU) point of view, it issues the memory read request, but the actual memory issue request source point comes from the fabric switch. However, the host still acts as a “monitor”, that is, if the address is polluted or invalid, the host will realize it and inform the application or runtime. Secondly, the repacking alters the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.4.5">SPID</span> (the ID of device that initiated the request) from the host to the fabric switch, ensuring that the retrieved data are stored in the fabric switch. Once the data are retrieved from the memory and sent to the fabric switch
 <svg class="ltx_picture" height="19.62" id="S4.SS1.SSS2.p2.3.pic3" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><g fill="#000000"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject color="#FFFFFF" height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS1.SSS2.p2.3.pic3.1.1.1.1.1">3</span></foreignobject></g></g></svg>, the process core dispatches a control signal back to the host
 <svg class="ltx_picture" height="19.62" id="S4.SS1.SSS2.p2.4.pic4" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><g fill="#000000"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject color="#FFFFFF" height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS1.SSS2.p2.4.pic4.1.1.1.1.1">3</span></foreignobject></g></g></svg>, indicating successful data retrieval.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p3.1.1">Asynchronous Communication. </span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">As mentioned earlier, the fabric switch’s processing core initiates data accumulation. When the embedding tables interleave between local DRAMs, remote DRAMs, and Type 3 devices, the host computes the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.1">SumCandidateCounter</span> for each request to accumulate rows. The host first identifies all related row vector candidates’ memory addresses (using the data_ptr() API in PyTorch). It then uses the memory addresses to determine the location of each row vector, checking whether it is in the local DRAM or elsewhere (using the move_pages() API in numactl). Subsequently, it calculates the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.2">SumCandidateCounter</span> by tallying the number of vectors not stored in the local DRAM. Note that <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.3">SumCandidateCounter</span> is configured into a fabric switch using the instruction. Specifically, the PC decrements the counter by 1 each time it accumulates a row candidate. The process is considered complete when the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.4">SumCandidateCounter</span> reaches 0. Upon the completion of the accumulation process, the accumulated result is transmitted to the previously reserved memory address of the host with CXL.cache {D2H}
 <svg class="ltx_picture" height="19.62" id="S4.SS1.SSS2.p4.1.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><g fill="#000000"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject color="#FFFFFF" height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS1.SSS2.p4.1.pic1.1.1.1.1.1">4</span></foreignobject></g></g></svg>through the egress queue. The host continuously monitors (snoops) the designated address using the standard CXL snooping mechanism. Upon detecting a change in the memory location, it recognizes that the data at this location represents an accumulated result. It then retrieves the accumulated data for further processing.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="S4.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="S4.F9.2.1">Blue chunks indicating modified/added fields and SPID modification by the fabric switch.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.5.1.1">IV-A</span>3 </span>Instruction Modification</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">To implement the described mechanism (§ <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS2" title="IV-A2 Process Flow ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>2</span></a>), modifications have been made to the instruction set (CXL 3.0), as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.F9" title="Figure 9 ‣ IV-A2 Process Flow ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">9</span></a>. Specifically, the memory opcode within the Request (Req) instruction serves dual purposes: it can either initiate a request for row vector data or configure the Accumulation Configuration Register (ACR). For row vector data access, the instruction includes a <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.1">sumtag</span> that designates the accumulation cluster to which it belongs and specifies the vector size. Conversely, if the instruction is intended for configuration, it conveys the number of row vectors needed for a particular row accumulation (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.2">SumCandidateCount</span>). In this case, the address field is re-purposed to specify the location reserved for the accumulated result, which is then set within the ACR. The minimum data granularity managed is 16B, while the row vector size can vary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib14" title="">14</a>]</cite> from 16B to 64B or 128B  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite>. The <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.3">vectorsize</span> field indicates the number of data chunks to form a row access, supporting 8 different row vector size configurations with 3-bit width using binary coding.
Considering the CXL standard’s slot size limitation of 16 bytes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite>, weight (since FP32 for weight has 32 bits) and other extra information are allocated within the data slot field. When a memory fetch based instruction arrives at the PC, it is stored in Instruction Ingress Registry (IIR). New data arriving from the CXL memory to the fabric switch is indexed in the IIR, and the corresponding instruction is retrieved by comparing the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.4">address</span> field. This instruction is then forwarded to the instruction decoder, which sets up the ACR based on instruction’s fields. Each new row accumulate request from host is assigned a <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.5">sumtag</span>; each new request increases the capacity counter; and each finished request decreases it. If the ACR hits its capacity limit <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.6">CapacityCounter</span>, the system imposes back-pressure on the upstream modules until space is freed. This cycle continues until all data elements are processed, culminating in the dispatch of the result to the host.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS4.5.1.1">IV-A</span>4 </span>On-Switch Buffer</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">The on-switch buffer in PIFS-Rec utilizes on-chip SRAM and acts as a “cache” to store frequently accessed “hot content”. Unlike prior works that use buffers for queue management or traffic shaping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib49" title="">49</a>]</cite>, our buffer is specifically designed to exploit the temporal locality observed in specific embedding tables where vectors are frequently reused <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite>. Conventional prefetching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib8" title="">8</a>]</cite> strategies are less effective due to the irregular, time-wise relationship patterns exhibited by row accesses, potentially degrading system performance by consuming available bandwidth budget or displacing vectors prematurely. RecNMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite>, a PNM-DIMM-based solution, explored DIMM caching to reduce latency by leveraging data locality. We integrated an on-switch “buffer” to exploit the reuse of embedding vectors. Fetching a single address from memory pools can take up to 270 ns, with approximately 37% attributed to frequent CXL I/O port transfers and retimer delays, as per profiling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite>. This reduction in latency is achieved by minimizing wire transfers and reducing CXL I/O port overhead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite>. Distinct from traditional strategies like LRU or FIFO, PIFS-Rec employs a strategy, Hottest Recording (HTR), akin to RecNMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite>. An address profiler logs and ranks frequently accessed row vectors, curating the cache to retain highest-priority candidates based on access frequency. Managed by the FM endpoint extension on the fabric switch, this memory region is inaccessible and unmanageable by the host.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS5.5.1.1">IV-A</span>5 </span>Out-of-Order Accumulation</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">In PIFS-Rec, the accumulation operations are processed in the accumulate logic unit. Here, we optimize existing data management solutions on computational logic, leveraging insights from previous research by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib10" title="">10</a>]</cite>. In scenarios involving multiple hosts or devices, batch requests from various hosts can trigger numerous accumulation requests to different devices. However, access congestion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib50" title="">50</a>]</cite> at frequently-used memory I/O ports may cause delays in the arrival of row data in the embedding table, as observed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib52" title="">52</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS5.p2.1.1">Eliminating Hardware Stalls.</span>
We do not <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS5.p2.1.2">solely</span> rely on hardware parallelism such as deploying multiple Near Data Processing (NDP) units <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite>, due to two main constraints. Firstly, the extensive computational logic required on the fabric switch demands significant amount of resources, limiting scalability. Secondly, the system’s throughput is ultimately constrained by the number of parallel compute units, potentially leading to stalls once this limit is reached. To overcome these limitations, we introduce an “out-of-order” compute approach, enabling immediate data processing upon arrival of the same accumulation request. In case of incoming data corresponding to a different request, the system transfers the accumulated intermediate result from the accumulation register to a swap register during the first half of the clock cycle, allowing for processing of the new data in the subsequent half. The shared swap region approach among multiple processing cores and accumulation logic ensures efficient data handling. Note that the SRAM in the switch buffer can also contain the intermediate result while the swap register is full. However, accessing data from the SRAM in the switch buffer requires at least two clock cycles, potentially causing stalls. In our current approach, we make this function configurable by configuring the Functional Configuration Register (FCR).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Software Architecture</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">From our characterization study (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S3" title="III Characterization Study and Motivation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">III</span></a>), we find that for DLRM, proper utilization of all the available memory channels simultaneously can provide the optimized performance in a CXL-enabled system.
Considering this, our software architecture incorporates the following design principles –
<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">(a)</span> as CPU-attached local memory node has the lowest access latency and comparatively high bandwidth over CXL-memory, hottest or most frequently accessed pages should reside on the local memory tier;
<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">(b)</span> When we need to access CXL memory, if we can spread the memory across multiple CXL nodes, then we can parallelize better and utilize the bandwidth across all the channels;
<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">(c)</span> As migration of pages is a widespread event in a tiered memory system, optimizing that software feature can significantly enhance the overall system’s performance.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.5.1.1">IV-B</span>1 </span>Page Granular Access</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">In DLRM, the dimension of an embedding vector can be very small (e.g., typically, ranges between 16–128B).
As CXL supports cache-line granular access, we can consider the embedding dimension to be the granularity of memory access and efficiently identify the hot-cold rows to perform fine-grained vector embedding management.
However, the metadata management overhead will be high.
On the other hand, a single OS page (e.g., typically, a 4KB-sized page) can contain multiple row vectors (e.g., 256 embeddings of 16B size).
It is possible that all the embeddings within a page may not be accessed simultaneously, which will cause amplification of data movement.
However, even with this caveat, in our system, same as previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib27" title="">27</a>]</cite>, we opt to manage memory placement at page-granular as page-granular metadata management and migration is supported and compatible with the current OS. Hot-cold detection also happens on page-granular.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.5.1.1">IV-B</span>2 </span>Global Hotness Detection</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We provide a unified memory architecture where all the hosts can access pages across the system.
When a host accesses a page frequently, we identify it as the hottest one and put it to its local DRAM (we call it “Private Hot Region”) (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.F10.1" title="Figure 10 ‣ IV-B2 Global Hotness Detection ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">10</span></a> (a)).
As CXL has higher latency (around 100ns extra over local DRAM), we put the relatively cold pages in the CXL memory address space (we call it “Public Cold Region”), which is shared between all the connected hosts.
To identify the global page temperature, each host monitors the access frequency of a page across all devices – the most frequently accessed pages within a device are categorized as “hottest” while the least frequently accessed pages are categorized as “coldest”.
After generating all the device’s page heatmaps, the host compares them.
Therefore, it finds the most frequently accessed pages across all devices and stores them in its private hot region.
If a host identifies a page already designated as a private hot page by another host, it selects its next most frequently accessed page as its private hot page.
Remote hosts access memory from another host’s private hot region over the flex bus, incorporating an accumulation process within the fabric switch.
If a host retrieves a row vector from local memory, accumulation happens locally, although it is capable of receiving (but only partially processing) the accumulated results.
Every host periodically reclassifies hot private pages as public cold pages if their access frequency exceeds the least accessed private hot page’s access frequency by more than “cold_age_threshold” (by default, 20%).</p>
</div>
<figure class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" id="S4.F10.1" style="width:433.6pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="382" id="S4.F10.1.g1" src="x10.png" width="747"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold" id="S4.F10.1.3.1">(a) Page migration and management. The socket on same board can access remote DIMM using the board-level interconnect, but it needs fabric switch to access remote DIMM on another board. (b) In the worst case, memory requests are not localized across devices. </span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="S4.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold" id="S4.F11.2.1">From CXL 2.0 to 3.0. PIFS-Rec supports scale-up with multi-host scenarios (T3: Type 3 memory devices).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS3.5.1.1">IV-B</span>3 </span>Embedding Spreading for Bandwidth Optimization</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">In our architecture, we address the potential bottleneck scenario due to disproportionate demand on specific memory devices.
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.F10.1" title="Figure 10 ‣ IV-B2 Global Hotness Detection ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">10</span></a> (b), despite dedicated processing threads and specialized task allocation, the system’s bandwidth may not be fully optimized if a particular memory node consistently handles most data requests.
To address this, we introduce a simple yet effective adaptive “page migration strategy” to ensure the maximal utilization of channel capabilities across the system. The objective is to <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">redistribute</span> the workload more evenly across the available memory nodes to alleviate the bottleneck and optimize the bandwidth usage.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">When we place the cold pages in the ”Public Cold Region,” we initially spread them across the CXL memory nodes through the interleave policy.
At a later point, if a CXL memory node becomes warm, i.e., the memory access count for a node exceeds the average access count for other nodes by “1 - migrate_threshold” (by default, 35%), we initiate the page redistribution process for that particular memory node.
This process entails transferring the most accessed pages from the overburdened memory node to the least accessed one.
If the destination node is out of capacity, we also move the coldest page of that device to the overburdened memory node.
Therefore, the page with the second-highest access frequency on the overburdened node becomes the new hottest page for that node.
Similarly, if the cold page is moved, the coldest page of the least frequently accessed memory node also gets redefined.
We re-iterate the procedure across all the memory nodes until the access frequency gets balanced.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS4.5.1.1">IV-B</span>4 </span>Optimization in Page Migration</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Existing work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>]</cite> usually focuses on page-level migration due to software compatibility and OS support.
However, page migration during the live-on inference system can stall query processing due to migration overhead and data inaccessibility.
When a page is migrated, the OS typically marks it as non-accessible (page block).
In that case, for a row vector dimension of 64B, a 4KB-sized page migration will block access to all the 64-row vectors residing within that page.
To address this, although we use page-granular memory management to reduce metadata overhead, during migration, we leverage CXL’s cache-line granular memory access feature.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.1">We enhance the peer-to-peer (P2P) communication mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib25" title="">25</a>]</cite>, with the support of the “Migration Controller” (MC) in FM endpoint extension. Once the OS triggers migration, instead of copying the whole page, the host migrates in cache-line granularity. So, when the page is migrated, locking on a particular cache line cannot restrict the accessibility of the remaining cache lines. During this process, the cache-line is not stored back in the secondary memory or returned to the host; instead, it is stored in a temporal location in the switch (cache-line block). This optimization reduces the overhead by up to <math alttext="5.1\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS4.p2.1.m1.1"><semantics id="S4.SS2.SSS4.p2.1.m1.1a"><mrow id="S4.SS2.SSS4.p2.1.m1.1b"><mn id="S4.SS2.SSS4.p2.1.m1.1.1">5.1</mn><mo id="S4.SS2.SSS4.p2.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p2.1.m1.1c">5.1\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p2.1.m1.1d">5.1 ×</annotation></semantics></math> over the OS’s page-granular migration process.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Fabric Switch at Scale</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.F11" title="Figure 11 ‣ IV-B2 Global Hotness Detection ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">11</span></a>, fabric switches play a crucial role in scaling out, connecting hosts and devices into a “unified fabric” for coherent memory sharing and device communication. In a scaled-out CXL environment, multiple hosts connect to a non-tree-shaped CXL fabric switch, facilitating connections to shared Type 3 memory devices or other hosts. This configuration enables a “distributed computing” model, distributing computational and memory resources across multiple nodes. Notably, while <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite> focuses on single fabric switch computation, our work demonstrates how multiple fabric switches can communicate and collaborate.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS1.5.1.1">IV-C</span>1 </span>Multi-layer Instruction Forwarding</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">In a simplified scenario, we assume each fabric switch has a process core. PIFS-Rec enables vector accumulation to be executed directly on the remote fabric switch (close to the Type 3 memory device), thereby conserving a significant network interconnect bandwidth. Consequently, instruction repacking is confined to the remote fabric switch that handles its local memory. The fabric switch tracks the row candidate requests sent to and received from other fabric switches. Its scheduler reads the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.1">sumtage</span> field and corresponding memory address, recording the number of memory requests for specific row access to each remote fabric switch, and sends the new <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.2">Sub-SumCandidateCount</span> to replace <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.3">SumCandidateCount</span>. This mechanism ensures the sanity of data exchange and accumulation processing. When a fabric switch positioned near the local host (the host issues the row access request) receives results from a remote fabric switch, there is a possibility that some candidates from other nodes have not yet arrived. The forward controller in the fabric switch monitors the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.4">sumcandidatecounter</span> to address such situations. The remote fabric switch transmits its <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.5">sub-sumcandidatecounter</span> back to the local switch, which then uses this information to determine whether to forward the accumulated result to the host (once all candidates have been processed), wait for missing data from other nodes, or discard the result if errors occurred during data transfer.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS2.5.1.1">IV-C</span>2 </span>Versatility</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">The framework can also work with fabric switches without processing cores. During the initial setup and configuration phase, the local fabric switch must identify remote fabric switches’ lack of processing capabilities. The scheduler will read a 1-bit <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.1">CNV</span> (Compute Node Valid) field for each fabric switch during the configuration process. If it is determined that a remote fabric switch lacks processing power, the local fabric switch will undertake all operations and instruction repackaging tasks by itself.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Programming-Related Aspects</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">PIFS employs an easy-to-use, OpenCL-like heterogeneous computing programming model similar to the one adopted in the previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite>. More specifically, PIFS-Rec provides SLS APIs that can be called from user-space, allowing integration with mainstream frameworks like PyTorch. Users can utilize this function call to accelerate specific types of DLRMs and all ML models that require sparse network embedding table accumulation. We aim to reduce the programmer’s burden by <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">abstracting</span> as much information as possible. When allocating memory, users must supply the embedding table file as a parameter, along with the number of embeddings and vector size. Additional parameters, including batch size, indices, offset, or length, are also required. After receiving the information, the PIFS kernel allocates memory space from the CXL memory pool using <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.2">numactl</span> mapping information. The allocated memory region will be implicitly defined as host-biased using the OpenCL API, e.g., <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.3">clEnqueueSVMUnmap</span>. Each function call also allocates an address that points to the output result and pins the address. It generates the embedding table iterative accumulation codes using parameters such as length, batch size, and indices. Simultaneously, the host allocates an embedding table based on the CXL-recognized memory space. The CXL-supported CPU compiles and generates CXL instructions with the corresponding <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.4">MemOpcode</span> field by reading the generated PIFS kernel code. The fabric switch then begins to receive the instruction and passively starts the computation. Meanwhile, a daemon process on the host starts snooping the result address and monitoring the process’s integrity for each called function. While instructions are being processed in the fabric switch, the memory indexing function directs data to the corresponding devices and retrieves the data. When page migration is triggered and certain pages are mapped to CXL memory, the bias table flip function hooks into <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.5">page migration()</span> and marks it as a device-biased page using APIs (e.g., <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.6">clEnqueueSVMMap</span>), informing the host and the fabric switch, changing the candidate count number, and indexing data to a new location (recall that the rest of the workflow is described earlier in Section (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS2" title="IV-A2 Process Flow ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>2</span></a>)).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Workload Generality.</span> Our proposed framework for PIFS is highly adaptable, facilitating its usage across various practical workloads (e.g. Kv aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib55" title="">55</a>]</cite>, Mapreduce <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib56" title="">56</a>]</cite> ). This adaptability is achieved by substituting the compute logic, and DLRM-specific registers with components tailored to the new workload. Unlike BEACON<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite>, which introduces optimizations specific to genome analysis, our proposed optimizations are designed to be applicable across new workloads without modifying the proposed instruction flow and process flow.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Hardware Compatibility.</span> PIFS-Rec is designed to <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">complement, not replace</span>, PNM/PIM-based approaches. PIFS-Rec is compatible with hardware that includes embedded cores on DIMM, such as RecNMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite>, which addresses bandwidth limitations by using intra-DIMM bandwidth. Integrating such technologies increases operating system complexity, demanding runtime and compiler adaptations for CXL-based heterogeneous computing. This integration requires significant software and hardware modifications, including the adoption of CXL-compatible and core-enabled DIMMs. Addressing these challenges will require extensive research in operating system support, memory management, and hardware innovations.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span><span class="ltx_text ltx_font_bold" id="S5.T1.3.1">Model parameters in the scope of this study.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.1" style="font-size:80%;">Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.2.1" style="font-size:80%;">Emb. Num</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.3.1" style="font-size:80%;">Emb. Dim</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.4.1" style="font-size:80%;">Bottom-MLP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.5.1" style="font-size:80%;">Top-MLP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.2.1.1"><span class="ltx_text" id="S5.T1.4.2.1.1.1" style="font-size:80%;">RMC1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.2.1.2"><span class="ltx_text" id="S5.T1.4.2.1.2.1" style="font-size:80%;">16384</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.2.1.3"><span class="ltx_text" id="S5.T1.4.2.1.3.1" style="font-size:80%;">64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.2.1.4"><span class="ltx_text" id="S5.T1.4.2.1.4.1" style="font-size:80%;">256-128-128</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.2.1.5"><span class="ltx_text" id="S5.T1.4.2.1.5.1" style="font-size:80%;">128-64-1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T1.4.3.2.1"><span class="ltx_text" id="S5.T1.4.3.2.1.1" style="font-size:80%;">RMC2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.3.2.2"><span class="ltx_text" id="S5.T1.4.3.2.2.1" style="font-size:80%;">131072</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.3.2.3"><span class="ltx_text" id="S5.T1.4.3.2.3.1" style="font-size:80%;">64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.3.2.4"><span class="ltx_text" id="S5.T1.4.3.2.4.1" style="font-size:80%;">1024-512-128</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.3.2.5"><span class="ltx_text" id="S5.T1.4.3.2.5.1" style="font-size:80%;">384-192-1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.4.3">
<td class="ltx_td ltx_align_left" id="S5.T1.4.4.3.1"><span class="ltx_text" id="S5.T1.4.4.3.1.1" style="font-size:80%;">RMC3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.4.3.2"><span class="ltx_text" id="S5.T1.4.4.3.2.1" style="font-size:80%;">1048576</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.4.3.3"><span class="ltx_text" id="S5.T1.4.4.3.3.1" style="font-size:80%;">64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.4.3.4"><span class="ltx_text" id="S5.T1.4.4.3.4.1" style="font-size:80%;">2048-1024-256</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.4.3.5"><span class="ltx_text" id="S5.T1.4.4.3.5.1" style="font-size:80%;">512-256-1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.5.4.1"><span class="ltx_text" id="S5.T1.4.5.4.1.1" style="font-size:80%;">RMC4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.5.4.2"><span class="ltx_text" id="S5.T1.4.5.4.2.1" style="font-size:80%;">1048576</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.5.4.3"><span class="ltx_text" id="S5.T1.4.5.4.3.1" style="font-size:80%;">128</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.5.4.4"><span class="ltx_text" id="S5.T1.4.5.4.4.1" style="font-size:80%;">2048-2048-256</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.5.4.5"><span class="ltx_text" id="S5.T1.4.5.4.5.1" style="font-size:80%;">768-384-1</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span><span class="ltx_text ltx_font_bold" id="S5.T2.3.1">Hardware configuration.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.1.2.1.1" style="padding-left:35.0pt;padding-right:35.0pt;">
<span class="ltx_text" id="S5.T2.1.2.1.1.1" style="font-size:70%;">           </span><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.1.2" style="font-size:70%;">DRAM Configuration</span><span class="ltx_text" id="S5.T2.1.2.1.1.3" style="font-size:70%;"></span>
</th>
<td class="ltx_td ltx_nopad_r ltx_border_tt" id="S5.T2.1.2.1.2" style="padding-left:35.0pt;padding-right:35.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.2.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.3.2.1.1" style="font-size:70%;">           DIMM Capacity</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.1.3.2.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.3.2.2.1" style="font-size:70%;">           64 GBs per DIMM</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.3.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.4.3.1.1" style="font-size:70%;">           DIMM Channels/Ranks</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.1.4.3.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.4.3.2.1" style="font-size:70%;">           4/2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.4.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.5.4.1.1" style="font-size:70%;">           Frequency (MHz)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.1.5.4.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.5.4.2.1" style="font-size:70%;">           4800</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.6.5.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.6.5.1.1" style="font-size:70%;">           Timings (CL-RCD-RP-RAS)</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.1.6.5.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.6.5.2.1" style="font-size:70%;">           28-28-28-52</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.7.6.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.7.6.1.1" style="font-size:70%;">           tRC/tWR/tRTP</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.1.7.6.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.7.6.2.1" style="font-size:70%;">           79/48/12</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.8.7.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.8.7.1.1" style="font-size:70%;">           tCWL/nRFC1/tCK_ps</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.1.8.7.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.8.7.2.1" style="font-size:70%;">           22/30/625</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.9.8.1" style="padding-left:35.0pt;padding-right:35.0pt;">
<span class="ltx_text" id="S5.T2.1.9.8.1.1" style="font-size:70%;">           </span><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.8.1.2" style="font-size:70%;">CXL Configuration</span><span class="ltx_text" id="S5.T2.1.9.8.1.3" style="font-size:70%;"></span>
</th>
<td class="ltx_td ltx_nopad_r ltx_border_t" id="S5.T2.1.9.8.2" style="padding-left:35.0pt;padding-right:35.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.1.2.1" style="font-size:70%;">           Fabric Switch Downstream Ports:</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.1.1.1" style="padding-left:35.0pt;padding-right:35.0pt;">
<span class="ltx_text" id="S5.T2.1.1.1.1" style="font-size:70%;">           64GB/s </span><math alttext="\times 16" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mrow id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.m1.1.1.2" xref="S5.T2.1.1.1.m1.1.1.2.cmml"></mi><mo id="S5.T2.1.1.1.m1.1.1.1" lspace="0.222em" mathsize="70%" rspace="0.222em" xref="S5.T2.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S5.T2.1.1.1.m1.1.1.3" mathsize="70%" xref="S5.T2.1.1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"><times id="S5.T2.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T2.1.1.1.m1.1.1.2.cmml" xref="S5.T2.1.1.1.m1.1.1.2">absent</csymbol><cn id="S5.T2.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.1.1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times 16</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">× 16</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.10.9.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.10.9.1.1" style="font-size:70%;">           Fabric Switch Buffer R/W Speed</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T2.1.10.9.2" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.10.9.2.1" style="font-size:70%;">           0.91-4.19 ns/0.91-4.17 ns</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.1.11.10.1" style="padding-left:35.0pt;padding-right:35.0pt;"><span class="ltx_text" id="S5.T2.1.11.10.1.1" style="font-size:70%;">           CXL Access Penalty over DRAM:</span></th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S5.T2.1.11.10.2" style="padding-left:35.0pt;padding-right:35.0pt;">
<span class="ltx_text" id="S5.T2.1.11.10.2.1" style="font-size:70%;">           100 ns </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.11.10.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a><span class="ltx_text" id="S5.T2.1.11.10.2.3.2" style="font-size:70%;">]</span></cite>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Implementation and Evaluation</span>
</h2>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="S6.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold" id="S6.F12.2.1">The performance of systems with different: (a) models, (b) types of traces (ZF: Zipfian, NoL: Normal, Um: uniform, Rm: Random), (c) memory devices, e.g., X2 means two memory devices, (d) DRAM size, e.g., X2 means 256 GB DRAM, and (e) ablation study: the PC (processing core) is PIFS-Rec specified. The plot uses min-max normalization.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Setup</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We adhered to the methodology outlined in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite>. As described in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S5.T2" title="TABLE II ‣ V Discussion ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">II</span></a>, we conducted cycle-level memory simulations using Ramulator 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib57" title="">57</a>]</cite> for a detailed evaluation of PIFS-Rec. We wrapped Ramulator 2.0 into our simulator; the top module includes a cycle-accurate processing logic for Process Core, FM Endpoint Extension, Instruction Repacking, and Memopcode Checker and with a top-module clock tick period of one ns/clk. We introduced additional latency (ticks) for data directed to the CXL memory to accurately simulate performance impacts, considering its inherently higher access latency than on-switch DRAM (refers to Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S5.T2" title="TABLE II ‣ V Discussion ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">II</span></a> ). We use the open-source Meta traces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib58" title="">58</a>]</cite> and models (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S5.T1" title="TABLE I ‣ V Discussion ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">I</span></a>) for reproducibility. A “lookup table” was developed to facilitate address indexing and mapping logic, directing the memory footprint to either CXL memory or an on-switch buffer. This table is used to record memory access and I/O patterns. Our comprehensive latency evaluation considered several critical factors, such as the additional DRAM cycles required for initializing accumulation counters, the latency introduced by the fabric switch, and the time needed to transfer the final computed sum back to the host system; we extracted the performance from top module synthesis.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Baselines</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We selected several previous works as “baselines” to highlight the state-of-the-art in various aspects of memory pooling and processing architectures. Pond <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib26" title="">26</a>]</cite> introduces a straightforward CXL-based memory pooling approach with OS support, emphasizing simplicity in design. We add our PM (page management) optimization to Pond, denoted as “Pond + PM” to demonstrate the performance of software optimization independently. In comparison, BEACON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite> presents the PIFS architecture to accelerate DNA computation. In our evaluations, we modified the compute logic only to process vector accumulation. Since our main workload is DLRM inference, we implemented the BEACON (BEACON-S) <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">without</span> algorithm-specific optimizations. RecNMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a>]</cite> is a DIMM-based hardware solution that accelerates SLS operations. We implemented the design using their computational hardware configuration with our memory setting. We used a fixed amount of 128GB local DRAM, and memory addresses exceeding this amount will be mapped into CXL regions. Even though BEACON does not support DRAM and CXL interleaving, we still used reduced DRAM latency to access the corresponding 128 GB for BEACON.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Evaluation</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">To demonstrate the performance benefits of our proposed design for Sparse Length Sum (SLS) operations, we employ various memory devices across different models. Specifically, we utilize four memory devices with default parameters: 8 per batch, RMC4 model, page swap threshold 12%, embedding migration threshold 35%, and 512 KB buffer size. We evaluate the performance by using the total ticks used to process the traces and use min-max normalization.</p>
</div>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S6.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span class="ltx_text ltx_font_bold" id="S6.F13.2.1">Performance Comparison of (a) Embedding migration strategy across different thresholds, displaying normalized latency on SLS-operations (blue). (b) Embedding migration across devices, comparing IO access frequency before and after page migration. (c) Instruction forwarding across various fabric switches and batches. (d) Page swapping strategy for private hot and public cold pages at different thresholds. For (a) and (d), the right Y-axis is a normalized overhead for page block (red) and cache-line block (green). The migration costs are with respect to the total latency.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS3.SSS1.5.1.1">VI-C</span>1 </span>HW/SW Co-Evaluation</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F12" title="Figure 12 ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">12</span></a> (a) presents the performance results with different schemes. Pond, which integrates standard CXL support, demonstrates the lowest performance among the evaluated systems. This is anticipated because, although CXL provides an increase in bandwidth over traditional DIMM-based systems, it is hindered by significant data retrieval latency. As the workload size increases from RMC1 to RMC4, the number of pages mapped to CXL increases. Consequently, the latency increases in all approaches. However, PIFS-Rec has the lowest latency for all these workloads. On average, PIFS-Rec outperforms Pond by 3.8x, Pond + PM by 3.5x, RecNMP by 8.5%, and BEACON by 1.94x across all the workloads.
On the other hand, the potential for bandwidth scalability through connecting multiple memory devices via a fabric switch has yet to be fully realized due to the lack of embedding table migration, limiting the system’s ability to exploit the increased bandwidth effectively. For the largest model (RMC4), PIFS-Rec achieves only about 11% improvement over RecNMP because the latter performs data fetch with bank-level parallelism. Also, PIFS-Rec achieves 3.89<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS1.p1.1.m1.1"><semantics id="S6.SS3.SSS1.p1.1.m1.1a"><mo id="S6.SS3.SSS1.p1.1.m1.1.1" xref="S6.SS3.SSS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.1.m1.1b"><times id="S6.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS1.p1.1.m1.1d">×</annotation></semantics></math>, 3.57<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS1.p1.2.m2.1"><semantics id="S6.SS3.SSS1.p1.2.m2.1a"><mo id="S6.SS3.SSS1.p1.2.m2.1.1" xref="S6.SS3.SSS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.2.m2.1b"><times id="S6.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS1.p1.2.m2.1d">×</annotation></semantics></math> and 2.03<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS1.p1.3.m3.1"><semantics id="S6.SS3.SSS1.p1.3.m3.1a"><mo id="S6.SS3.SSS1.p1.3.m3.1.1" xref="S6.SS3.SSS1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.3.m3.1b"><times id="S6.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S6.SS3.SSS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS1.p1.3.m3.1d">×</annotation></semantics></math> improvements over Pond, Pond + PM and BEACON, respectively, due to its intelligent page management and optimized device-level parallelism.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS3.SSS2.5.1.1">VI-C</span>2 </span>Generality</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.3">In addition to the Meta traces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib58" title="">58</a>]</cite>, we have conducted experiments with synthetic traces to cover a large spectrum of DLRM workload scenarios. Note that Meta traces primarily reflect workload distribution, particularly in Meta’s implementation of DLRM, which may not comprehensively represent the diversity of DLRM workloads. Our synthetic traces emulate various distribution types based on the access candidates observed in the Meta traces. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F12" title="Figure 12 ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">12</span></a> (b), our findings indicate that the uniform distribution yields the most favorable performance since it creates a perfectly balanced distribution of embedding table accesses across devices. This distribution strategy results in a 1.1<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p1.1.m1.1"><semantics id="S6.SS3.SSS2.p1.1.m1.1a"><mo id="S6.SS3.SSS2.p1.1.m1.1.1" xref="S6.SS3.SSS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p1.1.m1.1b"><times id="S6.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.p1.1.m1.1d">×</annotation></semantics></math> improvement over RecNMP. Conversely, the Zipfian distribution is identified as the least effective, yielding only a 2% performance enhancement over RecNMP. Without the help of hardware support to mitigate the bandwidth bottleneck, Pond + PM improves over the baseline by only 21%, on average. PIFS-Rec achieves improvements of 2-2.2<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p1.2.m2.1"><semantics id="S6.SS3.SSS2.p1.2.m2.1a"><mo id="S6.SS3.SSS2.p1.2.m2.1.1" xref="S6.SS3.SSS2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p1.2.m2.1b"><times id="S6.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.p1.2.m2.1d">×</annotation></semantics></math> over BEACON and 3.8-3.9<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p1.3.m3.1"><semantics id="S6.SS3.SSS2.p1.3.m3.1a"><mo id="S6.SS3.SSS2.p1.3.m3.1.1" xref="S6.SS3.SSS2.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p1.3.m3.1b"><times id="S6.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS3.SSS2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.p1.3.m3.1d">×</annotation></semantics></math> over Pond, underscoring the importance of careful memory mapping and maximized I/O parallelism.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS3.SSS3.5.1.1">VI-C</span>3 </span>Ablation Study</h4>
<div class="ltx_para" id="S6.SS3.SSS3.p1">
<p class="ltx_p" id="S6.SS3.SSS3.p1.1">We explore several optimizations encompassing both hardware and software enhancements. The results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F12" title="Figure 12 ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">12</span></a> (d) reveal that adding processing cores yields a modest 26% improvement compared to Pond, partially utilizing the high bandwidth. Incorporating out-of-order processing (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS5" title="IV-A5 Out-of-Order Accumulation ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>5</span></a>) provides at most 7.3% enhancement due to the elimination of cycle stalling. We observe a performance boost from page management (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS2" title="IV-B2 Global Hotness Detection ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span>2</span></a>,§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS3" title="IV-B3 Embedding Spreading for Bandwidth Optimization ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span>3</span></a>), resulting in around 27% improvement due to optimized memory access and better device-level parallelism. On-switch buffering (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS4" title="IV-A4 On-Switch Buffer ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>4</span></a>) with PIFS effectively mitigates CXL’s high retrieval latency, resulting in an additional 15% improvement over Pond. Combining out-of-order processing with page migration optimizes I/O parallelism and minimizes stall time. We analyzed the impact of varying on-switch buffer capacities on performance in the following section(§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.SS3.SSS5" title="VI-C5 On Switch Buffer Capacity ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span>5</span></a>).</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="397" id="S6.F14.g1" src="x14.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span class="ltx_text ltx_font_bold" id="S6.F14.2.1">Speedup of PIFS-Rec with different numbers of hosts sending concurrent requests under different models.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS3.SSS4.5.1.1">VI-C</span>4 </span>Scalability</h4>
<div class="ltx_para" id="S6.SS3.SSS4.p1">
<p class="ltx_p" id="S6.SS3.SSS4.p1.2">Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib33" title="">33</a>]</cite>, the above discussions, and our experimental analysis collectively confirm that CXL memory pooling enhances scalability compared to DIMM-based solutions.
We divide the trace file region evenly across memory devices. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F12" title="Figure 12 ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">12</span></a> (c), the performance can be reduced with limited hardware setup, likely due to constrained optimization opportunities for I/O performance and inherently poorer device-level parallelism (compared to bank-level parallelism).
Nevertheless, as the hardware inclusion expands, our design demonstrates superior performance in latency, achieving approximately a 12.5<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p1.1.m1.1"><semantics id="S6.SS3.SSS4.p1.1.m1.1a"><mo id="S6.SS3.SSS4.p1.1.m1.1.1" xref="S6.SS3.SSS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p1.1.m1.1b"><times id="S6.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p1.1.m1.1d">×</annotation></semantics></math> improvement over Pond, 8.3<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p1.2.m2.1"><semantics id="S6.SS3.SSS4.p1.2.m2.1a"><mo id="S6.SS3.SSS4.p1.2.m2.1.1" xref="S6.SS3.SSS4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p1.2.m2.1b"><times id="S6.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p1.2.m2.1d">×</annotation></semantics></math> improvement over Pond + PM, a 22% improvement on RecNMP when there are 16 memory devices. We conduct a sensitivity study by increasing the local DRAM capacity and find that PIFS-Rec still performs the best. Here, the DRAM capacity plays a relatively minor role in shaping the performance. Specifically, 256GB and 512GB DRAM budget result in average performance improvements of 4% and 6% compared to 128GB DRAM configuration. This limited effect of DRAM capacity is due to two main reasons – the model size is in the several terabytes range and the primary bottleneck is memory bandwidth. Therefore, increasing memory capacity alone cannot alleviate the issue of bandwidth saturation.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS4.p2">
<p class="ltx_p" id="S6.SS3.SSS4.p2.4">To estimate the improvements in end-to-end inference latency with multi-host and multi-batch cases, we calculate the speedup by <span class="ltx_text ltx_font_italic" id="S6.SS3.SSS4.p2.4.1">weighting</span> the speedup of both SLS and non-SLS operators. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F14" title="Figure 14 ‣ VI-C3 Ablation Study ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">14</span></a>, the performance enhancements due to PIFS-Rec vary with batch size. As the time spent in accelerated SLS operators grows, the model-level speedup increases with larger batch sizes. In RMC4, with the number of hosts increasing from 2 to 8, the performance improves by 1.9–4.7<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p2.1.m1.1"><semantics id="S6.SS3.SSS4.p2.1.m1.1a"><mo id="S6.SS3.SSS4.p2.1.m1.1.1" xref="S6.SS3.SSS4.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p2.1.m1.1b"><times id="S6.SS3.SSS4.p2.1.m1.1.1.cmml" xref="S6.SS3.SSS4.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p2.1.m1.1d">×</annotation></semantics></math>. With the support of the multi-layer instruction forwarding strategy (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS3.SSS1" title="IV-C1 Multi-layer Instruction Forwarding ‣ IV-C Fabric Switch at Scale ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span>1</span></a>), We illustrate the latency improvements over different fabric switch counts in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F13" title="Figure 13 ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">13</span></a> (c). Assuming each fabric switch has one local CXL memory and one host. These fabric switches are fully connected and we add an extra 100 ns latency when data needs to be transferred between them. The results with RMC4 indicate that as the fabric switch count is increased from 2<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p2.2.m2.1"><semantics id="S6.SS3.SSS4.p2.2.m2.1a"><mo id="S6.SS3.SSS4.p2.2.m2.1.1" xref="S6.SS3.SSS4.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p2.2.m2.1b"><times id="S6.SS3.SSS4.p2.2.m2.1.1.cmml" xref="S6.SS3.SSS4.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p2.2.m2.1d">×</annotation></semantics></math> to 32<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p2.3.m3.1"><semantics id="S6.SS3.SSS4.p2.3.m3.1a"><mo id="S6.SS3.SSS4.p2.3.m3.1.1" xref="S6.SS3.SSS4.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p2.3.m3.1b"><times id="S6.SS3.SSS4.p2.3.m3.1.1.cmml" xref="S6.SS3.SSS4.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p2.3.m3.1d">×</annotation></semantics></math>, the latency improves by 1.8–20.8<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS4.p2.4.m4.1"><semantics id="S6.SS3.SSS4.p2.4.m4.1a"><mo id="S6.SS3.SSS4.p2.4.m4.1.1" xref="S6.SS3.SSS4.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS4.p2.4.m4.1b"><times id="S6.SS3.SSS4.p2.4.m4.1.1.cmml" xref="S6.SS3.SSS4.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS4.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS4.p2.4.m4.1d">×</annotation></semantics></math> in the largest batch.</p>
</div>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S6.F15.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span><span class="ltx_text ltx_font_bold" id="S6.F15.2.1">Comparison of HTR performance across different cache sizes and replacement strategies.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS3.SSS5.5.1.1">VI-C</span>5 </span>On Switch Buffer Capacity</h4>
<div class="ltx_para" id="S6.SS3.SSS5.p1">
<p class="ltx_p" id="S6.SS3.SSS5.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F15" title="Figure 15 ‣ VI-C4 Scalability ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">15</span></a>, the speedup numbers demonstrate how each caching strategy benefits system performance by reducing access times compared to the baseline (no caching). As the model size increase from RMC1 to RMC4, our HTR’s maximum gain reduces from 19.3% to 14.8% with 512KB SRAM, due to larger footprint of the trace. In the largest model (RMC4), our hottest recording strategy (HTR) (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS1.SSS4" title="IV-A4 On-Switch Buffer ‣ IV-A Hardware Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>4</span></a>) generates better performance scaling with increasing cache size, achieving a speedup ranging from 7.6% to 14.8%, as the cache size increases from 64 KB to 512 KB. However, for the HTR strategy, a larger cache size (1 MB) results in performance degradation due to the absence of a significant increase in cache hit ratio (41.9%) for the 1 MB cache, coupled with an increase in cache hit latency. This suggests that HTR with a cache size of 512KB effectively leverages data locality. In contrast, the LRU and FIFO strategies exhibit more modest improvements.</p>
</div>
<figure class="ltx_figure" id="S6.F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F18.1" style="width:173.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="302" id="S6.F18.1.g1" src="x16.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span><span class="ltx_text ltx_font_bold" id="S6.F18.1.2.1">TCO under different models with increasing GPU budgets. e.g., X2 means 2 GPUs.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F18.2" style="width:108.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="459" id="S6.F18.2.g1" src="x17.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span><span class="ltx_text ltx_font_bold" id="S6.F18.2.2.1">Normalized throughput using different models.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F18.fig1" style="width:130.1pt;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.F18.fig1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.F18.fig1.1.1.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.F18.fig1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.1.1.1.1" style="font-size:70%;">System</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.F18.fig1.1.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.1.1.2.1" style="font-size:70%;">Power/Area</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.F18.fig1.1.2.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.F18.fig1.1.2.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.F18.fig1.1.2.1.1.1" style="font-size:70%;">RecNMP-base(X8) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.F18.fig1.1.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib7" title="">7</a><span class="ltx_text" id="S6.F18.fig1.1.2.1.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S6.F18.fig1.1.2.1.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.2.1.2.1" style="font-size:70%;">75.4 mW / 215984 um²</span></td>
</tr>
<tr class="ltx_tr" id="S6.F18.fig1.1.3.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.F18.fig1.1.3.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S6.F18.fig1.1.3.2.1.1" style="font-size:70%;">PIFS-Rec Breakdown</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t" id="S6.F18.fig1.1.3.2.2" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S6.F18.fig1.1.4.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.F18.fig1.1.4.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.4.3.1.1" style="font-size:70%;">Process Core</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S6.F18.fig1.1.4.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.4.3.2.1" style="font-size:70%;">9.3 mW / 33709 um²</span></td>
</tr>
<tr class="ltx_tr" id="S6.F18.fig1.1.5.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.F18.fig1.1.5.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.5.4.1.1" style="font-size:70%;">Control Logic + Registers</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S6.F18.fig1.1.5.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.5.4.2.1" style="font-size:70%;">3.2 mW / 73114 um²</span></td>
</tr>
<tr class="ltx_tr" id="S6.F18.fig1.1.6.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S6.F18.fig1.1.6.5.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.6.5.1.1" style="font-size:70%;">On Switch Buffer</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S6.F18.fig1.1.6.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.F18.fig1.1.6.5.2.1" style="font-size:70%;">15.2 mW / 2.38 mm²</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 18: </span><span class="ltx_text ltx_font_bold" id="S6.F18.fig1.4.1">Hardware overheads.</span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS3.SSS6.5.1.1">VI-C</span>6 </span>Page Management</h4>
<div class="ltx_para" id="S6.SS3.SSS6.p1">
<p class="ltx_p" id="S6.SS3.SSS6.p1.1">We collected the memory address ranges from the trace file and divided it into 4KB chunks as pages in the OS. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F13" title="Figure 13 ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">13</span></a> (a), the embedding migration (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS3" title="IV-B3 Embedding Spreading for Bandwidth Optimization ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span>3</span></a>) strategy shows optimal performance at a 35% migrate_threshold, reducing latency by 14% due to fewer page movements. Higher thresholds increase the embedding migration and degrade the performance (in fact, the migration cost increases from 1.67% to around 10% when we increase migrate_threshold from 10% to 50% using page block). Our cache-line granular migration approach (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS4" title="IV-B4 Optimization in Page Migration ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span>4</span></a>) outperforms standard OS page migration (page block) by up to 5.1<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS3.SSS6.p1.1.m1.1"><semantics id="S6.SS3.SSS6.p1.1.m1.1a"><mo id="S6.SS3.SSS6.p1.1.m1.1.1" xref="S6.SS3.SSS6.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS6.p1.1.m1.1b"><times id="S6.SS3.SSS6.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS6.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS6.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS6.p1.1.m1.1d">×</annotation></semantics></math>, which decrease the migration cost to less than 2%. We calculate the standard deviation (Std Dev) for access frequencies before and after the embedding migration (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F13" title="Figure 13 ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">13</span></a> (b)) to quantify the variability and assess the impact of 35% migrate_threshold. The standard deviation of access frequencies after the embedding migration drops from 20.6 to 7.8. This suggests that the PM effectively harmonizes access frequencies among the devices, leading to more uniform access frequency distributions and higher I/O parallelism. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F13" title="Figure 13 ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">13</span></a> (d), for page swapping strategy (§<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S4.SS2.SSS2" title="IV-B2 Global Hotness Detection ‣ IV-B Software Architecture ‣ IV System Design ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span>2</span></a>), the best performance was observed with a cold_age_threshold of 16% – leading to a 12% lower latency than TPP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib28" title="">28</a>]</cite>, which contributes to less page migration cost. The average migration cost decreases from around 8% to 1%. However, increasing the threshold further results in certain hot pages not being migrated to the local DRAM, consequently degrading the overall performance.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Hardware specifications.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S6.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T3.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1" style="font-size:70%;">Hardware</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T3.1.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.2.1" style="font-size:70%;">Spec</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.3.1" style="font-size:70%;">TDP</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.4.1" style="font-size:70%;">Price</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.2.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.2.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.2.1.1.1" style="font-size:70%;">Server CPU </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib59" title="">59</a><span class="ltx_text" id="S6.T3.1.2.1.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.2.1.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.2.1.2.1" style="font-size:70%;">AMD EPYC™ 9654 96C@2.4GHz</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" id="S6.T3.1.2.1.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.2.1.3.1" style="font-size:70%;">360W</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" id="S6.T3.1.2.1.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.2.1.4.1" style="font-size:70%;">$4,695</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.3.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.3.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.3.2.1.1" style="font-size:70%;">DIMM &amp; CXL mem </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.3.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib60" title="">60</a><span class="ltx_text" id="S6.T3.1.3.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.3.2.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.3.2.2.1" style="font-size:70%;">per GB, DDR4</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.3.2.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.3.2.3.1" style="font-size:70%;">21.6W (64GB)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.3.2.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.3.2.4.1" style="font-size:70%;">$4.90</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.4.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.4.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.4.3.1.1" style="font-size:70%;">DIMM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.4.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib60" title="">60</a><span class="ltx_text" id="S6.T3.1.4.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.4.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.4.3.2.1" style="font-size:70%;">per GB, DDR5</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.4.3.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.4.3.3.1" style="font-size:70%;">24W (64GB)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.4.3.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.4.3.4.1" style="font-size:70%;">$11.25</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.5.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.5.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.5.4.1.1" style="font-size:70%;">NIC </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.5.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib61" title="">61</a><span class="ltx_text" id="S6.T3.1.5.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.5.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.5.4.2.1" style="font-size:70%;">NVIDIA ConnectX-6@200Gbps IB</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.5.4.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.5.4.3.1" style="font-size:70%;">23.6W</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.5.4.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.5.4.4.1" style="font-size:70%;">$1,900</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.6.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.6.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.6.5.1.1" style="font-size:70%;">SWITCH </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.6.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib62" title="">62</a><span class="ltx_text" id="S6.T3.1.6.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.6.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.6.5.2.1" style="font-size:70%;">Juniper QFX10002-36Q @100Gbps</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.6.5.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.6.5.3.1" style="font-size:70%;">360W</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.6.5.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.6.5.4.1" style="font-size:70%;">$11,899</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.7.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.7.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.7.6.1.1" style="font-size:70%;">SWITCH + PUs </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.7.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib63" title="">63</a><span class="ltx_text" id="S6.T3.1.7.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.7.6.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.7.6.2.1" style="font-size:70%;">3.2Tbps, 2 pipelines (ASIC)</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.7.6.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.7.6.3.1" style="font-size:70%;">400W</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S6.T3.1.7.6.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.7.6.4.1" style="font-size:70%;">$13,039</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.8.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T3.1.8.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S6.T3.1.8.7.1.1" style="font-size:70%;">GPU </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.8.7.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib64" title="">64</a><span class="ltx_text" id="S6.T3.1.8.7.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T3.1.8.7.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.8.7.2.1" style="font-size:70%;">NVIDIA A100 80GB PCIe HBM2e</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" id="S6.T3.1.8.7.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.8.7.3.1" style="font-size:70%;">300W</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" id="S6.T3.1.8.7.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S6.T3.1.8.7.4.1" style="font-size:70%;">$18,900</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.T3.2"><span class="ltx_text ltx_font_italic" id="S6.T3.2.1" style="font-size:70%;">Note:<span class="ltx_text ltx_font_upright" id="S6.T3.2.1.1"> The prices shown here are subject to market fluctuations and may not accurately reflect the actual procurement prices.</span></span></p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Hardware Overheads</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.2">We compare the power consumptions and hardware overheads with DLRM-dedicated system RecNMP, and the traditional DRAM based solutions. Since the previous work use different fabrication processes and EDA platforms that enabling different post-design optimization strategies, we keep the same functions as the prior work describe and map them to our fabrication process. We use Synopsys Design Compiler (DC) with a 1GHz clock to estimate the area and power consumption values. We also use this information to calculate the total energy consumption using conventional 45nm technology. To evaluate the energy consumption of standalone DIMMs with CPUs, we use Cacti-3DD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib65" title="">65</a>]</cite> for memory devices and Cacti-IO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib66" title="">66</a>]</cite> for the off-chip input/output operations at the DIMM level.
In comparison to the prior solution that solely based on conventional DIMMs and CPU, PIFS-Rec reduces the energy consumption by 15.3% on average. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F18" title="Figure 18 ‣ VI-C5 On Switch Buffer Capacity ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">18</span></a>, PIFS-Rec reduces the power 2.7<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS4.p1.1.m1.1"><semantics id="S6.SS4.p1.1.m1.1a"><mo id="S6.SS4.p1.1.m1.1.1" xref="S6.SS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.m1.1b"><times id="S6.SS4.p1.1.m1.1.1.cmml" xref="S6.SS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.1.m1.1d">×</annotation></semantics></math> compared to RecNMPs. PIFS-Rec requires 2.02<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS4.p1.2.m2.1"><semantics id="S6.SS4.p1.2.m2.1a"><mo id="S6.SS4.p1.2.m2.1.1" xref="S6.SS4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.2.m2.1b"><times id="S6.SS4.p1.2.m2.1.1.cmml" xref="S6.SS4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.2.m2.1d">×</annotation></semantics></math> less area than an equivalent RecNMPs (x8) configuration with the same cache buffer.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS5.5.1.1">VI-E</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS5.6.2">Cost and Performance Analysis</span>
</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">We evaluate the performance of the parameter server over a simulation using one CPU-based server and a GPU server equipped with four A100 GPUs, obtaining power information using Nvidia-smi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib67" title="">67</a>]</cite>. We conservatively estimate – CXL memory’s power consumption is 90% of the local DRAM.</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.5"><span class="ltx_text ltx_font_bold" id="S6.SS5.p2.5.1">TCO Model:</span> We assess capital expenditure (CAPEX) (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.T3" title="TABLE III ‣ VI-C6 Page Management ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">III</span></a>) for RDMA hardware acquisition and operational expenditure (OPEX), including three years of power usage. Traditional setups involve a CPU in the GPU server along with NICs and a network switch. PIFS-Rec uses a CPU and fabric switch. We estimate the cost of a fabric switch considering the price of a standard network switch with an Intel Tofino core <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib63" title="">63</a>]</cite>. We get power costs from the network’s standalone consumption and DC analysis data. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F18" title="Figure 18 ‣ VI-C5 On Switch Buffer Capacity ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">18</span></a> demonstrates that PIFS-Rec offers superior TCO benefits compared to traditional GPU-based systems. For models with a few hundred parameters (RMC1), PIFS-Rec is 3.38<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS5.p2.1.m1.1"><semantics id="S6.SS5.p2.1.m1.1a"><mo id="S6.SS5.p2.1.m1.1.1" xref="S6.SS5.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.1.m1.1b"><times id="S6.SS5.p2.1.m1.1.1.cmml" xref="S6.SS5.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.1.m1.1d">×</annotation></semantics></math> more cost-effective. Even for the largest models (RMC4) utilizing one GPU, PIFS-Rec is 2.53<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS5.p2.2.m2.1"><semantics id="S6.SS5.p2.2.m2.1a"><mo id="S6.SS5.p2.2.m2.1.1" xref="S6.SS5.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.2.m2.1b"><times id="S6.SS5.p2.2.m2.1.1.cmml" xref="S6.SS5.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.2.m2.1d">×</annotation></semantics></math> cheaper. For instance, deploying RMC4 on a 2TB system with 64GB DIMMs requires $27,769 to build a PIFS-Rec system, whereas a parameter server with a single GPU costs $57,639. Assuming an energy cost of $0.05 per-KWh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#bib.bib68" title="">68</a>]</cite>, PIFS-Rec can save an additional $2,332.14 in OPEX over three years. In a traditional GPU system, memory cost increases with the model size, whereas in our system, TCO benefit converges to the cost-benefit of DIMM and CXL memory. For smaller models (RMC1), GPU provides better throughput (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16633v1#S6.F18" title="Figure 18 ‣ VI-C5 On Switch Buffer Capacity ‣ VI-C Evaluation ‣ VI Implementation and Evaluation ‣ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences"><span class="ltx_text ltx_ref_tag">18</span></a>). However, with a large memory footprint and vector size, when memory bandwidth on the parameter server becomes the bottleneck throughput drops. In contrast, PIFS-Rec demonstrates high robustness compared to parameter server-based solutions and outperforms a 4-GPU cluster by 1.6<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS5.p2.3.m3.1"><semantics id="S6.SS5.p2.3.m3.1a"><mo id="S6.SS5.p2.3.m3.1.1" xref="S6.SS5.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.3.m3.1b"><times id="S6.SS5.p2.3.m3.1.1.cmml" xref="S6.SS5.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.3.m3.1d">×</annotation></semantics></math>. To understand the margin gain, we calculate performance-per-watt (PPW). As the model size increases, the PIFS-Rec’s PPW improves from 1.22<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS5.p2.4.m4.1"><semantics id="S6.SS5.p2.4.m4.1a"><mo id="S6.SS5.p2.4.m4.1.1" xref="S6.SS5.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.4.m4.1b"><times id="S6.SS5.p2.4.m4.1.1.cmml" xref="S6.SS5.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.4.m4.1d">×</annotation></semantics></math> to 1.61<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS5.p2.5.m5.1"><semantics id="S6.SS5.p2.5.m5.1a"><mo id="S6.SS5.p2.5.m5.1.1" xref="S6.SS5.p2.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.5.m5.1b"><times id="S6.SS5.p2.5.m5.1.1.cmml" xref="S6.SS5.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.5.m5.1d">×</annotation></semantics></math>, compared to a 4-GPU conventional parameter server-based system.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Concluding Remarks</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Deep Learning Recommendation Models (DLRMs) consume extensive datacenter cycles and struggle with bandwidth due to large embeddings and growing parameters. This study introduces a Process-in-Fabric Switch (PIFS) strategy to enhance DLRM efficiency in CXL-based systems. By optimizing <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">both</span> hardware and software for large-scale DLRM workloads, our system PIFS-Rec notably surpasses existing solutions, achieving up to 3.89<math alttext="\times" class="ltx_Math" display="inline" id="S7.p1.1.m1.1"><semantics id="S7.p1.1.m1.1a"><mo id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><times id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S7.p1.1.m1.1d">×</annotation></semantics></math> greater efficiency compared to current systems and doubling that of comparable architectures.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank the anonymous reviewers for their useful feedback. This work was supported in part by gifts from AMD, INC and PRISM,
one of the seven centers in JUMP 2.0, a Semiconductor Research
Corporation (SRC) program sponsored by DARPA.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Deep learning recommendation model for personalization and recommendation systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">arXiv preprint arXiv:1906.00091</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, “An overview of machine learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Machine learning</em>, pp. 3–23, 1983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Science</em>, vol. 349, no. 6245, pp. 255–260, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Baltrušaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine learning: A survey and taxonomy,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 41, no. 2, pp. 423–443, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Desai and A. Shrivastava, “The trade-offs of model size in large recommendation models: 100gb to 10mb criteo-tb dlrm model,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 33 961–33 972, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H.-J. M. Shi, D. Mudigere, M. Naumov, and J. Yang, “Compositional embeddings using complementary partitions for memory-efficient recommendation systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, 2020, pp. 165–175.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. Ke, U. Gupta, B. Y. Cho, D. Brooks, V. Chandra, U. Diril, A. Firoozshahian, K. Hazelwood, B. Jia, H.-H. S. Lee <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Recnmp: Accelerating personalized recommendation with near-memory processing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>.   IEEE, 2020, pp. 790–803.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Jain, S. Cheng, V. Kalagi, V. Sanghavi, S. Kaul, M. Arunachalam, K. Maeng, A. Jog, A. Sivasubramaniam, M. T. Kandemir <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Optimizing cpu performance for recommendation systems at-scale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 2023, pp. 1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. Yang, H. Ma, Y. Zhao, F. Liu, Z. He, X. Sun, and L. Jiang, “Pimpr: Pim-based personalized recommendation with heterogeneous memory hierarchy,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</em>.   IEEE, 2023, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks, and G.-Y. Wei, “Recssd: near data processing for solid state drive based recommendation inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, 2021, pp. 717–729.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, “Rm-ssd: In-storage computing for large-scale recommendation inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>.   IEEE, 2022, pp. 1056–1070.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Kwon, Y. Lee, and M. Rhu, “Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</em>, 2019, pp. 740–753.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Hwang, T. Kim, Y. Kwon, and M. Rhu, “Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>.   IEEE, 2020, pp. 968–981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Naumov, D. Mudigere, H. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C. Wu, A. G. Azzolini, D. Dzhulgakov, A. Mallevich, I. Cherniavskii, Y. Lu, R. Krishnamoorthi, A. Yu, V. Kondratenko, S. Pereira, X. Chen, W. Chen, V. Rao, B. Jia, L. Xiong, and M. Smelyanskiy, “Deep learning recommendation model for personalization and recommendation systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, vol. abs/1906.00091, 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1906.00091" title="">https://arxiv.org/abs/1906.00091</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Guo, Y. Hao, C. Wu, P. Haghi, Z. Pan, M. Si, D. Tao, A. Li, M. Herbordt, and T. Geng, “Software-hardware co-design of heterogeneous smartnic system for recommendation models inference and training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 37th International Conference on Supercomputing</em>, 2023, pp. 336–347.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H. M. Shi, D. Mudigere, M. Naumov, and J. Yang, “Compositional embeddings using complementary partitions for memory-efficient recommendation systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">CoRR</em>, vol. abs/1909.02107, 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1909.02107" title="">https://arxiv.org/abs/1909.02107</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. Keeth and R. J. Baker, <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">DRAM circuit design: a tutorial</em>.   IEEE, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Shen, X. Liao, L. Zheng, Y. Huang, D. Chen, and H. Jin, “Archer: a reram-based accelerator for compressed recommendation systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Frontiers of Computer Science</em>, vol. 18, no. 5, p. 185607, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G. Singh, L. Chelini, S. Corda, A. J. Awan, S. Stuijk, R. Jordans, H. Corporaal, and A.-J. Boonstra, “A review of near-memory computing architectures: Opportunities and challenges,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">2018 21st Euromicro Conference on Digital System Design (DSD)</em>.   IEEE, 2018, pp. 608–617.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. Khan, S. Pasricha, and R. G. Kim, “A survey of resource management for processing-in-memory and near-memory processing architectures,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Journal of Low Power Electronics and Applications</em>, vol. 10, no. 4, p. 30, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. Kalamkar, E. Georganas, S. Srinivasan, J. Chen, M. Shiryaev, and A. Heinecke, “Optimizing deep learning recommender systems training on cpu cluster architectures,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em>.   IEEE, 2020, pp. 1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Wei, M. Lee, M. Langer, F. Yu, J. Liu, S. Liu, D. G. Abel, X. Guo, J. Dong <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">et al.</em>, “Merlin hugectr: Gpu-accelerated recommender system training and inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2">Proceedings of the 16th ACM Conference on Recommender Systems</em>, 2022, pp. 534–537.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Pumma and A. Vishnu, “Semantic-aware lossless data compression for deep learning recommendation model (dlrm),” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC)</em>.   IEEE, 2021, pp. 1–8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Yuan, J. Huang, Y. Sun, T. Wang, J. Nelson, D. R. Ports, Y. Wang, R. Wang, C. Tai, and N. S. Kim, “Rambda: Rdma-driven acceleration framework for memory-intensive <math alttext="\mu" class="ltx_Math" display="inline" id="bib.bib24.1.m1.1"><semantics id="bib.bib24.1.m1.1a"><mi id="bib.bib24.1.m1.1.1" xref="bib.bib24.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="bib.bib24.1.m1.1b"><ci id="bib.bib24.1.m1.1.1.cmml" xref="bib.bib24.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib24.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="bib.bib24.1.m1.1d">italic_μ</annotation></semantics></math>s-scale datacenter applications,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.1">2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>.   IEEE, 2023, pp. 499–515.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
“Compute Express Link (CXL),” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://computeexpresslink.org/" title="">https://computeexpresslink.org/</a>, accessed: 2023-03-14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Li, D. S. Berger, L. Hsu, D. Ernst, P. Zardoshti, S. Novakovic, M. Shah, S. Rajadnya, S. Lee, I. Agarwal <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Pond: Cxl-based memory pooling systems for cloud platforms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, 2023, pp. 574–587.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Sun, Y. Yuan, Z. Yu, R. Kuper, C. Song, J. Huang, H. Ji, S. Agarwal, J. Lou, I. Jeong <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “Demystifying cxl memory with genuine cxl-ready systems and devices,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture</em>, 2023, pp. 105–121.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal, P. Bhattacharya, C. Petersen, M. Chowdhury, S. Kanaujia, and P. Chauhan, “Tpp: Transparent page placement for cxl-enabled tiered-memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, 2023, pp. 742–755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X. Technologies, “World’s first cxl 2.0 and pcie gen5 switch ic,” 2023, accessed: 2023-03-14. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.xconn-tech.com/product" title="">https://www.xconn-tech.com/product</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. Park, H. Kim, K. Kim, J. So, J. Ahn, W. Lee, D. Kim, Y. Kim, J. Seok, J. Lee <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et al.</em>, “Scaling of memory performance and capacity with cxl memory expander.” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2">HCS</em>, 2022, pp. 1–27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
“Micron CXL,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.micron.com/products/memory/cxl-memory" title="">https://www.micron.com/products/memory/cxl-memory</a>, accessed: 2023-03-14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
“Samsung CXL,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://news.samsung.com/global/samsung-electronics-introduces-industrys-first-512gb-cxl-memory-module" title="">https://news.samsung.com/global/samsung-electronics-introduces-industrys-first-512gb-cxl-memory-module</a>, accessed: 2023-03-14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
W. Huangfu, K. T. Malladi, A. Chang, and Y. Xie, “Beacon: Scalable near-data-processing accelerators for genome analysis near memory pool with the cxl support,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>.   IEEE, 2022, pp. 727–743.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Jang, H. Choi, H. Bae, S. Lee, M. Kwon, and M. Jung, “<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib34.1.m1.1"><semantics id="bib.bib34.1.m1.1a"><mo id="bib.bib34.1.m1.1.1" stretchy="false" xref="bib.bib34.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib34.1.m1.1b"><ci id="bib.bib34.1.m1.1.1.cmml" xref="bib.bib34.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib34.1.m1.1d">{</annotation></semantics></math>CXL-ANNS<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib34.2.m2.1"><semantics id="bib.bib34.2.m2.1a"><mo id="bib.bib34.2.m2.1.1" stretchy="false" xref="bib.bib34.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib34.2.m2.1b"><ci id="bib.bib34.2.m2.1.1.cmml" xref="bib.bib34.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib34.2.m2.1d">}</annotation></semantics></math>:<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib34.3.m3.1"><semantics id="bib.bib34.3.m3.1a"><mo id="bib.bib34.3.m3.1.1" stretchy="false" xref="bib.bib34.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib34.3.m3.1b"><ci id="bib.bib34.3.m3.1.1.cmml" xref="bib.bib34.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib34.3.m3.1d">{</annotation></semantics></math>Software-Hardware<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib34.4.m4.1"><semantics id="bib.bib34.4.m4.1a"><mo id="bib.bib34.4.m4.1.1" stretchy="false" xref="bib.bib34.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib34.4.m4.1b"><ci id="bib.bib34.4.m4.1.1.cmml" xref="bib.bib34.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib34.4.m4.1d">}</annotation></semantics></math> collaborative memory disaggregation and computation for <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib34.5.m5.1"><semantics id="bib.bib34.5.m5.1a"><mo id="bib.bib34.5.m5.1.1" stretchy="false" xref="bib.bib34.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib34.5.m5.1b"><ci id="bib.bib34.5.m5.1.1.cmml" xref="bib.bib34.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.5.m5.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib34.5.m5.1d">{</annotation></semantics></math>Billion-Scale<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib34.6.m6.1"><semantics id="bib.bib34.6.m6.1a"><mo id="bib.bib34.6.m6.1.1" stretchy="false" xref="bib.bib34.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib34.6.m6.1b"><ci id="bib.bib34.6.m6.1.1.cmml" xref="bib.bib34.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.6.m6.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib34.6.m6.1d">}</annotation></semantics></math> approximate nearest neighbor search,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.7.1">2023 USENIX Annual Technical Conference (USENIX ATC 23)</em>, 2023, pp. 585–600.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Daghaghi, N. Meisburger, M. Zhao, and A. Shrivastava, “Accelerating slide deep learning on modern cpus: Vectorization, quantizations, memory optimizations, and more,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of Machine Learning and Systems</em>, vol. 3, pp. 156–166, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “The architectural implications of facebook’s dnn-based personalized recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</em>.   IEEE, 2020, pp. 488–501.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Firoozshahian, J. Coburn, R. Levenstein, R. Nattoji, A. Kamath, O. Wu, G. Grewal, H. Aepala, B. Jakka, B. Dreyer <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">et al.</em>, “Mtia: First generation silicon targeting meta’s recommendation systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.2.2">Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 2023, pp. 1–13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.-H. S. Lee, D. Brooks, and C.-J. Wu, “Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>.   IEEE, 2020, pp. 982–995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C. Yin, B. Acun, C.-J. Wu, and X. Liu, “Tt-rec: Tensor train compression for deep learning recommendation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of Machine Learning and Systems</em>, vol. 3, pp. 448–462, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
G. Sethi, B. Acun, N. Agarwal, C. Kozyrakis, C. Trippel, and C.-J. Wu, “Recshard: statistical feature-based memory optimization for industry-scale neural recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, 2022, pp. 344–358.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
H. Zhang, Z. Liu, B. Chen, Y. Zhao, T. Zhao, T. Yang, and B. Cui, “Cafe: Towards compact, adaptive, and fast embedding for large-scale recommendation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the ACM on Management of Data</em>, vol. 2, no. 1, pp. 1–28, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
L. Ke, U. Gupta, M. Hempstead, C.-J. Wu, H.-H. S. Lee, and X. Zhang, “Hercules: Heterogeneity-aware inference serving for at-scale personalized recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>.   IEEE, 2022, pp. 141–154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
L. Ke, X. Zhang, B. Lee, G. E. Suh, and H.-H. S. Lee, “Disaggrec: Architecting disaggregated systems for large-scale personalized recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2212.00939</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Wang, S. Li, Q. Zheng, A. Chang, H. Li, and Y. Chen, “Ems-i: An efficient memory system design with specialized caching mechanism for recommendation inference,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ACM Transactions on Embedded Computing Systems</em>, vol. 22, no. 5s, pp. 1–22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
I. Calciu, M. T. Imran, I. Puddu, S. Kashyap, H. A. Maruf, O. Mutlu, and A. Kolli, “Rethinking software runtimes for disaggregated memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ASPLOS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
V. Addanki, C. Avin, and S. Schmid, “Mars: Near-optimal throughput with shallow buffers in reconfigurable datacenter networks,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
V. Addanki, O. Michel, and S. Schmid, “<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib47.1.m1.1"><semantics id="bib.bib47.1.m1.1a"><mo id="bib.bib47.1.m1.1.1" stretchy="false" xref="bib.bib47.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib47.1.m1.1b"><ci id="bib.bib47.1.m1.1.1.cmml" xref="bib.bib47.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib47.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib47.1.m1.1d">{</annotation></semantics></math>PowerTCP<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib47.2.m2.1"><semantics id="bib.bib47.2.m2.1a"><mo id="bib.bib47.2.m2.1.1" stretchy="false" xref="bib.bib47.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib47.2.m2.1b"><ci id="bib.bib47.2.m2.1.1.cmml" xref="bib.bib47.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib47.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib47.2.m2.1d">}</annotation></semantics></math>: Pushing the performance limits of datacenter networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">19th USENIX symposium on networked systems design and implementation (NSDI 22)</em>, 2022, pp. 51–70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
M. Apostolaki, V. Addanki, M. Ghobadi, and L. Vanbever, “Fb: A flexible buffer management scheme for data center switches,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Y. Yu, X. Jiang, G. Jin, Z. Gao, and P. Li, “A buffer management algorithm based on dynamic marking threshold to restrain microburst in data center network,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Information</em>, vol. 12, no. 9, p. 369, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
D. Dillow, G. M. Shipman, S. H. Oral, and Z. Zhang, “I/o congestion avoidance via routing and object placement,” 2011. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:59774578" title="">https://api.semanticscholar.org/CorpusID:59774578</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
X. Liao, Z. Yang, and J. Shu, “Rio: Order-preserving and cpu-efficient remote storage access,” in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the Eighteenth European Conference on Computer Systems</em>, 2023, pp. 703–717.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
K. Kim, S. Kim, and T. Kim, “Hmb-i/o: Fast track for handling urgent i/os in nonvolatile memory express solid-state drives,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Applied Sciences</em>, vol. 10, no. 12, 2020. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2076-3417/10/12/4341" title="">https://www.mdpi.com/2076-3417/10/12/4341</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
J. Ren, J. Luo, K. Wu, M. Zhang, H. Jeon, and D. Li, “Sentinel: Efficient tensor migration and allocation on heterogeneous memory systems for deep learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>.   IEEE, 2021, pp. 598–611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. Choudhary, M. C. Govil, G. Singh, L. K. Awasthi, E. S. Pilli, and D. Kapil, “A critical survey of live virtual machine migration techniques,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Journal of Cloud Computing</em>, vol. 6, pp. 1–41, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
M. Inc., “Mongodb,” 2024, accessed: 2024-06-23. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mongodb.com/" title="">https://www.mongodb.com/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
J. Dean and S. Ghemawat, “Mapreduce: simplified data processing on large clusters,” <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Communications of the ACM</em>, vol. 51, no. 1, pp. 107–113, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
H. Luo, Y. C. Tuğrul, F. N. Bostancı, A. Olgun, A. G. Yağlıkçı, , and O. Mutlu, “Ramulator 2.0: A Modern, Modular, and Extensible DRAM Simulator,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
M. A. Research, “Meta inc. trace,” 2023, accessed: 2023-03-14. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/dlrm_datasets" title="">https://github.com/facebookresearch/dlrm_datasets</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
AMD, “Amd epyc 9654x 2.4 ghz processor oem,” 2024, accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amd.com/en/products/processors/server/epyc/4th-generation-9004-and-8004-series.html#specs" title="">https://www.amd.com/en/products/processors/server/epyc/4th-generation-9004-and-8004-series.html#specs</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
MemVerge, “Cxl use case: Slash memory costs and expand capacity,” 2024, accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://memverge.com/cxl-use-case-slash-memory-costs-and-expand-capacity/" title="">https://memverge.com/cxl-use-case-slash-memory-costs-and-expand-capacity/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J. Networks, “Juniper qfx10002,” 2024, accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.juniper.net/documentation/us/en/hardware/qfx10002/topics/topic-map/qfx10002-port-panel.html" title="">https://www.juniper.net/documentation/us/en/hardware/qfx10002/topics/topic-map/qfx10002-port-panel.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
NVIDIA, “Nvidia connectx-6 vpi adapter card hdr 200gbe,” 2024, accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://store.nvidia.com/en-us/networking/store/product/mcx653106a-hdat-sp/nvidia-connectx-6-vpi-adapter-card-hdr-200gbe/" title="">https://store.nvidia.com/en-us/networking/store/product/mcx653106a-hdat-sp/nvidia-connectx-6-vpi-adapter-card-hdr-200gbe/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Intel, “Intel tofino intelligent fabric processors,” 2024, accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.intel.com/content/www/us/en/products/details/network-io/intelligent-fabric-processors/tofino.html" title="">https://www.intel.com/content/www/us/en/products/details/network-io/intelligent-fabric-processors/tofino.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
NVIDIA, “Nvidia a100 80gb datasheet,” 2024, accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf" title="">https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
K. Chen, S. Li, N. Muralimanohar, J. H. Ahn, J. B. Brockman, and N. P. Jouppi, “Cacti-3dd: Architecture-level modeling for 3d die-stacked dram main memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">2012 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</em>.   IEEE, 2012, pp. 33–38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
N. P. Jouppi, A. B. Kahng, N. Muralimanohar, and V. Srinivas, “Cacti-io: Cacti with off-chip power-area-timing models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the International Conference on Computer-Aided Design</em>, 2012, pp. 294–301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Nvidia-SMI, “System Management Interface SMI,” 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/system-management-interface" title="">https://developer.nvidia.com/system-management-interface</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
BLS Strategies, “Power requirements, energy costs, and incentives for data centers,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.blsstrategies.com/insights-press/power-requirements-energy-costs-and-incentives-for-data-centers" title="">https://www.blsstrategies.com/insights-press/power-requirements-energy-costs-and-incentives-for-data-centers</a>, accessed: 2024-06-26.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 05:16:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
