<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation</title>
<!--Generated on Fri May 24 17:23:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.19267v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S1" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2.SS1" title="In 2. Related Works ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Named Entities in Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2.SS2" title="In 2. Related Works ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Selection for Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S3" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Existing Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S3.SS1" title="In 3. Existing Methods ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>EL2N</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S3.SS2" title="In 3. Existing Methods ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Entropy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S3.SS3" title="In 3. Existing Methods ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Selfsup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S3.SS4" title="In 3. Existing Methods ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Reference-free COMET</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S4" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Proposed Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.SS1" title="In 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Settings for Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.SS2" title="In 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.SS3" title="In 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Experiments for Generalizability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.SS4" title="In 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Additional Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S6" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S7" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S8" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Bibliographical References</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S9" title="In Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Language Resource References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Robust Guidance for Unsupervised Data Selection:
<br class="ltx_break"/>Capturing Perplexing Named Entities
<br class="ltx_break"/>for Domain-Specific Machine Translation</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Low-resourced data presents a significant challenge for neural machine translation. In most cases, the low-resourced environment is caused by high costs due to the need for domain experts or the lack of language experts. Therefore, identifying the most training-efficient data within an unsupervised setting emerges as a practical strategy. Recent research suggests that such effective data can be identified by selecting ’appropriately complex data’ based on its volume, providing strong intuition for unsupervised data selection. However, we have discovered that establishing criteria for unsupervised data selection remains a challenge, as the ’appropriate level of difficulty’ may vary depending on the data domain. We introduce a novel unsupervised data selection method named ’Capturing Perplexing Named Entities,’ which leverages the maximum inference entropy in translated named entities as a metric for selection. When tested with the ’Korean-English Parallel Corpus of Specialized Domains,’ our method served as robust guidance for identifying training-efficient data across different domains, in contrast to existing methods.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id7.id1.1">Keywords: </span>Machine Translation, Data Selection, Unsupervised Method</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1"></span></p>
</div>
<div class="ltx_logical-block" id="id6">
<div class="ltx_para" id="id6.p1">
<p class="ltx_p ltx_align_center" id="id6.p1.1"><span class="ltx_text ltx_font_bold" id="id6.p1.1.1" style="font-size:144%;">Robust Guidance for Unsupervised Data Selection:
<br class="ltx_break"/>Capturing Perplexing Named Entities
<br class="ltx_break"/>for Domain-Specific Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<table class="ltx_tabular ltx_centering ltx_align_top" id="id5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id3.3.3">
<td class="ltx_td ltx_align_center" id="id3.3.3.3"><span class="ltx_text ltx_font_bold" id="id3.3.3.3.3" style="font-size:120%;">Seunghyun Ji<sup class="ltx_sup" id="id3.3.3.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id3.3.3.3.3.1.1">12</span></sup>, Hagai Raja Sinulingga<sup class="ltx_sup" id="id3.3.3.3.3.2"><span class="ltx_text ltx_font_medium" id="id3.3.3.3.3.2.1">2</span></sup>, Darongsae Kwon<sup class="ltx_sup" id="id3.3.3.3.3.3"><span class="ltx_text ltx_font_medium" id="id3.3.3.3.3.3.1">2</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="id5.5.5">
<td class="ltx_td ltx_align_center" id="id5.5.5.2">
<sup class="ltx_sup" id="id5.5.5.2.1">1</sup>Ahancompany corporation, <sup class="ltx_sup" id="id5.5.5.2.2">2</sup>TelePIX</td>
</tr>
<tr class="ltx_tr" id="id5.5.6.1">
<td class="ltx_td ltx_align_center" id="id5.5.6.1.1">seunghyun.ji@a-ha.io, {hagairaja, darong.kwon}@telepix.net</td>
</tr>
</tbody>
</table>
<p class="ltx_p ltx_align_center" id="id6.p1.2"><span class="ltx_text ltx_font_italic" id="id6.p1.2.1">Abstract content</span></p>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>This work was initially started in TelePIX, the previous affiliation of the first author.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>The code is available in the following hyperlink : <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/comchobo/Capturing-Perplexing-Named-Entities" title="">https://github.com/comchobo/Capturing-Perplexing-Named-Entities</a></span></span></span>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the advent of large-scale models capable of translating numerous languages in various directions<cite class="ltx_cite ltx_citemacro_citep">(Aharoni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib2" title="">2019</a>)</cite>, the field of machine translation is entering a new era. For instance, ’No Language Left Behind<cite class="ltx_cite ltx_citemacro_citep">(NLLB Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib21" title="">2022</a>)</cite>’, which demonstrated outstanding performance across a range of languages, was trained on over 40,000 combinations of 200 languages. These models can be regarded as pre-trained or foundational, as they have acquired general knowledge for translation. Nevertheless, they might sometimes face challenges when translating domain-specific data, despite their extensive training on diverse datasets. To address this, fine-tuning the pre-trained models with target domain data can enhance their specialization<cite class="ltx_cite ltx_citemacro_cite">Fadaee and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib7" title="">2018</a>); Zan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib38" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S1.F1.1.g1" src="extracted/2402.19267v2/figures/diagram_F.png" width="565"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A diagram illustrates our method, which utilizes a pre-trained multilingual model for machine translation and a named entity recognition model that has been fine-tuned on the target language. Our method comprises three steps: 1) capturing named entity tokens in the machine-translated sentences, 2) calculating the inference entropy of those tokens, and 3) using the maximum entropy value as a measure for selection.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, when addressing narrow or specialized domains, the model must recognize words that are relatively rare in general corpora. This presents a challenge, as rare words often consist of sparse tokens, such as those composed of single character tokens. Named entities, such as names of persons, organizations, etc., frequently lack synonyms, making it even more perplexing to build contextualized representations, especially in narrow domains. This also underscores the point that acquiring domain-specific translation data is costly, as translators are required who possess not only domain expertise but also familiarity with domain-specific terminology.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To reduce data acquisition costs, one might consider strategically identifying data for labeling rather than making random selections. Several researchers<cite class="ltx_cite ltx_citemacro_citep">(Paul et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib22" title="">2021</a>; Feldman and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib8" title="">2020</a>; Sorscher et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite> have suggested various measurement methods aimed at selecting ’effective’ data for training. Some of those focus on ’Data difficulty,’<cite class="ltx_cite ltx_citemacro_citep">(Paul et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib22" title="">2021</a>; Meding et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib18" title="">2022</a>)</cite> identifying data that poses a challenge to a given model. ’Data forgettability<cite class="ltx_cite ltx_citemacro_citep">(Toneva et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib34" title="">2019</a>)</cite>’ or ’Memorization<cite class="ltx_cite ltx_citemacro_citep">(Feldman and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib8" title="">2020</a>)</cite>’ could serve as alternative criterion. However, these methods require a supervised setting for selection, which may be inefficient for machine translation. For instance, pruning a dataset is unlikely to yield a better model if the dataset was curated by domain experts <cite class="ltx_cite ltx_citemacro_citep">(Maillard et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib17" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In an unsupervised setting, where training-efficiency should be guessed without a label, <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite> demonstrated that the Euclidean distance between a data point’s representation and its cluster centroid can serve as an effective criterion for data selection. This approach is supported by several concrete theoretical analyses and provides straightforward guidance for data selection. However, it remains uncertain whether this criterion can be universally applied to parameter-efficient fine-tuning methods<cite class="ltx_cite ltx_citemacro_citep">(Houlsby et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib11" title="">2019</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib12" title="">2022</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib16" title="">2022</a>)</cite>, which are commonly used. We observed that this measurement method might not always align with training-efficiency, indicating that it may not consistently correlate with performance improvement, despite using the same pre-trained weights and dataset size. These findings are detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.SS2" title="5.2. Main Results ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We propose a novel method for unsupervised data selection, which we refer to as ’Capturing Perplexing Named Entities’. Our method identifies data that should be selected, by assessing the perplexity of named entity tokens translated by a given pre-trained model, as described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. The motivations behind this approach are as follow:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Since named entities in domain-specific data are challenging to translate without recognizing the complex patterns within the domain, they represent one of the most difficult portions to translate. Therefore, these entities should be given priority for efficient domain adaptation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The entropy score of a vocabulary distribution can indicate the model’s level of perplexity. Given that synonyms for named entities are unlikely to exist, the model should not exhibit a high entropy score for named entities.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In several experiments targeting domain-specific ’Korean to English’ translation, our method consistently identified the most training-efficient data. This indicates that our measurement method has a stronger correlation with performance improvement compared to existing methods, which can vary significantly across different data domains. For clarity in our discussion, ’MDS’ will serve as the abbreviation for Measurement method for Data Selection, and ’Value by MDS’ will denote the specific value it calculates.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Works</h2>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.1">Languages</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1">Data Examples</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3">Scores</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.2.1.1.1">Korean</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1"><span class="ltx_ERROR undefined" id="S2.T1.1.2.1.2.1.1">{CJK}</span>
<span class="ltx_p" id="S2.T1.1.2.1.2.1.2">UTF8mj
<span class="ltx_text" id="S2.T1.1.2.1.2.1.2.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S2.T1.1.2.1.2.1.2.1.1" style="width:327.2pt;">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.2.1.1.1">메이저리그 자유계약선수(FA) 최대어 투수 중 한 명인 스티븐 스트라스버그가 원 소속팀 워싱턴과 7년 2억4,500만달러에 도장을 찍었다.</span>
</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3"><span class="ltx_text" id="S2.T1.1.2.1.3.1">COMET</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.1" style="padding-bottom:15.00002pt;"><span class="ltx_text" id="S2.T1.1.3.2.1.1">90.92</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.3.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.4.3.1.1">English</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S2.T1.1.4.3.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_p" id="S2.T1.1.4.3.2.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S2.T1.1.4.3.2.1.1.1" style="width:327.2pt;">
<span class="ltx_p" id="S2.T1.1.4.3.2.1.1.1.1"><span class="ltx_text" id="S2.T1.1.4.3.2.1.1.1.1.1" style="color:#FF0000;">Steven Strasburg</span>, one of the biggest <span class="ltx_text" id="S2.T1.1.4.3.2.1.1.1.1.2" style="color:#FF0000;">free agent (FA) pitchers</span> in Major League Baseball, has signed a 7-year, $ 245 million contracts with his <span class="ltx_text" id="S2.T1.1.4.3.2.1.1.1.1.3" style="color:#FF0000;">original</span> team Washington.</span>
</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.3"><span class="ltx_text" id="S2.T1.1.4.3.3.1">ChrF++</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.1" style="padding-bottom:15.00002pt;"><span class="ltx_text" id="S2.T1.1.5.4.1.1">67.94</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.6.5.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.6.5.1.1">Translated</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S2.T1.1.6.5.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.2.1">
<span class="ltx_p" id="S2.T1.1.6.5.2.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S2.T1.1.6.5.2.1.1.1" style="width:327.2pt;">
<span class="ltx_p" id="S2.T1.1.6.5.2.1.1.1.1"><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.1.1.1" style="color:#FF0000;">Steven Strasberg</span>, one of the biggest pitchers in the Major League <span class="ltx_text" id="S2.T1.1.6.5.2.1.1.1.1.2" style="color:#FF0000;">Free Agent (FA) league</span>, signed a seven-year, $ 245 million contract with <span class="ltx_text" id="S2.T1.1.6.5.2.1.1.1.1.3" style="color:#FF0000;">former</span> team Washington.</span>
</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.3"><span class="ltx_text" id="S2.T1.1.6.5.3.1">BLEU</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.1" style="padding-bottom:20.00003pt;"><span class="ltx_text" id="S2.T1.1.7.6.1.1">27.38</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.7.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.8.7.1.1">Korean</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.1.8.7.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.2.1"><span class="ltx_ERROR undefined" id="S2.T1.1.8.7.2.1.1">{CJK}</span>
<span class="ltx_p" id="S2.T1.1.8.7.2.1.2">UTF8mj
<span class="ltx_text" id="S2.T1.1.8.7.2.1.2.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S2.T1.1.8.7.2.1.2.1.1" style="width:327.2pt;">
<span class="ltx_p" id="S2.T1.1.8.7.2.1.2.1.1.1">고메스 부상 이후 에버턴 지휘봉을 잡게된 카를로 안첼로티 감독은 지난주 "고메스의 회복이 순조롭게 이뤄지고 있다"고 밝혔다.</span>
</span></span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.8.7.3">COMET</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.8.1">90.99</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.10.9.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.10.9.1.1">English</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S2.T1.1.10.9.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.10.9.2.1">
<span class="ltx_p" id="S2.T1.1.10.9.2.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S2.T1.1.10.9.2.1.1.1" style="width:327.2pt;">
<span class="ltx_p" id="S2.T1.1.10.9.2.1.1.1.1"><span class="ltx_text" id="S2.T1.1.10.9.2.1.1.1.1.1" style="color:#FF0000;">Manager</span> Carlo Ancelotti, who took the helm of Everton after <span class="ltx_text" id="S2.T1.1.10.9.2.1.1.1.1.2" style="color:#FF0000;">Gomez’s</span> injury, revealed last week that <span class="ltx_text" id="S2.T1.1.10.9.2.1.1.1.1.3" style="color:#FF0000;">"Gomez’s</span> recovery is going smoothly."</span>
</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.9.3">ChrF++</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.10">
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.10.1">64.47</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.12.11.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.12.11.1.1">Translated</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r" id="S2.T1.1.12.11.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.11.2.1">
<span class="ltx_p" id="S2.T1.1.12.11.2.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S2.T1.1.12.11.2.1.1.1" style="width:327.2pt;">
<span class="ltx_p" id="S2.T1.1.12.11.2.1.1.1.1"><span class="ltx_text" id="S2.T1.1.12.11.2.1.1.1.1.1" style="color:#FF0000;">Coach</span> Carlo Ancelotti, who took over Everton after <span class="ltx_text" id="S2.T1.1.12.11.2.1.1.1.1.2" style="color:#FF0000;">Gomes’</span> injury, said last week, <span class="ltx_text" id="S2.T1.1.12.11.2.1.1.1.1.3" style="color:#FF0000;">"Gomes’</span> recovery is progressing smoothly."</span>
</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.11.3">BLEU</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.13.12.1">24.86</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Example pairs with high COMET and ChrF++ scores but low BLEU scores were selected from sports domain data. The first column represents the source (Korean), the target (English), and the machine-translated (Korean to English) result. Words that may cause critical semantic distortions are highlighted in red. The last column lists the evaluation scores of the machine-translated sentences, calculated using three different metrics.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.1.   Named Entities in Machine Translation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Translating named entities presents a significant challenge in machine translation<cite class="ltx_cite ltx_citemacro_citep">(Ugawa et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib35" title="">2018</a>)</cite>, although it is crucial for delivering accurate information<cite class="ltx_cite ltx_citemacro_citep">(Tjong Kim Sang and
De Meulder, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib33" title="">2003</a>)</cite>. Incorrect translations of named entities, even with few errors, can lead to information distortion. For instance, in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2.T1" title="Table 1 ‣ 2. Related Works ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, the human-translated and machine-translated Korean to English-sentences may seem similar. However, a closer examination reveals differences in the individual’s name (Steven Strasburg), the league (Major League Baseball), and an adjective (original). Despite these mistakes causing critical distortions, recent metrics such as COMET<cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib29" title="">2020</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We used <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Unbabel/wmt22-comet-da" title="">https://huggingface.co/Unbabel/wmt22-comet-da</a> to evaluate using COMET score.</span></span></span> and ChrF++<cite class="ltx_cite ltx_citemacro_citep">(Popović, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib24" title="">2015</a>)</cite> show scores high enough to be interpreted as satisfactory results. Given that some rare named entities are more common in domain-specific data, building precise contextualized representations of data, which contains named entities, is even difficult to capture by recent deep-model based metrics.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">One current approach to translate named entities precisely, integrates a knowledge base<cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib40" title="">2020</a>)</cite> or employs a transliteration model once tokens are identified as named entities<cite class="ltx_cite ltx_citemacro_citep">(Sharma et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib30" title="">2023</a>)</cite>. However, these strategies often rely on specialized algorithms that act as a workaround, rather than directly boosting the translation model’s performance or robustness. Multi-task learning has demonstrated improvements in translation performance when additional annotations for named entities are provided<cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib37" title="">2022</a>)</cite>. However, this method may incur significantly higher labeling costs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.2.   Data Selection for Training</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Throughout several training cycles, metrics such as forgetting scores<cite class="ltx_cite ltx_citemacro_citep">(Toneva et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib34" title="">2019</a>)</cite>, memorization<cite class="ltx_cite ltx_citemacro_citep">(Feldman and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib8" title="">2020</a>)</cite>, diverse ensembles<cite class="ltx_cite ltx_citemacro_citep">(Meding et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib18" title="">2022</a>)</cite>, and normed gradients<cite class="ltx_cite ltx_citemacro_citep">(Paul et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib22" title="">2021</a>)</cite> could be used as one of the measurement methods for data selection (MDS). EL2N, which quantifies the error magnitude, acts as a training-free MDS. However, these methods require annotations, limiting their application to supervised settings only. As high-quality data has been shown to significantly outperform large volumes of low-quality or synthetic data<cite class="ltx_cite ltx_citemacro_citep">(Maillard et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib17" title="">2023</a>)</cite>, it is generally recommended that the data with elaborate annotations should not be pruned.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In an unsupervised setting, one might explore data uniqueness—for example, by measuring the Euclidean distance between a data representation and its centroid<cite class="ltx_cite ltx_citemacro_citep">(Sorscher et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite> (referred to as Selfsup)—as a form of unsupervised MDS. Measuring uncertainty, which could be estimated by the entropy of the probability distribution, also might be one of MDS<cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib5" title="">1990</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib36" title="">2021</a>)</cite>. However, empirical evidence suggests that when training with small datasets, excessively unique data (indicated by high values in MDS Selfsup) may impede training<cite class="ltx_cite ltx_citemacro_citep">(Sorscher et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite>. Therefore, selecting data using the appropriate type of MDS and determining the optimal value for MDS are crucial. Nonetheless, establishing a standard for this is challenging, to the best of our knowledge.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In machine translation, reference-free Quality Estimation (QE) methods, which operate as an unsupervised MDS, are gaining focus. One strategy involves the intuition of ’seeking perplexing data’ by identifying attention distractions or uncertainties<cite class="ltx_cite ltx_citemacro_citep">(Peris and Casacuberta, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib23" title="">2018</a>)</cite>. More sophisticated reference-free QE algorithms, which can be implemented using deep models<cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib28" title="">2021</a>)</cite>, have demonstrated competitive results when compared to their reference-requiring counterparts<cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib29" title="">2020</a>)</cite>. However, these methods, relying on sentence embedding models, are often confounded by even slight literal differences. We have observed and discussed this phenomenon in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2.SS1" title="2.1. Named Entities in Machine Translation ‣ 2. Related Works ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">2.1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Existing Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We consider the multilingual translation model as a ’pre-trained model’, with subsequent training on specific data referred to as ’fine-tuning’.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   EL2N</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2"><cite class="ltx_cite ltx_citemacro_citet">Paul et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib22" title="">2021</a>)</cite> previously used the average error from several minimally trained models to identify data that could not be easily trained in a few epochs. This method requires paired data for its computations, hence categorized as a supervised approach. Intuitively, the EL2N value from a pre-trained model signifies an average error or incorrect confidence, enabling the identification of the most problematic data for a given model. If <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝒀</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝒀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\bm{Y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">bold_italic_Y</annotation></semantics></math> and <math alttext="\bm{\hat{Y}}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mover accent="true" id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S3.SS1.p1.2.m2.1.1.1" mathvariant="bold" xref="S3.SS1.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><ci id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1">bold-^</ci><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝒀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\bm{\hat{Y}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">overbold_^ start_ARG bold_italic_Y end_ARG</annotation></semantics></math> represent the original and translated sentences in the target language, respectively, EL2N can be described as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.2">
<p class="ltx_p ltx_align_center" id="S3.SS1.2.2">EL2N<math alttext="(\bm{Y},\bm{\hat{Y}})=\frac{1}{L}\sum_{l=1}^{L}\|\bm{y}_{l}-\hat{\bm{y}_{l}}\|" class="ltx_Math" display="inline" id="S3.SS1.1.1.m1.3"><semantics id="S3.SS1.1.1.m1.3a"><mrow id="S3.SS1.1.1.m1.3.3" xref="S3.SS1.1.1.m1.3.3.cmml"><mrow id="S3.SS1.1.1.m1.3.3.3.2" xref="S3.SS1.1.1.m1.3.3.3.1.cmml"><mo id="S3.SS1.1.1.m1.3.3.3.2.1" stretchy="false" xref="S3.SS1.1.1.m1.3.3.3.1.cmml">(</mo><mi id="S3.SS1.1.1.m1.1.1" xref="S3.SS1.1.1.m1.1.1.cmml">𝒀</mi><mo id="S3.SS1.1.1.m1.3.3.3.2.2" xref="S3.SS1.1.1.m1.3.3.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.1.1.m1.2.2" xref="S3.SS1.1.1.m1.2.2.cmml"><mi id="S3.SS1.1.1.m1.2.2.2" xref="S3.SS1.1.1.m1.2.2.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S3.SS1.1.1.m1.2.2.1" mathvariant="bold" xref="S3.SS1.1.1.m1.2.2.1.cmml">^</mo></mover><mo id="S3.SS1.1.1.m1.3.3.3.2.3" stretchy="false" xref="S3.SS1.1.1.m1.3.3.3.1.cmml">)</mo></mrow><mo id="S3.SS1.1.1.m1.3.3.2" xref="S3.SS1.1.1.m1.3.3.2.cmml">=</mo><mrow id="S3.SS1.1.1.m1.3.3.1" xref="S3.SS1.1.1.m1.3.3.1.cmml"><mfrac id="S3.SS1.1.1.m1.3.3.1.3" xref="S3.SS1.1.1.m1.3.3.1.3.cmml"><mn id="S3.SS1.1.1.m1.3.3.1.3.2" xref="S3.SS1.1.1.m1.3.3.1.3.2.cmml">1</mn><mi id="S3.SS1.1.1.m1.3.3.1.3.3" xref="S3.SS1.1.1.m1.3.3.1.3.3.cmml">L</mi></mfrac><mo id="S3.SS1.1.1.m1.3.3.1.2" xref="S3.SS1.1.1.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.SS1.1.1.m1.3.3.1.1" xref="S3.SS1.1.1.m1.3.3.1.1.cmml"><msubsup id="S3.SS1.1.1.m1.3.3.1.1.2" xref="S3.SS1.1.1.m1.3.3.1.1.2.cmml"><mo id="S3.SS1.1.1.m1.3.3.1.1.2.2.2" rspace="0em" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.2.cmml">∑</mo><mrow id="S3.SS1.1.1.m1.3.3.1.1.2.2.3" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.cmml"><mi id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.2" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.2.cmml">l</mi><mo id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.1" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.1.cmml">=</mo><mn id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.3" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.1.1.m1.3.3.1.1.2.3" xref="S3.SS1.1.1.m1.3.3.1.1.2.3.cmml">L</mi></msubsup><mrow id="S3.SS1.1.1.m1.3.3.1.1.1.1" xref="S3.SS1.1.1.m1.3.3.1.1.1.2.cmml"><mo id="S3.SS1.1.1.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.SS1.1.1.m1.3.3.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS1.1.1.m1.3.3.1.1.1.1.1" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.cmml"><msub id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.2" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.3" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.1" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.cmml"><msub id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.cmml"><mi id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.2" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.2.cmml">𝒚</mi><mi id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.3" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.3.cmml">l</mi></msub><mo id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.1" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S3.SS1.1.1.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.SS1.1.1.m1.3.3.1.1.1.2.1.cmml">‖</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.1.1.m1.3b"><apply id="S3.SS1.1.1.m1.3.3.cmml" xref="S3.SS1.1.1.m1.3.3"><eq id="S3.SS1.1.1.m1.3.3.2.cmml" xref="S3.SS1.1.1.m1.3.3.2"></eq><interval closure="open" id="S3.SS1.1.1.m1.3.3.3.1.cmml" xref="S3.SS1.1.1.m1.3.3.3.2"><ci id="S3.SS1.1.1.m1.1.1.cmml" xref="S3.SS1.1.1.m1.1.1">𝒀</ci><apply id="S3.SS1.1.1.m1.2.2.cmml" xref="S3.SS1.1.1.m1.2.2"><ci id="S3.SS1.1.1.m1.2.2.1.cmml" xref="S3.SS1.1.1.m1.2.2.1">bold-^</ci><ci id="S3.SS1.1.1.m1.2.2.2.cmml" xref="S3.SS1.1.1.m1.2.2.2">𝒀</ci></apply></interval><apply id="S3.SS1.1.1.m1.3.3.1.cmml" xref="S3.SS1.1.1.m1.3.3.1"><times id="S3.SS1.1.1.m1.3.3.1.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.2"></times><apply id="S3.SS1.1.1.m1.3.3.1.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.3"><divide id="S3.SS1.1.1.m1.3.3.1.3.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.3"></divide><cn id="S3.SS1.1.1.m1.3.3.1.3.2.cmml" type="integer" xref="S3.SS1.1.1.m1.3.3.1.3.2">1</cn><ci id="S3.SS1.1.1.m1.3.3.1.3.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.3.3">𝐿</ci></apply><apply id="S3.SS1.1.1.m1.3.3.1.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1"><apply id="S3.SS1.1.1.m1.3.3.1.1.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.1.1.m1.3.3.1.1.2.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2">superscript</csymbol><apply id="S3.SS1.1.1.m1.3.3.1.1.2.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.1.1.m1.3.3.1.1.2.2.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.SS1.1.1.m1.3.3.1.1.2.2.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.2"></sum><apply id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3"><eq id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.1"></eq><ci id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.2">𝑙</ci><cn id="S3.SS1.1.1.m1.3.3.1.1.2.2.3.3.cmml" type="integer" xref="S3.SS1.1.1.m1.3.3.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS1.1.1.m1.3.3.1.1.2.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.2.3">𝐿</ci></apply><apply id="S3.SS1.1.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.1.1.m1.3.3.1.1.1.2.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.2">norm</csymbol><apply id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1"><minus id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.1"></minus><apply id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.2">𝒚</ci><ci id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3"><ci id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.1">^</ci><apply id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.2">𝒚</ci><ci id="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.3.cmml" xref="S3.SS1.1.1.m1.3.3.1.1.1.1.1.3.2.3">𝑙</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.1.1.m1.3c">(\bm{Y},\bm{\hat{Y}})=\frac{1}{L}\sum_{l=1}^{L}\|\bm{y}_{l}-\hat{\bm{y}_{l}}\|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.1.1.m1.3d">( bold_italic_Y , overbold_^ start_ARG bold_italic_Y end_ARG ) = divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∥ bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT - over^ start_ARG bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG ∥</annotation></semantics></math>
<math alttext="L=min(|\bm{Y}|,|\bm{\hat{Y}}|)" class="ltx_Math" display="inline" id="S3.SS1.2.2.m2.4"><semantics id="S3.SS1.2.2.m2.4a"><mrow id="S3.SS1.2.2.m2.4.4" xref="S3.SS1.2.2.m2.4.4.cmml"><mi id="S3.SS1.2.2.m2.4.4.4" xref="S3.SS1.2.2.m2.4.4.4.cmml">L</mi><mo id="S3.SS1.2.2.m2.4.4.3" xref="S3.SS1.2.2.m2.4.4.3.cmml">=</mo><mrow id="S3.SS1.2.2.m2.4.4.2" xref="S3.SS1.2.2.m2.4.4.2.cmml"><mi id="S3.SS1.2.2.m2.4.4.2.4" xref="S3.SS1.2.2.m2.4.4.2.4.cmml">m</mi><mo id="S3.SS1.2.2.m2.4.4.2.3" xref="S3.SS1.2.2.m2.4.4.2.3.cmml">⁢</mo><mi id="S3.SS1.2.2.m2.4.4.2.5" xref="S3.SS1.2.2.m2.4.4.2.5.cmml">i</mi><mo id="S3.SS1.2.2.m2.4.4.2.3a" xref="S3.SS1.2.2.m2.4.4.2.3.cmml">⁢</mo><mi id="S3.SS1.2.2.m2.4.4.2.6" xref="S3.SS1.2.2.m2.4.4.2.6.cmml">n</mi><mo id="S3.SS1.2.2.m2.4.4.2.3b" xref="S3.SS1.2.2.m2.4.4.2.3.cmml">⁢</mo><mrow id="S3.SS1.2.2.m2.4.4.2.2.2" xref="S3.SS1.2.2.m2.4.4.2.2.3.cmml"><mo id="S3.SS1.2.2.m2.4.4.2.2.2.3" stretchy="false" xref="S3.SS1.2.2.m2.4.4.2.2.3.cmml">(</mo><mrow id="S3.SS1.2.2.m2.3.3.1.1.1.1.2" xref="S3.SS1.2.2.m2.3.3.1.1.1.1.1.cmml"><mo id="S3.SS1.2.2.m2.3.3.1.1.1.1.2.1" stretchy="false" xref="S3.SS1.2.2.m2.3.3.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.2.2.m2.1.1" xref="S3.SS1.2.2.m2.1.1.cmml">𝒀</mi><mo id="S3.SS1.2.2.m2.3.3.1.1.1.1.2.2" stretchy="false" xref="S3.SS1.2.2.m2.3.3.1.1.1.1.1.1.cmml">|</mo></mrow><mo id="S3.SS1.2.2.m2.4.4.2.2.2.4" xref="S3.SS1.2.2.m2.4.4.2.2.3.cmml">,</mo><mrow id="S3.SS1.2.2.m2.4.4.2.2.2.2.2" xref="S3.SS1.2.2.m2.4.4.2.2.2.2.1.cmml"><mo id="S3.SS1.2.2.m2.4.4.2.2.2.2.2.1" stretchy="false" xref="S3.SS1.2.2.m2.4.4.2.2.2.2.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.2.2.m2.2.2" xref="S3.SS1.2.2.m2.2.2.cmml"><mi id="S3.SS1.2.2.m2.2.2.2" xref="S3.SS1.2.2.m2.2.2.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S3.SS1.2.2.m2.2.2.1" mathvariant="bold" xref="S3.SS1.2.2.m2.2.2.1.cmml">^</mo></mover><mo id="S3.SS1.2.2.m2.4.4.2.2.2.2.2.2" stretchy="false" xref="S3.SS1.2.2.m2.4.4.2.2.2.2.1.1.cmml">|</mo></mrow><mo id="S3.SS1.2.2.m2.4.4.2.2.2.5" stretchy="false" xref="S3.SS1.2.2.m2.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.2.2.m2.4b"><apply id="S3.SS1.2.2.m2.4.4.cmml" xref="S3.SS1.2.2.m2.4.4"><eq id="S3.SS1.2.2.m2.4.4.3.cmml" xref="S3.SS1.2.2.m2.4.4.3"></eq><ci id="S3.SS1.2.2.m2.4.4.4.cmml" xref="S3.SS1.2.2.m2.4.4.4">𝐿</ci><apply id="S3.SS1.2.2.m2.4.4.2.cmml" xref="S3.SS1.2.2.m2.4.4.2"><times id="S3.SS1.2.2.m2.4.4.2.3.cmml" xref="S3.SS1.2.2.m2.4.4.2.3"></times><ci id="S3.SS1.2.2.m2.4.4.2.4.cmml" xref="S3.SS1.2.2.m2.4.4.2.4">𝑚</ci><ci id="S3.SS1.2.2.m2.4.4.2.5.cmml" xref="S3.SS1.2.2.m2.4.4.2.5">𝑖</ci><ci id="S3.SS1.2.2.m2.4.4.2.6.cmml" xref="S3.SS1.2.2.m2.4.4.2.6">𝑛</ci><interval closure="open" id="S3.SS1.2.2.m2.4.4.2.2.3.cmml" xref="S3.SS1.2.2.m2.4.4.2.2.2"><apply id="S3.SS1.2.2.m2.3.3.1.1.1.1.1.cmml" xref="S3.SS1.2.2.m2.3.3.1.1.1.1.2"><abs id="S3.SS1.2.2.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.SS1.2.2.m2.3.3.1.1.1.1.2.1"></abs><ci id="S3.SS1.2.2.m2.1.1.cmml" xref="S3.SS1.2.2.m2.1.1">𝒀</ci></apply><apply id="S3.SS1.2.2.m2.4.4.2.2.2.2.1.cmml" xref="S3.SS1.2.2.m2.4.4.2.2.2.2.2"><abs id="S3.SS1.2.2.m2.4.4.2.2.2.2.1.1.cmml" xref="S3.SS1.2.2.m2.4.4.2.2.2.2.2.1"></abs><apply id="S3.SS1.2.2.m2.2.2.cmml" xref="S3.SS1.2.2.m2.2.2"><ci id="S3.SS1.2.2.m2.2.2.1.cmml" xref="S3.SS1.2.2.m2.2.2.1">bold-^</ci><ci id="S3.SS1.2.2.m2.2.2.2.cmml" xref="S3.SS1.2.2.m2.2.2.2">𝒀</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.2.2.m2.4c">L=min(|\bm{Y}|,|\bm{\hat{Y}}|)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.2.2.m2.4d">italic_L = italic_m italic_i italic_n ( | bold_italic_Y | , | overbold_^ start_ARG bold_italic_Y end_ARG | )</annotation></semantics></math></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4">where <math alttext="\bm{\hat{y}}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mover accent="true" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S3.SS1.p2.1.m1.1.1.1" mathvariant="bold" xref="S3.SS1.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><ci id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1">bold-^</ci><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝒚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\bm{\hat{y}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">overbold_^ start_ARG bold_italic_y end_ARG</annotation></semantics></math> represents the predicted token distribution, and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\bm{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">bold_italic_y</annotation></semantics></math> is the actual label. Given that the translated sentence may contain a different number of tokens from original sentence, we chose the shorter token length, represented by the cardinality of <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">𝒀</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝒀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\bm{Y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">bold_italic_Y</annotation></semantics></math> and <math alttext="\bm{\hat{Y}}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mover accent="true" id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S3.SS1.p2.4.m4.1.1.1" mathvariant="bold" xref="S3.SS1.p2.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><ci id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1">bold-^</ci><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝒀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\bm{\hat{Y}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">overbold_^ start_ARG bold_italic_Y end_ARG</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   Entropy</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib5" title="">1990</a>)</cite> demonstrated that uncertainty in prediction is quantifiable by entropy. Various studies have reported performance improvements by employing entropy to select data for training<cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib13" title="">2021</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib36" title="">2021</a>)</cite>. Building on this concept, we considered entropy as an indicator of the pre-trained model’s perplexity regarding specific sentences, selecting them as candidates for fine-tuning. The entropy of the vocabulary distribution is defined as:</p>
</div>
<div class="ltx_para" id="S3.SS2.1">
<p class="ltx_p ltx_align_center" id="S3.SS2.1.1"><math alttext="H(\bm{\hat{y}})=\frac{1}{V}\sum_{i\in{}V}-P(\hat{y_{i}})logP(\hat{y_{i}})" class="ltx_Math" display="inline" id="S3.SS2.1.1.m1.3"><semantics id="S3.SS2.1.1.m1.3a"><mrow id="S3.SS2.1.1.m1.3.4" xref="S3.SS2.1.1.m1.3.4.cmml"><mrow id="S3.SS2.1.1.m1.3.4.2" xref="S3.SS2.1.1.m1.3.4.2.cmml"><mi id="S3.SS2.1.1.m1.3.4.2.2" xref="S3.SS2.1.1.m1.3.4.2.2.cmml">H</mi><mo id="S3.SS2.1.1.m1.3.4.2.1" xref="S3.SS2.1.1.m1.3.4.2.1.cmml">⁢</mo><mrow id="S3.SS2.1.1.m1.3.4.2.3.2" xref="S3.SS2.1.1.m1.1.1.cmml"><mo id="S3.SS2.1.1.m1.3.4.2.3.2.1" stretchy="false" xref="S3.SS2.1.1.m1.1.1.cmml">(</mo><mover accent="true" id="S3.SS2.1.1.m1.1.1" xref="S3.SS2.1.1.m1.1.1.cmml"><mi id="S3.SS2.1.1.m1.1.1.2" xref="S3.SS2.1.1.m1.1.1.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S3.SS2.1.1.m1.1.1.1" mathvariant="bold" xref="S3.SS2.1.1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.SS2.1.1.m1.3.4.2.3.2.2" stretchy="false" xref="S3.SS2.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.1.1.m1.3.4.1" xref="S3.SS2.1.1.m1.3.4.1.cmml">=</mo><mrow id="S3.SS2.1.1.m1.3.4.3" xref="S3.SS2.1.1.m1.3.4.3.cmml"><mrow id="S3.SS2.1.1.m1.3.4.3.2" xref="S3.SS2.1.1.m1.3.4.3.2.cmml"><mfrac id="S3.SS2.1.1.m1.3.4.3.2.2" xref="S3.SS2.1.1.m1.3.4.3.2.2.cmml"><mn id="S3.SS2.1.1.m1.3.4.3.2.2.2" xref="S3.SS2.1.1.m1.3.4.3.2.2.2.cmml">1</mn><mi id="S3.SS2.1.1.m1.3.4.3.2.2.3" xref="S3.SS2.1.1.m1.3.4.3.2.2.3.cmml">V</mi></mfrac><mo id="S3.SS2.1.1.m1.3.4.3.2.1" xref="S3.SS2.1.1.m1.3.4.3.2.1.cmml">⁢</mo><msub id="S3.SS2.1.1.m1.3.4.3.2.3" xref="S3.SS2.1.1.m1.3.4.3.2.3.cmml"><mo id="S3.SS2.1.1.m1.3.4.3.2.3.2" rspace="0em" xref="S3.SS2.1.1.m1.3.4.3.2.3.2.cmml">∑</mo><mrow id="S3.SS2.1.1.m1.3.4.3.2.3.3" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.cmml"><mi id="S3.SS2.1.1.m1.3.4.3.2.3.3.2" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.2.cmml">i</mi><mo id="S3.SS2.1.1.m1.3.4.3.2.3.3.1" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.1.cmml">∈</mo><mi id="S3.SS2.1.1.m1.3.4.3.2.3.3.3" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.3.cmml">V</mi></mrow></msub></mrow><mo id="S3.SS2.1.1.m1.3.4.3.1" lspace="0em" xref="S3.SS2.1.1.m1.3.4.3.1.cmml">−</mo><mrow id="S3.SS2.1.1.m1.3.4.3.3" xref="S3.SS2.1.1.m1.3.4.3.3.cmml"><mi id="S3.SS2.1.1.m1.3.4.3.3.2" xref="S3.SS2.1.1.m1.3.4.3.3.2.cmml">P</mi><mo id="S3.SS2.1.1.m1.3.4.3.3.1" xref="S3.SS2.1.1.m1.3.4.3.3.1.cmml">⁢</mo><mrow id="S3.SS2.1.1.m1.3.4.3.3.3.2" xref="S3.SS2.1.1.m1.2.2.cmml"><mo id="S3.SS2.1.1.m1.3.4.3.3.3.2.1" stretchy="false" xref="S3.SS2.1.1.m1.2.2.cmml">(</mo><mover accent="true" id="S3.SS2.1.1.m1.2.2" xref="S3.SS2.1.1.m1.2.2.cmml"><msub id="S3.SS2.1.1.m1.2.2.2" xref="S3.SS2.1.1.m1.2.2.2.cmml"><mi id="S3.SS2.1.1.m1.2.2.2.2" xref="S3.SS2.1.1.m1.2.2.2.2.cmml">y</mi><mi id="S3.SS2.1.1.m1.2.2.2.3" xref="S3.SS2.1.1.m1.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.1.1.m1.2.2.1" xref="S3.SS2.1.1.m1.2.2.1.cmml">^</mo></mover><mo id="S3.SS2.1.1.m1.3.4.3.3.3.2.2" stretchy="false" xref="S3.SS2.1.1.m1.2.2.cmml">)</mo></mrow><mo id="S3.SS2.1.1.m1.3.4.3.3.1a" xref="S3.SS2.1.1.m1.3.4.3.3.1.cmml">⁢</mo><mi id="S3.SS2.1.1.m1.3.4.3.3.4" xref="S3.SS2.1.1.m1.3.4.3.3.4.cmml">l</mi><mo id="S3.SS2.1.1.m1.3.4.3.3.1b" xref="S3.SS2.1.1.m1.3.4.3.3.1.cmml">⁢</mo><mi id="S3.SS2.1.1.m1.3.4.3.3.5" xref="S3.SS2.1.1.m1.3.4.3.3.5.cmml">o</mi><mo id="S3.SS2.1.1.m1.3.4.3.3.1c" xref="S3.SS2.1.1.m1.3.4.3.3.1.cmml">⁢</mo><mi id="S3.SS2.1.1.m1.3.4.3.3.6" xref="S3.SS2.1.1.m1.3.4.3.3.6.cmml">g</mi><mo id="S3.SS2.1.1.m1.3.4.3.3.1d" xref="S3.SS2.1.1.m1.3.4.3.3.1.cmml">⁢</mo><mi id="S3.SS2.1.1.m1.3.4.3.3.7" xref="S3.SS2.1.1.m1.3.4.3.3.7.cmml">P</mi><mo id="S3.SS2.1.1.m1.3.4.3.3.1e" xref="S3.SS2.1.1.m1.3.4.3.3.1.cmml">⁢</mo><mrow id="S3.SS2.1.1.m1.3.4.3.3.8.2" xref="S3.SS2.1.1.m1.3.3.cmml"><mo id="S3.SS2.1.1.m1.3.4.3.3.8.2.1" stretchy="false" xref="S3.SS2.1.1.m1.3.3.cmml">(</mo><mover accent="true" id="S3.SS2.1.1.m1.3.3" xref="S3.SS2.1.1.m1.3.3.cmml"><msub id="S3.SS2.1.1.m1.3.3.2" xref="S3.SS2.1.1.m1.3.3.2.cmml"><mi id="S3.SS2.1.1.m1.3.3.2.2" xref="S3.SS2.1.1.m1.3.3.2.2.cmml">y</mi><mi id="S3.SS2.1.1.m1.3.3.2.3" xref="S3.SS2.1.1.m1.3.3.2.3.cmml">i</mi></msub><mo id="S3.SS2.1.1.m1.3.3.1" xref="S3.SS2.1.1.m1.3.3.1.cmml">^</mo></mover><mo id="S3.SS2.1.1.m1.3.4.3.3.8.2.2" stretchy="false" xref="S3.SS2.1.1.m1.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.1.1.m1.3b"><apply id="S3.SS2.1.1.m1.3.4.cmml" xref="S3.SS2.1.1.m1.3.4"><eq id="S3.SS2.1.1.m1.3.4.1.cmml" xref="S3.SS2.1.1.m1.3.4.1"></eq><apply id="S3.SS2.1.1.m1.3.4.2.cmml" xref="S3.SS2.1.1.m1.3.4.2"><times id="S3.SS2.1.1.m1.3.4.2.1.cmml" xref="S3.SS2.1.1.m1.3.4.2.1"></times><ci id="S3.SS2.1.1.m1.3.4.2.2.cmml" xref="S3.SS2.1.1.m1.3.4.2.2">𝐻</ci><apply id="S3.SS2.1.1.m1.1.1.cmml" xref="S3.SS2.1.1.m1.3.4.2.3.2"><ci id="S3.SS2.1.1.m1.1.1.1.cmml" xref="S3.SS2.1.1.m1.1.1.1">bold-^</ci><ci id="S3.SS2.1.1.m1.1.1.2.cmml" xref="S3.SS2.1.1.m1.1.1.2">𝒚</ci></apply></apply><apply id="S3.SS2.1.1.m1.3.4.3.cmml" xref="S3.SS2.1.1.m1.3.4.3"><minus id="S3.SS2.1.1.m1.3.4.3.1.cmml" xref="S3.SS2.1.1.m1.3.4.3.1"></minus><apply id="S3.SS2.1.1.m1.3.4.3.2.cmml" xref="S3.SS2.1.1.m1.3.4.3.2"><times id="S3.SS2.1.1.m1.3.4.3.2.1.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.1"></times><apply id="S3.SS2.1.1.m1.3.4.3.2.2.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.2"><divide id="S3.SS2.1.1.m1.3.4.3.2.2.1.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.2"></divide><cn id="S3.SS2.1.1.m1.3.4.3.2.2.2.cmml" type="integer" xref="S3.SS2.1.1.m1.3.4.3.2.2.2">1</cn><ci id="S3.SS2.1.1.m1.3.4.3.2.2.3.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.2.3">𝑉</ci></apply><apply id="S3.SS2.1.1.m1.3.4.3.2.3.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.1.1.m1.3.4.3.2.3.1.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3">subscript</csymbol><sum id="S3.SS2.1.1.m1.3.4.3.2.3.2.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3.2"></sum><apply id="S3.SS2.1.1.m1.3.4.3.2.3.3.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3.3"><in id="S3.SS2.1.1.m1.3.4.3.2.3.3.1.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.1"></in><ci id="S3.SS2.1.1.m1.3.4.3.2.3.3.2.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.2">𝑖</ci><ci id="S3.SS2.1.1.m1.3.4.3.2.3.3.3.cmml" xref="S3.SS2.1.1.m1.3.4.3.2.3.3.3">𝑉</ci></apply></apply></apply><apply id="S3.SS2.1.1.m1.3.4.3.3.cmml" xref="S3.SS2.1.1.m1.3.4.3.3"><times id="S3.SS2.1.1.m1.3.4.3.3.1.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.1"></times><ci id="S3.SS2.1.1.m1.3.4.3.3.2.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.2">𝑃</ci><apply id="S3.SS2.1.1.m1.2.2.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.3.2"><ci id="S3.SS2.1.1.m1.2.2.1.cmml" xref="S3.SS2.1.1.m1.2.2.1">^</ci><apply id="S3.SS2.1.1.m1.2.2.2.cmml" xref="S3.SS2.1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.1.1.m1.2.2.2.1.cmml" xref="S3.SS2.1.1.m1.2.2.2">subscript</csymbol><ci id="S3.SS2.1.1.m1.2.2.2.2.cmml" xref="S3.SS2.1.1.m1.2.2.2.2">𝑦</ci><ci id="S3.SS2.1.1.m1.2.2.2.3.cmml" xref="S3.SS2.1.1.m1.2.2.2.3">𝑖</ci></apply></apply><ci id="S3.SS2.1.1.m1.3.4.3.3.4.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.4">𝑙</ci><ci id="S3.SS2.1.1.m1.3.4.3.3.5.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.5">𝑜</ci><ci id="S3.SS2.1.1.m1.3.4.3.3.6.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.6">𝑔</ci><ci id="S3.SS2.1.1.m1.3.4.3.3.7.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.7">𝑃</ci><apply id="S3.SS2.1.1.m1.3.3.cmml" xref="S3.SS2.1.1.m1.3.4.3.3.8.2"><ci id="S3.SS2.1.1.m1.3.3.1.cmml" xref="S3.SS2.1.1.m1.3.3.1">^</ci><apply id="S3.SS2.1.1.m1.3.3.2.cmml" xref="S3.SS2.1.1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.1.1.m1.3.3.2.1.cmml" xref="S3.SS2.1.1.m1.3.3.2">subscript</csymbol><ci id="S3.SS2.1.1.m1.3.3.2.2.cmml" xref="S3.SS2.1.1.m1.3.3.2.2">𝑦</ci><ci id="S3.SS2.1.1.m1.3.3.2.3.cmml" xref="S3.SS2.1.1.m1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.1.1.m1.3c">H(\bm{\hat{y}})=\frac{1}{V}\sum_{i\in{}V}-P(\hat{y_{i}})logP(\hat{y_{i}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.1.1.m1.3d">italic_H ( overbold_^ start_ARG bold_italic_y end_ARG ) = divide start_ARG 1 end_ARG start_ARG italic_V end_ARG ∑ start_POSTSUBSCRIPT italic_i ∈ italic_V end_POSTSUBSCRIPT - italic_P ( over^ start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ) italic_l italic_o italic_g italic_P ( over^ start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">where <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_V</annotation></semantics></math> is a vocabulary. We adopted averaged entropy as MDS which is as follows:</p>
<p class="ltx_p ltx_align_center" id="S3.SS2.p2.2.1"><math alttext="AvgEntropy(\bm{\hat{Y}})=\frac{1}{L}\sum_{l=1}^{L}H(\bm{\hat{y}}_{l})" class="ltx_Math" display="inline" id="S3.SS2.p2.2.1.m1.2"><semantics id="S3.SS2.p2.2.1.m1.2a"><mrow id="S3.SS2.p2.2.1.m1.2.2" xref="S3.SS2.p2.2.1.m1.2.2.cmml"><mrow id="S3.SS2.p2.2.1.m1.2.2.3" xref="S3.SS2.p2.2.1.m1.2.2.3.cmml"><mi id="S3.SS2.p2.2.1.m1.2.2.3.2" xref="S3.SS2.p2.2.1.m1.2.2.3.2.cmml">A</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.3" xref="S3.SS2.p2.2.1.m1.2.2.3.3.cmml">v</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1a" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.4" xref="S3.SS2.p2.2.1.m1.2.2.3.4.cmml">g</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1b" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.5" xref="S3.SS2.p2.2.1.m1.2.2.3.5.cmml">E</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1c" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.6" xref="S3.SS2.p2.2.1.m1.2.2.3.6.cmml">n</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1d" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.7" xref="S3.SS2.p2.2.1.m1.2.2.3.7.cmml">t</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1e" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.8" xref="S3.SS2.p2.2.1.m1.2.2.3.8.cmml">r</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1f" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.9" xref="S3.SS2.p2.2.1.m1.2.2.3.9.cmml">o</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1g" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.10" xref="S3.SS2.p2.2.1.m1.2.2.3.10.cmml">p</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1h" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.1.m1.2.2.3.11" xref="S3.SS2.p2.2.1.m1.2.2.3.11.cmml">y</mi><mo id="S3.SS2.p2.2.1.m1.2.2.3.1i" xref="S3.SS2.p2.2.1.m1.2.2.3.1.cmml">⁢</mo><mrow id="S3.SS2.p2.2.1.m1.2.2.3.12.2" xref="S3.SS2.p2.2.1.m1.1.1.cmml"><mo id="S3.SS2.p2.2.1.m1.2.2.3.12.2.1" stretchy="false" xref="S3.SS2.p2.2.1.m1.1.1.cmml">(</mo><mover accent="true" id="S3.SS2.p2.2.1.m1.1.1" xref="S3.SS2.p2.2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.2.1.m1.1.1.2" xref="S3.SS2.p2.2.1.m1.1.1.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.1.1.1" mathvariant="bold" xref="S3.SS2.p2.2.1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.SS2.p2.2.1.m1.2.2.3.12.2.2" stretchy="false" xref="S3.SS2.p2.2.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.2.1.m1.2.2.2" xref="S3.SS2.p2.2.1.m1.2.2.2.cmml">=</mo><mrow id="S3.SS2.p2.2.1.m1.2.2.1" xref="S3.SS2.p2.2.1.m1.2.2.1.cmml"><mfrac id="S3.SS2.p2.2.1.m1.2.2.1.3" xref="S3.SS2.p2.2.1.m1.2.2.1.3.cmml"><mn id="S3.SS2.p2.2.1.m1.2.2.1.3.2" xref="S3.SS2.p2.2.1.m1.2.2.1.3.2.cmml">1</mn><mi id="S3.SS2.p2.2.1.m1.2.2.1.3.3" xref="S3.SS2.p2.2.1.m1.2.2.1.3.3.cmml">L</mi></mfrac><mo id="S3.SS2.p2.2.1.m1.2.2.1.2" xref="S3.SS2.p2.2.1.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.2.1.m1.2.2.1.1" xref="S3.SS2.p2.2.1.m1.2.2.1.1.cmml"><msubsup id="S3.SS2.p2.2.1.m1.2.2.1.1.2" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.cmml"><mo id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.2" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.2.cmml">∑</mo><mrow id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.cmml"><mi id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.2" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.2.cmml">l</mi><mo id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.1" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.3" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p2.2.1.m1.2.2.1.1.2.3" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.3.cmml">L</mi></msubsup><mrow id="S3.SS2.p2.2.1.m1.2.2.1.1.1" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.2.1.m1.2.2.1.1.1.3" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.3.cmml">H</mi><mo id="S3.SS2.p2.2.1.m1.2.2.1.1.1.2" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.1" mathvariant="bold" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.3.cmml">l</mi></msub><mo id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.1.m1.2b"><apply id="S3.SS2.p2.2.1.m1.2.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2"><eq id="S3.SS2.p2.2.1.m1.2.2.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.2"></eq><apply id="S3.SS2.p2.2.1.m1.2.2.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3"><times id="S3.SS2.p2.2.1.m1.2.2.3.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.1"></times><ci id="S3.SS2.p2.2.1.m1.2.2.3.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.2">𝐴</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.3">𝑣</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.4.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.4">𝑔</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.5.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.5">𝐸</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.6.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.6">𝑛</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.7.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.7">𝑡</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.8.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.8">𝑟</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.9.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.9">𝑜</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.10.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.10">𝑝</ci><ci id="S3.SS2.p2.2.1.m1.2.2.3.11.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.11">𝑦</ci><apply id="S3.SS2.p2.2.1.m1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.3.12.2"><ci id="S3.SS2.p2.2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.1.1.1">bold-^</ci><ci id="S3.SS2.p2.2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.1.1.2">𝒀</ci></apply></apply><apply id="S3.SS2.p2.2.1.m1.2.2.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1"><times id="S3.SS2.p2.2.1.m1.2.2.1.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.2"></times><apply id="S3.SS2.p2.2.1.m1.2.2.1.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.3"><divide id="S3.SS2.p2.2.1.m1.2.2.1.3.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.3"></divide><cn id="S3.SS2.p2.2.1.m1.2.2.1.3.2.cmml" type="integer" xref="S3.SS2.p2.2.1.m1.2.2.1.3.2">1</cn><ci id="S3.SS2.p2.2.1.m1.2.2.1.3.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.3.3">𝐿</ci></apply><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1"><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.2"></sum><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3"><eq id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.1"></eq><ci id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.2">𝑙</ci><cn id="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS2.p2.2.1.m1.2.2.1.1.2.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.2.3">𝐿</ci></apply><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1"><times id="S3.SS2.p2.2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.2"></times><ci id="S3.SS2.p2.2.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.3">𝐻</ci><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2"><ci id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.1">bold-^</ci><ci id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.2.2">𝒚</ci></apply><ci id="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.1.1.1.1.1.1.3">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.1.m1.2c">AvgEntropy(\bm{\hat{Y}})=\frac{1}{L}\sum_{l=1}^{L}H(\bm{\hat{y}}_{l})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.1.m1.2d">italic_A italic_v italic_g italic_E italic_n italic_t italic_r italic_o italic_p italic_y ( overbold_^ start_ARG bold_italic_Y end_ARG ) = divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_H ( overbold_^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math></p>
<p class="ltx_p" id="S3.SS2.p2.4">where <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m1.1"><semantics id="S3.SS2.p2.3.m1.1a"><mi id="S3.SS2.p2.3.m1.1.1" xref="S3.SS2.p2.3.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m1.1b"><ci id="S3.SS2.p2.3.m1.1.1.cmml" xref="S3.SS2.p2.3.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m1.1d">italic_L</annotation></semantics></math> is a length of the sentence <math alttext="\bm{\hat{Y}}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m2.1"><semantics id="S3.SS2.p2.4.m2.1a"><mover accent="true" id="S3.SS2.p2.4.m2.1.1" xref="S3.SS2.p2.4.m2.1.1.cmml"><mi id="S3.SS2.p2.4.m2.1.1.2" xref="S3.SS2.p2.4.m2.1.1.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S3.SS2.p2.4.m2.1.1.1" mathvariant="bold" xref="S3.SS2.p2.4.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m2.1b"><apply id="S3.SS2.p2.4.m2.1.1.cmml" xref="S3.SS2.p2.4.m2.1.1"><ci id="S3.SS2.p2.4.m2.1.1.1.cmml" xref="S3.SS2.p2.4.m2.1.1.1">bold-^</ci><ci id="S3.SS2.p2.4.m2.1.1.2.cmml" xref="S3.SS2.p2.4.m2.1.1.2">𝒀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m2.1c">\bm{\hat{Y}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m2.1d">overbold_^ start_ARG bold_italic_Y end_ARG</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">However, given that the optimal entropy level may differ by token types, such as adjectives or synonyms, we hypothesized that employing <math alttext="AvgEntropy" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">A</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">v</mi><mo id="S3.SS2.p3.1.m1.1.1.1a" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.4" xref="S3.SS2.p3.1.m1.1.1.4.cmml">g</mi><mo id="S3.SS2.p3.1.m1.1.1.1b" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.5" xref="S3.SS2.p3.1.m1.1.1.5.cmml">E</mi><mo id="S3.SS2.p3.1.m1.1.1.1c" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.6" xref="S3.SS2.p3.1.m1.1.1.6.cmml">n</mi><mo id="S3.SS2.p3.1.m1.1.1.1d" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.7" xref="S3.SS2.p3.1.m1.1.1.7.cmml">t</mi><mo id="S3.SS2.p3.1.m1.1.1.1e" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.8" xref="S3.SS2.p3.1.m1.1.1.8.cmml">r</mi><mo id="S3.SS2.p3.1.m1.1.1.1f" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.9" xref="S3.SS2.p3.1.m1.1.1.9.cmml">o</mi><mo id="S3.SS2.p3.1.m1.1.1.1g" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.10" xref="S3.SS2.p3.1.m1.1.1.10.cmml">p</mi><mo id="S3.SS2.p3.1.m1.1.1.1h" xref="S3.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.11" xref="S3.SS2.p3.1.m1.1.1.11.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><times id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></times><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐴</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑣</ci><ci id="S3.SS2.p3.1.m1.1.1.4.cmml" xref="S3.SS2.p3.1.m1.1.1.4">𝑔</ci><ci id="S3.SS2.p3.1.m1.1.1.5.cmml" xref="S3.SS2.p3.1.m1.1.1.5">𝐸</ci><ci id="S3.SS2.p3.1.m1.1.1.6.cmml" xref="S3.SS2.p3.1.m1.1.1.6">𝑛</ci><ci id="S3.SS2.p3.1.m1.1.1.7.cmml" xref="S3.SS2.p3.1.m1.1.1.7">𝑡</ci><ci id="S3.SS2.p3.1.m1.1.1.8.cmml" xref="S3.SS2.p3.1.m1.1.1.8">𝑟</ci><ci id="S3.SS2.p3.1.m1.1.1.9.cmml" xref="S3.SS2.p3.1.m1.1.1.9">𝑜</ci><ci id="S3.SS2.p3.1.m1.1.1.10.cmml" xref="S3.SS2.p3.1.m1.1.1.10">𝑝</ci><ci id="S3.SS2.p3.1.m1.1.1.11.cmml" xref="S3.SS2.p3.1.m1.1.1.11">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">AvgEntropy</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_A italic_v italic_g italic_E italic_n italic_t italic_r italic_o italic_p italic_y</annotation></semantics></math> as an MDS might lead the model to become either overconfident or overly cautious.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Selfsup</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3"><cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite> observed that within clustered image representations, data points distant from their centroids often exhibit unique patterns, which have high Euclidean distance to the centroid. However, its effectiveness as an MDS for fine-tuning translation models remains unverified. To adapt this approach to the language domain, we utilized sentence embeddings for the source data and applied k-means clustering. If <math alttext="x_{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑥</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">x_{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT</annotation></semantics></math> represents a sentence embedding of source language data <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_x</annotation></semantics></math>, clustered around centroid <math alttext="A" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_A</annotation></semantics></math>, then the MDS Selfsup can be described as:</p>
</div>
<div class="ltx_para" id="S3.SS3.1">
<p class="ltx_p ltx_align_center" id="S3.SS3.1.1"><math alttext="Selfsup(x_{A})=||x_{A}-A||" class="ltx_Math" display="inline" id="S3.SS3.1.1.m1.2"><semantics id="S3.SS3.1.1.m1.2a"><mrow id="S3.SS3.1.1.m1.2.2" xref="S3.SS3.1.1.m1.2.2.cmml"><mrow id="S3.SS3.1.1.m1.1.1.1" xref="S3.SS3.1.1.m1.1.1.1.cmml"><mi id="S3.SS3.1.1.m1.1.1.1.3" xref="S3.SS3.1.1.m1.1.1.1.3.cmml">S</mi><mo id="S3.SS3.1.1.m1.1.1.1.2" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS3.1.1.m1.1.1.1.4" xref="S3.SS3.1.1.m1.1.1.1.4.cmml">e</mi><mo id="S3.SS3.1.1.m1.1.1.1.2a" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS3.1.1.m1.1.1.1.5" xref="S3.SS3.1.1.m1.1.1.1.5.cmml">l</mi><mo id="S3.SS3.1.1.m1.1.1.1.2b" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS3.1.1.m1.1.1.1.6" xref="S3.SS3.1.1.m1.1.1.1.6.cmml">f</mi><mo id="S3.SS3.1.1.m1.1.1.1.2c" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS3.1.1.m1.1.1.1.7" xref="S3.SS3.1.1.m1.1.1.1.7.cmml">s</mi><mo id="S3.SS3.1.1.m1.1.1.1.2d" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS3.1.1.m1.1.1.1.8" xref="S3.SS3.1.1.m1.1.1.1.8.cmml">u</mi><mo id="S3.SS3.1.1.m1.1.1.1.2e" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS3.1.1.m1.1.1.1.9" xref="S3.SS3.1.1.m1.1.1.1.9.cmml">p</mi><mo id="S3.SS3.1.1.m1.1.1.1.2f" xref="S3.SS3.1.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS3.1.1.m1.1.1.1.1.1" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.SS3.1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.1.1.m1.1.1.1.1.1.1" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.SS3.1.1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.1.1.m1.2.2.3" xref="S3.SS3.1.1.m1.2.2.3.cmml">=</mo><mrow id="S3.SS3.1.1.m1.2.2.2.1" xref="S3.SS3.1.1.m1.2.2.2.2.cmml"><mo id="S3.SS3.1.1.m1.2.2.2.1.2" stretchy="false" xref="S3.SS3.1.1.m1.2.2.2.2.1.cmml">‖</mo><mrow id="S3.SS3.1.1.m1.2.2.2.1.1" xref="S3.SS3.1.1.m1.2.2.2.1.1.cmml"><msub id="S3.SS3.1.1.m1.2.2.2.1.1.2" xref="S3.SS3.1.1.m1.2.2.2.1.1.2.cmml"><mi id="S3.SS3.1.1.m1.2.2.2.1.1.2.2" xref="S3.SS3.1.1.m1.2.2.2.1.1.2.2.cmml">x</mi><mi id="S3.SS3.1.1.m1.2.2.2.1.1.2.3" xref="S3.SS3.1.1.m1.2.2.2.1.1.2.3.cmml">A</mi></msub><mo id="S3.SS3.1.1.m1.2.2.2.1.1.1" xref="S3.SS3.1.1.m1.2.2.2.1.1.1.cmml">−</mo><mi id="S3.SS3.1.1.m1.2.2.2.1.1.3" xref="S3.SS3.1.1.m1.2.2.2.1.1.3.cmml">A</mi></mrow><mo id="S3.SS3.1.1.m1.2.2.2.1.3" stretchy="false" xref="S3.SS3.1.1.m1.2.2.2.2.1.cmml">‖</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.1.1.m1.2b"><apply id="S3.SS3.1.1.m1.2.2.cmml" xref="S3.SS3.1.1.m1.2.2"><eq id="S3.SS3.1.1.m1.2.2.3.cmml" xref="S3.SS3.1.1.m1.2.2.3"></eq><apply id="S3.SS3.1.1.m1.1.1.1.cmml" xref="S3.SS3.1.1.m1.1.1.1"><times id="S3.SS3.1.1.m1.1.1.1.2.cmml" xref="S3.SS3.1.1.m1.1.1.1.2"></times><ci id="S3.SS3.1.1.m1.1.1.1.3.cmml" xref="S3.SS3.1.1.m1.1.1.1.3">𝑆</ci><ci id="S3.SS3.1.1.m1.1.1.1.4.cmml" xref="S3.SS3.1.1.m1.1.1.1.4">𝑒</ci><ci id="S3.SS3.1.1.m1.1.1.1.5.cmml" xref="S3.SS3.1.1.m1.1.1.1.5">𝑙</ci><ci id="S3.SS3.1.1.m1.1.1.1.6.cmml" xref="S3.SS3.1.1.m1.1.1.1.6">𝑓</ci><ci id="S3.SS3.1.1.m1.1.1.1.7.cmml" xref="S3.SS3.1.1.m1.1.1.1.7">𝑠</ci><ci id="S3.SS3.1.1.m1.1.1.1.8.cmml" xref="S3.SS3.1.1.m1.1.1.1.8">𝑢</ci><ci id="S3.SS3.1.1.m1.1.1.1.9.cmml" xref="S3.SS3.1.1.m1.1.1.1.9">𝑝</ci><apply id="S3.SS3.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS3.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.1.1.m1.1.1.1.1.1.1.3">𝐴</ci></apply></apply><apply id="S3.SS3.1.1.m1.2.2.2.2.cmml" xref="S3.SS3.1.1.m1.2.2.2.1"><csymbol cd="latexml" id="S3.SS3.1.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.2">norm</csymbol><apply id="S3.SS3.1.1.m1.2.2.2.1.1.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1"><minus id="S3.SS3.1.1.m1.2.2.2.1.1.1.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1.1"></minus><apply id="S3.SS3.1.1.m1.2.2.2.1.1.2.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.1.1.m1.2.2.2.1.1.2.1.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS3.1.1.m1.2.2.2.1.1.2.2.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1.2.2">𝑥</ci><ci id="S3.SS3.1.1.m1.2.2.2.1.1.2.3.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1.2.3">𝐴</ci></apply><ci id="S3.SS3.1.1.m1.2.2.2.1.1.3.cmml" xref="S3.SS3.1.1.m1.2.2.2.1.1.3">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.1.1.m1.2c">Selfsup(x_{A})=||x_{A}-A||</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.1.1.m1.2d">italic_S italic_e italic_l italic_f italic_s italic_u italic_p ( italic_x start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ) = | | italic_x start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT - italic_A | |</annotation></semantics></math></p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">If the sentence embeddings are well-aligned, MDS Selfsup is expected to capture training-efficient data for fine-tuning. Although recent sentence embedding models demonstrate decent performance, their accuracy in domain-specific data remains questionable. Our findings provide support for this doubt, as illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2.T1" title="Table 1 ‣ 2. Related Works ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, where the COMET score failed to detect semantic distortion.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.4.   Reference-free COMET</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib28" title="">2021</a>)</cite> proposed a Reference-free COMET, which was trained to estimate quality without reference, only with source and translated sentences. Reference-free COMET was designed to predict quality annotations using a sentence embedding model. Its output range is 0 to 1, where 1 denotes the best quality. We expected that Reference-free COMET as an MDS would be inversely proportional to the training-efficiency since it would detect examples that the model could not translate well.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Proposed Method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our hypothesis posits that complex patterns possessed by named entities are essential for fine-tuning. This is particularly true in domain-specific machine translation, where rare words and expressions occur frequently but are not present in the general domain. By incorporating these characteristics into data selection, we measured the maximum entropy while translating named entities, which are unlikely to have alternative answers. In summary, our method specifically targets perplexing named entities.</p>
</div>
<div class="ltx_logical-block" id="S4.2">
<div class="ltx_para" id="S4.2.p1">
<p class="ltx_p ltx_align_center" id="S4.1.1"><math alttext="PerEnts(\bm{\hat{Y}})=max(\{H(\bm{\hat{y}}_{x})|x\in NE(\bm{\hat{y}})\})" class="ltx_Math" display="inline" id="S4.1.1.m1.3"><semantics id="S4.1.1.m1.3a"><mrow id="S4.1.1.m1.3.3" xref="S4.1.1.m1.3.3.cmml"><mrow id="S4.1.1.m1.3.3.3" xref="S4.1.1.m1.3.3.3.cmml"><mi id="S4.1.1.m1.3.3.3.2" xref="S4.1.1.m1.3.3.3.2.cmml">P</mi><mo id="S4.1.1.m1.3.3.3.1" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.3.3" xref="S4.1.1.m1.3.3.3.3.cmml">e</mi><mo id="S4.1.1.m1.3.3.3.1a" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.3.4" xref="S4.1.1.m1.3.3.3.4.cmml">r</mi><mo id="S4.1.1.m1.3.3.3.1b" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.3.5" xref="S4.1.1.m1.3.3.3.5.cmml">E</mi><mo id="S4.1.1.m1.3.3.3.1c" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.3.6" xref="S4.1.1.m1.3.3.3.6.cmml">n</mi><mo id="S4.1.1.m1.3.3.3.1d" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.3.7" xref="S4.1.1.m1.3.3.3.7.cmml">t</mi><mo id="S4.1.1.m1.3.3.3.1e" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.3.8" xref="S4.1.1.m1.3.3.3.8.cmml">s</mi><mo id="S4.1.1.m1.3.3.3.1f" xref="S4.1.1.m1.3.3.3.1.cmml">⁢</mo><mrow id="S4.1.1.m1.3.3.3.9.2" xref="S4.1.1.m1.1.1.cmml"><mo id="S4.1.1.m1.3.3.3.9.2.1" stretchy="false" xref="S4.1.1.m1.1.1.cmml">(</mo><mover accent="true" id="S4.1.1.m1.1.1" xref="S4.1.1.m1.1.1.cmml"><mi id="S4.1.1.m1.1.1.2" xref="S4.1.1.m1.1.1.2.cmml">𝒀</mi><mo class="ltx_mathvariant_bold" id="S4.1.1.m1.1.1.1" mathvariant="bold" xref="S4.1.1.m1.1.1.1.cmml">^</mo></mover><mo id="S4.1.1.m1.3.3.3.9.2.2" stretchy="false" xref="S4.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.1.1.m1.3.3.2" xref="S4.1.1.m1.3.3.2.cmml">=</mo><mrow id="S4.1.1.m1.3.3.1" xref="S4.1.1.m1.3.3.1.cmml"><mi id="S4.1.1.m1.3.3.1.3" xref="S4.1.1.m1.3.3.1.3.cmml">m</mi><mo id="S4.1.1.m1.3.3.1.2" xref="S4.1.1.m1.3.3.1.2.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.1.4" xref="S4.1.1.m1.3.3.1.4.cmml">a</mi><mo id="S4.1.1.m1.3.3.1.2a" xref="S4.1.1.m1.3.3.1.2.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.1.5" xref="S4.1.1.m1.3.3.1.5.cmml">x</mi><mo id="S4.1.1.m1.3.3.1.2b" xref="S4.1.1.m1.3.3.1.2.cmml">⁢</mo><mrow id="S4.1.1.m1.3.3.1.1.1" xref="S4.1.1.m1.3.3.1.cmml"><mo id="S4.1.1.m1.3.3.1.1.1.2" stretchy="false" xref="S4.1.1.m1.3.3.1.cmml">(</mo><mrow id="S4.1.1.m1.3.3.1.1.1.1.2" xref="S4.1.1.m1.3.3.1.1.1.1.3.cmml"><mo id="S4.1.1.m1.3.3.1.1.1.1.2.3" stretchy="false" xref="S4.1.1.m1.3.3.1.1.1.1.3.1.cmml">{</mo><mrow id="S4.1.1.m1.3.3.1.1.1.1.1.1" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S4.1.1.m1.3.3.1.1.1.1.1.1.3" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.3.cmml">H</mi><mo id="S4.1.1.m1.3.3.1.1.1.1.1.1.2" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1" mathvariant="bold" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.1.1.m1.3.3.1.1.1.1.2.4" lspace="0em" rspace="0em" xref="S4.1.1.m1.3.3.1.1.1.1.3.1.cmml">|</mo><mrow id="S4.1.1.m1.3.3.1.1.1.1.2.2" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.cmml"><mi id="S4.1.1.m1.3.3.1.1.1.1.2.2.2" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.2.cmml">x</mi><mo id="S4.1.1.m1.3.3.1.1.1.1.2.2.1" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.1.cmml">∈</mo><mrow id="S4.1.1.m1.3.3.1.1.1.1.2.2.3" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.2" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.2.cmml">N</mi><mo id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.1" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.3" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.3.cmml">E</mi><mo id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.1a" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.1.cmml">⁢</mo><mrow id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.4.2" xref="S4.1.1.m1.2.2.cmml"><mo id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.4.2.1" stretchy="false" xref="S4.1.1.m1.2.2.cmml">(</mo><mover accent="true" id="S4.1.1.m1.2.2" xref="S4.1.1.m1.2.2.cmml"><mi id="S4.1.1.m1.2.2.2" xref="S4.1.1.m1.2.2.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S4.1.1.m1.2.2.1" mathvariant="bold" xref="S4.1.1.m1.2.2.1.cmml">^</mo></mover><mo id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.4.2.2" stretchy="false" xref="S4.1.1.m1.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.1.1.m1.3.3.1.1.1.1.2.5" stretchy="false" xref="S4.1.1.m1.3.3.1.1.1.1.3.1.cmml">}</mo></mrow><mo id="S4.1.1.m1.3.3.1.1.1.3" stretchy="false" xref="S4.1.1.m1.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.1.1.m1.3b"><apply id="S4.1.1.m1.3.3.cmml" xref="S4.1.1.m1.3.3"><eq id="S4.1.1.m1.3.3.2.cmml" xref="S4.1.1.m1.3.3.2"></eq><apply id="S4.1.1.m1.3.3.3.cmml" xref="S4.1.1.m1.3.3.3"><times id="S4.1.1.m1.3.3.3.1.cmml" xref="S4.1.1.m1.3.3.3.1"></times><ci id="S4.1.1.m1.3.3.3.2.cmml" xref="S4.1.1.m1.3.3.3.2">𝑃</ci><ci id="S4.1.1.m1.3.3.3.3.cmml" xref="S4.1.1.m1.3.3.3.3">𝑒</ci><ci id="S4.1.1.m1.3.3.3.4.cmml" xref="S4.1.1.m1.3.3.3.4">𝑟</ci><ci id="S4.1.1.m1.3.3.3.5.cmml" xref="S4.1.1.m1.3.3.3.5">𝐸</ci><ci id="S4.1.1.m1.3.3.3.6.cmml" xref="S4.1.1.m1.3.3.3.6">𝑛</ci><ci id="S4.1.1.m1.3.3.3.7.cmml" xref="S4.1.1.m1.3.3.3.7">𝑡</ci><ci id="S4.1.1.m1.3.3.3.8.cmml" xref="S4.1.1.m1.3.3.3.8">𝑠</ci><apply id="S4.1.1.m1.1.1.cmml" xref="S4.1.1.m1.3.3.3.9.2"><ci id="S4.1.1.m1.1.1.1.cmml" xref="S4.1.1.m1.1.1.1">bold-^</ci><ci id="S4.1.1.m1.1.1.2.cmml" xref="S4.1.1.m1.1.1.2">𝒀</ci></apply></apply><apply id="S4.1.1.m1.3.3.1.cmml" xref="S4.1.1.m1.3.3.1"><times id="S4.1.1.m1.3.3.1.2.cmml" xref="S4.1.1.m1.3.3.1.2"></times><ci id="S4.1.1.m1.3.3.1.3.cmml" xref="S4.1.1.m1.3.3.1.3">𝑚</ci><ci id="S4.1.1.m1.3.3.1.4.cmml" xref="S4.1.1.m1.3.3.1.4">𝑎</ci><ci id="S4.1.1.m1.3.3.1.5.cmml" xref="S4.1.1.m1.3.3.1.5">𝑥</ci><apply id="S4.1.1.m1.3.3.1.1.1.1.3.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2"><csymbol cd="latexml" id="S4.1.1.m1.3.3.1.1.1.1.3.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.3">conditional-set</csymbol><apply id="S4.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1"><times id="S4.1.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.2"></times><ci id="S4.1.1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.3">𝐻</ci><apply id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2"><ci id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1">bold-^</ci><ci id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2">𝒚</ci></apply><ci id="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S4.1.1.m1.3.3.1.1.1.1.2.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2"><in id="S4.1.1.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.1"></in><ci id="S4.1.1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.2">𝑥</ci><apply id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3"><times id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.1"></times><ci id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.2">𝑁</ci><ci id="S4.1.1.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.3">𝐸</ci><apply id="S4.1.1.m1.2.2.cmml" xref="S4.1.1.m1.3.3.1.1.1.1.2.2.3.4.2"><ci id="S4.1.1.m1.2.2.1.cmml" xref="S4.1.1.m1.2.2.1">bold-^</ci><ci id="S4.1.1.m1.2.2.2.cmml" xref="S4.1.1.m1.2.2.2">𝒚</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.1.1.m1.3c">PerEnts(\bm{\hat{Y}})=max(\{H(\bm{\hat{y}}_{x})|x\in NE(\bm{\hat{y}})\})</annotation><annotation encoding="application/x-llamapun" id="S4.1.1.m1.3d">italic_P italic_e italic_r italic_E italic_n italic_t italic_s ( overbold_^ start_ARG bold_italic_Y end_ARG ) = italic_m italic_a italic_x ( { italic_H ( overbold_^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) | italic_x ∈ italic_N italic_E ( overbold_^ start_ARG bold_italic_y end_ARG ) } )</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.2">where <math alttext="NE(\bm{\hat{y}})" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.2" xref="S4.p2.1.m1.1.2.cmml"><mi id="S4.p2.1.m1.1.2.2" xref="S4.p2.1.m1.1.2.2.cmml">N</mi><mo id="S4.p2.1.m1.1.2.1" xref="S4.p2.1.m1.1.2.1.cmml">⁢</mo><mi id="S4.p2.1.m1.1.2.3" xref="S4.p2.1.m1.1.2.3.cmml">E</mi><mo id="S4.p2.1.m1.1.2.1a" xref="S4.p2.1.m1.1.2.1.cmml">⁢</mo><mrow id="S4.p2.1.m1.1.2.4.2" xref="S4.p2.1.m1.1.1.cmml"><mo id="S4.p2.1.m1.1.2.4.2.1" stretchy="false" xref="S4.p2.1.m1.1.1.cmml">(</mo><mover accent="true" id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S4.p2.1.m1.1.1.1" mathvariant="bold" xref="S4.p2.1.m1.1.1.1.cmml">^</mo></mover><mo id="S4.p2.1.m1.1.2.4.2.2" stretchy="false" xref="S4.p2.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.2.cmml" xref="S4.p2.1.m1.1.2"><times id="S4.p2.1.m1.1.2.1.cmml" xref="S4.p2.1.m1.1.2.1"></times><ci id="S4.p2.1.m1.1.2.2.cmml" xref="S4.p2.1.m1.1.2.2">𝑁</ci><ci id="S4.p2.1.m1.1.2.3.cmml" xref="S4.p2.1.m1.1.2.3">𝐸</ci><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.2.4.2"><ci id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1">bold-^</ci><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝒚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">NE(\bm{\hat{y}})</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_N italic_E ( overbold_^ start_ARG bold_italic_y end_ARG )</annotation></semantics></math> represents a set of named entity token indices in the machine-translated sentence <math alttext="\bm{\hat{y}}" class="ltx_Math" display="inline" id="S4.p2.2.m2.1"><semantics id="S4.p2.2.m2.1a"><mover accent="true" id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" id="S4.p2.2.m2.1.1.1" mathvariant="bold" xref="S4.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><ci id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1">bold-^</ci><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝒚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\bm{\hat{y}}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m2.1d">overbold_^ start_ARG bold_italic_y end_ARG</annotation></semantics></math>, predicted by a named entity recognition model. We will use the abbreviation ’PerEnts,’ to refer to our method.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.1.   Settings for Experiments</h3>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S5.F2.1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Pseudo code for the experiment data preparation. We sorted and split the data into 4 segments based on each value by MDS. Then, we sampled 2,000 sentences from each segment for fine-tuning.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We attempted to evaluate our method, which is one of the unsupervised MDSs, with various datasets. We sorted the data based on the values of each MDS and divided it into four segments to verify that each MDS is proportional to training-efficiency. If it is proportional and invariant across data domains, it can be regarded as ’robust guidance’ for unsupervised data selection. We also conducted multiple data samplings for fine-tuning to precisely assess the capabilities of MDSs. This process follows the same cycle as described in the pseudo-code, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.F2" title="Figure 2 ‣ 5.1. Settings for Experiments ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. Note that the highest segment index (3 in our case) represents data subsets with the highest values according to each MDS.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.1">Data Domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.2">Train / Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.1.2.1.1">Medical</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">200k / 25k</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.3.2.1">Travel</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.2">160k / 20k</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.4.3.1">Law</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.2">120k / 15k</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.1.5.4.1">Sports</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.2">160k / 20k</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The number of sentences of ’Korean-English Parallel Corpus of Specialized Domains’ dataset, released with train/test splits.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Models and Datasets</span>
As a pre-trained translation model, we used ’NLLB-1.3B<cite class="ltx_cite ltx_citemacro_citep">(NLLB Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib21" title="">2022</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/facebook/nllb-200-distilled-1.3B" title="">https://huggingface.co/facebook/nllb-200-distilled-1.3B</a></span></span></span>’ multilingual model. We then employed the ’Korean-English Parallel Corpus of Specialized Domains<cite class="ltx_cite ltx_citemacro_citep">(Flitto, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#biba.bib1" title="">2021</a>)</cite>
<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This research (paper) used datasets from ’The Open AI Dataset Project (AI-Hub, S. Korea)’. All data information can be accessed through ’AI-Hub (<a class="ltx_ref ltx_url ltx_font_typewriter" href="www.aihub.or.kr" title="">www.aihub.or.kr</a>)’.</span></span></span>’, published by the National Information Society Agency of South Korea, as the domain-specific dataset. Given the scarcity of open datasets in the Korean language available for public download, we adopted this approach despite its limited access being restricted to nationals. There are ’Law, Medical, Travel, Sports’ domains, showing each distribution in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.T2" title="Table 2 ‣ 5.1. Settings for Experiments ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. The ’Law’ domain consists of precedents from the Supreme Court of South Korea. The ’Sports’ domain includes various articles about international sports events. The other domains were compiled from domain-specific articles, thus containing names of locations (in the Travel domain) or names of medicines (in the Medical domain).</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Training and Hyperparameters</span>
Given the potential variability in domain-specific translation, such as extremely unique domains or low-resource environments, we randomly sampled 2,000 sentences from each segment, regarding the pre-defined seeds. We employed IA3 training<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib16" title="">2022</a>)</cite> to simulate practical fine-tuning environments. For hyperparameters, we set the epoch to 10, and the batch size to 32, and searched for the best learning rate from three options [1e-2, 2e-2, 3e-2] during each fine-tuning trial. Given that fine-tuning with a low-resource dataset might result in high variance between models, we took the average scores of three fine-tuned models, using sampled data with 3 different seeds.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.1.1.1.1" style="padding-bottom:2.5pt;">MDSs</th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T3.1.1.1.2" style="padding-bottom:2.5pt;">Average Performance</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.2.2.1"></th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.2.2.2">BLEU</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.2.2.3">ChrF++</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.2.2.4">COMET</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.3.3.1">Not fine-tuned</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.3.2">21.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.3.3">45.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.3.4">76.39</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.4.4.1">Random</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.4.2">33.71</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.4.3">56.90</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.4.4">80.71</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.5.5.1" style="padding-bottom:1.07639pt;"><span class="ltx_text ltx_font_italic" id="S5.T3.1.5.5.1.1">Supervised method</span></th>
<td class="ltx_td ltx_border_t" id="S5.T3.1.5.5.2" style="padding-bottom:1.07639pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.5.5.3" style="padding-bottom:1.07639pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.5.5.4" style="padding-bottom:1.07639pt;"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.6.6.1">EL2N <cite class="ltx_cite ltx_citemacro_citep">(Paul et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib22" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.2">34.01</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.3">57.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.6.4">80.84</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.7.7.1" style="padding-bottom:2.5pt;"><span class="ltx_text ltx_font_italic" id="S5.T3.1.7.7.1.1">Unsupervised methods</span></th>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.2" style="padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.3" style="padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.7.7.4" style="padding-bottom:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.8.8.1" style="padding-bottom:2.5pt;">Entropy <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib13" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.2" style="padding-bottom:2.5pt;">33.64</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.3" style="padding-bottom:2.5pt;">57.05</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.8.8.4" style="padding-bottom:2.5pt;">80.86</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.9.9.1" style="padding-bottom:2.5pt;">Selfsup <cite class="ltx_cite ltx_citemacro_citep">(Sorscher et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite>*</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.2" style="padding-bottom:2.5pt;">33.85</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.3" style="padding-bottom:2.5pt;">57.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.9.4" style="padding-bottom:2.5pt;">80.81</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.10.10.1" style="padding-bottom:2.5pt;">Reference-Free COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib28" title="">2021</a>)</cite>*</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.2" style="padding-bottom:2.5pt;">33.88</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.3" style="padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.3.1">57.22</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.10.10.4" style="padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.4.1">80.92</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.1.11.11.1">PerEnts (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.11.11.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.11.11.2.1">34.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.11.11.3">57.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.11.11.4">80.82</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average test-set performance across 4 domains. We divided the dataset for each domain into four segments after sorting by each MDS and sampled 2,000 sentences three times from each segment. Given our conjecture that invariance across data domains is an important characteristic of an unsupervised MDS, we reported scores fine-tuned with subsets from either the highest (3) or lowest (0), denoted with an asterisk) segment. The highest scores among the unsupervised MDSs are highlighted in bold.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S5.F3.1.g1" src="extracted/2402.19267v2/figures/res2.png" width="630"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The scores for each segment index across the four domains. The best BLEU scores among the segment indices were marked with a black star. Experimental results demonstrated that our method consistently identified the most training-efficient data by selecting the highest segment (3), whereas other methods varied by data domain.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1" style="padding-bottom:5.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S5.T4.1.1.1.2" style="padding-bottom:5.0pt;">The numbers of <span class="ltx_text ltx_font_italic" id="S5.T4.1.1.1.2.1">Correctly Guessed</span> / <span class="ltx_text ltx_font_italic" id="S5.T4.1.1.1.2.2">Newly Guessed</span> named entities</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.2.2.1"></th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.2.2">EL2N</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.2.3">Entropy</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.2.4">Selfsup</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.2.5">Reference-Free</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.2.6">PerEnts</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.3.3.1"></th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.3.2">(Supervised)</td>
<td class="ltx_td" id="S5.T4.1.3.3.3"></td>
<td class="ltx_td" id="S5.T4.1.3.3.4"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.3.5">COMET</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.3.6">(Ours)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.4.4.1">Law</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.4.2">652/3842</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.4.3">721/3788</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.4.4">589/3837</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.4.5">690/3732</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.4.4.6">702/3968</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.5.5.1">Travel</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.5.2">2242/17389</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.5.3">2079/17693</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.5.4">1610/18159</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.5.5">2035/17686</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.5.6">1944/18554</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T4.1.6.6.1">Sports</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.6.2">1822/8087</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.6.3">1841/8785</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.6.4">1875/8648</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.6.5">1900/8736</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.6.6">1922/8442</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>We observed the number of named entities that models could guess for each domain test dataset. Among the words translated by the NLLB model for each test set, named entities (NEs) were stored and classified as a ’Pre-trained Named Entities’. Additionally, NEs observed in the learning datasets created by each method were stored and classified as an ’Observed Named Entities’. If an NE inferred from a model’s test data is not present in either the Pre-trained or Observed, it is categorized as ’Newly Guessed’. Furthermore, if such a guess is accurate, it is classified as ’Correctly Guessed’.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Implementations of MDSs</span>
Since our method requires named entity recognition model in the target language, which is English in our case, we employed the ’d4data/biomedical-ner-all<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/d4data/biomedical-ner-all" title="">https://huggingface.co/d4data/biomedical-ner-all</a></span></span></span>’ fine-tuned model to capture entities in the ’medical’ domain dataset, such as names of medicines. For datasets in other domains, we used ’RashidNLP/NER-Deberta<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/RashidNLP/NER-Deberta" title="">https://huggingface.co/RashidNLP/NER-Deberta</a></span></span></span>’ model, trained with Few-NERD dataset<cite class="ltx_cite ltx_citemacro_citep">(Ding et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib6" title="">2021</a>)</cite>, which we conjectured far more comprehensive than CoNLL-2003 dataset<cite class="ltx_cite ltx_citemacro_citep">(Tjong Kim Sang and
De Meulder, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib33" title="">2003</a>)</cite>. To implement MDS Selfsup, we used the monolingual sentence embedding model ’BM-K/KoSimCSE-roberta-multitask<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/BM-K/KoSimCSE-roberta-multitask" title="">https://huggingface.co/BM-K/KoSimCSE-roberta-multitask</a></span></span></span>’, which is specialized for the Korean (source) language. Lastly, ’Unbabel/wmt23-cometkiwi-da-xl’ was employed for Reference-Free COMET<cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib28" title="">2021</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Unbabel/wmt23-cometkiwi-da-xl" title="">https://huggingface.co/Unbabel/wmt23-cometkiwi-da-xl</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.2.   Main Results</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We employed BLEU<cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib25" title="">2018</a>)</cite>, ChrF++<cite class="ltx_cite ltx_citemacro_citep">(Popović, <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib24" title="">2015</a>)</cite>, and COMET scores<cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib29" title="">2020</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We used <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Unbabel/wmt22-comet-da" title="">https://huggingface.co/Unbabel/wmt22-comet-da</a> to evaluate using COMET score.</span></span></span> for evaluation, as presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.T3" title="Table 3 ‣ 5.1. Settings for Experiments ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. The fine-tuned models were evaluated using pre-split test sets. It is important to note that, identifying the optimal value for each MDS requires access to every segment index, necessitating a complete parallel corpus for comparison. To simulate a practical strategy where access is limited, we reported averaged scores by selecting either the highest (3) or lowest (0) segment index. For instance, the domain-average score for EL2N was determined by selecting segment index 3, while for MDS Selfsup, segment index 0 was chosen.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Our method, referred to by the abbreviation ’PerEnts,’ achieved the highest BLEU score among the MDSs, even surpassing the supervised method (EL2N). Although other existing methods outperformed ours for COMET and ChrF++ scores, we propose that the BLEU score might be the most critical metric for domain-specific translation due to its ability to capture semantic distortion, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S2.T1" title="Table 1 ‣ 2. Related Works ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Additionally, to assess the robustness of the MDSs, we calculated the average scores across four different domains, as presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.F3" title="Figure 3 ‣ 5.1. Settings for Experiments ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. The best performing segment index, selected by other MDSs, was neither 0 nor 3, suggesting that these MDSs are sensitive to the data domain. We conjectured that this observation could complement the assertion by <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib31" title="">2022</a>)</cite> that ’The best selection strategy depends on the amount of initial data.’ Even though the same pre-trained weights and the same volume of data were used for each fine-tuning procedure, the data domain could play an important role as a factor. Furthermore, our selection of a well-regarded monolingual sentence embedding model<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/BM-K/KoSimCSE-roberta-multitask" title="">https://huggingface.co/BM-K/KoSimCSE-roberta-multitask</a></span></span></span> for implementing MDS Selfsup did not result in decent performance, supporting the idea that the sentence embedding model could be confounded by slight literal differences.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.3.   Experiments for Generalizability</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Fine-tuning on overly complex or specialized domains can lead to overfitting, which undermines generalization. Particularly, our method, which identifies data with complex named entities, may be prone to overfitting. To verify this, we evaluated the generalizability of each model trained with data generated by MDSs. Initially, for each test set, words translated by the NLLB model were stored and classified as a ’Pre-trained Named Entities’. Similarly, named entities identified in the training datasets selected by each MDSs were cataloged as an ’Observed Named Entities’. While translating test data, a new named entity predicted by a model, which is not in Pre-trained or Observed Named Entities, it is considered ’Newly Guessed’. If such a guess is accurate, it is deemed ’Correctly Guessed’. The counts of Newly Guessed and Correctly Guessed named entities are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.T4" title="Table 4 ‣ 5.1. Settings for Experiments ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We could observed that our method do not just memorize named entities in a given train dataset. Although obvious correlations between ’Correctly Guessed Named Entities’ were not exposed, our method can help a model to guess correct named entities, without an abuse generating named entities.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.4.   Additional Study</h3>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.1">MDSs</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T5.1.1.1.2">Averaged Performance</th>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T5.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.1.2.2.2">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.1.2.2.3">ChrF++</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.1.2.2.4">COMET</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.3.1.1">PerEnts</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.3.1.2.1">34.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.1.3">57.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.1.4">80.82</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.4.2.1">
<span class="ltx_text" id="S5.T5.1.4.2.1.1">*</span>Mean</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.2.2">33.94</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.2.3.1">57.19</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.2.4">80.82</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.5.3.1">Selfsup</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.3.2.1">33.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.3.3.1">57.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.3.4.1">80.81</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.1.6.4.1">
<span class="ltx_text" id="S5.T5.1.6.4.1.1">*</span>Multilingual</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.6.4.2">33.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.6.4.3">56.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.6.4.4">80.02</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The results of MDS variants. ’*Mean’ denotes that it averaged entropy instead of choosing max in our method(PerEnts), and ’Multilingual’ adopted a multilingual sentence embedding model for ’Selfsup’. Both variants used the same segment index to achieve the highest average performance.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Since the intuition for each MDS could be implemented in various forms, we implemented some MDS variants. e.g., adopting average entropy instead of max for our method. We also employed multilingual sentence embedding model ’sentence-transformers/LaBSE<cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#bib.bib9" title="">2022</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sentence-transformers/LaBSE" title="">https://huggingface.co/sentence-transformers/LaBSE</a></span></span></span>’ for implementing MDS Selfsup. The results are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19267v2#S5.T5" title="Table 5 ‣ 5.4. Additional Study ‣ 5. Experiments ‣ Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>. Although there were less significant degradations, it can be argued that our method’s focus on finding maximum entropy more effectively captures the ’unlearned parts.’ and it reveals a limitation in the representation ability of multilingual sentence embedding models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Limitations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We attempted to verify our method under various situations and data domains. However, it’s important to note that our experiments were conducted with a single translation direction and a single data size (2k). We acknowledge that testing on multiple translation directions and diverse amounts of datasets could potentially provide a more comprehensive validation of MDSs, including our method. Additionally, the impact of utilizing named entities may vary by language, e.g., languages that use uppercase letters. Although we recognize the importance of diverse environments and theoretical analysis, limited experiments were done based on a strategic decision to verify generalizability for practical usage. We believe that these limitations could be interesting topics for future research, exploring which measurement method can generally affect the performances of fine-tuned models.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">To identify the most training-efficient data for annotating in domain-specific machine translation, we explored various measurement methods that could serve as a benchmark for selection, collectively referred to as ’MDS.’ We recognized named entities as ’complex patterns’ requiring highly confident prediction. As a result, we introduced ’Capturing Perplexing Named Entity’ as one of the MDSs. This approach has seen effective as a guidance for selecting training data, even in unsupervised settings. Despite the common challenge of identifying effective data for annotation in deep learning—a challenge that we could not directly address in terms of the relationship between memorizable patterns and generalizability due to a lack of theoretical analysis—we hope our findings will pave the way for more in-depth research in the future.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Bibliographical References</h2>
<div class="ltx_para" id="S8.p1">
<span class="ltx_ERROR undefined" id="S8.p1.1">\c@NAT@ctr</span>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2022)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan
Ghazvininejad. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.02437" title="">In-context examples
selection for machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2019)</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1388" title="">Massively multilingual
neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 3874–3884,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahia et al. (2021)</span>
<span class="ltx_bibblock">
Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-emnlp.282" title="">The
low-resource double bind: An empirical study of pruning for low-resource
machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 3316–3333, Punta Cana, Dominican Republic. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al. (2022)</span>
<span class="ltx_bibblock">
Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Colin Cherry, Behnam
Neyshabur, and Orhan Firat. 2022.

</span>
<span class="ltx_bibblock">Data scaling laws in nmt: The effect of noise and architecture.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International Conference on Machine Learning</em>, pages
1466–1482. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (1990)</span>
<span class="ltx_bibblock">
Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra,
Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin. 1990.

</span>
<span class="ltx_bibblock">A statistical approach to machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Computational linguistics</em>, 16(2):79–85.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2021)</span>
<span class="ltx_bibblock">
Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao
Zheng, and Zhiyuan Liu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.248" title="">Few-NERD: A
few-shot named entity recognition dataset</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3198–3213,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fadaee and Monz (2018)</span>
<span class="ltx_bibblock">
Marzieh Fadaee and Christof Monz. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1040" title="">Back-translation
sampling by targeting difficult words in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 436–446, Brussels, Belgium. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman and Zhang (2020)</span>
<span class="ltx_bibblock">
Vitaly Feldman and Chiyuan Zhang. 2020.

</span>
<span class="ltx_bibblock">What neural networks memorize and why: Discovering the long tail via
influence estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in Neural Information Processing Systems</em>,
33:2881–2891.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.62" title="">Language-agnostic BERT sentence embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 878–891,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haviv et al. (2022)</span>
<span class="ltx_bibblock">
Adi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster, Yoav Goldberg, and Mor Geva.
2022.

</span>
<span class="ltx_bibblock">Understanding transformer memorization recall through idioms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2210.03588</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International Conference on Machine Learning</em>, pages
2790–2799. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank
adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al. (2021)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R Lyu, and Irwin
King. 2021.

</span>
<span class="ltx_bibblock">Self-training sampling with monolingual data uncertainty for neural
machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2106.00941</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joos (1936)</span>
<span class="ltx_bibblock">
Martin Joos. 1936.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Language</em>, <a class="ltx_ref ltx_href" href="http://www.jstor.org/stable/408930" title="">12(3):196–210</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A
simple and language independent subword tokenizer and detokenizer for neural
text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 66–71, Brussels,
Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
Bansal, and Colin A Raffel. 2022.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than
in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in Neural Information Processing Systems</em>,
35:1950–1965.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maillard et al. (2023)</span>
<span class="ltx_bibblock">
Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj
Goswami, Philipp Koehn, Angela Fan, and Francisco Guzman. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.154" title="">Small data,
big impact: Leveraging minimal data for effective machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2740–2756,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meding et al. (2022)</span>
<span class="ltx_bibblock">
Kristof Meding, Luca M. Schulze Buschoff, Robert Geirhos, and Felix A.
Wichmann. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=C_vsGwEIjAr" title="">Trivial or
impossible — dichotomous data difficulty masks model differences (on
imagenet and beyond)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mota et al. (2022)</span>
<span class="ltx_bibblock">
Pedro Mota, Vera Cabarrao, and Eduardo Farah. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.eamt-1.17" title="">Fast-paced
improvements to named entity handling for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 23rd Annual Conference of the European
Association for Machine Translation</em>, pages 141–149, Ghent, Belgium.
European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2020)</span>
<span class="ltx_bibblock">
Xuan-Phi Nguyen, Shafiq Joty, Kui Wu, and Ai Ti Aw. 2020.

</span>
<span class="ltx_bibblock">Data diversification: A simple strategy for neural machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems</em>,
33:10018–10029.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi
Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John
Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,
Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,
Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn,
Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and
Jeff Wang. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et al. (2021)</span>
<span class="ltx_bibblock">
Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Uj7pF-D-YvT" title="">Deep learning on
a data diet: Finding important examples early in training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peris and Casacuberta (2018)</span>
<span class="ltx_bibblock">
Álvaro Peris and Francisco Casacuberta. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/K18-1015" title="">Active learning for
interactive neural machine translation of data streams</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 22nd Conference on Computational Natural
Language Learning</em>, pages 151–160, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2015)</span>
<span class="ltx_bibblock">
Maja Popović. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W15-3049" title="">chrF: character
n-gram F-score for automatic MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Tenth Workshop on Statistical Machine
Translation</em>, pages 392–395, Lisbon, Portugal. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting bleu scores.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">WMT 2018</em>, page 186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahaman et al. (2019)</span>
<span class="ltx_bibblock">
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
Hamprecht, Yoshua Bengio, and Aaron Courville. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v97/rahaman19a.html" title="">On the
spectral bias of neural networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 36th International Conference on Machine
Learning</em>, volume 97 of <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">Proceedings of Machine Learning Research</em>,
pages 5301–5310. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raunak et al. (2021)</span>
<span class="ltx_bibblock">
Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.92" title="">The curious
case of hallucinations in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1172–1183, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2021)</span>
<span class="ltx_bibblock">
Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart,
Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie.
2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.111" title="">Are references
really needed? unbabel-IST 2021 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Sixth Conference on Machine Translation</em>,
pages 1030–1040, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.213" title="">COMET: A
neural framework for MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2023)</span>
<span class="ltx_bibblock">
Radhika Sharma, Pragya Katyayan, and Nisheeth Joshi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ISCON57294.2023.10111938" title="">Improving
the quality of neural machine translation through proper translation of name
entities</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">2023 6th International Conference on Information Systems and
Computer Networks (ISCON)</em>, pages 1–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorscher et al. (2022)</span>
<span class="ltx_bibblock">
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S.
Morcos. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=UmvSlP-PyV" title="">Beyond neural
scaling laws: beating power law scaling via data pruning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soto et al. (2020)</span>
<span class="ltx_bibblock">
Xabier Soto, Dimitar Shterionov, Alberto Poncelas, and Andy Way. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.359" title="">Selecting
backtranslated data from multiple sources for improved neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3898–3908, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tjong Kim Sang and
De Meulder (2003)</span>
<span class="ltx_bibblock">
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W03-0419" title="">Introduction to the
CoNLL-2003 shared task: Language-independent named entity recognition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003</em>, pages 142–147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva et al. (2019)</span>
<span class="ltx_bibblock">
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler,
Yoshua Bengio, and Geoffrey J. Gordon. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=BJlxm30cKm" title="">An empirical
study of example forgetting during deep neural network learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ugawa et al. (2018)</span>
<span class="ltx_bibblock">
Arata Ugawa, Akihiro Tamura, Takashi Ninomiya, Hiroya Takamura, and Manabu
Okumura. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/C18-1274" title="">Neural machine translation
incorporating named entity</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 27th International Conference on
Computational Linguistics</em>, pages 3240–3250, Santa Fe, New Mexico, USA.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Minghao Wu, Yitong Li, Meng Zhang, Liangyou Li, Gholamreza Haffari, and Qun
Liu. 2021.

</span>
<span class="ltx_bibblock">Uncertainty-aware balancing for multilingual and multi-domain neural
machine translation training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7291–7305.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2022)</span>
<span class="ltx_bibblock">
Shufang Xie, Yingce Xia, Lijun Wu, Yiqing Huang, Yang Fan, and Tao Qin. 2022.

</span>
<span class="ltx_bibblock">End-to-end entity-aware neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Machine Learning</em>, pages 1–23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zan et al. (2022)</span>
<span class="ltx_bibblock">
Changtong Zan, Keqin Peng, Liang Ding, Baopu Qiu, Boan Liu, Shwai He, Qingyu
Lu, Zheng Zhang, Chuang Liu, Weifeng Liu, et al. 2022.

</span>
<span class="ltx_bibblock">Vega-mt: The jd explore academy translation system for wmt22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2017)</span>
<span class="ltx_bibblock">
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Sy8gdB9xx" title="">Understanding deep
learning requires rethinking generalization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2020)</span>
<span class="ltx_bibblock">
Yang Zhao, Lu Xiang, Junnan Zhu, Jiajun Zhang, Yu Zhou, and Chengqing Zong.
2020.

</span>
<span class="ltx_bibblock">Knowledge graph enhanced neural machine translation via multi-task
learning on sub-entity granularity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 28th International Conference on
Computational Linguistics</em>, pages 4495–4505.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2020)</span>
<span class="ltx_bibblock">
Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, and Lidia S. Chao. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.620" title="">Uncertainty-aware curriculum learning for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 6934–6944, Online. Association for
Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.   Language Resource References</h2>
<div class="ltx_para" id="S9.p1">
<span class="ltx_ERROR undefined" id="S9.p1.1">\c@NAT@ctr</span>
</div>
</section>
<section class="ltx_bibliography" id="biba">
<h2 class="ltx_title ltx_title_bibliography"> </h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="biba.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flitto (2021)</span>
<span class="ltx_bibblock">
Flitto. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&amp;topMenu=100&amp;aihubDataSe=realm&amp;dataSetSn=111" title=""><em class="ltx_emph ltx_font_italic" id="biba.bib1.1.1.1">Korean-English Parallel Corpus of Specialized Domains</em></a>.

</span>
<span class="ltx_bibblock">AI Hub.

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1"></p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 24 17:23:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
