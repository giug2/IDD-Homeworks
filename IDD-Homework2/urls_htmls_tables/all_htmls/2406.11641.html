<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.11641] YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection</title><meta property="og:description" content="Predominant methods for image-based drone detection frequently rely on employing generic object detection algorithms like YOLOv5. While proficient in identifying drones against homogeneous backgrounds, these algorithmsâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.11641">

<!--Generated on Fri Jul  5 22:11:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.2" class="ltx_ERROR undefined">\AtBeginShipout</span><span id="p1.3" class="ltx_ERROR undefined">\AtBeginShipoutUpperLeft</span>
<p id="p1.1" class="ltx_p"><svg version="1.1" width="748.03" height="59.42" overflow="visible"><g transform="translate(0,59.42) scale(1,-1)"><g transform="translate(0,-1.11)"><g transform="translate(0,59.3607305936073) scale(1, -1)"><foreignObject width="748.028227480282" height="9.685900096859" overflow="visible">
<div id="p1.1.pic1.1.1" class="ltx_block ltx_parbox ltx_align_top" style="width:540.6pt;">
<p id="p1.1.pic1.1.1.1" class="ltx_p">Â©Â 2024 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes,
creating new collective works, for resale or redistribution to servers or lists,
or reuse of any copyrighted component of this work in other works.</p>
</div></foreignObject></g></g></g></svg></p>
</div>
<h1 class="ltx_title ltx_title_document">YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">Predominant methods for image-based drone detection frequently rely on employing generic object detection algorithms like YOLOv5. While proficient in identifying drones against homogeneous backgrounds, these algorithms often struggle in complex, highly textured environments. In such scenarios, drones seamlessly integrate into the background, creating camouflage effects that adversely affect the detection quality. To address this issue, we introduce a novel deep learning architecture called YOLO-FEDER FusionNet. Unlike conventional approaches, YOLO-FEDER FusionNet combines generic object detection methods with the specialized strength of camouflage object detection techniques to enhance drone detection capabilities. Comprehensive evaluations of YOLO-FEDER FusionNet show the efficiency of the proposed model and demonstrate substantial improvements in both reducing missed detections and false alarms.</span></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Index Terms<span id="p2.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰<span id="p2.1.1.1.1" class="ltx_text ltx_font_medium">
Drone detection, camouflage object detection, feature fusion, synthetic data</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Robust drone detection systems play a vital role in enhancing security systems, protecting privacy and ensuring regulatory complianceÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:90%;">. Leveraging advanced computer vision techniques, image-based drone detection establishes a proactive mechanism to analyze visual data, facilitating early threat detection, and enabling effective mitigation measures. The widespread adoption of image-based detection techniques is primarily driven by the cost-effectiveness of camera sensors, their broad availability, and their seamless integration into established security systemsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.7" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">In the field of drone detection, the processing of acquired image data typically relies on the application of generic object detection models (e.g., YOLOv5Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:90%;"> in various network configurations). These models enjoy widespread popularity due to their adeptness in balancing real-time processing speed and precision. Furthermore, generic object detection models exhibit notable effectiveness in detecting drones against homogeneous backgrounds (e.g., clear blue sky), or in scenarios where drones distinctly contrast with their surroundingsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.7" class="ltx_text" style="font-size:90%;">. However, their performance tends to decline considerably in scenarios where drones operate against complex and highly textured backgroundsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p2.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.10" class="ltx_text" style="font-size:90%;">. Our previous investigations specifically emphasized the substantial challenge inherent in detecting drones amidst or in close proximity to trees. The heterogeneous background composition, combined with difficult lighting conditions and the similarity between tree branches and drone rotor arms, facilitates a seamless integration of drones into their surroundings. The resulting camouflage effect severely impedes the capacity of generic object detection systems to accurately identify and delineate the boundaries of drones, undermining the overall detection qualityÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p2.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.13" class="ltx_text" style="font-size:90%;">. The phenomenon of camouflage effects extends beyond drone detection. For instance, it constitutes a considerable challenge in accurately detecting animals within their natural habitats, fostering the development of diverse camouflage object detection (COD) techniquesÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.p2.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.16" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S1.F1.1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2406.11641/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="388" height="253" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.1.5.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span> Visual comparison between YOLO-FEDER FusionNet (red bounding boxes) and YOLOv5l (blue bounding box), showcasing every fifth image frame. YOLO-FEDER FusionNet consistently detects the drone across all six frames, whereas YOLOv5l only identifies it in the last one.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">While COD techniques have demonstrated efficiency in animal detection, their direct transposition to drone detection is unexplored. Therefore, our study aims to assess the viability of leveraging insights from COD methods to enhance the reliability of generic drone detectors, especially in scenarios where they encounter limitations (cf. FigureÂ </span><a href="#S1.F1.1" title="Figure 1 â€£ 1 Introduction â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p3.1.2" class="ltx_text" style="font-size:90%;">). We introduce YOLO-FEDER FusionNet, a novel deep learning (DL) architecture that combines the strengths of generic object detection with the specialized capabilities of COD. Furthermore, we provide an examination of YOLO-FEDER FusionNet across diverse real-world datasets. This also entails a comparative analysis against established drone detection techniques, offering a robust evaluation of the effectiveness and performance enhancements achieved by our approach. Additionally, we introduce a simple techniques for false negative mitigation in image sequences.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">Given the limited availability of annotated real-world data for drone detection and the problem-specific nature of existing datasets, our method strategically incorporates synthetic data â€“ a prevalent practice to mitigate this scarcityÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.1.4" class="ltx_text" style="font-size:90%;">. Nevertheless, there is still a discrepancy between synthetically generated and manually labeled real-world data. Consequently, our study also addresses the gap between simulated scenarios and real-world conditions, especially focusing on the inherent bias induced by manual labeling procedures.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text" style="font-size:90%;">The remainder of the paper is structured as follows: SectionÂ </span><a href="#S2" title="2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S1.p5.1.2" class="ltx_text" style="font-size:90%;"> presents a review of the current state-of-the-art, followed by a detailed exposition of the proposed drone detection framework (SectionÂ </span><a href="#S3" title="3 Framework â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S1.p5.1.3" class="ltx_text" style="font-size:90%;">). SectionÂ </span><a href="#S4" title="4 Experimental Setup â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S1.p5.1.4" class="ltx_text" style="font-size:90%;"> describes the experimental setup including datasets, evaluation metrics, and implementation details. The results are presented and discussed in SectionÂ </span><a href="#S5" title="5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S1.p5.1.5" class="ltx_text" style="font-size:90%;">. In SectionÂ </span><a href="#S6" title="6 Conclusion â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S1.p5.1.6" class="ltx_text" style="font-size:90%;">, we draw conclusions.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">In the following, we discuss the latest advancements in image-based drone detection, focusing on their effectiveness in challenging environments. Additionally, we address the principle idea of COD along with prevalent COD techniques. Furthermore, we explore essential research and concepts related to generating synthetic data, addressing the inherent discrepancies between simulated and real-world scenarios in the context of drone detection.</span></p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Drone Detection.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:90%;">Developing precise and reliable drone detection systems is a multidimensional challenge, accommodating various interpretations, sub-problems and strategic directions. Within the domain of image-based drone detection, considerable emphasis is directed towards the detection of small dronesÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S2.SS0.SSS0.Px1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.4" class="ltx_text" style="font-size:90%;"> and accurately discerning them from other aerial entities, such as birdsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.SS0.SSS0.Px1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.7" class="ltx_text" style="font-size:90%;">. A notable deficiency persists in methodologies specifically tailored to the enhancement of drone detection amidst complex or highly textured backgroundsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S2.SS0.SSS0.Px1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.10" class="ltx_text" style="font-size:90%;">. Recent strategies addressing this challenge commonly entail the refinement of distinct modules within established object detection frameworks, such as YOLOv5Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.SS0.SSS0.Px1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.13" class="ltx_text" style="font-size:90%;">. For instance, Lv et al.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.SS0.SSS0.Px1.p1.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.16" class="ltx_text" style="font-size:90%;"> introduce SAG-YOLOv5s â€“ an adapted smaller version of YOLOv5s (denoting the small variant within the YOLOv5 series), specifically optimized for intricate environments. Their method integrates SimAM attentionÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S2.SS0.SSS0.Px1.p1.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.19" class="ltx_text" style="font-size:90%;"> and Ghost modulesÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.SS0.SSS0.Px1.p1.1.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.22" class="ltx_text" style="font-size:90%;"> into YOLOv5sâ€™ bottleneck structure, to elevate drone target extraction and refine background suppression during feature analysis. Concurrently, alternative methodologies seek to simplify the complexity of natural environments by dividing the process into two distinct stages. Typically, this involves eliminating background elementsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.23.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S2.SS0.SSS0.Px1.p1.1.24.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.25" class="ltx_text" style="font-size:90%;"> and extracting moving objectsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.26.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S2.SS0.SSS0.Px1.p1.1.27.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.28" class="ltx_text" style="font-size:90%;">, followed by a classification. Furthermore, some methodologies contemplate camera-based drone detection either as a preliminary stage in a tracking procedure or an integral part of a multi-sensor systemÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px1.p1.1.29.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.SS0.SSS0.Px1.p1.1.30.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px1.p1.1.31" class="ltx_text" style="font-size:90%;">. This strategic consideration aims to offset potential shortcomings of a purely camera-based system, augmenting its robustness especially in complex environments. However, the challenge of camouflage effects induced by natural elements like trees remains unaddressed in current drone detection strategies.</span></p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Camouflage Object Detection.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:90%;">The development of specialized methodologies dedicated to the detection of camouflage objects is an emerging field of research. Camouflage object detection (COD) embodies a class-independent detection taskÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S2.SS0.SSS0.Px2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px2.p1.1.4" class="ltx_text" style="font-size:90%;">, particularly prevalent in the domain of animal detection. Its objective is the precise identification of objects that closely replicate the inherent characteristics of their surrounding, minimizing their visual contrast and distinctiveness. The majority of COD techniques address the challenges posed by intrinsic similarity and edge disruption through emulation of the human visual systemÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.SS0.SSS0.Px2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px2.p1.1.7" class="ltx_text" style="font-size:90%;">. Only a small selection of approaches seeks to compensate for perception limitations by disassembling the camouflage scenario and emphasizing subtle distinguishing featuresÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px2.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S2.SS0.SSS0.Px2.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px2.p1.1.10" class="ltx_text" style="font-size:90%;">. A promising model belonging to the second algorithmic category is the feature decomposition and edge reconstruction (FEDER) model by He et al.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px2.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S2.SS0.SSS0.Px2.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px2.p1.1.13" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S2.F2.1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_bottom" style="width:411.9pt;"><img src="/html/2406.11641/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="388" height="179" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.1.8.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span> Overview of YOLO-FEDER FusionNet. Key components fusing and processing information from both backbones are highlighted in red. Layers are abbreviated as follows: CFE (camouflage feature encoder), CAM (channel attention module), CBS (convolution + batch normalization + SiLU activation), CBAM (convolutional block attention module), C3 (CSP bottleneck with three convolutional layers), SED (segmentation-oriented edge-assisted decoder), SPPF (spatial pyramid pooling fusion). The visualization is inspired by the illustration of the YOLOv5l architecture inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Simulation-Reality Gap.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p"><span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text" style="font-size:90%;">Leveraging synthetic data is a popular approach for training DL models in drone detectionÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S2.SS0.SSS0.Px3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.4" class="ltx_text" style="font-size:90%;"> and other application domainsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S2.SS0.SSS0.Px3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.7" class="ltx_text" style="font-size:90%;"> due to the high expenses tied to acquiring real-world data. Techniques like domain randomizationÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.SS0.SSS0.Px3.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.10" class="ltx_text" style="font-size:90%;"> or game engine-based simulationsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.SS0.SSS0.Px3.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.13" class="ltx_text" style="font-size:90%;"> facilitate the generation of extensive, domain-specific datasets. These methods demonstrate cost-effectiveness through automated labeling processes, ensuring precise annotations, unlike manual labeling techniques. Furthermore, they enable the circumvention of real-world constraints (e.g., privacy regulations), fostering dataset diversification. However, transferring detection models exclusively trained on synthetic data to real-world applications frequently leads to performance degradation, attributed to the simulation-reality gap. The gapâ€™s severity is closely linked to the quality of both synthetic and real-world data, and is typically assessed through diverse quality measures such as mAP across various intersection over union (IoU) thresholdsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.SS0.SSS0.Px3.p1.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.16" class="ltx_text" style="font-size:90%;">. Two primary strategies for narrowing this gap include fine-tuning with real-world dataÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.SS0.SSS0.Px3.p1.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.19" class="ltx_text" style="font-size:90%;"> and mixed-data trainingÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px3.p1.1.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S2.SS0.SSS0.Px3.p1.1.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS0.SSS0.Px3.p1.1.22" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p"><span id="S3.p1.2.1" class="ltx_text" style="font-size:90%;">Given the complexity inherent to drone detection, the proposed YOLO-FEDER FusionNet strategically combines the benefits of generic object detection with the specific strengths of COD algorithms. The model relies on two essential components for feature extraction: the well-established YOLOv5l backbone architectureÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.p1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.2.4" class="ltx_text" style="font-size:90%;"> and the specialized camouflage object detector FEDERÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.2.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S3.p1.2.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.2.7" class="ltx_text" style="font-size:90%;">. YOLOv5l refers to the larger model configuration within the YOLOv5 series. As both algorithms yield complementary results, the YOLOv5l backbone and the FEDER algorithm operate as an ensemble system to extract significant features. This involves parallel processing an RGB image </span><math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{X}\in\mathbb{R}^{W\times H\times 3}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">ğ—</mi><mo mathsize="90%" id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml"><mi mathsize="90%" id="S3.p1.1.m1.1.1.3.3.2" xref="S3.p1.1.m1.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1" xref="S3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.p1.1.m1.1.1.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1a" xref="S3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.p1.1.m1.1.1.3.3.4" xref="S3.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ğ—</ci><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3"><times id="S3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.1"></times><ci id="S3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.p1.1.m1.1.1.3.3.2">ğ‘Š</ci><ci id="S3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3">ğ»</ci><cn type="integer" id="S3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathbf{X}\in\mathbb{R}^{W\times H\times 3}</annotation></semantics></math><span id="S3.p1.2.8" class="ltx_text" style="font-size:90%;"> with </span><math id="S3.p1.2.m2.1" class="ltx_Math" alttext="W=H" display="inline"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">W</mi><mo mathsize="90%" id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">=</mo><mi mathsize="90%" id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><eq id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"></eq><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ‘Š</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">W=H</annotation></semantics></math><span id="S3.p1.2.9" class="ltx_text" style="font-size:90%;">, by both components (cf. FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.p1.2.10" class="ltx_text" style="font-size:90%;">). The information from both components is fused at feature level within the networkâ€™s neck, whose architectural design is inspired by YOLOv5lÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.2.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.p1.2.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.2.13" class="ltx_text" style="font-size:90%;">. The feature maps issued by the neck are processed within the networkâ€™s head to generate predictions for objects across three distinct sizes. Detailed descriptions of all network components are presented in the following sections.</span></p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">YOLOv5l Backbone.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:90%;">The employed YOLOv5l backboneÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.SS0.SSS0.Px1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px1.p1.1.4" class="ltx_text" style="font-size:90%;"> relies on CSPDarkNet53, which incorporates DarkNet-53Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.SS0.SSS0.Px1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px1.p1.1.7" class="ltx_text" style="font-size:90%;"> in conjunction with an advanced CSPNet strategyÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S3.SS0.SSS0.Px1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px1.p1.1.10" class="ltx_text" style="font-size:90%;">. The foundational architecture features a sequential arrangement of multiple CBS modules (consisting of convolutional, batch normalization, and SiLU activation layers) and C3 modules (comprising a CSP bottleneck with three convolutional layers). A spatial pyramid pooling fusion (SPPF) moduleÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.SS0.SSS0.Px1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px1.p1.1.13" class="ltx_text" style="font-size:90%;"> completes the backbone structure (see FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px1.p1.1.14" class="ltx_text" style="font-size:90%;">, bottom left).</span></p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">FEDER Backbone.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.5" class="ltx_p"><span id="S3.SS0.SSS0.Px2.p1.5.1" class="ltx_text" style="font-size:90%;">The feature decomposition and edge reconstruction (FEDER) model introduced by He et al.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S3.SS0.SSS0.Px2.p1.5.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.4" class="ltx_text" style="font-size:90%;"> consists of three main components: a camouflaged feature encoder (CFE), a deep wavelet-like decomposition (DWD) module, and a segmentation-oriented edge-assisted decoder (SED) (see FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px2.p1.5.5" class="ltx_text" style="font-size:90%;">, top left). The CFE leverages a Res2Net50Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.SS0.SSS0.Px2.p1.5.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.8" class="ltx_text" style="font-size:90%;">, in combination with R-NetÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S3.SS0.SSS0.Px2.p1.5.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.11" class="ltx_text" style="font-size:90%;">, to generate a series of feature maps given an input image </span><math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{X}\in\mathbb{R}^{W\times H\times 3}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">ğ—</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1a" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1"><in id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2">ğ—</ci><apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3">ğ»</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">\mathbf{X}\in\mathbb{R}^{W\times H\times 3}</annotation></semantics></math><span id="S3.SS0.SSS0.Px2.p1.5.12" class="ltx_text" style="font-size:90%;">, with </span><math id="S3.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="W=H" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">W</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1"></eq><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.2.m2.1c">W=H</annotation></semantics></math><span id="S3.SS0.SSS0.Px2.p1.5.13" class="ltx_text" style="font-size:90%;">. These feature maps serve as inputs for an efficient atrous spatial pyramid pooling (e-ASPP) moduleÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.SS0.SSS0.Px2.p1.5.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.16" class="ltx_text" style="font-size:90%;"> and the DWD. As discriminative attributes in COD primarily reside in high-frequency (HF) and low-frequency (LF) componentsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.SS0.SSS0.Px2.p1.5.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.19" class="ltx_text" style="font-size:90%;"> â€“ such as texture and edges (HF), as well as color and illumination (LF) â€“ the feature maps within the DWD module are partitioned into distinct HF and LF parts. The partitioning process involves employing learnable HF and LF filters, coupled with adaptive wavelet distillationÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.SS0.SSS0.Px2.p1.5.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.22" class="ltx_text" style="font-size:90%;"> for updating the coefficients. Furthermore, the DWD leverages HF and LF attention modules, alongside guidance-based feature aggregation, to systematically extract discriminative information from the decomposed features and fuse this information in a meaningful way. The features derived from the DWD and the e-ASPP are decoded via the SED. Within the SED, a reversible re-calibration segmentation (RRS) module and an ordinary differential equation (ODE)-inspired edge reconstruction (OER) module are employed for sophisticated feature processing and auxiliary edge reconstruction. The final output generated by the FEDER backbone comprises a binary segmentation map </span><math id="S3.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{O}_{S}\in\mathbb{R}^{W\times H\times 1}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><msub id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">ğ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">S</mi></msub><mo mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.1" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1a" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.4" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1"><in id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.1"></in><apply id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2">ğ</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3">ğ‘†</ci></apply><apply id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.3">ğ»</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.3.m3.1c">\mathbf{O}_{S}\in\mathbb{R}^{W\times H\times 1}</annotation></semantics></math><span id="S3.SS0.SSS0.Px2.p1.5.23" class="ltx_text" style="font-size:90%;"> and an edge prediction map </span><math id="S3.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{O}_{E}\in\mathbb{R}^{W\times H\times 1}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S3.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><msub id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml">ğ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml">E</mi></msub><mo mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1a" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.4" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1"><in id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1"></in><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2">ğ</ci><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3">ğ¸</ci></apply><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3">ğ»</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.4.m4.1c">\mathbf{O}_{E}\in\mathbb{R}^{W\times H\times 1}</annotation></semantics></math><span id="S3.SS0.SSS0.Px2.p1.5.24" class="ltx_text" style="font-size:90%;"> with </span><math id="S3.SS0.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="W=H" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">W</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.1" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml">=</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1"><eq id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.1"></eq><ci id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.5.m5.1c">W=H</annotation></semantics></math><span id="S3.SS0.SSS0.Px2.p1.5.25" class="ltx_text" style="font-size:90%;">. In YOLO-FEDER FusionNet, only the segmentation map is further processed. For more details on FEDER, refer toÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px2.p1.5.26.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S3.SS0.SSS0.Px2.p1.5.27.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px2.p1.5.28" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Neck.</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p"><span id="S3.SS0.SSS0.Px3.p1.1.1" class="ltx_text" style="font-size:90%;">The neck of YOLO-FEDER FusionNet is specifically designed to unify information from both backbones across diverse layers (cf. FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px3.p1.1.2" class="ltx_text" style="font-size:90%;">). Drawing inspiration from the foundational architecture of YOLOv5lÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px3.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.SS0.SSS0.Px3.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px3.p1.1.5" class="ltx_text" style="font-size:90%;">, the neckâ€™s architectural design features CBS, C3, and upsampling modules (akin to YOLOv5l). In addition, modified concatenation layers are incorporated to facilitate the integration of outputs from the FEDER backbone (cf. FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px3.p1.1.6" class="ltx_text" style="font-size:90%;">, red connections), effectively complementing information gathered from preceding layers (cf. FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px3.p1.1.7" class="ltx_text" style="font-size:90%;">, gray connections). Furthermore, attention mechanisms are strategically embedded at multiple positions within the networkâ€™s neck (cf. FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px3.p1.1.8" class="ltx_text" style="font-size:90%;">, red components) to enable the prioritization of significant features. Attention mechanisms frequently only concentrate on either spatial or channel-related feature relationships (cf.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px3.p1.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.SS0.SSS0.Px3.p1.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px3.p1.1.11" class="ltx_text" style="font-size:90%;">). A widely adopted attention mechanism, combining spatial and channel-wise attention, is the convolutional block attention module (CBAM) introduced by Woo et al.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px3.p1.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS0.SSS0.Px3.p1.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px3.p1.1.14" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p2.1" class="ltx_p"><span id="S3.SS0.SSS0.Px3.p2.1.1" class="ltx_text" style="font-size:90%;">Inspired by Woo et al.â€™s Res50 + CBAM modelÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px3.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS0.SSS0.Px3.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px3.p2.1.4" class="ltx_text" style="font-size:90%;">, where CBAM was embedded within the residual blocks, we integrated this module into our proposed network architecture in a similar way. Precisely, it is located within the CSP bottleneck of the C3 module (cf. FigureÂ </span><a href="#S2.F2.1" title="Figure 2 â€£ Camouflage Object Detection. â€£ 2 Related Work â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS0.SSS0.Px3.p2.1.5" class="ltx_text" style="font-size:90%;">, C3 + CBAM). Considering an intermediate feature map </span><math id="S3.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{F}\in\mathbb{R}^{W\times H\times C}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.1.m1.1a"><mrow id="S3.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.1a" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.4" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1"><in id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2">ğ…</ci><apply id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.3">ğ»</ci><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.1.m1.1c">\mathbf{F}\in\mathbb{R}^{W\times H\times C}</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p2.1.6" class="ltx_text" style="font-size:90%;"> derived from a preceding CBS module (cf. FigureÂ </span><a href="#S3.F3.1" title="Figure 3 â€£ Neck. â€£ 3 Framework â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS0.SSS0.Px3.p2.1.7" class="ltx_text" style="font-size:90%;">), the overall attention process initiated by CBAM can be described as follows:</span></p>
<p id="S3.SS0.SSS0.Px3.p2.2.1" class="ltx_p ltx_align_center"><math id="S3.SS0.SSS0.Px3.p2.2.1.m1.4" class="ltx_Math" alttext="\mathbf{F}^{\prime}=\mathbf{M}_{S}(\mathbf{M}_{C}(\mathbf{F})\otimes\mathbf{F})\otimes(\mathbf{M}_{C}(\mathbf{F})\otimes\mathbf{F})" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.2.1.m1.4a"><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.cmml"><msup id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.2.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.3.cmml">â€²</mo></msup><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.3.cmml">=</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.cmml"><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.cmml"><msub id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.3.cmml">S</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.2.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.cmml"><msub id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.3.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.3.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.3.2.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.1.1.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" rspace="0.055em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.3.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.1.cmml">âŠ—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.3.cmml">ğ…</mi></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.3.cmml">âŠ—</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.cmml">(</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.cmml"><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.cmml"><msub id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.3.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.3.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.3.2.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.2.2.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" rspace="0.055em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.3.2.2" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.1" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.1.cmml">âŠ—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.3.cmml">ğ…</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.3" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4b"><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4"><eq id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.3"></eq><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.2">ğ…</ci><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.4.3">â€²</ci></apply><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.3">tensor-product</csymbol><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1"><times id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.2"></times><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.3.3">ğ‘†</ci></apply><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.1">tensor-product</csymbol><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2"><times id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.1"></times><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.2.2.3">ğ¶</ci></apply><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.1.1">ğ…</ci></apply><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.3.3.1.1.1.1.1.3">ğ…</ci></apply></apply><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.1">tensor-product</csymbol><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2"><times id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.1"></times><apply id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.2.2.3">ğ¶</ci></apply><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.2.2">ğ…</ci></apply><ci id="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.1.m1.4.4.2.2.1.1.3">ğ…</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.2.1.m1.4c">\mathbf{F}^{\prime}=\mathbf{M}_{S}(\mathbf{M}_{C}(\mathbf{F})\otimes\mathbf{F})\otimes(\mathbf{M}_{C}(\mathbf{F})\otimes\mathbf{F})</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p2.2.1.1" class="ltx_text" style="font-size:90%;"> .</span></p>
</div>
<div id="S3.SS0.SSS0.Px3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px3.p3.8" class="ltx_p"><span id="S3.SS0.SSS0.Px3.p3.8.1" class="ltx_text" style="font-size:90%;">Here, </span><math id="S3.SS0.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{M}_{C}(\mathbf{F})\in\mathbb{R}^{1\times 1\times C}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.1.m1.1a"><mrow id="S3.SS0.SSS0.Px3.p3.1.m1.1.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.cmml"><mrow id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.cmml"><msub id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.3" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.3.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.1" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.3.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.3.2.1" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.1.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.3.2.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.1" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.cmml"><mn mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.2" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.2.cmml">1</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.1" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.3" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.3.cmml">1</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.1a" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.4" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2"><in id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.1"></in><apply id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2"><times id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.1"></times><apply id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.2.2.3">ğ¶</ci></apply><ci id="S3.SS0.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.1">ğ…</ci></apply><apply id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.2">â„</ci><apply id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3"><times id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.1"></times><cn type="integer" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.2">1</cn><cn type="integer" id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.3">1</cn><ci id="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.4.cmml" xref="S3.SS0.SSS0.Px3.p3.1.m1.1.2.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.1.m1.1c">\mathbf{M}_{C}(\mathbf{F})\in\mathbb{R}^{1\times 1\times C}</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.2" class="ltx_text" style="font-size:90%;"> represents the one-dimensional channel attention map, </span><math id="S3.SS0.SSS0.Px3.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{M}_{S}(\mathbf{F})\in\mathbb{R}^{W\times H\times 1}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.2.m2.1a"><mrow id="S3.SS0.SSS0.Px3.p3.2.m2.1.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.cmml"><mrow id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.cmml"><msub id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.3" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.3.cmml">S</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.1" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.3.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.3.2.1" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.1" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.1.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.3.2.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.1" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.2" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.1" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.3" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.1a" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.4" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.2.m2.1b"><apply id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2"><in id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.1"></in><apply id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2"><times id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.1"></times><apply id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.2.2.3">ğ‘†</ci></apply><ci id="S3.SS0.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.1">ğ…</ci></apply><apply id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.2">â„</ci><apply id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3"><times id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.1"></times><ci id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.3">ğ»</ci><cn type="integer" id="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.4.cmml" xref="S3.SS0.SSS0.Px3.p3.2.m2.1.2.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.2.m2.1c">\mathbf{M}_{S}(\mathbf{F})\in\mathbb{R}^{W\times H\times 1}</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.3" class="ltx_text" style="font-size:90%;"> the two-dimensional spatial attention map, </span><math id="S3.SS0.SSS0.Px3.p3.3.m3.1" class="ltx_Math" alttext="\otimes" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.3.m3.1a"><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.3.m3.1.1" xref="S3.SS0.SSS0.Px3.p3.3.m3.1.1.cmml">âŠ—</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.3.m3.1b"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.3.m3.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.3.m3.1c">\otimes</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.4" class="ltx_text" style="font-size:90%;"> denotes the element-wise multiplication, and </span><math id="S3.SS0.SSS0.Px3.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{F}^{\prime}\in\mathbb{R}^{W\times H\times C}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.4.m4.1a"><mrow id="S3.SS0.SSS0.Px3.p3.4.m4.1.1" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.cmml"><msup id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.2" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.2.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.3" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.3.cmml">â€²</mo></msup><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.1" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.2" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.2" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.1" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.3" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.1a" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.4" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.4.m4.1b"><apply id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1"><in id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.1"></in><apply id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.2">ğ…</ci><ci id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.2.3">â€²</ci></apply><apply id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3"><times id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.3">ğ»</ci><ci id="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px3.p3.4.m4.1.1.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.4.m4.1c">\mathbf{F}^{\prime}\in\mathbb{R}^{W\times H\times C}</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.5" class="ltx_text" style="font-size:90%;"> signifies the refined feature map. Note that during multiplication, </span><math id="S3.SS0.SSS0.Px3.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{M}_{C}(\mathbf{F})" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.5.m5.1a"><mrow id="S3.SS0.SSS0.Px3.p3.5.m5.1.2" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.cmml"><msub id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.2" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.3" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.3.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.1" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.3.2" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.3.2.1" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.5.m5.1.1" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.1.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.3.2.2" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.5.m5.1b"><apply id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2"><times id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.1"></times><apply id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.2.2.3">ğ¶</ci></apply><ci id="S3.SS0.SSS0.Px3.p3.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.5.m5.1.1">ğ…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.5.m5.1c">\mathbf{M}_{C}(\mathbf{F})</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.6" class="ltx_text" style="font-size:90%;"> is replicated along the spatial dimensions, while </span><math id="S3.SS0.SSS0.Px3.p3.6.m6.1" class="ltx_Math" alttext="\mathbf{M}_{S}(\mathbf{F})" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.6.m6.1a"><mrow id="S3.SS0.SSS0.Px3.p3.6.m6.1.2" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.cmml"><msub id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.2" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.3" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.3.cmml">S</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.1" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.3.2" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.3.2.1" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.6.m6.1.1" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.1.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.3.2.2" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.6.m6.1b"><apply id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2"><times id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.1"></times><apply id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.2.2.3">ğ‘†</ci></apply><ci id="S3.SS0.SSS0.Px3.p3.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.6.m6.1.1">ğ…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.6.m6.1c">\mathbf{M}_{S}(\mathbf{F})</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.7" class="ltx_text" style="font-size:90%;"> is duplicated along the channel dimensionÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px3.p3.8.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS0.SSS0.Px3.p3.8.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px3.p3.8.10" class="ltx_text" style="font-size:90%;">. The integration of CBAM aims to direct the modelâ€™s attention towards relevant areas and optimize its focus. Additionally, we implemented a channel-wise attention mechanism (akin to the one featured in CBAM, cf.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS0.SSS0.Px3.p3.8.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS0.SSS0.Px3.p3.8.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS0.SSS0.Px3.p3.8.13" class="ltx_text" style="font-size:90%;"> for details) following the concatenation of information across diverse layers. For instance, when linking an intermediate feature map </span><math id="S3.SS0.SSS0.Px3.p3.7.m7.1" class="ltx_Math" alttext="\mathbf{F}\in\mathbb{R}^{W\times H\times C}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.7.m7.1a"><mrow id="S3.SS0.SSS0.Px3.p3.7.m7.1.1" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.2" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.2.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.1" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.2" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.2" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.1" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.3" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.1a" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.4" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.7.m7.1b"><apply id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1"><in id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.1"></in><ci id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.2">ğ…</ci><apply id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3"><times id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.3">ğ»</ci><ci id="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px3.p3.7.m7.1.1.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.7.m7.1c">\mathbf{F}\in\mathbb{R}^{W\times H\times C}</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.14" class="ltx_text" style="font-size:90%;"> obtained by the YOLOv5l backbone with the binary segmentation map </span><math id="S3.SS0.SSS0.Px3.p3.8.m8.1" class="ltx_Math" alttext="\mathbf{O}_{S}\in\mathbb{R}^{W\times H\times 1}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p3.8.m8.1a"><mrow id="S3.SS0.SSS0.Px3.p3.8.m8.1.1" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.cmml"><msub id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.2" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.2.cmml">ğ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.3" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.3.cmml">S</mi></msub><mo mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.1" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.2" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.2.cmml">â„</mi><mrow id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.2" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.2.cmml">W</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.1" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.1.cmml">Ã—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.3" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.1a" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.1.cmml">Ã—</mo><mn mathsize="90%" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.4" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p3.8.m8.1b"><apply id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1"><in id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.1"></in><apply id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.2">ğ</ci><ci id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.2.3">ğ‘†</ci></apply><apply id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.2">â„</ci><apply id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3"><times id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.2">ğ‘Š</ci><ci id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.3">ğ»</ci><cn type="integer" id="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px3.p3.8.m8.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p3.8.m8.1c">\mathbf{O}_{S}\in\mathbb{R}^{W\times H\times 1}</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p3.8.15" class="ltx_text" style="font-size:90%;"> derived from FEDER, the instantiated attention mechanism can be described as follows:</span></p>
</div>
<div id="S3.SS0.SSS0.Px3.1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.1.1" class="ltx_p ltx_align_center"><math id="S3.SS0.SSS0.Px3.1.1.m1.4" class="ltx_Math" alttext="\mathbf{F}^{\prime}=\mathbf{M}_{C}(\mathrm{Concat}(\mathbf{F},\mathbf{O}_{S}))\otimes\mathrm{Concat}(\mathbf{F},\mathbf{O}_{S})" display="inline"><semantics id="S3.SS0.SSS0.Px3.1.1.m1.4a"><mrow id="S3.SS0.SSS0.Px3.1.1.m1.4.4" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.cmml"><msup id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.2" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.2.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.3" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.3.cmml">â€²</mo></msup><mo mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.3" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.3.cmml">=</mo><mrow id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.cmml"><mrow id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.cmml"><mrow id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.cmml"><msub id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.3.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.3.cmml">Concat</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.1.1.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">ğ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">S</mi></msub><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.4" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" rspace="0.222em" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.2.cmml">âŠ—</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.3.cmml">Concat</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.3" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.3.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.2.2" xref="S3.SS0.SSS0.Px3.1.1.m1.2.2.cmml">ğ…</mi><mo mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.2.cmml">,</mo><msub id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.2" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.2.cmml">ğ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.3" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.3.cmml">S</mi></msub><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.4" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.1.1.m1.4b"><apply id="S3.SS0.SSS0.Px3.1.1.m1.4.4.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4"><eq id="S3.SS0.SSS0.Px3.1.1.m1.4.4.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.3"></eq><apply id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.2">ğ…</ci><ci id="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.4.3">â€²</ci></apply><apply id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2"><times id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.3"></times><apply id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.2">tensor-product</csymbol><apply id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1"><times id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.2"></times><apply id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.3.3">ğ¶</ci></apply><apply id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1"><times id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.2"></times><ci id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.3">Concat</ci><interval closure="open" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1"><ci id="S3.SS0.SSS0.Px3.1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.1.1">ğ…</ci><apply id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2">ğ</ci><ci id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3">ğ‘†</ci></apply></interval></apply></apply><ci id="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.3.3.1.1.3">Concat</ci></apply><interval closure="open" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1"><ci id="S3.SS0.SSS0.Px3.1.1.m1.2.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.2.2">ğ…</ci><apply id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.2">ğ</ci><ci id="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.1.1.m1.4.4.2.2.1.1.3">ğ‘†</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.1.1.m1.4c">\mathbf{F}^{\prime}=\mathbf{M}_{C}(\mathrm{Concat}(\mathbf{F},\mathbf{O}_{S}))\otimes\mathrm{Concat}(\mathbf{F},\mathbf{O}_{S})</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<div id="S3.SS0.SSS0.Px3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px3.p4.1" class="ltx_p"><span id="S3.SS0.SSS0.Px3.p4.1.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.SS0.SSS0.Px3.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{M}_{C}(\mathbf{F})" display="inline"><semantics id="S3.SS0.SSS0.Px3.p4.1.m1.1a"><mrow id="S3.SS0.SSS0.Px3.p4.1.m1.1.2" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.cmml"><msub id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.cmml"><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.2" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.2.cmml">ğŒ</mi><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.3" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.3.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.1" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.3.2" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.3.2.1" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.SS0.SSS0.Px3.p4.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.1.cmml">ğ…</mi><mo maxsize="90%" minsize="90%" id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.3.2.2" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p4.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2"><times id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.1"></times><apply id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.2">ğŒ</ci><ci id="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.2.2.3">ğ¶</ci></apply><ci id="S3.SS0.SSS0.Px3.p4.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p4.1.m1.1.1">ğ…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p4.1.m1.1c">\mathbf{M}_{C}(\mathbf{F})</annotation></semantics></math><span id="S3.SS0.SSS0.Px3.p4.1.2" class="ltx_text" style="font-size:90%;"> is once again replicated along the spatial dimensions. This mechanism aims to account for inter-dependencies and relationships among different channels within a feature map. Therefore, it is particular beneficial when consolidating data from multiple sources via concatenation, facilitating the selection and prioritization of the most relevant details from each source.</span></p>
</div>
<figure id="S3.F3.1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2406.11641/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="388" height="126" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.1.5.1.1" class="ltx_text ltx_font_bold">Fig.Â 3</span>: </span> Integration of CBAM into the bottleneck of the C3 module, located by default in the neck and head of YOLOv5l. Modified parts are highlighted in red.</figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Head.</h4>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.1" class="ltx_p"><span id="S3.SS0.SSS0.Px4.p1.1.1" class="ltx_text" style="font-size:90%;">The head of the proposed network replicates the design of the standard YOLOv5l head. Its primary function involves predicting objects across three distinct sizes (small, medium, and large).</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">To assess the proposed framework, we employ the following experimental setup, incorporating diverse datasets and evaluation metrics.</span></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Considering the scarcity of accessible data in the context of drone detection, we utilize self-captured real-world data sourced from a potential application site for evaluation. Concurrently, we leverage synthetically generated data, derived from physically-realistic simulations, to effectively train the proposed detection model. TableÂ </span><a href="#S4.T1" title="Table 1 â€£ Synthetic Data. â€£ 4.1 Datasets â€£ 4 Experimental Setup â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p1.1.2" class="ltx_text" style="font-size:90%;"> provides an overview of the datasets employed in this study, with details discussed below. Both real and synthetically generated data have already been included in our prior workÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.SS1.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Real-World Data.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.5" class="ltx_p"><span id="S4.SS1.SSS0.Px1.p1.5.1" class="ltx_text" style="font-size:90%;">To acquire real-world data, we employ a fixed Basler acA200-165c camera system firmly stationed on the ground. The system is equipped with dual lenses (25mm and 8mm), enabling the capture of two distinct field of views from each vantage point. The selected recording environment replicates structural and environmental characteristics of a potential installation site for a drone detection system in an urban surveillance setting (cf.Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.SSS0.Px1.p1.5.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.SS1.SSS0.Px1.p1.5.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.SSS0.Px1.p1.5.4" class="ltx_text" style="font-size:90%;"> for more details). The original RGB images are recorded at a resolution of 2040</span><math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><times id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px1.p1.5.5" class="ltx_text" style="font-size:90%;">1086 pixels, leading to two distinct datasets R1 and R2 (cf. TableÂ </span><a href="#S4.T1" title="Table 1 â€£ Synthetic Data. â€£ 4.1 Datasets â€£ 4 Experimental Setup â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.SSS0.Px1.p1.5.6" class="ltx_text" style="font-size:90%;">). While the background composition in dataset R1 is characterized by an increased prevalence of building structures, highly textured objects â€“ more precisely trees â€“ form a substantial part of the image backgrounds in dataset R2. Thus, R2 exhibits a greater level of complexity in comparison to R1. Given the modelâ€™s constraint necessitating square images, a coarse cropping strategy is deployed, contingent upon the precise localization of the drone object within the image frame. Subsequently, a random cropping technique is applied using distinct dimensions: 640</span><math id="S4.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.1b"><times id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px1.p1.5.7" class="ltx_text" style="font-size:90%;">640 (YOLOv5lâ€™s default input size) and 1080</span><math id="S4.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.3.m3.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m3.1b"><times id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m3.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px1.p1.5.8" class="ltx_text" style="font-size:90%;">1080. This procedure yields two different versions of each dataset: one set featuring images at a resolution of 640</span><math id="S4.SS1.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.4.m4.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.4.m4.1b"><times id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.4.m4.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px1.p1.5.9" class="ltx_text" style="font-size:90%;">640 pixels, and another set comprising images of size 1080</span><math id="S4.SS1.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.5.m5.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.5.m5.1b"><times id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.5.m5.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px1.p1.5.10" class="ltx_text" style="font-size:90%;">1080 pixels (to amplify the informational content). Employing this systematic approach ensures the preservation of crucial information while concurrently enhancing dataset diversity. Alongside drone imagery, all datasets also include approximately 7-8Â % of background images.</span></p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Synthetic Data.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.2" class="ltx_p"><span id="S4.SS1.SSS0.Px2.p1.2.1" class="ltx_text" style="font-size:90%;">To generate synthetic training data, we employ the game engine-based data generation pipeline detailed inÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.SSS0.Px2.p1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S4.SS1.SSS0.Px2.p1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.SSS0.Px2.p1.2.4" class="ltx_text" style="font-size:90%;">. The pipeline harnesses the functionalities of the Unreal Engine version 4.27Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.SSS0.Px2.p1.2.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S4.SS1.SSS0.Px2.p1.2.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.SSS0.Px2.p1.2.7" class="ltx_text" style="font-size:90%;"> and Microsoft AirSimÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.SSS0.Px2.p1.2.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S4.SS1.SSS0.Px2.p1.2.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.SSS0.Px2.p1.2.10" class="ltx_text" style="font-size:90%;">, enabling the efficient extraction of automatically labeled RGB images. Leveraging the Urban City environmentÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.SSS0.Px2.p1.2.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S4.SS1.SSS0.Px2.p1.2.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.SSS0.Px2.p1.2.13" class="ltx_text" style="font-size:90%;">, we aim to emulate essential attributes of the application scenario defined by R1 and R2. Data collection is performed from five unique camera perspectives, employing three distinct drone models (seeÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.SSS0.Px2.p1.2.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.SS1.SSS0.Px2.p1.2.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.SSS0.Px2.p1.2.16" class="ltx_text" style="font-size:90%;"> for more details). Aligned with the characteristics of R1 and R2, synthetic RGB images are initially captured at a resolution of 2040</span><math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><times id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px2.p1.2.17" class="ltx_text" style="font-size:90%;">1080 pixels (leading to dataset S1, cf. TableÂ </span><a href="#S4.T1" title="Table 1 â€£ Synthetic Data. â€£ 4.1 Datasets â€£ 4 Experimental Setup â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.SSS0.Px2.p1.2.18" class="ltx_text" style="font-size:90%;">). Subsequently, a cropping procedure is applied (in analogy to R1 and R2) to achieve a final resolution of 640</span><math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mo mathsize="90%" id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><times id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">\times</annotation></semantics></math><span id="S4.SS1.SSS0.Px2.p1.2.19" class="ltx_text" style="font-size:90%;">640 pixels. Dataset S1 also includes a small share of background images (7-8Â %).</span></p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Overview of training and validation datasets.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â </figcaption>
<table id="S4.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.5.1.1" class="ltx_tr">
<th id="S4.T1.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.5.1.1.1.1" class="ltx_text" style="font-size:80%;">Dataset</span></th>
<th id="S4.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.5.1.1.2.1" class="ltx_text" style="font-size:80%;">Type</span></th>
<th id="S4.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3"><span id="S4.T1.5.1.1.3.1" class="ltx_text" style="font-size:80%;">Image Count</span></th>
<th id="S4.T1.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S4.T1.5.1.1.4.1" class="ltx_text" style="font-size:80%;">Camera</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.5.2.1" class="ltx_tr">
<td id="S4.T1.5.2.1.1" class="ltx_td"></td>
<th id="S4.T1.5.2.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.5.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.5.2.1.3.1" class="ltx_text" style="font-size:80%;">Train</span></th>
<th id="S4.T1.5.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.5.2.1.4.1" class="ltx_text" style="font-size:80%;">Val</span></th>
<th id="S4.T1.5.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.5.2.1.5.1" class="ltx_text" style="font-size:80%;">Test</span></th>
<th id="S4.T1.5.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.5.2.1.6.1" class="ltx_text" style="font-size:80%;">Pos.</span></th>
<th id="S4.T1.5.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.5.2.1.7.1" class="ltx_text" style="font-size:80%;">Focal Length</span></th>
</tr>
<tr id="S4.T1.5.3.2" class="ltx_tr">
<td id="S4.T1.5.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.1.1" class="ltx_text" style="font-size:80%;">R1</span></td>
<td id="S4.T1.5.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.2.1" class="ltx_text" style="font-size:80%;">real</span></td>
<td id="S4.T1.5.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.3.1" class="ltx_text" style="font-size:80%;">7,524</span></td>
<td id="S4.T1.5.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.4.1" class="ltx_text" style="font-size:80%;">2,508</span></td>
<td id="S4.T1.5.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.5.1" class="ltx_text" style="font-size:80%;">2,508</span></td>
<td id="S4.T1.5.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.6.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S4.T1.5.3.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.3.2.7.1" class="ltx_text" style="font-size:80%;">8 mm</span></td>
</tr>
<tr id="S4.T1.5.4.3" class="ltx_tr">
<td id="S4.T1.5.4.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.1.1" class="ltx_text" style="font-size:80%;">R2</span></td>
<td id="S4.T1.5.4.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.2.1" class="ltx_text" style="font-size:80%;">real</span></td>
<td id="S4.T1.5.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.3.1" class="ltx_text" style="font-size:80%;">3,834</span></td>
<td id="S4.T1.5.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.4.1" class="ltx_text" style="font-size:80%;">1,279</span></td>
<td id="S4.T1.5.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.5.1" class="ltx_text" style="font-size:80%;">1,278</span></td>
<td id="S4.T1.5.4.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.6.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S4.T1.5.4.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.4.3.7.1" class="ltx_text" style="font-size:80%;">25 mm</span></td>
</tr>
<tr id="S4.T1.5.5.4" class="ltx_tr">
<td id="S4.T1.5.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.1.1" class="ltx_text" style="font-size:80%;">S1</span></td>
<td id="S4.T1.5.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.2.1" class="ltx_text" style="font-size:80%;">synth.</span></td>
<td id="S4.T1.5.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.3.1" class="ltx_text" style="font-size:80%;">10,446</span></td>
<td id="S4.T1.5.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.4.1" class="ltx_text" style="font-size:80%;">3,483</span></td>
<td id="S4.T1.5.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.5.1" class="ltx_text" style="font-size:80%;">3,483</span></td>
<td id="S4.T1.5.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.6.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S4.T1.5.5.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.5.5.4.7.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Ensuring security against unauthorized drone intrusion necessitates precise early-stage detection. Thus, an exceptionally low false negative rate (FNR) is essential for a reliable detection system. However, in scenarios involving continuous data streams (e.g., commonly encountered in surveillance settings), detecting a drone in every frame of the sequence is not imperative. Extrapolating from adjacent frames enables (to a certain extent) partial inference of missed detections. Considering drone detection as an integral component of a comprehensive security framework, the reduction of false positives is also crucial for system credibility. This is akin to reducing the false discovery rate (FDR). Complementing the evaluation via FNR and FDR, we include the mean average precision (mAP) at an intersection over union (IoU) threshold of 0.5, due to its widespread adoption as key performance indicator for object detection models. As the requirement for precise bounding box localization can be alleviated in our application context and manually generated annotations exhibit notable variance in quality, we also consider mAP values at an IoU threshold of 0.25.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">YOLO-FEDER FusionNet is implemented in PyTorch, leveraging the foundation of the original YOLOv5 framework provided byÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S4.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.4" class="ltx_text" style="font-size:90%;">. It incorporates a YOLOv5l backbone pre-trained on the COCO benchmark datasetÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.SS3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.7" class="ltx_text" style="font-size:90%;">, as well as a FEDER network initialized with COD10KÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S4.SS3.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.10" class="ltx_text" style="font-size:90%;"> weights, both remaining frozen during training. The modelâ€™s neck and head are optimized using stochastic gradient descent (SGD) with an initial learning rate of 0.01, a momentum of 0.937, and a weight decay of 0.005. In the training phase, we maintain a batch size of 32. We assume square input images that are uniformly resized to 640</span><math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo mathsize="90%" id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><times id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\times</annotation></semantics></math><span id="S4.SS3.p1.1.11" class="ltx_text" style="font-size:90%;">640 for training and inference. We deliberately avoid letter-boxing or random resizing to handle rectangular images. Additionally, we train two standard YOLOv5l modelsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S4.SS3.p1.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.14" class="ltx_text" style="font-size:90%;"> for comparative analysis, using the same hyper-parameter configuration as for YOLO-FEDER FusionNet. The first model is trained on dataset S1 in its original, un-cropped version, as in our prior workÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.SS3.p1.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.17" class="ltx_text" style="font-size:90%;">. The second model (YOLOv5l SQ) is trained on the cropped version of S1 (cf. SectionÂ </span><a href="#S4.SS1" title="4.1 Datasets â€£ 4 Experimental Setup â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4.1</span></a><span id="S4.SS3.p1.1.18" class="ltx_text" style="font-size:90%;">). In both scenarios, no layers are frozen during the training process. All experiments are conducted on a single NVIDIA Quadro RTX-8000 GPU.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">In this section, we present the evaluation results of our proposed framework, providing an examination of its performance on real-world data, the mitigation of labeling biases through a post-processing strategy, and the assessment of the frameworkâ€™s effectiveness in an alarm scenario.</span></p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.7.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Evaluation and comparison of YOLOv5l, YOLOv5l SQ, and YOLO-FEDER FusionNet on dataset R1.</figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.4.1" class="ltx_tr">
<th id="S5.T2.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T2.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S5.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.3.4.1.2.1" class="ltx_text" style="font-size:80%;">Img Size</span></th>
<th id="S5.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T2.3.4.1.3.1" class="ltx_text" style="font-size:80%;">mAP</span></th>
<th id="S5.T2.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.3.4.1.4.1" class="ltx_text" style="font-size:80%;">FNR</span></th>
<th id="S5.T2.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.3.4.1.5.1" class="ltx_text" style="font-size:80%;">FDR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.3.5.1" class="ltx_tr">
<th id="S5.T2.3.5.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.3.5.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S5.T2.3.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T2.3.5.1.3.1" class="ltx_text" style="font-size:80%;">@0.25</span></th>
<th id="S5.T2.3.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T2.3.5.1.4.1" class="ltx_text" style="font-size:80%;">@0.5</span></th>
<th id="S5.T2.3.5.1.5" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S5.T2.3.5.1.6" class="ltx_td"></td>
</tr>
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T2.1.1.2.1" class="ltx_text" style="font-size:80%;">YOLOv5l</span></th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S5.T2.1.1.1.1" class="ltx_text" style="font-size:80%;">2040</span><math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo mathsize="80%" id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><times id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times</annotation></semantics></math><span id="S5.T2.1.1.1.2" class="ltx_text" style="font-size:80%;">1086</span>
</th>
<th id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.3.1" class="ltx_text" style="font-size:80%;">0.572</span></th>
<th id="S5.T2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.4.1" class="ltx_text" style="font-size:80%;">0.551</span></th>
<th id="S5.T2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.5.1" class="ltx_text" style="font-size:80%;">0.463</span></th>
<th id="S5.T2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.6.1" class="ltx_text" style="font-size:80%;">0.500</span></th>
</tr>
<tr id="S5.T2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.2.2.2.1" class="ltx_text" style="font-size:80%;">YOLOv5l</span></th>
<td id="S5.T2.2.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S5.T2.2.2.1.1" class="ltx_text" style="font-size:80%;">640<math id="S5.T2.2.2.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.2.2.1.1.m1.1a"><mo id="S5.T2.2.2.1.1.m1.1.1" xref="S5.T2.2.2.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.1.1.m1.1b"><times id="S5.T2.2.2.1.1.m1.1.1.cmml" xref="S5.T2.2.2.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.1.1.m1.1c">\times</annotation></semantics></math>640</span></td>
<td id="S5.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.3.1" class="ltx_text" style="font-size:80%;">0.433</span></td>
<td id="S5.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.4.1" class="ltx_text" style="font-size:80%;">0.401</span></td>
<td id="S5.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.5.1" class="ltx_text" style="font-size:80%;">0.601</span></td>
<td id="S5.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.6.1" class="ltx_text" style="font-size:80%;">0.311</span></td>
</tr>
<tr id="S5.T2.3.6.2" class="ltx_tr">
<th id="S5.T2.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.6.2.1.1" class="ltx_text" style="font-size:80%;">YOLOv5l SQ</span></th>
<td id="S5.T2.3.6.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.3.6.2.2.1" class="ltx_text" style="font-size:80%;">0.209</span></td>
<td id="S5.T2.3.6.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.3.6.2.3.1" class="ltx_text" style="font-size:80%;">0.191</span></td>
<td id="S5.T2.3.6.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.3.6.2.4.1" class="ltx_text" style="font-size:80%;">0.913</span></td>
<td id="S5.T2.3.6.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.3.6.2.5.1" class="ltx_text" style="font-size:80%;">0.033</span></td>
</tr>
<tr id="S5.T2.3.7.3" class="ltx_tr">
<th id="S5.T2.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.7.3.1.1" class="ltx_text" style="font-size:80%;">FusionNet (Ours)</span></th>
<td id="S5.T2.3.7.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.3.7.3.2.1" class="ltx_text" style="font-size:80%;">0.729</span></td>
<td id="S5.T2.3.7.3.3" class="ltx_td ltx_align_center"><span id="S5.T2.3.7.3.3.1" class="ltx_text" style="font-size:80%;">0.669</span></td>
<td id="S5.T2.3.7.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.3.7.3.4.1" class="ltx_text" style="font-size:80%;">0.372</span></td>
<td id="S5.T2.3.7.3.5" class="ltx_td ltx_align_center"><span id="S5.T2.3.7.3.5.1" class="ltx_text" style="font-size:80%;">0.114</span></td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<th id="S5.T2.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.3.3.2.1" class="ltx_text" style="font-size:80%;">YOLOv5l</span></th>
<td id="S5.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="3"><span id="S5.T2.3.3.1.1" class="ltx_text" style="font-size:80%;">1080<math id="S5.T2.3.3.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.3.3.1.1.m1.1a"><mo id="S5.T2.3.3.1.1.m1.1.1" xref="S5.T2.3.3.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.1.m1.1b"><times id="S5.T2.3.3.1.1.m1.1.1.cmml" xref="S5.T2.3.3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.1.m1.1c">\times</annotation></semantics></math>1080</span></td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.3.3.3.1" class="ltx_text" style="font-size:80%;">0.568</span></td>
<td id="S5.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.3.3.4.1" class="ltx_text" style="font-size:80%;">0.548</span></td>
<td id="S5.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.3.3.5.1" class="ltx_text" style="font-size:80%;">0.457</span></td>
<td id="S5.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.3.3.6.1" class="ltx_text" style="font-size:80%;">0.261</span></td>
</tr>
<tr id="S5.T2.3.8.4" class="ltx_tr">
<th id="S5.T2.3.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.3.8.4.1.1" class="ltx_text" style="font-size:80%;">YOLOv5l SQ</span></th>
<td id="S5.T2.3.8.4.2" class="ltx_td ltx_align_center"><span id="S5.T2.3.8.4.2.1" class="ltx_text" style="font-size:80%;">0.454</span></td>
<td id="S5.T2.3.8.4.3" class="ltx_td ltx_align_center"><span id="S5.T2.3.8.4.3.1" class="ltx_text" style="font-size:80%;">0.380</span></td>
<td id="S5.T2.3.8.4.4" class="ltx_td ltx_align_center"><span id="S5.T2.3.8.4.4.1" class="ltx_text" style="font-size:80%;">0.932</span></td>
<td id="S5.T2.3.8.4.5" class="ltx_td ltx_align_center"><span id="S5.T2.3.8.4.5.1" class="ltx_text" style="font-size:80%;">0.078</span></td>
</tr>
<tr id="S5.T2.3.9.5" class="ltx_tr">
<th id="S5.T2.3.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S5.T2.3.9.5.1.1" class="ltx_text" style="font-size:80%;">FusionNet (Ours)</span></th>
<td id="S5.T2.3.9.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.9.5.2.1" class="ltx_text" style="font-size:80%;">0.708</span></td>
<td id="S5.T2.3.9.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.9.5.3.1" class="ltx_text" style="font-size:80%;">0.636</span></td>
<td id="S5.T2.3.9.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.9.5.4.1" class="ltx_text" style="font-size:80%;">0.449</span></td>
<td id="S5.T2.3.9.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.3.9.5.5.1" class="ltx_text" style="font-size:80%;">0.066</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.7.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Evaluation and comparison of YOLOv5l, YOLOv5l SQ, and YOLO-FEDER FusionNet on dataset R2.</figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.3.4.1" class="ltx_tr">
<th id="S5.T3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S5.T3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.3.4.1.2.1" class="ltx_text" style="font-size:80%;">Img Size</span></th>
<th id="S5.T3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T3.3.4.1.3.1" class="ltx_text" style="font-size:80%;">mAP</span></th>
<th id="S5.T3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.3.4.1.4.1" class="ltx_text" style="font-size:80%;">FNR</span></th>
<th id="S5.T3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.3.4.1.5.1" class="ltx_text" style="font-size:80%;">FDR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.3.5.1" class="ltx_tr">
<th id="S5.T3.3.5.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.3.5.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S5.T3.3.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T3.3.5.1.3.1" class="ltx_text" style="font-size:80%;">@0.25</span></th>
<th id="S5.T3.3.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T3.3.5.1.4.1" class="ltx_text" style="font-size:80%;">@0.5</span></th>
<th id="S5.T3.3.5.1.5" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S5.T3.3.5.1.6" class="ltx_td"></td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<th id="S5.T3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T3.1.1.2.1" class="ltx_text" style="font-size:80%;">YOLOv5l</span></th>
<th id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S5.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">2040</span><math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo mathsize="80%" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><times id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\times</annotation></semantics></math><span id="S5.T3.1.1.1.2" class="ltx_text" style="font-size:80%;">1086</span>
</th>
<th id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.3.1" class="ltx_text" style="font-size:80%;">0.571</span></th>
<th id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.4.1" class="ltx_text" style="font-size:80%;">0.432</span></th>
<th id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.5.1" class="ltx_text" style="font-size:80%;">0.745</span></th>
<th id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.6.1" class="ltx_text" style="font-size:80%;">0.290</span></th>
</tr>
<tr id="S5.T3.2.2" class="ltx_tr">
<th id="S5.T3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.2.2.2.1" class="ltx_text" style="font-size:80%;">YOLOv5l</span></th>
<td id="S5.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S5.T3.2.2.1.1" class="ltx_text" style="font-size:80%;">640<math id="S5.T3.2.2.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.2.2.1.1.m1.1a"><mo id="S5.T3.2.2.1.1.m1.1.1" xref="S5.T3.2.2.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.1.m1.1b"><times id="S5.T3.2.2.1.1.m1.1.1.cmml" xref="S5.T3.2.2.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.1.m1.1c">\times</annotation></semantics></math>640</span></td>
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.3.1" class="ltx_text" style="font-size:80%;">0.102</span></td>
<td id="S5.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.4.1" class="ltx_text" style="font-size:80%;">0.047</span></td>
<td id="S5.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.5.1" class="ltx_text" style="font-size:80%;">0.858</span></td>
<td id="S5.T3.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.6.1" class="ltx_text" style="font-size:80%;">0.638</span></td>
</tr>
<tr id="S5.T3.3.6.2" class="ltx_tr">
<th id="S5.T3.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.6.2.1.1" class="ltx_text" style="font-size:80%;">YOLOv5l SQ</span></th>
<td id="S5.T3.3.6.2.2" class="ltx_td ltx_align_center"><span id="S5.T3.3.6.2.2.1" class="ltx_text" style="font-size:80%;">0.252</span></td>
<td id="S5.T3.3.6.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.6.2.3.1" class="ltx_text" style="font-size:80%;">0.078</span></td>
<td id="S5.T3.3.6.2.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.6.2.4.1" class="ltx_text" style="font-size:80%;">0.955</span></td>
<td id="S5.T3.3.6.2.5" class="ltx_td ltx_align_center"><span id="S5.T3.3.6.2.5.1" class="ltx_text" style="font-size:80%;">0.010</span></td>
</tr>
<tr id="S5.T3.3.7.3" class="ltx_tr">
<th id="S5.T3.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.7.3.1.1" class="ltx_text" style="font-size:80%;">FusionNet (Ours)</span></th>
<td id="S5.T3.3.7.3.2" class="ltx_td ltx_align_center"><span id="S5.T3.3.7.3.2.1" class="ltx_text" style="font-size:80%;">0.685</span></td>
<td id="S5.T3.3.7.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.7.3.3.1" class="ltx_text" style="font-size:80%;">0.270</span></td>
<td id="S5.T3.3.7.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.7.3.4.1" class="ltx_text" style="font-size:80%;">0.473</span></td>
<td id="S5.T3.3.7.3.5" class="ltx_td ltx_align_center"><span id="S5.T3.3.7.3.5.1" class="ltx_text" style="font-size:80%;">0.029</span></td>
</tr>
<tr id="S5.T3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.3.3.2.1" class="ltx_text" style="font-size:80%;">YOLOv5l</span></th>
<td id="S5.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="3"><span id="S5.T3.3.3.1.1" class="ltx_text" style="font-size:80%;">1080<math id="S5.T3.3.3.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.3.3.1.1.m1.1a"><mo id="S5.T3.3.3.1.1.m1.1.1" xref="S5.T3.3.3.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.1.1.m1.1b"><times id="S5.T3.3.3.1.1.m1.1.1.cmml" xref="S5.T3.3.3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.1.1.m1.1c">\times</annotation></semantics></math>1080</span></td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.3.3.3.1" class="ltx_text" style="font-size:80%;">0.396</span></td>
<td id="S5.T3.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.3.3.4.1" class="ltx_text" style="font-size:80%;">0.196</span></td>
<td id="S5.T3.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.3.3.5.1" class="ltx_text" style="font-size:80%;">0.698</span></td>
<td id="S5.T3.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.3.3.6.1" class="ltx_text" style="font-size:80%;">0.249</span></td>
</tr>
<tr id="S5.T3.3.8.4" class="ltx_tr">
<th id="S5.T3.3.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.8.4.1.1" class="ltx_text" style="font-size:80%;">YOLOv5l SQ</span></th>
<td id="S5.T3.3.8.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.3.8.4.2.1" class="ltx_text" style="font-size:80%;">0.343</span></td>
<td id="S5.T3.3.8.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.8.4.3.1" class="ltx_text" style="font-size:80%;">0.101</span></td>
<td id="S5.T3.3.8.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.8.4.4.1" class="ltx_text" style="font-size:80%;">0.986</span></td>
<td id="S5.T3.3.8.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.3.8.4.5.1" class="ltx_text" style="font-size:80%;">0.058</span></td>
</tr>
<tr id="S5.T3.3.9.5" class="ltx_tr">
<th id="S5.T3.3.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S5.T3.3.9.5.1.1" class="ltx_text" style="font-size:80%;">FusionNet (Ours)</span></th>
<td id="S5.T3.3.9.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.3.9.5.2.1" class="ltx_text" style="font-size:80%;">0.816</span></td>
<td id="S5.T3.3.9.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.3.9.5.3.1" class="ltx_text" style="font-size:80%;">0.423</span></td>
<td id="S5.T3.3.9.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.3.9.5.4.1" class="ltx_text" style="font-size:80%;">0.335</span></td>
<td id="S5.T3.3.9.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.3.9.5.5.1" class="ltx_text" style="font-size:80%;">0.007</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Performance on Real-World Data</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Examining the performance of YOLO-FEDER FusionNet on real-world datasets R1 and R2 (with varying cutout sizes, cf. SectionÂ </span><a href="#S4.SS1" title="4.1 Datasets â€£ 4 Experimental Setup â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4.1</span></a><span id="S5.SS1.p1.1.2" class="ltx_text" style="font-size:90%;">) reveals promising results, particularly in contrast to the original YOLOv5l model trained and assessed on 2040</span><math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mo mathsize="90%" id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><times id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\times</annotation></semantics></math><span id="S5.SS1.p1.1.3" class="ltx_text" style="font-size:90%;">1086 imagesÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS1.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S5.SS1.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS1.p1.1.6" class="ltx_text" style="font-size:90%;">. Specifically, YOLO-FEDER FusionNet exhibits a significant decline in FDR of 77.2Â % (from 0.5 to 0.114) and 86.8Â % (from 0.5 to 0.066) for dataset R1 (cf. TableÂ </span><a href="#S5.T2" title="Table 2 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.SS1.p1.1.7" class="ltx_text" style="font-size:90%;">). Additionally, an exceptional FDR reduction exceeding 90.0Â % is observed for R2 across both image sizes, with values decreasing from 0.29 to 0.029 and 0.007 (cf. TableÂ </span><a href="#S5.T3" title="Table 3 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS1.p1.1.8" class="ltx_text" style="font-size:90%;">). Furthermore, there is also a distinct reduction in FNRs.</span></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.2" class="ltx_p"><span id="S5.SS1.p2.2.1" class="ltx_text" style="font-size:90%;">The direct comparison between YOLOv5l â€“ trained on un-cropped images of S1 and evaluated on un-cropped images of R1 â€“ and YOLO-FEDER FusionNet reveals a variance in FNRs of 0.091 and 0.014, respectively. This disparity is further highlighted when contrasting the evaluation results of YOLO-FEDER FusionNet with those of YOLOv5l on cropped versions of R1 (cf. TableÂ </span><a href="#S5.T2" title="Table 2 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.SS1.p2.2.2" class="ltx_text" style="font-size:90%;">). A substantial discrepancy in FNRs becomes even more evident for dataset R2, where highly textured objects form a significant portion of the imagesâ€™ background (cf. FigureÂ </span><a href="#S1.F1.1" title="Figure 1 â€£ 1 Introduction â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S5.SS1.p2.2.3" class="ltx_text" style="font-size:90%;">). While YOLOv5l, evaluated on the original-sized R2 images (cf. 2040</span><math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mo mathsize="90%" id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><times id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\times</annotation></semantics></math><span id="S5.SS1.p2.2.4" class="ltx_text" style="font-size:90%;">1086, TableÂ </span><a href="#S5.T3" title="Table 3 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS1.p2.2.5" class="ltx_text" style="font-size:90%;">), exhibits a FNR of 0.745, YOLO-FEDER FusionNet significantly reduces this rate. Specifically, when evaluated on 1080</span><math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mo mathsize="90%" id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><times id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\times</annotation></semantics></math><span id="S5.SS1.p2.2.6" class="ltx_text" style="font-size:90%;">1080 image cutouts of R2, the FNR diminishes to less than half of its original magnitude. This observed trend remains consistent when contrasted with YOLOv5l SQ, trained similarly to YOLO-FEDER FusionNet on a cropped version of S1. However, owing to its consistently high FNRs and FDRs close to zero (cf. TablesÂ </span><a href="#S5.T2" title="Table 2 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.SS1.p2.2.7" class="ltx_text" style="font-size:90%;"> and </span><a href="#S5.T3" title="Table 3 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS1.p2.2.8" class="ltx_text" style="font-size:90%;">), YOLOv5l SQ demonstrates a general inefficiency in the present context of drone detection. On the contrary, this highlights the performance advantages attained through the integration of YOLOv5l and FEDER within YOLO-FEDER FusionNet.</span></p>
</div>
<figure id="S5.F4.1" class="ltx_figure ltx_minipage ltx_align_bottom" style="width:433.6pt;"><img src="/html/2406.11641/assets/x4.png" id="S5.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="388" height="256" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.1.5.1.1" class="ltx_text ltx_font_bold">Fig.Â 4</span>: </span> Visual comparison of the manually labeled GT bounding boxes (blue) and the bounding boxes predicted by YOLO-FEDER FusionNet (red). While the GT boxes provide a more generous encapsulation of the drone, the predicted bounding boxes demonstrate a superior level of accuracy.</figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">Analyzing the mAP values at an IoU threshold of 0.5 reveals distinct trends. Within dataset R1, there is a significant improvement in mAP, rising from 0.559 (cf. YOLOv5l, 2040</span><math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mo mathsize="90%" id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><times id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\times</annotation></semantics></math><span id="S5.SS1.p3.1.2" class="ltx_text" style="font-size:90%;">1086, TableÂ </span><a href="#S5.T2" title="Table 2 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.SS1.p3.1.3" class="ltx_text" style="font-size:90%;">) to 0.636 and 0.669 upon implementing YOLO-FEDER FusionNet. Conversely, a marginal decline in mAP values is evident within dataset R2 (cf. TableÂ </span><a href="#S5.T3" title="Table 3 â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS1.p3.1.4" class="ltx_text" style="font-size:90%;">). Upon closer examination of the mAP values at an IoU threshold of 0.25, YOLO-FEDER FusionNet demonstrates superior performance when compared to YOLOv5l. Specifically, YOLO-FEDER FusionNet exhibits mAP values ranging between 0.685 and 0.816, while YOLOv5l solely registers values below 0.572.</span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Labeling Bias</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p"><span id="S5.SS2.p1.2.1" class="ltx_text" style="font-size:90%;">Despite the precise drone localization capabilities of YOLO-FEDER FusionNet (see FigureÂ </span><a href="#S5.F4.1" title="Figure 4 â€£ 5.1 Performance on Real-World Data â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S5.SS2.p1.2.2" class="ltx_text" style="font-size:90%;">), a deeper analysis comparing predicted bounding boxes with the ground truth (GT) reveals a discrepancy in their spatial overlap. Unlike synthetic training data, which features pixel-precise labeling, the manual annotations of R1 and R2 seem to include more pixels than necessary to accurately localize a drone. Consequently, they tend to cover a slightly larger area to ensure comprehensive object encapsulation with high certainty. For instance, 72.71Â % of the predicted bounding boxes of R1 (640</span><math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mo mathsize="90%" id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><times id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\times</annotation></semantics></math><span id="S5.SS2.p1.2.3" class="ltx_text" style="font-size:90%;">640) are entirely contained within the GT. For dataset R2 (1080</span><math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mo mathsize="90%" id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><times id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\times</annotation></semantics></math><span id="S5.SS2.p1.2.4" class="ltx_text" style="font-size:90%;">1080), itâ€™s 63.45Â %. However, this bias in manual labeling significantly compromises detection quality, leading to an inferior model performance in terms of mAP (especially in scenarios involving both synthetic and real-world data).</span></p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.6.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Categorization of predicted bounding box dimensions into distinct groups, including size-dependent scaling factors for bounding box correction against labeling biases.</figcaption>
<table id="S5.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.2.3.1" class="ltx_tr">
<th id="S5.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T4.2.3.1.1.1" class="ltx_text" style="font-size:80%;">Category</span></th>
<th id="S5.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T4.2.3.1.2.1" class="ltx_text" style="font-size:80%;">Width Ratio</span></th>
<th id="S5.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T4.2.3.1.3.1" class="ltx_text" style="font-size:80%;">Height Ratio</span></th>
<th id="S5.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T4.2.3.1.4.1" class="ltx_text" style="font-size:80%;">Scaling Factors</span></th>
</tr>
<tr id="S5.T4.2.2" class="ltx_tr">
<th id="S5.T4.2.2.3" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T4.2.2.4.1" class="ltx_text" style="font-size:80%;">min</span></th>
<th id="S5.T4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T4.2.2.5.1" class="ltx_text" style="font-size:80%;">max</span></th>
<th id="S5.T4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T4.2.2.6.1" class="ltx_text" style="font-size:80%;">min</span></th>
<th id="S5.T4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T4.2.2.7.1" class="ltx_text" style="font-size:80%;">max</span></th>
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="\lambda_{w}" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><msub id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S5.T4.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.m1.1.1.2.cmml">Î»</mi><mi mathsize="80%" id="S5.T4.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T4.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.m1.1.1.2">ğœ†</ci><ci id="S5.T4.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.m1.1.1.3">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\lambda_{w}</annotation></semantics></math></th>
<th id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="\lambda_{h}" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><msub id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml"><mi mathsize="80%" id="S5.T4.2.2.2.m1.1.1.2" xref="S5.T4.2.2.2.m1.1.1.2.cmml">Î»</mi><mi mathsize="80%" id="S5.T4.2.2.2.m1.1.1.3" xref="S5.T4.2.2.2.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><apply id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.2.2.2.m1.1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T4.2.2.2.m1.1.1.2.cmml" xref="S5.T4.2.2.2.m1.1.1.2">ğœ†</ci><ci id="S5.T4.2.2.2.m1.1.1.3.cmml" xref="S5.T4.2.2.2.m1.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\lambda_{h}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.2.4.1" class="ltx_tr">
<th id="S5.T4.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T4.2.4.1.1.1" class="ltx_text" style="font-size:80%;">Extra Small</span></th>
<td id="S5.T4.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.4.1.2.1" class="ltx_text" style="font-size:80%;">0.000</span></td>
<td id="S5.T4.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.4.1.3.1" class="ltx_text" style="font-size:80%;">0.034</span></td>
<td id="S5.T4.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.4.1.4.1" class="ltx_text" style="font-size:80%;">0.000</span></td>
<td id="S5.T4.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.4.1.5.1" class="ltx_text" style="font-size:80%;">0.014</span></td>
<td id="S5.T4.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.4.1.6.1" class="ltx_text" style="font-size:80%;">0.0155</span></td>
<td id="S5.T4.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.4.1.7.1" class="ltx_text" style="font-size:80%;">0.0110</span></td>
</tr>
<tr id="S5.T4.2.5.2" class="ltx_tr">
<th id="S5.T4.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T4.2.5.2.1.1" class="ltx_text" style="font-size:80%;">Small</span></th>
<td id="S5.T4.2.5.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.5.2.2.1" class="ltx_text" style="font-size:80%;">0.034</span></td>
<td id="S5.T4.2.5.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.5.2.3.1" class="ltx_text" style="font-size:80%;">0.059</span></td>
<td id="S5.T4.2.5.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.5.2.4.1" class="ltx_text" style="font-size:80%;">0.014</span></td>
<td id="S5.T4.2.5.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.5.2.5.1" class="ltx_text" style="font-size:80%;">0.027</span></td>
<td id="S5.T4.2.5.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.5.2.6.1" class="ltx_text" style="font-size:80%;">0.0107</span></td>
<td id="S5.T4.2.5.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.5.2.7.1" class="ltx_text" style="font-size:80%;">0.0055</span></td>
</tr>
<tr id="S5.T4.2.6.3" class="ltx_tr">
<th id="S5.T4.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T4.2.6.3.1.1" class="ltx_text" style="font-size:80%;">Medium</span></th>
<td id="S5.T4.2.6.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.6.3.2.1" class="ltx_text" style="font-size:80%;">0.059</span></td>
<td id="S5.T4.2.6.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.6.3.3.1" class="ltx_text" style="font-size:80%;">0.094</span></td>
<td id="S5.T4.2.6.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.6.3.4.1" class="ltx_text" style="font-size:80%;">0.027</span></td>
<td id="S5.T4.2.6.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.6.3.5.1" class="ltx_text" style="font-size:80%;">0.044</span></td>
<td id="S5.T4.2.6.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.6.3.6.1" class="ltx_text" style="font-size:80%;">0.0071</span></td>
<td id="S5.T4.2.6.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.6.3.7.1" class="ltx_text" style="font-size:80%;">0.0020</span></td>
</tr>
<tr id="S5.T4.2.7.4" class="ltx_tr">
<th id="S5.T4.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T4.2.7.4.1.1" class="ltx_text" style="font-size:80%;">Large</span></th>
<td id="S5.T4.2.7.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.7.4.2.1" class="ltx_text" style="font-size:80%;">0.094</span></td>
<td id="S5.T4.2.7.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.7.4.3.1" class="ltx_text" style="font-size:80%;">0.144</span></td>
<td id="S5.T4.2.7.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.7.4.4.1" class="ltx_text" style="font-size:80%;">0.044</span></td>
<td id="S5.T4.2.7.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.7.4.5.1" class="ltx_text" style="font-size:80%;">0.072</span></td>
<td id="S5.T4.2.7.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.7.4.6.1" class="ltx_text" style="font-size:80%;">0.0044</span></td>
<td id="S5.T4.2.7.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.7.4.7.1" class="ltx_text" style="font-size:80%;">0.0014</span></td>
</tr>
<tr id="S5.T4.2.8.5" class="ltx_tr">
<th id="S5.T4.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.1.1" class="ltx_text" style="font-size:80%;">Extra Large</span></th>
<td id="S5.T4.2.8.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.2.1" class="ltx_text" style="font-size:80%;">0.144</span></td>
<td id="S5.T4.2.8.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.3.1" class="ltx_text" style="font-size:80%;">1.000</span></td>
<td id="S5.T4.2.8.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.4.1" class="ltx_text" style="font-size:80%;">0.072</span></td>
<td id="S5.T4.2.8.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.5.1" class="ltx_text" style="font-size:80%;">1.000</span></td>
<td id="S5.T4.2.8.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.6.1" class="ltx_text" style="font-size:80%;">0.0022</span></td>
<td id="S5.T4.2.8.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T4.2.8.5.7.1" class="ltx_text" style="font-size:80%;">0.0011</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.8.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>Mean average precision of YOLO-FEDER FusionNet on datasets R1 and R2 considering manual labeling biases.</figcaption>
<table id="S5.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.4.5.1" class="ltx_tr">
<th id="S5.T5.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T5.4.5.1.1.1" class="ltx_text" style="font-size:80%;">Dataset</span></th>
<th id="S5.T5.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T5.4.5.1.2.1" class="ltx_text" style="font-size:80%;">Img Size</span></th>
<th id="S5.T5.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T5.4.5.1.3.1" class="ltx_text" style="font-size:80%;">Labeling Bias</span></th>
<th id="S5.T5.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S5.T5.4.5.1.4.1" class="ltx_text" style="font-size:80%;">mAP</span></th>
</tr>
<tr id="S5.T5.4.6.2" class="ltx_tr">
<th id="S5.T5.4.6.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T5.4.6.2.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S5.T5.4.6.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T5.4.6.2.3.1" class="ltx_text" style="font-size:80%;">Included</span></th>
<th id="S5.T5.4.6.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T5.4.6.2.4.1" class="ltx_text" style="font-size:80%;">Type</span></th>
<th id="S5.T5.4.6.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T5.4.6.2.5.1" class="ltx_text" style="font-size:80%;">@0.25</span></th>
<th id="S5.T5.4.6.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T5.4.6.2.6.1" class="ltx_text" style="font-size:80%;">@0.5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1" class="ltx_tr">
<th id="S5.T5.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T5.1.1.2.1" class="ltx_text" style="font-size:80%;">R1</span></th>
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S5.T5.1.1.1.1" class="ltx_text" style="font-size:80%;">640<math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><times id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\times</annotation></semantics></math>640</span></td>
<td id="S5.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.1.3.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S5.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.1.4.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S5.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.1.5.1" class="ltx_text" style="font-size:80%;">0.729</span></td>
<td id="S5.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.1.6.1" class="ltx_text" style="font-size:80%;">0.669</span></td>
</tr>
<tr id="S5.T5.4.7.1" class="ltx_tr">
<td id="S5.T5.4.7.1.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.7.1.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.7.1.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.7.1.2.1" class="ltx_text" style="font-size:80%;">fixed</span></td>
<td id="S5.T5.4.7.1.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.7.1.3.1" class="ltx_text" style="font-size:80%;">0.729</span></td>
<td id="S5.T5.4.7.1.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.7.1.4.1" class="ltx_text" style="font-size:80%;">0.700</span></td>
</tr>
<tr id="S5.T5.4.8.2" class="ltx_tr">
<td id="S5.T5.4.8.2.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.8.2.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.8.2.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.8.2.2.1" class="ltx_text" style="font-size:80%;">variable</span></td>
<td id="S5.T5.4.8.2.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.8.2.3.1" class="ltx_text" style="font-size:80%;">0.729</span></td>
<td id="S5.T5.4.8.2.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.8.2.4.1" class="ltx_text" style="font-size:80%;">0.710</span></td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr">
<th id="S5.T5.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T5.2.2.2.1" class="ltx_text" style="font-size:80%;">R1</span></th>
<td id="S5.T5.2.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S5.T5.2.2.1.1" class="ltx_text" style="font-size:80%;">1080<math id="S5.T5.2.2.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T5.2.2.1.1.m1.1a"><mo id="S5.T5.2.2.1.1.m1.1.1" xref="S5.T5.2.2.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.1.m1.1b"><times id="S5.T5.2.2.1.1.m1.1.1.cmml" xref="S5.T5.2.2.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.1.m1.1c">\times</annotation></semantics></math>1080</span></td>
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.2.2.3.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S5.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.2.2.4.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S5.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.2.2.5.1" class="ltx_text" style="font-size:80%;">0.708</span></td>
<td id="S5.T5.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.2.2.6.1" class="ltx_text" style="font-size:80%;">0.636</span></td>
</tr>
<tr id="S5.T5.4.9.3" class="ltx_tr">
<td id="S5.T5.4.9.3.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.9.3.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.9.3.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.9.3.2.1" class="ltx_text" style="font-size:80%;">fixed</span></td>
<td id="S5.T5.4.9.3.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.9.3.3.1" class="ltx_text" style="font-size:80%;">0.708</span></td>
<td id="S5.T5.4.9.3.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.9.3.4.1" class="ltx_text" style="font-size:80%;">0.666</span></td>
</tr>
<tr id="S5.T5.4.10.4" class="ltx_tr">
<td id="S5.T5.4.10.4.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.10.4.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.10.4.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.10.4.2.1" class="ltx_text" style="font-size:80%;">variable</span></td>
<td id="S5.T5.4.10.4.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.10.4.3.1" class="ltx_text" style="font-size:80%;">0.708</span></td>
<td id="S5.T5.4.10.4.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.10.4.4.1" class="ltx_text" style="font-size:80%;">0.681</span></td>
</tr>
<tr id="S5.T5.3.3" class="ltx_tr">
<th id="S5.T5.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T5.3.3.2.1" class="ltx_text" style="font-size:80%;">R2</span></th>
<td id="S5.T5.3.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S5.T5.3.3.1.1" class="ltx_text" style="font-size:80%;">640<math id="S5.T5.3.3.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T5.3.3.1.1.m1.1a"><mo id="S5.T5.3.3.1.1.m1.1.1" xref="S5.T5.3.3.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.1.1.m1.1b"><times id="S5.T5.3.3.1.1.m1.1.1.cmml" xref="S5.T5.3.3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.1.1.m1.1c">\times</annotation></semantics></math>640</span></td>
<td id="S5.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.3.3.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S5.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.3.4.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S5.T5.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.3.5.1" class="ltx_text" style="font-size:80%;">0.685</span></td>
<td id="S5.T5.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.3.6.1" class="ltx_text" style="font-size:80%;">0.270</span></td>
</tr>
<tr id="S5.T5.4.11.5" class="ltx_tr">
<td id="S5.T5.4.11.5.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.11.5.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.11.5.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.11.5.2.1" class="ltx_text" style="font-size:80%;">fixed</span></td>
<td id="S5.T5.4.11.5.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.11.5.3.1" class="ltx_text" style="font-size:80%;">0.714</span></td>
<td id="S5.T5.4.11.5.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.11.5.4.1" class="ltx_text" style="font-size:80%;">0.317</span></td>
</tr>
<tr id="S5.T5.4.12.6" class="ltx_tr">
<td id="S5.T5.4.12.6.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.12.6.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.12.6.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.12.6.2.1" class="ltx_text" style="font-size:80%;">variable</span></td>
<td id="S5.T5.4.12.6.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.12.6.3.1" class="ltx_text" style="font-size:80%;">0.720</span></td>
<td id="S5.T5.4.12.6.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.12.6.4.1" class="ltx_text" style="font-size:80%;">0.416</span></td>
</tr>
<tr id="S5.T5.4.4" class="ltx_tr">
<th id="S5.T5.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="3"><span id="S5.T5.4.4.2.1" class="ltx_text" style="font-size:80%;">R2</span></th>
<td id="S5.T5.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="3"><span id="S5.T5.4.4.1.1" class="ltx_text" style="font-size:80%;">1080<math id="S5.T5.4.4.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T5.4.4.1.1.m1.1a"><mo id="S5.T5.4.4.1.1.m1.1.1" xref="S5.T5.4.4.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.1.1.m1.1b"><times id="S5.T5.4.4.1.1.m1.1.1.cmml" xref="S5.T5.4.4.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.1.1.m1.1c">\times</annotation></semantics></math>1080</span></td>
<td id="S5.T5.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.4.4.3.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S5.T5.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.4.4.4.1" class="ltx_text" style="font-size:80%;">â€“</span></td>
<td id="S5.T5.4.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.4.4.5.1" class="ltx_text" style="font-size:80%;">0.816</span></td>
<td id="S5.T5.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.4.4.6.1" class="ltx_text" style="font-size:80%;">0.423</span></td>
</tr>
<tr id="S5.T5.4.13.7" class="ltx_tr">
<td id="S5.T5.4.13.7.1" class="ltx_td ltx_align_center"><span id="S5.T5.4.13.7.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.13.7.2" class="ltx_td ltx_align_center"><span id="S5.T5.4.13.7.2.1" class="ltx_text" style="font-size:80%;">fixed</span></td>
<td id="S5.T5.4.13.7.3" class="ltx_td ltx_align_center"><span id="S5.T5.4.13.7.3.1" class="ltx_text" style="font-size:80%;">0.827</span></td>
<td id="S5.T5.4.13.7.4" class="ltx_td ltx_align_center"><span id="S5.T5.4.13.7.4.1" class="ltx_text" style="font-size:80%;">0.472</span></td>
</tr>
<tr id="S5.T5.4.14.8" class="ltx_tr">
<td id="S5.T5.4.14.8.1" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.4.14.8.1.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S5.T5.4.14.8.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.4.14.8.2.1" class="ltx_text" style="font-size:80%;">variable</span></td>
<td id="S5.T5.4.14.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.4.14.8.3.1" class="ltx_text" style="font-size:80%;">0.836</span></td>
<td id="S5.T5.4.14.8.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.4.14.8.4.1" class="ltx_text" style="font-size:80%;">0.701</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.12" class="ltx_p"><span id="S5.SS2.p2.12.1" class="ltx_text" style="font-size:90%;">To address this issue, we propose the integration of a post-processing strategy designed to compensate for deviations stemming from manual labeling. A key advantage of this strategy lies in its capacity to obviate the necessity for modifying existing datasets and undergoing re-training. The process entails the refinement of predicted bounding boxes, characterized by width </span><math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi mathsize="90%" id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">ğ‘¤</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">w</annotation></semantics></math><span id="S5.SS2.p2.12.2" class="ltx_text" style="font-size:90%;"> and height </span><math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mi mathsize="90%" id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">h</annotation></semantics></math><span id="S5.SS2.p2.12.3" class="ltx_text" style="font-size:90%;">, through a formal bias compensation approach: </span><math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="w^{\prime}=w+\lambda_{w}(w\cdot h)" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mrow id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><msup id="S5.SS2.p2.3.m3.1.1.3" xref="S5.SS2.p2.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S5.SS2.p2.3.m3.1.1.3.2" xref="S5.SS2.p2.3.m3.1.1.3.2.cmml">w</mi><mo mathsize="90%" id="S5.SS2.p2.3.m3.1.1.3.3" xref="S5.SS2.p2.3.m3.1.1.3.3.cmml">â€²</mo></msup><mo mathsize="90%" id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">=</mo><mrow id="S5.SS2.p2.3.m3.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.3.m3.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.3.cmml">w</mi><mo mathsize="90%" id="S5.SS2.p2.3.m3.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.2.cmml">+</mo><mrow id="S5.SS2.p2.3.m3.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.cmml"><msub id="S5.SS2.p2.3.m3.1.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.3.cmml"><mi mathsize="90%" id="S5.SS2.p2.3.m3.1.1.1.1.3.2" xref="S5.SS2.p2.3.m3.1.1.1.1.3.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.3.m3.1.1.1.1.3.3" xref="S5.SS2.p2.3.m3.1.1.1.1.3.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.3.m3.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.2.cmml">â€‹</mo><mrow id="S5.SS2.p2.3.m3.1.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.2" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.2.cmml">w</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.1.cmml">â‹…</mo><mi mathsize="90%" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.3.cmml">h</mi></mrow><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.3.m3.1.1.1.1.1.1.3" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><eq id="S5.SS2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.2"></eq><apply id="S5.SS2.p2.3.m3.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.3.1.cmml" xref="S5.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.3.2.cmml" xref="S5.SS2.p2.3.m3.1.1.3.2">ğ‘¤</ci><ci id="S5.SS2.p2.3.m3.1.1.3.3.cmml" xref="S5.SS2.p2.3.m3.1.1.3.3">â€²</ci></apply><apply id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1"><plus id="S5.SS2.p2.3.m3.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.2"></plus><ci id="S5.SS2.p2.3.m3.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.3">ğ‘¤</ci><apply id="S5.SS2.p2.3.m3.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1"><times id="S5.SS2.p2.3.m3.1.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.2"></times><apply id="S5.SS2.p2.3.m3.1.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.1.3.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.3">subscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.1.1.3.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.3.2">ğœ†</ci><ci id="S5.SS2.p2.3.m3.1.1.1.1.3.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.3.3">ğ‘¤</ci></apply><apply id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1"><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.1">â‹…</ci><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.2">ğ‘¤</ci><ci id="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.1.1.1.1.1.3">â„</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">w^{\prime}=w+\lambda_{w}(w\cdot h)</annotation></semantics></math><span id="S5.SS2.p2.12.4" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="h^{\prime}=h+\lambda_{h}(w\cdot h)" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mrow id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml"><msup id="S5.SS2.p2.4.m4.1.1.3" xref="S5.SS2.p2.4.m4.1.1.3.cmml"><mi mathsize="90%" id="S5.SS2.p2.4.m4.1.1.3.2" xref="S5.SS2.p2.4.m4.1.1.3.2.cmml">h</mi><mo mathsize="90%" id="S5.SS2.p2.4.m4.1.1.3.3" xref="S5.SS2.p2.4.m4.1.1.3.3.cmml">â€²</mo></msup><mo mathsize="90%" id="S5.SS2.p2.4.m4.1.1.2" xref="S5.SS2.p2.4.m4.1.1.2.cmml">=</mo><mrow id="S5.SS2.p2.4.m4.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.4.m4.1.1.1.3" xref="S5.SS2.p2.4.m4.1.1.1.3.cmml">h</mi><mo mathsize="90%" id="S5.SS2.p2.4.m4.1.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.2.cmml">+</mo><mrow id="S5.SS2.p2.4.m4.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.cmml"><msub id="S5.SS2.p2.4.m4.1.1.1.1.3" xref="S5.SS2.p2.4.m4.1.1.1.1.3.cmml"><mi mathsize="90%" id="S5.SS2.p2.4.m4.1.1.1.1.3.2" xref="S5.SS2.p2.4.m4.1.1.1.1.3.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.4.m4.1.1.1.1.3.3" xref="S5.SS2.p2.4.m4.1.1.1.1.3.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.4.m4.1.1.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.1.2.cmml">â€‹</mo><mrow id="S5.SS2.p2.4.m4.1.1.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.4.m4.1.1.1.1.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.2" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.2.cmml">w</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml">â‹…</mo><mi mathsize="90%" id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.3" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.3.cmml">h</mi></mrow><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.4.m4.1.1.1.1.1.1.3" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><apply id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1"><eq id="S5.SS2.p2.4.m4.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.2"></eq><apply id="S5.SS2.p2.4.m4.1.1.3.cmml" xref="S5.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.1.1.3.1.cmml" xref="S5.SS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S5.SS2.p2.4.m4.1.1.3.2.cmml" xref="S5.SS2.p2.4.m4.1.1.3.2">â„</ci><ci id="S5.SS2.p2.4.m4.1.1.3.3.cmml" xref="S5.SS2.p2.4.m4.1.1.3.3">â€²</ci></apply><apply id="S5.SS2.p2.4.m4.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1"><plus id="S5.SS2.p2.4.m4.1.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.2"></plus><ci id="S5.SS2.p2.4.m4.1.1.1.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.3">â„</ci><apply id="S5.SS2.p2.4.m4.1.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1"><times id="S5.SS2.p2.4.m4.1.1.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.2"></times><apply id="S5.SS2.p2.4.m4.1.1.1.1.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m4.1.1.1.1.3.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.3">subscript</csymbol><ci id="S5.SS2.p2.4.m4.1.1.1.1.3.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.3.2">ğœ†</ci><ci id="S5.SS2.p2.4.m4.1.1.1.1.3.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.3.3">â„</ci></apply><apply id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1"><ci id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.1">â‹…</ci><ci id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.2">ğ‘¤</ci><ci id="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.4.m4.1.1.1.1.1.1.1.3">â„</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">h^{\prime}=h+\lambda_{h}(w\cdot h)</annotation></semantics></math><span id="S5.SS2.p2.12.5" class="ltx_text" style="font-size:90%;">, where </span><math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="w^{\prime}" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><msup id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.5.m5.1.1.2" xref="S5.SS2.p2.5.m5.1.1.2.cmml">w</mi><mo mathsize="90%" id="S5.SS2.p2.5.m5.1.1.3" xref="S5.SS2.p2.5.m5.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><apply id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">superscript</csymbol><ci id="S5.SS2.p2.5.m5.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.2">ğ‘¤</ci><ci id="S5.SS2.p2.5.m5.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">w^{\prime}</annotation></semantics></math><span id="S5.SS2.p2.12.6" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p2.6.m6.1" class="ltx_Math" alttext="h^{\prime}" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><msup id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.6.m6.1.1.2" xref="S5.SS2.p2.6.m6.1.1.2.cmml">h</mi><mo mathsize="90%" id="S5.SS2.p2.6.m6.1.1.3" xref="S5.SS2.p2.6.m6.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><apply id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1">superscript</csymbol><ci id="S5.SS2.p2.6.m6.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.2">â„</ci><ci id="S5.SS2.p2.6.m6.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">h^{\prime}</annotation></semantics></math><span id="S5.SS2.p2.12.7" class="ltx_text" style="font-size:90%;"> signify the adjusted bounding box width and height. The scaling factors </span><math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="\lambda_{w}" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><msub id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.7.m7.1.1.2" xref="S5.SS2.p2.7.m7.1.1.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.7.m7.1.1.3" xref="S5.SS2.p2.7.m7.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><apply id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.7.m7.1.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S5.SS2.p2.7.m7.1.1.2.cmml" xref="S5.SS2.p2.7.m7.1.1.2">ğœ†</ci><ci id="S5.SS2.p2.7.m7.1.1.3.cmml" xref="S5.SS2.p2.7.m7.1.1.3">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">\lambda_{w}</annotation></semantics></math><span id="S5.SS2.p2.12.8" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p2.8.m8.1" class="ltx_Math" alttext="\lambda_{h}" display="inline"><semantics id="S5.SS2.p2.8.m8.1a"><msub id="S5.SS2.p2.8.m8.1.1" xref="S5.SS2.p2.8.m8.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.8.m8.1.1.2" xref="S5.SS2.p2.8.m8.1.1.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.8.m8.1.1.3" xref="S5.SS2.p2.8.m8.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><apply id="S5.SS2.p2.8.m8.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.8.m8.1.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S5.SS2.p2.8.m8.1.1.2.cmml" xref="S5.SS2.p2.8.m8.1.1.2">ğœ†</ci><ci id="S5.SS2.p2.8.m8.1.1.3.cmml" xref="S5.SS2.p2.8.m8.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">\lambda_{h}</annotation></semantics></math><span id="S5.SS2.p2.12.9" class="ltx_text" style="font-size:90%;"> can be tailored individually. In our evaluation, we considered both fixed factors (</span><math id="S5.SS2.p2.9.m9.1" class="ltx_Math" alttext="\lambda_{w}=0.0057" display="inline"><semantics id="S5.SS2.p2.9.m9.1a"><mrow id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml"><msub id="S5.SS2.p2.9.m9.1.1.2" xref="S5.SS2.p2.9.m9.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p2.9.m9.1.1.2.2" xref="S5.SS2.p2.9.m9.1.1.2.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.9.m9.1.1.2.3" xref="S5.SS2.p2.9.m9.1.1.2.3.cmml">w</mi></msub><mo mathsize="90%" id="S5.SS2.p2.9.m9.1.1.1" xref="S5.SS2.p2.9.m9.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p2.9.m9.1.1.3" xref="S5.SS2.p2.9.m9.1.1.3.cmml">0.0057</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.1b"><apply id="S5.SS2.p2.9.m9.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1"><eq id="S5.SS2.p2.9.m9.1.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1.1"></eq><apply id="S5.SS2.p2.9.m9.1.1.2.cmml" xref="S5.SS2.p2.9.m9.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.9.m9.1.1.2.1.cmml" xref="S5.SS2.p2.9.m9.1.1.2">subscript</csymbol><ci id="S5.SS2.p2.9.m9.1.1.2.2.cmml" xref="S5.SS2.p2.9.m9.1.1.2.2">ğœ†</ci><ci id="S5.SS2.p2.9.m9.1.1.2.3.cmml" xref="S5.SS2.p2.9.m9.1.1.2.3">ğ‘¤</ci></apply><cn type="float" id="S5.SS2.p2.9.m9.1.1.3.cmml" xref="S5.SS2.p2.9.m9.1.1.3">0.0057</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.1c">\lambda_{w}=0.0057</annotation></semantics></math><span id="S5.SS2.p2.12.10" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p2.10.m10.1" class="ltx_Math" alttext="\lambda_{h}=0.0023" display="inline"><semantics id="S5.SS2.p2.10.m10.1a"><mrow id="S5.SS2.p2.10.m10.1.1" xref="S5.SS2.p2.10.m10.1.1.cmml"><msub id="S5.SS2.p2.10.m10.1.1.2" xref="S5.SS2.p2.10.m10.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p2.10.m10.1.1.2.2" xref="S5.SS2.p2.10.m10.1.1.2.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.10.m10.1.1.2.3" xref="S5.SS2.p2.10.m10.1.1.2.3.cmml">h</mi></msub><mo mathsize="90%" id="S5.SS2.p2.10.m10.1.1.1" xref="S5.SS2.p2.10.m10.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p2.10.m10.1.1.3" xref="S5.SS2.p2.10.m10.1.1.3.cmml">0.0023</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.10.m10.1b"><apply id="S5.SS2.p2.10.m10.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1"><eq id="S5.SS2.p2.10.m10.1.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1.1"></eq><apply id="S5.SS2.p2.10.m10.1.1.2.cmml" xref="S5.SS2.p2.10.m10.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m10.1.1.2.1.cmml" xref="S5.SS2.p2.10.m10.1.1.2">subscript</csymbol><ci id="S5.SS2.p2.10.m10.1.1.2.2.cmml" xref="S5.SS2.p2.10.m10.1.1.2.2">ğœ†</ci><ci id="S5.SS2.p2.10.m10.1.1.2.3.cmml" xref="S5.SS2.p2.10.m10.1.1.2.3">â„</ci></apply><cn type="float" id="S5.SS2.p2.10.m10.1.1.3.cmml" xref="S5.SS2.p2.10.m10.1.1.3">0.0023</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.10.m10.1c">\lambda_{h}=0.0023</annotation></semantics></math><span id="S5.SS2.p2.12.11" class="ltx_text" style="font-size:90%;">) and adaptive scaling factors linked to the objectâ€™s size (cf. TableÂ </span><a href="#S5.T4" title="Table 4 â€£ 5.2 Labeling Bias â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S5.SS2.p2.12.12" class="ltx_text" style="font-size:90%;">). Notably, </span><math id="S5.SS2.p2.11.m11.1" class="ltx_Math" alttext="\lambda_{w}" display="inline"><semantics id="S5.SS2.p2.11.m11.1a"><msub id="S5.SS2.p2.11.m11.1.1" xref="S5.SS2.p2.11.m11.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.11.m11.1.1.2" xref="S5.SS2.p2.11.m11.1.1.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.11.m11.1.1.3" xref="S5.SS2.p2.11.m11.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.11.m11.1b"><apply id="S5.SS2.p2.11.m11.1.1.cmml" xref="S5.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.11.m11.1.1.1.cmml" xref="S5.SS2.p2.11.m11.1.1">subscript</csymbol><ci id="S5.SS2.p2.11.m11.1.1.2.cmml" xref="S5.SS2.p2.11.m11.1.1.2">ğœ†</ci><ci id="S5.SS2.p2.11.m11.1.1.3.cmml" xref="S5.SS2.p2.11.m11.1.1.3">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.11.m11.1c">\lambda_{w}</annotation></semantics></math><span id="S5.SS2.p2.12.13" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p2.12.m12.1" class="ltx_Math" alttext="\lambda_{h}" display="inline"><semantics id="S5.SS2.p2.12.m12.1a"><msub id="S5.SS2.p2.12.m12.1.1" xref="S5.SS2.p2.12.m12.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.12.m12.1.1.2" xref="S5.SS2.p2.12.m12.1.1.2.cmml">Î»</mi><mi mathsize="90%" id="S5.SS2.p2.12.m12.1.1.3" xref="S5.SS2.p2.12.m12.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.12.m12.1b"><apply id="S5.SS2.p2.12.m12.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.12.m12.1.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1">subscript</csymbol><ci id="S5.SS2.p2.12.m12.1.1.2.cmml" xref="S5.SS2.p2.12.m12.1.1.2">ğœ†</ci><ci id="S5.SS2.p2.12.m12.1.1.3.cmml" xref="S5.SS2.p2.12.m12.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.12.m12.1c">\lambda_{h}</annotation></semantics></math><span id="S5.SS2.p2.12.14" class="ltx_text" style="font-size:90%;"> diminish with larger object sizes, as smaller objects tend to exhibit more pronounced labeling bias due to the intricacy involved in their labeling process.</span></p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text" style="font-size:90%;">As illustrated in TableÂ </span><a href="#S5.T5" title="Table 5 â€£ 5.2 Labeling Bias â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S5.SS2.p3.1.2" class="ltx_text" style="font-size:90%;">, accounting for manual labeling bias enhances the mAP, especially at an IoU threshold of 0.5. A considerable improvement can be observed for dataset R2, suggesting an impact of background complexity on the extent of the labeling bias. Thus, addressing this bias seems to be particularly beneficial in scenarios characterized by intricate or highly textured backgrounds, such as trees.</span></p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Drone Detection in an Alarm Scenario</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">Drone detection can also be seen as an integral component of a comprehensive security system, specifically targeting the identification of potential drone threats and the subsequent activation of warning mechanisms. Hence, inferring the existence of a drone within a video sequence does not necessarily mandate a frame-by-frame detection. Alternatively, the presence of a drone can be inferred based on a partial sequence of frames, where its appearance in at least one frame indicates its existence. This strategy leads to a decline of missed detections (cf. TableÂ </span><a href="#S5.T6" title="Table 6 â€£ 5.3 Drone Detection in an Alarm Scenario â€£ 5 Results â€£ YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S5.SS3.p1.1.2" class="ltx_text" style="font-size:90%;">), albeit at the expense of an increased inference time.</span></p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T6.7.1.1" class="ltx_text ltx_font_bold">Table 6</span>: </span>Evolution of FNR for individual camera positions of dataset R2 (1080<math id="S5.T6.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T6.2.m1.1b"><mo id="S5.T6.2.m1.1.1" xref="S5.T6.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.m1.1c"><times id="S5.T6.2.m1.1.1.cmml" xref="S5.T6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.m1.1d">\times</annotation></semantics></math>1080) relative to the sequence size.</figcaption>
<table id="S5.T6.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.8.1.1" class="ltx_tr">
<th id="S5.T6.8.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T6.8.1.1.1.1" class="ltx_text" style="font-size:80%;">Camera Pos.</span></th>
<td id="S5.T6.8.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="6"><span id="S5.T6.8.1.1.2.1" class="ltx_text" style="font-size:80%;">Sequence Size (# Frames)</span></td>
</tr>
<tr id="S5.T6.8.2.2" class="ltx_tr">
<th id="S5.T6.8.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T6.8.2.2.2" class="ltx_td ltx_align_center"><span id="S5.T6.8.2.2.2.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S5.T6.8.2.2.3" class="ltx_td ltx_align_center"><span id="S5.T6.8.2.2.3.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S5.T6.8.2.2.4" class="ltx_td ltx_align_center"><span id="S5.T6.8.2.2.4.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S5.T6.8.2.2.5" class="ltx_td ltx_align_center"><span id="S5.T6.8.2.2.5.1" class="ltx_text" style="font-size:80%;">17</span></td>
<td id="S5.T6.8.2.2.6" class="ltx_td ltx_align_center"><span id="S5.T6.8.2.2.6.1" class="ltx_text" style="font-size:80%;">21</span></td>
<td id="S5.T6.8.2.2.7" class="ltx_td ltx_align_center"><span id="S5.T6.8.2.2.7.1" class="ltx_text" style="font-size:80%;">27</span></td>
</tr>
<tr id="S5.T6.8.3.3" class="ltx_tr">
<th id="S5.T6.8.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T6.8.3.3.1.1" class="ltx_text" style="font-size:80%;">POS1</span></th>
<td id="S5.T6.8.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T6.8.3.3.2.1" class="ltx_text" style="font-size:80%;">0.269</span></td>
<td id="S5.T6.8.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T6.8.3.3.3.1" class="ltx_text" style="font-size:80%;">0.153</span></td>
<td id="S5.T6.8.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T6.8.3.3.4.1" class="ltx_text" style="font-size:80%;">0.118</span></td>
<td id="S5.T6.8.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T6.8.3.3.5.1" class="ltx_text" style="font-size:80%;">0.087</span></td>
<td id="S5.T6.8.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T6.8.3.3.6.1" class="ltx_text" style="font-size:80%;">0.053</span></td>
<td id="S5.T6.8.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T6.8.3.3.7.1" class="ltx_text" style="font-size:80%;">0.052</span></td>
</tr>
<tr id="S5.T6.8.4.4" class="ltx_tr">
<th id="S5.T6.8.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.1.1" class="ltx_text" style="font-size:80%;">POS2</span></th>
<td id="S5.T6.8.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.2.1" class="ltx_text" style="font-size:80%;">0.266</span></td>
<td id="S5.T6.8.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.3.1" class="ltx_text" style="font-size:80%;">0.175</span></td>
<td id="S5.T6.8.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.4.1" class="ltx_text" style="font-size:80%;">0.155</span></td>
<td id="S5.T6.8.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.5.1" class="ltx_text" style="font-size:80%;">0.117</span></td>
<td id="S5.T6.8.4.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.6.1" class="ltx_text" style="font-size:80%;">0.113</span></td>
<td id="S5.T6.8.4.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T6.8.4.4.7.1" class="ltx_text" style="font-size:80%;">0.122</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">In this work, we explored the effectiveness of integrating generic object detection algorithms with COD techniques for drone detection in environments with complex backgrounds. We introduced YOLO-FEDER FusionNet, a novel DL architecture. Alongside the integration of dual backbones, we implemented a redesigned neck structure to enable seamless information fusion and facilitate the prioritization of essential features. We systematically evaluated the proposed detection model on a variety of real and synthetic datasets, characterized by different complexity levels. Our analyses demonstrated substantial improvements of YOLO-FEDER FusionNet over conventional drone detectors, especially in terms of FNRs and FDRs. Furthermore, we revealed a labeling bias originating from manually generated annotations in real-world data, adversely affecting mAP values. Addressing this bias via post-processing led to improvements w.r.t. mAP. We also showed that leveraging information from previous frames in a video stream can further reduce FNRs.</span></p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Funding:</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="S6.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:90%;">No funding was received for conducting this study.</span></p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Conflicts of Interest:</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="S6.SS0.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:90%;">The authors declare no relevant financial or non-financial conflicts of interest.</span></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
F.-L. Chiper, A.Â Martian, C.Â Vladeanu, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">â€œDrone Detection and Defense Systems: Survey and a Software-Defined
Radio-Based Solution,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, vol. 22, no. 4, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
M.Â Elsayed, M.Â Reda, A.Â S. Mashaly, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">â€œReview on Real-Time Drone Detection Based on Visual Band
Electro-Optical (EO) Sensor,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">10th International Conference on Intelligent Computing and
Information Systems</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 57â€“65.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Ultralytics,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">â€œYOLOv5: The Friendliest AI Architecture Youâ€™ll Ever Use,â€
https://ultralytics.com/yolov5/, accessed: 2024-01-18.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
U.Â Seidaliyeva, D.Â Akhmetov, L.Â Ilipbayeva, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">â€œReal-Time and Accurate Drone Detection in a Video with a Static
Background,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, vol. 20, no. 14, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
T.Â R. Dieter, A.Â Weinmann, S.Â JÃ¤ger, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">â€œQuantifying the Simulationâ€“Reality Gap for Deep Learning-Based
Drone Detection,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Electronics</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, vol. 12, no. 10, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
D.-P. Fan, G.-P. Ji, G.Â Sun, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">â€œCamouflaged Object Detection,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 2774â€“2784.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
A.Â Barisic, F.Â Petric, and S.Â Bogdan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">â€œSim2Air - Synthetic Aerial Dataset for UAV Monitoring,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, vol. 7, no. 2, pp.
3757â€“3764, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
D.Â Marez, S.Â Borden, and L.Â Nans,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">â€œUAV Detection with a Dataset Augmented by Domain Randomization,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Geospatial Informatics X</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2020, vol. 11398.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
C.Â Symeonidis, C.Â Anastasiadis, and N.Â Nikolaidis,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">â€œA UAV Video Data Generation Framework for Improved Robustness of
UAV Detection Methods,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE 24th International Workshop on Multimedia Signal
Processing</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 1â€“5.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Y.Â Lv, Z.Â Ai, M.Â Chen, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">â€œHigh-Resolution Drone Detection Based on Background Difference and
SAG-YOLOv5s,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, vol. 22, no. 15, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
H.Â Liu, K.Â Fan, Q.Â Ouyang, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">â€œReal-Time Small Drones Detection Based on Pruned YOLOv4,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, vol. 21, no. 10, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
L.Â Yang, R.-Y. Zhang, L.Â Li, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">â€œSimAM: A Simple, Parameter-Free Attention Module for Convolutional
Neural Networks,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">38th International Conference on Machine Learning</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2021,
vol. 139 of </span><span id="bib.bib12.6.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning Research</span><span id="bib.bib12.7.5" class="ltx_text" style="font-size:90%;">, pp. 11863â€“11874.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
K.Â Han, Y.Â Wang, Q.Â Tian, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">â€œGhostNet: More Features From Cheap Operations,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 1577â€“1586.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Y.Â Chen, P.Â Aggarwal, J.Â Choi, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">â€œA Deep Learning Approach to Drone Monitoring,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asia-Pacific Signal and Information Processing Association
Annual Summit and Conference</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 686â€“691.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
F.Â SvanstrÃ¶m, F.Â Alonso-Fernandez, and C.Â Englund,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">â€œDrone Detection and Tracking in Real-Time by Fusion of Different
Sensing Modalities,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Drones</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, vol. 6, no. 11, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Q.Â Jia, S.Â Yao, Y.Â Liu, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">â€œSegment, Magnify and Reiterate: Detecting Camouflaged Objects the
Hard Way,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 4713â€“4722.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
C.Â He, K.Â Li, Y.Â Zhang, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">â€œCamouflaged Object Detection with Feature Decomposition and Edge
Reconstruction,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2023, pp. 22046â€“22055.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
H.Â X. Pham, A.Â Sarabakha, M.Â Odnoshyvkin, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">â€œPencilNet: Zero-Shot Sim-to-Real Transfer Learning for Robust Gate
Perception in Autonomous Drone Racing,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, vol. 7, no. 4, pp.
11847â€“11854, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
T.Â Dieter, A.Â Weinmann, and E.Â Brucherseifer,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">â€œGenerating Synthetic Data for Deep Learning-Based Drone
Detection,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">48th International Conference of Applications of Mathematics in
Engineering and Economics</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J.Â Redmon and A.Â Farhadi,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">â€œYolov3: An incremental improvement,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, vol. abs/1804.02767, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
C.-Y. Wang, H.-Y. MarkÂ Liao, Y.-H. Wu, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">â€œCSPNet: A New Backbone that can Enhance Learning Capability of
CNN,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshop</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 1571â€“1580.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
K.Â He, X.Â Zhang, S.Â Ren, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">â€œSpatial Pyramid Pooling in Deep Convolutional Networks for Visual
Recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">,
vol. 37, pp. 1904â€“1916, 2014.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
S.Â Gao, M.-M. Cheng, K.Â Zhao, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">â€œRes2Net: A New Multi-Scale Backbone Architecture,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">,
vol. 43, pp. 652â€“662, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
L.-C. Chen, G.Â Papandreou, I.Â Kokkinos, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">â€œDeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
Atrous Convolution, and Fully Connected CRFs,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">,
vol. 40, pp. 834â€“848, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
M.Â Stevens and S.Â Merilaita,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">â€œAnimal Camouflage: Current Issues and New Perspectives,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Philosophical Transactions of the Royal Society B: Biological
Sciences</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, vol. 364, no. 1516, pp. 423â€“427, 2009.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
W.Â Ha, C.Â Singh, F.Â Lanusse, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">â€œAdaptive Wavelet Distillation from Neural Networks Through
Interpretations,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Information Processing Systems</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
M.-H. Guo, T.Â Xu, J.Â Liu, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">â€œAttention Mechanisms in Computer Vision: A Survey,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Visual Media</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, vol. 8, pp. 331 â€“ 368, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
S.Â Woo, J.Â Park, J.-Y. Lee, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">â€œCBAM: Convolutional Block Attention Module,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2018, p. 3â€“19.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Epic Games,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">â€œUnreal Engine,â€ https://www.unrealengine.com /en-US/, accessed:
2024-01-18.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Microsoft Research,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">â€œWelcome to AirSim,â€ https://microsoft. github.io/AirSim/,
accessed: 2024-01-18.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
PolyPixel,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">â€œUrban City,â€ https://www.unrealengine.com/
marketplace/en-US/product/urban-city, accessed: 2024-01-18.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M.Â Maire, S.Â J. Belongie, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">â€œMicrosoft COCO: Common Objects in Context,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.11640" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.11641" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.11641">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.11641" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.11643" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 22:11:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
