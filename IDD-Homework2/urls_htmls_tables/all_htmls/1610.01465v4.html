<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1610.01465] Visual Question Answering: Datasets, Algorithms, and Future Challenges</title><meta property="og:description" content="Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering: Datasets, Algorithms, and Future Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering: Datasets, Algorithms, and Future Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1610.01465">

<!--Generated on Sun Mar 10 12:46:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Visual Question Answering: 
<br class="ltx_break">Datasets, Algorithms, and Future Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kushal Kafle
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Kanan
</span><span class="ltx_author_notes">Corresponding author.
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and activity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Given enough data, deep convolutional neural networks (CNNs) rival the abilities of humans to do image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. With annotated datasets rapidly increasing in size thanks to crowd-sourcing, similar outcomes can be anticipated for other focused computer vision problems. However, these problems are narrow in scope and do not require holistic understanding of images. As humans, we can identify the objects in an image, understand the spatial positions of these objects, infer their attributes and relationships to each other, and also reason about the purpose of each object given the surrounding context. We can ask arbitrary questions about images and also communicate the information gleaned from them.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Until recently, developing a computer vision system that can answer arbitrary natural language questions about images has been thought to be an ambitious, but intractable, goal. However, since 2014, there has been enormous progress in developing systems with these abilities. Visual Question Answering (VQA) is a computer vision task where a system is given a text-based question about an image, and it must infer the answer. Questions can be arbitrary and they encompass many sub-problems in computer vision, e.g.,</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Object recognition - What is in the image?</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Object detection - Are there any cats in the image?</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Attribute classification - What color is the cat?</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Scene classification - Is it sunny?</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">Counting - How many cats are in the image?</p>
</div>
</li>
</ul>
<p id="S1.p2.2" class="ltx_p">Beyond these, there are many more complex questions that can be asked, such as questions about the spatial relationships among objects (What is between the cat and the sofa?) and common sense reasoning questions (Why is the the girl crying?). A robust VQA system must be capable of solving a wide range of classical computer vision tasks as well as needing the ability to reason about images.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There are many potential applications for VQA. The most immediate is as an aid to blind and visually impaired individuals, enabling them to get information about images both on the web and in the real world. For example, as a blind user scrolls through their social media feed, a captioning system can describe the image and then the user could use VQA to query the image to get more insight about the scene. More generally, VQA could be used to improve human-computer interaction as a natural way to query visual content. A VQA system can also be used for image retrieval, without using image meta-data or tags. For example, to find all images taken in a rainy setting, we can simply ask ‘Is it raining?’ to all images in the dataset. Beyond applications, VQA is an important basic research problem. Because a good VQA system must be able to solve many computer vision problems, it can be considered a component of a Turing Test for image understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Passing this test requires a system to be capable of many different visual tasks. VQA can be considered a kind of Visual Turing Test that also requires the ability to understand questions, but not necessarily more sophisticated natural language processing. If an algorithm performs as well as or better than humans on arbitrary questions about images, then arguably much of computer vision would be solved. But, this is only true if the benchmarks and evaluation tools are sufficient to make such bold claims.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this review, we discuss existing datasets and methods for VQA.
We place particular emphasis on exploring whether current VQA benchmarks are suitable for evaluating whether a system is capable of robust image understanding. In Section <a href="#S2" title="2 Vision and Language Tasks Related to VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare VQA with other computer vision tasks, some of which also require the integration of vision and language (e.g., image captioning). Then, in Section <a href="#S3" title="3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we describe currently available datasets for VQA with an emphasis on their strengths and weaknesses. We discuss how biases in some of these datasets severely limit their ability to assess algorithms. In Section <a href="#S4" title="4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we discuss the evaluation metrics used for VQA. Then, we review existing algorithms for VQA and analyze their efficacy in Section <a href="#S5" title="5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Finally, we discuss possible future developments in VQA and open questions.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/no_ann.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="568" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/object_detection.png" id="S1.F1.2.g1" class="ltx_graphics ltx_img_square" width="598" height="568" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/semantic.png" id="S1.F1.3.g1" class="ltx_graphics ltx_img_square" width="598" height="566" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.10.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.11.2" class="ltx_text" style="font-size:90%;"> Object detection, semantic segmentation, and image captioning compared to VQA. The middle figure shows the ideal output of a typical object detection system, and the right figure shows the semantic segmentation map from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Both tasks lack the ability to provide contextual information about the objects. The captions for this COCO image range from very generic descriptions of the scene, e.g., <span id="S1.F1.11.2.1" class="ltx_text ltx_font_italic">A busy town sidewalk next to street parking and intersections.</span>, to very focused discussion of a single activity without qualifying the overall scene, e.g., <span id="S1.F1.11.2.2" class="ltx_text ltx_font_italic">A woman jogging with a dog on a leash.</span> Both are acceptable captions, but significantly more information can be extracted with VQA. For the COCO-VQA dataset, the questions asked about this image are <span id="S1.F1.11.2.3" class="ltx_text ltx_font_italic">What kind of shoes is the skater wearing?</span>, <span id="S1.F1.11.2.4" class="ltx_text ltx_font_italic">Urban or suburban?</span>, and <span id="S1.F1.11.2.5" class="ltx_text ltx_font_italic">What animal is there?</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Vision and Language Tasks Related to VQA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The overarching goal of VQA is to extract question-relevant semantic information from the images, which ranges from the detection of minute details to the inference of abstract scene attributes for the whole image, based on the question. While many computer vision problems involve extracting information from the images, they are limited in scope and generality compared to VQA. Object recognition, activity recognition, and scene classification can all be posed as image classification tasks, with today’s best methods doing this using CNNs trained to classify images into particular semantic categories. The most successful of these is object recognition, where algorithms now rival humans in accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. But, object recognition requires only classifying the dominant object in an image without knowledge of its spatial position or its role within the larger scene. Object detection involves the localization of specific semantic concepts (e.g., cars or people) by placing a bounding box around each instance of the object in an image. The best object detection methods all use deep CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Semantic segmentation takes the task of localization a step further by classifying each pixel as belonging to a particular semantic class <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Instance segmentation further builds upon localization by differentiating between separate instances of the same semantic class <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">While semantic and instance segmentation are important computer vision problems that generalize object detection and recognition, they are not sufficient for holistic scene understanding. One of the major problems they face is label ambiguity. For example, in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the assigned semantic label for the position of the yellow cross can be ‘bag’, ‘black,’ or ‘person.’ The label depends on the task. Moreover, these approaches alone have no understanding of the role of an object within a larger context. In this example, labeling a pixel as ‘bag’ does not inform us about whether it is being carried by the person, and labeling a pixel as ‘person’ does not tell us if the person is sitting, running, or skateboarding. This is in contrast with VQA, where a system is required to answer arbitrary questions about images, which may require reasoning about the relationships of objects with each other and the overall scene. The appropriate label is specified by the question.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Besides VQA, there is a significant amount of recent work that combines vision with language. One of the most studied is image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, in which an algorithm’s goal is to produce a natural language description of a given image. Image captioning is a very broad task that potentially involves describing complex attributes and object relationships to provide a detailed description of an image.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">However, there are several problems with the visual captioning task, with evaluation of captions being a particular challenge. The ideal method is evaluation by human judges, but this is slow and expensive. For this reason, multiple automatic evaluation schemes have been proposed. The most widely used caption evaluation schemes are BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. With the exception of CIDEr, which was developed specifically for scoring image descriptions, all caption evaluation metrics were originally developed for machine translation evaluation. Each of these metrics has limitations. BLEU, the most widely used metric, is known to have the same score for large variations in sentence structure with largely varying semantic content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. For captions generated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, BLEU scores ranked machine generated captions above human captions. However, when human judges were used to judge the same captions, only 23.3% of the judges ranked the captions to be of equal or better quality than human captions. While other evaluation metrics, especially CIDEr and METEOR, show more robustness in terms of agreement with human judges, they still often rank automatically generated captions higher than human captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">One reason why evaluating captions is challenging is that a given image can have many valid captions, with some being very specific and others generic in nature (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). However, captioning systems that produce generic captions that only superficially describe an image’s content are often ranked high by the evaluation metrics. Generic captions such as ‘A person is walking down a street’ or ‘Several cars are parked on the side of the road’ that can be applicable to a large number of images are often ranked highly by evaluation schemes and human judges. In fact, a simple system that returns the caption of the training image with the most similar visual features using nearest neighbor yields relatively high scores using automatic evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Dense image captioning (DenseCap) avoids the generic caption problem by annotating an image densely with short visual descriptions pertaining to small, but salient, image regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. For example, a DenseCap system may output ‘a man wearing black shirt,’ ’large green trees,’ and ‘roof of a building,’ with each description accompanied by a bounding box. A system may generate a large number of these descriptions for rich scenes. Although many of these descriptions are short, it is still difficult to automatically assess their quality. DenseCap can also omit important relationships between the objects in the scene by only producing isolated descriptions for each regions. Captioning and DenseCap are also task agnostic and a system is not required to perform exhaustive image understanding.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">In conclusion, a captioning system is at liberty to arbitrarily choose the level of granularity of its image analysis which is in contrast to VQA, where the level of granularity is specified by the nature of the question asked. For example, ‘What season is this?’ will require understanding the entire scene, but ‘What is the color of dog standing behind the girl with white dress?’ would require attention to specific details of the scene. Moreover, many kinds of questions have specific and unambiguous answers, making VQA more amenable to automated evaluation metric than captioning. Ambiguity may still exist for some question types (see Section <a href="#S4" title="4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), but for many questions the answer produced by a VQA algorithm can be evaluated with one-to-one matching with the ground truth answer.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets for VQA</h2>

<div id="S3.T1" class="ltx_table ltx_transformed_outer" style="width:253.0pt;height:1015.5pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:1015.5pt;transform:translate(-381.24pt,-380.74pt) rotate(-90deg) ;"><figure>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Statistics for VQA datasets using either open-ended (OE) or multiple-choice (MC) evaluation schemes.</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S3.T1.4.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">COCO-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S3.T1.4.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We were unable to retrieve the English version of the dataset from provided download link.</span></span></span>
</th>
<th id="S3.T1.4.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<th id="S3.T1.4.1.1.7" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Visual genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<th id="S3.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Total Images</th>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_right ltx_border_t">1,449</td>
<td id="S3.T1.4.2.1.3" class="ltx_td ltx_align_right ltx_border_t">123,287</td>
<td id="S3.T1.4.2.1.4" class="ltx_td ltx_align_right ltx_border_t">204,721</td>
<td id="S3.T1.4.2.1.5" class="ltx_td ltx_align_right ltx_border_t">120,360</td>
<td id="S3.T1.4.2.1.6" class="ltx_td ltx_align_right ltx_border_t">47,300</td>
<td id="S3.T1.4.2.1.7" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">108,000</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<th id="S3.T1.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">QA Pairs</th>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_right">12,468</td>
<td id="S3.T1.4.3.2.3" class="ltx_td ltx_align_right">117,684</td>
<td id="S3.T1.4.3.2.4" class="ltx_td ltx_align_right">614,163</td>
<td id="S3.T1.4.3.2.5" class="ltx_td ltx_align_right">250,569</td>
<td id="S3.T1.4.3.2.6" class="ltx_td ltx_align_right">327,939</td>
<td id="S3.T1.4.3.2.7" class="ltx_td ltx_nopad_r ltx_align_right">1,773,258</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<th id="S3.T1.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Distinct Answers</th>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_right">968</td>
<td id="S3.T1.4.4.3.3" class="ltx_td ltx_align_right">430</td>
<td id="S3.T1.4.4.3.4" class="ltx_td ltx_align_right">105,969</td>
<td id="S3.T1.4.4.3.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.4.3.6" class="ltx_td ltx_align_right">65,161</td>
<td id="S3.T1.4.4.3.7" class="ltx_td ltx_nopad_r ltx_align_right">207,675</td>
</tr>
<tr id="S3.T1.4.5.4" class="ltx_tr">
<th id="S3.T1.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">% covered by top-1000</th>
<td id="S3.T1.4.5.4.2" class="ltx_td ltx_align_right">100</td>
<td id="S3.T1.4.5.4.3" class="ltx_td ltx_align_right">100</td>
<td id="S3.T1.4.5.4.4" class="ltx_td ltx_align_right">82.8</td>
<td id="S3.T1.4.5.4.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.5.4.6" class="ltx_td ltx_align_right">56.29</td>
<td id="S3.T1.4.5.4.7" class="ltx_td ltx_nopad_r ltx_align_right">60.8</td>
</tr>
<tr id="S3.T1.4.6.5" class="ltx_tr">
<th id="S3.T1.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">% covered by top-10</th>
<td id="S3.T1.4.6.5.2" class="ltx_td ltx_align_right">25.04</td>
<td id="S3.T1.4.6.5.3" class="ltx_td ltx_align_right">19.71</td>
<td id="S3.T1.4.6.5.4" class="ltx_td ltx_align_right">51.13</td>
<td id="S3.T1.4.6.5.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.6.5.6" class="ltx_td ltx_align_right">17.13</td>
<td id="S3.T1.4.6.5.7" class="ltx_td ltx_nopad_r ltx_align_right">13.07</td>
</tr>
<tr id="S3.T1.4.7.6" class="ltx_tr">
<th id="S3.T1.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Human Accuracy</th>
<td id="S3.T1.4.7.6.2" class="ltx_td ltx_align_right">50.2</td>
<td id="S3.T1.4.7.6.3" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.7.6.4" class="ltx_td ltx_align_right">83.3</td>
<td id="S3.T1.4.7.6.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.7.6.6" class="ltx_td ltx_align_right">96.6</td>
<td id="S3.T1.4.7.6.7" class="ltx_td ltx_nopad_r ltx_align_right">N/A</td>
</tr>
<tr id="S3.T1.4.8.7" class="ltx_tr">
<th id="S3.T1.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Longest Question</th>
<td id="S3.T1.4.8.7.2" class="ltx_td ltx_align_right">25 words</td>
<td id="S3.T1.4.8.7.3" class="ltx_td ltx_align_right">24 words</td>
<td id="S3.T1.4.8.7.4" class="ltx_td ltx_align_right">32 words</td>
<td id="S3.T1.4.8.7.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.8.7.6" class="ltx_td ltx_align_right">24 words</td>
<td id="S3.T1.4.8.7.7" class="ltx_td ltx_nopad_r ltx_align_right">26 words</td>
</tr>
<tr id="S3.T1.4.9.8" class="ltx_tr">
<th id="S3.T1.4.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Longest Answer</th>
<td id="S3.T1.4.9.8.2" class="ltx_td ltx_align_right">7 (list of 1 words)</td>
<td id="S3.T1.4.9.8.3" class="ltx_td ltx_align_right">1 word</td>
<td id="S3.T1.4.9.8.4" class="ltx_td ltx_align_right">17 words</td>
<td id="S3.T1.4.9.8.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.9.8.6" class="ltx_td ltx_align_right">20 words</td>
<td id="S3.T1.4.9.8.7" class="ltx_td ltx_nopad_r ltx_align_right">24 words</td>
</tr>
<tr id="S3.T1.4.10.9" class="ltx_tr">
<th id="S3.T1.4.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Avg. Answer Length</th>
<td id="S3.T1.4.10.9.2" class="ltx_td ltx_align_right">1.2 words</td>
<td id="S3.T1.4.10.9.3" class="ltx_td ltx_align_right">1.0 words</td>
<td id="S3.T1.4.10.9.4" class="ltx_td ltx_align_right">1.1 words</td>
<td id="S3.T1.4.10.9.5" class="ltx_td ltx_align_right">N/A</td>
<td id="S3.T1.4.10.9.6" class="ltx_td ltx_align_right">2.0 words</td>
<td id="S3.T1.4.10.9.7" class="ltx_td ltx_nopad_r ltx_align_right">1.8 words</td>
</tr>
<tr id="S3.T1.4.11.10" class="ltx_tr">
<th id="S3.T1.4.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Image Source</th>
<td id="S3.T1.4.11.10.2" class="ltx_td ltx_align_right">NYUDv2</td>
<td id="S3.T1.4.11.10.3" class="ltx_td ltx_align_right">COCO</td>
<td id="S3.T1.4.11.10.4" class="ltx_td ltx_align_right">COCO</td>
<td id="S3.T1.4.11.10.5" class="ltx_td ltx_align_right">COCO</td>
<td id="S3.T1.4.11.10.6" class="ltx_td ltx_align_right">COCO</td>
<td id="S3.T1.4.11.10.7" class="ltx_td ltx_nopad_r ltx_align_right">COCO, YFCC</td>
</tr>
<tr id="S3.T1.4.12.11" class="ltx_tr">
<th id="S3.T1.4.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Annotation</th>
<td id="S3.T1.4.12.11.2" class="ltx_td ltx_align_right">Manual+Auto</td>
<td id="S3.T1.4.12.11.3" class="ltx_td ltx_align_right">Auto</td>
<td id="S3.T1.4.12.11.4" class="ltx_td ltx_align_right">Manual</td>
<td id="S3.T1.4.12.11.5" class="ltx_td ltx_align_right">Manual</td>
<td id="S3.T1.4.12.11.6" class="ltx_td ltx_align_right">Manual</td>
<td id="S3.T1.4.12.11.7" class="ltx_td ltx_nopad_r ltx_align_right">Manual</td>
</tr>
<tr id="S3.T1.4.13.12" class="ltx_tr">
<th id="S3.T1.4.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Evaluation Type</th>
<td id="S3.T1.4.13.12.2" class="ltx_td ltx_align_right">OE</td>
<td id="S3.T1.4.13.12.3" class="ltx_td ltx_align_right">OE</td>
<td id="S3.T1.4.13.12.4" class="ltx_td ltx_align_right">MC or OE</td>
<td id="S3.T1.4.13.12.5" class="ltx_td ltx_align_right">OE</td>
<td id="S3.T1.4.13.12.6" class="ltx_td ltx_align_right">MC or OE</td>
<td id="S3.T1.4.13.12.7" class="ltx_td ltx_nopad_r ltx_align_right">OE</td>
</tr>
<tr id="S3.T1.4.14.13" class="ltx_tr">
<th id="S3.T1.4.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Question Types</th>
<td id="S3.T1.4.14.13.2" class="ltx_td ltx_align_right ltx_border_bb">3</td>
<td id="S3.T1.4.14.13.3" class="ltx_td ltx_align_right ltx_border_bb">4</td>
<td id="S3.T1.4.14.13.4" class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td id="S3.T1.4.14.13.5" class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td id="S3.T1.4.14.13.6" class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td id="S3.T1.4.14.13.7" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</figure></div></div>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Beginning in 2014, five major datasets for VQA have been publicly released. These datasets enable VQA systems to be trained and evaluated. As of this article, the main datasets for VQA are DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, The VQA Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. With exception of DAQUAR, all of the datasets include images from the Microsoft Common Objects in Context (COCO) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which consists of 328,000 images, 91 common object categories with over 2 million labeled instances, and an average of 5 captions per image. Visual Genome and Visual7W use images from Flickr100M in addition to the COCO images. A portion of The VQA Dataset contains synthetic cartoon imagery, which we will refer to as SYNTH-VQA. Consistent with other papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, the rest of The VQA Dataset will be referred as COCO-VQA, since it contains images from the COCO image dataset. Table <a href="#S3.T1" title="Table 1 ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> contains statistics for each of these datasets.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">An ideal VQA dataset needs to be sufficiently large to capture the variability within questions, images, and concepts that occur in real world scenarios. It should also have a fair evaluation scheme that is difficult to ‘game’ and doing well on it indicates that an algorithm can answer a large variety of question types about images that have definitive answers. If a dataset contains easily exploitable biases in the distribution of the questions or answers, it may be possible for an algorithm to perform well on the dataset without really solving the VQA problem.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In the following subsections, we critically review the available datasets. We describe how the datasets were created and discuss their limitations.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>DAQUAR</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The DAtaset for QUestion Answering on Real-world images (DAQUAR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> was the first major VQA dataset to be released. It is one of the smallest VQA datasets. It consists of 6795 training and 5673 testing QA pairs based on images from the NYU-DepthV2 Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. The dataset is also available in an even smaller configuration consisting of only 37 object categories, known as DAQUAR-37. DAQUAR-37 consists of only 3825 training QA pairs and 297 testing QA pairs. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, additional ground truth answers were collected for DAQUAR to create an alternative evaluation metric. This variant of DAQUAR is called DAQUAR-consensus, named after the evaluation metric. While DAQUAR was a pioneering dataset for VQA, it is too small to successfully train and evaluate more complex models. Apart from the small size, DAQUAR contains exclusively indoor scenes, which constrains the variety of questions available. The images tend to have significant clutter and in some cases extreme lighting conditions (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 COCO-QA ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). This makes many questions difficult to answer, and even humans are only able to achieve 50.2% accuracy on the full dataset.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>COCO-QA</h3>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/comp4.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="396" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.4.2" class="ltx_text" style="font-size:90%;"> COCO-QA: <span id="S3.F2.sf1.4.2.1" class="ltx_text" style="color:#000000;"> What does an intersection show on one side and two double-decker buses and a third vehicle,?
<br class="ltx_break">Ground Truth: Building</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/daquar3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="454" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.4.2" class="ltx_text" style="font-size:90%;">DAQUAR: <span id="S3.F2.sf2.4.2.1" class="ltx_text" style="color:#000000;">What is behind the computer in the corner of the table?
<br class="ltx_break">Ground Truth: papers</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Sample images from DAQUAR and the COCO-QA datasets and the corresponding QA pairs. A significant number of COCO-QA questions have grammatical errors and are nonsensical, whereas DAQUAR images are often marred with clutter and low resolution images.</span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, QA pairs are created for images using an Natural Language Processing (NLP) algorithm that derives them from the COCO image captions. For example, using the image caption <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">A boy is playing Frisbee</span>, it is possible to create the question <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">What is the boy playing?</span> with <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">frisbee</span> as the answer. COCO-QA contains 78,736 training and 38,948 testing QA pairs. Most questions ask about the object in the image (69.84%), with the other questions being about color (16.59%), counting (7.47%) and location (6.10%). All of the questions have a single word answer, and there are only 435 unique answers. These constraints on the answers makes evaluation relatively straightforward.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The biggest shortcoming of COCO-QA is due to flaws in the NLP algorithm that was used to generate the QA pairs. Longer sentences are broken into smaller chunks for ease of processing, but in many of these cases the algorithm does not cope well with the presence of clauses and grammatical variations in sentence formation. This results in awkwardly phrased questions, with many containing grammatical errors, and others being completely unintelligible (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 COCO-QA ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The other major shortcoming is that it only has four kinds of questions, and these are limited to the kinds of things described in COCO’s captions.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>The VQA Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The VQA Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> consists of both real images from COCO and abstract cartoon images. Most work on this dataset has focused solely on the portion containing real world imagery from COCO, which we refer to as COCO-VQA. We refer to the synthetic portion of the dataset as SYNTH-VQA.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">COCO-VQA consists of three questions per image, with ten answers per question. Amazon Mechanical Turk (AMT) workers were employed to generate questions for each image by being asked to ‘Stump a smart robot,’ and a separate pool of workers were hired to generate the answers to the questions. Compared to other VQA datasets, COCO-VQA consists of a relatively large number of questions (614,163 total, with 248,349 for training, 121,512 for validation, and 244,302 for testing). Each of the questions is then answered by 10 independent annotators. The multiple answers per question are used in the consensus-based evaluation metric for the dataset, which is discussed in Section <a href="#S4" title="4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">SYNTH-VQA consists of 50,000 synthetic scenes that depict cartoon images in different simulated scenarios. Scenes are made from over 100 different objects, 30 different animal models, and 20 human cartoon models. The human models are the same as those used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and they contain deformable limbs and eight different facial expressions. The models also span different age, gender, and races to provide variation in appearance. SYNTH-VQA has 150,000 QA pairs with 3 questions per scene and 10 ground truth answers per question. By using synthetic images, it becomes possible to create a more varied and balanced dataset. Natural image datasets tend to have more consistent context and biases, e.g., a street scene is more likely to have picture of a dog than a zebra. Using synthetic images, these biases can be reduced. Yin and Yang <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> is a dataset built on top of SYNTH-VQA that tried to eliminate biases in the answers people have to questions. We further discuss Yin and Yang in Section <a href="#S6.SS1" title="6.1 Vision vs. Language in VQA ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Both SYNTH-VQA and COCO-VQA come in both open-ended and multiple-choice formats. The multiple-choice format contains all the same QA pairs, but it also contains 18 different choices that are comprised of</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">The Correct Answer</span>, which is the most frequent answer given by the ten annotators.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Plausible Answers</span>, which are three answers collected from annotators without looking at the image.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Popular Answers</span>, which are the top ten most popular answers in the dataset.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Random Answers</span>, which are randomly selected correct answers for other questions.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/vqa1.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Q: Would you like to fly in that? GT: yes (4x), no (6x). The VQA Dataset contains subjective questions that are prone to cause disagreement between annotators and also clearly lack a single objectively correct answer.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/vqa2.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Q: What color are the trees? GT: green. There are 73 total questions in the dataset asking this question. For 70 of those questions, the majority answer is green. Such questions can be often answered without information from the image.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/vqa3.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="342" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">Q: Why would you say this woman is strong? GT: yes (5x), can lift up on arms, headstand, handstand, can stand on her head, she is standing upside down on stool. Questions seeking descriptive or explanatory answers can pose significant difficulty in evaluation.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Open ended QA pairs from The VQA Dataset for both real and abstract images.</span></figcaption>
</figure>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Due to diversity and size of the dataset, COCO-VQA has been widely used to evaluate algorithms. However, there are multiple problems with the dataset. COCO-VQA has a large variety of questions, but many of them can be accurately answered without using the image due to language biases. Relatively simple image-blind algorithms have achieved 49.6% accuracy on COCO-VQA using the question alone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. The dataset also contains many subjective, opinion-seeking questions that do not have a single objective answer (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 The VQA Dataset ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Similarly, many questions seek explanations or verbose descriptions. An example of this is given in Figure <a href="#S3.F3.sf3" title="In Figure 3 ‣ 3.3 The VQA Dataset ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a>, which also shows unreliability of human annotators as the most popular answer is ‘yes’ which is completely wrong for the given question. These complications are reflected by inter-human agreement on this dataset, which is about 83%. Several other practical issues also arise out of the dataset’s biases. For example, ‘yes/no’ answers span about 38% of all questions, and almost 59% of them are answered with ‘yes.’ Combined with the evaluation metric used with COCO-VQA (see Section <a href="#S4" title="4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), these biases can make it difficult to assess whether an algorithm is truly solving the VQA problem using solely this dataset. We discuss this further in Section <a href="#S4" title="4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>FM-IQA</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The Freestyle Multilingual Image Question Answering (FM-IQA) dataset is another dataset based on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. It contains human generated answers and questions. The dataset was originally collected in Chinese, but English translations have been made available. Unlike COCO-QA and DAQUAR, this dataset also allowed for answers to be full sentences. This makes automatic evaluation with common metrics intractable. For this reason, the authors suggested using human judges for evaluation, where the judges are tasked with deciding whether or not the answer is provided by a human or not as well as assessing the quality of an answer on a scale of 0–2. This approach is impractical for most research groups and makes developing algorithms difficult. We further discuss the importance of automatic evaluation metrics in Section <a href="#S4" title="4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Visual Genome</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> consists of 108,249 images that occur in both YFCC100M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and COCO images. It contains 1.7 million QA pairs for images, with an average of 17 QA pairs per image. As of this article, Visual Genome is the largest VQA dataset. Because it was only recently introduced, no methods have been evaluated on it beyond the baselines established by the authors.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Visual Genome consists of six types of ‘W’ questions: <span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_typewriter">What, Where, How, When, Who, and Why</span>. Two distinct modes of data collection were used to make the dataset. In the free-form method, annotators were free to ask any question about an image. However, when asking free-form questions, human annotators tend to ask similar questions about an image’s holistic content, e.g., asking ‘How many horses are there?’ or ‘Is it sunny?’ This can promote bias in the kinds of questions asked. The creators of Visual Genome combated this by also prompting workers to ask questions about specific image regions. When using this region-specific method, a worker might be prompted to ask a question about a region of an image containing a fire hydrant. Region-specific question prompting was made possible using Visual Genome’s descriptive bounding-box annotations. An example of region bounding boxes and QA pairs from Visual Genome are shown in Figure <a href="#S3.F4.sf1" title="In Figure 4 ‣ 3.5 Visual Genome ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Visual Genome has much greater answer diversity compared to other datasets, which is shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.5 Visual Genome ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The 1000 answers that occur most frequently in Visual Genome only cover 65% of all answers in the dataset, whereas they cover 82% for COCO-VQA and 100% for DAQUAR and COCO-QA. Visual Genome’s long-tailed distribution is also observed in the length of the answers. Only 57% of answers are single words, compared to 88% of answers in COCO-VQA, 100% of answers in COCO-QA, and 90% of answers in DAQUAR. This diversity in answers makes open-ended evaluation significantly more challenging. Moreover, since the categories themselves are required to strictly belong to one of the six ‘W’ types, the diversity in answer may at times artificially stem simply from variations in phrasing which could be eliminated by prompting the annotators to choose more concise answers. For example, <span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_italic">Where are the cars parked?</span> can be answered with ‘on the street’ or more concisely with ‘street.’</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">Visual Genome has no binary (yes/no) questions. The dataset creators argue that this will encourage using more complex questions. This is in contrast to The VQA Dataset, where ‘yes’ and ‘no’ are the more frequent answers in the dataset. We discuss this issue further in Section <a href="#S6.SS4" title="6.4 Are Binary Questions Sufficient? ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/visualgenome.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Example image from the Visual Genome dataset along with annotated image regions. This figure is taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. 
<br class="ltx_break">Free form QA: What does the sky look like? 
<br class="ltx_break">Region based QA: What color is the horse? </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/visual7w_pointing.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="398" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Example of the pointing QA task in Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The bounding boxes are the given choices. Correct answer is shown in green 
<br class="ltx_break">Q: Which object can you stab food with?</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Visual7W is a subset of Visual Genome. Apart from the pointing task, all of the questions in Visual7W are sourced from Visual Genome data. Visual Genome, however, includes more than just QA pairs, such as region annotations.</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/1610.01465/assets/images/dataset_bias.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="404" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">This graph shows the long-tailed nature of answer distributions in newer VQA datasets. For example, choosing the 500 most repeated answers in the training set would cover a 100% of all possible answers in COCO-QA but less than 50% in the Visual Genome dataset. For classification based frameworks, this translates to training a model with more output classes.</span></figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Visual7W</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">The Visual7W dataset is a subset of Visual Genome. Visual7W contains 47,300 images from Visual Genome that are also present in COCO. Visual7W is named after the seven categories of questions it contains: <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_typewriter">What, Where, How, When, Who, Why, and Which</span>. The dataset consists of two distinct types of questions. The ‘telling’ questions are identical to Visual Genome questions, and the answer is text-based. The ‘pointing’ questions are the ones that begin with ‘Which,’ and for these questions the algorithm has to select the correct bounding box among alternatives. An example pointing question is shown in Figure <a href="#S3.F4.sf2" title="In Figure 4 ‣ 3.5 Visual Genome ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Visual7W uses a multiple-choice answer framework as the standard evaluation, with four possible answers being made available to an algorithm during evaluation. To make the task challenging, the multiple-choices consist of answers that are plausible for the given question. Plausible answers are collected by prompting annotators to answer the question without seeing the image. For pointing questions, the multiple-choice options are four plausible bounding boxes surrounding the likely answer. Like Visual Genome, the dataset does not contain any binary questions.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>SHAPES</h3>

<figure id="S3.F6" class="ltx_figure"><img src="/html/1610.01465/assets/images/shapes.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="359" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Example image from the SHAPES dataset. Questions in the SHAPES dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> include counting (How many triangles are there?), spatial reasoning (Is there a red shape above a circle?), and inference (Is there a blue shape red?)</span></figcaption>
</figure>
<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">While the other VQA datasets contain either real or synthetic scenes, the SHAPES dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> consists of shapes of varying arrangements, types, and colors. Questions are about the attributes, relationships, and positions of the shapes. This approach enables the creation of a vast amount of data, free of many of the biases that plague other datasets to varying degrees.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">SHAPES consists of 244 unique questions, with every question asked about each of the 64 images in the dataset. Unlike other datasets, this means it is completely balanced and free of bias. All questions are binary, with yes/no answers. Many of the questions require positional reasoning about the layout and properties of the shapes. While, SHAPES cannot be a substitute for using real-world imagery, the idea behind it is extremely valuable. An algorithm that cannot perform well on SHAPES, but performs well on other VQA datasets may indicate that it is only capable of analyzing images in a limited manner.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation Metrics for VQA</h2>

<figure id="S4.F7" class="ltx_figure"><img src="/html/1610.01465/assets/images/vqa_disagreement.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Simple questions can also evoke diverse answers from annotators in COCO-VQA. Q: Where is the dog? A: 1) eating out of his bowl; 2) on floor; 3) feeding station; 4) by his food; 5) inside; 6) on floor eating out of his dish; 7) floor; 8) in front of gray bowl, to right of trash can; 9) near food bowl; 10) on floor</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">VQA has been posed as either an open-ended task, in which an algorithm generates a string to answer a question, or as a multiple-choice question where it picks among choices. For multiple-choice, simple accuracy is often used to evaluate, with an algorithm getting an answer right if it makes the correct choice. For open-ended VQA, simple accuracy can also be used. In this case, an algorithm’s predicted answer string must exactly match the ground truth answer. However, accuracy can be too stringent because some errors are much worse than others. For example, if the question was ‘What animals are in the photo?’ and a system outputs ‘dog’ instead of the correct label ‘dogs,’ it is penalized just as strongly as it would be if it output ‘zebra.’ Questions may also have multiple correct answers, e.g., ‘What is in the tree?’ might have ‘bald eagle’ listed as the correct ground truth answer, so a system that outputs ‘eagle’ or ‘bird’ would be penalized just as much as if it had output ‘yes’ as the answer. Due to these issues, several alternatives to exact accuracy have been proposed for evaluating open-ended VQA algorithms.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Comparison of different evaluation metrics proposed for VQA.</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.4.1.1.2.1" class="ltx_text ltx_font_bold">Pros</span></th>
<th id="S4.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.4.1.1.3.1" class="ltx_text ltx_font_bold">Cons</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.2.1" class="ltx_tr">
<th id="S4.T2.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<table id="S4.T2.4.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.4.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Simple</span></td>
</tr>
<tr id="S4.T2.4.2.1.1.1.2" class="ltx_tr">
<td id="S4.T2.4.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.2.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
</table>
</th>
<td id="S4.T2.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">
<table id="S4.T2.4.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.2.1.2.1.1" class="ltx_tr">
<td id="S4.T2.4.2.1.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.2.1.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.1.2.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.2.1.2.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Very simple to evaluate and interpret</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.2.1.2.1.2" class="ltx_tr">
<td id="S4.T2.4.2.1.2.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.2.1.2.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.1.2.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.2.1.2.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Works well for small number of unique answers</span>
</span>
</td>
</tr>
</table>
</td>
<td id="S4.T2.4.2.1.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S4.T2.4.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.2.1.3.1.1" class="ltx_tr">
<td id="S4.T2.4.2.1.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.2.1.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.1.3.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.2.1.3.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Both minor and major errors are penalized equally</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.2.1.3.1.2" class="ltx_tr">
<td id="S4.T2.4.2.1.3.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.2.1.3.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.1.3.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.2.1.3.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Can lead to explosion in number of unique answers,</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.2.1.3.1.3" class="ltx_tr">
<td id="S4.T2.4.2.1.3.1.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.2.1.3.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.1.3.1.3.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.2.1.3.1.3.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  especially with presence of phrasal or sentence answers</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.4.3.2" class="ltx_tr">
<th id="S4.T2.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<table id="S4.T2.4.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.3.2.1.1.1" class="ltx_tr">
<td id="S4.T2.4.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Modified</span></td>
</tr>
<tr id="S4.T2.4.3.2.1.1.2" class="ltx_tr">
<td id="S4.T2.4.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.3.2.1.1.2.1.1" class="ltx_text ltx_font_bold">WUPS</span></td>
</tr>
</table>
</th>
<td id="S4.T2.4.3.2.2" class="ltx_td ltx_align_left ltx_border_t">
<table id="S4.T2.4.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.3.2.2.1.1" class="ltx_tr">
<td id="S4.T2.4.3.2.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.3.2.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.2.2.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.3.2.2.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  More forgiving to simple variations and errors</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.3.2.2.1.2" class="ltx_tr">
<td id="S4.T2.4.3.2.2.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.3.2.2.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.2.2.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.3.2.2.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Does not require exact match</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.3.2.2.1.3" class="ltx_tr">
<td id="S4.T2.4.3.2.2.1.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.3.2.2.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.2.2.1.3.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.3.2.2.1.3.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Easy to evaluate with simple script</span>
</span>
</td>
</tr>
</table>
</td>
<td id="S4.T2.4.3.2.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S4.T2.4.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.3.2.3.1.1" class="ltx_tr">
<td id="S4.T2.4.3.2.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.3.2.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.2.3.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.3.2.3.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Generates high scores for answers that are lexically related but have diametrically opposite meaning</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.3.2.3.1.2" class="ltx_tr">
<td id="S4.T2.4.3.2.3.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.3.2.3.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.2.3.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.3.2.3.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Cannot be used for phrasal or sentence answers</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.4.4.3" class="ltx_tr">
<th id="S4.T2.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<table id="S4.T2.4.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.4.3.1.1.1" class="ltx_tr">
<td id="S4.T2.4.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.4.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Consensus</span></td>
</tr>
<tr id="S4.T2.4.4.3.1.1.2" class="ltx_tr">
<td id="S4.T2.4.4.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.4.3.1.1.2.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
</tr>
</table>
</th>
<td id="S4.T2.4.4.3.2" class="ltx_td ltx_align_left ltx_border_t">
<table id="S4.T2.4.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.4.3.2.1.1" class="ltx_tr">
<td id="S4.T2.4.4.3.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.4.3.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.3.2.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.4.3.2.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Common variances of same answer could be captured</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.4.3.2.1.2" class="ltx_tr">
<td id="S4.T2.4.4.3.2.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.4.3.2.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.3.2.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.4.3.2.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Easy to evaluate after collecting consensus data</span>
</span>
</td>
</tr>
</table>
</td>
<td id="S4.T2.4.4.3.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S4.T2.4.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.4.3.3.1.1" class="ltx_tr">
<td id="S4.T2.4.4.3.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.4.3.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.3.3.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.4.3.3.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Can allow for some questions having two correct answers</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.4.3.3.1.2" class="ltx_tr">
<td id="S4.T2.4.4.3.3.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.4.3.3.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.3.3.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.4.3.3.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Expensive to collect ground truth</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.4.3.3.1.3" class="ltx_tr">
<td id="S4.T2.4.4.3.3.1.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.4.3.3.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.3.3.1.3.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.4.3.3.1.3.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Difficulty due to lack of consensus</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.4.5.4" class="ltx_tr">
<th id="S4.T2.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">
<table id="S4.T2.4.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.5.4.1.1.1" class="ltx_tr">
<td id="S4.T2.4.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.5.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Manual</span></td>
</tr>
<tr id="S4.T2.4.5.4.1.1.2" class="ltx_tr">
<td id="S4.T2.4.5.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.4.5.4.1.1.2.1.1" class="ltx_text ltx_font_bold">Evaluation</span></td>
</tr>
</table>
</th>
<td id="S4.T2.4.5.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<table id="S4.T2.4.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.5.4.2.1.1" class="ltx_tr">
<td id="S4.T2.4.5.4.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.5.4.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.4.2.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.5.4.2.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Variances to same answer is easily captured</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.5.4.2.1.2" class="ltx_tr">
<td id="S4.T2.4.5.4.2.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.5.4.2.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.4.2.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.5.4.2.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Can work equally well for single word as well as phrase or sentence answers</span>
</span>
</td>
</tr>
</table>
</td>
<td id="S4.T2.4.5.4.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t">
<table id="S4.T2.4.5.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.5.4.3.1.1" class="ltx_tr">
<td id="S4.T2.4.5.4.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.5.4.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.4.3.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.5.4.3.1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Can introduce subjective opinion of individual annotators</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.5.4.3.1.2" class="ltx_tr">
<td id="S4.T2.4.5.4.3.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.4.5.4.3.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.4.3.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">  <span id="S4.T2.4.5.4.3.1.2.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">•</span>  Very expensive to setup and slow to evaluate, especially for larger datasets</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Wu-Palmer Similarity (WUPS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> was proposed as an alternative to accuracy in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. It tries to measure how much a predicted answer differs from the ground truth based on the difference in their semantic meaning. Given a ground truth answer and a predicted answer to a question, WUPS will assign a value between 0 and 1 based on their similarity to each other. It does this by finding the least common subsumer between two semantic senses and assigning scores based on how far back the semantic tree needs to be traversed to find the common subsumer. Using WUPS, semantically similar, but non-identical, words are penalized relatively less. Following our earlier example, ‘bald eagle’ and ‘eagle’ have similarity of 0.96, whereas ‘bald eagle’ and ‘bird’ have similarity of 0.88. However, WUPS tends to assign relatively high scores to even distant concepts, e.g., ‘raven’ and ‘writing desk’ have a WUPS score of 0.4. To remedy this, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> proposed to threshold WUPS scores, where a score that is below a threshold will be scaled down by a factor. A threshold of 0.9 and scaling factor of 0.1 was suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. This modified WUPS metric is the standard measure used for evaluating performance on DAQUAR and COCO-QA, in addition to simple accuracy.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">There are two major shortcomings to WUPS that make it difficult to use. First, despite using a thresholded version of WUPS, certain pairs of words are lexically very similar but carry vastly different meaning. This is particularly problematic for questions about object attributes, such as color questions. For example, if the correct answer was ‘white’ and the predicted answer was ‘black,’ the answer would still receive a WUPS score of 0.91, which seems excessively high. Another major problem with WUPS is that it only works with rigid semantic concepts, which are almost always single words. WUPS cannot be used for phrasal or sentence answers that are occasionally found in The VQA Dataset and in much of Visual7W.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">An alternative to relying on semantic similarity measures is to have multiple independently collected ground truth answers for each question, which was done for The VQA Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and DAQUAR-consensus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. For DAQUAR-consensus, an average of five human annotated ground truth answers per question were collected. The dataset’s creators proposed two ways to use these answers, which they called average consensus and min consensus. For average consensus, the final score is weighted toward preferring the more popular answer provided by the annotators. For min consensus, the answer needs to agree with at least one annotator.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.2" class="ltx_p">For The VQA Dataset, annotators generated ten answers per question. These are used with a variation of the accuracy metric, which is given by</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.4" class="ltx_Math" alttext="{Accuracy}_{{VQA}}=\min(\frac{n}{3},1)," display="block"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.2" xref="S4.E1.m1.4.4.1.1.2.cmml"><mi id="S4.E1.m1.4.4.1.1.2.2" xref="S4.E1.m1.4.4.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.3" xref="S4.E1.m1.4.4.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1a" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.4" xref="S4.E1.m1.4.4.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1b" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.5" xref="S4.E1.m1.4.4.1.1.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1c" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.6" xref="S4.E1.m1.4.4.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1d" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.7" xref="S4.E1.m1.4.4.1.1.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1e" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.8" xref="S4.E1.m1.4.4.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.1f" xref="S4.E1.m1.4.4.1.1.2.1.cmml">​</mo><msub id="S4.E1.m1.4.4.1.1.2.9" xref="S4.E1.m1.4.4.1.1.2.9.cmml"><mi id="S4.E1.m1.4.4.1.1.2.9.2" xref="S4.E1.m1.4.4.1.1.2.9.2.cmml">y</mi><mrow id="S4.E1.m1.4.4.1.1.2.9.3" xref="S4.E1.m1.4.4.1.1.2.9.3.cmml"><mi id="S4.E1.m1.4.4.1.1.2.9.3.2" xref="S4.E1.m1.4.4.1.1.2.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.9.3.1" xref="S4.E1.m1.4.4.1.1.2.9.3.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.9.3.3" xref="S4.E1.m1.4.4.1.1.2.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.2.9.3.1a" xref="S4.E1.m1.4.4.1.1.2.9.3.1.cmml">​</mo><mi id="S4.E1.m1.4.4.1.1.2.9.3.4" xref="S4.E1.m1.4.4.1.1.2.9.3.4.cmml">A</mi></mrow></msub></mrow><mo id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.4.4.1.1.3.2" xref="S4.E1.m1.4.4.1.1.3.1.cmml"><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">min</mi><mo id="S4.E1.m1.4.4.1.1.3.2a" xref="S4.E1.m1.4.4.1.1.3.1.cmml">⁡</mo><mrow id="S4.E1.m1.4.4.1.1.3.2.1" xref="S4.E1.m1.4.4.1.1.3.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.1.1.3.2.1.1" xref="S4.E1.m1.4.4.1.1.3.1.cmml">(</mo><mfrac id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mi id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml">n</mi><mn id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.3.cmml">3</mn></mfrac><mo id="S4.E1.m1.4.4.1.1.3.2.1.2" xref="S4.E1.m1.4.4.1.1.3.1.cmml">,</mo><mn id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">1</mn><mo stretchy="false" id="S4.E1.m1.4.4.1.1.3.2.1.3" xref="S4.E1.m1.4.4.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.4.4.1.2" xref="S4.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.1.1.cmml" xref="S4.E1.m1.4.4.1"><eq id="S4.E1.m1.4.4.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1"></eq><apply id="S4.E1.m1.4.4.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.2"><times id="S4.E1.m1.4.4.1.1.2.1.cmml" xref="S4.E1.m1.4.4.1.1.2.1"></times><ci id="S4.E1.m1.4.4.1.1.2.2.cmml" xref="S4.E1.m1.4.4.1.1.2.2">𝐴</ci><ci id="S4.E1.m1.4.4.1.1.2.3.cmml" xref="S4.E1.m1.4.4.1.1.2.3">𝑐</ci><ci id="S4.E1.m1.4.4.1.1.2.4.cmml" xref="S4.E1.m1.4.4.1.1.2.4">𝑐</ci><ci id="S4.E1.m1.4.4.1.1.2.5.cmml" xref="S4.E1.m1.4.4.1.1.2.5">𝑢</ci><ci id="S4.E1.m1.4.4.1.1.2.6.cmml" xref="S4.E1.m1.4.4.1.1.2.6">𝑟</ci><ci id="S4.E1.m1.4.4.1.1.2.7.cmml" xref="S4.E1.m1.4.4.1.1.2.7">𝑎</ci><ci id="S4.E1.m1.4.4.1.1.2.8.cmml" xref="S4.E1.m1.4.4.1.1.2.8">𝑐</ci><apply id="S4.E1.m1.4.4.1.1.2.9.cmml" xref="S4.E1.m1.4.4.1.1.2.9"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.2.9.1.cmml" xref="S4.E1.m1.4.4.1.1.2.9">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.2.9.2.cmml" xref="S4.E1.m1.4.4.1.1.2.9.2">𝑦</ci><apply id="S4.E1.m1.4.4.1.1.2.9.3.cmml" xref="S4.E1.m1.4.4.1.1.2.9.3"><times id="S4.E1.m1.4.4.1.1.2.9.3.1.cmml" xref="S4.E1.m1.4.4.1.1.2.9.3.1"></times><ci id="S4.E1.m1.4.4.1.1.2.9.3.2.cmml" xref="S4.E1.m1.4.4.1.1.2.9.3.2">𝑉</ci><ci id="S4.E1.m1.4.4.1.1.2.9.3.3.cmml" xref="S4.E1.m1.4.4.1.1.2.9.3.3">𝑄</ci><ci id="S4.E1.m1.4.4.1.1.2.9.3.4.cmml" xref="S4.E1.m1.4.4.1.1.2.9.3.4">𝐴</ci></apply></apply></apply><apply id="S4.E1.m1.4.4.1.1.3.1.cmml" xref="S4.E1.m1.4.4.1.1.3.2"><min id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"></min><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><divide id="S4.E1.m1.2.2.1.cmml" xref="S4.E1.m1.2.2"></divide><ci id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2">𝑛</ci><cn type="integer" id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3">3</cn></apply><cn type="integer" id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">{Accuracy}_{{VQA}}=\min(\frac{n}{3},1),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.p5.1" class="ltx_p">where <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p5.1.m1.1a"><mi id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">n</annotation></semantics></math> is the total number of annotators that had the same answer as the algorithm. Using this metric, if the algorithm agrees with three or more annotators then it is awarded a full score for a question. Although this metric helps greatly with the ambiguity problem, substantial problems remain, especially with the COCO-VQA portion of the dataset, which we study further in the next few paragraphs<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that our analysis for COCO-VQA was only done on the train and validation portions of the dataset, because the test answers are not publicly available.</span></span></span>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Using <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="Accuracy_{VQA}" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1a" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.4" xref="S4.p6.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1b" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.5" xref="S4.p6.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1c" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.6" xref="S4.p6.1.m1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1d" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.7" xref="S4.p6.1.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1e" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.8" xref="S4.p6.1.m1.1.1.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.1f" xref="S4.p6.1.m1.1.1.1.cmml">​</mo><msub id="S4.p6.1.m1.1.1.9" xref="S4.p6.1.m1.1.1.9.cmml"><mi id="S4.p6.1.m1.1.1.9.2" xref="S4.p6.1.m1.1.1.9.2.cmml">y</mi><mrow id="S4.p6.1.m1.1.1.9.3" xref="S4.p6.1.m1.1.1.9.3.cmml"><mi id="S4.p6.1.m1.1.1.9.3.2" xref="S4.p6.1.m1.1.1.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.9.3.1" xref="S4.p6.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.9.3.3" xref="S4.p6.1.m1.1.1.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.9.3.1a" xref="S4.p6.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p6.1.m1.1.1.9.3.4" xref="S4.p6.1.m1.1.1.9.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><times id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></times><ci id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">𝐴</ci><ci id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">𝑐</ci><ci id="S4.p6.1.m1.1.1.4.cmml" xref="S4.p6.1.m1.1.1.4">𝑐</ci><ci id="S4.p6.1.m1.1.1.5.cmml" xref="S4.p6.1.m1.1.1.5">𝑢</ci><ci id="S4.p6.1.m1.1.1.6.cmml" xref="S4.p6.1.m1.1.1.6">𝑟</ci><ci id="S4.p6.1.m1.1.1.7.cmml" xref="S4.p6.1.m1.1.1.7">𝑎</ci><ci id="S4.p6.1.m1.1.1.8.cmml" xref="S4.p6.1.m1.1.1.8">𝑐</ci><apply id="S4.p6.1.m1.1.1.9.cmml" xref="S4.p6.1.m1.1.1.9"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.9.1.cmml" xref="S4.p6.1.m1.1.1.9">subscript</csymbol><ci id="S4.p6.1.m1.1.1.9.2.cmml" xref="S4.p6.1.m1.1.1.9.2">𝑦</ci><apply id="S4.p6.1.m1.1.1.9.3.cmml" xref="S4.p6.1.m1.1.1.9.3"><times id="S4.p6.1.m1.1.1.9.3.1.cmml" xref="S4.p6.1.m1.1.1.9.3.1"></times><ci id="S4.p6.1.m1.1.1.9.3.2.cmml" xref="S4.p6.1.m1.1.1.9.3.2">𝑉</ci><ci id="S4.p6.1.m1.1.1.9.3.3.cmml" xref="S4.p6.1.m1.1.1.9.3.3">𝑄</ci><ci id="S4.p6.1.m1.1.1.9.3.4.cmml" xref="S4.p6.1.m1.1.1.9.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">Accuracy_{VQA}</annotation></semantics></math>, the inter-human agreement on COCO-VQA is only 83.3%. It is impossible for an algorithm to achieve 100% accuracy. Inter-human agreement is especially poor for ‘Why’ questions, with over 59% of these questions having less than three annotators giving exactly the same answer. This makes it impossible to get a full score on these questions. Lack of inter-human agreement can also be seen in simpler, more straightforward questions (see Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). In this example, if a system predicts any of the 10 answers, it will be awarded a score of at least 1/3. In several cases, the answers provided by annotators consist complete antonyms (e.g., left and right).</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">In many other cases, <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="Accuracy_{VQA}" display="inline"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1a" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.4" xref="S4.p7.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1b" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.5" xref="S4.p7.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1c" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.6" xref="S4.p7.1.m1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1d" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.7" xref="S4.p7.1.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1e" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.8" xref="S4.p7.1.m1.1.1.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.1f" xref="S4.p7.1.m1.1.1.1.cmml">​</mo><msub id="S4.p7.1.m1.1.1.9" xref="S4.p7.1.m1.1.1.9.cmml"><mi id="S4.p7.1.m1.1.1.9.2" xref="S4.p7.1.m1.1.1.9.2.cmml">y</mi><mrow id="S4.p7.1.m1.1.1.9.3" xref="S4.p7.1.m1.1.1.9.3.cmml"><mi id="S4.p7.1.m1.1.1.9.3.2" xref="S4.p7.1.m1.1.1.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.9.3.1" xref="S4.p7.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.9.3.3" xref="S4.p7.1.m1.1.1.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.9.3.1a" xref="S4.p7.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.9.3.4" xref="S4.p7.1.m1.1.1.9.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><times id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1"></times><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">𝐴</ci><ci id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3">𝑐</ci><ci id="S4.p7.1.m1.1.1.4.cmml" xref="S4.p7.1.m1.1.1.4">𝑐</ci><ci id="S4.p7.1.m1.1.1.5.cmml" xref="S4.p7.1.m1.1.1.5">𝑢</ci><ci id="S4.p7.1.m1.1.1.6.cmml" xref="S4.p7.1.m1.1.1.6">𝑟</ci><ci id="S4.p7.1.m1.1.1.7.cmml" xref="S4.p7.1.m1.1.1.7">𝑎</ci><ci id="S4.p7.1.m1.1.1.8.cmml" xref="S4.p7.1.m1.1.1.8">𝑐</ci><apply id="S4.p7.1.m1.1.1.9.cmml" xref="S4.p7.1.m1.1.1.9"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.9.1.cmml" xref="S4.p7.1.m1.1.1.9">subscript</csymbol><ci id="S4.p7.1.m1.1.1.9.2.cmml" xref="S4.p7.1.m1.1.1.9.2">𝑦</ci><apply id="S4.p7.1.m1.1.1.9.3.cmml" xref="S4.p7.1.m1.1.1.9.3"><times id="S4.p7.1.m1.1.1.9.3.1.cmml" xref="S4.p7.1.m1.1.1.9.3.1"></times><ci id="S4.p7.1.m1.1.1.9.3.2.cmml" xref="S4.p7.1.m1.1.1.9.3.2">𝑉</ci><ci id="S4.p7.1.m1.1.1.9.3.3.cmml" xref="S4.p7.1.m1.1.1.9.3.3">𝑄</ci><ci id="S4.p7.1.m1.1.1.9.3.4.cmml" xref="S4.p7.1.m1.1.1.9.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">Accuracy_{VQA}</annotation></semantics></math> leads to multiple correct answers for a question that are in direct opposition to each other. For example, in COCO-VQA more than 13% of the ‘yes/no’ answers have both ‘yes’ and ‘no’ repeated by more than three annotators. Either answering ‘yes’ or ‘no’ would receive the highest possible score. Even if eight annotators answered ‘yes,’ if two answered ‘no’ then an algorithm would still receive a score of 0.67 for the question. The weight of the majority does not play a role in evaluation.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">These problems can result in the scores being inflated. For example, answering ‘yes’ to all yes/no questions should ideally have a score of around 50% for those questions. However, using <math id="S4.p8.1.m1.1" class="ltx_Math" alttext="Accuracy_{VQA}" display="inline"><semantics id="S4.p8.1.m1.1a"><mrow id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml"><mi id="S4.p8.1.m1.1.1.2" xref="S4.p8.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.3" xref="S4.p8.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1a" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.4" xref="S4.p8.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1b" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.5" xref="S4.p8.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1c" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.6" xref="S4.p8.1.m1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1d" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.7" xref="S4.p8.1.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1e" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.8" xref="S4.p8.1.m1.1.1.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1f" xref="S4.p8.1.m1.1.1.1.cmml">​</mo><msub id="S4.p8.1.m1.1.1.9" xref="S4.p8.1.m1.1.1.9.cmml"><mi id="S4.p8.1.m1.1.1.9.2" xref="S4.p8.1.m1.1.1.9.2.cmml">y</mi><mrow id="S4.p8.1.m1.1.1.9.3" xref="S4.p8.1.m1.1.1.9.3.cmml"><mi id="S4.p8.1.m1.1.1.9.3.2" xref="S4.p8.1.m1.1.1.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.9.3.1" xref="S4.p8.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.9.3.3" xref="S4.p8.1.m1.1.1.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.9.3.1a" xref="S4.p8.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p8.1.m1.1.1.9.3.4" xref="S4.p8.1.m1.1.1.9.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><apply id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1"><times id="S4.p8.1.m1.1.1.1.cmml" xref="S4.p8.1.m1.1.1.1"></times><ci id="S4.p8.1.m1.1.1.2.cmml" xref="S4.p8.1.m1.1.1.2">𝐴</ci><ci id="S4.p8.1.m1.1.1.3.cmml" xref="S4.p8.1.m1.1.1.3">𝑐</ci><ci id="S4.p8.1.m1.1.1.4.cmml" xref="S4.p8.1.m1.1.1.4">𝑐</ci><ci id="S4.p8.1.m1.1.1.5.cmml" xref="S4.p8.1.m1.1.1.5">𝑢</ci><ci id="S4.p8.1.m1.1.1.6.cmml" xref="S4.p8.1.m1.1.1.6">𝑟</ci><ci id="S4.p8.1.m1.1.1.7.cmml" xref="S4.p8.1.m1.1.1.7">𝑎</ci><ci id="S4.p8.1.m1.1.1.8.cmml" xref="S4.p8.1.m1.1.1.8">𝑐</ci><apply id="S4.p8.1.m1.1.1.9.cmml" xref="S4.p8.1.m1.1.1.9"><csymbol cd="ambiguous" id="S4.p8.1.m1.1.1.9.1.cmml" xref="S4.p8.1.m1.1.1.9">subscript</csymbol><ci id="S4.p8.1.m1.1.1.9.2.cmml" xref="S4.p8.1.m1.1.1.9.2">𝑦</ci><apply id="S4.p8.1.m1.1.1.9.3.cmml" xref="S4.p8.1.m1.1.1.9.3"><times id="S4.p8.1.m1.1.1.9.3.1.cmml" xref="S4.p8.1.m1.1.1.9.3.1"></times><ci id="S4.p8.1.m1.1.1.9.3.2.cmml" xref="S4.p8.1.m1.1.1.9.3.2">𝑉</ci><ci id="S4.p8.1.m1.1.1.9.3.3.cmml" xref="S4.p8.1.m1.1.1.9.3.3">𝑄</ci><ci id="S4.p8.1.m1.1.1.9.3.4.cmml" xref="S4.p8.1.m1.1.1.9.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">Accuracy_{VQA}</annotation></semantics></math>, the score is 71%. This is partially due to the dataset being biased, with the majority answer for these questions being ‘yes’ 58% of the time, but a score of 71% is excessively inflated.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">Evaluating the open-ended responses of VQA systems is made simpler when the answers consist of one word answers. This occurs in 87% of COCO-VQA questions, 100% of COCO-QA questions, and 90% of DAQAUR questions. The possibility of multiple correct answers increases greatly when answers need to be multiple words. This occurs frequently in FM-IQA, Visual7W, and Visual Genome, e.g., 27% of Visual7W answers have three or more words. In this scenario, metrics such as <math id="S4.p9.1.m1.1" class="ltx_Math" alttext="Accuracy_{VQA}" display="inline"><semantics id="S4.p9.1.m1.1a"><mrow id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml"><mi id="S4.p9.1.m1.1.1.2" xref="S4.p9.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.3" xref="S4.p9.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1a" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.4" xref="S4.p9.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1b" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.5" xref="S4.p9.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1c" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.6" xref="S4.p9.1.m1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1d" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.7" xref="S4.p9.1.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1e" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.8" xref="S4.p9.1.m1.1.1.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.1f" xref="S4.p9.1.m1.1.1.1.cmml">​</mo><msub id="S4.p9.1.m1.1.1.9" xref="S4.p9.1.m1.1.1.9.cmml"><mi id="S4.p9.1.m1.1.1.9.2" xref="S4.p9.1.m1.1.1.9.2.cmml">y</mi><mrow id="S4.p9.1.m1.1.1.9.3" xref="S4.p9.1.m1.1.1.9.3.cmml"><mi id="S4.p9.1.m1.1.1.9.3.2" xref="S4.p9.1.m1.1.1.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.9.3.1" xref="S4.p9.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.9.3.3" xref="S4.p9.1.m1.1.1.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.9.3.1a" xref="S4.p9.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.9.3.4" xref="S4.p9.1.m1.1.1.9.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.1b"><apply id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1"><times id="S4.p9.1.m1.1.1.1.cmml" xref="S4.p9.1.m1.1.1.1"></times><ci id="S4.p9.1.m1.1.1.2.cmml" xref="S4.p9.1.m1.1.1.2">𝐴</ci><ci id="S4.p9.1.m1.1.1.3.cmml" xref="S4.p9.1.m1.1.1.3">𝑐</ci><ci id="S4.p9.1.m1.1.1.4.cmml" xref="S4.p9.1.m1.1.1.4">𝑐</ci><ci id="S4.p9.1.m1.1.1.5.cmml" xref="S4.p9.1.m1.1.1.5">𝑢</ci><ci id="S4.p9.1.m1.1.1.6.cmml" xref="S4.p9.1.m1.1.1.6">𝑟</ci><ci id="S4.p9.1.m1.1.1.7.cmml" xref="S4.p9.1.m1.1.1.7">𝑎</ci><ci id="S4.p9.1.m1.1.1.8.cmml" xref="S4.p9.1.m1.1.1.8">𝑐</ci><apply id="S4.p9.1.m1.1.1.9.cmml" xref="S4.p9.1.m1.1.1.9"><csymbol cd="ambiguous" id="S4.p9.1.m1.1.1.9.1.cmml" xref="S4.p9.1.m1.1.1.9">subscript</csymbol><ci id="S4.p9.1.m1.1.1.9.2.cmml" xref="S4.p9.1.m1.1.1.9.2">𝑦</ci><apply id="S4.p9.1.m1.1.1.9.3.cmml" xref="S4.p9.1.m1.1.1.9.3"><times id="S4.p9.1.m1.1.1.9.3.1.cmml" xref="S4.p9.1.m1.1.1.9.3.1"></times><ci id="S4.p9.1.m1.1.1.9.3.2.cmml" xref="S4.p9.1.m1.1.1.9.3.2">𝑉</ci><ci id="S4.p9.1.m1.1.1.9.3.3.cmml" xref="S4.p9.1.m1.1.1.9.3.3">𝑄</ci><ci id="S4.p9.1.m1.1.1.9.3.4.cmml" xref="S4.p9.1.m1.1.1.9.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.1c">Accuracy_{VQA}</annotation></semantics></math> are unlikely to help score predicted answers to ground truth answers in open-ended VQA.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">The creators of FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> suggested using human judges to assess multi-word answers, but this presents a number of problems. First, using human judges is an extremely demanding process in terms of time, resources, and expenses. It would make it difficult to iteratively improve a system by measuring how changing the algorithm altered performance. Second, human judges need to be given criteria for judging the quality of an answer. The creators of FM-IQA proposed two metrics for human judges. The first is to determine whether the answer was produced by a human or not, regardless of the answer’s correctness. This metric alone may be a poor indicator of a VQA system’s abilities and could potentially be manipulated. The second metric is to rate an answer on a 3-point scale of totally wrong (0), partially correct (1), and perfectly correct (2).</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">An alternative to using judges for handling multi-word answers is to use a multiple-choice paradigm, which is used by part of The VQA Dataset, Visual7W, and Visual Genome. Instead of generating an answer, a system only needs to predict which of the given choices is correct. This greatly simplifies evaluation, but we believe that unless it is used carefully, multiple-choice is ill-suited for VQA because it undermines the effort by allowing a system to peek at the correct answer. We discuss this issue in Section <a href="#S6.SS5" title="6.5 Open Ended vs. Multiple Choice ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a>.</p>
</div>
<div id="S4.p12" class="ltx_para">
<p id="S4.p12.1" class="ltx_p">The best way to evaluate a VQA system is still an open question. Each evaluation method has its own strengths and weaknesses (see Table <a href="#S4.T2" title="Table 2 ‣ 4 Evaluation Metrics for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for a summary). The method to use depends on how the dataset was constructed, the level of bias within it, and available resources. Considerable work needs to be done to develop better tools for measuring the semantic similarity of answers and for handling multi-word answers.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Algorithms for VQA</h2>

<figure id="S5.F8" class="ltx_figure"><img src="/html/1610.01465/assets/images/VQA-Classification.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">Simplified illustration of the classification based framework for VQA. In this framework, image and question features are extracted, and then they are combined so that a classifier can predict the answer. A variety of feature extraction methods and algorithms for combining these features have been proposed, and some of the more common approaches are listed in their respective blocks in the figure. Full details are presented in Section <a href="#S5" title="5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Results across VQA datasets for both open-ended (OE) and multiple-choice (MC) evaluation schemes. Simple models trained only on the image data (IMG-ONLY) and only on the question data (QUES-ONLY) as well as human performance are also shown. IMG-ONLY and QUES-ONLY models are evaluated on the ‘test-dev’ section of COCO-VQA. MCB-ensemble <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and AMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> are presented separately as they use additional data for training.</span></figcaption>
<table id="S5.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.4.1.1" class="ltx_tr">
<th id="S5.T3.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S5.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T3.4.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">DAQUAR</span></td>
<td id="S5.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.4.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">COCO-QA</span></td>
<td id="S5.T3.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T3.4.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">COCO-VQA</span></td>
</tr>
<tr id="S5.T3.4.2.2" class="ltx_tr">
<th id="S5.T3.4.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S5.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FULL</span></td>
<td id="S5.T3.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">37</span></td>
<td id="S5.T3.4.2.2.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OE</span></td>
<td id="S5.T3.4.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MC</span></td>
</tr>
<tr id="S5.T3.4.3.3" class="ltx_tr">
<th id="S5.T3.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T3.4.3.3.1.1" class="ltx_text" style="font-size:90%;">IMG-ONLY </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.3.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S5.T3.4.3.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.3.3.2.1" class="ltx_text" style="font-size:90%;">  6.19</span></td>
<td id="S5.T3.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.3.3.3.1" class="ltx_text" style="font-size:90%;">  7.93</span></td>
<td id="S5.T3.4.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.3.3.4.1" class="ltx_text" style="font-size:90%;">34.36</span></td>
<td id="S5.T3.4.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.3.3.5.1" class="ltx_text" style="font-size:90%;">29.59</span></td>
<td id="S5.T3.4.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.3.3.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.4.4" class="ltx_tr">
<th id="S5.T3.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.4.4.1.1" class="ltx_text" style="font-size:90%;">QUES-ONLY </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S5.T3.4.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.4.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.4.2.1" class="ltx_text" style="font-size:90%;">25.57</span></td>
<td id="S5.T3.4.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.4.3.1" class="ltx_text" style="font-size:90%;">39.66</span></td>
<td id="S5.T3.4.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.4.4.1" class="ltx_text" style="font-size:90%;">39.24</span></td>
<td id="S5.T3.4.4.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.4.5.1" class="ltx_text" style="font-size:90%;">49.56</span></td>
<td id="S5.T3.4.4.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.4.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.5.5" class="ltx_tr">
<th id="S5.T3.4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T3.4.5.5.1.1" class="ltx_text" style="font-size:90%;">MULTI-WORLD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S5.T3.4.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.5.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.5.5.2.1" class="ltx_text" style="font-size:90%;">  7.86</span></td>
<td id="S5.T3.4.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.5.5.3.1" class="ltx_text" style="font-size:90%;">12.73</span></td>
<td id="S5.T3.4.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.5.5.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.5.5.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.5.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.5.5.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.6.6" class="ltx_tr">
<th id="S5.T3.4.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.6.6.1.1" class="ltx_text" style="font-size:90%;">ASK-NEURON </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S5.T3.4.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.6.6.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.6.6.2.1" class="ltx_text" style="font-size:90%;">21.67</span></td>
<td id="S5.T3.4.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.6.6.3.1" class="ltx_text" style="font-size:90%;">34.68</span></td>
<td id="S5.T3.4.6.6.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.6.6.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.6.6.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.6.6.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.6.6.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.6.6.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.7.7" class="ltx_tr">
<th id="S5.T3.4.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.7.7.1.1" class="ltx_text" style="font-size:90%;">ENSEMBLE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S5.T3.4.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.7.7.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.7.7.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.7.7.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.7.7.3.1" class="ltx_text" style="font-size:90%;">36.94</span></td>
<td id="S5.T3.4.7.7.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.7.7.4.1" class="ltx_text" style="font-size:90%;">57.84</span></td>
<td id="S5.T3.4.7.7.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.7.7.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.7.7.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.7.7.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.8.8" class="ltx_tr">
<th id="S5.T3.4.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.8.8.1.1" class="ltx_text" style="font-size:90%;">LSTM Q+I </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.8.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S5.T3.4.8.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.8.8.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.8.8.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.8.8.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.8.8.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.8.8.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.8.8.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.8.8.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.8.8.5.1" class="ltx_text" style="font-size:90%;">54.06</span></td>
<td id="S5.T3.4.8.8.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.8.8.6.1" class="ltx_text" style="font-size:90%;">57.17</span></td>
</tr>
<tr id="S5.T3.4.9.9" class="ltx_tr">
<th id="S5.T3.4.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.9.9.1.1" class="ltx_text" style="font-size:90%;">iBOWIMG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.9.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S5.T3.4.9.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.9.9.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.9.9.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.9.9.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.9.9.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.9.9.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.9.9.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.9.9.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.9.9.5.1" class="ltx_text" style="font-size:90%;">55.89</span></td>
<td id="S5.T3.4.9.9.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.9.9.6.1" class="ltx_text" style="font-size:90%;">61.97</span></td>
</tr>
<tr id="S5.T3.4.10.10" class="ltx_tr">
<th id="S5.T3.4.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.10.10.1.1" class="ltx_text" style="font-size:90%;">DPPNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.10.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a><span id="S5.T3.4.10.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.10.10.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.10.10.2.1" class="ltx_text" style="font-size:90%;">28.98</span></td>
<td id="S5.T3.4.10.10.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.10.10.3.1" class="ltx_text" style="font-size:90%;">44.48</span></td>
<td id="S5.T3.4.10.10.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.10.10.4.1" class="ltx_text" style="font-size:90%;">61.19</span></td>
<td id="S5.T3.4.10.10.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.10.10.5.1" class="ltx_text" style="font-size:90%;">57.36</span></td>
<td id="S5.T3.4.10.10.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.10.10.6.1" class="ltx_text" style="font-size:90%;">62.69</span></td>
</tr>
<tr id="S5.T3.4.11.11" class="ltx_tr">
<th id="S5.T3.4.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.11.11.1.1" class="ltx_text" style="font-size:90%;">SMem </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.11.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="S5.T3.4.11.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.11.11.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.11.11.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.11.11.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.11.11.3.1" class="ltx_text" style="font-size:90%;">40.07</span></td>
<td id="S5.T3.4.11.11.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.11.11.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.11.11.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.11.11.5.1" class="ltx_text" style="font-size:90%;">58.24</span></td>
<td id="S5.T3.4.11.11.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.11.11.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.12.12" class="ltx_tr">
<th id="S5.T3.4.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.12.12.1.1" class="ltx_text" style="font-size:90%;">SAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.12.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="S5.T3.4.12.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.12.12.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.12.12.2.1" class="ltx_text" style="font-size:90%;">29.3</span></td>
<td id="S5.T3.4.12.12.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.12.12.3.1" class="ltx_text" style="font-size:90%;">45.5</span></td>
<td id="S5.T3.4.12.12.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.12.12.4.1" class="ltx_text" style="font-size:90%;">61.6</span></td>
<td id="S5.T3.4.12.12.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.12.12.5.1" class="ltx_text" style="font-size:90%;">58.9</span></td>
<td id="S5.T3.4.12.12.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.12.12.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.13.13" class="ltx_tr">
<th id="S5.T3.4.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.13.13.1.1" class="ltx_text" style="font-size:90%;">NMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.13.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S5.T3.4.13.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.13.13.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.13.13.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.13.13.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.13.13.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.13.13.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.13.13.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.13.13.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.13.13.5.1" class="ltx_text" style="font-size:90%;">58.7</span></td>
<td id="S5.T3.4.13.13.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.13.13.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.14.14" class="ltx_tr">
<th id="S5.T3.4.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.14.14.1.1" class="ltx_text" style="font-size:90%;">D-NMN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.14.14.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S5.T3.4.14.14.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.14.14.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.14.14.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.14.14.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.14.14.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.14.14.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.14.14.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.14.14.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.14.14.5.1" class="ltx_text" style="font-size:90%;">59.4</span></td>
<td id="S5.T3.4.14.14.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.14.14.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.15.15" class="ltx_tr">
<th id="S5.T3.4.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.15.15.1.1" class="ltx_text" style="font-size:90%;">FDA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.15.15.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S5.T3.4.15.15.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.15.15.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.15.15.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.15.15.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.15.15.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.15.15.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.15.15.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.15.15.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.15.15.5.1" class="ltx_text" style="font-size:90%;">59.54</span></td>
<td id="S5.T3.4.15.15.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.15.15.6.1" class="ltx_text" style="font-size:90%;">64.18</span></td>
</tr>
<tr id="S5.T3.4.16.16" class="ltx_tr">
<th id="S5.T3.4.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.16.16.1.1" class="ltx_text" style="font-size:90%;">HYBRID </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.16.16.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S5.T3.4.16.16.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.16.16.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.16.16.2.1" class="ltx_text" style="font-size:90%;">28.96</span></td>
<td id="S5.T3.4.16.16.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.16.16.3.1" class="ltx_text" style="font-size:90%;">45.17</span></td>
<td id="S5.T3.4.16.16.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.16.16.4.1" class="ltx_text" style="font-size:90%;">63.18</span></td>
<td id="S5.T3.4.16.16.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.16.16.5.1" class="ltx_text" style="font-size:90%;">60.06</span></td>
<td id="S5.T3.4.16.16.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.16.16.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.17.17" class="ltx_tr">
<th id="S5.T3.4.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.17.17.1.1" class="ltx_text" style="font-size:90%;">DMN+ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.17.17.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="S5.T3.4.17.17.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.17.17.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.17.17.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.17.17.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.17.17.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.17.17.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.17.17.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.17.17.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.17.17.5.1" class="ltx_text" style="font-size:90%;">60.4</span></td>
<td id="S5.T3.4.17.17.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.17.17.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.18.18" class="ltx_tr">
<th id="S5.T3.4.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.18.18.1.1" class="ltx_text" style="font-size:90%;">MRN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.18.18.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S5.T3.4.18.18.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.18.18.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.18.18.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.18.18.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.18.18.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.18.18.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.18.18.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.18.18.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.18.18.5.1" class="ltx_text" style="font-size:90%;">61.84</span></td>
<td id="S5.T3.4.18.18.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.18.18.6.1" class="ltx_text" style="font-size:90%;">66.33</span></td>
</tr>
<tr id="S5.T3.4.19.19" class="ltx_tr">
<th id="S5.T3.4.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.19.19.1.1" class="ltx_text" style="font-size:90%;">HieCoAtten </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.19.19.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S5.T3.4.19.19.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.19.19.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.19.19.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.19.19.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.19.19.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.19.19.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.19.19.4.1" class="ltx_text" style="font-size:90%;">65.4</span></td>
<td id="S5.T3.4.19.19.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.19.19.5.1" class="ltx_text" style="font-size:90%;">62.1</span></td>
<td id="S5.T3.4.19.19.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.19.19.6.1" class="ltx_text" style="font-size:90%;">66.1</span></td>
</tr>
<tr id="S5.T3.4.20.20" class="ltx_tr">
<th id="S5.T3.4.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.20.20.1.1" class="ltx_text" style="font-size:90%;">RAU_ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.20.20.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S5.T3.4.20.20.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.20.20.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.20.20.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.20.20.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.20.20.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.20.20.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.20.20.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.20.20.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.20.20.5.1" class="ltx_text" style="font-size:90%;">63.2</span></td>
<td id="S5.T3.4.20.20.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.20.20.6.1" class="ltx_text" style="font-size:90%;">67.3</span></td>
</tr>
<tr id="S5.T3.4.21.21" class="ltx_tr">
<th id="S5.T3.4.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.21.21.1.1" class="ltx_text" style="font-size:90%;">DAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.21.21.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S5.T3.4.21.21.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.21.21.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.21.21.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.21.21.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.21.21.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.21.21.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.21.21.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.21.21.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.21.21.5.1" class="ltx_text" style="font-size:90%;">64.2</span></td>
<td id="S5.T3.4.21.21.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.21.21.6.1" class="ltx_text" style="font-size:90%;">69.0</span></td>
</tr>
<tr id="S5.T3.4.22.22" class="ltx_tr">
<th id="S5.T3.4.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.22.22.1.1" class="ltx_text" style="font-size:90%;">MCB+Att </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.22.22.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S5.T3.4.22.22.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.22.22.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.22.22.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.22.22.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.22.22.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.22.22.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.22.22.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.22.22.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.22.22.5.1" class="ltx_text" style="font-size:90%;">64.2</span></td>
<td id="S5.T3.4.22.22.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.22.22.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.23.23" class="ltx_tr">
<th id="S5.T3.4.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.23.23.1.1" class="ltx_text" style="font-size:90%;">MLB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.23.23.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S5.T3.4.23.23.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.23.23.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.23.23.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.23.23.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.23.23.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.23.23.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.23.23.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.23.23.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.23.23.5.1" class="ltx_text" style="font-size:90%;">65.07</span></td>
<td id="S5.T3.4.23.23.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.23.23.6.1" class="ltx_text" style="font-size:90%;">68.89</span></td>
</tr>
<tr id="S5.T3.4.24.24" class="ltx_tr">
<th id="S5.T3.4.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S5.T3.4.24.24.1.1" class="ltx_text" style="font-size:90%;">AMA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.24.24.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S5.T3.4.24.24.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.24.24.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.24.24.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.24.24.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.24.24.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.24.24.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.24.24.4.1" class="ltx_text" style="font-size:90%;">69.73</span></td>
<td id="S5.T3.4.24.24.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.24.24.5.1" class="ltx_text" style="font-size:90%;">59.44</span></td>
<td id="S5.T3.4.24.24.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.4.24.24.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T3.4.25.25" class="ltx_tr">
<th id="S5.T3.4.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S5.T3.4.25.25.1.1" class="ltx_text" style="font-size:90%;">MCB-ensemble </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.4.25.25.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S5.T3.4.25.25.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S5.T3.4.25.25.2" class="ltx_td ltx_align_center"><span id="S5.T3.4.25.25.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.25.25.3" class="ltx_td ltx_align_center"><span id="S5.T3.4.25.25.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.25.25.4" class="ltx_td ltx_align_center"><span id="S5.T3.4.25.25.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.25.25.5" class="ltx_td ltx_align_center"><span id="S5.T3.4.25.25.5.1" class="ltx_text" style="font-size:90%;">66.5</span></td>
<td id="S5.T3.4.25.25.6" class="ltx_td ltx_align_center"><span id="S5.T3.4.25.25.6.1" class="ltx_text" style="font-size:90%;">70.1</span></td>
</tr>
<tr id="S5.T3.4.26.26" class="ltx_tr">
<th id="S5.T3.4.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S5.T3.4.26.26.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">HUMAN</span></th>
<td id="S5.T3.4.26.26.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.4.26.26.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">50.20</span></td>
<td id="S5.T3.4.26.26.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.4.26.26.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.27</span></td>
<td id="S5.T3.4.26.26.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.4.26.26.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">-</span></td>
<td id="S5.T3.4.26.26.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.4.26.26.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">83.30</span></td>
<td id="S5.T3.4.26.26.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.4.26.26.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">91.54</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.25.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.26.2" class="ltx_text" style="font-size:90%;">Overview of different methods that were evaluated on open-ended COCO-VQA and their design choices. Results are report on the ‘test-dev’ split when ‘test-standard’ results are not available (Denoted by *). </span></figcaption>
<table id="S5.T4.23" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1" class="ltx_tr">
<th id="S5.T4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Method</th>
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">
<table id="S5.T4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Accuracy (%)</td>
</tr>
<tr id="S5.T4.1.1.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">(<math id="S5.T4.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="Acc_{VQA}" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.m1.1a"><mrow id="S5.T4.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T4.1.1.1.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.1.1.1.m1.1.1.1" xref="S5.T4.1.1.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T4.1.1.1.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.1.1.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.1.1.1.m1.1.1.1a" xref="S5.T4.1.1.1.1.1.1.m1.1.1.1.cmml">​</mo><msub id="S5.T4.1.1.1.1.1.1.m1.1.1.4" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.cmml"><mi id="S5.T4.1.1.1.1.1.1.m1.1.1.4.2" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.2.cmml">c</mi><mrow id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.cmml"><mi id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.2" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.1" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.1.cmml">​</mo><mi id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.3" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.1a" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.1.cmml">​</mo><mi id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.4" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1"><times id="S5.T4.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.1"></times><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.2">𝐴</ci><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.3">𝑐</ci><apply id="S5.T4.1.1.1.1.1.1.m1.1.1.4.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T4.1.1.1.1.1.1.m1.1.1.4.1.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.4.2.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.2">𝑐</ci><apply id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3"><times id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.1.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.1"></times><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.2.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.2">𝑉</ci><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.3.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.3">𝑄</ci><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.4.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1.4.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.m1.1c">Acc_{VQA}</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S5.T4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">
<table id="S5.T4.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.3.1.1" class="ltx_tr">
<td id="S5.T4.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">CNN</td>
</tr>
<tr id="S5.T4.1.1.3.1.2" class="ltx_tr">
<td id="S5.T4.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Network</td>
</tr>
</table>
</th>
<th id="S5.T4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S5.T4.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Use of</td>
</tr>
<tr id="S5.T4.1.1.4.1.2" class="ltx_tr">
<td id="S5.T4.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Attention</td>
</tr>
</table>
</th>
<th id="S5.T4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S5.T4.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.5.1.1" class="ltx_tr">
<td id="S5.T4.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Ext.</td>
</tr>
<tr id="S5.T4.1.1.5.1.2" class="ltx_tr">
<td id="S5.T4.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Data</td>
</tr>
</table>
</th>
<th id="S5.T4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S5.T4.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.6.1.1" class="ltx_tr">
<td id="S5.T4.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Compo-</td>
</tr>
<tr id="S5.T4.1.1.6.1.2" class="ltx_tr">
<td id="S5.T4.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">sitional</td>
</tr>
</table>
</th>
<td id="S5.T4.1.1.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S5.T4.23.24.1" class="ltx_tr">
<td id="S5.T4.23.24.1.1" class="ltx_td ltx_align_left ltx_border_t">LSTM Q+I <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S5.T4.23.24.1.2" class="ltx_td ltx_align_left ltx_border_t">54.1</td>
<td id="S5.T4.23.24.1.3" class="ltx_td ltx_align_left ltx_border_t">VGGNet</td>
<td id="S5.T4.23.24.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T4.23.24.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T4.23.24.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T4.23.24.1.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T4.23.25.2" class="ltx_tr">
<td id="S5.T4.23.25.2.1" class="ltx_td ltx_align_left">iBOWIMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S5.T4.23.25.2.2" class="ltx_td ltx_align_left">55.9</td>
<td id="S5.T4.23.25.2.3" class="ltx_td ltx_align_left">GoogLeNet</td>
<td id="S5.T4.23.25.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.25.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.25.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.25.2.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.23.26.3" class="ltx_tr">
<td id="S5.T4.23.26.3.1" class="ltx_td ltx_align_left">DPPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S5.T4.23.26.3.2" class="ltx_td ltx_align_left">57.4</td>
<td id="S5.T4.23.26.3.3" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.23.26.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.26.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.26.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.26.3.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.2.2" class="ltx_tr">
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_left">SMem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S5.T4.2.2.3" class="ltx_td ltx_align_left">58.2</td>
<td id="S5.T4.2.2.4" class="ltx_td ltx_align_left">GoogLeNet</td>
<td id="S5.T4.2.2.1" class="ltx_td ltx_align_center"><svg id="S5.T4.2.2.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.2.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.2.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.2.2.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.3.3" class="ltx_tr">
<td id="S5.T4.3.3.2" class="ltx_td ltx_align_left">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S5.T4.3.3.3" class="ltx_td ltx_align_left">58.9</td>
<td id="S5.T4.3.3.4" class="ltx_td ltx_align_left">GoogLeNet</td>
<td id="S5.T4.3.3.1" class="ltx_td ltx_align_center"><svg id="S5.T4.3.3.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.3.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.3.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.3.3.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.5.5" class="ltx_tr">
<td id="S5.T4.5.5.3" class="ltx_td ltx_align_left">NMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S5.T4.5.5.4" class="ltx_td ltx_align_left">58.7</td>
<td id="S5.T4.5.5.5" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.4.4.1" class="ltx_td ltx_align_center"><svg id="S5.T4.4.4.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.5.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.5.5.2" class="ltx_td ltx_align_center"><svg id="S5.T4.5.5.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.5.5.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.7.7" class="ltx_tr">
<td id="S5.T4.7.7.3" class="ltx_td ltx_align_left">D-NMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S5.T4.7.7.4" class="ltx_td ltx_align_left">59.4</td>
<td id="S5.T4.7.7.5" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.6.6.1" class="ltx_td ltx_align_center"><svg id="S5.T4.6.6.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.7.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.7.7.2" class="ltx_td ltx_align_center"><svg id="S5.T4.7.7.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.7.7.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.8.8" class="ltx_tr">
<td id="S5.T4.8.8.2" class="ltx_td ltx_align_left">AMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S5.T4.8.8.3" class="ltx_td ltx_align_left">59.4</td>
<td id="S5.T4.8.8.4" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.8.8.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.8.8.1" class="ltx_td ltx_align_center"><svg id="S5.T4.8.8.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.8.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.8.8.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.9.9" class="ltx_tr">
<td id="S5.T4.9.9.2" class="ltx_td ltx_align_left">FDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S5.T4.9.9.3" class="ltx_td ltx_align_left">59.5</td>
<td id="S5.T4.9.9.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.9.9.1" class="ltx_td ltx_align_center"><svg id="S5.T4.9.9.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.9.9.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.9.9.6" class="ltx_td"></td>
<td id="S5.T4.9.9.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.23.27.4" class="ltx_tr">
<td id="S5.T4.23.27.4.1" class="ltx_td ltx_align_left">HYBRID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S5.T4.23.27.4.2" class="ltx_td ltx_align_left">60.1</td>
<td id="S5.T4.23.27.4.3" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.23.27.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.27.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.27.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.27.4.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.10.10" class="ltx_tr">
<td id="S5.T4.10.10.2" class="ltx_td ltx_align_left">DMN+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S5.T4.10.10.3" class="ltx_td ltx_align_left">60.4</td>
<td id="S5.T4.10.10.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.10.10.1" class="ltx_td ltx_align_center"><svg id="S5.T4.10.10.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.10.10.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.10.10.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.10.10.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.11.11" class="ltx_tr">
<td id="S5.T4.11.11.2" class="ltx_td ltx_align_left">MRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S5.T4.11.11.3" class="ltx_td ltx_align_left">61.8</td>
<td id="S5.T4.11.11.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.11.11.1" class="ltx_td ltx_align_center"><svg id="S5.T4.11.11.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.11.11.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.11.11.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.11.11.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.12.12" class="ltx_tr">
<td id="S5.T4.12.12.2" class="ltx_td ltx_align_left">HieCoAtten-VGG* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S5.T4.12.12.3" class="ltx_td ltx_align_left">60.5</td>
<td id="S5.T4.12.12.4" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.12.12.1" class="ltx_td ltx_align_center"><svg id="S5.T4.12.12.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.12.12.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.12.12.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.12.12.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.13.13" class="ltx_tr">
<td id="S5.T4.13.13.2" class="ltx_td ltx_align_left">HieCoAtten-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S5.T4.13.13.3" class="ltx_td ltx_align_left">62.1</td>
<td id="S5.T4.13.13.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.13.13.1" class="ltx_td ltx_align_center"><svg id="S5.T4.13.13.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.13.13.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.13.13.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.13.13.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.14.14" class="ltx_tr">
<td id="S5.T4.14.14.2" class="ltx_td ltx_align_left">RAU_VGG* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S5.T4.14.14.3" class="ltx_td ltx_align_left">61.3</td>
<td id="S5.T4.14.14.4" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.14.14.1" class="ltx_td ltx_align_center"><svg id="S5.T4.14.14.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.14.14.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.14.14.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.14.14.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.15.15" class="ltx_tr">
<td id="S5.T4.15.15.2" class="ltx_td ltx_align_left">RAU_ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S5.T4.15.15.3" class="ltx_td ltx_align_left">63.2</td>
<td id="S5.T4.15.15.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.15.15.1" class="ltx_td ltx_align_center"><svg id="S5.T4.15.15.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.15.15.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.15.15.6" class="ltx_td"></td>
<td id="S5.T4.15.15.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.23.28.5" class="ltx_tr">
<td id="S5.T4.23.28.5.1" class="ltx_td ltx_align_left">MCB* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T4.23.28.5.2" class="ltx_td ltx_align_left">61.2</td>
<td id="S5.T4.23.28.5.3" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.23.28.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.28.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.28.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.28.5.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.16.16" class="ltx_tr">
<td id="S5.T4.16.16.2" class="ltx_td ltx_align_left">MCB-ATT* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T4.16.16.3" class="ltx_td ltx_align_left">64.2</td>
<td id="S5.T4.16.16.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.16.16.1" class="ltx_td ltx_align_center"><svg id="S5.T4.16.16.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.16.16.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.16.16.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.16.16.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.17.17" class="ltx_tr">
<td id="S5.T4.17.17.2" class="ltx_td ltx_align_left">DAN-VGG* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S5.T4.17.17.3" class="ltx_td ltx_align_left">62.0</td>
<td id="S5.T4.17.17.4" class="ltx_td ltx_align_left">VGGNet</td>
<td id="S5.T4.17.17.1" class="ltx_td ltx_align_center"><svg id="S5.T4.17.17.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.17.17.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.17.17.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.17.17.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.18.18" class="ltx_tr">
<td id="S5.T4.18.18.2" class="ltx_td ltx_align_left">DAN-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S5.T4.18.18.3" class="ltx_td ltx_align_left">64.3</td>
<td id="S5.T4.18.18.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.18.18.1" class="ltx_td ltx_align_center"><svg id="S5.T4.18.18.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.18.18.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.18.18.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.18.18.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.19.19" class="ltx_tr">
<td id="S5.T4.19.19.2" class="ltx_td ltx_align_left">MLB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S5.T4.19.19.3" class="ltx_td ltx_align_left">65.1</td>
<td id="S5.T4.19.19.4" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.19.19.1" class="ltx_td ltx_align_center"><svg id="S5.T4.19.19.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.19.19.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.19.19.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.19.19.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.21.21" class="ltx_tr">
<td id="S5.T4.21.21.3" class="ltx_td ltx_align_left">MLB+VG* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S5.T4.21.21.4" class="ltx_td ltx_align_left">65.8</td>
<td id="S5.T4.21.21.5" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.20.20.1" class="ltx_td ltx_align_center"><svg id="S5.T4.20.20.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.21.21.2" class="ltx_td ltx_align_center"><svg id="S5.T4.21.21.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.21.21.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.21.21.7" class="ltx_td"></td>
</tr>
<tr id="S5.T4.23.23" class="ltx_tr">
<td id="S5.T4.23.23.3" class="ltx_td ltx_align_left">MCB-ensemble <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T4.23.23.4" class="ltx_td ltx_align_left">66.5</td>
<td id="S5.T4.23.23.5" class="ltx_td ltx_align_left">ResNet</td>
<td id="S5.T4.22.22.1" class="ltx_td ltx_align_center"><svg id="S5.T4.22.22.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.23.23.2" class="ltx_td ltx_align_center"><svg id="S5.T4.23.23.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S5.T4.23.23.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.23.23.7" class="ltx_td"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">A large number of VQA algorithms have been proposed in the past three years. All existing methods consist of 1) extracting image features (image featurization), 2) extracting question features (question featurization), and 3) an algorithm that combines these features to produce an answer. For image features, most algorithms use CNNs that are pre-trained on ImageNet, with common examples being VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. A wider variety of question featurizations have been explored, including bag-of-words (BOW), long short term memory (LSTM) encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, gated recurrent units (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, and skip-thought vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. To generate an answer, the most common approach is to treat VQA as a classification problem. In this framework, the image and question features are the input to the classification system and each unique answer is treated as a distinct category. As illustrated in Figure <a href="#S5.F8" title="Figure 8 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the featurization scheme and the classification system can take widely varied forms. These systems differ significantly in how they integrate the question and image features. Some examples include:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Combining the image and question features using simple mechanisms, e.g., concatenation, elementwise multiplication, or elementwise addition, and then giving them to a linear classifier or a neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>,</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Combining the image and question features using bilinear pooling or related schemes in a neural network framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>,</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Having a classifier that uses the question features to compute spatial attention maps for the visual features or that adaptively scales local features based on their relative importance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>,</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">Using Bayesian models that exploit the underlying relationships between question-image-answer feature distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p">Using the question to break the VQA task into a series of sub-problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</p>
</div>
</li>
</ul>
<p id="S5.p1.2" class="ltx_p">In later subsections, we describe each of these classification-based approaches in detail.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">While the classification framework is used by most open-ended VQA algorithms, this approach can only generate answers that are seen during training, prompting some to explore alternatives. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> an LSTM is used to produce multi-word answer one word at a time. However, the answer produced is still limited to words seen during training. For multiple-choice VQA, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> proposed treating VQA as a ranking problem, where a system is trained to produced a score for each possible multiple-choice answer, question, and image trio, and then it selects the highest scoring answer choice.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In the following subsections, we group VQA algorithms based on their common themes. Results on DAQUAR, COCO-QA, and COCO-VQA for these methods are given in Table <a href="#S5.T3" title="Table 3 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, in increasing order of performance. In Table <a href="#S5.T3" title="Table 3 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we report plain accuracy for DAQUAR and COCO-QA, and we report <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="Accuracy_{VQA}" display="inline"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1a" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.4" xref="S5.p3.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1b" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.5" xref="S5.p3.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1c" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.6" xref="S5.p3.1.m1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1d" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.7" xref="S5.p3.1.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1e" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.8" xref="S5.p3.1.m1.1.1.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1f" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><msub id="S5.p3.1.m1.1.1.9" xref="S5.p3.1.m1.1.1.9.cmml"><mi id="S5.p3.1.m1.1.1.9.2" xref="S5.p3.1.m1.1.1.9.2.cmml">y</mi><mrow id="S5.p3.1.m1.1.1.9.3" xref="S5.p3.1.m1.1.1.9.3.cmml"><mi id="S5.p3.1.m1.1.1.9.3.2" xref="S5.p3.1.m1.1.1.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.9.3.1" xref="S5.p3.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.9.3.3" xref="S5.p3.1.m1.1.1.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.9.3.1a" xref="S5.p3.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S5.p3.1.m1.1.1.9.3.4" xref="S5.p3.1.m1.1.1.9.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><times id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1"></times><ci id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2">𝐴</ci><ci id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">𝑐</ci><ci id="S5.p3.1.m1.1.1.4.cmml" xref="S5.p3.1.m1.1.1.4">𝑐</ci><ci id="S5.p3.1.m1.1.1.5.cmml" xref="S5.p3.1.m1.1.1.5">𝑢</ci><ci id="S5.p3.1.m1.1.1.6.cmml" xref="S5.p3.1.m1.1.1.6">𝑟</ci><ci id="S5.p3.1.m1.1.1.7.cmml" xref="S5.p3.1.m1.1.1.7">𝑎</ci><ci id="S5.p3.1.m1.1.1.8.cmml" xref="S5.p3.1.m1.1.1.8">𝑐</ci><apply id="S5.p3.1.m1.1.1.9.cmml" xref="S5.p3.1.m1.1.1.9"><csymbol cd="ambiguous" id="S5.p3.1.m1.1.1.9.1.cmml" xref="S5.p3.1.m1.1.1.9">subscript</csymbol><ci id="S5.p3.1.m1.1.1.9.2.cmml" xref="S5.p3.1.m1.1.1.9.2">𝑦</ci><apply id="S5.p3.1.m1.1.1.9.3.cmml" xref="S5.p3.1.m1.1.1.9.3"><times id="S5.p3.1.m1.1.1.9.3.1.cmml" xref="S5.p3.1.m1.1.1.9.3.1"></times><ci id="S5.p3.1.m1.1.1.9.3.2.cmml" xref="S5.p3.1.m1.1.1.9.3.2">𝑉</ci><ci id="S5.p3.1.m1.1.1.9.3.3.cmml" xref="S5.p3.1.m1.1.1.9.3.3">𝑄</ci><ci id="S5.p3.1.m1.1.1.9.3.4.cmml" xref="S5.p3.1.m1.1.1.9.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">Accuracy_{VQA}</annotation></semantics></math> for COCO-VQA. Table <a href="#S5.T4" title="Table 4 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> breaks down the results for COCO-VQA based on the techniques used in each paper.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baseline Models</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Baseline methods help determine the difficulty of a dataset, and establish the minimal level of performance that a more sophisticated algorithms should exceed. For VQA, the simplest baselines are random guessing and guessing the most repeated answers. A widely used baseline classification system is to apply a linear or non-linear, e.g., multi-layer perceptron (MLP), classifier to the image and question features after they have been combined into a single vector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Common methods to combine the features include concatenation, the elementwise product, or the elementwise sum. Combining these schemes has also been explored and can lead to improved results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">A variety of featurization approaches have been used with baseline classification frameworks. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, the authors used a bag-of-words to represent the question and CNN features from GoogLeNet for the visual features. They then fed concatenation of these features into a multi-class logistic regression classifier. Their approach worked well, surpassing the previous baseline on COCO-VQA, which used a theoretically more powerful model, an LSTM, to represent the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> used skip-thought vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> for question features and ResNet-152 to extract image features. They found that an MLP model with two hidden layers trained on these off-the-shelf features worked well for all datasets. However, in their work a linear classifier outperformed the MLP model on smaller datasets, likely due to the MLP model overfitting.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Several VQA algorithms have used LSTMs to encode questions. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, an LSTM encoder acting on a one-hot encoding of the sentence was used to represent question features, and GoogLeNet was used for image features. The dimensionality of the CNN features was reduced to match the dimensionality of the LSTM encoding, and then the Hadamard product of these two vectors was used to fuse them together. The fused vector was used as input to an MLP with two hidden layers. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, an LSTM model was fed an embedding of each word sequentially with CNN features concatenated to it. This continued until the end of the question was reached. The subsequent time-steps were used to generate a list of answers. A related approach was used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, where an LSTM was fed CNN features during the first and last time-steps, with word features in between. The image features acted as the first and last words in the sentence. The LSTM network was followed by a softmax classifier to predict the answer. A similar approach was used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, but the CNN image features were only fed into the LSTM at the end of the question and instead of a classifier, another LSTM was used to generate the answer one word at a time.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Bayesian and Question-Aware Models</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">VQA requires drawing inferences and modeling relationships between the question and the image. Once the questions and images are featurized, modeling co-occurrence statistics of the question and image features can be helpful for drawing inferences about the correct answers. Two major Bayesian VQA frameworks have explored modeling these relationships. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the first Bayesian framework for VQA was proposed. The authors used semantic segmentation to identify the objects in an image and their positions. Then, a Bayesian algorithm was trained to model the spatial relationships of the objects, which was used to compute each answer’s probability. This was the earliest known algorithm for VQA, but its efficacy is surpassed by simple baseline models. This is partially due to it being dependent on the results of the semantic segmentation, which was imperfect.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">A very different Bayesian model was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. The model exploited the fact that the type of answer can be predicted using solely the question. For example, ‘What color is the flower?’ would be assigned as a color question by the model, essentially turning the open-ended problem into a multiple-choice one. To do this, the model used a variant of quadratic discriminant analysis, which modeled the probability of image features given the question features and the answer type. ResNet-152 was used for the image features, and skip-thought vectors were used to represent the question.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Attention Based Models</h3>

<figure id="S5.F9" class="ltx_figure"><img src="/html/1610.01465/assets/images/attention.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="300" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.6.3.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.4.2" class="ltx_text" style="font-size:90%;">This figure illustrates a common way to incorporate attention into a VQA system. A convolutional layer in a CNN outputs a <math id="S5.F9.3.1.m1.1" class="ltx_Math" alttext="K\times K\times N" display="inline"><semantics id="S5.F9.3.1.m1.1b"><mrow id="S5.F9.3.1.m1.1.1" xref="S5.F9.3.1.m1.1.1.cmml"><mi id="S5.F9.3.1.m1.1.1.2" xref="S5.F9.3.1.m1.1.1.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S5.F9.3.1.m1.1.1.1" xref="S5.F9.3.1.m1.1.1.1.cmml">×</mo><mi id="S5.F9.3.1.m1.1.1.3" xref="S5.F9.3.1.m1.1.1.3.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S5.F9.3.1.m1.1.1.1b" xref="S5.F9.3.1.m1.1.1.1.cmml">×</mo><mi id="S5.F9.3.1.m1.1.1.4" xref="S5.F9.3.1.m1.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F9.3.1.m1.1c"><apply id="S5.F9.3.1.m1.1.1.cmml" xref="S5.F9.3.1.m1.1.1"><times id="S5.F9.3.1.m1.1.1.1.cmml" xref="S5.F9.3.1.m1.1.1.1"></times><ci id="S5.F9.3.1.m1.1.1.2.cmml" xref="S5.F9.3.1.m1.1.1.2">𝐾</ci><ci id="S5.F9.3.1.m1.1.1.3.cmml" xref="S5.F9.3.1.m1.1.1.3">𝐾</ci><ci id="S5.F9.3.1.m1.1.1.4.cmml" xref="S5.F9.3.1.m1.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.3.1.m1.1d">K\times K\times N</annotation></semantics></math> tensor of feature responses, corresponding to <math id="S5.F9.4.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.F9.4.2.m2.1b"><mi id="S5.F9.4.2.m2.1.1" xref="S5.F9.4.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.F9.4.2.m2.1c"><ci id="S5.F9.4.2.m2.1.1.cmml" xref="S5.F9.4.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.4.2.m2.1d">N</annotation></semantics></math> feature maps. One way to apply attention to this representation is by suppressing or enhancing the features at different spatial locations. Using the question features with these local image features, a weighting factor for each grid location can be computed that determines the spatial location’s relevance to the question, which can then be used to compute attention-weighted image features.</span></figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Using global features alone may obscure task-relevant regions of the input space. Attentive models attempt to overcome this limitation. These models learn to ‘attend’ to the most relevant regions of the input space. Attention models have shown great successes in other vision and NLP tasks, such as object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In VQA, numerous models have used spatial attention to create region-specific CNN features, rather than using global features from the entire image. Fewer models have also explored incorporating attention into the text representation. The basic idea behind all these models is that certain visual regions in an image and certain words in a question are more informative than others for answering a given question. For example, for a system answering ‘What color is the umbrella?’ the image region containing the umbrella is more informative than other image regions. Similarly, ‘color’ and ‘umbrella’ are the textual inputs that need to be addressed more directly than the others. Global image features, e.g., the last hidden layer of a CNN, and global text features, e.g., bag-of-words, skip-thoughts etc. may not be granular enough to address region specific questions.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Before using spatially attentive mechanisms, an algorithm must represent the visual features at all spatial regions, instead of solely at the global level. Then, local features from relevant regions can be given higher prominence based on the question asked. There are two common ways to achieve local feature encoding. As shown in Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3 Attention Based Models ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, one way to do this is to impose a uniform grid over all image locations, with the local image features present at each grid location. This is often done by operating on the last CNN layer prior to the final spatial pooling that flattens the features. The relevance of each grid location is then determined by the question. An alternative way to implement spatial attention is to generate region proposals (bounding boxes) for an image, encode each of these boxes using a CNN, and then determine the relevance of each box’s features using the question. While multiple papers have focused on using spatial visual attention for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, there are significant differences among these methods.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">The Focus Regions for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and Focused Dynamic Attention (FDA) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> both used Edge Boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> to generate bounding box region proposals for images. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, a CNN was used to extract features from each of these boxes. The input to their VQA system consisted of these CNN features, question features, and one of the multiple choice answers. Their system was trained to produced a score for each multiple-choice answer, and the highest scoring answer was selected. The score is calculated using a weighted average of scores from each of the regions where the weights are simply learned by passing the dot product of regional CNN feature and question embedding to a fully connected layer.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">In FDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, the authors proposed to only use the region proposals that have the objects mentioned in the question. Their VQA algorithm requires as input a list of bounding boxes with their corresponding object label. During training, the object labels and bounding boxes are obtained from COCO annotations. During test, the labels are obtained by classifying each bounding box using ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Subsequently, word2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> was used to compute the similarity between words in the question and the object labels assigned to each of the bounding boxes. Any box with a score greater than 0.5 is successively fed into an LSTM network. At the last time-step, global CNN features from the entire image are also fed into the network, giving it access to both global and local features. A separate LSTM was also used as the question representation. The output from these two LSTMs are then fed into a fully connected layer that is fed to a softmax classifier to produce the answer predictions.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.2" class="ltx_p">In contrast to using region proposals, the Stacked Attention Network (SAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and the Dynamic Memory Network (DMN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> models both used visual features from the spatial grid of a CNN’s feature maps (see Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3 Attention Based Models ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Both <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> used the last convolutional layer from VGG-19 with <math id="S5.SS3.p6.1.m1.1" class="ltx_Math" alttext="448\times 448" display="inline"><semantics id="S5.SS3.p6.1.m1.1a"><mrow id="S5.SS3.p6.1.m1.1.1" xref="S5.SS3.p6.1.m1.1.1.cmml"><mn id="S5.SS3.p6.1.m1.1.1.2" xref="S5.SS3.p6.1.m1.1.1.2.cmml">448</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.p6.1.m1.1.1.1" xref="S5.SS3.p6.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS3.p6.1.m1.1.1.3" xref="S5.SS3.p6.1.m1.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.1.m1.1b"><apply id="S5.SS3.p6.1.m1.1.1.cmml" xref="S5.SS3.p6.1.m1.1.1"><times id="S5.SS3.p6.1.m1.1.1.1.cmml" xref="S5.SS3.p6.1.m1.1.1.1"></times><cn type="integer" id="S5.SS3.p6.1.m1.1.1.2.cmml" xref="S5.SS3.p6.1.m1.1.1.2">448</cn><cn type="integer" id="S5.SS3.p6.1.m1.1.1.3.cmml" xref="S5.SS3.p6.1.m1.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.1.m1.1c">448\times 448</annotation></semantics></math> images to produce a <math id="S5.SS3.p6.2.m2.1" class="ltx_Math" alttext="14\times 14" display="inline"><semantics id="S5.SS3.p6.2.m2.1a"><mrow id="S5.SS3.p6.2.m2.1.1" xref="S5.SS3.p6.2.m2.1.1.cmml"><mn id="S5.SS3.p6.2.m2.1.1.2" xref="S5.SS3.p6.2.m2.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.p6.2.m2.1.1.1" xref="S5.SS3.p6.2.m2.1.1.1.cmml">×</mo><mn id="S5.SS3.p6.2.m2.1.1.3" xref="S5.SS3.p6.2.m2.1.1.3.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.2.m2.1b"><apply id="S5.SS3.p6.2.m2.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1"><times id="S5.SS3.p6.2.m2.1.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1.1"></times><cn type="integer" id="S5.SS3.p6.2.m2.1.1.2.cmml" xref="S5.SS3.p6.2.m2.1.1.2">14</cn><cn type="integer" id="S5.SS3.p6.2.m2.1.1.3.cmml" xref="S5.SS3.p6.2.m2.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.2.m2.1c">14\times 14</annotation></semantics></math> filter response map with 512 dimensional features at each grid location.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p">In SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, an attention layer is specified by a single layer of weights that uses the question and the CNN feature map with a softmax activation function to compute the attention distribution across image locations. This distribution is then applied to the CNN feature map to pool across spatial feature locations using a weighted sum, which generates a global image representation that emphasizes certain spatial regions more than others. This feature vector is then combined with a vector of question features to create a representation that can be used with a softmax layer to predict the answer. They generalized this approach to handle multiple (stacked) attention layers, enabling the system to model complex relationships among multiple objects in an image.</p>
</div>
<div id="S5.SS3.p8" class="ltx_para">
<p id="S5.SS3.p8.1" class="ltx_p">A similar attentive mechanism was used in the Spatial Memory Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> model, where spatial attention is produced by estimating the correlation of image patches with individual words in the question. This word-guided attention is used to predict an attention distribution, which is then used to compute the weighted sum of the visual features embedding across image regions. Two different models were then explored. In the one-hop model, the features encoding the entire question are combined with the weighted visual features to predict the answer. In the two-hop model, the combination of the visual and question features is looped back into the attentive mechanism for refining the attention distribution.</p>
</div>
<div id="S5.SS3.p9" class="ltx_para">
<p id="S5.SS3.p9.1" class="ltx_p">Another approach that incorporated spatial attention using CNN feature maps is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. To do this, they used a modified Dynamic Memory Network (DMN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. A DMN consists of an input module, an episodic memory module, and an answering module. DMNs have been used for text based QA, where each word in a sentence is fed into a recurrent neural network and the output of the network is used to extract ‘facts.’ Then, the episodic memory module makes multiple passes over a subset of these facts. With each pass, the internal memory representation of the network is updated. An answering module uses the final state of the memory representation and the input question to predict an answer. To use a DMN for VQA, they used visual facts in addition to text. To generate visual facts, the CNN features at each spatial grid location are treated as words in a sentence that are sequentially fed into a recurrent neural network. The episodic memory module then makes passes through both text and visual facts to update its memory. The answering module remains unchanged.</p>
</div>
<div id="S5.SS3.p10" class="ltx_para">
<p id="S5.SS3.p10.1" class="ltx_p">The Hierarchical Co-Attention model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> applies attention to both the image and question to jointly reason about the two different streams of information. The model’s approach to visual attention is similar to the method used in Spatial Memory Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. In addition to visual attention, this method uses a hierarchical encoding of the question, in which the encoding occurs at the word level (using a one-hot encoding), at the phrase level (using bi- or tri-gram window size), and at the question level (using the final time-step of an LSTM network). Using this hierarchical question representation, the authors proposed to use two different attentive mechanisms. The parallel co-attention approach simultaneously attended to both the question and image. The alternative co-attention approach alternated between attending to the question or the image. This approach allowed the relevance of words in the question and the relevance of specific image regions to be determined by each other. The answer prediction is made by recursively combining the co-attended features from all three levels of the question hierarchy.</p>
</div>
<div id="S5.SS3.p11" class="ltx_para">
<p id="S5.SS3.p11.2" class="ltx_p">Using joint attention for image and question features was also explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. The main idea is to allow image and question attention to guide each other, directing attention to relevant words and visual regions simultaneously. To achieve this, visual and question input are jointly represented by a memory vector that is used to simultaneously predict attention for both question and image features. The attentive mechanism computes updated image and question representations, which are then used to recursively update the memory vector. This recursive memory update mechanism can be repeated <math id="S5.SS3.p11.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS3.p11.1.m1.1a"><mi id="S5.SS3.p11.1.m1.1.1" xref="S5.SS3.p11.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p11.1.m1.1b"><ci id="S5.SS3.p11.1.m1.1.1.cmml" xref="S5.SS3.p11.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p11.1.m1.1c">K</annotation></semantics></math> times to refine the attention in multiple steps. The authors’ found that a value of <math id="S5.SS3.p11.2.m2.1" class="ltx_Math" alttext="K=2" display="inline"><semantics id="S5.SS3.p11.2.m2.1a"><mrow id="S5.SS3.p11.2.m2.1.1" xref="S5.SS3.p11.2.m2.1.1.cmml"><mi id="S5.SS3.p11.2.m2.1.1.2" xref="S5.SS3.p11.2.m2.1.1.2.cmml">K</mi><mo id="S5.SS3.p11.2.m2.1.1.1" xref="S5.SS3.p11.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS3.p11.2.m2.1.1.3" xref="S5.SS3.p11.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p11.2.m2.1b"><apply id="S5.SS3.p11.2.m2.1.1.cmml" xref="S5.SS3.p11.2.m2.1.1"><eq id="S5.SS3.p11.2.m2.1.1.1.cmml" xref="S5.SS3.p11.2.m2.1.1.1"></eq><ci id="S5.SS3.p11.2.m2.1.1.2.cmml" xref="S5.SS3.p11.2.m2.1.1.2">𝐾</ci><cn type="integer" id="S5.SS3.p11.2.m2.1.1.3.cmml" xref="S5.SS3.p11.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p11.2.m2.1c">K=2</annotation></semantics></math> worked best for COCO-VQA.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Bilinear Pooling Methods</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">VQA relies on jointly analyzing the image and the question. Early models did this by combining their respective features using simple methods, e.g., concatenation or using an element-wise product between the question and image features, but more complex interactions would be possible with an outer-product between these two streams of information. Similar ideas were shown to work well for improving fine-grained image recognition  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. Below, we describe the two most prominent VQA methods that have used bilinear pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, Multimodal Compact Bilinear (MCB) pooling was proposed as a novel method for combining image and text features in VQA. This idea is to approximate the outer-product between the image and text features, allowing a deeper interaction between the two modalities, compared to other mechanisms, e.g., concatenation or element-wise multiplication. Rather than doing the outer-product explicitly, which would be very high dimensional, MCB does the outer-product in a lower dimensional space. This is then used to predict which spatial features are relevant to the question. In a variation of this model, a soft-attention mechanism, similar to the method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, was also used, with the only major change being the use of MCB for combining text and question features instead of element-wise multiplication in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. This combination yielded very good results on COCO-VQA, and it was the winner of the 2016 VQA Challenge workshop.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, the authors’ argued that MCB is too computationally expensive, despite using an approximate outer-product. Instead, they proposed to use a multi-modal low-rank bilinear pooling (MLB) scheme that uses the Hadamard product and a linear mapping to achieve approximate bilinear pooling. When used with a spatial visual attention mechanism, MLB rivaled MCB at VQA, but with reduced computational complexity and using a neural network with fewer parameters.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Compositional VQA Models</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">In VQA, questions often require multiple steps of reasoning to answer properly. For example, questions like ‘What is to the left of the horse?’ can involve first finding the horse, and then naming the object to the left of it. Two compositional frameworks have been proposed for VQA that attempt to tackle solving VQA in a series of sub-steps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. The Neural Module Network (NMN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> framework uses external question parsers to find the sub-task in the question whereas Recurrent Answering Units (RAU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> is trained end-to-end and sub-tasks can be implicitly learned.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">NMN is an especially interesting approach to VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. The NMN framework treats VQA as a sequence of sub-tasks that are carried out by separate neural sub-networks. Each of the sub-network performs a single well-defined task, e.g., the <span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_typewriter">find[X]</span> module produces a heat map for the presence of certain object. Other modules include <span id="S5.SS5.p2.1.2" class="ltx_text ltx_font_typewriter">describe</span>, <span id="S5.SS5.p2.1.3" class="ltx_text ltx_font_typewriter">measure</span>, and <span id="S5.SS5.p2.1.4" class="ltx_text ltx_font_typewriter">transform</span>. These modules then must be assembled into a meaningful layout. Two methods have been explored for inferring the required layout. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, a natural language parser is used on the input question to both find the sub-tasks in the question and to infer the required layout of the sub-tasks that when executed in sequence would produce an answer to the given question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. For example, answering ‘What color is the tie?’ would involve executing the <span id="S5.SS5.p2.1.5" class="ltx_text ltx_font_typewriter">find[tie]</span> module followed by the <span id="S5.SS5.p2.1.6" class="ltx_text ltx_font_typewriter">describe[color]</span> module, which generates the answer. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, the same group explored using algorithms to dynamically select the best layout for the given question from a set of automatically generated layout candidates.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">The RAU model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> can implicitly perform compositional reasoning without depending on an external language parser. In their model, they used multiple self-contained answering units that can solve VQA sub-tasks. These answering units are arranged in recurrent manner. Each answering unit on the chain is equipped with an attentive mechanism derived from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and a classifier. The authors’ claimed that the inclusion of multiple recurrent answering units allows inferring the answer from a series of sub-tasks solved by each answering unit. However, they did not perform visualization or ablation studies to show how the answer might get refined in each time-step. This makes it difficult to assess whether progressive refinement and reasoning is occurring or not, especially considering that the complete image and question information is available to all answering units at every time step and that only the output from the first answering unit is used during the test stage.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Other Noteworthy Models</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">Answering questions about images can often require information beyond what can be directly inferred by analyzing the image. Having knowledge about the uses and typical context for the objects present in an image can be helpful for VQA. For example, a VQA system that has access to a knowledge bank could use it to answer questions about particular animals, such as their habitats, colors, sizes, and feeding habits. This idea was explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, and they demonstrated that the knowledge bank improved performance. The external knowledge bases were tailored to general information obtained from DBpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, and it is possible that using a source tailored to VQA could yield greater improvement.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, the authors’ incorporated a Dynamic Parameter Prediction layer into the fully connected layers of a CNN. The parameters of this layer are predicted from the question by using a recurrent neural network. This allows the visual features that the model uses to be specific to the question before the final classification step. This approach can be seen as a kind of implicit attentive mechanism in that it modifies the visual input based on the question.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p id="S5.SS6.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, Multimodal Residual Networks (MRN) were proposed for VQA, which were motivated by the success of the ResNet architecture in image classification. Their system is a modification of ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to use both visual and question features in the residual mapping. The visual and question embedding are allowed to have their own residual blocks with skip connections. However, after each residual block the visual data is inter-weaved with the question embedding. The authors explored several alternate arrangement for constructing the residual architecture with multi-modal input and chose the above network based on performance.</p>
</div>
</section>
<section id="S5.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7 </span>What methods and techniques work better?</h3>

<div id="S5.SS7.p1" class="ltx_para">
<p id="S5.SS7.p1.1" class="ltx_p">Although many methods have been proposed for VQA, it is difficult to determine what general techniques are superior. Table <a href="#S5.T4" title="Table 4 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides a breakdown of the different algorithms evaluated on COCO-VQA based on the techniques and design choices that they utilize. Table <a href="#S5.T4" title="Table 4 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> also includes ablation models from respective algorithms, whenever possible. The ablation models help us to identify the individual contributions of the design choices made by the authors. The first observation we can make is that ResNet produces superior performance over VGGNet or GoogLeNet across multiple algorithms. This is evident from the models that use identical setup and only change the image representation. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, an increase of 2% was observed by using ResNet-101 instead of the VGG-16 CNN for image features. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, they found an increase of 1.3% when making the same change in their model. Similarly, changing VGG-19 to ResNet-152 increased performance by
2.3% in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. In all cases, rest of the architecture was kept unchanged.</p>
</div>
<div id="S5.SS7.p2" class="ltx_para">
<p id="S5.SS7.p2.1" class="ltx_p">In general, we believe that spatial attention can be used to increase performance for a model. This is shown by experiments in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, where the models were evaluated with and without attention, and the attentive version performed better in both cases. However, attention alone does not appear to be sufficient. We further discuss this in Section <a href="#S6.SS2" title="6.2 How useful is attention for VQA? ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>.</p>
</div>
<div id="S5.SS7.p3" class="ltx_para">
<p id="S5.SS7.p3.1" class="ltx_p">Bayesian and compositional architectures do not significantly improve over comparable models, despite being interesting approaches. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, the model performed competitively only after it was combined with an MLP model. It is unclear whether the increase was due to model averaging or the proposed Bayesian method. Similarly, the NMN models in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> do not outperform comparable non-compositional models, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. It is possible that both of these methods perform well on specific VQA sub-tasks, e.g., NMN was shown to be specially helpful for positional reasoning questions in the SHAPES dataset. However, since major datasets do not provide a detailed breakdown of question types, it is not possible to quantify how systems perform on specific question types. Moreover, any improvements on rare question types will have negligible impact on the overall performance score, making it difficult to properly evaluate the benefits of these methods. We further discuss these issues in Section <a href="#S6.SS3" title="6.3 Bias Impairs Method Evaluation ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<figure id="S6.F10" class="ltx_figure"><img src="/html/1610.01465/assets/images/increase.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="397" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F10.4.2.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S6.F10.2.1" class="ltx_text" style="font-size:90%;">Current state-of-the-art results across datasets compared to the earliest baseline and human performance. The earliest baseline refers to the numbers reported by the creators of the datasets and the current state-of-the-art models are chosen from the highest performing methods in Table <a href="#S5.T3" title="Table 3 ‣ 5 Algorithms for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. DAQUAR, DAQUAR-37 and COCO-QA report plain accuracy and COCO-VQA reports <math id="S6.F10.2.1.m1.1" class="ltx_Math" alttext="Accuracy_{VQA}" display="inline"><semantics id="S6.F10.2.1.m1.1b"><mrow id="S6.F10.2.1.m1.1.1" xref="S6.F10.2.1.m1.1.1.cmml"><mi id="S6.F10.2.1.m1.1.1.2" xref="S6.F10.2.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.3" xref="S6.F10.2.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1b" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.4" xref="S6.F10.2.1.m1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1c" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.5" xref="S6.F10.2.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1d" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.6" xref="S6.F10.2.1.m1.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1e" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.7" xref="S6.F10.2.1.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1f" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.8" xref="S6.F10.2.1.m1.1.1.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.1g" xref="S6.F10.2.1.m1.1.1.1.cmml">​</mo><msub id="S6.F10.2.1.m1.1.1.9" xref="S6.F10.2.1.m1.1.1.9.cmml"><mi id="S6.F10.2.1.m1.1.1.9.2" xref="S6.F10.2.1.m1.1.1.9.2.cmml">y</mi><mrow id="S6.F10.2.1.m1.1.1.9.3" xref="S6.F10.2.1.m1.1.1.9.3.cmml"><mi id="S6.F10.2.1.m1.1.1.9.3.2" xref="S6.F10.2.1.m1.1.1.9.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.9.3.1" xref="S6.F10.2.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.9.3.3" xref="S6.F10.2.1.m1.1.1.9.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S6.F10.2.1.m1.1.1.9.3.1b" xref="S6.F10.2.1.m1.1.1.9.3.1.cmml">​</mo><mi id="S6.F10.2.1.m1.1.1.9.3.4" xref="S6.F10.2.1.m1.1.1.9.3.4.cmml">A</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.F10.2.1.m1.1c"><apply id="S6.F10.2.1.m1.1.1.cmml" xref="S6.F10.2.1.m1.1.1"><times id="S6.F10.2.1.m1.1.1.1.cmml" xref="S6.F10.2.1.m1.1.1.1"></times><ci id="S6.F10.2.1.m1.1.1.2.cmml" xref="S6.F10.2.1.m1.1.1.2">𝐴</ci><ci id="S6.F10.2.1.m1.1.1.3.cmml" xref="S6.F10.2.1.m1.1.1.3">𝑐</ci><ci id="S6.F10.2.1.m1.1.1.4.cmml" xref="S6.F10.2.1.m1.1.1.4">𝑐</ci><ci id="S6.F10.2.1.m1.1.1.5.cmml" xref="S6.F10.2.1.m1.1.1.5">𝑢</ci><ci id="S6.F10.2.1.m1.1.1.6.cmml" xref="S6.F10.2.1.m1.1.1.6">𝑟</ci><ci id="S6.F10.2.1.m1.1.1.7.cmml" xref="S6.F10.2.1.m1.1.1.7">𝑎</ci><ci id="S6.F10.2.1.m1.1.1.8.cmml" xref="S6.F10.2.1.m1.1.1.8">𝑐</ci><apply id="S6.F10.2.1.m1.1.1.9.cmml" xref="S6.F10.2.1.m1.1.1.9"><csymbol cd="ambiguous" id="S6.F10.2.1.m1.1.1.9.1.cmml" xref="S6.F10.2.1.m1.1.1.9">subscript</csymbol><ci id="S6.F10.2.1.m1.1.1.9.2.cmml" xref="S6.F10.2.1.m1.1.1.9.2">𝑦</ci><apply id="S6.F10.2.1.m1.1.1.9.3.cmml" xref="S6.F10.2.1.m1.1.1.9.3"><times id="S6.F10.2.1.m1.1.1.9.3.1.cmml" xref="S6.F10.2.1.m1.1.1.9.3.1"></times><ci id="S6.F10.2.1.m1.1.1.9.3.2.cmml" xref="S6.F10.2.1.m1.1.1.9.3.2">𝑉</ci><ci id="S6.F10.2.1.m1.1.1.9.3.3.cmml" xref="S6.F10.2.1.m1.1.1.9.3.3">𝑄</ci><ci id="S6.F10.2.1.m1.1.1.9.3.4.cmml" xref="S6.F10.2.1.m1.1.1.9.3.4">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F10.2.1.m1.1d">Accuracy_{VQA}</annotation></semantics></math>.</span></figcaption>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">As shown in Figure <a href="#S6.F10" title="Figure 10 ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, there has been rapid improvement in the performance of VQA algorithms, but there is still a significant gap between the best methods and humans. It remains unclear whether the improvements in performance come from the mechanisms incorporated into later systems, e.g., attention, or if it is due to other factors. Moreover, it can be difficult to decouple the contributions of text and image data in isolation. There are also numerous challenges to comparing algorithms due to the variations in how they are evaluated. In this section, we discuss each of these issues.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Vision vs. Language in VQA</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">VQA consists of two distinct data streams that need to be correctly used to ensure robust performance: images and questions. But, do current systems adequately use both vision and language? Ablation studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> have routinely shown that question only models perform drastically better than image only models, especially on open-ended COCO-VQA. On COCO-QA, simple image-blind models that use only the question can achieve 50% accuracy with the gain from using the image being comparatively modest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, it was also shown that for DAQUAR-37, using a better language embedding with an image-blind model produced results superior to earlier works employing both images and questions. This is primarily due to two factors. First, the question severely constrains the kinds of answers expected in many cases, essentially turning an open-ended question into a multiple-choice one, e.g., questions about the color of an object will have a color as an answer. Second, the datasets tend to have strong bias. These two factors make language a much stronger prior than the image features alone.</p>
</div>
<figure id="S6.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/207.png" id="S6.F11.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="716" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.sf1.6.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F11.sf1.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Q:<span id="S6.F11.sf1.7.2.1" class="ltx_text ltx_font_medium"> What are they doing? </span>A:<span id="S6.F11.sf1.7.2.2" class="ltx_text ltx_font_medium"> Playing baseball 
<br class="ltx_break"></span>Q:<span id="S6.F11.sf1.7.2.3" class="ltx_text ltx_font_medium"> What are they playing? </span>A:<span id="S6.F11.sf1.7.2.4" class="ltx_text ltx_font_medium"> Soccer</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1610.01465/assets/images/askimage_bias.png" id="S6.F11.sf2.g1" class="ltx_graphics ltx_img_portrait" width="479" height="699" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.sf2.6.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F11.sf2.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Q:<span id="S6.F11.sf2.7.2.1" class="ltx_text ltx_font_medium"> Is the weather rainy in the picture? </span>A:<span id="S6.F11.sf2.7.2.2" class="ltx_text ltx_font_medium"> Yes 
<br class="ltx_break"></span>Q:<span id="S6.F11.sf2.7.2.3" class="ltx_text ltx_font_medium"> Is it rainy in the picture? </span>A:<span id="S6.F11.sf2.7.2.4" class="ltx_text ltx_font_medium"> No </span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S6.F11.3.2" class="ltx_text" style="font-size:90%;">Slight variations in the way a question is phased causes current VQA algorithms to produce different answers. The left example uses the system in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and the right example uses the system from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</span></figcaption>
</figure>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">The predictive power of language over images have been corroborated by ablation studies. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, the authors studied a model that had been trained using both image and question features. They then studied how the predictions of the model differed when it was given only the image or only the question, compared to when it was given both. They found that the image-only model’s predictions differed from the combined model 40% more often than the question only model. They also showed that the way the question is phrased strongly biases the answer. When training a neural network, these regularities will be incorporated into the model. While this produces increased performance on the dataset, it is potentially detrimental to creating a general VQA system.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, bias in VQA was studied using synthetic cartoon images. They created a dataset with solely binary questions, in which the same question could be asked about two images that were mostly identical, except for a minor change that caused the correct answer to be different. They found that a model trained on an unbalanced version of this dataset performed 11% worse (absolute difference) on a balanced test dataset compared to a model trained on a balanced version of the dataset.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">We conducted two experiments to assess the effect of language bias in VQA. First, we used the model<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>An online demo is available here: <a target="_blank" href="http://visualqa.csail.mit.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://visualqa.csail.mit.edu/</a></span></span></span> from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. This model was trained on COCO-VQA, and it allows the contribution of the question and image features to be assessed independently by splitting the weights of the softmax output layer into image and question components. We asked simple binary questions with a relatively equal prior for both choices so that the image must be analyzed to answer the question. Examples are shown in Figure <a href="#S6.F12" title="Figure 12 ‣ 6.1 Vision vs. Language in VQA ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. We can see that the system performs poorly, especially when considering that the baseline accuracy for yes/no questions for COCO-VQA is about 80%. Next, we studied how language bias affected the more complex MCB-ensemble model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> that was trained on COCO-VQA. This model was the winner of the 2016 VQA Challenge workshop. To do this, we created a small dataset consisting only of yes/no questions. To create this dataset, we used annotations from the validation split of the COCO dataset to determine whether an image contained a person, and then asked an equal number of ‘yes’ and ‘no’ questions about whether there are any people present. We used the questions ‘Are there any people in the photo?’, ‘Is there a person in the picture?’, and ‘Is there a person in the photo?’ For each variation, there were 38,514 yes/no questions (115,542 total). The accuracy of MCB-ensemble on this dataset was worse than chance (47%), which starkly contrasts with its results on COCO-VQA (i.e., 83% on COCO-VQA yes/no questions). This is likely due to severe bias in the training dataset, and not due to an inability for MCB to learn the task.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">As shown in Figure <a href="#S6.F11" title="Figure 11 ‣ 6.1 Vision vs. Language in VQA ‣ 6 Discussion ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, VQA systems are sensitive to the way a question is phrased. We observed similar results when using the system in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. To quantify this issue, we created another toy dataset from the validation split of the COCO dataset and used it to evaluate the MCB-ensemble model that was trained on COCO-VQA. In this toy dataset, the task is to identify which sport was being played. We asked three variations of the same question: 1) ‘What are they doing?’, 2) ‘What are they playing?’, and 3) ‘What sport are they playing?’ Each variation contains 5,237 questions about seven common sports (15,711 questions total). MCB-ensemble achieved 33.6% for variation 1, 78% for variation 2, and 86.4% for variation 3. The dramatic increase in performance from variation 1 to 2 is caused by the inclusion of keyword ‘playing’ instead of the generic verb ‘doing.’ The increment from variation 2 to 3 is caused by explicitly including the keyword ‘sport’ in the question. This suggests that VQA systems are over-dependent on language ‘clues’ that annotators often include. Taken together, these experiments show that language bias is an issue that critically affects the performance of current VQA systems.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.1" class="ltx_p">In conclusion, current VQA systems are more dependent on the question than the image content. Language bias in datasets critically affects the performance of the current VQA systems, which limits their deployment. New VQA datasets must endeavor to compensate for this issue, by either having questions that force analysis of image content and/or by making datasets less biased.</p>
</div>
<figure id="S6.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F12.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/1610.01465/assets/images/223.png" id="S6.F12.sf1.g1" class="ltx_graphics ltx_img_landscape" width="568" height="379" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F12.sf1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">no<span id="S6.F12.sf1.5.2.1" class="ltx_text ltx_font_medium"> (11.07 w/ 2.57 [image] + 8.50 [word])
<br class="ltx_break"></span>yes<span id="S6.F12.sf1.5.2.2" class="ltx_text ltx_font_medium"> (10.94 w/ 2.71 [image] + 8.23 [word])</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F12.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/1610.01465/assets/images/453.png" id="S6.F12.sf2.g1" class="ltx_graphics ltx_img_landscape" width="568" height="426" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F12.sf2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">yes<span id="S6.F12.sf2.5.2.1" class="ltx_text ltx_font_medium"> (12.45 w/ 4.22 [image] + 8.23 [word])
<br class="ltx_break"></span>no<span id="S6.F12.sf2.5.2.2" class="ltx_text ltx_font_medium"> (12.05 w/ 3.55 [image] + 8.50 [word])</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F12.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/1610.01465/assets/images/419.png" id="S6.F12.sf3.g1" class="ltx_graphics ltx_img_landscape" width="568" height="378" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.sf3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S6.F12.sf3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">no<span id="S6.F12.sf3.5.2.1" class="ltx_text ltx_font_medium"> (12.04 w/ 3.54 [image] + 8.50 [word])
<br class="ltx_break"></span>yes<span id="S6.F12.sf3.5.2.2" class="ltx_text ltx_font_medium"> (11.96 w/ 3.72 [image] + 8.23 [word])</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F12.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/1610.01465/assets/images/138.png" id="S6.F12.sf4.g1" class="ltx_graphics ltx_img_landscape" width="568" height="426" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.sf4.4.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S6.F12.sf4.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">yes<span id="S6.F12.sf4.5.2.1" class="ltx_text ltx_font_medium"> (12.30 w/ 4.07 [image] + 8.23 [word])
<br class="ltx_break"></span>no<span id="S6.F12.sf4.5.2.2" class="ltx_text ltx_font_medium"> (12.14 w/ 3.64 [image] + 8.50 [word])</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S6.F12.3.2" class="ltx_text" style="font-size:90%;">Using the system in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, the answer score for the question ‘Are there any people in the picture?’ is roughly the same for ‘yes’ (8.23) and ‘no’ (8.50). Answering the question correctly requires examining the image, but the model fails to appropriately use the image information.</span></figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>How useful is attention for VQA?</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">It is difficult to determine how much attention helps VQA algorithms. In ablation studies, when attentive mechanisms are removed from models it impairs their performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Currently, the best model for COCO-VQA does employ spatial visual attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, but simple models that do not use attention have been shown to exceed earlier models that used complex attentive mechanisms. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, for example, an attention-free model that used multiple global image feature representations (VGG-19, ResNet-101, and ResNet-152), instead of a single CNN, performed very well compared some attentive models. They combined image and question features using both element-wise multiplication and addition, instead of solely concatenating them. Combined with ensembling, this yielded results significantly higher than the complex attention-based models used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Similar results have been obtained by other systems that do not employ spatial attention, e.g, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. Attention alone does not ensure good VQA performance, but incorporating attention into a VQA model appears to improve performance over the same model when attention is not used.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, the authors showed that methods commonly used to incorporate spatial attention to specific image features do not cause models to attend to the same regions as humans tasked with VQA. They made this observation using both the attentive mechanisms used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. This may be because the regions the model learns to attend to are discriminative due to biases in the dataset and not due to where the algorithm should attend. For example, when asked a question about whether drapes are in an image, the algorithm may instead look at the bottom of the image for a bed rather than windows because questions about drapes tend to be found in bedrooms. This is an indication that attentive mechanisms may not be correctly deployed due to biases.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Bias Impairs Method Evaluation</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Dataset bias significantly impairs the ability to evaluate VQA algorithms. Questions that require the use of the image content are often relatively easy to answer. Many are about the presence of objects or scene attributes. These questions tend to be handled well by CNNs and also have strong language biases. Harder questions, such as those beginning with ‘Why’ are comparatively rare. This has serious implications for evaluating performance. For COCO-VQA (train and validation partitions), a system that improves accuracy on questions beginning with ‘Is’ and ’Are’ by 15% will increase overall accuracy by 5%. However, the same increase in both ‘Why’ and ‘Where’ questions will only increase accuracy by 0.6%. In fact, even if all ‘Why’ and ‘Where’ questions are answered correctly, the overall increase in accuracy will only be 4.1%. On the other hand, answering ‘yes’ to all questions beginning with ‘Is there’ will yield an accuracy of 85.2% on those questions. These problems could be overcome if each <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_italic">type</span> of question was evaluated in isolation, and then the mean accuracy across question types was used instead of overall accuracy for benchmarking the algorithms. This approach is similar to the mean per-class accuracy metric used for evaluating object classification algorithms, which was adopted due to bias in the amount of test data available for different object categories.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Are Binary Questions Sufficient?</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Using binary (yes/no or true/false) questions to evaluate algorithms has attracted significant discussion in the VQA community. The main argument against using binary questions is the lack of complex questions and the relative ease in answering the questions that are typically generated by human annotators. Visual Genome and Visual7W exclude binary questions altogether. The authors argued that this choice would encourage more complex questions from the annotators.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">On the other hand, binary questions are easy to evaluate and these questions can, in theory, encompass an enormous variety of tasks. The SHAPES dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> uses binary questions exclusively but contains complex questions involving spatial reasoning, counting, and drawing inferences (see Figure <a href="#S3.F6" title="Figure 6 ‣ 3.7 SHAPES ‣ 3 Datasets for VQA ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Using cartoon images, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> also showed that these questions can be especially difficult for VQA algorithms when the dataset is balanced. However, there are challenges to creating balanced binary questions for real world imagery. In COCO-VQA, ‘yes’ is a much more common answer than ‘no,’ simply because people tend to ask questions biased toward ‘yes’ as an answer.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">As long as bias is controlled, yes/no questions can play an important role in future VQA benchmarks, but a VQA system should be capable of more than solely binary questions so that its abilities can be fully assessed. All real-world applications for VQA, such as enabling the blind to ask questions about visual content, require the output of the VQA system to be open-ended. A system that can solely handle binary questions will have limited real-world utility.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Open Ended vs. Multiple Choice</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">Because it is challenging to evaluate open-ended multi-word answers, multiple-choice has been proposed as a way to evaluate VQA algorithms. As long as the alternatives are sufficiently difficult, a system could be evaluated in this manner but then be deployed to answer open-ended questions. For these reasons, multiple choice is used to evaluate Visual7W, Visual Genome, and a variant of The VQA Dataset. In this framework, an algorithm has access to a number of possible answers (e.g., 18 for COCO-VQA), along with the question and image. It must then select among possible choices.</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">A major problem with multiple-choice evaluation is that the problem can be reduced to determining which of the answers is correct instead of actually answering the question. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, they formulated VQA as an answer scoring task, where the system was trained to produce a score based on the image, question, and potential answers. The answers themselves were fed into the system as features. It achieved state-of-the-art results on Visual7W and rivals the best methods on COCO-VQA, with their method performing better than many complex systems that use attention. To a large extent, we believe their system performed well because it learned to better exploit biases in the answers instead of reasoning about images. On Visual7W, they showed that a variant of their system that used solely the answers and was both image- and question-blind rivaled baselines using the question and image.</p>
</div>
<div id="S6.SS5.p3" class="ltx_para">
<p id="S6.SS5.p3.1" class="ltx_p">We argue that any VQA system should be able to operate without being given answers as inputs. Multiple-choice can be an important ingredient for evaluating multi-word answers, but it alone is not sufficient. When multiple-choice is used, the choices must be selected carefully to ensure that a question is hard and not deducible from the provided answers alone. A system that is solely capable of operating with answers provided is not really solving VQA, because these are not available when a system is deployed.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Recommendations for Future VQA Datasets</h2>

<figure id="S7.F13" class="ltx_figure"><img src="/html/1610.01465/assets/images/limited_training.png" id="S7.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S7.F13.3.2" class="ltx_text" style="font-size:90%;">The graph showing test accuracy as a function of available training data on the COCO-VQA dataset.</span></figcaption>
</figure>
<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Existing VQA benchmarks are not sufficient to evaluate whether an algorithm has ‘solved’ VQA. In this section, we discuss future developments in VQA datasets that will make them better benchmarks for the problem.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Future datasets need to be larger. While VQA datasets have been growing in size and diversity, algorithms do not have enough data for training and evaluation. We did a small experiment where we trained a simple MLP baseline model for VQA using ResNet-152 image features and skip-thought features for the questions, and we assessed performance as a function of the amount of training data available on COCO-VQA. The results are shown in Figure <a href="#S7.F13" title="Figure 13 ‣ 7 Recommendations for Future VQA Datasets ‣ Visual Question Answering: Datasets, Algorithms, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, where it is clear that the curve has not started to approach an asymptote. This suggests that even on datasets that are biased, increasing the size of the dataset could significantly improve accuracy. However, this does not mean that increasing the size of the dataset is sufficient to turn it into a good benchmark, because humans tend to create questions with strong biases.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Future datasets need to be less biased. We have repeatedly discussed the problem of bias in existing VQA datasets in this paper, and pointed out the kinds of problems these biases cause for truly evaluating a VQA algorithm. For real-world open-ended VQA, this will be difficult to achieve without carefully instructing the humans that generate the questions. Bias has long been a problem in images used for computer vision datasets (for a review see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>), and for VQA this problem is compounded by bias in the questions as well.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">In addition to being larger and less biased, future datasets need more nuanced analysis for benchmarking. All of the publicly released datasets use evaluation metrics that treat every question with equal weight, but some kinds of questions are far easier, either because of bias or because existing algorithms excel at answering that kind of question, e.g., object recognition questions. Some datasets such as COCO-QA have divided VQA questions into distinct categories, e.g., for COCO-QA these are color, counting (number), location, and object. We believe that mean per-question type performance should replace standard accuracy, so each question would not have equal weight in evaluating performance. This would go a long way towards making a VQA algorithm have to perform well at a wide variety of question types to perform well overall, otherwise a system that excelled at answering ‘Why’ questions but was slightly worse than another model at more common questions would not be fairly evaluated. To do this, each question would need to be assigned a category. We believe this effort would make benchmark results significantly more meaningful. The scores on each question type could also be used to compare algorithms to see which kind of questions they excel at.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">VQA is an important basic research problem in computer vision and natural language processing that requires a system to do much more than task specific algorithms, such as object recognition and object detection. An algorithm that can answer arbitrary questions about images would be a milestone in artificial intelligence. We believe that VQA should be a necessary part of any visual Turing test.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">In this paper, we critically reviewed existing datasets and algorithms for VQA. We discussed the challenges of evaluating answers generated by algorithms, especially multi-word answers. We described how biases and other problems plague existing datasets. This is a major problem, and the field needs a dataset that evaluates the important characteristics of a VQA algorithm, so that if an algorithm performs well on that dataset then it means it is doing well on VQA in general.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Future work on VQA involves the creation of larger and far more varied datasets. Bias in these datasets will be difficult to overcome, but evaluating different kinds of questions individually in a nuanced manner, rather than using naive accuracy alone, will help significantly. Further work will be needed to develop VQA algorithms that can reason about image content, but these algorithms may lead to significant new areas of research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We thank Ronald Kemker for helpful comments on an earlier draft of this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International Conference on Learning
Representations (ICLR)</span>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems (NIPS)</span>, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for
visual recognition and description,” in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
“Large-scale video classification with convolutional neural networks,” in
<span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
2014.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Two-stream convolutional networks for action
recognition in videos,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing
Systems (NIPS)</span>, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. Geman, S. Geman, N. Hallonquist, and L. Younes, “Visual turing test for
computer vision systems,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the National Academy of
Sciences</span>, vol. 112, no. 12, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Malinowski and M. Fritz, “Towards a visual turing challenge,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1410.8027</span>, 2014.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object
detection,” in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
(NIPS)</span>, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
semantic segmentation,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. Noh, S. Hong, and B. Han, “Learning deconvolution network for semantic
segmentation,” in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
N. Silberman, D. Sontag, and R. Fergus, “Instance segmentation of indoor
scenes using a coverage loss,” in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">European Conference on Computer
Vision (ECCV)</span>, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Z. Zhang, A. G. Schwing, S. Fidler, and R. Urtasun, “Monocular object instance
segmentation and depth ordering with CNNs,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Zhang, S. Fidler, and R. Urtasun, “Instance-level segmentation with deep
densely connected MRFs,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
image descriptions,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille, “Deep captioning
with multimodal recurrent neural networks (m-rnn),” in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International
Conference on Learning Representations (ICLR)</span>, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image
caption generator,” in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and
Y. Bengio, “Show, attend and tell: Neural image caption generation with
visual attention,” in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning
(ICML)</span>, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for
automatic evaluation of machine translation,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Annual Meeting of the
Association for Computational Linguistics (ACL)</span>, 2002.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Text summarization branches out: Proceedings of the ACL-04 workshop</span>,
vol. 8, Barcelona, Spain, 2004.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evaluation with
improved correlation with human judgments,” in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACL
workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization</span>, vol. 29, pp. 65–72, 2005.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-based image
description evaluation,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Callison-Burch, M. Osborne, and P. Koehn, “Re-evaluation the role of BLEU
in machine translation research.,” 2006.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao,
X. He, M. Mitchell, J. Platt, <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">et al.</span>, “From captions to visual
concepts and back,” in <span id="bib.bib26.2.2" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis,
F. Keller, A. Muscat, and B. Plank, “Automatic description generation from
images: A survey of models, datasets, and evaluation measures,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Journal
of Artificial Intelligence Research</span>, vol. 55, pp. 409–442, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Devlin, S. Gupta, R. Girshick, M. Mitchell, and C. L. Zitnick, “Exploring
nearest neighbor approaches for image captioning,” <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1505.04467</span>, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully convolutional
localization networks for dense captioning,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Malinowski and M. Fritz, “A multi-world approach to question answering
about real-world scenes based on uncertain input,” in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Advances in
Neural Information Processing Systems (NIPS)</span>, 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. Ren, R. Kiros, and R. Zemel, “Exploring models and data for image question
answering,” in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
(NIPS)</span>, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “VQA: Visual question answering,” in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">The IEEE
International Conference on Computer Vision (ICCV)</span>, 2015.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, “Are you talking to a
machine? Dataset and methods for multilingual image question answering,”
in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NIPS)</span>, 2015.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei, “Visual7w: Grounded question
answering in images,” in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Visual genome:
Connecting language and vision using crowdsourced dense image annotations,”
<span id="bib.bib35.2.2" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, vol. 123, no. 1, pp. 32–73,
2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. Kafle and C. Kanan, “Answer-type prediction for visual question
answering,” in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Q. Wu, P. Wang, C. Shen, A. van den Hengel, and A. R. Dick, “Ask me anything:
Free-form visual question answering based on knowledge from external
sources,” in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus, “Simple baseline for
visual question answering,” <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span>, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation and
support inference from rgbd images,” in <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">European Conference on Computer
Vision (ECCV)</span>, 2012.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons: A neural-based
approach to answering questions about images,” in <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">The IEEE
International Conference on Computer Vision (ICCV)</span>, 2015.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
S. Antol, C. L. Zitnick, and D. Parikh, “Zero-shot learning via visual
abstraction,” in <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>, 2014.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh, “Yin and yang:
Balancing and answering binary visual questions,” in <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth,
and L.-J. Li, “Yfcc100m: The new data in multimedia research,” <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, vol. 59, no. 2, pp. 64–73, 2016.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Deep compositional
question answering with neural module networks,” in <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Z. Wu and M. Palmer, “Verbs semantics and lexical selection,” in <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Annual
Meeting of the Association for Computational Linguistics (ACL)</span>, 1994.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach,
“Multimodal compact bilinear pooling for visual question answering and
visual grounding,” in <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Conference on Empirical Methods on Natural
Language Processing (EMNLP)</span>, 2016.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
H. Noh, P. H. Seo, and B. Han, “Image question answering using convolutional
neural network with dynamic parameter prediction,” in <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-guided
spatial attention for visual question answering,” in <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">European
Conference on Computer Vision (ECCV)</span>, 2016.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola, “Stacked attention networks
for image question answering,” in <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Learning to compose neural
networks for question answering,” in <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Annual Meeting of the North
American Chapter of the Association for Computational Linguistics (NAACL)</span>,
2016.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
I. Ilievski, S. Yan, and J. Feng, “A focused dynamic attention model for
visual question answering,” <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.01485</span>, 2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
C. Xiong, S. Merity, and R. Socher, “Dynamic memory networks for visual and
textual question answering,” in <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">International Conference on Machine
Learning (ICML)</span>, 2016.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
J.-H. Kim, S.-W. Lee, D.-H. Kwak, M.-O. Heo, J. Kim, J.-W. Ha, and B.-T. Zhang,
“Multimodal residual learning for visual qa,” in <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems (NIPS)</span>, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
co-attention for visual question answering,” in <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems (NIPS)</span>, 2016.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
H. Noh and B. Han, “Training recurrent answering units with joint loss
minimization for VQA,” <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.03647</span>, 2016.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
H. Nam, J.-W. Ha, and J. Kim, “Dual attention networks for multimodal
reasoning and matching,” <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.00471</span>, 2016.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
J.-H. Kim, K.-W. On, J. Kim, J.-W. Ha, and B.-T. Zhang, “Hadamard product for
low-rank bilinear pooling,” <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.04325</span>, 2016.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2015.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">Neural
computation</span>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using rnn
encoder-decoder for statistical machine translation,” in <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Conference on
Empirical Methods on Natural Language Processing (EMNLP)</span>, 2014.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and
S. Fidler, “Skip-thought vectors,” in <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems (NIPS)</span>, 2015.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
K. Saito, A. Shin, Y. Ushiku, and T. Harada, “Dualnet: Domain-invariant
network for visual question answering,” <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1606.06108</span>, 2016.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem, “Where to look: Focus regions for visual
question answering,” in <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2016.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
A. Jabri, A. Joulin, and L. van der Maaten, “Revisiting visual question
answering baselines,” in <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision
(ECCV)</span>, 2016.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
J. Ba, V. Mnih, and K. Kavukcuoglu, “Multiple object recognition with visual
attention,” in <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations
(ICLR)</span>, 2015.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to
attention-based neural machine translation,” in <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Conference on Empirical
Methods on Natural Language Processing (EMNLP)</span>, 2015.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
learning to align and translate,” in <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">International Conference on
Learning Representations (ICLR)</span>, 2015.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from
edges,” in <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pp. 391–405,
Springer, 2014.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
T. Mikolov and J. Dean, “Distributed representations of words and phrases and
their compositionality,” in <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing
Systems (NIPS)</span>, 2013.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska,
I. Gulrajani, and R. Socher, “Ask me anything: Dynamic memory networks for
natural language processing,” in <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">International Conference on Machine
Learning (ICML)</span>, 2016.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for fine-grained
visual recognition,” in <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">The IEEE International Conference on Computer
Vision (ICCV)</span>, 2015.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
S. Hellmann, M. Morsey, P. van Kleef, S. Auer, <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">et al.</span>, “DBpedia–a
large-scale, multilingual knowledge base extracted from Wikipedia,” <span id="bib.bib72.2.2" class="ltx_text ltx_font_italic">Semantic Web</span>, vol. 6, no. 2, pp. 167–195, 2015.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
A. Agrawal, D. Batra, and D. Parikh, “Analyzing the behavior of visual
question answering models,” in <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Conference on Empirical Methods on
Natural Language Processing (EMNLP)</span>, 2016.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra, “Human attention
in visual question answering: Do humans and deep networks look at the same
regions?,” in <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Conference on Empirical Methods on Natural Language
Processing (EMNLP)</span>, 2016.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
A. Torralba and A. Efros, “Unbiased look at dataset bias,” in <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2011.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1610.01464" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1610.01465" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1610.01465">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1610.01465" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1610.01466" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 10 12:46:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
