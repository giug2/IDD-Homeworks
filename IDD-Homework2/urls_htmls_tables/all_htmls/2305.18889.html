<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.18889] Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks</title><meta property="og:description" content="In this paper, we propose a novel distributed learning scheme, named group-based split federated learning (GSFL), to speed up artificial intelligence (AI) model training. Specifically, the GSFL operates in a split-then…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.18889">

<!--Generated on Thu Feb 29 03:39:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Songge Zhang<sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">∗,⋆</span></sup>, Wen Wu<math id="id2.2.m2.1" class="ltx_Math" alttext="{}^{\star\textrm{,\;{\char 0\relax}}}" display="inline"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mrow id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml"><mi id="id2.2.m2.1.1.1.2" xref="id2.2.m2.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="id2.2.m2.1.1.1.1" xref="id2.2.m2.1.1.1.1.cmml">⋆</mo><mtext id="id2.2.m2.1.1.1.3" xref="id2.2.m2.1.1.1.3a.cmml">, ✉</mtext></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><apply id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"><ci id="id2.2.m2.1.1.1.1.cmml" xref="id2.2.m2.1.1.1.1">⋆</ci><csymbol cd="latexml" id="id2.2.m2.1.1.1.2.cmml" xref="id2.2.m2.1.1.1.2">absent</csymbol><ci id="id2.2.m2.1.1.1.3a.cmml" xref="id2.2.m2.1.1.1.3"><mtext mathsize="70%" id="id2.2.m2.1.1.1.3.cmml" xref="id2.2.m2.1.1.1.3">, ✉</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\star\textrm{,\;{\char 0\relax}}}</annotation></semantics></math>, Penghui Hu<sup id="id11.11.id2" class="ltx_sup">†</sup>,
Shaofeng Li<sup id="id12.12.id3" class="ltx_sup">⋆</sup>, and Ning Zhang<sup id="id13.13.id4" class="ltx_sup">‡</sup>
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Peking University Shenzhen Graduate School, Peking University, Shenzhen, China<sup id="id14.14.id1" class="ltx_sup">∗</sup>
<br class="ltx_break">Frontier Research Center, Peng Cheng Laboratory, Shenzhen, China<sup id="id15.15.id2" class="ltx_sup">⋆</sup>
<br class="ltx_break">School of Electronic Science and Engineering, Nanjing University, Nanjing, China<sup id="id16.16.id3" class="ltx_sup">†</sup>
<br class="ltx_break">Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada<sup id="id17.17.id4" class="ltx_sup">‡</sup>
<br class="ltx_break">Email: <span id="id18.18.id5" class="ltx_text">zhangsongge@stu.pku.edu.cn</span>, <span id="id19.19.id6" class="ltx_text">{wuw02, lishf}@pcl.ac.cn</span>, <span id="id20.20.id7" class="ltx_text">huph197@smail.nju.edu.cn</span>, and <span id="id21.21.id8" class="ltx_text">ning.zhang@uwindsor.ca</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">In this paper, we propose a novel distributed learning scheme, named group-based split federated learning (GSFL), to speed up artificial intelligence (AI) model training. Specifically, the GSFL operates in a <em id="id22.id1.1" class="ltx_emph ltx_font_italic">split-then-federated</em> manner, which consists of three steps: 1) Model distribution, in which the access point (AP) splits the AI models and distributes the client-side models to clients; 2) Model training, in which each client executes forward propagation and transmit the smashed data to the edge server. The edge server executes forward and backward propagation and then returns the gradient to the clients for updating local client-side models; and 3) Model aggregation, in which edge servers aggregate the server-side and client-side models. Simulation results show that the GSFL outperforms vanilla split learning and federated learning schemes in terms of overall training latency while achieving satisfactory accuracy.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">${\textrm{{\char0\relax}}}$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">${\textrm{{\char0\relax}}}$</sup><span class="ltx_note_type">footnotetext: </span>Wen Wu (wuw02@pcl.ac.cn) is the corresponding author of this paper.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Distributed learning paradigms, e.g., federated learning (FL) and split learning (SL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, have been widely applied in the fields of healthcare, finance, and autonomous driving. FL enables parallel training of a shared artificial intelligence (AI) learning model on the local dataset of participating clients and only delivers the model parameters without sharing raw data. FL suffers from significant communication overhead issues in resource-limited wireless networks due to uploading large data-size AI models. SL splits an AI model into two parts at a cut layer, i.e., a client-side model on the client and a server-side model on the edge server. With the participation of multiple clients in SL, long training latency becomes a significant issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Thus, there is a need for designing a novel scheme that can facilitate parallel model training to reduce training latency.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A hybrid federated split learning scheme can reduce communication overhead and computation overload on clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, the simple combination scheme requires equipping each client with a server-side model. When there are many clients, the number of server-side models is large, consuming prohibitive storage resources. Therefore, a new resource-efficient distributed learning scheme is required.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a group-based split federated learning scheme (GSFL) to reduce training delay. The advantage of a group-based strategy is that it can facilitate parallel model training across different clients. This strategy circumvents the sequential training process of all clients, thereby accelerating the overall training process. Specifically, the proposed scheme employs a split-then-federated manner to train clients. In the inter-group, clients sequentially interact with the server to complete split training. In the intra-groups, the groups can share the same server-side model for parallel training to improve training efficiency. Thus, the proposed GSFL scheme can speed up the training process compared with conventional SL and FL. Simulation results show that the proposed scheme can reduce training latency, as well as converge to satisfactory accuracy.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.18889/assets/figure1.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="234" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Proposed GSFL framework.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Group-Based Split Federated Learning Scheme </span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.2" class="ltx_p">As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we consider a generic wireless network scenario, comprising one access point (AP) and <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">N</annotation></semantics></math> clients, i.e., mobile devices. To reduce the communication overhead and computational overload, we partition the clients into <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">M</annotation></semantics></math> groups. Clients within each group do not share their local data but collaboratively train a local AI model. At the AP, an edge server is deployed, featuring abundant computation and storage resources. Concurrently, the edge server hosts multiple identical server-side models.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The proposed scheme mainly consists of three steps: model distribution, model training, and model aggregation. The main purpose of model distribution is to distribute the client-side model to clients. Model training involves model execution, model updating, and model sharing, which is to complete the split learning process. Model aggregation is performed at the AP, where the well-trained models are aggregated into the whole model.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Step 1: Model Distribution</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the model distribution process, the AP adopts a partitioning strategy to partition the original AI training model into distinct segments. This partitioning allows the AP to obtain the initial server-side models and client-side models. The client-side models can be distributed to the first-trained clients in each group using wireless communication links.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Step 2: Model Training in Each Group</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p"><span id="S2.SS2.p1.2.1" class="ltx_text ltx_font_bold">1. Model execution: </span>During the model execution phase of the training process, data sampling, client-side model forward propagation, and server-side model forward propagation are necessary. Specifically, in each training epoch for the <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">n</annotation></semantics></math>-indexed client of the <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">m</annotation></semantics></math>-indexed group, a mini-batch of data samples are sampled from the local data set. Then, the client executes the forward propagation process of model training, inputs the sampled data into the local model, obtains the inference output “smashed data”of the segmentation layer, and transmits it to the AP. Upon receiving the smashed data from the client, the AP inputs it into the server-side model along with prediction results, thereby completing the forward propagation process of the server-side model.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">2. Model updating: </span> In the process of updating the model, updates are required for both the server-side and the client-side models. Specifically, the AP can compute the loss function and its gradient based on the prediction results obtained through forward propagation. The update of the server-side model parameters can be accomplished through methods such as stochastic gradient descent, which completes the backward propagation of the server-side model training. Then, the loss function gradient of the smashed data is transmitted to the corresponding client in the group. Upon receiving the gradient, each client-side model is updated locally, which completes the backward propagation of the client-side model.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">3. Model sharing: </span>In the process of model sharing, the well-trained client-side model is delivered to the next client through the AP. After all clients in each group have completed local training, the last-trained client sends the client-side model to the AP.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Step 3: Model Aggregation among Groups</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">After all groups have completed the model training process, AP aggregates all the server-side models and client-side models into a new one respectively. As such, one training round of the AI model is accomplished. Model aggregation can be conducted through FedAVG. The aggregated model is used in the next training round until satisfactory accuracy is reached.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S2.F2.1" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:181.7pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.3pt,3.4pt) scale(0.96323,0.96323) ;">
<p id="S2.F2.1.1" class="ltx_p"><span id="S2.F2.1.1.1" class="ltx_text">
<span id="S2.F2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:450.2pt;height:188.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S2.F2.1.1.1.1.1" class="ltx_p"><span id="S2.F2.1.1.1.1.1.1" class="ltx_text">
<span id="S2.F2.1.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S2.F2.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S2.F2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-bottom:-5.69054pt;"></span>
<span id="S2.F2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-bottom:-5.69054pt;"></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2305.18889/assets/x1.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="173" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Accuracy</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2305.18889/assets/x2.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="173" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Delay</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Performance comparison with respect to training accuracy and training latency on the GTSRB dataset.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Initial Simulation Results</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">We conduct simulations to evaluate the performance of the proposed GSFL scheme on the GTSRB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The considered network consists of 30 clients, which are divided into 6 groups. We compare the proposed GSFL scheme with three benchmark schemes: centralized learning (CL), vanilla SL, and FL. Fig. <a href="#S2.F2" title="Figure 2 ‣ II-C Step 3: Model Aggregation among Groups ‣ II Group-Based Split Federated Learning Scheme ‣ Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a) presents the comparison results in terms of accuracy, showing the accuracy performance with respect to the number of training rounds. The simulation results demonstrate that the proposed GSFL scheme achieves an accuracy level comparable to that of the SL scheme and CL scheme, indicating its effectiveness in model training. The proposed GSFL scheme requires more training rounds to converge compared to benchmark two schemes due to the model aggregation among groups. In addition, the proposed GSFL scheme exhibits significant advantages over FL, with nearly <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="500\%" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mn id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">500</mn><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">500\%</annotation></semantics></math> improvement in convergence speed, indicating the superiority of the proposed scheme. To evaluate the delay performance, we compare the proposed GSFL scheme and the vanilla SL scheme in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-C Step 3: Model Aggregation among Groups ‣ II Group-Based Split Federated Learning Scheme ‣ Split Federated Learning: Speed up Model Training in Resource-Limited Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b). It can be seen that the GSFL achieves faster convergence and shorter delay compared to the SL scheme, indicating its superiority in optimizing the global model with reduced training delay. Specifically, the proposed GSFL scheme reduces the delay by about <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="31.45\%" display="inline"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mn id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">31.45</mn><mo id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">31.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">31.45\%</annotation></semantics></math>, demonstrating superior performance in terms of speeding up model training.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we have validated that our proposed scheme can greatly reduce training latency while preserving high model accuracy. For the future work, we will study the impact of the cut layer selection and client grouping on the system performance. Meanwhile, rationally allocating communication bandwidth and computing resource is crucial for enhancing system performance, and hence we will design an effective resource allocation scheme for the proposed scheme.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">ACKNOWLEDGEMENT</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported in part by the Peng Cheng Laboratory Major Key Project under Grant PCL2021A09-B2 and by the Natural Science Foundation of China under Grant 6220012314.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
O. Gupta and R. Raskar, “Distributed learning of deep neural network over
multiple agents,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">J. Netw. Comput. Appl.</em>, vol. 116, pp. 1–8, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W. Wu, M. Li, K. Qu, C. Zhou, X. Shen, W. Zhuang, X. Li, and W. Shi, “Split
learning over wireless networks: Parallel design and resource management,”
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas Commun.</em>, vol. 41, no. 4, pp. 1051–1066, Apr. 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Yin, Z. Chen, and M. Tao, “Predictive gan-powered multi-objective
optimization for hybrid federated split learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2209.02428</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. A. Haque, S. Arefin, A. S. M. Shihavuddin, and M. A. Hasan, “Deepthin: A
novel lightweight cnn architecture for traffic sign recognition without gpu
requirements,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Expert Syst. Appl.</em>, vol. 168, p. 114481, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.18888" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.18889" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.18889">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.18889" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.18890" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 03:39:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
