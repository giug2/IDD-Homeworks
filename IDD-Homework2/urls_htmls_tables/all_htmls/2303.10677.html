<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.10677] A Survey of Federated Learning for Connected and Automated Vehicles</title><meta property="og:description" content="Connected and Automated Vehicles (CAVs) are one of the emerging technologies in the automotive domain that has the potential to alleviate the issues of accidents, traffic congestion, and pollutant emissions, leading toâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey of Federated Learning for Connected and Automated Vehicles">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey of Federated Learning for Connected and Automated Vehicles">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.10677">

<!--Generated on Thu Feb 29 19:25:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">A Survey of Federated Learning for Connected and Automated Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Vishnu Pandi Chellapandi, Liangqi Yuan, Stanislaw H Å»ak and Ziran Wang
</span><span class="ltx_author_notes">V. P. Chellapandi, L. Yuan, S. H. Å»ak and Z. Wang are with College of Engineering, Purdue University, West Lafayette, IN 47907, USA. Emails: <span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{cvp,yuan383,zak,ziran}@purdue.edu</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p"><span id="id2.id1.1" class="ltx_text">Connected and Automated Vehicles (CAVs) are one of the emerging technologies in the automotive domain that has the potential to alleviate the issues of accidents, traffic congestion, and pollutant emissions, leading to a safe, efficient, and sustainable transportation system. Machine learning-based methods are widely used in CAVs for crucial tasks like perception, motion planning, and motion control, where machine learning models in CAVs are solely trained using the local vehicle data, and the performance is not certain when exposed to new environments or unseen conditions. Federated learning (FL) is an effective solution for CAVs that enables a collaborative model development with multiple vehicles in a distributed learning framework. FL enables CAVs to learn from a wide range of driving environments and improve their overall performance while ensuring the privacy and security of local vehicle data. In this paper, we review the progress accomplished by researchers in applying FL to CAVs. A broader view of the various data modalities and algorithms that have been implemented on CAVs is provided. Specific applications of FL are reviewed in detail, and an analysis of the challenges and future scope of research are presented.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Connected and automated vehicles (CAVs) are the key to future intelligent transport systems. With the advent of big data, the Internet of things (IoT), edge computing, and intelligent systems CAVs have the potential to improve the efficiency of the overall transportation system, and reduce traffic congestion and accidents. Robust network communication and significantly increased internet speed are expected to be guaranteed with the onset of advanced communication infrastructures. Currently, CAVs are generating a tremendous amount of raw data, up to one to two terabytes per vehicle per day <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> from various sources like engine components, electronic control units (ECU), perception sensors, and vehicle-to-everything (V2X) communications. This large amount of data is sent to the cloud continuously or periodically for monitoring, prognostics, diagnostics, and connectivity features. These data are also private and come under strict privacy protection regulations in various regions. One such example is the General Data Protection Regulation (GDPR) in the European Union <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Even with the development of advanced machine learning (ML) techniques and vehicle connectivity, it has not been feasible to have a centralized framework to collect data from every vehicle and train an ML model securely. These limitations led to the development of a new ML paradigm known as Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">FL is a new technology breakthrough that has been actively implemented in several application domains. FL has been coined by GoogleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and was initially used for mobile keyboard prediction in GboardÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to allow multiple mobile phones to cooperatively and securely train a neural network (NN) model. In FL, the edge devices/clients only send the gradients or the learnable parameters to the cloud server rather than sending massive local datasets in a Centralized Learning (CL) framework. The cloud server performs a secure aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> of the received gradients/weights and updates the global model parameters that are transmitted back to the clients/edge devices. This procedure, known as a communication round, continues iteratively until the convergence criteria are met in the global model optimization. The key advantage of FL is reducing the strain on the network while also preserving the privacy of the local data. FL is a potential candidate that can utilize the data available from each CAV and develop a robust ML model. A detailed comparison of the on-device vehicle training, CL, and FL approach is described in TableÂ <a href="#S1.T1" title="Table I â€£ I Introduction â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S1.T1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table I: </span>Comparison of ML Approaches in Connected and Automated Vehicles</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.1.1.1" class="ltx_text ltx_font_bold">Features</span></td>
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.1" class="ltx_p"><span id="S1.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Edge Learning (On-Vehicle only)</span></span>
</span>
</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.1.1" class="ltx_p"><span id="S1.T1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Centralized Learning</span></span>
</span>
</td>
<td id="S1.T1.1.1.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.1.1" class="ltx_p"><span id="S1.T1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Federated Learning</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.2" class="ltx_tr">
<td id="S1.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Model training</td>
<td id="S1.T1.1.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.2.1.1" class="ltx_p">Local vehicle</span>
</span>
</td>
<td id="S1.T1.1.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.3.1.1" class="ltx_p">Central server</span>
</span>
</td>
<td id="S1.T1.1.2.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.4.1.1" class="ltx_p">Local vehicle training and central server aggregation</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.3" class="ltx_tr">
<td id="S1.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Model applicability</td>
<td id="S1.T1.1.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.1.1" class="ltx_p">Personalized model</span>
</span>
</td>
<td id="S1.T1.1.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.3.1.1" class="ltx_p">Single global model</span>
</span>
</td>
<td id="S1.T1.1.3.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.4.1.1" class="ltx_p">Single global model but can be personalized</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.4" class="ltx_tr">
<td id="S1.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Privacy protection</td>
<td id="S1.T1.1.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.2.1.1" class="ltx_p"><span id="S1.T1.1.4.2.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.1.1" class="ltx_p"><span id="S1.T1.1.4.3.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—âœ—</span></span>
</span>
</td>
<td id="S1.T1.1.4.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.4.1.1" class="ltx_p"><span id="S1.T1.1.4.4.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.5" class="ltx_tr">
<td id="S1.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Learning efficiency</td>
<td id="S1.T1.1.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.2.1.1" class="ltx_p"><span id="S1.T1.1.5.2.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—</span></span>
</span>
</td>
<td id="S1.T1.1.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.3.1.1" class="ltx_p"><span id="S1.T1.1.5.3.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.5.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.1.1" class="ltx_p"><span id="S1.T1.1.5.4.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.6" class="ltx_tr">
<td id="S1.T1.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Performance on heterogeneous/anomaly data</td>
<td id="S1.T1.1.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.2.1.1" class="ltx_p"><span id="S1.T1.1.6.2.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—</span></span>
</span>
</td>
<td id="S1.T1.1.6.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.3.1.1" class="ltx_p"><span id="S1.T1.1.6.3.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.6.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.4.1.1" class="ltx_p"><span id="S1.T1.1.6.4.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.7" class="ltx_tr">
<td id="S1.T1.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Communication (Data transmission) requirement</td>
<td id="S1.T1.1.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.2.1.1" class="ltx_p"><span id="S1.T1.1.7.2.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.7.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.3.1.1" class="ltx_p"><span id="S1.T1.1.7.3.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—âœ—</span></span>
</span>
</td>
<td id="S1.T1.1.7.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.4.1.1" class="ltx_p"><span id="S1.T1.1.7.4.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.8" class="ltx_tr">
<td id="S1.T1.1.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Training data volume</td>
<td id="S1.T1.1.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.2.1.1" class="ltx_p"><span id="S1.T1.1.8.2.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—âœ—</span></span>
</span>
</td>
<td id="S1.T1.1.8.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.3.1.1" class="ltx_p"><span id="S1.T1.1.8.3.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.8.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.4.1.1" class="ltx_p"><span id="S1.T1.1.8.4.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.9" class="ltx_tr">
<td id="S1.T1.1.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Current research progress</td>
<td id="S1.T1.1.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.2.1.1" class="ltx_p"><span id="S1.T1.1.9.2.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.9.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.3.1.1" class="ltx_p"><span id="S1.T1.1.9.3.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.9.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S1.T1.1.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.4.1.1" class="ltx_p"><span id="S1.T1.1.9.4.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.10" class="ltx_tr">
<td id="S1.T1.1.10.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Compatibility with CAVs</td>
<td id="S1.T1.1.10.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.2.1.1" class="ltx_p"><span id="S1.T1.1.10.2.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“</span></span>
</span>
</td>
<td id="S1.T1.1.10.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.3.1.1" class="ltx_p"><span id="S1.T1.1.10.3.1.1.1" class="ltx_text" style="color:#FF0000;">âœ—âœ—</span></span>
</span>
</td>
<td id="S1.T1.1.10.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.4.1.1" class="ltx_p"><span id="S1.T1.1.10.4.1.1.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span></span>
</span>
</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S1.T1.2" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S1.T1.2.1" class="ltx_text" style="color:#00FF00;">âœ“âœ“</span> best, <span id="S1.T1.2.2" class="ltx_text" style="color:#00FF00;">âœ“</span> high, <span id="S1.T1.2.3" class="ltx_text" style="color:#FF0000;">âœ—</span> low, <span id="S1.T1.2.4" class="ltx_text" style="color:#FF0000;">âœ—âœ—</span> worst.</p>
</div>
</div>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the mutual benefits of connectivity between vehicles, the issues of invasion of privacy, accuracy, effectiveness, and communication resources are essential problems to be addressed. FL frameworks have received attentions for their natural ability to preserve privacy by transmitting only model data between the server and its clients without including user data. In particular, the model data packets are smaller than the user data, thus saving the consumption of communication resources. Similarly, FL frameworks distribute training to each client, and the server does not perform training but only aggregates, which can reduce the pressure on the server and improves training efficiency.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this survey, we provide an insightful and comprehensive survey of FL for CAV (FL4CAV), including diverse applications and key challenges. After a review of data modality, data security and FL algorithms in CAV, this survey focus on most of the critical applications of FL4CAV, such as steering wheel angle prediction, vehicle trajectory prediction, object detection, motion control application, and driver monitoring. This survey also highlights the current challenges and future research directions of FL4CAV, such as performance, safety, fairness, applicability, etc.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The remainder of this paper is organized as follows: SectionÂ <a href="#S2" title="II Overview of Data Modalities, Data Securities and Algorithms â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> highlights the diverse data modalities, data securities, and algorithms of FL in CAVs. SectionÂ <a href="#S3" title="III Applications of FL for CAV â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> reviews the application of FL in CAVs with detailed examples. The multi-modal data, algorithms, and datasets used in the relevant literature are also summarized. Current challenges and future research directions are discussed in SectionÂ <a href="#S4" title="IV Challenges â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. SectionÂ <a href="#S5" title="V Conclusion â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the conclusion of this study and outlines future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Overview of Data Modalities, Data Securities and Algorithms</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The FL4CAV is illustrated in Fig.Â <a href="#S2.F1" title="Figure 1 â€£ II Overview of Data Modalities, Data Securities and Algorithms â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Each CAV as a client, undertakes sensing data acquisition, signal processing, storage, communication, perception, and decision-making. For sensing data acquisition, a variety of sensors are integrated into CAVs, including global navigation satellite systems (GNSS), multi-modal cameras, Radio Detection And Ranging (RADAR), Light Detection And Ranging (LiDAR), and Inertial Measurement Unit (IMU) to capture the vehicle, driver, passenger, and external information.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.F1.1.1" class="ltx_text"><img src="/html/2303.10677/assets/Figure/FL_CAV_R1.png" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="526" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of FL4CAV.</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The tasks for CAV are also diversified, including target speed tracking, behavior prediction, object detection, driver monitoring, and more. After training on an ML model with local data, clients send the trained model to the server. Then, the server shares a generalized model with clients for perception, prediction, and decision-making purposes. The FL4CAV framework shows a trend towards multi-modal sensing data, massively parallel clients, and multi-class tasks. Recent efforts have been ongoing to understand the applicability and challenges of implementing FL to CAVs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. A detailed review of the data modalities of CAVs, Data security, and FL algorithm is presented below.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Data Modality</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">CAVs collect multi-modal data from various sensors for tasks, such as navigation, perception, obstacle detection, and vehicle control. The FL training process involves vehicles that may have a different variety of sensors. The data collected by sensors depends on the sensor type, the sensorâ€™s range, the accuracy/precision of the sensor, sensor placement, and the operating environment. The operating environment, such as snow, heavy rain, or fog, can reduce the sensor visibility, thereby deteriorating the data quality. These factors lead to variations that can significantly affect the sensor performance. The performance of the FL model is directly dependent on the quality of the data collected by the vehicles. The data resolution, size, sampling rate, etc., obtained from the CAVs are generally heterogeneous, and processing the data is also a challenging task. Hence a detailed review is presented to understand the various data modalities in FL4CAV applications.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Images, especially visible RGB images, are one of the most important data modalities for CAV. Vision-related tasks such as steering wheel angle predictionÂ <a href="#S3.SS2" title="III-B Steering Wheel Angle Prediction â€£ III Applications of FL for CAV â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, Traffic sign recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, semantic segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, object detectionÂ <a href="#S3.SS4" title="III-D Object Detection â€£ III Applications of FL for CAV â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>, and driver monitoringÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
typically use images captured by the camera as the data source. In most applications, a variant of the Convolutional Neural Network (CNN) model is trained to achieve the intended functionality. However, due to its intrusive design, privacy issues are always criticized for image-based systems, especially for in-cabin, and driver-related applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. FL focuses on the model parameters and ignores the data, which addresses the drawback that visual image-based systems tend to compromise user privacy. Moreover, FL also solves the data transmission problem caused by the inflated data size of images and videos, leading to a more efficient learning framework.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">LiDAR data provides a solid foundation for autonomous driving capabilities. LiDAR data have also been utilized for object detection tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. LiDAR generates 3D point clouds that can detect objects accurately even under adverse weather conditions, unlike cameras that are not robust. However, the dense point cloud of LiDAR data makes transmission a daunting task. Therefore, compared with the image-based FL systems, the system of FL for LiDAR data is more interested in improving learning efficiency and saving communication resources.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Vehicle status data is also an important part of the CAV data modality, which reflects more about the vehicle rather than external information. Application of FL such as collision avoidanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, vehicle trajectory predictionÂ <a href="#S3.SS3" title="III-C Vehicle Trajectory Prediction â€£ III Applications of FL for CAV â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>, and Motion control applicationÂ <a href="#S3.SS5" title="III-E Motion Control Application â€£ III Applications of FL for CAV â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a> use vehicle status data like the vehicle coordinates, velocity, acceleration, throttle, and braking. These are typically time-series data at uniform time intervals (200 millisecond/1 second) and can generally be easily processed. Recurrent neural networks (RNN) such as Long Short-term Memory (LSTM) have been used for these applications because of their capability to capture long-term dependencies in the data. Reinforcement learning (RL) is a relatively newer method and is starting to be widely used for various applications such as collision avoidance due to its ability to learn from trial-and-error interactions with the environment, allowing it to adapt to changes in the environment and respond to unseen situations. Vehicle state data is not as strongly motivated as images and LiDAR, but FL can still inspire knowledge sharing among CAVs. For example, sharing contextual knowledge of rare events such as traffic accidents can reduce the risk of CAVs re-encountering the same event.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Data Security </span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Robust and secure privacy-preserving techniques are essential for protecting sensitive data during the FL process for CAVs. It is demonstrated that the FL process can still be vulnerable to various malicious attacks, such as when one or more participants are compromised, and they could transmit false parameters to hinder the global model performance. The FL central server is also prone to attack and thereby causing the entire learning process to collapseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
In this paper, we highlight a few existing solutions, and a detailed review of the techniques is discussed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Homomorphic Encryption (HE) is a technique that enables the server to perform the training on encrypted data from the vehicles without the need for decryption. Secure Multi-Party Computation (SMPC) enables multiple vehicles to collaboratively perform computation with the individual vehicle data without sharing the data with each other. Differential Privacy (DP) is a popular method that ensures privacy in data by adding random noise to the data before being sent to the server to prevent extraction of the information. Blockchain technology-based method is a disruptive technology that has recently been widely deployed in CAV applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Blockchain is a type of digital ledger technology that securely transfer data in a decentralized framework. Data from AVs share their data with the vehicular network, and the information is stored on the blockchain. The system is designed to protect data privacy and data security as well as to provide higher security to the overall vehicular networks engaged in the learning processÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Federated Learning Algorithm</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Most of the existing literature uses the FedAvg algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for the FL aggregation process in the serverâ€”see TableÂ <a href="#S2.T2" title="Table II â€£ II-C Federated Learning Algorithm â€£ II Overview of Data Modalities, Data Securities and Algorithms â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. FedAvg applies Stochastic Gradient Descent (SGD) optimization on local vehicles and does a weighted averaging of the weights from the vehicles in the central server. FedAvg performs multiple local gradient updates before sending the parameters to the server, reducing the number of communication rounds. For FL4CAV, the data on each CAVs is dynamically updated at each communication roundâ€”see AlgorithmÂ <a href="#alg1" title="Algorithm 1 â€£ II-C Federated Learning Algorithm â€£ II Overview of Data Modalities, Data Securities and Algorithms â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.4.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> FedAvg for Dynamic Data Updating CAVs</figcaption>
<div id="alg1.5" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1.2" class="ltx_text" style="font-size:90%;">Â Â </span><span id="alg1.l1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Input:</span><span id="alg1.l1.4" class="ltx_text" style="font-size:90%;"> Vehicle set </span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="\mathbb{V}" display="inline"><semantics id="alg1.l1.m1.1a"><mi mathsize="90%" id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">ğ•</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">ğ•</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">\mathbb{V}</annotation></semantics></math><span id="alg1.l1.5" class="ltx_text" style="font-size:90%;">, communication rounds </span><math id="alg1.l1.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l1.m2.1a"><mi mathsize="90%" id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">T</annotation></semantics></math><span id="alg1.l1.6" class="ltx_text" style="font-size:90%;">, isolated time-varying local dataset </span><math id="alg1.l1.m3.3" class="ltx_Math" alttext="\xi=\{\xi_{v}^{(t)}|v\in\mathbb{V}\}" display="inline"><semantics id="alg1.l1.m3.3a"><mrow id="alg1.l1.m3.3.3" xref="alg1.l1.m3.3.3.cmml"><mi mathsize="90%" id="alg1.l1.m3.3.3.4" xref="alg1.l1.m3.3.3.4.cmml">Î¾</mi><mo mathsize="90%" id="alg1.l1.m3.3.3.3" xref="alg1.l1.m3.3.3.3.cmml">=</mo><mrow id="alg1.l1.m3.3.3.2.2" xref="alg1.l1.m3.3.3.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l1.m3.3.3.2.2.3" xref="alg1.l1.m3.3.3.2.3.1.cmml">{</mo><msubsup id="alg1.l1.m3.2.2.1.1.1" xref="alg1.l1.m3.2.2.1.1.1.cmml"><mi mathsize="90%" id="alg1.l1.m3.2.2.1.1.1.2.2" xref="alg1.l1.m3.2.2.1.1.1.2.2.cmml">Î¾</mi><mi mathsize="90%" id="alg1.l1.m3.2.2.1.1.1.2.3" xref="alg1.l1.m3.2.2.1.1.1.2.3.cmml">v</mi><mrow id="alg1.l1.m3.1.1.1.3" xref="alg1.l1.m3.2.2.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l1.m3.1.1.1.3.1" xref="alg1.l1.m3.2.2.1.1.1.cmml">(</mo><mi mathsize="90%" id="alg1.l1.m3.1.1.1.1" xref="alg1.l1.m3.1.1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l1.m3.1.1.1.3.2" xref="alg1.l1.m3.2.2.1.1.1.cmml">)</mo></mrow></msubsup><mo lspace="0em" mathsize="90%" rspace="0em" id="alg1.l1.m3.3.3.2.2.4" xref="alg1.l1.m3.3.3.2.3.1.cmml">|</mo><mrow id="alg1.l1.m3.3.3.2.2.2" xref="alg1.l1.m3.3.3.2.2.2.cmml"><mi mathsize="90%" id="alg1.l1.m3.3.3.2.2.2.2" xref="alg1.l1.m3.3.3.2.2.2.2.cmml">v</mi><mo mathsize="90%" id="alg1.l1.m3.3.3.2.2.2.1" xref="alg1.l1.m3.3.3.2.2.2.1.cmml">âˆˆ</mo><mi mathsize="90%" id="alg1.l1.m3.3.3.2.2.2.3" xref="alg1.l1.m3.3.3.2.2.2.3.cmml">ğ•</mi></mrow><mo maxsize="90%" minsize="90%" id="alg1.l1.m3.3.3.2.2.5" xref="alg1.l1.m3.3.3.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.3b"><apply id="alg1.l1.m3.3.3.cmml" xref="alg1.l1.m3.3.3"><eq id="alg1.l1.m3.3.3.3.cmml" xref="alg1.l1.m3.3.3.3"></eq><ci id="alg1.l1.m3.3.3.4.cmml" xref="alg1.l1.m3.3.3.4">ğœ‰</ci><apply id="alg1.l1.m3.3.3.2.3.cmml" xref="alg1.l1.m3.3.3.2.2"><csymbol cd="latexml" id="alg1.l1.m3.3.3.2.3.1.cmml" xref="alg1.l1.m3.3.3.2.2.3">conditional-set</csymbol><apply id="alg1.l1.m3.2.2.1.1.1.cmml" xref="alg1.l1.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l1.m3.2.2.1.1.1.1.cmml" xref="alg1.l1.m3.2.2.1.1.1">superscript</csymbol><apply id="alg1.l1.m3.2.2.1.1.1.2.cmml" xref="alg1.l1.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l1.m3.2.2.1.1.1.2.1.cmml" xref="alg1.l1.m3.2.2.1.1.1">subscript</csymbol><ci id="alg1.l1.m3.2.2.1.1.1.2.2.cmml" xref="alg1.l1.m3.2.2.1.1.1.2.2">ğœ‰</ci><ci id="alg1.l1.m3.2.2.1.1.1.2.3.cmml" xref="alg1.l1.m3.2.2.1.1.1.2.3">ğ‘£</ci></apply><ci id="alg1.l1.m3.1.1.1.1.cmml" xref="alg1.l1.m3.1.1.1.1">ğ‘¡</ci></apply><apply id="alg1.l1.m3.3.3.2.2.2.cmml" xref="alg1.l1.m3.3.3.2.2.2"><in id="alg1.l1.m3.3.3.2.2.2.1.cmml" xref="alg1.l1.m3.3.3.2.2.2.1"></in><ci id="alg1.l1.m3.3.3.2.2.2.2.cmml" xref="alg1.l1.m3.3.3.2.2.2.2">ğ‘£</ci><ci id="alg1.l1.m3.3.3.2.2.2.3.cmml" xref="alg1.l1.m3.3.3.2.2.2.3">ğ•</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.3c">\xi=\{\xi_{v}^{(t)}|v\in\mathbb{V}\}</annotation></semantics></math><span id="alg1.l1.7" class="ltx_text" style="font-size:90%;">, local epochs </span><math id="alg1.l1.m4.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l1.m4.1a"><mi mathsize="90%" id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">E</annotation></semantics></math><span id="alg1.l1.8" class="ltx_text" style="font-size:90%;">, learning rate </span><math id="alg1.l1.m5.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg1.l1.m5.1a"><mi mathsize="90%" id="alg1.l1.m5.1.1" xref="alg1.l1.m5.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m5.1b"><ci id="alg1.l1.m5.1.1.cmml" xref="alg1.l1.m5.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m5.1c">\eta</annotation></semantics></math><span id="alg1.l1.9" class="ltx_text" style="font-size:90%;">, loss function </span><math id="alg1.l1.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="alg1.l1.m6.1a"><mi mathsize="90%" id="alg1.l1.m6.1.1" xref="alg1.l1.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m6.1b"><ci id="alg1.l1.m6.1.1.cmml" xref="alg1.l1.m6.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m6.1c">f</annotation></semantics></math><span id="alg1.l1.10" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text" style="font-size:90%;">Â Â </span><span id="alg1.l2.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Output:</span><span id="alg1.l2.4" class="ltx_text" style="font-size:90%;"> Aggregated global model </span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="alg1.l2.m1.1a"><mi mathsize="90%" id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">ğ‘¤</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">w</annotation></semantics></math><span id="alg1.l2.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.2" class="ltx_text" style="font-size:90%;">Â Â initialize </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">w</mi><mn mathsize="90%" id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">ğ‘¤</ci><cn type="integer" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">w_{0}</annotation></semantics></math><span id="alg1.l3.3" class="ltx_text" style="font-size:90%;"> </span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.2" class="ltx_text" style="font-size:90%;">Â Â </span><span id="alg1.l4.3" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l4.4" class="ltx_text" style="font-size:90%;">Â </span><math id="alg1.l4.m1.3" class="ltx_Math" alttext="t=0,\dots,T-1" display="inline"><semantics id="alg1.l4.m1.3a"><mrow id="alg1.l4.m1.3.3" xref="alg1.l4.m1.3.3.cmml"><mi mathsize="90%" id="alg1.l4.m1.3.3.3" xref="alg1.l4.m1.3.3.3.cmml">t</mi><mo mathsize="90%" id="alg1.l4.m1.3.3.2" xref="alg1.l4.m1.3.3.2.cmml">=</mo><mrow id="alg1.l4.m1.3.3.1.1" xref="alg1.l4.m1.3.3.1.2.cmml"><mn mathsize="90%" id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">0</mn><mo mathsize="90%" id="alg1.l4.m1.3.3.1.1.2" xref="alg1.l4.m1.3.3.1.2.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="alg1.l4.m1.2.2" xref="alg1.l4.m1.2.2.cmml">â€¦</mi><mo mathsize="90%" id="alg1.l4.m1.3.3.1.1.3" xref="alg1.l4.m1.3.3.1.2.cmml">,</mo><mrow id="alg1.l4.m1.3.3.1.1.1" xref="alg1.l4.m1.3.3.1.1.1.cmml"><mi mathsize="90%" id="alg1.l4.m1.3.3.1.1.1.2" xref="alg1.l4.m1.3.3.1.1.1.2.cmml">T</mi><mo mathsize="90%" id="alg1.l4.m1.3.3.1.1.1.1" xref="alg1.l4.m1.3.3.1.1.1.1.cmml">âˆ’</mo><mn mathsize="90%" id="alg1.l4.m1.3.3.1.1.1.3" xref="alg1.l4.m1.3.3.1.1.1.3.cmml">1</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.3b"><apply id="alg1.l4.m1.3.3.cmml" xref="alg1.l4.m1.3.3"><eq id="alg1.l4.m1.3.3.2.cmml" xref="alg1.l4.m1.3.3.2"></eq><ci id="alg1.l4.m1.3.3.3.cmml" xref="alg1.l4.m1.3.3.3">ğ‘¡</ci><list id="alg1.l4.m1.3.3.1.2.cmml" xref="alg1.l4.m1.3.3.1.1"><cn type="integer" id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">0</cn><ci id="alg1.l4.m1.2.2.cmml" xref="alg1.l4.m1.2.2">â€¦</ci><apply id="alg1.l4.m1.3.3.1.1.1.cmml" xref="alg1.l4.m1.3.3.1.1.1"><minus id="alg1.l4.m1.3.3.1.1.1.1.cmml" xref="alg1.l4.m1.3.3.1.1.1.1"></minus><ci id="alg1.l4.m1.3.3.1.1.1.2.cmml" xref="alg1.l4.m1.3.3.1.1.1.2">ğ‘‡</ci><cn type="integer" id="alg1.l4.m1.3.3.1.1.1.3.cmml" xref="alg1.l4.m1.3.3.1.1.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.3c">t=0,\dots,T-1</annotation></semantics></math><span id="alg1.l4.5" class="ltx_text" style="font-size:90%;">Â </span><span id="alg1.l4.6" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l4.7" class="ltx_text" style="font-size:90%;">


</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.2" class="ltx_text" style="font-size:90%;">Â Â Â Â Â </span><span id="alg1.l5.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Perform</span><span id="alg1.l5.4" class="ltx_text" style="font-size:90%;"> local SGD </span><span id="alg1.l5.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l5.6" class="ltx_text" style="font-size:90%;"> vehicle </span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="v\in\mathbb{V}" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">v</mi><mo mathsize="90%" id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">âˆˆ</mo><mi mathsize="90%" id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml">ğ•</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><in id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1"></in><ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">ğ‘£</ci><ci id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">ğ•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">v\in\mathbb{V}</annotation></semantics></math><span id="alg1.l5.7" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l5.8" class="ltx_text ltx_font_bold" style="font-size:90%;">in parallel do</span><span id="alg1.l5.9" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text" style="font-size:90%;">Â Â Â Â Â Â Â Â Sample </span><math id="alg1.l6.m1.6" class="ltx_Math" alttext="\xi_{v}^{(t)}\mbox{, compute }g_{v}^{(t)}:=\widetilde{\nabla}f_{v}(w_{v}^{(t)},\xi_{v}^{(t)})" display="inline"><semantics id="alg1.l6.m1.6a"><mrow id="alg1.l6.m1.6.6" xref="alg1.l6.m1.6.6.cmml"><mrow id="alg1.l6.m1.6.6.4" xref="alg1.l6.m1.6.6.4.cmml"><msubsup id="alg1.l6.m1.6.6.4.2" xref="alg1.l6.m1.6.6.4.2.cmml"><mi mathsize="90%" id="alg1.l6.m1.6.6.4.2.2.2" xref="alg1.l6.m1.6.6.4.2.2.2.cmml">Î¾</mi><mi mathsize="90%" id="alg1.l6.m1.6.6.4.2.2.3" xref="alg1.l6.m1.6.6.4.2.2.3.cmml">v</mi><mrow id="alg1.l6.m1.1.1.1.3" xref="alg1.l6.m1.6.6.4.2.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.1.1.1.3.1" xref="alg1.l6.m1.6.6.4.2.cmml">(</mo><mi mathsize="90%" id="alg1.l6.m1.1.1.1.1" xref="alg1.l6.m1.1.1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.1.1.1.3.2" xref="alg1.l6.m1.6.6.4.2.cmml">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="alg1.l6.m1.6.6.4.1" xref="alg1.l6.m1.6.6.4.1.cmml">â€‹</mo><mtext mathsize="90%" id="alg1.l6.m1.6.6.4.3" xref="alg1.l6.m1.6.6.4.3a.cmml">, computeÂ </mtext><mo lspace="0em" rspace="0em" id="alg1.l6.m1.6.6.4.1a" xref="alg1.l6.m1.6.6.4.1.cmml">â€‹</mo><msubsup id="alg1.l6.m1.6.6.4.4" xref="alg1.l6.m1.6.6.4.4.cmml"><mi mathsize="90%" id="alg1.l6.m1.6.6.4.4.2.2" xref="alg1.l6.m1.6.6.4.4.2.2.cmml">g</mi><mi mathsize="90%" id="alg1.l6.m1.6.6.4.4.2.3" xref="alg1.l6.m1.6.6.4.4.2.3.cmml">v</mi><mrow id="alg1.l6.m1.2.2.1.3" xref="alg1.l6.m1.6.6.4.4.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.2.2.1.3.1" xref="alg1.l6.m1.6.6.4.4.cmml">(</mo><mi mathsize="90%" id="alg1.l6.m1.2.2.1.1" xref="alg1.l6.m1.2.2.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.2.2.1.3.2" xref="alg1.l6.m1.6.6.4.4.cmml">)</mo></mrow></msubsup></mrow><mo lspace="0.278em" mathsize="90%" rspace="0.278em" id="alg1.l6.m1.6.6.3" xref="alg1.l6.m1.6.6.3.cmml">:=</mo><mrow id="alg1.l6.m1.6.6.2" xref="alg1.l6.m1.6.6.2.cmml"><mover accent="true" id="alg1.l6.m1.6.6.2.4" xref="alg1.l6.m1.6.6.2.4.cmml"><mo mathsize="90%" id="alg1.l6.m1.6.6.2.4.2" xref="alg1.l6.m1.6.6.2.4.2.cmml">âˆ‡</mo><mo mathsize="90%" id="alg1.l6.m1.6.6.2.4.1" xref="alg1.l6.m1.6.6.2.4.1.cmml">~</mo></mover><mo lspace="0.167em" rspace="0em" id="alg1.l6.m1.6.6.2.3" xref="alg1.l6.m1.6.6.2.3.cmml">â€‹</mo><msub id="alg1.l6.m1.6.6.2.5" xref="alg1.l6.m1.6.6.2.5.cmml"><mi mathsize="90%" id="alg1.l6.m1.6.6.2.5.2" xref="alg1.l6.m1.6.6.2.5.2.cmml">f</mi><mi mathsize="90%" id="alg1.l6.m1.6.6.2.5.3" xref="alg1.l6.m1.6.6.2.5.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="alg1.l6.m1.6.6.2.3a" xref="alg1.l6.m1.6.6.2.3.cmml">â€‹</mo><mrow id="alg1.l6.m1.6.6.2.2.2" xref="alg1.l6.m1.6.6.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.6.6.2.2.2.3" xref="alg1.l6.m1.6.6.2.2.3.cmml">(</mo><msubsup id="alg1.l6.m1.5.5.1.1.1.1" xref="alg1.l6.m1.5.5.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m1.5.5.1.1.1.1.2.2" xref="alg1.l6.m1.5.5.1.1.1.1.2.2.cmml">w</mi><mi mathsize="90%" id="alg1.l6.m1.5.5.1.1.1.1.2.3" xref="alg1.l6.m1.5.5.1.1.1.1.2.3.cmml">v</mi><mrow id="alg1.l6.m1.3.3.1.3" xref="alg1.l6.m1.5.5.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.3.3.1.3.1" xref="alg1.l6.m1.5.5.1.1.1.1.cmml">(</mo><mi mathsize="90%" id="alg1.l6.m1.3.3.1.1" xref="alg1.l6.m1.3.3.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.3.3.1.3.2" xref="alg1.l6.m1.5.5.1.1.1.1.cmml">)</mo></mrow></msubsup><mo mathsize="90%" id="alg1.l6.m1.6.6.2.2.2.4" xref="alg1.l6.m1.6.6.2.2.3.cmml">,</mo><msubsup id="alg1.l6.m1.6.6.2.2.2.2" xref="alg1.l6.m1.6.6.2.2.2.2.cmml"><mi mathsize="90%" id="alg1.l6.m1.6.6.2.2.2.2.2.2" xref="alg1.l6.m1.6.6.2.2.2.2.2.2.cmml">Î¾</mi><mi mathsize="90%" id="alg1.l6.m1.6.6.2.2.2.2.2.3" xref="alg1.l6.m1.6.6.2.2.2.2.2.3.cmml">v</mi><mrow id="alg1.l6.m1.4.4.1.3" xref="alg1.l6.m1.6.6.2.2.2.2.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.4.4.1.3.1" xref="alg1.l6.m1.6.6.2.2.2.2.cmml">(</mo><mi mathsize="90%" id="alg1.l6.m1.4.4.1.1" xref="alg1.l6.m1.4.4.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.4.4.1.3.2" xref="alg1.l6.m1.6.6.2.2.2.2.cmml">)</mo></mrow></msubsup><mo maxsize="90%" minsize="90%" id="alg1.l6.m1.6.6.2.2.2.5" xref="alg1.l6.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.6b"><apply id="alg1.l6.m1.6.6.cmml" xref="alg1.l6.m1.6.6"><csymbol cd="latexml" id="alg1.l6.m1.6.6.3.cmml" xref="alg1.l6.m1.6.6.3">assign</csymbol><apply id="alg1.l6.m1.6.6.4.cmml" xref="alg1.l6.m1.6.6.4"><times id="alg1.l6.m1.6.6.4.1.cmml" xref="alg1.l6.m1.6.6.4.1"></times><apply id="alg1.l6.m1.6.6.4.2.cmml" xref="alg1.l6.m1.6.6.4.2"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.4.2.1.cmml" xref="alg1.l6.m1.6.6.4.2">superscript</csymbol><apply id="alg1.l6.m1.6.6.4.2.2.cmml" xref="alg1.l6.m1.6.6.4.2"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.4.2.2.1.cmml" xref="alg1.l6.m1.6.6.4.2">subscript</csymbol><ci id="alg1.l6.m1.6.6.4.2.2.2.cmml" xref="alg1.l6.m1.6.6.4.2.2.2">ğœ‰</ci><ci id="alg1.l6.m1.6.6.4.2.2.3.cmml" xref="alg1.l6.m1.6.6.4.2.2.3">ğ‘£</ci></apply><ci id="alg1.l6.m1.1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1.1">ğ‘¡</ci></apply><ci id="alg1.l6.m1.6.6.4.3a.cmml" xref="alg1.l6.m1.6.6.4.3"><mtext mathsize="90%" id="alg1.l6.m1.6.6.4.3.cmml" xref="alg1.l6.m1.6.6.4.3">, computeÂ </mtext></ci><apply id="alg1.l6.m1.6.6.4.4.cmml" xref="alg1.l6.m1.6.6.4.4"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.4.4.1.cmml" xref="alg1.l6.m1.6.6.4.4">superscript</csymbol><apply id="alg1.l6.m1.6.6.4.4.2.cmml" xref="alg1.l6.m1.6.6.4.4"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.4.4.2.1.cmml" xref="alg1.l6.m1.6.6.4.4">subscript</csymbol><ci id="alg1.l6.m1.6.6.4.4.2.2.cmml" xref="alg1.l6.m1.6.6.4.4.2.2">ğ‘”</ci><ci id="alg1.l6.m1.6.6.4.4.2.3.cmml" xref="alg1.l6.m1.6.6.4.4.2.3">ğ‘£</ci></apply><ci id="alg1.l6.m1.2.2.1.1.cmml" xref="alg1.l6.m1.2.2.1.1">ğ‘¡</ci></apply></apply><apply id="alg1.l6.m1.6.6.2.cmml" xref="alg1.l6.m1.6.6.2"><times id="alg1.l6.m1.6.6.2.3.cmml" xref="alg1.l6.m1.6.6.2.3"></times><apply id="alg1.l6.m1.6.6.2.4.cmml" xref="alg1.l6.m1.6.6.2.4"><ci id="alg1.l6.m1.6.6.2.4.1.cmml" xref="alg1.l6.m1.6.6.2.4.1">~</ci><ci id="alg1.l6.m1.6.6.2.4.2.cmml" xref="alg1.l6.m1.6.6.2.4.2">âˆ‡</ci></apply><apply id="alg1.l6.m1.6.6.2.5.cmml" xref="alg1.l6.m1.6.6.2.5"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.2.5.1.cmml" xref="alg1.l6.m1.6.6.2.5">subscript</csymbol><ci id="alg1.l6.m1.6.6.2.5.2.cmml" xref="alg1.l6.m1.6.6.2.5.2">ğ‘“</ci><ci id="alg1.l6.m1.6.6.2.5.3.cmml" xref="alg1.l6.m1.6.6.2.5.3">ğ‘£</ci></apply><interval closure="open" id="alg1.l6.m1.6.6.2.2.3.cmml" xref="alg1.l6.m1.6.6.2.2.2"><apply id="alg1.l6.m1.5.5.1.1.1.1.cmml" xref="alg1.l6.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.5.5.1.1.1.1.1.cmml" xref="alg1.l6.m1.5.5.1.1.1.1">superscript</csymbol><apply id="alg1.l6.m1.5.5.1.1.1.1.2.cmml" xref="alg1.l6.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.5.5.1.1.1.1.2.1.cmml" xref="alg1.l6.m1.5.5.1.1.1.1">subscript</csymbol><ci id="alg1.l6.m1.5.5.1.1.1.1.2.2.cmml" xref="alg1.l6.m1.5.5.1.1.1.1.2.2">ğ‘¤</ci><ci id="alg1.l6.m1.5.5.1.1.1.1.2.3.cmml" xref="alg1.l6.m1.5.5.1.1.1.1.2.3">ğ‘£</ci></apply><ci id="alg1.l6.m1.3.3.1.1.cmml" xref="alg1.l6.m1.3.3.1.1">ğ‘¡</ci></apply><apply id="alg1.l6.m1.6.6.2.2.2.2.cmml" xref="alg1.l6.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.2.2.2.2.1.cmml" xref="alg1.l6.m1.6.6.2.2.2.2">superscript</csymbol><apply id="alg1.l6.m1.6.6.2.2.2.2.2.cmml" xref="alg1.l6.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l6.m1.6.6.2.2.2.2.2.1.cmml" xref="alg1.l6.m1.6.6.2.2.2.2">subscript</csymbol><ci id="alg1.l6.m1.6.6.2.2.2.2.2.2.cmml" xref="alg1.l6.m1.6.6.2.2.2.2.2.2">ğœ‰</ci><ci id="alg1.l6.m1.6.6.2.2.2.2.2.3.cmml" xref="alg1.l6.m1.6.6.2.2.2.2.2.3">ğ‘£</ci></apply><ci id="alg1.l6.m1.4.4.1.1.cmml" xref="alg1.l6.m1.4.4.1.1">ğ‘¡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.6c">\xi_{v}^{(t)}\mbox{, compute }g_{v}^{(t)}:=\widetilde{\nabla}f_{v}(w_{v}^{(t)},\xi_{v}^{(t)})</annotation></semantics></math><span id="alg1.l6.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:90%;">Â Â Â Â Â Â Â Â </span><math id="alg1.l7.m1.5" class="ltx_Math" alttext="w_{v}^{(t+1)}\ \leftarrow\ w_{v}^{(t)}-\eta_{t}g_{v}^{(t)}\hskip 14.22636pt\implies" display="inline"><semantics id="alg1.l7.m1.5a"><mrow id="alg1.l7.m1.5.5" xref="alg1.l7.m1.5.5.cmml"><msubsup id="alg1.l7.m1.5.5.3" xref="alg1.l7.m1.5.5.3.cmml"><mi mathsize="90%" id="alg1.l7.m1.5.5.3.2.2" xref="alg1.l7.m1.5.5.3.2.2.cmml">w</mi><mi mathsize="90%" id="alg1.l7.m1.5.5.3.2.3" xref="alg1.l7.m1.5.5.3.2.3.cmml">v</mi><mrow id="alg1.l7.m1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l7.m1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l7.m1.1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l7.m1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="alg1.l7.m1.1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo lspace="0.728em" mathsize="90%" rspace="0.728em" stretchy="false" id="alg1.l7.m1.5.5.2" xref="alg1.l7.m1.5.5.2.cmml">â†</mo><mrow id="alg1.l7.m1.5.5.1.1" xref="alg1.l7.m1.5.5.1.2.cmml"><mrow id="alg1.l7.m1.5.5.1.1.1" xref="alg1.l7.m1.5.5.1.1.1.cmml"><msubsup id="alg1.l7.m1.5.5.1.1.1.2" xref="alg1.l7.m1.5.5.1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.2.2.2" xref="alg1.l7.m1.5.5.1.1.1.2.2.2.cmml">w</mi><mi mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.2.2.3" xref="alg1.l7.m1.5.5.1.1.1.2.2.3.cmml">v</mi><mrow id="alg1.l7.m1.2.2.1.3" xref="alg1.l7.m1.5.5.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.2.2.1.3.1" xref="alg1.l7.m1.5.5.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="alg1.l7.m1.2.2.1.1" xref="alg1.l7.m1.2.2.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.2.2.1.3.2" xref="alg1.l7.m1.5.5.1.1.1.2.cmml">)</mo></mrow></msubsup><mo mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.1" xref="alg1.l7.m1.5.5.1.1.1.1.cmml">âˆ’</mo><mrow id="alg1.l7.m1.5.5.1.1.1.3" xref="alg1.l7.m1.5.5.1.1.1.3.cmml"><msub id="alg1.l7.m1.5.5.1.1.1.3.2" xref="alg1.l7.m1.5.5.1.1.1.3.2.cmml"><mi mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.3.2.2" xref="alg1.l7.m1.5.5.1.1.1.3.2.2.cmml">Î·</mi><mi mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.3.2.3" xref="alg1.l7.m1.5.5.1.1.1.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="alg1.l7.m1.5.5.1.1.1.3.1" xref="alg1.l7.m1.5.5.1.1.1.3.1.cmml">â€‹</mo><msubsup id="alg1.l7.m1.5.5.1.1.1.3.3" xref="alg1.l7.m1.5.5.1.1.1.3.3.cmml"><mi mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.3.3.2.2" xref="alg1.l7.m1.5.5.1.1.1.3.3.2.2.cmml">g</mi><mi mathsize="90%" id="alg1.l7.m1.5.5.1.1.1.3.3.2.3" xref="alg1.l7.m1.5.5.1.1.1.3.3.2.3.cmml">v</mi><mrow id="alg1.l7.m1.3.3.1.3" xref="alg1.l7.m1.5.5.1.1.1.3.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.3.3.1.3.1" xref="alg1.l7.m1.5.5.1.1.1.3.3.cmml">(</mo><mi mathsize="90%" id="alg1.l7.m1.3.3.1.1" xref="alg1.l7.m1.3.3.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l7.m1.3.3.1.3.2" xref="alg1.l7.m1.5.5.1.1.1.3.3.cmml">)</mo></mrow></msubsup></mrow></mrow><mspace width="1.42em" id="alg1.l7.m1.5.5.1.1.2" xref="alg1.l7.m1.5.5.1.2.cmml"></mspace><mo mathsize="90%" stretchy="false" id="alg1.l7.m1.4.4" xref="alg1.l7.m1.4.4.cmml">âŸ¹</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.5b"><apply id="alg1.l7.m1.5.5.cmml" xref="alg1.l7.m1.5.5"><ci id="alg1.l7.m1.5.5.2.cmml" xref="alg1.l7.m1.5.5.2">â†</ci><apply id="alg1.l7.m1.5.5.3.cmml" xref="alg1.l7.m1.5.5.3"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.3.1.cmml" xref="alg1.l7.m1.5.5.3">superscript</csymbol><apply id="alg1.l7.m1.5.5.3.2.cmml" xref="alg1.l7.m1.5.5.3"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.3.2.1.cmml" xref="alg1.l7.m1.5.5.3">subscript</csymbol><ci id="alg1.l7.m1.5.5.3.2.2.cmml" xref="alg1.l7.m1.5.5.3.2.2">ğ‘¤</ci><ci id="alg1.l7.m1.5.5.3.2.3.cmml" xref="alg1.l7.m1.5.5.3.2.3">ğ‘£</ci></apply><apply id="alg1.l7.m1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1"><plus id="alg1.l7.m1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1"></plus><ci id="alg1.l7.m1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.2">ğ‘¡</ci><cn type="integer" id="alg1.l7.m1.1.1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.3">1</cn></apply></apply><list id="alg1.l7.m1.5.5.1.2.cmml" xref="alg1.l7.m1.5.5.1.1"><apply id="alg1.l7.m1.5.5.1.1.1.cmml" xref="alg1.l7.m1.5.5.1.1.1"><minus id="alg1.l7.m1.5.5.1.1.1.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.1"></minus><apply id="alg1.l7.m1.5.5.1.1.1.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.1.1.1.2.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.2">superscript</csymbol><apply id="alg1.l7.m1.5.5.1.1.1.2.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.1.1.1.2.2.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.2">subscript</csymbol><ci id="alg1.l7.m1.5.5.1.1.1.2.2.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.2.2.2">ğ‘¤</ci><ci id="alg1.l7.m1.5.5.1.1.1.2.2.3.cmml" xref="alg1.l7.m1.5.5.1.1.1.2.2.3">ğ‘£</ci></apply><ci id="alg1.l7.m1.2.2.1.1.cmml" xref="alg1.l7.m1.2.2.1.1">ğ‘¡</ci></apply><apply id="alg1.l7.m1.5.5.1.1.1.3.cmml" xref="alg1.l7.m1.5.5.1.1.1.3"><times id="alg1.l7.m1.5.5.1.1.1.3.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.1"></times><apply id="alg1.l7.m1.5.5.1.1.1.3.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.1.1.1.3.2.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.2">subscript</csymbol><ci id="alg1.l7.m1.5.5.1.1.1.3.2.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.2.2">ğœ‚</ci><ci id="alg1.l7.m1.5.5.1.1.1.3.2.3.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.2.3">ğ‘¡</ci></apply><apply id="alg1.l7.m1.5.5.1.1.1.3.3.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.3"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.1.1.1.3.3.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.3">superscript</csymbol><apply id="alg1.l7.m1.5.5.1.1.1.3.3.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.3"><csymbol cd="ambiguous" id="alg1.l7.m1.5.5.1.1.1.3.3.2.1.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.3">subscript</csymbol><ci id="alg1.l7.m1.5.5.1.1.1.3.3.2.2.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.3.2.2">ğ‘”</ci><ci id="alg1.l7.m1.5.5.1.1.1.3.3.2.3.cmml" xref="alg1.l7.m1.5.5.1.1.1.3.3.2.3">ğ‘£</ci></apply><ci id="alg1.l7.m1.3.3.1.1.cmml" xref="alg1.l7.m1.3.3.1.1">ğ‘¡</ci></apply></apply></apply><implies id="alg1.l7.m1.4.4.cmml" xref="alg1.l7.m1.4.4"></implies></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.5c">w_{v}^{(t+1)}\ \leftarrow\ w_{v}^{(t)}-\eta_{t}g_{v}^{(t)}\hskip 14.22636pt\implies</annotation></semantics></math><span id="alg1.l7.3" class="ltx_text" style="font-size:90%;"> SGD (</span><math id="alg1.l7.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l7.m2.1a"><mi mathsize="90%" id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">E</annotation></semantics></math><span id="alg1.l7.4" class="ltx_text" style="font-size:90%;"> epochs)
</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text" style="font-size:90%;">Â Â Â Â Â </span><span id="alg1.l8.3" class="ltx_text ltx_font_bold" style="font-size:90%;">end for</span><span id="alg1.l8.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text" style="font-size:90%;">Â Â Â Â Â </span><math id="alg1.l9.m1.6" class="ltx_Math" alttext="w^{(t+1)}\ \leftarrow\ \sum_{v\in\mathbb{V}}\frac{|\xi_{v}^{(t)}|}{|\xi^{(t)}|}(w_{v}^{t+1})\hskip 3.41418pt\implies" display="inline"><semantics id="alg1.l9.m1.6a"><mrow id="alg1.l9.m1.6.6" xref="alg1.l9.m1.6.6.cmml"><msup id="alg1.l9.m1.6.6.3" xref="alg1.l9.m1.6.6.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.6.6.3.2" xref="alg1.l9.m1.6.6.3.2.cmml">w</mi><mrow id="alg1.l9.m1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.1.1.1.1.2" xref="alg1.l9.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l9.m1.1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l9.m1.1.1.1.1.1.2" xref="alg1.l9.m1.1.1.1.1.1.2.cmml">t</mi><mo mathsize="90%" id="alg1.l9.m1.1.1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="alg1.l9.m1.1.1.1.1.1.3" xref="alg1.l9.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.1.1.1.1.3" xref="alg1.l9.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo lspace="0.728em" mathsize="90%" rspace="0.561em" stretchy="false" id="alg1.l9.m1.6.6.4" xref="alg1.l9.m1.6.6.4.cmml">â†</mo><mrow id="alg1.l9.m1.6.6.1" xref="alg1.l9.m1.6.6.1.cmml"><msub id="alg1.l9.m1.6.6.1.2" xref="alg1.l9.m1.6.6.1.2.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="alg1.l9.m1.6.6.1.2.2" xref="alg1.l9.m1.6.6.1.2.2.cmml">âˆ‘</mo><mrow id="alg1.l9.m1.6.6.1.2.3" xref="alg1.l9.m1.6.6.1.2.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.6.6.1.2.3.2" xref="alg1.l9.m1.6.6.1.2.3.2.cmml">v</mi><mo mathsize="90%" id="alg1.l9.m1.6.6.1.2.3.1" xref="alg1.l9.m1.6.6.1.2.3.1.cmml">âˆˆ</mo><mi mathsize="90%" id="alg1.l9.m1.6.6.1.2.3.3" xref="alg1.l9.m1.6.6.1.2.3.3.cmml">ğ•</mi></mrow></msub><mrow id="alg1.l9.m1.6.6.1.1" xref="alg1.l9.m1.6.6.1.1.cmml"><mfrac id="alg1.l9.m1.5.5" xref="alg1.l9.m1.5.5.cmml"><mrow id="alg1.l9.m1.3.3.2.2" xref="alg1.l9.m1.3.3.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.3.3.2.2.2" xref="alg1.l9.m1.3.3.2.3.1.cmml">|</mo><msubsup id="alg1.l9.m1.3.3.2.2.1" xref="alg1.l9.m1.3.3.2.2.1.cmml"><mi mathsize="90%" id="alg1.l9.m1.3.3.2.2.1.2.2" xref="alg1.l9.m1.3.3.2.2.1.2.2.cmml">Î¾</mi><mi mathsize="90%" id="alg1.l9.m1.3.3.2.2.1.2.3" xref="alg1.l9.m1.3.3.2.2.1.2.3.cmml">v</mi><mrow id="alg1.l9.m1.2.2.1.1.1.3" xref="alg1.l9.m1.3.3.2.2.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.2.2.1.1.1.3.1" xref="alg1.l9.m1.3.3.2.2.1.cmml">(</mo><mi mathsize="90%" id="alg1.l9.m1.2.2.1.1.1.1" xref="alg1.l9.m1.2.2.1.1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.2.2.1.1.1.3.2" xref="alg1.l9.m1.3.3.2.2.1.cmml">)</mo></mrow></msubsup><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.3.3.2.2.3" xref="alg1.l9.m1.3.3.2.3.1.cmml">|</mo></mrow><mrow id="alg1.l9.m1.5.5.4.2" xref="alg1.l9.m1.5.5.4.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.5.5.4.2.2" xref="alg1.l9.m1.5.5.4.3.1.cmml">|</mo><msup id="alg1.l9.m1.5.5.4.2.1" xref="alg1.l9.m1.5.5.4.2.1.cmml"><mi mathsize="90%" id="alg1.l9.m1.5.5.4.2.1.2" xref="alg1.l9.m1.5.5.4.2.1.2.cmml">Î¾</mi><mrow id="alg1.l9.m1.4.4.3.1.1.3" xref="alg1.l9.m1.5.5.4.2.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.4.4.3.1.1.3.1" xref="alg1.l9.m1.5.5.4.2.1.cmml">(</mo><mi mathsize="90%" id="alg1.l9.m1.4.4.3.1.1.1" xref="alg1.l9.m1.4.4.3.1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.4.4.3.1.1.3.2" xref="alg1.l9.m1.5.5.4.2.1.cmml">)</mo></mrow></msup><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.5.5.4.2.3" xref="alg1.l9.m1.5.5.4.3.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="alg1.l9.m1.6.6.1.1.2" xref="alg1.l9.m1.6.6.1.1.2.cmml">â€‹</mo><mrow id="alg1.l9.m1.6.6.1.1.1.1" xref="alg1.l9.m1.6.6.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l9.m1.6.6.1.1.1.1.2" xref="alg1.l9.m1.6.6.1.1.1.1.1.cmml">(</mo><msubsup id="alg1.l9.m1.6.6.1.1.1.1.1" xref="alg1.l9.m1.6.6.1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l9.m1.6.6.1.1.1.1.1.2.2" xref="alg1.l9.m1.6.6.1.1.1.1.1.2.2.cmml">w</mi><mi mathsize="90%" id="alg1.l9.m1.6.6.1.1.1.1.1.2.3" xref="alg1.l9.m1.6.6.1.1.1.1.1.2.3.cmml">v</mi><mrow id="alg1.l9.m1.6.6.1.1.1.1.1.3" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.6.6.1.1.1.1.1.3.2" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.2.cmml">t</mi><mo mathsize="90%" id="alg1.l9.m1.6.6.1.1.1.1.1.3.1" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.1.cmml">+</mo><mn mathsize="90%" id="alg1.l9.m1.6.6.1.1.1.1.1.3.3" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.3.cmml">1</mn></mrow></msubsup><mo maxsize="90%" minsize="90%" rspace="0.340em" id="alg1.l9.m1.6.6.1.1.1.1.3" xref="alg1.l9.m1.6.6.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo mathsize="90%" stretchy="false" id="alg1.l9.m1.6.6.5" xref="alg1.l9.m1.6.6.5.cmml">âŸ¹</mo><mi id="alg1.l9.m1.6.6.6" xref="alg1.l9.m1.6.6.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.6b"><apply id="alg1.l9.m1.6.6.cmml" xref="alg1.l9.m1.6.6"><and id="alg1.l9.m1.6.6a.cmml" xref="alg1.l9.m1.6.6"></and><apply id="alg1.l9.m1.6.6b.cmml" xref="alg1.l9.m1.6.6"><ci id="alg1.l9.m1.6.6.4.cmml" xref="alg1.l9.m1.6.6.4">â†</ci><apply id="alg1.l9.m1.6.6.3.cmml" xref="alg1.l9.m1.6.6.3"><csymbol cd="ambiguous" id="alg1.l9.m1.6.6.3.1.cmml" xref="alg1.l9.m1.6.6.3">superscript</csymbol><ci id="alg1.l9.m1.6.6.3.2.cmml" xref="alg1.l9.m1.6.6.3.2">ğ‘¤</ci><apply id="alg1.l9.m1.1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1"><plus id="alg1.l9.m1.1.1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1.1.1"></plus><ci id="alg1.l9.m1.1.1.1.1.1.2.cmml" xref="alg1.l9.m1.1.1.1.1.1.2">ğ‘¡</ci><cn type="integer" id="alg1.l9.m1.1.1.1.1.1.3.cmml" xref="alg1.l9.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="alg1.l9.m1.6.6.1.cmml" xref="alg1.l9.m1.6.6.1"><apply id="alg1.l9.m1.6.6.1.2.cmml" xref="alg1.l9.m1.6.6.1.2"><csymbol cd="ambiguous" id="alg1.l9.m1.6.6.1.2.1.cmml" xref="alg1.l9.m1.6.6.1.2">subscript</csymbol><sum id="alg1.l9.m1.6.6.1.2.2.cmml" xref="alg1.l9.m1.6.6.1.2.2"></sum><apply id="alg1.l9.m1.6.6.1.2.3.cmml" xref="alg1.l9.m1.6.6.1.2.3"><in id="alg1.l9.m1.6.6.1.2.3.1.cmml" xref="alg1.l9.m1.6.6.1.2.3.1"></in><ci id="alg1.l9.m1.6.6.1.2.3.2.cmml" xref="alg1.l9.m1.6.6.1.2.3.2">ğ‘£</ci><ci id="alg1.l9.m1.6.6.1.2.3.3.cmml" xref="alg1.l9.m1.6.6.1.2.3.3">ğ•</ci></apply></apply><apply id="alg1.l9.m1.6.6.1.1.cmml" xref="alg1.l9.m1.6.6.1.1"><times id="alg1.l9.m1.6.6.1.1.2.cmml" xref="alg1.l9.m1.6.6.1.1.2"></times><apply id="alg1.l9.m1.5.5.cmml" xref="alg1.l9.m1.5.5"><divide id="alg1.l9.m1.5.5.5.cmml" xref="alg1.l9.m1.5.5"></divide><apply id="alg1.l9.m1.3.3.2.3.cmml" xref="alg1.l9.m1.3.3.2.2"><abs id="alg1.l9.m1.3.3.2.3.1.cmml" xref="alg1.l9.m1.3.3.2.2.2"></abs><apply id="alg1.l9.m1.3.3.2.2.1.cmml" xref="alg1.l9.m1.3.3.2.2.1"><csymbol cd="ambiguous" id="alg1.l9.m1.3.3.2.2.1.1.cmml" xref="alg1.l9.m1.3.3.2.2.1">superscript</csymbol><apply id="alg1.l9.m1.3.3.2.2.1.2.cmml" xref="alg1.l9.m1.3.3.2.2.1"><csymbol cd="ambiguous" id="alg1.l9.m1.3.3.2.2.1.2.1.cmml" xref="alg1.l9.m1.3.3.2.2.1">subscript</csymbol><ci id="alg1.l9.m1.3.3.2.2.1.2.2.cmml" xref="alg1.l9.m1.3.3.2.2.1.2.2">ğœ‰</ci><ci id="alg1.l9.m1.3.3.2.2.1.2.3.cmml" xref="alg1.l9.m1.3.3.2.2.1.2.3">ğ‘£</ci></apply><ci id="alg1.l9.m1.2.2.1.1.1.1.cmml" xref="alg1.l9.m1.2.2.1.1.1.1">ğ‘¡</ci></apply></apply><apply id="alg1.l9.m1.5.5.4.3.cmml" xref="alg1.l9.m1.5.5.4.2"><abs id="alg1.l9.m1.5.5.4.3.1.cmml" xref="alg1.l9.m1.5.5.4.2.2"></abs><apply id="alg1.l9.m1.5.5.4.2.1.cmml" xref="alg1.l9.m1.5.5.4.2.1"><csymbol cd="ambiguous" id="alg1.l9.m1.5.5.4.2.1.1.cmml" xref="alg1.l9.m1.5.5.4.2.1">superscript</csymbol><ci id="alg1.l9.m1.5.5.4.2.1.2.cmml" xref="alg1.l9.m1.5.5.4.2.1.2">ğœ‰</ci><ci id="alg1.l9.m1.4.4.3.1.1.1.cmml" xref="alg1.l9.m1.4.4.3.1.1.1">ğ‘¡</ci></apply></apply></apply><apply id="alg1.l9.m1.6.6.1.1.1.1.1.cmml" xref="alg1.l9.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l9.m1.6.6.1.1.1.1.1.1.cmml" xref="alg1.l9.m1.6.6.1.1.1.1">superscript</csymbol><apply id="alg1.l9.m1.6.6.1.1.1.1.1.2.cmml" xref="alg1.l9.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l9.m1.6.6.1.1.1.1.1.2.1.cmml" xref="alg1.l9.m1.6.6.1.1.1.1">subscript</csymbol><ci id="alg1.l9.m1.6.6.1.1.1.1.1.2.2.cmml" xref="alg1.l9.m1.6.6.1.1.1.1.1.2.2">ğ‘¤</ci><ci id="alg1.l9.m1.6.6.1.1.1.1.1.2.3.cmml" xref="alg1.l9.m1.6.6.1.1.1.1.1.2.3">ğ‘£</ci></apply><apply id="alg1.l9.m1.6.6.1.1.1.1.1.3.cmml" xref="alg1.l9.m1.6.6.1.1.1.1.1.3"><plus id="alg1.l9.m1.6.6.1.1.1.1.1.3.1.cmml" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.1"></plus><ci id="alg1.l9.m1.6.6.1.1.1.1.1.3.2.cmml" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="alg1.l9.m1.6.6.1.1.1.1.1.3.3.cmml" xref="alg1.l9.m1.6.6.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></apply><apply id="alg1.l9.m1.6.6c.cmml" xref="alg1.l9.m1.6.6"><implies id="alg1.l9.m1.6.6.5.cmml" xref="alg1.l9.m1.6.6.5"></implies><share href="#alg1.l9.m1.6.6.1.cmml" id="alg1.l9.m1.6.6d.cmml" xref="alg1.l9.m1.6.6"></share><csymbol cd="latexml" id="alg1.l9.m1.6.6.6.cmml" xref="alg1.l9.m1.6.6.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.6c">w^{(t+1)}\ \leftarrow\ \sum_{v\in\mathbb{V}}\frac{|\xi_{v}^{(t)}|}{|\xi^{(t)}|}(w_{v}^{t+1})\hskip 3.41418pt\implies</annotation></semantics></math><span id="alg1.l9.3" class="ltx_text" style="font-size:90%;"> FedAvg
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text" style="font-size:90%;">Â Â </span><span id="alg1.l10.3" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l10.4" class="ltx_text" style="font-size:90%;">Â </span><span id="alg1.l10.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><span id="alg1.l11.2" class="ltx_text" style="font-size:90%;">Â Â Output the aggregated global model </span><math id="alg1.l11.m1.1" class="ltx_Math" alttext="w\leftarrow w^{(T)}" display="inline"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.2" xref="alg1.l11.m1.1.2.cmml"><mi mathsize="90%" id="alg1.l11.m1.1.2.2" xref="alg1.l11.m1.1.2.2.cmml">w</mi><mo mathsize="90%" stretchy="false" id="alg1.l11.m1.1.2.1" xref="alg1.l11.m1.1.2.1.cmml">â†</mo><msup id="alg1.l11.m1.1.2.3" xref="alg1.l11.m1.1.2.3.cmml"><mi mathsize="90%" id="alg1.l11.m1.1.2.3.2" xref="alg1.l11.m1.1.2.3.2.cmml">w</mi><mrow id="alg1.l11.m1.1.1.1.3" xref="alg1.l11.m1.1.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l11.m1.1.1.1.3.1" xref="alg1.l11.m1.1.2.3.cmml">(</mo><mi mathsize="90%" id="alg1.l11.m1.1.1.1.1" xref="alg1.l11.m1.1.1.1.1.cmml">T</mi><mo maxsize="90%" minsize="90%" id="alg1.l11.m1.1.1.1.3.2" xref="alg1.l11.m1.1.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.2.cmml" xref="alg1.l11.m1.1.2"><ci id="alg1.l11.m1.1.2.1.cmml" xref="alg1.l11.m1.1.2.1">â†</ci><ci id="alg1.l11.m1.1.2.2.cmml" xref="alg1.l11.m1.1.2.2">ğ‘¤</ci><apply id="alg1.l11.m1.1.2.3.cmml" xref="alg1.l11.m1.1.2.3"><csymbol cd="ambiguous" id="alg1.l11.m1.1.2.3.1.cmml" xref="alg1.l11.m1.1.2.3">superscript</csymbol><ci id="alg1.l11.m1.1.2.3.2.cmml" xref="alg1.l11.m1.1.2.3.2">ğ‘¤</ci><ci id="alg1.l11.m1.1.1.1.1.cmml" xref="alg1.l11.m1.1.1.1.1">ğ‘‡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">w\leftarrow w^{(T)}</annotation></semantics></math><span id="alg1.l11.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
</div>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Data heterogeneity, client drift, and data imbalance from clients have proved to significantly impact the performance of FedAvg optimization resulting in unstable convergence. The data obtained from CAVs typically have non-independent and identically distributed (non-iid) and imbalanced data distribution. There is a need to develop an FL framework that could perform well with the varying data distribution from CAVs. FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> algorithm combines FedAvg with a proximal term to improve convergence and reduce communication cost. Fed-ADAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> has shown improved convergence and optimization performance by incorporating ADAM optimization in FedAvg algorithm. Dynamic Federated Proximal (DFP) algorithm is an extension of the Federated Proximal Algorithm (FPA) that can effectively deal with non-iid data distribution by dynamically varying the learning rate and regularization coefficient during the learning processÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Federated Distillation (FD)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> uses knowledge distillation to transfer knowledge in a decentralized manner leading to a significant reduction in the communication size compared to a traditional FL and also can have the ability to handle non-iid data samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. There have been efforts to address the client heterogeneity, and it is an ongoing research areaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table II: </span>Literature Overview of FL Application to Autonomous Vehicles</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T2.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span></td>
<td id="S2.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T2.1.1.2.1" class="ltx_text ltx_font_bold">Time</span></td>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.3.1.1" class="ltx_p" style="width:70.0pt;"><span id="S2.T2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Data Modality</span></span>
</span>
</td>
<td id="S2.T2.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.4.1.1" class="ltx_p" style="width:85.0pt;"><span id="S2.T2.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Application</span></span>
</span>
</td>
<td id="S2.T2.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.5.1.1" class="ltx_p" style="width:60.0pt;"><span id="S2.T2.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Base Model</span></span>
</span>
</td>
<td id="S2.T2.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.6.1.1" class="ltx_p" style="width:60.0pt;"><span id="S2.T2.1.1.6.1.1.1" class="ltx_text ltx_font_bold">FL Algorithm</span></span>
</span>
</td>
<td id="S2.T2.1.1.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.7.1.1" class="ltx_p"><span id="S2.T2.1.1.7.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.2" class="ltx_tr">
<td id="S2.T2.1.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></td>
<td id="S2.T2.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2020</td>
<td id="S2.T2.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.3.1.1" class="ltx_p" style="width:70.0pt;">Time series data of multiple features from sensors</span>
</span>
</td>
<td id="S2.T2.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.4.1.1" class="ltx_p" style="width:85.0pt;">Turn signal prediction</span>
</span>
</td>
<td id="S2.T2.1.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.5.1.1" class="ltx_p" style="width:60.0pt;">LSTM</span>
</span>
</td>
<td id="S2.T2.1.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.6.1.1" class="ltx_p" style="width:60.0pt;">FedAvg</span>
</span>
</td>
<td id="S2.T2.1.2.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.7.1.1" class="ltx_p">Fordâ€™s Big Data Drive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.3" class="ltx_tr">
<td id="S2.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></td>
<td id="S2.T2.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2021</td>
<td id="S2.T2.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image</span>
</span>
</td>
<td id="S2.T2.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.4.1.1" class="ltx_p" style="width:85.0pt;">Steering angle prediction</span>
</span>
</td>
<td id="S2.T2.1.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.5.1.1" class="ltx_p" style="width:60.0pt;">Two-stream CNN</span>
</span>
</td>
<td id="S2.T2.1.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.6.1.1" class="ltx_p" style="width:60.0pt;">Async FL</span>
</span>
</td>
<td id="S2.T2.1.3.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.7.1.1" class="ltx_p">Self-collected</span>
</span>
</td>
</tr>
<tr id="S2.T2.1.4" class="ltx_tr">
<td id="S2.T2.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></td>
<td id="S2.T2.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2021</td>
<td id="S2.T2.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image</span>
</span>
</td>
<td id="S2.T2.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.4.1.1" class="ltx_p" style="width:85.0pt;">Steering angle prediction</span>
</span>
</td>
<td id="S2.T2.1.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.5.1.1" class="ltx_p" style="width:60.0pt;">Self-defined CNN</span>
</span>
</td>
<td id="S2.T2.1.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.6.1.1" class="ltx_p" style="width:60.0pt;">FedAvg</span>
</span>
</td>
<td id="S2.T2.1.4.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.7.1.1" class="ltx_p">Self-collected</span>
</span>
</td>
</tr>
<tr id="S2.T2.1.5" class="ltx_tr">
<td id="S2.T2.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></td>
<td id="S2.T2.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2021</td>
<td id="S2.T2.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image and Li-
DAR</span>
</span>
</td>
<td id="S2.T2.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.1.1" class="ltx_p" style="width:85.0pt;">Object detection</span>
</span>
</td>
<td id="S2.T2.1.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.5.1.1" class="ltx_p" style="width:60.0pt;">YOLO CNN</span>
</span>
</td>
<td id="S2.T2.1.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.6.1.1" class="ltx_p" style="width:60.0pt;">FedSGD</span>
</span>
</td>
<td id="S2.T2.1.5.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.7.1.1" class="ltx_p">Canadian Adverse Driving Conditions Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.6" class="ltx_tr">
<td id="S2.T2.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></td>
<td id="S2.T2.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.6.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image and trajectory data</span>
</span>
</td>
<td id="S2.T2.1.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.6.4.1.1" class="ltx_p" style="width:85.0pt;">Target speed tracking</span>
</span>
</td>
<td id="S2.T2.1.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.6.5.1.1" class="ltx_p" style="width:60.0pt;">Self-defined NN</span>
</span>
</td>
<td id="S2.T2.1.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.6.6.1.1" class="ltx_p" style="width:60.0pt;">DFP (FedAvg for aggregation)</span>
</span>
</td>
<td id="S2.T2.1.6.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.6.7.1.1" class="ltx_p">Berkeley deep drive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and dataset of annotated car trajectories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.7" class="ltx_tr">
<td id="S2.T2.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></td>
<td id="S2.T2.1.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.7.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image</span>
</span>
</td>
<td id="S2.T2.1.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.7.4.1.1" class="ltx_p" style="width:85.0pt;">Traffic sign recognition</span>
</span>
</td>
<td id="S2.T2.1.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.7.5.1.1" class="ltx_p" style="width:60.0pt;">LeNet-5</span>
</span>
</td>
<td id="S2.T2.1.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.7.6.1.1" class="ltx_p" style="width:60.0pt;">FedAvg</span>
</span>
</td>
<td id="S2.T2.1.7.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.7.7.1.1" class="ltx_p">German Traffic Sign Recognition Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.8" class="ltx_tr">
<td id="S2.T2.1.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></td>
<td id="S2.T2.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.8.3.1.1" class="ltx_p" style="width:70.0pt;">Multi-modal image</span>
</span>
</td>
<td id="S2.T2.1.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.8.4.1.1" class="ltx_p" style="width:85.0pt;">Semantic Segmentation</span>
</span>
</td>
<td id="S2.T2.1.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.8.5.1.1" class="ltx_p" style="width:60.0pt;">BiSeNet V2</span>
</span>
</td>
<td id="S2.T2.1.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.8.6.1.1" class="ltx_p" style="width:60.0pt;">FedAvg + Variants</span>
</span>
</td>
<td id="S2.T2.1.8.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.8.7.1.1" class="ltx_p">Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and IDDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.9" class="ltx_tr">
<td id="S2.T2.1.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></td>
<td id="S2.T2.1.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.9.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image and LiDAR</span>
</span>
</td>
<td id="S2.T2.1.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.9.4.1.1" class="ltx_p" style="width:85.0pt;">3D object detection</span>
</span>
</td>
<td id="S2.T2.1.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.9.5.1.1" class="ltx_p" style="width:60.0pt;">U-Net</span>
</span>
</td>
<td id="S2.T2.1.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.9.6.1.1" class="ltx_p" style="width:60.0pt;">HFCL (FedAvg for aggregation)</span>
</span>
</td>
<td id="S2.T2.1.9.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.9.7.1.1" class="ltx_p">Lyft Level 5 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.10" class="ltx_tr">
<td id="S2.T2.1.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite></td>
<td id="S2.T2.1.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.10.3.1.1" class="ltx_p" style="width:70.0pt;">Vehicle position, velocity and acceleration + Driver behavior</span>
</span>
</td>
<td id="S2.T2.1.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.10.4.1.1" class="ltx_p" style="width:85.0pt;">Trajectory prediction</span>
</span>
</td>
<td id="S2.T2.1.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.10.5.1.1" class="ltx_p" style="width:60.0pt;">LSTM</span>
</span>
</td>
<td id="S2.T2.1.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.10.6.1.1" class="ltx_p" style="width:60.0pt;">FedAvg+Variants</span>
</span>
</td>
<td id="S2.T2.1.10.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.10.7.1.1" class="ltx_p">USâ€101 and Iâ€80 data sets of NGSIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.11" class="ltx_tr">
<td id="S2.T2.1.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></td>
<td id="S2.T2.1.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.11.3.1.1" class="ltx_p" style="width:70.0pt;">Vehicle position, velocity and acceleration</span>
</span>
</td>
<td id="S2.T2.1.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.11.4.1.1" class="ltx_p" style="width:85.0pt;">Collision avoidance</span>
</span>
</td>
<td id="S2.T2.1.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.11.5.1.1" class="ltx_p" style="width:60.0pt;">Deep RL</span>
</span>
</td>
<td id="S2.T2.1.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.11.6.1.1" class="ltx_p" style="width:60.0pt;">SFRL (FedAvg for aggregation)</span>
</span>
</td>
<td id="S2.T2.1.11.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.11.7.1.1" class="ltx_p">Self-generated</span>
</span>
</td>
</tr>
<tr id="S2.T2.1.12" class="ltx_tr">
<td id="S2.T2.1.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></td>
<td id="S2.T2.1.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2022</td>
<td id="S2.T2.1.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.12.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image</span>
</span>
</td>
<td id="S2.T2.1.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.12.4.1.1" class="ltx_p" style="width:85.0pt;">Driver activity recognition</span>
</span>
</td>
<td id="S2.T2.1.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.12.5.1.1" class="ltx_p" style="width:60.0pt;">ResNet-56</span>
</span>
</td>
<td id="S2.T2.1.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.12.6.1.1" class="ltx_p" style="width:60.0pt;">FedGKT</span>
</span>
</td>
<td id="S2.T2.1.12.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.12.7.1.1" class="ltx_p">State Farm Distracted Driver Detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and AI City Challenge 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.13" class="ltx_tr">
<td id="S2.T2.1.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></td>
<td id="S2.T2.1.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2023</td>
<td id="S2.T2.1.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.13.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image</span>
</span>
</td>
<td id="S2.T2.1.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.13.4.1.1" class="ltx_p" style="width:85.0pt;">Driver activity recognition</span>
</span>
</td>
<td id="S2.T2.1.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.13.5.1.1" class="ltx_p" style="width:60.0pt;">ResNet-34</span>
</span>
</td>
<td id="S2.T2.1.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T2.1.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.13.6.1.1" class="ltx_p" style="width:60.0pt;">FedProx + Variants</span>
</span>
</td>
<td id="S2.T2.1.13.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S2.T2.1.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.13.7.1.1" class="ltx_p">State Farm Distracted Driver Detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and Drive&amp;Act <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T2.1.14" class="ltx_tr">
<td id="S2.T2.1.14.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></td>
<td id="S2.T2.1.14.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">2023</td>
<td id="S2.T2.1.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T2.1.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.14.3.1.1" class="ltx_p" style="width:70.0pt;">RGB image</span>
</span>
</td>
<td id="S2.T2.1.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T2.1.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.14.4.1.1" class="ltx_p" style="width:85.0pt;">Driver fatigue detection</span>
</span>
</td>
<td id="S2.T2.1.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T2.1.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.14.5.1.1" class="ltx_p" style="width:60.0pt;">Bayesian CNN</span>
</span>
</td>
<td id="S2.T2.1.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T2.1.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.14.6.1.1" class="ltx_p" style="width:60.0pt;">FedSup</span>
</span>
</td>
<td id="S2.T2.1.14.7" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T2.1.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.14.7.1.1" class="ltx_p">Blinking Video Database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> and Eyeblink8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Applications of FL for CAV</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section reviews a few important applications in detail of FL in CAVs. The FL4CAV literature, including data modalities, underlying models, applications, and datasets, are highlighted in TableÂ <a href="#S2.T2" title="Table II â€£ II-C Federated Learning Algorithm â€£ II Overview of Data Modalities, Data Securities and Algorithms â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Different applications on CAV are highly dependent on different strengths of FL, such as protecting privacy, improving learning efficiency, enhancing generalization ability, reducing communication overhead, etc.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">In-vehicle Human Monitoring</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">FL has the potential to enhance the security of user data on board, while enabling knowledge transfer and ensuring the generalization ability of the model. However, in human-related applications where data is highly heterogeneous and personalized, it can be challenging to balance the generalization ability of the model with the need for personalization to specific users.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Driver monitoring applications, such as distraction detection, are critical safety features that monitor the driverâ€™s steadiness, and alertness and warn the distracted driver to apply the brakesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Driver privacy may be a bigger concern than steering wheel angle prediction and object recognition, leading to FLâ€™s ability to be more highlighted in terms of privacy protection. However, the driver monitoring application is a highly personalized application where the driverâ€™s behavior is strongly associated with personal habits, emotions, cultural background, and even the interpretation of instructions. This user heterogeneity poses a challenge for FL systems. For human-related applications like driver monitoring, personalized FL is the dominant solution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Passenger monitoring applications are an emerging research area that involves detecting passenger intents of boarding and alighting and warning of dangerous behavior in public transportationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. However, this field has not yet received much attention due to the lack of available datasets and the difficulty of monitoring multiple users simultaneously. Nevertheless, the ability of FL to integrate knowledge about public transportation and the growing demand for passenger monitoring makes it a promising application in this area.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Steering Wheel Angle Prediction</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">While non-human related applications are generally less invasive to user privacy, they still incur significant learning costs and communication overhead when CAVs are involved. FL remains a strong candidate for these applications, especially when more CAVs are involved.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Steering wheel angle prediction improves driving safety by detecting vehicle yaw as an essential component of Advanced Driver Assistance Systems (ADAS). As of 2020 in the United States, about 30% of the roads are still unpaved <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. Recent technologies like the end-to-end learning approach enable AVs to drive safely even under challenging circumstances, such as unpaved and unmarked roads. The prediction of steering wheel angle is one of the critical aspects of an autonomous vehicle (Society of Automotive Engineers (SAE) Level 4 and Level 5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>) that controls the lateral position of the vehicle. The steering wheel angle is predicted from the RGB images collected from the front-facing camera as input. The dataset consisting of camera images and the steering wheel angle is trained on a CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The CNN models have been demonstrated to efficiently perform a lane following task on an unmarked road <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. Related literature has demonstrated that FL can collaboratively learn the neural network model at a significantly lower communication cost while also preserving privacyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Vehicle Trajectory Prediction</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">FL can improve the modelâ€™s ability to handle rare events, such as traffic accidents, adverse weather conditions, and risky behaviors, by leveraging the collective knowledge of CAVs.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Vehicle trajectory prediction allows drivers or ADAS to anticipate potentially dangerous behavior of other vehicles, such as sudden lane changes, skidding, or tire blowouts, in order to react proactively and prevent accidents. Autonomous vehicles navigate in highly-uncertain and interactive environments shared with other dynamic agents. In order to plan safe and comfortable maneuvers, they need to predict the future trajectories of surrounding vehicles. The inherent uncertainty of the future makes trajectory prediction a challenging taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. AVs are often required to drive in a dynamic and challenging environment along with other vehicles. In these scenarios, predicting the trajectories of the surrounding vehicles/environment are crucial for safer and more comfortable navigation. Accurate prediction of trajectories is one of the complex tasks of an AV due to high computation cost, diverse driving styles (aggressive/defensive), dynamic behavior of the road obstacles (vehicles, pedestrians, objects), and several noise factors (sensors, weather). The task of predicting an optimal vehicle trajectory is challenging, but the use of a collaborative learning platform can facilitate the development of an efficient model for safe driving. By enabling models to learn from rare hazard events of each CAVS, they can react in a timely manner to avoid potential accidents and create a safer driving environment. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, it has been reported that the FL approach achieves a similar performance over centralized learning.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Object Detection</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">FL enables the CAV framework to learn efficiently with low communication overhead, which is particularly advantageous when the volume of data is much larger than the size of the model.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Object detection is one of the most important functions of the visual perception system, and FL can effectively help CAVs detect diverse objects in different driving scenarios, such as road, traffic, vehicles, obstacles, and pedestrians. Most of the current techniques use 2D detection and 3D detection methods. The 3D detection methods provide additional information, such as the sizes, locations, and classes of the objects that are necessary for motion planning, collision avoidance, and motion control. 3D object detection has witnessed significant advances due to the rapid evolution of deep learning-based methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. However, the data size of 3D object detection, such as LiDAR and high resolution image, is typically large. As a result, there are significant challenges to deploying robust object detection models on a traditional centralized learning approach due to the high communication and computation overhead. These concerns can be mitigated through the use of a FL based approach for CAVs.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">FL has already been in practice much before for computer vision-related tasks such as developing safety hazard warning solutions in smart city applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. Object detection accuracy generally struggles under adverse weather circumstances such as snow. It has demonstrated that the CNN-FL framework improves the detection accuracy and performs better than the centralized and gossip decentralized modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Recently there have been numerous studies to improve the performance of FL on complex tasks like object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. A hybrid federated and centralized learning (HFCL) framework was proposed that allows vehicles with computational resources to be part of the FL training process while the others transmit their local data to the server like a centralized learning process. The trade-off between the computational and communication overhead of the vehicles is addressed. The performance of HFCL is not shown to be better than a centralized learning approach in this example <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and is a subject for further research and improvement. It is demonstrated that with multi-stage resource allocation and robust device selection, the performance of FL significantly improved compared to traditional centralized learning and baseline FL approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Motion Control Application</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">FL enables CAVs to quickly adapt to different driving scenarios, including unfamiliar and unvisited roads, cities, and countries. Additionally, FL may enable CAVs to adjust driving styles based on different driving habits, climate, scenarios, and cultural norms.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Motion control application, including learning-based methods such as RL, and Genetic Algorithm (GA) have proved effective in solving difficult and challenging control-related tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. The controls typically include throttle, steering wheel angle, and braking. It has been demonstrated that reinforcement methods-based driving achieves better performance than human-based driving. However, implementing RL-based methods on vehicles is challenging due to the high computational power demand.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">FL approach enables CAVs to train and optimize controller parameters collaboratively. A few potential benefits are enabling AVs to adapt to unseen routes/traffic scenarios or operating conditions because of past data from other AVs, on-ramp acceleration, driving in congested traffic scenarios, and so on. For instance, if a driver from a rural area is driving in a city, FL can help the CAV quickly adapt to the new driving environment and enhance the safety, comfort, energy efficiency, and effectiveness of the driving experience. Target speed tracking is one application that has used FL recently to perform target speed tracking under various scenarios. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, a two-layer Multi-layer Perceptron (MLP) model was trained in an FL framework to dynamically adjust the Proportional Integral Derivative (PID) gains controller parameters to achieve the desired target speed efficiently under various traffic scenarios.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Challenges</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we review and summarize the remaining challenges in state-of-the-art technology and the future scope of research.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Resource Limitations</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Massively parallel CAVs raise questions about collaboration capabilities, management, and resources.</span> Huge CAVs participation in the FL could increase the solve time, memory utilization, and therefore the computational power for the global model update. In particular, the vision-related perception tasks have concerns such as high communication costs and not being flexible towards heterogeneous datasets. Decentralized FL and Clustered FLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> are also being explored to reduce the communication overhead.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Lack of sufficient real-world datasets, simulator, and pre-trained base model.</span> There is a need for more real-world datasets (different weather conditions and traffic scenarios), a realistic high-fidelity FL4CAV simulator for seamless FL integrationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, and a good pre-training model. Federated transfer learning is a new approach that has been adopted to improve the model performance, and accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Imperfect Methodology</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Privacy and security issues.</span> Massive data also leads to privacy and security concerns. This problem must be addressed to train the ML model efficiently without compromising on the modelâ€™s accuracy and redundancy.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Lack of robust approach for vehicle selection and resource allocation.</span> Currently, there is no popular mechanism that can select non-redundant data from CAVs to minimize the network strain. There are ongoing efforts to develop robust methods to select vehicles and resource allocation schemesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, the overall training process was demonstrated to be efficient due to incorporating a client selection model. The setup looks at the resource availability of the clients and then determines the clients eligible to be part of the FL global model learning process. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, it is demonstrated that the model performance was improved with AVs that were selected by a trust-based deep reinforcement.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Catastrophic forgetting.</span> CAV cannot keep all user data due to storage capacity limitations, and new data will always be generated during iteration. Therefore, when the FL framework is updated on new data in iterationâ€”see Algorithm <a href="#alg1" title="Algorithm 1 â€£ II-C Federated Learning Algorithm â€£ II Overview of Data Modalities, Data Securities and Algorithms â€£ A Survey of Federated Learning for Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the global model forgets the previous knowledge and leads to catastrophic forgetting.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Lack of robust fairness and incentive mechanism.</span> Need for a robust rewarding mechanism. The amount of information shared by CAVs is different and highly inconsistent (Data imbalance). Hence, there needs to be a fair incentive mechanism to reward CAVs for their contribution.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Inadequate Evaluation Criteria</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">FL suitability evaluation for new users.</span> It is often difficult for the newcomer vehicle to make any informed decisions. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, a trust-aware Deep RL model is proposed to assist new vehicles in making superior trajectory and motion planning decisions.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Need for high capability diagnostics.</span> There are several noise factors that could influence the decision of the FL, such as faulty sensors in a visual perception case and incorrect imputation of missing data. The development of a robust diagnostic that can identify and eliminate the updates from these vehicles is needed.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In summary, FL is a new technology breakthrough and has started to be applied in the CAV domain. This paper reviews the various developments, data modalities, and algorithms of FL4CAV and provides a broad list of applications of FL in CAVs. In particular, our focus was on current challenges and the future scope of FL4CAV.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Future work lies in continuing the effort in this paper to provide a more detailed, in-depth, and comprehensive survey of FL4CAV and related vehicle fields. Especially some of the most advanced technologies aim to address communication, security and privacy, and system heterogeneity issues. Some feasible solutions, such as fully decentralized FL and model compression, are feasible for communication problems. Current techniques of threats and malicious attacks on FL are also open problems, such as backdoor attacks, free-riding attacks, and eavesdropping. Some protective techniques, such as differential privacy techniques and homomorphic encryption, are used as potential solutions for security and privacy. Our focus is to apply vehicular FL by drawing on relevant research in other areas, such as healthcare, IoT, and industry. At the same time, a matching FL framework needs to be developed, taking into account the communication, computation, storage, and usage scenarios underlying the vehicles.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
F.Â Sterk, D.Â Dann, and C.Â Weinhardt, â€œMonetizing Car Data: A Literature
Review on Data-Driven Business Models in the Connected Car
Domain,â€ in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Hawaii International Conference on System
Sciences</span>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P.Â Voigt and A.Â VonÂ dem Bussche, â€œThe eu general data protection regulation
(gdpr),â€ <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</span>, vol.Â 10, no.Â 3152676, pp.Â 10â€“5555, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P.Â Kairouz, H.Â B. McMahan, <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œAdvances and open problems in
federated learning,â€ <span id="bib.bib3.2.2" class="ltx_text ltx_font_italic">Foundations and TrendsÂ® in Machine Learning</span>,
vol.Â 14, no.Â 1â€“2, pp.Â 1â€“210, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Q.Â Yang, Y.Â Liu, T.Â Chen, and Y.Â Tong, â€œFederated machine learning:
Concept and applications,â€ <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems
and Technology (TIST)</span>, vol.Â 10, no.Â 2, pp.Â 1â€“19, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B.Â McMahan, E.Â Moore, D.Â Ramage, S.Â Hampson, and B.Â A. yÂ Arcas,
â€œCommunication-Efficient Learning of Deep Networks from
Decentralized Data,â€ in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics</span>,
pp.Â 1273â€“1282, PMLR, Apr. 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.Â Hard, K.Â Rao, R.Â Mathews, F.Â Beaufays, S.Â Augenstein, H.Â Eichner, C.Â Kiddon,
and D.Â Ramage, â€œFederated learning for mobile keyboard prediction,â€ <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol.Â abs/1811.03604, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K.Â Bonawitz, V.Â Ivanov, B.Â Kreuter, A.Â Marcedone, H.Â B. McMahan, S.Â Patel,
D.Â Ramage, A.Â Segal, and K.Â Seth, â€œPractical secure aggregation for
privacy-preserving machine learning,â€ in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</span>, pp.Â 1175â€“1191,
2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S.Â Savazzi, M.Â Nicoli, M.Â Bennis, S.Â Kianoush, and L.Â Barbieri, â€œOpportunities
of Federated Learning in Connected, Cooperative, and Automated
Industrial Systems,â€ <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Communications Magazine</span>, vol.Â 59,
pp.Â 16â€“21, Feb. 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z.Â Du, C.Â Wu, T.Â Yoshinaga, K.-L.Â A. Yau, Y.Â Ji, and J.Â Li, â€œFederated
Learning for Vehicular Internet of Things: Recent Advances
and Open Issues,â€ <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Open Journal of the Computer Society</span>,
vol.Â 1, pp.Â 45â€“61, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
L.Â Chen, M.Â Torstensson, and C.Â Englund, â€œFederated learning to enable
automotive collaborative ecosystem: Opportunities and challenges,â€ <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Virtual ITS European Congress</span>, Nov. 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A.Â Eskandarian, C.Â Wu, and C.Â Sun, â€œResearch Advances and Challenges
of Autonomous and Connected Ground Vehicles,â€ <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Transactions
on Intelligent Transportation Systems</span>, vol.Â 22, pp.Â 683â€“711, Feb. 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S.Â Grigorescu, B.Â Trasnea, T.Â Cocias, and G.Â Macesanu, â€œA Survey of Deep
Learning Techniques for Autonomous Driving,â€ <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Journal of Field
Robotics</span>, vol.Â 37, pp.Â 362â€“386, Apr. 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K.Â D. Stergiou, K.Â E. Psannis, V.Â Vitsas, and Y.Â Ishibashi, â€œA Federated
Learning Approach for Enhancing Autonomous Vehicles Image
Recognition,â€ in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2022 4th International Conference on Computer
Communication and the Internet (ICCCI)</span>, (Chiba, Japan),
pp.Â 87â€“90, IEEE, July 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L.Â Fantauzzo, E.Â FanÃ¬, D.Â Caldarola, A.Â Tavera, F.Â Cermelli, M.Â Ciccone,
and B.Â Caputo, â€œFedDrive: Generalizing federated learning to semantic
segmentation in autonomous driving,â€ in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2022 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)</span>, pp.Â 11504â€“11511, IEEE,
2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L.Â Yuan, L.Â Su, and Z.Â Wang, â€œFederated transfer-ordered-personalized learning
for driver monitoring application,â€ <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.04829</span>,
2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A.Â Mishra, S.Â Lee, D.Â Kim, and S.Â Kim, â€œIn-cabin monitoring system for
autonomous vehicles,â€ <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol.Â 22, no.Â 12, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K.Â Doshi and Y.Â Yilmaz, â€œFederated learning-based driver activity recognition
for edge devices,â€ in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</span>, pp.Â 3338â€“3346, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C.Â Zhao, Z.Â Gao, Q.Â Wang, K.Â Xiao, Z.Â Mo, and M.Â J. Deen, â€œFedsup: A
communication-efficient federated learning fatigue driving behaviors
supervision approach,â€ <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Future Gener. Comput. Syst.</span>, vol.Â 138,
pp.Â 52â€“60, Jan. 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G.Â Rjoub, O.Â A. Wahab, J.Â Bentahar, and A.Â S. Bataineh, â€œImproving Autonomous
Vehicles Safety in Snow Weather using Federated YOLO CNN Learning,â€ in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Mobile Web and Intelligent Information Systems: 17th International
Conference, MobiWIS 2021, Virtual Event, August 23â€“25, 2021, Proceedings</span>,
pp.Â 121â€“134, Springer, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A.Â M. Elbir, S.Â Coleri, A.Â K. Papazafeiropoulos, P.Â Kourtessis, and
S.Â Chatzinotas, â€œA hybrid architecture for federated and centralized
learning,â€ <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Cogn. Commun. Netw.</span>, Jun. 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y.Â Fu, C.Â Li, F.Â R. Yu, T.Â H. Luan, and Y.Â Zhang, â€œA Selective Federated
Reinforcement Learning Strategy for Autonomous Driving,â€ <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE
Transactions on Intelligent Transportation Systems</span>, pp.Â 1â€“14, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E.Â Bagdasaryan, A.Â Veit, Y.Â Hua, D.Â Estrin, and V.Â Shmatikov, â€œHow to backdoor
federated learning,â€ in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial
Intelligence and Statistics</span>, pp.Â 2938â€“2948, PMLR, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.Â Fang, X.Â Cao, J.Â Jia, and N.Â Gong, â€œLocal model poisoning attacks to
Byzantine-Robust federated learning,â€ in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">29th USENIX Security
Symposium (USENIX Security 20)</span>, pp.Â 1605â€“1622, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
G.Â A. Kaissis, M.Â R. Makowski, D.Â RÃ¼ckert, and R.Â F. Braren, â€œSecure,
Privacy-Preserving and Federated Machine Learning in Medical Imaging,â€ <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, vol.Â 2, pp.Â 305â€“311, June 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M.Â Singh and S.Â Kim, â€œBlockchain based intelligent vehicle data sharing
framework,â€ <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1708.09721</span>, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
G.Â Rathee, A.Â Sharma, R.Â Iqbal, M.Â Aloqaily, N.Â Jaglan, and R.Â Kumar, â€œA
blockchain framework for securing connected and autonomous vehicles,â€ <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol.Â 19, no.Â 14, p.Â 3165, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y.Â Fu, F.Â R. Yu, C.Â Li, T.Â H. Luan, and Y.Â Zhang, â€œVehicular
Blockchain-Based Collective Learning for Connected and Autonomous
Vehicles,â€ <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, vol.Â 27, pp.Â 197â€“203, Apr.
2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S.Â R. Pokhrel and J.Â Choi, â€œFederated Learning With Blockchain for
Autonomous Vehicles: Analysis and Design Challenges,â€ <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE
Transactions on Communications</span>, vol.Â 68, pp.Â 4734â€“4746, Aug. 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S.Â M. Basha, S.Â T. Ahmed, N.Â S.Â N. Iyengar, and R.Â D. Caytiles,
â€œInter-Locking Dependency Evaluation Schema based on Block-chain
Enabled Federated Transfer Learning for Autonomous Vehicular Systems,â€
in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">2021 Second International Conference on Innovative Technology
Convergence (CITC)</span>, (Sibalom, Philippines), pp.Â 46â€“51, IEEE, Dec.
2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y.Â He, K.Â Huang, G.Â Zhang, F.Â R. Yu, J.Â Chen, and J.Â Li, â€œBift: A
Blockchain-Based Federated Learning System for Connected and
Autonomous Vehicles,â€ <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, vol.Â 9,
pp.Â 12311â€“12322, July 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A.Â R. Javed, M.Â A. Hassan, F.Â Shahzad, W.Â Ahmed, S.Â Singh, T.Â Baker, and T.Â R.
Gadekallu, â€œIntegration of blockchain technology and federated learning in
vehicular (iot) networks: A comprehensive survey,â€ <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Sensors</span>,
vol.Â 22, no.Â 12, p.Â 4394, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A.Â Dorri, M.Â Steger, S.Â S. Kanhere, and R.Â Jurdak, â€œBlockChain: A
Distributed Solution to Automotive Security and Privacy,â€ <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Communications Magazine</span>, vol.Â 55, pp.Â 119â€“125, Dec. 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
T.Â Li, A.Â K. Sahu, M.Â Zaheer, M.Â Sanjabi, A.Â Talwalkar, and V.Â Smith,
â€œFederated Optimization in Heterogeneous Networks,â€ <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, vol.Â 2, pp.Â 429â€“450, Mar.
2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S.Â J. Reddi, Z.Â Charles, M.Â Zaheer, Z.Â Garrett, K.Â Rush, J.Â KoneÄnÃ½,
S.Â Kumar, and H.Â B. McMahan, â€œAdaptive Federated Optimization,â€ in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, Mar. 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T.Â Zeng, O.Â Semiari, M.Â Chen, W.Â Saad, and M.Â Bennis, â€œFederated Learning on
the Road Autonomous Controller Design for Connected and Autonomous
Vehicles,â€ <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Wireless Communications</span>, vol.Â 21,
no.Â 12, pp.Â 10407â€“10423, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
E.Â Jeong, S.Â Oh, H.Â Kim, J.Â Park, M.Â Bennis, and S.-L. Kim,
â€œCommunication-efficient on-device machine learning: Federated distillation
and augmentation under non-iid private data,â€ <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ArXiv</span>,
vol.Â abs/1811.11479, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
L.Â Liu, J.Â Zhang, S.Â H. Song, and K.Â B. Letaief, â€œCommunication-efficient
federated distillation with active data sampling,â€ in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">ICC 2022 - IEEE
International Conference on Communications</span>, pp.Â 201â€“206, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
D.Â A.Â E. Acar, Y.Â Zhao, R.Â Matas, M.Â Mattina, P.Â Whatmough, and V.Â Saligrama,
â€œFederated Learning Based on Dynamic Regularization,â€ in <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, Feb. 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
X.Â Li, K.Â Huang, W.Â Yang, S.Â Wang, and Z.Â Zhang, â€œOn the Convergence of
FedAvg on Non-IID Data,â€ in <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">International Conference on
Learning Representations</span>, Mar. 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J.Â Wang, Q.Â Liu, H.Â Liang, G.Â Joshi, and H.Â V. Poor, â€œTackling the objective
inconsistency problem in heterogeneous federated optimization,â€ <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol.Â 33, pp.Â 7611â€“7623,
2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J.Â Wang, Z.Â Charles, <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œA field guide to federated optimization,â€
<span id="bib.bib41.2.2" class="ltx_text ltx_font_italic">CoRR</span>, vol.Â abs/2107.06917, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S.Â Doomra, N.Â Kohli, and S.Â Athavale, â€œTurn Signal Prediction: A Federated
Learning Case Study,â€ <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol.Â abs/2012.12401, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
H.Â Zhang, J.Â Bosch, and H.Â H. Olsson, â€œEnd-to-End Federated Learning for
Autonomous Driving Vehicles,â€ in <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">2021 International Joint
Conference on Neural Networks (IJCNN)</span>, (Shenzhen, China),
pp.Â 1â€“8, IEEE, July 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
H.Â Zhang, J.Â Bosch, and H.Â H. Olsson, â€œReal-time End-to-End Federated
Learning: An Automotive Case Study,â€ in <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">2021 IEEE 45th
Annual Computers, Software, and Applications Conference
(COMPSAC)</span>, (Madrid, Spain), pp.Â 459â€“468, IEEE, July 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M.Â Aparna, R.Â Gandhiraj, and M.Â Panda, â€œSteering Angle Prediction for
Autonomous Driving using Federated Learning: The Impact of
Vehicle-To-Everything Communication,â€ in <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">2021 12th International
Conference on Computing Communication and Networking Technologies (ICCCNT)</span>,
pp.Â 1â€“7, IEEE, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
M.Â Pitropov, D.Â Garcia, J.Â Evan, Rebello, M.Â Smart, C.Â Wang, K.Â Czarnecki, and
S.Â Waslander, â€œCanadian Adverse Driving Conditions dataset,â€ <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">The
International Journal of Robotics Research</span>, vol.Â 40, no.Â 4-5, pp.Â 681â€“690,
2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
F.Â Yu, H.Â Chen, X.Â Wang, W.Â Xian, Y.Â Chen, F.Â Liu, V.Â Madhavan, and T.Â Darrell,
â€œBDD100k: A diverse driving dataset for heterogeneous multitask
learning,â€ in <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</span>, pp.Â 2636â€“2645, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S.Â Moosavi, B.Â Omidvar-Tehrani, and R.Â Ramnath, â€œTrajectory annotation by
discovering driving patterns,â€ in <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of the 3rd ACM SIGSPATIAL
Workshop on Smart Cities and Urban Analytics</span>, pp.Â 1â€“4, 2017.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J.Â Stallkamp, M.Â Schlipsing, J.Â Salmen, and C.Â Igel, â€œThe german traffic sign
recognition benchmark: a multi-class classification competition,â€ in <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">The 2011 International Joint Conference on Neural Networks</span>, pp.Â 1453â€“1460,
IEEE, 2011.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
M.Â Cordts, M.Â Omran, S.Â Ramos, T.Â Rehfeld, M.Â Enzweiler, R.Â Benenson,
U.Â Franke, S.Â Roth, and B.Â Schiele, â€œThe Cityscapes Dataset for
Semantic Urban Scene Understanding,â€ in <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</span>,
pp.Â 3213â€“3223, 2016.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
E.Â Alberti, A.Â Tavera, C.Â Masone, and B.Â Caputo, â€œIDDA: A large-scale
multi-domain dataset for autonomous driving,â€ <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and
Automation Letters</span>, vol.Â 5, no.Â 4, pp.Â 5526â€“5533, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
R.Â Kesten, M.Â Usman, J.Â Houston, T.Â Pandya, K.Â Nadhamuni, A.Â Ferreira, M.Â Yuan,
B.Â Low, A.Â Jain, P.Â Ondruska, <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œLyft level 5 av dataset 2019,â€
<span id="bib.bib52.2.2" class="ltx_text ltx_font_italic">urlhttps://level5. lyft. com/dataset</span>, vol.Â 1, p.Â 3, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
M.Â Han, K.Â Xu, S.Â Ma, A.Â Li, and H.Â Jiang, â€œFederated learning-based
trajectory prediction model with privacy preserving for intelligent
vehicle,â€ <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">International Journal of Intelligent Systems</span>, vol.Â 37,
no.Â 12, pp.Â 10861â€“10879, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
U.Â DOT, â€œNext generation simulation (NGSIM) vehicle trajectories and
supporting data,â€ <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">US Department of Transportation</span>, 2018.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
State Farm, â€œState farm distracted driver detection,â€ <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Kaggle</span>, Apr.
2016.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M.Â Naphade, S.Â Wang, D.Â C. Anastasiu, Z.Â Tang, M.Â Chang, Y.Â Yao, L.Â Zheng,
M.Â S. Rahman, A.Â Venkatachalapathy, A.Â Sharma, Q.Â Feng, V.Â Ablavsky,
S.Â Sclaroff, P.Â Chakraborty, A.Â Li, S.Â Li, and R.Â Chellappa, â€œThe 6th ai
city challenge,â€ in <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW)</span>, pp.Â 3346â€“3355, IEEE Computer
Society, June 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M.Â Martin, A.Â Roitberg, M.Â Haurilet, M.Â Horne, S.Â ReiÃŸ, M.Â Voit, and
R.Â Stiefelhagen, â€œDrive&amp;act: A multi-modal dataset for fine-grained driver
behavior recognition in autonomous vehicles,â€ in <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE/CVF International Conference on Computer Vision</span>, pp.Â 2801â€“2810, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
G.Â Pan, L.Â Sun, Z.Â Wu, and S.Â Lao, â€œEyeblink-based anti-spoofing in face
recognition from a generic webcamera,â€ in <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">2007 IEEE 11th International
Conference on Computer Vision</span>, pp.Â 1â€“8, IEEE, 2007.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
T.Â Drutarovsky and A.Â Fogelton, â€œEye blink detection using variance of motion
vectors.,â€ in <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">ECCV Workshops (3)</span>, pp.Â 436â€“448, 2014.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Q.Â Liu, Q.Â Guo, W.Â Wang, Y.Â Zhang, and Q.Â Kang, â€œAn automatic detection
algorithm of metro passenger boarding and alighting based on deep learning
and optical flow,â€ <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Instrum. Meas.</span>, vol.Â 70, pp.Â 1â€“13,
Jan. 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
â€œTable HM-12â€”Highway Statistics 2020â€”Policy Federal Highway
Administration.â€ https://www.fhwa.dot.gov/policyinformation
/statistics/2020/hm12.cfm.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
On-Road Automated Driving (ORAD) Committee, â€œTaxonomy and definitions for
terms related to driving automation systems for on-road motor vehicles,â€
June 2018.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
U.Â M. Gidado, H.Â Chiroma, N.Â Aljojo, S.Â Abubakar, S.Â I. Popoola, and M.Â A.
Al-Garadi, â€œA Survey on Deep Learning for Steering Angle
Prediction in Autonomous Vehicles,â€ <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol.Â 8,
pp.Â 163797â€“163817, 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
M.Â Bojarski, D.Â DelÂ Testa, D.Â Dworakowski, B.Â Firner, B.Â Flepp, P.Â Goyal, L.Â D.
Jackel, M.Â Monfort, U.Â Muller, J.Â Zhang, <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œEnd to end learning
for self-driving cars,â€ <span id="bib.bib64.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.07316</span>, 2016.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Y.Â Huang, J.Â Du, Z.Â Yang, Z.Â Zhou, L.Â Zhang, and H.Â Chen, â€œA Survey on
Trajectory-Prediction Methods for Autonomous Driving,â€ <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">IEEE
Transactions on Intelligent Vehicles</span>, vol.Â 7, pp.Â 652â€“674, Sept. 2022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
G.Â Rjoub, J.Â Bentahar, and O.Â A. Wahab, â€œExplainable AI-based Federated Deep
Reinforcement Learning for Trusted Autonomous Driving,â€ in <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">2022
International Wireless Communications and Mobile Computing
(IWCMC)</span>, pp.Â 318â€“323, May 2022.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
E.Â Arnold, O.Â Y. Al-Jarrah, M.Â Dianati, S.Â Fallah, D.Â Oxtoby, and
A.Â Mouzakitis, â€œA survey on 3d object detection methods for autonomous
driving applications,â€ <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation
Systems</span>, vol.Â 20, no.Â 10, pp.Â 3782â€“3795, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Y.Â Liu, A.Â Huang, Y.Â Luo, H.Â Huang, Y.Â Liu, Y.Â Chen, L.Â Feng, T.Â Chen, H.Â Yu,
and Q.Â Yang, â€œFedVision: An online visual object detection platform
powered by federated learning,â€ <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on
Artificial Intelligence</span>, vol.Â 34, pp.Â 13172â€“13179, Apr. 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
S.Â Wang, Y.Â Hong, R.Â Wang, Q.Â Hao, Y.-C. Wu, and D.Â W.Â K. Ng, â€œEdge federated
learning via unit-modulus over-the-air computation,â€ <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">IEEE Transactions
on Communications</span>, vol.Â 70, no.Â 5, pp.Â 3141â€“3156, 2022.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
S.Â Wang, C.Â Li, D.Â W.Â K. Ng, Y.Â C. Eldar, H.Â V. Poor, Q.Â Hao, and C.Â Xu,
â€œFederated Deep Learning Meets Autonomous Vehicle Perception: Design
and Verification,â€ <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">IEEE Network</span>, pp.Â 1â€“10, 2022.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Ã“.Â PÃ©rez-Gil, R.Â Barea, E.Â LÃ³pez-GuillÃ©n, L.Â M. Bergasa,
C.Â GÃ³mez-HuÃ©lamo, R.Â GutiÃ©rrez, and A.Â DÃ­az-DÃ­az,
â€œDeep reinforcement learning based control for Autonomous Vehicles in
CARLA,â€ <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, vol.Â 81,
pp.Â 3553â€“3576, Jan. 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
A.Â TaÃ¯k, Z.Â Mlika, and S.Â Cherkaoui, â€œClustered Vehicular Federated
Learning: Process and Optimization,â€ <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on
Intelligent Transportation Systems</span>, vol.Â 23, pp.Â 25371â€“25383, Dec. 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
S.Â Wang, F.Â Liu, and H.Â Xia, â€œContent-based Vehicle Selection and
Resource Allocation for Federated Learning in IoV,â€ in <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">2021
IEEE Wireless Communications and Networking Conference Workshops
(WCNCW)</span>, (Nanjing, China), pp.Â 1â€“7, IEEE, Mar. 2021.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Z.Â Tianqing, W.Â Zhou, D.Â Ye, Z.Â Cheng, and J.Â Li, â€œResource allocation in iot
edge computing via concurrent federated reinforcement learning,â€ <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">IEEE
Internet of Things Journal</span>, vol.Â 9, no.Â 2, pp.Â 1414â€“1426, 2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
R.Â Albelaihi, L.Â Yu, W.Â D. Craft, X.Â Sun, C.Â Wang, and R.Â Gazda, â€œGreen
Federated Learning via Energy-Aware Client Selection,â€ in <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">GLOBECOM 2022 - 2022 IEEE Global Communications Conference</span>,
pp.Â 13â€“18, Dec. 2022.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
T.Â Nishio and R.Â Yonetani, â€œClient Selection for Federated Learning
with Heterogeneous Resources in Mobile Edge,â€ in <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">ICC 2019 -
2019 IEEE International Conference on Communications (ICC)</span>,
pp.Â 1â€“7, May 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.10676" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.10677" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.10677">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.10677" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.10678" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 19:25:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
