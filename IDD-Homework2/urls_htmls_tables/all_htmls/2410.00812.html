<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A generative framework to bridge data-driven models and scientific theories in language neuroscience</title>
<!--Generated on Tue Oct  1 03:14:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.00812v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S2" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Language encoding models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S3" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Generating and evaluating natural-language explanations for single voxels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S4" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Explaining selectivity in regions of interest (ROIs)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S5" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Responses to synthetic stories reveal underlying contrast effects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Factors underlying effective GEM-V stories and voxels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S7" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S8" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1" title="In A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:173%;">A</span> </span><span class="ltx_text" style="font-size:173%;">Extended Data</span></span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Prompting details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1.SSS0.Px1" title="In A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title">Prompts used for explanation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1.SSS0.Px2" title="In A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title">Prompts used for story generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1.SSS0.Px3" title="In A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title">Prompts used for ROI-driving story generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS2" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Driving polysemantic voxels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS3" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Story-level breakdowns</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS4" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Correlates of driving performance at the voxel level</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS5" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Selectively driving pairs of voxels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS6" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Checkerboard driving</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS7" title="In Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7 </span>Semantic explanation average constrast maps<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Full set of flatmaps can also be found at <span class="ltx_ref ltx_href">github.com/microsoft/automated-explanations</span></span></span></span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\ListProperties</span>
<p class="ltx_p" id="p1.2">(Hide2=1,Hide3=2,Progressive*=.5cm,Numbers3=l, Numbers4=r,FinalMark3=))








</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\equalcont</span>
<p class="ltx_p" id="p2.2">These authors contributed equally to this work.</p>
</div>
<div class="ltx_para" id="p3">
<span class="ltx_ERROR undefined" id="p3.1">\equalcont</span>
<p class="ltx_p" id="p3.2">These authors contributed equally to this work.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">[4,5,6]<span class="ltx_ERROR undefined" id="p4.1.1">\fnm</span>Bin <span class="ltx_ERROR undefined" id="p4.1.2">\sur</span>Yu
<span class="ltx_ERROR undefined" id="p4.1.3">\equalcontsup</span>These authors jointly supervised this work.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">[1,7]<span class="ltx_ERROR undefined" id="p5.1.1">\fnm</span>Alexander <span class="ltx_ERROR undefined" id="p5.1.2">\sur</span>Huth
<span class="ltx_ERROR undefined" id="p5.1.3">\equalcontsup</span>These authors jointly supervised this work.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">1]<span class="ltx_ERROR undefined" id="p6.1.1">\orgdiv</span>Computer Science Department, <span class="ltx_ERROR undefined" id="p6.1.2">\orgname</span>University of Texas at Austin, <span class="ltx_ERROR undefined" id="p6.1.3">\state</span>TX, <span class="ltx_ERROR undefined" id="p6.1.4">\country</span>USA
2]<span class="ltx_ERROR undefined" id="p6.1.5">\orgname</span>Microsoft Research, <span class="ltx_ERROR undefined" id="p6.1.6">\city</span>Redmond, <span class="ltx_ERROR undefined" id="p6.1.7">\state</span>WA, <span class="ltx_ERROR undefined" id="p6.1.8">\country</span>USA
3]<span class="ltx_ERROR undefined" id="p6.1.9">\orgdiv</span>Neurosurgery Department, <span class="ltx_ERROR undefined" id="p6.1.10">\orgname</span>University of California, <span class="ltx_ERROR undefined" id="p6.1.11">\orgaddress</span><span class="ltx_ERROR undefined" id="p6.1.12">\city</span>San Francisco, <span class="ltx_ERROR undefined" id="p6.1.13">\state</span>CA, <span class="ltx_ERROR undefined" id="p6.1.14">\country</span>USA
4]<span class="ltx_ERROR undefined" id="p6.1.15">\orgdiv</span>EECS Department, <span class="ltx_ERROR undefined" id="p6.1.16">\orgname</span>University of California, <span class="ltx_ERROR undefined" id="p6.1.17">\orgaddress</span><span class="ltx_ERROR undefined" id="p6.1.18">\city</span>Berkeley, <span class="ltx_ERROR undefined" id="p6.1.19">\state</span>CA, <span class="ltx_ERROR undefined" id="p6.1.20">\country</span>USA
5]<span class="ltx_ERROR undefined" id="p6.1.21">\orgdiv</span>Statistics Department, <span class="ltx_ERROR undefined" id="p6.1.22">\orgname</span>University of California, <span class="ltx_ERROR undefined" id="p6.1.23">\orgaddress</span><span class="ltx_ERROR undefined" id="p6.1.24">\city</span>Berkeley, <span class="ltx_ERROR undefined" id="p6.1.25">\state</span>CA, <span class="ltx_ERROR undefined" id="p6.1.26">\country</span>USA
6]<span class="ltx_ERROR undefined" id="p6.1.27">\orgdiv</span>Center for Computational Biology, <span class="ltx_ERROR undefined" id="p6.1.28">\orgname</span>University of California, <span class="ltx_ERROR undefined" id="p6.1.29">\orgaddress</span><span class="ltx_ERROR undefined" id="p6.1.30">\city</span>Berkeley, <span class="ltx_ERROR undefined" id="p6.1.31">\state</span>CA, <span class="ltx_ERROR undefined" id="p6.1.32">\country</span>USA
7]<span class="ltx_ERROR undefined" id="p6.1.33">\orgdiv</span>Neuroscience Department, <span class="ltx_ERROR undefined" id="p6.1.34">\orgname</span>University of Texas at Austin, <span class="ltx_ERROR undefined" id="p6.1.35">\state</span>TX, <span class="ltx_ERROR undefined" id="p6.1.36">\country</span>USA
a]Work done while at UT Austin</p>
</div>
<h1 class="ltx_title ltx_title_document">A generative framework to bridge data-driven models and scientific theories in language neuroscience</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\fnm</span>Richard <span class="ltx_ERROR undefined" id="id2.2.id2">\sur</span>Antonello
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rjantonello@utexas.edu">rjantonello@utexas.edu</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id3.1.id1">\fnm</span>Chandan <span class="ltx_ERROR undefined" id="id4.2.id2">\sur</span>Singh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chansingh@microsoft.com">chansingh@microsoft.com</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id5.1.id1">\fnm</span>Shailee <span class="ltx_ERROR undefined" id="id6.2.id2">\sur</span>Jain
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:shailee.jain@ucsf.edu">shailee.jain@ucsf.edu</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id7.1.id1">\fnm</span>Aliyah <span class="ltx_ERROR undefined" id="id8.2.id2">\sur</span>Hsu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aliyahhsu@berkeley.edu">aliyahhsu@berkeley.edu</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id9.1.id1">\fnm</span>Jianfeng <span class="ltx_ERROR undefined" id="id10.2.id2">\sur</span>Gao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jfgao@microsoft.com">jfgao@microsoft.com</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:binyu@berkeley.edu">binyu@berkeley.edu</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:huth@cs.texas.edu">huth@cs.texas.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">Representations from large language models are highly effective at predicting BOLD fMRI responses to language stimuli.
However, these representations are largely opaque: it is unclear what features of the language stimulus drive the response in each brain area.
We present <span class="ltx_text ltx_framed ltx_framed_underline" id="id11.id1.1">g</span>enerative <span class="ltx_text ltx_framed ltx_framed_underline" id="id11.id1.2">e</span>xplanation-<span class="ltx_text ltx_framed ltx_framed_underline" id="id11.id1.3">m</span>ediated <span class="ltx_text ltx_framed ltx_framed_underline" id="id11.id1.4">v</span>alidation (GEM-V), a framework for generating concise explanations of language selectivity in the brain and then validating those explanations in follow-up experiments that use synthetic stimuli.
This approach is successful at explaining selectivity both in individual voxels and cortical regions of interest (ROIs).
We show that explanatory accuracy is closely related to the predictive power and stability of the underlying statistical models.
These results demonstrate that LLMs can be used to bridge the widening gap between data-driven models and formal scientific theories.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Language models, Encoding models, fMRI, Synthetic data
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section">1   Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Science faces an explainability crisis: data-driven deep learning methods are proving capable of modeling many natural phenomena, like protein folding, meteorological events, and computations in the brain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib3" title="">3</a>]</cite>. However, these models are not scientific theories that describe the world in natural language. Instead, they are implemented in the form of vast neural networks with millions or billions of largely inscrutable parameters. One emblematic field is language neuroscience, where large language models (LLMs) are highly effective at predicting human brain responses to natural language, but are virtually impossible to interpret or analyze by hand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To overcome this challenge, we introduce the <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p2.1.1">g</span>enerative <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p2.1.2">e</span>xplanation-<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p2.1.3">m</span>ediated <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p2.1.4">v</span>alidation (GEM-V) framework. GEM-V translates deep learning models of language selectivity in the brain into concise verbal explanations, and then designs follow-up experiments to verify that these explanations are causally related to brain activity.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center" id="S1.F1.1"><span class="ltx_text ltx_inline-block" id="S1.F1.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="558" id="S1.F1.1.1.g1" src="x1.png" width="996"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.29.1">Driving single-voxel response with generative explanation-mediated validation.</span>
(a) Voxelwise BOLD responses were recorded using fMRI as human subjects listened to 20 hours of narrative stories. An encoding model <math alttext="f" class="ltx_Math" display="inline" id="S1.F1.13.m1.1"><semantics id="S1.F1.13.m1.1b"><mi id="S1.F1.13.m1.1.1" xref="S1.F1.13.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.F1.13.m1.1c"><ci id="S1.F1.13.m1.1.1.cmml" xref="S1.F1.13.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.13.m1.1d">f</annotation><annotation encoding="application/x-llamapun" id="S1.F1.13.m1.1e">italic_f</annotation></semantics></math> was fit to predict these responses from the story text. <math alttext="f" class="ltx_Math" display="inline" id="S1.F1.14.m2.1"><semantics id="S1.F1.14.m2.1b"><mi id="S1.F1.14.m2.1.1" xref="S1.F1.14.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.F1.14.m2.1c"><ci id="S1.F1.14.m2.1.1.cmml" xref="S1.F1.14.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.14.m2.1d">f</annotation><annotation encoding="application/x-llamapun" id="S1.F1.14.m2.1e">italic_f</annotation></semantics></math> consists of a linear model fit on representations extracted from an LLM, which are not readily interpretable. Encoding models were tested by predicting responses on held-out fMRI data, and only well-performing models were selected for further analysis.
(b) We used an automated procedure to find a verbal description of the function that <math alttext="f" class="ltx_Math" display="inline" id="S1.F1.15.m3.1"><semantics id="S1.F1.15.m3.1b"><mi id="S1.F1.15.m3.1.1" xref="S1.F1.15.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.F1.15.m3.1c"><ci id="S1.F1.15.m3.1.1.cmml" xref="S1.F1.15.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.15.m3.1d">f</annotation><annotation encoding="application/x-llamapun" id="S1.F1.15.m3.1e">italic_f</annotation></semantics></math> computes for each voxel. First, we tested <math alttext="f" class="ltx_Math" display="inline" id="S1.F1.16.m4.1"><semantics id="S1.F1.16.m4.1b"><mi id="S1.F1.16.m4.1.1" xref="S1.F1.16.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.F1.16.m4.1c"><ci id="S1.F1.16.m4.1.1.cmml" xref="S1.F1.16.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.16.m4.1d">f</annotation><annotation encoding="application/x-llamapun" id="S1.F1.16.m4.1e">italic_f</annotation></semantics></math> on a large catalog of <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.17.m5.1"><semantics id="S1.F1.17.m5.1b"><mi id="S1.F1.17.m5.1.1" xref="S1.F1.17.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.17.m5.1c"><ci id="S1.F1.17.m5.1.1.cmml" xref="S1.F1.17.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.17.m5.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.17.m5.1e">italic_n</annotation></semantics></math>-grams (<math alttext="n=1,2,3)" class="ltx_math_unparsed" display="inline" id="S1.F1.18.m6.3"><semantics id="S1.F1.18.m6.3b"><mrow id="S1.F1.18.m6.3c"><mi id="S1.F1.18.m6.3.4">n</mi><mo id="S1.F1.18.m6.3.5">=</mo><mn id="S1.F1.18.m6.1.1">1</mn><mo id="S1.F1.18.m6.3.6">,</mo><mn id="S1.F1.18.m6.2.2">2</mn><mo id="S1.F1.18.m6.3.7">,</mo><mn id="S1.F1.18.m6.3.3">3</mn><mo id="S1.F1.18.m6.3.8" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S1.F1.18.m6.3d">n=1,2,3)</annotation><annotation encoding="application/x-llamapun" id="S1.F1.18.m6.3e">italic_n = 1 , 2 , 3 )</annotation></semantics></math> and found those that maximally drove predicted responses. These <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.19.m7.1"><semantics id="S1.F1.19.m7.1b"><mi id="S1.F1.19.m7.1.1" xref="S1.F1.19.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.19.m7.1c"><ci id="S1.F1.19.m7.1.1.cmml" xref="S1.F1.19.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.19.m7.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.19.m7.1e">italic_n</annotation></semantics></math>-grams were then summarized into stable explanation candidates using a powerful instruction-tuned LLM. Finally, we evaluated each explanation candidate by generating corresponding synthetic sentences and testing that these sentences yielded large predictions from <math alttext="f" class="ltx_Math" display="inline" id="S1.F1.20.m8.1"><semantics id="S1.F1.20.m8.1b"><mi id="S1.F1.20.m8.1.1" xref="S1.F1.20.m8.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.F1.20.m8.1c"><ci id="S1.F1.20.m8.1.1.cmml" xref="S1.F1.20.m8.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.20.m8.1d">f</annotation><annotation encoding="application/x-llamapun" id="S1.F1.20.m8.1e">italic_f</annotation></semantics></math>.
(c) To test whether the generated explanations were causally related to activation in the brain, we used an LLM to produce synthetic narrative stories where each paragraph is designed to drive responses based on the generated explanation for one voxel. For each subject we constructed stories to drive 17 well-modeled voxels with diverse selectivity. These stories were then presented to the subjects in a second fMRI experiment.
(d) Average BOLD response during its driving paragraph for each voxel, relative to baseline, i.e. average response to all non-driving paragraphs. On average, driven responses were significantly higher than baseline for each subject (<math alttext="p=0.020" class="ltx_Math" display="inline" id="S1.F1.21.m9.1"><semantics id="S1.F1.21.m9.1b"><mrow id="S1.F1.21.m9.1.1" xref="S1.F1.21.m9.1.1.cmml"><mi id="S1.F1.21.m9.1.1.2" xref="S1.F1.21.m9.1.1.2.cmml">p</mi><mo id="S1.F1.21.m9.1.1.1" xref="S1.F1.21.m9.1.1.1.cmml">=</mo><mn id="S1.F1.21.m9.1.1.3" xref="S1.F1.21.m9.1.1.3.cmml">0.020</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.21.m9.1c"><apply id="S1.F1.21.m9.1.1.cmml" xref="S1.F1.21.m9.1.1"><eq id="S1.F1.21.m9.1.1.1.cmml" xref="S1.F1.21.m9.1.1.1"></eq><ci id="S1.F1.21.m9.1.1.2.cmml" xref="S1.F1.21.m9.1.1.2">𝑝</ci><cn id="S1.F1.21.m9.1.1.3.cmml" type="float" xref="S1.F1.21.m9.1.1.3">0.020</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.21.m9.1d">p=0.020</annotation><annotation encoding="application/x-llamapun" id="S1.F1.21.m9.1e">italic_p = 0.020</annotation></semantics></math> (S01), <math alttext="p&lt;10^{-5}" class="ltx_Math" display="inline" id="S1.F1.22.m10.1"><semantics id="S1.F1.22.m10.1b"><mrow id="S1.F1.22.m10.1.1" xref="S1.F1.22.m10.1.1.cmml"><mi id="S1.F1.22.m10.1.1.2" xref="S1.F1.22.m10.1.1.2.cmml">p</mi><mo id="S1.F1.22.m10.1.1.1" xref="S1.F1.22.m10.1.1.1.cmml">&lt;</mo><msup id="S1.F1.22.m10.1.1.3" xref="S1.F1.22.m10.1.1.3.cmml"><mn id="S1.F1.22.m10.1.1.3.2" xref="S1.F1.22.m10.1.1.3.2.cmml">10</mn><mrow id="S1.F1.22.m10.1.1.3.3" xref="S1.F1.22.m10.1.1.3.3.cmml"><mo id="S1.F1.22.m10.1.1.3.3b" xref="S1.F1.22.m10.1.1.3.3.cmml">−</mo><mn id="S1.F1.22.m10.1.1.3.3.2" xref="S1.F1.22.m10.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.22.m10.1c"><apply id="S1.F1.22.m10.1.1.cmml" xref="S1.F1.22.m10.1.1"><lt id="S1.F1.22.m10.1.1.1.cmml" xref="S1.F1.22.m10.1.1.1"></lt><ci id="S1.F1.22.m10.1.1.2.cmml" xref="S1.F1.22.m10.1.1.2">𝑝</ci><apply id="S1.F1.22.m10.1.1.3.cmml" xref="S1.F1.22.m10.1.1.3"><csymbol cd="ambiguous" id="S1.F1.22.m10.1.1.3.1.cmml" xref="S1.F1.22.m10.1.1.3">superscript</csymbol><cn id="S1.F1.22.m10.1.1.3.2.cmml" type="integer" xref="S1.F1.22.m10.1.1.3.2">10</cn><apply id="S1.F1.22.m10.1.1.3.3.cmml" xref="S1.F1.22.m10.1.1.3.3"><minus id="S1.F1.22.m10.1.1.3.3.1.cmml" xref="S1.F1.22.m10.1.1.3.3"></minus><cn id="S1.F1.22.m10.1.1.3.3.2.cmml" type="integer" xref="S1.F1.22.m10.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.22.m10.1d">p&lt;10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.22.m10.1e">italic_p &lt; 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> (S02), <math alttext="p=0.009" class="ltx_Math" display="inline" id="S1.F1.23.m11.1"><semantics id="S1.F1.23.m11.1b"><mrow id="S1.F1.23.m11.1.1" xref="S1.F1.23.m11.1.1.cmml"><mi id="S1.F1.23.m11.1.1.2" xref="S1.F1.23.m11.1.1.2.cmml">p</mi><mo id="S1.F1.23.m11.1.1.1" xref="S1.F1.23.m11.1.1.1.cmml">=</mo><mn id="S1.F1.23.m11.1.1.3" xref="S1.F1.23.m11.1.1.3.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.23.m11.1c"><apply id="S1.F1.23.m11.1.1.cmml" xref="S1.F1.23.m11.1.1"><eq id="S1.F1.23.m11.1.1.1.cmml" xref="S1.F1.23.m11.1.1.1"></eq><ci id="S1.F1.23.m11.1.1.2.cmml" xref="S1.F1.23.m11.1.1.2">𝑝</ci><cn id="S1.F1.23.m11.1.1.3.cmml" type="float" xref="S1.F1.23.m11.1.1.3">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.23.m11.1d">p=0.009</annotation><annotation encoding="application/x-llamapun" id="S1.F1.23.m11.1e">italic_p = 0.009</annotation></semantics></math> (S03); permutation test, FDR-corrected).
For well-driven voxels, this means that the generated explanation is causally related to activation of that voxel, and thus that we have successfully translated the LLM-based encoding model into a verbal explanation.
(e) Average BOLD response for each selected voxel to each of the driving paragraphs in one subject (S02). Responses to the driving paragraph generated using the explanation for that voxel appear along the main diagonal. Explanations that were used to construct the driving paragraphs are shown below. BOLD responses were generally high for the driving paragraphs for each voxel as well as semantically related paragraphs (e.g. <span class="ltx_text ltx_font_italic" id="S1.F1.30.2">directions</span> and <span class="ltx_text ltx_font_italic" id="S1.F1.31.3">locations</span>, <span class="ltx_text ltx_font_italic" id="S1.F1.32.4">emotional expression</span> and <span class="ltx_text ltx_font_italic" id="S1.F1.33.5">laughter</span>).
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section">2   Language encoding models</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.2">We designed GEM-V to interpret <span class="ltx_text ltx_font_italic" id="S2.p1.2.1">language encoding models</span>, which are data-driven, LLM-based models of cortical language selectivity. We thus began by fitting encoding models (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>a) for each voxel in each of 3 human subjects using 20 hours of passive language listening fMRI data per subject collected in a prior study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib11" title="">11</a>]</cite>. The 20 hours of speech stimuli were transcribed and then passed through multiple open-source large language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib13" title="">13</a>]</cite> to obtain activation vectors for each word. Multiple models were built using different LLMs to ensure that predictions are stable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib15" title="">15</a>]</cite>, i.e. not idiosyncratic to a single model. Regularized linear regression was then used to predict the response timecourse of each voxel as a weighted combination of LLM activations.
Encoding models were tested by predicting responses on held out fMRI data and then computing the correlation between predicted and actual responses.
Encoding models using advanced LLMs achieve very high prediction performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib8" title="">8</a>]</cite>; here, only models with sufficient prediction performance (<math alttext="r&gt;0.15" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">r</mi><mo id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">0.15</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><gt id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></gt><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">𝑟</ci><cn id="S2.p1.1.m1.1.1.3.cmml" type="float" xref="S2.p1.1.m1.1.1.3">0.15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">r&gt;0.15</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_r &gt; 0.15</annotation></semantics></math>) were used in further analyses, with many voxels predicted at <math alttext="r&gt;0.7" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">r</mi><mo id="S2.p1.2.m2.1.1.1" xref="S2.p1.2.m2.1.1.1.cmml">&gt;</mo><mn id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><gt id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1"></gt><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">𝑟</ci><cn id="S2.p1.2.m2.1.1.3.cmml" type="float" xref="S2.p1.2.m2.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">r&gt;0.7</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_r &gt; 0.7</annotation></semantics></math>.
Full preprocessing, modeling, and data collection details are provided in the <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S8" title="8 Methods ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience">Methods</a>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section">3   Generating and evaluating natural-language explanations for single voxels</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Each encoding model represents the language selectivity of one part of the brain, but does so as a linear combination of LLM activations that are not human-interpretable.
The first step in GEM-V is to convert these models into concise natural language explanations: a word or short phrase summarizing the properties of language that drive responses most strongly.
This was accomplished by using an instruction-finetuned LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib16" title="">16</a>]</cite> to generate candidate explanations by summarizing the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">n</span>-grams that yield the largest predictions from the model (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>b) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib17" title="">17</a>]</cite>.
The resulting explanation is given in easily understandable natural language. For example, we found that some voxels appeared to be selective for language describing <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">Food preparation</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>b).
To ensure that the generated explanation correctly captures the function computed by the encoding model, we used the same LLM to generate synthetic text from the explanation and checked that this text could successfully drive the voxel encoding model.
However, this does not guarantee that this function is correctly aligned to the activity of the brain in the real world.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To fully close the loop and confirm that a generated explanation accurately reflects language selectivity in the brain, the second step of GEM-V is to automatically design a new neuroimaging experiment to test the generated explanation <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">in vivo</span>.
This was done by prompting an instruction-finetuned LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib16" title="">16</a>]</cite> to generate narratives that should selectively drive cortical activation based on that explanation (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>c).
If the explanation generated for a voxel is accurate, then text generated according to that explanation should elicit large responses in that voxel;
this would demonstrate that the explanation has causal influence over that voxel’s activity.
Given a selected set of voxels and their explanations, we constructed narrative stories by iteratively prompting an LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib16" title="">16</a>]</cite> to prioritize a different voxel’s explanation as the main focus of each paragraph. This allowed us to test whether voxels selectively respond to paragraphs that match their explanation.
The LLM ensures these stories remain coherent and engaging, helping to keep subjects attentive during the fMRI experiment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.3">We used GEM-V to generate and test explanations for 17 voxels with strong encoding model predictive performance in each of 3 subjects. The number of voxels was selected so that resulting narratives would be similar in length to the stimuli used to initially fit the encoding models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib11" title="">11</a>]</cite>.
We then measured the average response of each target voxel during the paragraph that was designed to drive it and compared this to the average response to other paragraphs (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>d).
Across voxels, responses to driving paragraphs were significantly greater than baseline responses for each subject (<math alttext="p=0.020" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">p</mi><mo id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">0.020</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><eq id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"></eq><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝑝</ci><cn id="S3.p3.1.m1.1.1.3.cmml" type="float" xref="S3.p3.1.m1.1.1.3">0.020</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">p=0.020</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_p = 0.020</annotation></semantics></math> (S01), <math alttext="p&lt;10^{-5}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">p</mi><mo id="S3.p3.2.m2.1.1.1" xref="S3.p3.2.m2.1.1.1.cmml">&lt;</mo><msup id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml"><mn id="S3.p3.2.m2.1.1.3.2" xref="S3.p3.2.m2.1.1.3.2.cmml">10</mn><mrow id="S3.p3.2.m2.1.1.3.3" xref="S3.p3.2.m2.1.1.3.3.cmml"><mo id="S3.p3.2.m2.1.1.3.3a" xref="S3.p3.2.m2.1.1.3.3.cmml">−</mo><mn id="S3.p3.2.m2.1.1.3.3.2" xref="S3.p3.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><lt id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1"></lt><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑝</ci><apply id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.3.1.cmml" xref="S3.p3.2.m2.1.1.3">superscript</csymbol><cn id="S3.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S3.p3.2.m2.1.1.3.2">10</cn><apply id="S3.p3.2.m2.1.1.3.3.cmml" xref="S3.p3.2.m2.1.1.3.3"><minus id="S3.p3.2.m2.1.1.3.3.1.cmml" xref="S3.p3.2.m2.1.1.3.3"></minus><cn id="S3.p3.2.m2.1.1.3.3.2.cmml" type="integer" xref="S3.p3.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">p&lt;10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_p &lt; 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> (S02), <math alttext="p=0.009" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mrow id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">p</mi><mo id="S3.p3.3.m3.1.1.1" xref="S3.p3.3.m3.1.1.1.cmml">=</mo><mn id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><eq id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1.1"></eq><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝑝</ci><cn id="S3.p3.3.m3.1.1.3.cmml" type="float" xref="S3.p3.3.m3.1.1.3">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">p=0.009</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_p = 0.009</annotation></semantics></math> (S03); permutation test with Benjamini-Hochberg false discovery rate correction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib19" title="">19</a>]</cite>).
Of the 51 tested voxels across the 3 subjects, 41 show increased response, with an average increase of 0.198 standard deviations over baseline.
Differences in effect size are related to head motion of subjects during test time and variation in sample size (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.T2" title="In A.3 Story-level breakdowns ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>).
This demonstrates that the generated explanations are mostly effective drivers of brain activity for their chosen voxels.
Further, because this experiment was not based directly on the LLM encoding model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib9" title="">9</a>]</cite> but was designed using the generated explanation, this suggests that we have successfully found explanations that are causally linked to brain activity.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The results in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>d compare responses during the driving paragraphs generated for each voxel to average responses over all other paragraphs. But if two voxels have very similar explanations and both are correct, then we would expect each to also be driven by the other’s generated stimuli. To explore this question we disaggregated the results by showing the response of each voxel to every driving paragraph in one subject (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>e). Most voxels are driven by their own explanations, as can be seen by the positive values on the main diagonal. However, many voxels are also strongly driven by related explanations (e.g. <span class="ltx_text ltx_font_italic" id="S3.p4.1.1">directions</span>, <span class="ltx_text ltx_font_italic" id="S3.p4.1.2">measurements</span>, and <span class="ltx_text ltx_font_italic" id="S3.p4.1.3">locations</span>; <span class="ltx_text ltx_font_italic" id="S3.p4.1.4">communication</span> and <span class="ltx_text ltx_font_italic" id="S3.p4.1.5">emotional expression</span>). This demonstrates that, in most cases, semantically related explanations will drive similar sets of voxels. We find that the same setting succeeds in driving pairs of voxels rather than individual voxels (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS5" title="A.5 Selectively driving pairs of voxels ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.5</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center" id="S3.F2.1"><span class="ltx_text ltx_inline-block" id="S3.F2.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="445" id="S3.F2.1.1.g1" src="x2.png" width="996"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span class="ltx_text ltx_font_bold" id="S3.F2.8.1">Driving ROI response with generative explanation-mediated validation for subject S02.
</span>
(a) Explanations were generated and used to drive 8 well-defined regions of interest.
Responses in all ROIs were significantly driven above baseline (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S3.F2.3.m1.1"><semantics id="S3.F2.3.m1.1b"><mrow id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml"><mi id="S3.F2.3.m1.1.1.2" xref="S3.F2.3.m1.1.1.2.cmml">p</mi><mo id="S3.F2.3.m1.1.1.1" xref="S3.F2.3.m1.1.1.1.cmml">&lt;</mo><mn id="S3.F2.3.m1.1.1.3" xref="S3.F2.3.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><apply id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1"><lt id="S3.F2.3.m1.1.1.1.cmml" xref="S3.F2.3.m1.1.1.1"></lt><ci id="S3.F2.3.m1.1.1.2.cmml" xref="S3.F2.3.m1.1.1.2">𝑝</ci><cn id="S3.F2.3.m1.1.1.3.cmml" type="float" xref="S3.F2.3.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.m1.1e">italic_p &lt; 0.05</annotation></semantics></math>; permutation test, FDR-corrected).
(b) To understand driving performance with more granularity, we color each voxel in each ROI by how well it was driven by its corresponding driving paragraph.
The resulting composite flatmap occasionally shows subregions within ROIs that are more selectively driven for a particular explanation.
(c) GEM-V can also be used to build more nuanced theories of cortical semantic selectivity.
We focused on three ROIs that are known to have similar selectivity for place concepts: retrosplenial cortex (RSC), the parahippocampal place area (PPA), and the occipital place area (OPA).
When explanations were generated for each ROI independently we found that each ROI was driven by all three driving paragraphs (left side).
To distinguish these ROIs, we used GEM-V to find new explanations and construct stories that would selectively drive each area while suppressing the other two.
Testing these stories in an fMRI experiment showed that we succeeded in finding selective explanations for two ROIs: RSC is selectively driven by <span class="ltx_text ltx_font_italic" id="S3.F2.9.2">location names</span> and PPA by <span class="ltx_text ltx_font_italic" id="S3.F2.10.3">unappetizing foods</span>. However, the explanation for OPA, <span class="ltx_text ltx_font_italic" id="S3.F2.11.4">spatial positioning &amp; directions</span>, drove responses in all three ROIs (right side).
(d) Visualization of the place area driving experiment with voxel-level granularity.
We show a 3-channel flatmap showing the outcome of each location-selective driving experiment;
a voxel is more red/green/blue if that voxel was driven by the corresponding ROI explanation.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section">4   Explaining selectivity in regions of interest (ROIs)</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Theories of language selectivity often revolve around regions of interest (ROIs) that are believed to be selective for specific semantic categories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib20" title="">20</a>]</cite>, such as locations (OPA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib22" title="">22</a>]</cite> or body parts (EBA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib24" title="">24</a>]</cite>. We explored whether GEM-V could be used to independently uncover the semantic selectivity of regions of interest, both for generating new hypotheses as well as re-establishing known functional selectivity.
We used GEM-V to find explanations and design synthetic driving stimuli for a diverse selection of individual ROIs that were localized using separate stimuli: the extrastriate body area (EBA), intraparietal sulcus (IPS), occipital face area (OFA), posterior superior temporal sulcus (pSTS), superior premotor ventral (sPMv), retrosplenial cortex (RSC), the parahippocampal place area (PPA), and the occipital place area (OPA). These ROIs were identified in each subject using separate localizer scans.
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S3.F2" title="In 3 Generating and evaluating natural-language explanations for single voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>a shows the generated explanations and average response above baseline for each ROI during each driving experiment. The explanations found by GEM-V and validated in the follow-up experiment are well-matched to known selectivity, e.g. <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Body parts</span> in EBA, and <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">Scenes and settings</span> in PPA.
Driving succeeded for every ROI (all <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><lt id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></lt><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.p1.1.m1.1.1.3.cmml" type="float" xref="S4.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>; permutation test with FDR correction), although for some ROIs, e.g. RSC, the effect is stronger.
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S3.F2" title="In 3 Generating and evaluating natural-language explanations for single voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>b visualizes driving responses for each ROI on a composite flatmap, where each voxel is colored on the basis of how much it was driven by its ROI’s explanation. This map suggests that the variability in driving between ROIs reflects functional heterogeneity within each region.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We next tested where GEM-V could be used to describe differences between regions that appear to have similar selectivity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib20" title="">20</a>]</cite>.
Examining three ROIs known to be selective for places or locations (RSC, PPA, OPA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib25" title="">25</a>]</cite>, we found that the driving paragraphs designed independently for each ROI also drove responses in the others (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S3.F2" title="In 3 Generating and evaluating natural-language explanations for single voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>c left).
To differentiate these ROIs, we used GEM-V to generate new explanations that should selectively drive each one of these regions while suppressing the other two <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib26" title="">26</a>]</cite>.
These selective explanations deviated somewhat from the original ROI explanations,
e.g. for RSC the explanation changed from <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">Travel and location names</span> to just <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">Location names</span>, suggesting that the <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">travel</span> part of the explanation is common across ROIs while <span class="ltx_text ltx_font_italic" id="S4.p2.1.4">location names</span> are more unique to RSC. New stories were generated to include only instances of the corresponding explanation, for instance, when driving only RSC, location names were included, while directions were specifically excluded (see prompts in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1" title="A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a>). Since these two categories commonly cooccur in natural language, these synthetic stimuli enabled us to examine differences that might not be present in typical naturalistic studies.
Testing these stories in another fMRI experiment showed that GEM-V was able to find explanations that can selectively drive two of the ROIs—<span class="ltx_text ltx_font_italic" id="S4.p2.1.5">Location names</span> for RSC and <span class="ltx_text ltx_font_italic" id="S4.p2.1.6">Unappetizing foods</span> for PPA—while the explanation of <span class="ltx_text ltx_font_italic" id="S4.p2.1.7">Spatial positioning &amp; directions</span> for OPA still drove responses in all three ROIs (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S3.F2" title="In 3 Generating and evaluating natural-language explanations for single voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>c right; <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S3.F2" title="In 3 Generating and evaluating natural-language explanations for single voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>d).
These results demonstrate that GEM-V can be used to build more nuanced theories of cortical semantic selectivity beyond general trends that are well-understood.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<p class="ltx_p ltx_align_center" id="S4.F3.1"><span class="ltx_text ltx_inline-block" id="S4.F3.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="469" id="S4.F3.1.1.g1" src="x3.png" width="996"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S4.F3.5.1">Responses to driving paragraphs reproduce known semantic contrasts and open new hypotheses</span>.
To understand how each explanation activates the entire cortex, we visualize the driven response (average normalized BOLD response) for six explanations.
(a) Some maps recovered well-established results, e.g. the explanation <span class="ltx_text ltx_font_italic" id="S4.F3.6.2">Locations</span> gave strong responses in the place areas RSC, OPA, and PPA.
(b) Other maps independently confirmed newer hypotheses, e.g. the explanation <span class="ltx_text ltx_font_italic" id="S4.F3.7.3">Food Preparation</span> activates a region in ventral occipital cortex near the fusiform face area (FFA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib28" title="">28</a>]</cite>.
(c) Finally, some explanations did not clearly map to any known hypotheses, but may suggest new directions for future research.
All observed activation patterns along with ROI localization explanations are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS7" title="A.7 Semantic explanation average constrast maps1footnote 1FootnoteFootnoteFootnotesFootnotes1footnote 1Full set of flatmaps can also be found at github.com/microsoft/automated-explanations ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">A.7</span></a>.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section">5   Responses to synthetic stories reveal underlying contrast effects</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Although the stories generated by GEM-V are designed to activate single voxels or ROIs, we can measure how much each explanation activates every part of cortex. These experiments can thus be seen as a hybrid between classical block-designs that are easy to analyze and natural stimulus experiments that elicit stronger and more widespread activation in cortex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib7" title="">7</a>]</cite>. To demonstrate this, we computed the average activation map for each explanation by averaging responses over timepoints within each paragraph. The results for six selected explanations in one subject are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S4.F3" title="In 4 Explaining selectivity in regions of interest (ROIs) ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Some explanations were similar to previously well-established contrasts (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S4.F3" title="In 4 Explaining selectivity in regions of interest (ROIs) ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>a) and gave the expected results. For example, the explanation <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">Locations</span> gave strong responses in the place areas RSC, OPA, and PPA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib25" title="">25</a>]</cite>.
Others confirmed newer hypotheses. For example, the <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">Food Preparation</span> explanation (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S4.F3" title="In 4 Explaining selectivity in regions of interest (ROIs) ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>b <span class="ltx_text ltx_font_italic" id="S5.p2.1.3">Top</span>) activates a region in ventral occipital cortex near the fusiform face area (FFA). Selectivity for this area to food was found recently in responses to images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib28" title="">28</a>]</cite>, but these results demonstrate for the first time that this selectivity also extends to linguistic descriptions of food.
Finally, some explanations did not clearly map to any known hypotheses (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S4.F3" title="In 4 Explaining selectivity in regions of interest (ROIs) ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>c), but still demonstrate suggestive patterns of activation that could be explored in future experiments. Maps for other explanations are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS7" title="A.7 Semantic explanation average constrast maps1footnote 1FootnoteFootnoteFootnotesFootnotes1footnote 1Full set of flatmaps can also be found at github.com/microsoft/automated-explanations ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">A.7</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section">6   Factors underlying effective GEM-V stories and voxels</h2>
<figure class="ltx_figure" id="S6.F4">
<p class="ltx_p ltx_align_center" id="S6.F4.1"><span class="ltx_text ltx_inline-block" id="S6.F4.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="592" id="S6.F4.1.1.g1" src="x4.png" width="963"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S6.F4.12.1">Analyzing factors that impact explanation-mediated validation.</span>
To evaluate whether GEM-V succeeds in generating effective stimulus stories, we assessed the driving paragraphs of the stimulus.
(a) To confirm that generated paragraphs match the explanation used to construct them, a matching score was computed for each explanation and paragraph by using an LLM to evaluate the fraction of trigrams in the paragraph that are relevant to the paragraph’s generating explanation and then z-scoring the result. Each driving paragraph showed a strong match with its generating explanation. Plot shows one subject (S02), similar plots for other subjects are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS4" title="A.4 Correlates of driving performance at the voxel level ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.4</span></a>.
(b) To confirm that each driving paragraph effectively drives its corresponding encoding model predictive performance, we computed the predicted response in each selected voxel to each generated paragraph. This revealed strong matches for most voxels, along with some matches between driving paragraphs and voxels with semantically similar explanations, e.g. <span class="ltx_text ltx_font_italic" id="S6.F4.13.2">directions</span> and <span class="ltx_text ltx_font_italic" id="S6.F4.14.3">locations</span>. Plot shows one subject (S02), similar plots for other subjects are shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS4" title="A.4 Correlates of driving performance at the voxel level ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.4</span></a>.
(c) After running the fMRI driving experiment, we found that a key factor determining whether a voxel is driven well by the GEM-V framework was the stability score for the voxel, i.e. the correlation between the <span class="ltx_text ltx_font_italic" id="S6.F4.15.4">n</span>-gram rankings provided by the LLaMA-based encoding model and the OPT-based encoding model.
(d) Another important factor for eliciting increased driving responses is the presence of key <span class="ltx_text ltx_font_italic" id="S6.F4.16.5">n</span>-grams in the driving paragraphs.
These <span class="ltx_text ltx_font_italic" id="S6.F4.17.6">n</span>-grams induce a standard hemodynamic response curve that peaks at around 6 seconds, yielding a significant increase (<math alttext="p=0.009" class="ltx_Math" display="inline" id="S6.F4.4.m1.1"><semantics id="S6.F4.4.m1.1b"><mrow id="S6.F4.4.m1.1.1" xref="S6.F4.4.m1.1.1.cmml"><mi id="S6.F4.4.m1.1.1.2" xref="S6.F4.4.m1.1.1.2.cmml">p</mi><mo id="S6.F4.4.m1.1.1.1" xref="S6.F4.4.m1.1.1.1.cmml">=</mo><mn id="S6.F4.4.m1.1.1.3" xref="S6.F4.4.m1.1.1.3.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F4.4.m1.1c"><apply id="S6.F4.4.m1.1.1.cmml" xref="S6.F4.4.m1.1.1"><eq id="S6.F4.4.m1.1.1.1.cmml" xref="S6.F4.4.m1.1.1.1"></eq><ci id="S6.F4.4.m1.1.1.2.cmml" xref="S6.F4.4.m1.1.1.2">𝑝</ci><cn id="S6.F4.4.m1.1.1.3.cmml" type="float" xref="S6.F4.4.m1.1.1.3">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.4.m1.1d">p=0.009</annotation><annotation encoding="application/x-llamapun" id="S6.F4.4.m1.1e">italic_p = 0.009</annotation></semantics></math>; one-sided t-test).
(e) Finally, to test whether the driving results were sensitive to the particular voxels that were selected, we evaluated whether the GEM-V stories drove alternative voxels in each subject that were assigned the same explanation as the target voxels being driven.
Both the targed voxels and alternative voxels also showed significantly increased driving responses (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S6.F4.5.m2.1"><semantics id="S6.F4.5.m2.1b"><mrow id="S6.F4.5.m2.1.1" xref="S6.F4.5.m2.1.1.cmml"><mi id="S6.F4.5.m2.1.1.2" xref="S6.F4.5.m2.1.1.2.cmml">p</mi><mo id="S6.F4.5.m2.1.1.1" xref="S6.F4.5.m2.1.1.1.cmml">&lt;</mo><mn id="S6.F4.5.m2.1.1.3" xref="S6.F4.5.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F4.5.m2.1c"><apply id="S6.F4.5.m2.1.1.cmml" xref="S6.F4.5.m2.1.1"><lt id="S6.F4.5.m2.1.1.1.cmml" xref="S6.F4.5.m2.1.1.1"></lt><ci id="S6.F4.5.m2.1.1.2.cmml" xref="S6.F4.5.m2.1.1.2">𝑝</ci><cn id="S6.F4.5.m2.1.1.3.cmml" type="float" xref="S6.F4.5.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.5.m2.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F4.5.m2.1e">italic_p &lt; 0.05</annotation></semantics></math>; permutation test, FDR-corrected).
</figcaption>
</figure>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The experiments thus far showed that GEM-V can effectively explain and drive a variety of locations throughout cortex, but fails to successfully drive some voxels.
To understand these limitations, we evaluated factors that influence driving performance, beginning with factors surrounding the generation of story stimuli.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">First, we considered whether driving failures may arise from a semantic mismatch between the proposed explanation and the generated story stimulus.
We measured the match between each driving paragraph and its target explanation for subject S02 by using an LLM to evaluate similarity between the explanation and trigrams in the driving paragraph (see <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S8" title="8 Methods ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience">Methods</a>).
There is a strong correspondence between each voxel explanation and its driving paragraph (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>a, orange diagonal), suggesting that LLM stimulus generation does not account for the failures.
Moreover, there is no clear correlation between these scores and driving success (average correlation of -0.05; see <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F9" title="In A.4 Correlates of driving performance at the voxel level ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>a).
Second, we tested whether the failed cases result from a misalignment between the original encoding model for each voxel and the generated story stimulus.
Again, each voxel’s encoding model showed an increased response for its driving paragraph (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>b, orange diagonal), suggesting the driving paragraphs are successfully aligned with the encoding model.
In this case, higher driving scores for the encoding model did yield higher driving scores in the follow-up experiment for 2 of the 3 subjects (average correlation of 0.19; see <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F9" title="In A.4 Correlates of driving performance at the voxel level ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>b).</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Given that driving failures are neither the result of stimulus-explanation mismatches nor stimulus-model mismatches, we concluded that they must largely stem from limitations in the encoding models.
We only applied GEM-V to voxels with high encoding model test performance, but this does not guarantee that the encoding model is accurate. Model performance is measured on a dataset that is limited in scope and that comprises real, noisy data. Both of these factors could cause a voxel to appear well-predicted despite having an inaccurate encoding model, or to appear poorly-predicted despite having an accurate encoding model.
Since these limitations cannot be addressed directly, we investigated factors that improve confidence in the generated explanations
by assessing their stability, a key principle underlying effective statistical interpretation according to the predictability, computability, and stability (PCS) framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib29" title="">29</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">To identify which explanations are stable, we defined a stability score that measures agreement between the predictions of two encoding models built from different LLMs (see <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S8" title="8 Methods ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience">Methods</a>) for the same voxel using the same fMRI data.
We found a strong positive correlation between the stability score of a voxel and its mean driving response across all three subjects (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>c), suggesting that the stability score reliably informs the causal reliability of an explanation. This result underscores the idea that models must be predictive and stable in order to effectively generate hypotheses for follow-up experiments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">We also assessed the stability of our framework to many modeling choices including
the prompts used to generate synthetic stories (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F7" title="In A.3 Story-level breakdowns ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>) and the granularity of units we drive (e.g. single voxels versus semantic clusters of voxels in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F8" title="In A.3 Story-level breakdowns ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>).
In both cases, we found that the magnitude of the evoked driving responses may change, but the overall trends in driving voxels are consistent.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">To further understand how driving paragraphs succeed, we measured whether the inclusion of key driving <span class="ltx_text ltx_font_italic" id="S6.p6.1.1">n</span>-grams extracted from the encoding model drove responses in a temporally specific fashion (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>d).
During the fMRI experiment, the presentation of these <span class="ltx_text ltx_font_italic" id="S6.p6.1.2">n</span>-grams evoked a significant increase in responses, peaking 6 seconds after presentation (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>d; <math alttext="p=0.009" class="ltx_Math" display="inline" id="S6.p6.1.m1.1"><semantics id="S6.p6.1.m1.1a"><mrow id="S6.p6.1.m1.1.1" xref="S6.p6.1.m1.1.1.cmml"><mi id="S6.p6.1.m1.1.1.2" xref="S6.p6.1.m1.1.1.2.cmml">p</mi><mo id="S6.p6.1.m1.1.1.1" xref="S6.p6.1.m1.1.1.1.cmml">=</mo><mn id="S6.p6.1.m1.1.1.3" xref="S6.p6.1.m1.1.1.3.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p6.1.m1.1b"><apply id="S6.p6.1.m1.1.1.cmml" xref="S6.p6.1.m1.1.1"><eq id="S6.p6.1.m1.1.1.1.cmml" xref="S6.p6.1.m1.1.1.1"></eq><ci id="S6.p6.1.m1.1.1.2.cmml" xref="S6.p6.1.m1.1.1.2">𝑝</ci><cn id="S6.p6.1.m1.1.1.3.cmml" type="float" xref="S6.p6.1.m1.1.1.3">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.1.m1.1c">p=0.009</annotation><annotation encoding="application/x-llamapun" id="S6.p6.1.m1.1d">italic_p = 0.009</annotation></semantics></math>; one-sided t-test).
This strongly aligns with hemodynamic response curves which tend to peak about six seconds after stimulus presentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib31" title="">31</a>]</cite>, and are further validation that the generated stimuli are the cause of the driving effects we observe.</p>
</div>
<div class="ltx_para" id="S6.p7">
<p class="ltx_p" id="S6.p7.3">Finally, we tested whether the GEM-V framework is sensitive to the particular voxels we selected for followup experiments.
To evaluate this,
we evaluated whether the driving paragraphs we generated also drove alternative voxels that were assigned the same explanation by GEM-V.
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>e shows the mean driving voxel responses for targeted voxels versus alternative voxels and finds that both are driven significantly (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S6.p7.1.m1.1"><semantics id="S6.p7.1.m1.1a"><mrow id="S6.p7.1.m1.1.1" xref="S6.p7.1.m1.1.1.cmml"><mi id="S6.p7.1.m1.1.1.2" xref="S6.p7.1.m1.1.1.2.cmml">p</mi><mo id="S6.p7.1.m1.1.1.1" xref="S6.p7.1.m1.1.1.1.cmml">&lt;</mo><mn id="S6.p7.1.m1.1.1.3" xref="S6.p7.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p7.1.m1.1b"><apply id="S6.p7.1.m1.1.1.cmml" xref="S6.p7.1.m1.1.1"><lt id="S6.p7.1.m1.1.1.1.cmml" xref="S6.p7.1.m1.1.1.1"></lt><ci id="S6.p7.1.m1.1.1.2.cmml" xref="S6.p7.1.m1.1.1.2">𝑝</ci><cn id="S6.p7.1.m1.1.1.3.cmml" type="float" xref="S6.p7.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S6.p7.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>; permutation test with FDR correction).
The alternative voxels are driven slightly less reliably (mean 0.14<math alttext="\sigma" class="ltx_Math" display="inline" id="S6.p7.2.m2.1"><semantics id="S6.p7.2.m2.1a"><mi id="S6.p7.2.m2.1.1" xref="S6.p7.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S6.p7.2.m2.1b"><ci id="S6.p7.2.m2.1.1.cmml" xref="S6.p7.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.2.m2.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S6.p7.2.m2.1d">italic_σ</annotation></semantics></math> vs 0.19<math alttext="\sigma" class="ltx_Math" display="inline" id="S6.p7.3.m3.1"><semantics id="S6.p7.3.m3.1a"><mi id="S6.p7.3.m3.1.1" xref="S6.p7.3.m3.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S6.p7.3.m3.1b"><ci id="S6.p7.3.m3.1.1.cmml" xref="S6.p7.3.m3.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.3.m3.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S6.p7.3.m3.1d">italic_σ</annotation></semantics></math>), likely because the prompts for generating the stories contain <span class="ltx_text ltx_font_italic" id="S6.p7.3.1">n</span>-grams that specify the explanation in slightly more detail than the explanation alone.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section">7   Discussion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Many prior studies have demonstrated the predictive power of black-box encoding models in language neuroscience across modalities such as fMRI, ECoG, EEG, and MEG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib8" title="">8</a>]</cite>, but these models have been criticized for their inability to produce accurate and reliable explanations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib34" title="">34</a>]</cite> and limited efforts have been made to explicitly interpret these models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib37" title="">37</a>]</cite>.
Our work takes the first steps in overcoming this criticism by enabling us to directly test the causality of an encoding model explanation
through the use of LLM-generated synthetic stimuli.
</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">This work is related to works that have generated stimuli to drive neurons in the visual cortex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib39" title="">39</a>]</cite>, as well as more recent work that has used decoding to provide insight into visual cortex selectivity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib40" title="">40</a>]</cite>.
In language fMRI, the closest work is <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib9" title="">Tuckute et al.</a></cite> <cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib9" title="">2023</a></cite>, which demonstrated the ability to drive the language network as a whole (rather than for individual voxels or ROIs) using non-generative sampling techniques as demonstrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F13" title="Figure 13 ‣ A.6 Checkerboard driving ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">13</span></a>. In comparison to these prior works, ours is the first to demonstrate driving at an explanatory level, not only identifying stimuli that can drive a particular voxel or ROI, but also hypothesizing the shared semantic features that cause the underlying activity.
The methodology here runs parallel to methods for causal interventions in mechanistic interpretability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib43" title="">43</a>]</cite>, which have been applied to understanding artificial neural networks.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Our work can be applied to generating a new class of stimuli for online, causal experiments of language processing. We consider this work a piece of a larger online feedback system that continually refines its understanding of brain selectivity by generating hypotheses, testing these hypotheses with new experiments, and refining the hypotheses based on the experimental results.
As an example, such a framework could be an especially effective tool in lesion case studies where general observations must be made efficiently from relatively little data.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">While effective,
GEM-V has several limitations.
Most notably, it focuses on a single explanation, missing voxel activity that is polysemantic, i.e. that is not driven well by any single explanation.
See <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS2" title="A.2 Driving polysemantic voxels ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a> for a more detailed discussion of this limitation and how future work might overcome it. Second, the effectiveness of GEM-V depends on staying within the manifold of stimuli trained on during encoding model training.
This limitation is explored further in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS6" title="A.6 Checkerboard driving ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">A.6</span></a> where we show that poorly generated, off-manifold stimuli are more difficult to predict and yield inferior driving outcomes.
GEM-V is also sensitive to the properties of the underlying encoding models–using different encoding models could yield different explanations which might be equally valid. Thus we must be cautious not to interpret the explanations given by GEM-V as uniquely capturing the functional properties of a voxel or ROI. However, this issue will become less severe over time as better and more diverse encoding models become available.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">The work here opens the door to a great deal of future research.
One avenue should explore whether the degree of granularity of the explanation that GEM-V generates is appropriate for cortical language selectivity.
For example, one of the explanations proposed by GEM-V is <span class="ltx_text ltx_font_italic" id="S7.p5.1.1">birthdays</span>, but it would be somewhat surprising to have a specifically birthday-selective area in human cortex. Rather, it is more plausible that such a voxel is selective for something more generic, such as social gatherings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib44" title="">44</a>]</cite>. Such ambiguities are of the type that are liable to be resolved by higher-fidelity modelling or iterative causal experiments, such as the hypothesis-refinement feedback system proposed above. GEM-V is also fundamentally limited by the spatiotemporal granularity of the scanning modality we test it on, fMRI. Applying GEM-V with faster or more fine-grained scanning modalities, such as 7T laminar fMRI, ECoG, or NeuroPixels, could reveal selectivity patterns that cannot be effectively observed through fMRI.</p>
</div>
<div class="ltx_para" id="S7.p6">
<p class="ltx_p" id="S7.p6.1">GEM-V demonstrates a general framework for aligning the data-driven predictions of models to the theory-oriented hypotheses of classical science. In this paper, we show that applying techniques from modern LLM interpretability research can improve our understanding of cognitive brain processes. Through simple prompting-based methods, we successfully recover semantic selectivity hypotheses that have previously been the providence of painstaking controlled experiments created over decades, and open the door to many more similar results. Our framework is extensible to other modalities such as vision and - requiring only naturalistic data from a single subject - can be easily adapted for use in case study analyses. Furthermore, GEM-V is data-driven and requires minimal human input, countering a frequent criticism of such theories as cherry-picked or driven by preexisting scientific biases.
GEM-V will help move toward a future with increasingly nuanced, operationalized, and falsifiable explanations in language neuroscience.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_font_bold ltx_title_section">8   Methods</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text ltx_font_bold" id="S8.p1.1.1">Data collection</span>
Two sets of MRI data are used in this study: the first for fitting encoding models and the second for selectively driving brain regions.
The original set is described and made openly available in previous work <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib45" title="">45</a>]</cite>;
we describe the details for the second newly collected set here.
functional magnetic resonance imaging (fMRI) data were collected from the same 3 human subjects as the original set as they listened to English-language podcast stories over Sensimetrics S14 headphones.
Subjects were not asked to make any responses, but simply to listen attentively to the stories.
All subjects were healthy and had normal hearing.
The experimental protocol was approved by the Institutional Review Board at the University of Texas at Austin.
Written informed consent was obtained from all subjects.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">All MRI data were collected on a 3T Siemens Skyra scanner at University of Texas at Austin using a 64-channel Siemens volume coil.
Functional scans were collected using a gradient echo EPI sequence with repetition time (TR) = 2.00 s, echo time (TE) = 30.8 ms, flip angle = 71°, multi-band factor (simultaneous multi-slice) = 2, voxel size = 2.6mm x 2.6mm x 2.6mm (slice thickness = 2.6mm), matrix size = 84x84, and field of view = 220 mm.
Anatomical data were collected using a T1-weighted multi-echo MP-RAGE sequence with voxel size = 1mm x 1mm x 1mm following the Freesurfer morphometry protocol  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib46" title="">46</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1"><span class="ltx_text ltx_font_bold" id="S8.p3.1.1">Data preprocessing</span>
All functional data were motion corrected using the FMRIB Linear Image Registration Tool (FLIRT) from FSL 5.0. FLIRT was used to align all data to a template that was made from the average across the first functional run in the first story session for each subject. These automatic alignments were manually checked for accuracy.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">Low frequency voxel response drift was identified using a 2nd order Savitzky-Golay filter with a 120 second window and then subtracted from the signal. To avoid onset artifacts and poor detrending performance near each end of the scan, responses were trimmed by removing 20 seconds (10 volumes) at the beginning and end of each scan, which removed the 10-second silent period and the first and last 10 seconds of each story. The mean response for each voxel was subtracted and the remaining response was scaled to have unit variance.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1"><span class="ltx_text ltx_font_bold" id="S8.p5.1.1">Voxelwise encoding models</span>
Encoding models were fit for each of the three subjects on listening data from roughly 20 hours of unique stories across 20 scanning sessions, yielding a total of <math alttext="\sim" class="ltx_Math" display="inline" id="S8.p5.1.m1.1"><semantics id="S8.p5.1.m1.1a"><mo id="S8.p5.1.m1.1.1" xref="S8.p5.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S8.p5.1.m1.1b"><csymbol cd="latexml" id="S8.p5.1.m1.1.1.cmml" xref="S8.p5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S8.p5.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S8.p5.1.m1.1d">∼</annotation></semantics></math>33,000 datapoints for each voxel across the whole brain.
For model testing, the subjects listened to the two test stories five times each, and one test story 10 times, at a rate of 1 test story per session. These test responses were averaged across repetitions.</p>
</div>
<div class="ltx_para" id="S8.p6">
<p class="ltx_p" id="S8.p6.1">For each subject and each voxel, we fit a separate encoding model to predict the BOLD response from the features we extract from the stimulus.
Features were extracted from the 18th layer of the 30-billion parameter LLaMA model <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib13" title="">13</a>]</cite>,
and the 33rd layer of the 30-billion parameter OPT model <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib12" title="">12</a>]</cite>. These layers were chosen based on prior work <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib8" title="">8</a>]</cite> that determined that they were the most performant at predicting brain activity.
To temporally align word times with TR times, Lanczos interpolation was applied with a window size of 3.
The hemodynamic response function was approximated with a finite impulse response model using 4 delays at -8,-6,-4 and -2 seconds <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S8.p7">
<p class="ltx_p" id="S8.p7.1">To evaluate the voxelwise encoding models, we used the learned encoding model to generate and evaluate predictions on a held-out test set.
The OPT features achieved a mean voxelwise correlation of about 0.128 whereas the LLaMA features achieved a mean voxelwise correlation of about 0.132.
These performances exceed that of previously published models on the same dataset (mean correlation 0.111) that were able to produce meaningful semantic decoding <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S8.p8">
<p class="ltx_p" id="S8.p8.1"><span class="ltx_text ltx_font_bold" id="S8.p8.1.1">Voxel selection for explanation</span>
We selected 500 well-modeled, diverse voxels to explain for each subject.
To ensure that these voxels were well-modeled, we selected only from voxels with a test correlation above 0.15, (this corresponds to the top 4̃0% most well-predicted voxels).
Then, we applied principal components analysis to the learned ridge weights across all well-modeled voxels in cortex. We project all voxels to the first four principal components, which are known to encode differences in semantic selectivity <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib20" title="">20</a>]</cite> and then uniformly sampled voxels from the within the convex hull in <math alttext="\mathcal{R}^{4}" class="ltx_Math" display="inline" id="S8.p8.1.m1.1"><semantics id="S8.p8.1.m1.1a"><msup id="S8.p8.1.m1.1.1" xref="S8.p8.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S8.p8.1.m1.1.1.2" xref="S8.p8.1.m1.1.1.2.cmml">ℛ</mi><mn id="S8.p8.1.m1.1.1.3" xref="S8.p8.1.m1.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S8.p8.1.m1.1b"><apply id="S8.p8.1.m1.1.1.cmml" xref="S8.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S8.p8.1.m1.1.1.1.cmml" xref="S8.p8.1.m1.1.1">superscript</csymbol><ci id="S8.p8.1.m1.1.1.2.cmml" xref="S8.p8.1.m1.1.1.2">ℛ</ci><cn id="S8.p8.1.m1.1.1.3.cmml" type="integer" xref="S8.p8.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p8.1.m1.1c">\mathcal{R}^{4}</annotation><annotation encoding="application/x-llamapun" id="S8.p8.1.m1.1d">caligraphic_R start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> induced by this projection. This uniform sampling ensures diversity in the selected voxels.
The mean voxel correlation for the 1,500 voxels we study is 0.35.</p>
</div>
<div class="ltx_para" id="S8.p9">
<p class="ltx_p" id="S8.p9.1"><span class="ltx_text ltx_font_bold" id="S8.p9.1.1">Framework for generating explanations</span>
GEM-V yields a short, natural-language explanation describing what elicits the strongest response from each of the 1,500 selected voxel encoding models.
To obtain this explanation, we follow the summarize and score (SASC) framework introduced in previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib17" title="">17</a>]</cite> (see <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>b).
SASC first generates candidate explanations (using GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib16" title="">16</a>]</cite>)
based on the <span class="ltx_text ltx_font_italic" id="S8.p9.1.2">n</span>-grams (<math alttext="n=1,2,3" class="ltx_Math" display="inline" id="S8.p9.1.m1.3"><semantics id="S8.p9.1.m1.3a"><mrow id="S8.p9.1.m1.3.4" xref="S8.p9.1.m1.3.4.cmml"><mi id="S8.p9.1.m1.3.4.2" xref="S8.p9.1.m1.3.4.2.cmml">n</mi><mo id="S8.p9.1.m1.3.4.1" xref="S8.p9.1.m1.3.4.1.cmml">=</mo><mrow id="S8.p9.1.m1.3.4.3.2" xref="S8.p9.1.m1.3.4.3.1.cmml"><mn id="S8.p9.1.m1.1.1" xref="S8.p9.1.m1.1.1.cmml">1</mn><mo id="S8.p9.1.m1.3.4.3.2.1" xref="S8.p9.1.m1.3.4.3.1.cmml">,</mo><mn id="S8.p9.1.m1.2.2" xref="S8.p9.1.m1.2.2.cmml">2</mn><mo id="S8.p9.1.m1.3.4.3.2.2" xref="S8.p9.1.m1.3.4.3.1.cmml">,</mo><mn id="S8.p9.1.m1.3.3" xref="S8.p9.1.m1.3.3.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S8.p9.1.m1.3b"><apply id="S8.p9.1.m1.3.4.cmml" xref="S8.p9.1.m1.3.4"><eq id="S8.p9.1.m1.3.4.1.cmml" xref="S8.p9.1.m1.3.4.1"></eq><ci id="S8.p9.1.m1.3.4.2.cmml" xref="S8.p9.1.m1.3.4.2">𝑛</ci><list id="S8.p9.1.m1.3.4.3.1.cmml" xref="S8.p9.1.m1.3.4.3.2"><cn id="S8.p9.1.m1.1.1.cmml" type="integer" xref="S8.p9.1.m1.1.1">1</cn><cn id="S8.p9.1.m1.2.2.cmml" type="integer" xref="S8.p9.1.m1.2.2">2</cn><cn id="S8.p9.1.m1.3.3.cmml" type="integer" xref="S8.p9.1.m1.3.3">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p9.1.m1.3c">n=1,2,3</annotation><annotation encoding="application/x-llamapun" id="S8.p9.1.m1.3d">italic_n = 1 , 2 , 3</annotation></semantics></math>) that elicit the most positive response from the LLaMA encoding model.
Each candidate explanation is then evaluated by generating synthetic data based on the explanation and testing the response of the encoding model to the data.</p>
</div>
<div class="ltx_para" id="S8.p10">
<p class="ltx_p" id="S8.p10.1"><span class="ltx_text ltx_font_bold" id="S8.p10.1.1">Stability score screening</span>
After extracting an explanation for 500 voxels per subject, we selected 17 voxels per subject for followup fMRI experiments.
17 voxels were selected so that the resulting 17-paragraph stories would be similar in length to the stimuli used to initially fit the encoding models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib11" title="">11</a>]</cite>.
To select voxels which are not only well-predicted but also likely to be well-explained, we introduced the <span class="ltx_text ltx_font_italic" id="S8.p10.1.2">stability score</span>,
which measures the correlation of predictions by the encoding models based on OPT and LLaMA for each unique <span class="ltx_text ltx_font_italic" id="S8.p10.1.3">n</span>-gram present in the dataset of stories.
A higher correlation implies greater agreement between the different encoding models and thus greater stability.
For each subject, we first filter the 40 voxels with the highest stability score and then manually select 17 voxels from this set that have diverse explanations.</p>
</div>
<div class="ltx_para" id="S8.p11">
<p class="ltx_p" id="S8.p11.1"><span class="ltx_text ltx_font_bold" id="S8.p11.1.1">Story generation (single-voxel setting)</span>
For follow-up experiments, we prompted a large LLM (GPT-4 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib16" title="">16</a>]</cite>) to generate stories based on the explanations for our selected voxels.
Stories are generated by repeatedly prompting the LLM in a chat to continue the story, one paragraph at a time.
For each paragraph, the LLM is asked to focus on one explanation and to include related key <span class="ltx_text ltx_font_italic" id="S8.p11.1.2">n</span>-grams (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>a, see full prompts in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1" title="A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a>).</p>
</div>
<div class="ltx_para" id="S8.p12">
<p class="ltx_p" id="S8.p12.1">Before running follow-up experiments, we checked that each paragraph’s text matched its generated explanation (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>a).
This match was measured by prompting an LLM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib47" title="">47</a>]</cite> to evaluate the fraction of trigrams in the paragraph that are relevant to the paragraph’s generating explanation.
We also validated that each story paragraph drives the encoding model for its corresponding voxel (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>b).
We generated 8 different stories by changing the random seed for each subject and kept the best 2 for S01, the best 6 for S02 (the pilot subject), and the best 2 for S03.</p>
</div>
<div class="ltx_para" id="S8.p13">
<p class="ltx_p" id="S8.p13.1"><span class="ltx_text ltx_font_bold" id="S8.p13.1.1">ROI setting</span>
The ROI setting uses the same framework as the single-voxel setting, but instead of maximizing the response in a single voxel, we sought to maximize the average response of the encoding model outputs over all voxels in an ROI.
When selecting explanations, we used the average outputs of the OPT and LLaMA encoding models to help make results more stable.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.1.1">
<span class="ltx_bibblock"><span class="ltx_ERROR undefined" id="bib.1.1.1.1">\bibcommenthead</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramson et al. [2024]</span>
<span class="ltx_bibblock">
Abramson, J.,
Adler, J.,
Dunger, J.,
Evans, R.,
Green, T.,
Pritzel, A.,
Ronneberger, O.,
Willmore, L.,
Ballard, A.J.,
Bambrick, J., et al.:
Accurate structure prediction of biomolecular interactions with alphafold 3.
Nature,
1–3
(2024)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kochkov et al. [2024]</span>
<span class="ltx_bibblock">
Kochkov, D.,
Yuval, J.,
Langmore, I., <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">et al.</span>:
Neural general circulation models for weather and climate.
Nature
(2024)
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/%****%20main.bbl%20Line%2075%20****10.1038/s41586-024-07744-y" title="">https://doi.org/%****␣main.bbl␣Line␣75␣****10.1038/s41586-024-07744-y</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamins and DiCarlo [2016]</span>
<span class="ltx_bibblock">
Yamins, D.L.,
DiCarlo, J.J.:
Using goal-driven deep learning models to understand sensory cortex.
Nature neuroscience
<span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">19</span>(3),
356–365
(2016)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abnar et al. [2019]</span>
<span class="ltx_bibblock">
Abnar, S.,
Beinborn, L.,
Choenni, R.,
Zuidema, W.H.:
Blackbox meets blackbox: Representational similarity and stability analysis of neural language models and brains.
CoRR
<span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">abs/1906.01539</span>
(2019)
<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1906.01539" title="">1906.01539</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein et al. [2022]</span>
<span class="ltx_bibblock">
Goldstein, A.,
Zada, Z.,
Buchnik, E.,
Schain, M.,
Price, A.,
Aubrey, B.,
Nastase, S.A.,
Feder, A.,
Emanuel, D.,
Cohen, A., <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">et al.</span>:
Shared computational principles for language processing in humans and deep language models.
Nature neuroscience
<span class="ltx_text ltx_font_bold" id="bib.bib5.2.2">25</span>(3),
369–380
(2022)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaidya et al. [2022]</span>
<span class="ltx_bibblock">
Vaidya, A.R.,
Jain, S.,
Huth, A.G.:
Self-supervised models of audio effectively explain human cortical responses to speech.
arXiv preprint arXiv:2205.14252
(2022)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. [2023]</span>
<span class="ltx_bibblock">
Jain, S.,
Vo, V.A.,
Wehbe, L.,
Huth, A.G.:
Computational language modeling and the promise of in silico experimentation.
Neurobiology of Language,
1–65
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonello et al. [2023]</span>
<span class="ltx_bibblock">
Antonello, R.,
Vaidya, A.,
Huth, A.G.:
Scaling laws for language encoding models in fMRI
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuckute et al. [2023]</span>
<span class="ltx_bibblock">
Tuckute, G.,
Sathe, A.,
Srikant, S.,
Taliaferro, M.,
Wang, M.,
Schrimpf, M.,
Kay, K.,
Fedorenko, E.:
Driving and suppressing the human language network using large language models.
bioRxiv
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. [2023]</span>
<span class="ltx_bibblock">
Oota, S.,
Gupta, M.,
Toneva, M.:
Joint processing of linguistic properties in brains and language models.
In: Oh, A.,
Naumann, T.,
Globerson, A.,
Saenko, K.,
Hardt, M.,
Levine, S. (eds.)
Advances in Neural Information Processing Systems,
vol. 36,
pp. 18001–18014.
Curran Associates, Inc., ???
(2023).
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/3a0e2de215bd17c39ad08ba1d16c1b12-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/3a0e2de215bd17c39ad08ba1d16c1b12-Paper-Conference.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeBel et al. [2022]</span>
<span class="ltx_bibblock">
LeBel, A.,
Wagner, L.,
Jain, S.,
Adhikari-Desai, A.,
Gupta, B.,
Morgenthal, A.,
Tang, J.,
Xu, L.,
Huth, A.G.:
A natural language fmri dataset for voxelwise encoding models.
bioRxiv,
2022–09
(2022)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022]</span>
<span class="ltx_bibblock">
Zhang, S.,
Roller, S.,
Goyal, N.,
Artetxe, M.,
Chen, M.,
Chen, S.,
Dewan, C.,
Diab, M.,
Li, X.,
Lin, X.V., et al.:
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068
(2022)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
Touvron, H.,
Lavril, T.,
Izacard, G.,
Martinet, X.,
Lachaux, M.-A.,
Lacroix, T.,
Rozière, B.,
Goyal, N.,
Hambro, E.,
Azhar, F., et al.:
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu and Kumbier [2020]</span>
<span class="ltx_bibblock">
Yu, B.,
Kumbier, K.:
Veridical data science.
Proceedings of the National Academy of Sciences
<span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">117</span>(8),
3920–3929
(2020)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbasi-Asl et al. [2018]</span>
<span class="ltx_bibblock">
Abbasi-Asl, R.,
Chen, Y.,
Bloniarz, A.,
Oliver, M.,
Willmore, B.D.,
Gallant, J.L.,
Yu, B.:
The deeptune framework for modeling and characterizing neurons in visual cortex area v4.
bioRxiv,
465534
(2018)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI:
GPT-4 System Card.
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/papers/gpt-4-system-card.pdf" title="">https://cdn.openai.com/papers/gpt-4-system-card.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2023]</span>
<span class="ltx_bibblock">
Singh, C.,
Hsu, A.R.,
Antonello, R.,
Jain, S.,
Huth, A.G.,
Yu, B.,
Gao, J.:
Explaining black box text modules in natural language with language models.
arXiv preprint arXiv:2305.09863
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton and Huth [2020]</span>
<span class="ltx_bibblock">
Hamilton, L.S.,
Huth, A.G.:
The revolution will not be controlled: natural stimuli in speech neuroscience.
Language, cognition and neuroscience
<span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">35</span>(5),
573–582
(2020)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benjamini and Hochberg [1995]</span>
<span class="ltx_bibblock">
Benjamini, Y.,
Hochberg, Y.:
Controlling the false discovery rate: a practical and powerful approach to multiple testing.
Journal of the Royal statistical society: series B (Methodological)
<span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">57</span>(1),
289–300
(1995)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huth et al. [2016]</span>
<span class="ltx_bibblock">
Huth, A.G.,
De Heer, W.A.,
Griffiths, T.L.,
Theunissen, F.E.,
Gallant, J.L.:
Natural speech reveals the semantic maps that tile human cerebral cortex.
Nature
<span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">532</span>(7600),
453–458
(2016)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Julian et al. [2016]</span>
<span class="ltx_bibblock">
Julian, J.B.,
Ryan, J.,
Hamilton, R.H.,
Epstein, R.A.:
The occipital place area is causally involved in representing environmental boundaries during navigation.
Current Biology
<span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">26</span>(8),
1104–1109
(2016)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamps et al. [2016]</span>
<span class="ltx_bibblock">
Kamps, F.S.,
Julian, J.B.,
Kubilius, J.,
Kanwisher, N.,
Dilks, D.D.:
The occipital place area represents the local elements of scenes.
Neuroimage
<span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">132</span>,
417–424
(2016)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor et al. [2007]</span>
<span class="ltx_bibblock">
Taylor, J.C.,
Wiggett, A.J.,
Downing, P.E.:
Functional mri analysis of body and body part representations in the extrastriate and fusiform body areas.
Journal of neurophysiology
<span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">98</span>(3),
1626–1633
(2007)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calvo-Merino et al. [2010]</span>
<span class="ltx_bibblock">
Calvo-Merino, B.,
Urgesi, C.,
Orgs, G.,
Aglioti, S.M.,
Haggard, P.:
Extrastriate body area underlies aesthetic evaluation of body stimuli.
Experimental brain research
<span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">204</span>,
447–456
(2010)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epstein and Baker [2019]</span>
<span class="ltx_bibblock">
Epstein, R.A.,
Baker, C.I.:
Scene perception in the human brain.
Annual review of vision science
<span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">5</span>,
373–397
(2019)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinkle and Lescroart [2023]</span>
<span class="ltx_bibblock">
Shinkle, M.,
Lescroart, M.:
Control of bold fmri responses via stimuli generated with voxel-weighted neural network activation maximization.
Journal of Vision
<span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">23</span>(9),
5834–5834
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. [2023]</span>
<span class="ltx_bibblock">
Jain, N.,
Wang, A.,
Henderson, M.M.,
Lin, R.,
Prince, J.S.,
Tarr, M.J.,
Wehbe, L.:
Selectivity for food in human ventral visual cortex.
Communications Biology
<span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">6</span>(1),
175
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla et al. [2022]</span>
<span class="ltx_bibblock">
Khosla, M.,
Murty, N.A.R.,
Kanwisher, N.:
A highly selective response to food in human visual cortex revealed by hypothesis-free voxel decomposition.
Current Biology
<span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">32</span>(19),
4159–4171
(2022)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu and Barter [2024]</span>
<span class="ltx_bibblock">
Yu, B.,
Barter, R.L.:
Veridical Data Science: The Practice of Responsible Data Analysis and Decision Making,
(2024)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Behr et al. [2024]</span>
<span class="ltx_bibblock">
Behr, M.,
Kumbier, K.,
Cordova-Palomera, A.,
Aguirre, M.,
Ronen, O.,
Ye, C.,
Ashley, E.,
Butte, A.J.,
Arnaout, R.,
Brown, B., <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">et al.</span>:
Learning epistatic polygenic phenotypes with boolean interactions.
Plos one
<span class="ltx_text ltx_font_bold" id="bib.bib30.2.2">19</span>(4),
0298906
(2024)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. [2002]</span>
<span class="ltx_bibblock">
Liao, C.H.,
Worsley, K.J.,
Poline, J.-B.,
Aston, J.A.,
Duncan, G.H.,
Evans, A.C.:
Estimating the delay of the fmri response.
NeuroImage
<span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">16</span>(3),
593–606
(2002)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain and Huth [2018]</span>
<span class="ltx_bibblock">
Jain, S.,
Huth, A.:
Incorporating context into language encoding models for fmri.
In: Bengio, S.,
Wallach, H.,
Larochelle, H.,
Grauman, K.,
Cesa-Bianchi, N.,
Garnett, R. (eds.)
Advances in Neural Information Processing Systems,
vol. 31.
Curran Associates, Inc., ???
(2018).
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2018/file/f471223d1a1614b58a7dc45c9d01df19-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2018/file/f471223d1a1614b58a7dc45c9d01df19-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva and Wehbe [2019]</span>
<span class="ltx_bibblock">
Toneva, M.,
Wehbe, L.:
Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain).
In: Wallach, H.,
Larochelle, H.,
Beygelzimer, A.,
Alché-Buc, F.,
Fox, E.,
Garnett, R. (eds.)
Advances in Neural Information Processing Systems,
vol. 32.
Curran Associates, Inc., ???
(2019).
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2019/file/749a8e6c231831ef7756db230b4359c8-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2019/file/749a8e6c231831ef7756db230b4359c8-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chirimuuta [2021]</span>
<span class="ltx_bibblock">
Chirimuuta, M.:
Prediction versus understanding in computationally enhanced neuroscience.
Synthese
<span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">199</span>(1),
767–790
(2021)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. [2020]</span>
<span class="ltx_bibblock">
Jain, S.,
Vo, V.,
Mahto, S.,
LeBel, A.,
Turek, J.S.,
Huth, A.:
Interpretable multi-timescale models for predicting fmri responses to continuous natural speech.
In: Larochelle, H.,
Ranzato, M.,
Hadsell, R.,
Balcan, M.F.,
Lin, H. (eds.)
Advances in Neural Information Processing Systems,
vol. 33,
pp. 13738–13749.
Curran Associates, Inc., ???
(2020).
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2020/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Chen, C.,
Tour, T.,
Gallant, J.,
Klein, D.,
Deniz, F.:
The cortical representation of language timescales is shared between reading and listening.
bioRxiv,
2023–01
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vo et al. [2023]</span>
<span class="ltx_bibblock">
Vo, V.A.,
Jain, S.,
Beckage, N.,
Chien, H.-Y.S.,
Obinwa, C.,
Huth, A.G.:
A unifying computational account of temporal context effects in language across the human cortex.
bioRxiv
(2023)
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1101/2023.08.03.551886" title="">https://doi.org/10.1101/2023.08.03.551886</a>
<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/https://www.biorxiv.org/content/early/2023/08/04/2023.08.03.551886.full.pdf" title="">https://www.biorxiv.org/content/early/2023/08/04/2023.08.03.551886.full.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bashivan et al. [2019]</span>
<span class="ltx_bibblock">
Bashivan, P.,
Kar, K.,
DiCarlo, J.J.:
Neural population control via deep image synthesis.
Science
<span class="ltx_text ltx_font_bold" id="bib.bib38.1.1">364</span>(6439),
9436
(2019)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murty et al. [2021]</span>
<span class="ltx_bibblock">
Murty, N.,
Bashivan, P.,
Abate, A.,
Dicarlo, J.,
Kanwisher, N.:
Computational models of category-selective brain regions enable high-throughput tests of selectivity.
Nature Communications
<span class="ltx_text ltx_font_bold" id="bib.bib39.1.1">12</span>,
5540
(2021)
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41467-021-25409-6" title="">https://doi.org/10.1038/s41467-021-25409-6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2024]</span>
<span class="ltx_bibblock">
Luo, A.,
Henderson, M.M.,
Tarr, M.J.,
Wehbe, L.:
BrainSCUBA: Fine-grained natural language captions of visual cortex selectivity.
In: The Twelfth International Conference on Learning Representations
(2024).
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=mQYHXUUTkU" title="">https://openreview.net/forum?id=mQYHXUUTkU</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
Wu, Z.,
D’Oosterlinck, K.,
Geiger, A.,
Zur, A.,
Potts, C.:
Causal proxy models for concept-based model explanations.
In: International Conference on Machine Learning,
pp. 37313–37334
(2023).
PMLR


</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feder et al. [2021]</span>
<span class="ltx_bibblock">
Feder, A.,
Oved, N.,
Shalit, U.,
Reichart, R.:
Causalm: Causal model explanation through counterfactual language models.
Computational Linguistics
<span class="ltx_text ltx_font_bold" id="bib.bib42.1.1">47</span>(2),
333–386
(2021)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elazar et al. [2021]</span>
<span class="ltx_bibblock">
Elazar, Y.,
Ravfogel, S.,
Jacovi, A.,
Goldberg, Y.:
Amnesic probing: Behavioral explanation with amnesic counterfactuals.
Transactions of the Association for Computational Linguistics
<span class="ltx_text ltx_font_bold" id="bib.bib43.1.1">9</span>,
160–175
(2021)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bedny et al. [2014]</span>
<span class="ltx_bibblock">
Bedny, M.,
Dravida, S.,
Saxe, R.:
Shindigs, brunches, and rodeos: The neural basis of event words.
Cognitive, Affective, &amp; Behavioral Neuroscience
<span class="ltx_text ltx_font_bold" id="bib.bib44.1.1">14</span>,
891–901
(2014)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023]</span>
<span class="ltx_bibblock">
Tang, J.,
LeBel, A.,
Jain, S.,
Huth, A.G.:
Semantic reconstruction of continuous language from non-invasive brain recordings.
Nature Neuroscience,
1–9
(2023)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fischl [2012]</span>
<span class="ltx_bibblock">
Fischl, B.:
Freesurfer.
Neuroimage
<span class="ltx_text ltx_font_bold" id="bib.bib46.1.1">62</span>(2),
774–781
(2012)


</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. [2022]</span>
<span class="ltx_bibblock">
Zhong, R.,
Snell, C.,
Klein, D.,
Steinhardt, J.:
Describing differences between text distributions with natural language.
In: International Conference on Machine Learning,
pp. 27099–27116
(2022).
PMLR


</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_bold ltx_title_section">Data availability</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">All newly collected fMRI data will be made publicly available upon acceptance.
Data for fitting encoding models and generating explanations is available on OpenNeuro: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openneuro.org/datasets/ds003020" title="">https://openneuro.org/datasets/ds003020</a>.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_font_bold ltx_title_section">Code availability</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Code for running all experiments (as well as applying GEM-V in new settings) is available on Github at <a class="ltx_ref ltx_href" href="https://github.com/microsoft/automated-explanations" title="">github.com/microsoft/automated-explanations</a>.
Code uses python 3.10,
huggingface transformers 4.29.088–100, sklearn 1.2.040, and OpenAI API gpt-4 (gpt-4-0613 and gpt-4-0125-preview).</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_font_bold ltx_title_section">Funding Information</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">We gratefully acknowledge support from NSF grants DMS-2413265, NSF grant 2023505 on Collaborative Research: Foundations of Data Science Institute (FODSI), the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and 814639, NSF grant MC2378 to the Institute for Artificial CyberThreat Intelligence and Opera- tioN (ACTION) and NSF grant 1R01DC020088-001. In addition to these sources, this work received further support from the Burroughs-Wellcome Foundation, the Dingwall Foundation, and a gift from Microsoft Research.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Extended Data</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Prompting details</h3>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Prompts used for explanation</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.1">Following the original SASC study <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#bib.bib17" title="">17</a>]</cite>, we use the following prompts for explanation.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p2.1">The summarization step summarizes 30 randomly chosen ngrams from the top 50 and generates 5 candidate explanations using the prompt <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p2.1.1">Here is a list of phrases:\n</span><span class="ltx_text" id="A1.SS1.SSS0.Px1.p2.1.2" style="color:#404040;">{<span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p2.1.2.1">phrases</span>}</span><span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p2.1.3">\nWhat is a common theme among these phrases?\nThe common theme among these phrases is <span class="ltx_text ltx_framed ltx_framed_underline" id="A1.SS1.SSS0.Px1.p2.1.3.1">     </span></span>.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p3.1">In the synthetic scoring step, we generate similar synthetic strings with the prompt <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p3.1.1">Generate 10 phrases that are similar to the concept of </span><span class="ltx_text" id="A1.SS1.SSS0.Px1.p3.1.2" style="color:#404040;">{<span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p3.1.2.1">explanation</span>}</span><span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p3.1.3">:</span>.
For dissimilar synthetic strings we use the prompt <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p3.1.4">Generate 10 phrases that are not similar to the concept of </span><span class="ltx_text" id="A1.SS1.SSS0.Px1.p3.1.5" style="color:#404040;">{<span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p3.1.5.1">explanation</span>}</span><span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px1.p3.1.6">:</span>.
Minor automatic processing is applied to LLM outputs, e.g. parsing a bulleted list, converting to lowercase, and removing extra whitespaces.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Prompts used for story generation</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">We use two variations of prompts.
In the main setting, we begin with <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px2.p1.1.1">Write the beginning paragraph of a long, coherent story. The story should be about ”expl”. Make sure it contains several words related to ”expl”, such as examples</span>.
Between paragraphs, it changes to <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px2.p1.1.2">Write the next paragraph of the story, staying consistent with the story so far, but now make it about…</span></p>
</div>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p2.1">In the second setting, we instead begin with <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS0.Px2.p2.1.1">Write the beginning paragraph of an interesting story told in first person. The story should have a plot and characters. The story should be about… and continue with Write the next paragraph of the story, but now make it about…</span></p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Prompts used for ROI-driving story generation</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px3.p1.1">For driving ROIs (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S4" title="4 Explaining selectivity in regions of interest (ROIs) ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>), many explanations are related to locations, and so we append a suffix to the prompt for reach paragraph (see <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.T1" title="In Prompts used for ROI-driving story generation ‣ A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>) to help make paragraphs more distinct.</p>
</div>
<figure class="ltx_table" id="A1.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Suffixes added to the prompt for generating the story paragraph for each ROI.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T1.3">
<tr class="ltx_tr" id="A1.T1.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T1.3.1.1"><span class="ltx_text ltx_font_bold" id="A1.T1.3.1.1.1" style="font-size:80%;">ROI</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T1.3.1.2"><span class="ltx_text ltx_font_bold" id="A1.T1.3.1.2.1" style="font-size:80%;">Driving explanation</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T1.3.1.3"><span class="ltx_text ltx_font_bold" id="A1.T1.3.1.3.1" style="font-size:80%;">Suffix</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.3.2.1"><span class="ltx_text" id="A1.T1.3.2.1.1" style="font-size:80%;">IPS</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.3.2.2"><span class="ltx_text" id="A1.T1.3.2.2.1" style="font-size:80%;">Descriptive elements of scenes or objects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.3.2.3"><span class="ltx_text" id="A1.T1.3.2.3.1" style="font-size:80%;">Avoid mentioning any locations.</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.3">
<td class="ltx_td ltx_align_left" id="A1.T1.3.3.1"><span class="ltx_text" id="A1.T1.3.3.1.1" style="font-size:80%;">OFA</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.3.2"><span class="ltx_text" id="A1.T1.3.3.2.1" style="font-size:80%;">Conversational transitions</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.3.3"><span class="ltx_text" id="A1.T1.3.3.3.1" style="font-size:80%;">Avoid mentioning any locations.</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.4">
<td class="ltx_td ltx_align_left" id="A1.T1.3.4.1"><span class="ltx_text" id="A1.T1.3.4.1.1" style="font-size:80%;">OPA</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.4.2"><span class="ltx_text" id="A1.T1.3.4.2.1" style="font-size:80%;">Direction and location descriptions</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.4.3"><span class="ltx_text" id="A1.T1.3.4.3.1" style="font-size:80%;">Avoid mentioning any specific location names (like ”New York” or ”Europe”).</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.5">
<td class="ltx_td ltx_align_left" id="A1.T1.3.5.1"><span class="ltx_text" id="A1.T1.3.5.1.1" style="font-size:80%;">OPA-only</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.5.2"><span class="ltx_text" id="A1.T1.3.5.2.1" style="font-size:80%;">Self-reflection and growth</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.5.3"><span class="ltx_text" id="A1.T1.3.5.3.1" style="font-size:80%;">Avoid mentioning any specific location names (like ”New York” or ”Europe”).</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.6">
<td class="ltx_td ltx_align_left" id="A1.T1.3.6.1"><span class="ltx_text" id="A1.T1.3.6.1.1" style="font-size:80%;">PPA</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.6.2"><span class="ltx_text" id="A1.T1.3.6.2.1" style="font-size:80%;">Scenes and settings</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.6.3"><span class="ltx_text" id="A1.T1.3.6.3.1" style="font-size:80%;">Avoid mentioning any specific location names (like ”New York” or ”Europe”).</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.7">
<td class="ltx_td ltx_align_left" id="A1.T1.3.7.1"><span class="ltx_text" id="A1.T1.3.7.1.1" style="font-size:80%;">PPA-only</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.7.2"><span class="ltx_text" id="A1.T1.3.7.2.1" style="font-size:80%;">Unappetizing foods</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.7.3"><span class="ltx_text" id="A1.T1.3.7.3.1" style="font-size:80%;">Avoid mentioning any specific location names (like ”New York” or ”Europe”).</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.8">
<td class="ltx_td ltx_align_left" id="A1.T1.3.8.1"><span class="ltx_text" id="A1.T1.3.8.1.1" style="font-size:80%;">RSC</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.8.2"><span class="ltx_text" id="A1.T1.3.8.2.1" style="font-size:80%;">Travel and location names</span></td>
<td class="ltx_td" id="A1.T1.3.8.3"></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.9">
<td class="ltx_td ltx_align_left" id="A1.T1.3.9.1"><span class="ltx_text" id="A1.T1.3.9.1.1" style="font-size:80%;">RSC-only</span></td>
<td class="ltx_td ltx_align_left" id="A1.T1.3.9.2"><span class="ltx_text" id="A1.T1.3.9.2.1" style="font-size:80%;">Location names</span></td>
<td class="ltx_td" id="A1.T1.3.9.3"></td>
</tr>
<tr class="ltx_tr" id="A1.T1.3.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.3.10.1"><span class="ltx_text" id="A1.T1.3.10.1.1" style="font-size:80%;">sPMv</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.3.10.2"><span class="ltx_text" id="A1.T1.3.10.2.1" style="font-size:80%;">Dialogue and responses</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.3.10.3"><span class="ltx_text" id="A1.T1.3.10.3.1" style="font-size:80%;">Avoid mentioning any locations.</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Driving polysemantic voxels</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We additionally run experiments seeking to drive multiple descriptions corresponding to the same voxel.
These experiments generate and test GEM-V stories in the same manner as testing individual explanations (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S1.F1" title="In 1 Introduction ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>c) but differ in the way they source voxel descriptions.
In our first investigation (with subject S02), we select two explanations that both come from SASC, in the case that its summaries of the top ngrams yield two distinct explanations.
In our second investigation (now with subject S03), we instead select two explanations that each come from a different encoding model.
In both cases, each story tests 8 voxels with two explanations each.
We average the results over two stories per subject.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">We generally find that we are able to only successfully drive one of the two explanations we seek to drive for an individual voxel (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F5" title="In A.2 Driving polysemantic voxels ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>).
We believe this is due to SASC’s overreliance on summarizing top-driving ngrams, and could potentially be mitigated by summarizing a broader range of encoding model inputs.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="A1.F5.g1" src="x5.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="A1.F5.2.1">Results for driving polysemantic voxels.</span>
(a) Voxel response for driving paragraphs (blue) show a small increase relative to the baseline responses of the remaining paragraphs (gray).
Each voxel appears as two points connected by a vertical line corresponding; each point shows the result when driving the voxel using a different explanation. Most voxels are only driven successfully for a single explanation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Story-level breakdowns</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">In this section, we show results for GEM-V at the level of individual stories.
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F6" title="In A.3 Story-level breakdowns ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the mean driving voxel response across all subjects and settings.
<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F7" title="In A.3 Story-level breakdowns ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a> stratifies these synthetic stories based on the underlying prompt used to generate the stories and <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F8" title="In A.3 Story-level breakdowns ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows results for these stories at driving voxel clusters rather than individual voxels.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="A1.F6.g1" src="x6.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Different stories vary in their ability to drive voxels.
There is large variance between the driving response produced for different stories, even for the same subject.
Driving response shows the difference from baseline paragraphs.
For ROI stories, this shows the average response of voxels in an ROI; for other stories this shows the average response of the single target voxel.
</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="A1.F7.g1" src="x7.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Different prompts yield different driving performance. The driving ability of the generated story differs based on the precise wording of the prompt. We compare two versions of the prompt, and find that results for version 1 are noticeably better.
Version 1 is our default prompt, described in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.SS1" title="A.1 Prompting details ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a>.
Version 0 instead begins with <span class="ltx_text ltx_font_italic" id="A1.F7.3.1">Write the beginning paragraph of an interesting story told in first person. The story should have a plot and characters. The story should be about..</span> and transitions between paragraphs with the prompt <span class="ltx_text ltx_font_italic" id="A1.F7.4.2">Write the next paragraph of the story, but now make it about…</span>.
Results show the 6 stories run for S02 in the default setting.
</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="A1.F8.g1" src="x8.png" width="456"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Clusters of voxels semantically related to the voxel cluster are driven, but less reliably than the target voxels themselves. Voxel clusters group semantically similar voxels by using the learned ridge regression weights of the encoding model.
Results show the 6 stories run for S02 in the default setting.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Head motion for different subjects during GEM-V stories.
S01 shows substantially larger motion than the other two subjects.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T2.1">
<tr class="ltx_tr" id="A1.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T2.1.1.1">Subject</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T2.1.1.2">
<span class="ltx_text" id="A1.T2.1.1.2.1"></span> <span class="ltx_text" id="A1.T2.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T2.1.1.2.2.1">
<span class="ltx_tr" id="A1.T2.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T2.1.1.2.2.1.1.1">Average framewise</span></span>
<span class="ltx_tr" id="A1.T2.1.1.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T2.1.1.2.2.1.2.1">displacement (mm)</span></span>
</span></span><span class="ltx_text" id="A1.T2.1.1.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T2.1.1.3">
<span class="ltx_text" id="A1.T2.1.1.3.1"></span> <span class="ltx_text" id="A1.T2.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T2.1.1.3.2.1">
<span class="ltx_tr" id="A1.T2.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T2.1.1.3.2.1.1.1">Fraction of TRs with framewise</span></span>
<span class="ltx_tr" id="A1.T2.1.1.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T2.1.1.3.2.1.2.1">displacement above 0.5 mm</span></span>
</span></span><span class="ltx_text" id="A1.T2.1.1.3.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.2.1">S01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.2.2">0.273</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.2.3">0.094</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.3">
<td class="ltx_td ltx_align_left" id="A1.T2.1.3.1">S02</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.3.2">0.108</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.3.3">0.000</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T2.1.4.1">S03</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.4.2">0.102</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.4.3">0.000</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Correlates of driving performance at the voxel level</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">In this section we provide additional analyses of what voxel-level factors inform whether driving performance will succeed.
The strongest correlate is the stability score, given in the main text (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>c).</p>
</div>
<figure class="ltx_figure" id="A1.F9">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.F9.2">
<tr class="ltx_tr" id="A1.F9.2.3">
<td class="ltx_td ltx_align_left" id="A1.F9.2.3.1"><span class="ltx_text ltx_font_bold" id="A1.F9.2.3.1.1">a</span></td>
<td class="ltx_td ltx_align_left" id="A1.F9.2.3.2"><span class="ltx_text ltx_font_bold" id="A1.F9.2.3.2.1">b</span></td>
</tr>
<tr class="ltx_tr" id="A1.F9.2.2">
<td class="ltx_td ltx_align_left" id="A1.F9.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="329" id="A1.F9.1.1.1.g1" src="x9.png" width="399"/></td>
<td class="ltx_td ltx_align_left" id="A1.F9.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="329" id="A1.F9.2.2.2.g1" src="x10.png" width="399"/></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
Correlates of driving performance at the voxel level.
(a) Ngram matching score, i.e. how well the driving paragraphs match their generating explanations, does not correlate well with driving score.
(b) Encoding driving score, i.e. how well the driving paragraphs succeed in driving the encoding model, correlates well with driving score for 2 of the 3 subjects.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.F10.2">
<tr class="ltx_tr" id="A1.F10.2.3">
<td class="ltx_td ltx_align_center" id="A1.F10.2.3.1">S01</td>
<td class="ltx_td ltx_align_center" id="A1.F10.2.3.2">S03</td>
</tr>
<tr class="ltx_tr" id="A1.F10.2.2">
<td class="ltx_td ltx_align_center" id="A1.F10.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="325" id="A1.F10.1.1.1.g1" src="x11.png" width="373"/></td>
<td class="ltx_td ltx_align_center" id="A1.F10.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="326" id="A1.F10.2.2.2.g1" src="x12.png" width="373"/></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>To confirm that generated paragraphs match the explanation used to construct them, a matching score was computed for each explanation and paragraph by using an LLM to evaluate the fraction of trigrams in the paragraph that are relevant to the paragraph’s generating explanation and then z-scoring the result. Each driving paragraph showed a strong match with its generating explanation. Plot shows subject S01 and S03; plot for subject S02 shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>a.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.F11.2">
<tr class="ltx_tr" id="A1.F11.2.3">
<td class="ltx_td ltx_align_center" id="A1.F11.2.3.1">        S01</td>
<td class="ltx_td ltx_align_center" id="A1.F11.2.3.2">             S03</td>
</tr>
<tr class="ltx_tr" id="A1.F11.2.2">
<td class="ltx_td ltx_align_center" id="A1.F11.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="325" id="A1.F11.1.1.1.g1" src="x13.png" width="374"/></td>
<td class="ltx_td ltx_align_center" id="A1.F11.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="326" id="A1.F11.2.2.2.g1" src="x14.png" width="374"/></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>To confirm that each driving paragraph effectively drives its corresponding encoding model, we computed the predicted response in each selected voxel to each generated paragraph. This revealed strong matches for most voxels, along with some matches between driving paragraphs and voxels with semantically similar explanations, e.g. <span class="ltx_text ltx_font_italic" id="A1.F11.5.1">directions</span> and <span class="ltx_text ltx_font_italic" id="A1.F11.6.2">locations</span>. Plot shows subject S01 and S03, plot for subject S02 shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#S6.F4" title="In 6 Factors underlying effective GEM-V stories and voxels ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>b.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Selectively driving pairs of voxels</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1">Useful and explanatory scientific theories are robust to unrelated distributional shifts. For example, if a region or voxel is selective for food concepts, then that region should remain selective for food concepts even if those concepts cooccur with unrelated semantic concepts. In order to test this independence of our generated explanations, we test whether GEM-V can be used to simultaneously drive pairs of voxels in different areas across the brain.</p>
</div>
<div class="ltx_para" id="A1.SS5.p2">
<p class="ltx_p" id="A1.SS5.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F12" title="In A.5 Selectively driving pairs of voxels ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">12</span></a>a shows the the multi-voxel driving pipeline. Pairs of voxels with semantically independent generated explanations are selected and stimuli are generated so that the stimuli have the following pattern:
(i) the first voxel is driven without respect to the second voxel by using the explanation for that voxel,
(ii) then both are driven simultaneously with a generated stimulus that contains both generated explanations,
and then (iii) only the second voxel is driven without respect to the first voxel. <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F12" title="In A.5 Selectively driving pairs of voxels ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">12</span></a>b demonstrates that we are able to effectively combine the voxel-level explanations to drive separate voxels at the same time, or independently drive one without driving the other.
This shows that the generated explanations are resilient to unrelated semantic interventions, demonstrating that they do not work merely because they tend to semantically co-occur with stimuli that actually drives those voxels.</p>
</div>
<figure class="ltx_figure" id="A1.F12">
<p class="ltx_p ltx_align_center" id="A1.F12.1"><span class="ltx_text ltx_inline-block" id="A1.F12.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="202" id="A1.F12.1.1.g1" src="x15.png" width="996"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>
<span class="ltx_text ltx_font_bold" id="A1.F12.3.1">LLM stories can selectively drive fMRI responses for pairs of voxels.</span>
(a) The pipeline for selectively driving a pair voxels generates explanations on a per voxel basis and prompts an LLM to generate paragraphs that either drive an individual voxel or a pair of voxels. (b) Voxel response when alternating between driving a single voxel (blue) or a pair of voxels (purple) generally succeeds in driving relative to the baseline (gray dotted line).
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Checkerboard driving</h3>
<figure class="ltx_figure" id="A1.F13">
<p class="ltx_p ltx_align_center" id="A1.F13.1"><span class="ltx_text ltx_inline-block" id="A1.F13.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="207" id="A1.F13.1.1.g1" src="x16.png" width="1079"/>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>(a) A suitable location for a target checkerboard pattern is chosen from prefrontal cortex by cross-referencing the target pattern with a pre-generated fMRI passive listening dataset.
(b) A subject listens to a story that elicits high variance for the checkerboard pattern.
We reconstruct the target pattern from the data by weighting the response at each timepoint by its predicted similarity to the target pattern.
The reconstruction partially recovers the target pattern, achieving reasonably high correlation with the target pattern.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS6.p1">
<p class="ltx_p" id="A1.SS6.p1.1">To test the limits of our voxel-driving paradigm, we further examine whether they can be used directly to produce arbitrary activation patterns if explanatory considerations were dropped. The experimental pipeline for this test is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F13" title="In A.6 Checkerboard driving ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">13</span></a>a.
We select a location on the cortical surface to drive by searching our training dataset for areas that contain at least one response from our training dataset with a high cosine similarity to a target checkerboard pattern. We choose a checkerboard pattern for this experiment to demonstrate that driving can be achieved even for relatively complex targets, so long as the target is biologically possible.</p>
</div>
<div class="ltx_para" id="A1.SS6.p2">
<p class="ltx_p" id="A1.SS6.p2.1">We demonstrate that we can partially manifest a checkerboard pattern in prefrontal cortex.
As the checkerboard pattern is fairly unnatural, we aim to reconstruct the checkerboard from the response pattern rather than directly drive it (<a class="ltx_ref" href="https://arxiv.org/html/2410.00812v1#A1.F13" title="In A.6 Checkerboard driving ‣ Appendix A Extended Data ‣ A generative framework to bridge data-driven models and scientific theories in language neuroscience"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">13</span></a>b).
We compute the sum of the responses for the voxels in the checkerboard location across a single high-variance story,
weighted by the cosine similarity of the responses to the target pattern.
This results in a pattern that bears some resemblance to the desired checkerboard (pearson correlation coefficient <math alttext="\rho=0.67" class="ltx_Math" display="inline" id="A1.SS6.p2.1.m1.1"><semantics id="A1.SS6.p2.1.m1.1a"><mrow id="A1.SS6.p2.1.m1.1.1" xref="A1.SS6.p2.1.m1.1.1.cmml"><mi id="A1.SS6.p2.1.m1.1.1.2" xref="A1.SS6.p2.1.m1.1.1.2.cmml">ρ</mi><mo id="A1.SS6.p2.1.m1.1.1.1" xref="A1.SS6.p2.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS6.p2.1.m1.1.1.3" xref="A1.SS6.p2.1.m1.1.1.3.cmml">0.67</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p2.1.m1.1b"><apply id="A1.SS6.p2.1.m1.1.1.cmml" xref="A1.SS6.p2.1.m1.1.1"><eq id="A1.SS6.p2.1.m1.1.1.1.cmml" xref="A1.SS6.p2.1.m1.1.1.1"></eq><ci id="A1.SS6.p2.1.m1.1.1.2.cmml" xref="A1.SS6.p2.1.m1.1.1.2">𝜌</ci><cn id="A1.SS6.p2.1.m1.1.1.3.cmml" type="float" xref="A1.SS6.p2.1.m1.1.1.3">0.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p2.1.m1.1c">\rho=0.67</annotation><annotation encoding="application/x-llamapun" id="A1.SS6.p2.1.m1.1d">italic_ρ = 0.67</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Semantic explanation average constrast maps<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Full set of flatmaps can also be found at <a class="ltx_ref ltx_href" href="https://github.com/microsoft/automated-explanations" title="">github.com/microsoft/automated-explanations</a></span></span></span>
</h3>
<figure class="ltx_figure" id="A1.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="657" id="A1.F14.g1" src="x17.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Explanation contrast for all tested explanations in S01. Arrows indicate source voxel where driving was attempted.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="903" id="A1.F15.g1" src="x18.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Explanation contrast for all tested explanations in S02. Arrows indicate source voxel where driving was attempted.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="824" id="A1.F16.g1" src="x19.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Explanation contrast for all tested explanations in S03. Arrows indicate source voxel where driving was attempted.</figcaption>
</figure>
</section>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 03:14:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
