<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.14086] Brain MRI Screening Tool with Federated Learning</title><meta property="og:description" content="In clinical practice, we often see significant delays between MRI scans and
the diagnosis made by radiologists, even for severe cases.
In some cases, this may be caused by the lack of additional information and
clues, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Brain MRI Screening Tool with Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Brain MRI Screening Tool with Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.14086">

<!--Generated on Tue Feb 27 17:28:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Brain MRI Screening Tool with Federated Learning</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">In clinical practice, we often see significant delays between MRI scans and
the diagnosis made by radiologists, even for severe cases.
In some cases, this may be caused by the lack of additional information and
clues, so even the severe cases need to wait in the queue for diagnosis.
This can be avoided if there is an automatic software tool, which would
supplement additional information, alerting radiologists that
the particular patient may be a severe case.
We are presenting an automatic brain MRI Screening Tool and we are demonstrating
its capabilities for detecting tumor-like pathologies.
It is the first version on the path toward a robust multi-pathology
screening solution. The tool supports Federated Learning,
so multiple institutions may contribute to the model without disclosing
their private data.
</span></p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
MRI, brain, tumor, screening, FL</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">At clinical routine, radiologists typically deal with an overwhelming amount of MRI exams,
often without prior knowledge of the case’s context. In the evaluation queue,
there are normal cases, abnormal, and often quite severe abnormal cases.
Typically, patients must wait for a delivery of a verified diagnosis significant time.
However, fast access to diagnosis means time-saving
or even life-saving therapeutic or pharmaceutical decisions.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In clinical practice, one can easily find the lack of uniform protocol for
examination prioritization.
Radiologists can judge the priority of diagnosis by several factors, e.g.,
the clinical state of the patient, previous evaluated imaging exams, patient history,
indications of the referral and the radiographer’s initial findings during the acquisition.
Unfortunately, these criteria are not always available, or they are barely applied
under intense workload conditions, so it happens pretty often that radiologists
take patients for diagnosis in chronological order.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The goal of our work is to develop a Screening Tool, software that would automatically
evaluate all brain MRI scans in a given hospital,
and which would produce pre-diagnostic reports for radiologists.
Based on such reports, radiologists could easily decide which examinations need to
be processed sooner and with higher priority, or, they might decide to
process the “easy cases” first (i.e., cases that can be completed quickly and easily),
to increase diagnostic throughput.
The ultimate goal is to help decrease the waiting time between the scan and the diagnosis,
especially for severe cases, by assisting radiologists to work more efficiently with better prioritization.
The pre-diagnostic report predicts whether the patient’s brain contains
pathological regions, and if so, how large these regions are and where they are located.
The report also presents important slices that support the tool’s prediction.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We used a commonly known machine learning (ML) approach with deep convolutional
neural networks to develop the Screening Tool.
To train ML models to a decent level of performance, a large number of training examples
are needed, accompanied by manual labels made by expert radiologists.
The problem is that the creation of manual labels is costly in terms of
required time and work done by experts in the field.
Also, if we want to develop ML models that would work for a wide range of clinics in practice,
we would ideally need examples from many institutions.
This is often very problematic since medical data are personal and strictly confidential.
Thus, we used Federated Learning to overcome both problems.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a technology that enables
the distributed training of ML models with a set of remote devices (nodes) without
sharing or disclosing the training data. The data remains securely stored within
individual nodes (e.g., hospitals),
and only the training updates and models are shared among nodes (inside the federation).
FL has been successfully demonstrated multiple times for brain MRI images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Thanks to the FL approach, multiple institutions can team up and form a federation where each institution can supply fewer samples.
The manual labels for a limited number of samples can be produced more easily by radiologists at those institutions,
and since they can be prepared at many institutions in parallel, the overall time until a suitable model can be trained is much less compared
to the situation where each institution would like to develop its own model.
Together with data privacy, this is one of the most significant advantages of the FL approach for practical applications.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">This paper presents our developed Screening Tool for brain MRI images, trained with Federated Learning,
demonstrating impressive results. Thanks to the autonomous and fully automatic processing, this tool
can be deployed in multiple institutions to process every brain MRI scan, producing clear
pre-diagnostic reports for radiologists. Generated screening results can be used to draw radiologists’
attention to cases that need to be diagnosed sooner and/or require more focus.
While the tool internally performs brain anomaly segmentation, the default presentation
for users is in the form of detections (bounding boxes) for the purpose of enhanced clarity.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Screening Tool</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Our aim is to develop a Screening Tool software, whose responsibility would be to
automatically process every brain scan from an MRI machine in a clinic and produce
screening report for radiologists. The reports should indicate if there are
any pathologies inside the patient’s brain, and if yes, to indicate their location,
size and visual appearance.
The software is intended to operate independently and to process the scans either
immediately after acquisition or in batches, e.g., over the night, to prepare a set
of reports before radiologists start their new working day.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The heart of the tool is the deep learning convolutional neural network.
For the currently presented version,
we decided to use U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> with Inception-v3
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as a backbone, utilizing pre-trained encoder weights.
The implementation can be found in the Segmentation Models library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
The model processes individual 2D axial slices separately.
We selected this smaller and simpler model for 2D processing to
decrease the computational requirements for the individual FL training nodes.
Models that process 3D volumes (e.g., nnU-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>) often have very
high GPU memory requirements, which may not be available in some medical institutions.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The whole data processing pipeline is composed of multiple steps.
First, required MRI modalities are selected from the MRI scan package.
We designed our current model to work with three modalities: FLAIR, T2, and T1ce.
T1ce can be substituted by standard T1 if the scan with contrast was not
performed for a particular patient.
Next, automatic skull-stripping and co-registration of all modalities are performed.
The registration produces only the transformation matrices that are used later
for resampling.
Next, the intensities of all modalities are normalized using the method that fits the
cumulative distribution function (CDF) of image intensities to a pre-computed template.
This approach is similar to histogram matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, but we allow
only the linear scaling and uniform shifting of the intensity values.
This restricted histogram matching method preserves local relationships of intensities
inside one image, while the overall distribution of intensities is very similar across individual
examinations. Each modality is normalized separately, and the
desired templates are derived from the training set.
Then, 2D axial slices are extracted from the modality with the highest axial resolution,
and are resampled to a common desired pixel resolution (0.72 mm/px in our case).
This normalizes depicted brain size across different examinations and different institutions.
Finally, the DL model predicts pathologies for every 2D slice independently by producing
the binary mask of detected pathology.
The last step is to combine all of these results together and to produce the report.
Then, the report is delivered to radiologists by a suitable method, e.g., either be saved to
an appropriate location, it can be, e.g., sent by email or delivered by any other means.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The report, as depicted in Fig <a href="#S2.F1" title="Figure 1 ‣ 2.1 Screening Tool ‣ 2 Materials and Methods ‣ Brain MRI Screening Tool with Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, contains three main information areas:
scores of detected pathologies along the axial and the sagittal axis,
and the main area, where the 18 most “interesting” axial slices are depicted.
The slides are displayed as RGB color images, mapping intensities from FLAIR,
T2 and T1ce modalities to red, green, and blue color channels, respectively.
The tool tries to select the most representative mix of abnormal and normal slices.
If some pathologies are detected, slices containing them are prioritized
for display, but the tool also tries to display some surrounding normal ones
for a better perception of the context.
Detected anomalies are highlighted by oriented bounding boxes, which are placed
around the detected object to avoid occluding the anomaly and its boundaries.
If the user prefers, the tool can also display precise segmentation boundaries
obtained from the U-Net model, as well as display sagittal or coronal preview slices
(not shown here).
</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2311.14086/assets/imgs/Bt-022-ScreeningResult.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">
Example of output from our Screening Tool. The radiologist is presented with abnormality scores along the axial and sagittal axis and the 18 most interesting slices that should support model findings.
Numbers displayed along the vertical Y axis correspond to depicted slides in the main display area, and the value represents the index of the axial slide. The GREEN boxes show what the Screening Tool predicted,
while REDs are derived from manual labels.
</span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">One of the key features of our solution is the support of Federated Learning.
We build the Screening Tool on top of CAFEIN FL Platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
developed at CERN. This platform is implemented with a focus on practical
applicability and usability in real-world situations, and is easily adaptable to any
domain or application.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The communication between the nodes is implemented through the MQTT protocol
(Message Queuing Telemetry Transport).
In this use-case, we chose the topology with Parameter Server, though other
fully-distributed and decentralized topologies are also possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
in the future.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The CAFEIN FL platform supports proper real-world network communication.
The federation, formed to train and evaluate the experiments described below,
was set up on multiple distinct physical computers distributed across Europe.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">For the development and demonstration of the Screening Tool, we used real clinical data
from two hospitals in two different countries:
Aiginiteio University Hospital in Athens, Greece, and University Hospital Brno, Czech Republic.
All examinations contain three modalities: FLAIR, T2, and T1ce (contrast-enhanced T1).</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">We decided to start with examinations that contained tumor-like anomalies
for this first version of the Screening Tool.
We included various types of tumors in the dataset,
namely High-Grade Glioma (HGG), Low-Grade Glioma (LGG), Lymfoma, Meningeoma, and metastatic tumors.
This is a significant difference between our dataset and other public datasets,
e.g. BraTS, which contains only HGG and LGG.
In total, our dataset contains 160 examinations, where most of the exams
were used for testing, and only the smaller part for training
(more details in Section <a href="#S3" title="3 Results and Discussion ‣ Brain MRI Screening Tool with Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experiment setup</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The experiment we conducted is a demonstration of how a Federated Learning
can boost the gains for institutions if they decide to join the federation
despite the fact that each institution contributes only a limited number of
training data.
</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In our setup, we devoted only 58 MRI examinations to the training dataset,
while the remaining 102 exams were left for validation and testing.
This division was chosen on purpose to demonstrate the vitality of FL training,
where each node (hospital) has just a limited number of annotated examples.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The 58 training exams were divided into 8 clients, each possessing
between 6 and 8 examples. The data from Athens and Brno were not
mixed together, so there were 4 FL clients backed up with Athens data,
and 4 clients with Brno data.
Since the clients had only a limited number of examinations, an extensive data
augmentation was used (affine and elastic transforms, flipping).
The augmentation ratio was set to 1:19, i.e., for every
original axial slice taken for the training, 19 transformed slices were generated.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The Parameter Server was responsible for orchestrating the training,
and for merging model updates from individual clients.
We used Federated Averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> in this experiment.
The server was also allowed to throttle the fast clients
in order to balance the overall number of “samples seen”
(i.e., the number of training inputs) between individual clients.
</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The model was trained during 200 FL rounds, while each round was set to last approx. 6 minutes.
The learning rate was progressively decreasing during the training.
In total, the final model was trained with approx. 13 mil. samples (slices) seen
(i.e., the sum of samples seen from all 8 nodes).</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">The model was then tested on 102 examinations. We are reporting on two main metrics:
i) the Exam-averaged Dice coefficient (EaD), and ii) the Global Dice (GD) coefficient.
The Exam-averaged Dice is derived as the average of Dice coefficients computed for
each MRI exam separately. This metric stresses out errors that occur inside
examinations with small anomalies, as these would inflict a higher penalty on the final score.
To counteract this bias, we report also
the Global Dice coefficient, which is computed over the whole volume of all
examinations together (i.e., as if all the examinations would be concatenated
into one huge 3D image).
The complete evaluation of a single MRI examination took approx. 9 seconds using
a rather old GTX 1080 GPU and Intel i7-8700K CPU.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">To understand better our achieved performance in a context, we compare the results
of our FL model with two alternative scenarios: i) the node would like to train its
own model using only its own limited data, and ii) the node would like to use
some available “off-the-shelf” model.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Validation results obtained on 102 MRI exams.</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Exam-avrg Dice</th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Glob. Dice</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<th id="S3.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">FL clients (average)</th>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.4.2.1.2.1" class="ltx_text ltx_font_bold">0.837</span></td>
<td id="S3.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.4.2.1.3.1" class="ltx_text ltx_font_bold">0.884</span></td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<th id="S3.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Single client (max)</th>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.744</td>
<td id="S3.T1.4.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.817</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<th id="S3.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Rixez (BraTS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.816</td>
<td id="S3.T1.4.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.883</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Results</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We evaluated our model on the complete set of 102 examinations, simulating that
these exams are the new scans at one of the nodes.
Since each node stores its locally-best model, the models across nodes may differ slightly.
Thus, we performed the evaluation on each of the 8 clients and took the average.
The Screening Tool trained using Federated Learning and all 8 clients
achieves EaD score of 0.84 and GD score of 0.89, see Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Experiment setup ‣ 3 Results and Discussion ‣ Brain MRI Screening Tool with Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We also evaluated the situation when each node would train
its own private model using only its small dataset.
The training protocol and parameters
were the same as in the FL training.
We tested each of the 8 models against the testing set,
but the best-achieved Dice scores are significantly lower than in the FL case:
reaching 0.74 for EaD and 0.82 for GD.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For the experiment with the off-the-shelf model, we took the winning submission
to the 2021 Brain Tumor Segmentation Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>,
which we denote as “Rixez model”.
We ran the prediction of the model and performed the same evaluation.
The achieved results of the Rixez model are slightly worse than ours for EaD,
and on par with ours for the GD metric.
It is also worth noting, that the processing time of the Rixez model
was significantly higher, with approx. 45 seconds per exam
on a much faster NVIDIA A100 GPU.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Discussion</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The results show that Federated Learning may be a very useful concept, how
different institutions may obtain useful machine learning model/tool, requiring
only a limited amount of annotated data contributed to the federation.
Since the amount of data required from any node is small, the whole federation
can benefit from a trained model in much less time compared to the situation,
when the institution would like to train its own model.
Federated Learning ensures the privacy of the data, so the institutions need not
to worry about collaborating with many partners.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Also, our results show that training a custom model still makes sense.
The off-the-shelf BraTS model, trained to a slightly different task,
works almost equally well for the tumor-like pathologies in our current dataset.
However, the off-the-shelf model might become a serious limitation in the near
future when we plan to extend the screening tool for various pathologies
(e.g., white matter hyper-intensity, multiple sclerosis, and possibly also brain strokes).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">As shown in Fig <a href="#S3.F2" title="Figure 2 ‣ 3.3 Discussion ‣ 3 Results and Discussion ‣ Brain MRI Screening Tool with Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our model works well for various tumor types.
However, as depicted in Fig <a href="#S3.F2.sf6" title="In Figure 2 ‣ 3.3 Discussion ‣ 3 Results and Discussion ‣ Brain MRI Screening Tool with Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(f)</span></a>, the model sometimes detects also
some other brain pathologies.
This is the correct output for the practical use of the Screening Tool,
but currently, it decreases our Dice scores since these types of pathologies are
not marked by manual labels.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.14086/assets/imgs/results-samples/HGG_Bt-014.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="110" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">HGG</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.14086/assets/imgs/results-samples/LGG_Bt-041.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="107" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">LGG</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.14086/assets/imgs/results-samples/Lymfoma_Bt-132.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="112" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Lymfoma</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.14086/assets/imgs/results-samples/Meta_Bt-028.png" id="S3.F2.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="110" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F2.sf4.3.2" class="ltx_text" style="font-size:90%;">Metastatic</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.14086/assets/imgs/results-samples/Meningeom_Bt-009.png" id="S3.F2.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="110" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S3.F2.sf5.3.2" class="ltx_text" style="font-size:90%;">Meningeoma</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.14086/assets/imgs/results-samples/FP-WMH_Bt-096.png" id="S3.F2.sf6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="110" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S3.F2.sf6.3.2" class="ltx_text" style="font-size:90%;">WMH</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">
Examples of detections for various tumor types. GREEN rectangles represent Screening Tool prediction,
REDs are derived from manual labels, and BLUE color is where they overlap.
As shown in Fig. <a href="#S3.F2.sf6" title="In Figure 2 ‣ 3.3 Discussion ‣ 3 Results and Discussion ‣ Brain MRI Screening Tool with Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(f)</span></a>, the model sometimes
detects also other brain pathologies, here White matter hyperintensities (WMH),
which were not manually labeled but are considered as correct detections
from the radiological perspective.
</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We presented our automated brain MRI Screening Tool,
trained with federated learning with 8 clients,
which has been tested and validated for tumor-like pathologies.
The tool exhibits very good sensitivity and specificity, which gives
promises for practical usefulness for practical diagnosis prioritization
once the tool is deployed for real clinical use.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In the near future, we are aiming to enrich the model to be capable of
detecting reliably multiple brain pathologies (e.g., white matter
hyperintensities, multiple sclerosis, brain strokes, and others).
Also, we plan to evaluate the effect of this tool on diagnosis prioritization
in real clinical practice and its impacts on medical care improvement.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jakub Konečný, Brendan McMahan, and Daniel Ramage,

</span>
<span class="ltx_bibblock">“Federated optimization: Distributed optimization beyond the datacenter,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.03575</span>, 11 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. B. McMahan, Eider Moore, D. Ramage, and B. A. Y. Arcas,

</span>
<span class="ltx_bibblock">“Federated learning of deep networks using model averaging,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, vol. abs/1602.05629, 2 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bernardo Camajori Tedeschini, Stefano Savazzi, Roman Stoklasa, Luca Barbieri, Ioannis Stathopoulos, Monica Nicoli, and Luigi Serio,

</span>
<span class="ltx_bibblock">“Decentralized federated learning for healthcare networks: A case study on tumor segmentation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 10, pp. 8693–8708, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Sarthak Pati, Ujjwal Baid, Brandon Edwards, Micah Sheller, Shih-Han Wang, G Anthony Reina, Patrick Foley, Alexey Gruzdev, Deepthi Karkada, Christos Davatzikos, et al.,

</span>
<span class="ltx_bibblock">“Federated learning enables big data for rare cancer boundary detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Nature communications</span>, vol. 13, no. 1, pp. 7346, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox,

</span>
<span class="ltx_bibblock">“U-net: Convolutional networks for biomedical image segmentation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</span>, Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi, Eds. 11 2015, pp. 234–241, Springer International Publishing.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna,

</span>
<span class="ltx_bibblock">“Rethinking the inception architecture for computer vision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 6 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Pavel Iakubovskii,

</span>
<span class="ltx_bibblock">“Segmentation models,” <a target="_blank" href="https://github.com/qubvel/segmentation_models" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/qubvel/segmentation_models</a>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein,

</span>
<span class="ltx_bibblock">“nnu-net: a self-configuring method for deep learning-based biomedical image segmentation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Nature methods</span>, vol. 18, no. 2, pp. 203–211, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Richard E Woods and Rafael C Gonzalez,

</span>
<span class="ltx_bibblock">“Digital image processing,” 2008.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
CERN Knowledge Transfer,

</span>
<span class="ltx_bibblock">“Cafein - federated network platform for the development and deployment ai-based analysis,” https://kt.cern/kt-fund/projects/cafein-federated-network-platform-development-and-deployment-ai-based-analysis-and, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Huan Minh Luu and Sung-Hong Park,

</span>
<span class="ltx_bibblock">“Extending nn-unet for brain tumor segmentation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</span>, Cham, 2022, pp. 173–186, Springer International Publishing.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Huan Minh Luu and Sung-Hong Park,

</span>
<span class="ltx_bibblock">“Winning submission to the 2021 brain tumor segmentation challenge,” <a target="_blank" href="https://github.com/rixez/Brats21_KAIST_MRI_Lab" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rixez/Brats21_KAIST_MRI_Lab</a>, 2023.

</span>
</li>
</ul>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Compliance with ethical standards</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study was performed in line with the principles of the Declaration of Helsinki.
Approval was granted by the Ethics Committee of University Hospital Brno.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.14085" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.14086" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.14086">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.14086" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.14088" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 17:28:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
