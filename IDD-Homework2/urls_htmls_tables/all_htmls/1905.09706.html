<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1905.09706] Watermark retrieval from 3D printed objects via synthetic data training</title><meta property="og:description" content="We present a deep neural network based method for the retrieval of watermarks from images of 3D printed objects. To deal with the variability of all possible 3D printing and image acquisition settings we train the netw…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Watermark retrieval from 3D printed objects via synthetic data training">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Watermark retrieval from 3D printed objects via synthetic data training">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1905.09706">

<!--Generated on Tue Mar 19 17:30:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Watermark retrieval from 3D printed objects via synthetic data training</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id1.1.id1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:469.8pt;">
<span id="id1.1.id1.1" class="ltx_p">Xin Zhang, Ning Jia and Ioannis Ivrissimtzis</span>
<span id="id1.1.id1.2" class="ltx_p ltx_align_center">Durham University</span>
<span id="id1.1.id1.3" class="ltx_p ltx_align_center">Department of Computer Science</span>
<span id="id1.1.id1.4" class="ltx_p ltx_align_center">Durham, DH1 3LE, UK</span>
<span id="id1.1.id1.5" class="ltx_p ltx_align_center">{xin.zhang3, ning.jia, ioannis.ivrissimtzis}@durham.ac.uk</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We present a deep neural network based method for the retrieval of watermarks from images of 3D printed objects. To deal with the variability of all possible 3D printing and image acquisition settings we train the network with synthetic data. The main simulator parameters such as texture, illumination and camera position are dynamically randomized in non-realistic ways, forcing the neural network to learn the intrinsic features of the 3D printed watermarks. At the end of the pipeline, the watermark, in the form of a two-dimensional bit array, is retrieved through a series of simple image processing and statistical operations applied on the confidence map generated by the neural network.
The results demonstrate that the inclusion of synthetic DR data in the training set increases the generalization power of the network, which performs better on images from previously unseen 3D printed objects. We conclude that in our application domain of information retrieval from 3D printed objects, where access to the exact CAD files of the printed objects can be assumed, one can use inexpensive synthetic data to enhance neural network training, reducing the need for the labour intensive process of creating large amounts of hand labelled real data or the need to generate photorealistic synthetic data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the past few years, deep Convolutional Neural Networks (CNNs) have transformed the field of computer vision, extending its domain of application into new directions. While CNNs are now the tool of choice in several fundamental computer vision tasks, such as image classification, semantic segmentation, image captioning, object detection, and human identification, nevertheless, their suitability for various specific applications still awaits to be verified, especially in situations where there is a shortage of annotated data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Watermark retrieval from 3D printed objects is one such application domain that remains less studied. <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">Watermarking</span>, a term used in a broad sense in this paper, is the embedding of information on a physical object or a digital file. It may serve various purposes, such as object or file authentication, copy control, protection against unauthorized alteration, or dissemination of machine readable information. The latter is an increasingly popular application, especially in the form of QR codes, and is the target application of the proposed method for watermarking 3D printed objects. In our study, similarly to QR codes, the watermark is a two-dimensional bit array, which is printed on a flat, non-slanted surface of the object in the form of small bumps arranged in a regular grid.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The superior performance of CNNs over classical machine learning models lies in the tuning of millions of parameters stored in the deep hierarchical structures by training them with abundant data and the corresponding annotations. That means that the accurate retrieval of the bit array of the watermark from a digital image of a 3D printed object requires a large amount of training images, annotated with the locations of the watermark bumps. The standard training data acquisition process would require the 3D printing of several objects, which can be expensive, and hand-annotating images of these objects which in an error-prone and laborious process.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In particular, the hand-annotation of this type of training data, as in <span id="S1.p4.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S1.p4.1.2" class="ltx_text ltx_font_bold">?<span id="S1.p4.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, has two known sources of error. Firstly, there is a human error introduced when the marked coordinates of the watermark bumps are not exactly on the centroid of the bump. Secondly, even if we assume semi-spherical bumps, as it is the case in our experiments, there is a systematic error introduced when a Gaussian kernel is applied on the marked coordinates to generate a smooth confidence map. Indeed, unless we manually adjust the size of the Gaussian kernel over each training image, which in practice it is infeasible, the training confidence maps do not adjust to the scale of the object’s image, i.e., assuming objects of the same size, to the distance of the camera from the object. Moreover, in what is the most intrinsic source of error in that process, when the image of the object is taken from a low angle, the images of the bumps deviate considerably form the circular shape and will always be over or under-covered by the circular support of the Gaussians.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To address the problems of human and systematic errors in the hand-labelling process, which confuse the CNN models and reduce their accuracy, we propose to use the Computer Aided Design models of the 3D printed models, which anyway exist since they are needed for 3D printing, to automatically produce synthetic training images with exact annotations. Apart from increasing the accuracy of the annotations, the proposed solution also reduces the cost of creating real training data, i.e. printing materials and labor, which means that we can have more variability in terms of object colors, illumination conditions and camera distance and angle.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our approach follows recently proposed cost efficient solutions for bridging the gap between real and synthetic data using graphic generators such as UnrealStereo <span id="S1.p6.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S1.p6.1.2" class="ltx_text ltx_font_bold">?<span id="S1.p6.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, which can generate photo-realistic synthetic data and the corresponding ground truths, and has been successfully applied in training neural networks for optical flow<span id="S1.p6.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S1.p6.1.4" class="ltx_text ltx_font_bold">?</span>; <span id="S1.p6.1.5" class="ltx_text ltx_font_bold">?<span id="S1.p6.1.5.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, semantic segmentation and stereo estimation. As UnrealStereo requires designers at artist level, domain randomization (DR) <span id="S1.p6.1.6" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S1.p6.1.7" class="ltx_text ltx_font_bold">?<span id="S1.p6.1.7.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> has been recently proposed to address this limitation. Domain randomization is currently used for robots to locate objects with simple shapes, forcing the network to focus on the essential features of the objects in the images rather than photorealism.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Contribution.</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">Addressing shortcomings in Zhang <span id="S1.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">et al. </span> <span id="S1.SS0.SSS0.Px1.p1.1.2" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S1.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_bold">?<span id="S1.SS0.SSS0.Px1.p1.1.3.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, which relies on a high cost, small, manually labelled real image dataset, in this paper we employ domain randomization (DR) for watermark retrieval from 3D printed objects, training the CNN with synthetic image data and confidence maps. The use of synthetic data not only reduces the cost of creating the training dataset, but also eliminates the human and the systematic errors of hand-labelling. The confidence maps are now precise in that they correspond exactly to the watermark bumps areas, see Figure <a href="#S1.F1" title="Figure 1 ‣ Contribution. ‣ 1 Introduction ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The main contributions are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Application of the domain randomization method to generate synthetic data for a non-trivial problem, watermark retrieval from 3D printed objects. The practicability of our approach rests in the fact that no artistic input is required at any stage of the process.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Experimental investigation of the effect of the parameters of the synthetic data.</p>
</div>
</li>
</ul>
<p id="S1.SS0.SSS0.Px1.p1.2" class="ltx_p">Our experiments show that on images from previously unseen 3D printed objects the highest precision and recall rates are achieved when we train the neural network with a combined dataset consisting of synthetic DR and real data. A series of further experiments shows that the inclusion of the synthetic data in the training set increases the generalization power of the network, enabling it to learn the basic invariant features of the target objects.</p>
</div>
<div id="S1.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p2.1" class="ltx_p">The rest of the paper is organized as follows. In Section <a href="#S2" title="2 Related Work ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we discuss related work;
in Section <a href="#S3" title="3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we introduce our synthetic data image generator and our
Convolutional Neural Network - 3D Watermarking (CNN-3DW); in section <a href="#S4" title="4 Experiment ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we test with real image
data and explore how synthetic data support training; and finally we briefly conclude in
section <a href="#S5" title="5 Conclusion ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1905.09706/assets/groundTruthHand.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="234" height="234" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1905.09706/assets/groundTruthAuto.jpg" id="S1.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="347" height="235" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Real image with hand-annotated coordinates of the watermark centroids (left1). Ground thruth image generated by the simulator (left2).</figcaption>
</figure>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We review prior work in three domains: 3D model watermarking in section <a href="#S2.SS1" title="2.1 Digital 3D Watermarking ‣ 2 Related Work ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, object detection with CNNs in section <a href="#S2.SS2" title="2.2 Object Detection with CNNs ‣ 2 Related Work ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, and synthetic data with domain randomization in section <a href="#S2.SS3" title="2.3 Synthetic dataset creation and domain randomization ‣ 2 Related Work ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Digital 3D Watermarking</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The focus of the research on 3D model watermarking is digital 3D model watermarking <span id="S2.SS1.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p1.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> aiming at the development of algorithms that are robust against a variety of malicious or unintentional attacks, including 3D printing and rescanning. The watermark survivability under such an attack has been tested in <span id="S2.SS1.p1.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p1.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p1.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. Surface watermarking algorithms resilient to the attack have been proposed in <span id="S2.SS1.p1.1.7" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p1.1.8" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS1.p1.1.9" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p1.1.9.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> the former encoding the watermark as modulations of horizontal slices of the 3D model. A survey of digital rights management and protection technologies for 3D printing can be found in <span id="S2.SS1.p1.1.10" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p1.1.11" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p1.1.11.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The above 3D printing related approaches have a different objective than ours, that is, they want to protect the 3D digital model rather than embed a physical object with machine readable information. As a result, the retrieval process requires the use of a laser scanner for the reconstruction of the original 3D model, while we apply instead standard computer vision techniques and never need to reconstruct a digital 3D model from the 3D printed object. In practice, their different objective also means that the capacity of the proposed algorithms is usually low, just enough for the purposes of model authentication.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Object Detection with CNNs</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Convolutional neural networks, AlexNet <span id="S2.SS2.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, VGGNet <span id="S2.SS2.p1.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>and GoogleNet <span id="S2.SS2.p1.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> being some famous examples, have demonstrated impressive performance on object detection tasks and are considered particularly well suited for feature extraction and classification. More recently, CNN models have been used in crowd counting tasks <span id="S2.SS2.p1.1.7" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.8" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.9" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.9.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, often formulated as regression problems with the raw images as input and a feature map characterizing the density of the crowd in the image as output.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Here, we take the simple but effective approach of training a CNN with synthetic image data for confidence map extraction from real images.
The choice to use a CNN was informed by an earlier approach to the problem where we used local binary patterns (LBPs) <span id="S2.SS2.p2.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p2.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. However, LBPs did not perform very well under adverse conditions, particularly when background patterns were too prominent; under extreme uneven illumination; or under unfavorable camera viewpoints.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span> Synthetic dataset creation and domain randomization</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The synthetic data methods support accurate ground truth annotations, and are cheap alternatives to annotating images. They have been widely used in deep learning. Jaderberg <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <span id="S2.SS3.p1.1.2" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.3.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> and Gupta <span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_italic">et al.</span> <span id="S2.SS3.p1.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> paste real text images to blank backgrounds and real natural background respectively. Dosovitskiy <span id="S2.SS3.p1.1.7" class="ltx_text ltx_font_italic">et al.</span> <span id="S2.SS3.p1.1.8" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.9" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.9.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> combines renderings of 3D chairs images with natural image backgrounds, such as city, landscape and mountains, to train FlowNet. Handa<span id="S2.SS3.p1.1.10" class="ltx_text ltx_font_italic">et al.</span> <span id="S2.SS3.p1.1.11" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.12" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.12.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> build indoor scenes using CAD models to understand real world indoor scenes. Atapour-Abarghouei and Breckon <span id="S2.SS3.p1.1.13" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.14" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.14.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> train a depth estimation model using synthetic game scene data. Zhang <span id="S2.SS3.p1.1.15" class="ltx_text ltx_font_italic">et al.</span> <span id="S2.SS3.p1.1.16" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.17" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.17.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> develop a synthetic image generation tool to analyse stereo vision. The first three of the above methods use existing datasets and models, while the last two require artist level synthetic image data.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Domain randomization is a simpler technique for training models on simulated images. It randomizes some of the parameters of the simulator and generates a dataset of sufficient variety consisting of non-artistic synthetic data <span id="S2.SS3.p2.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS3.p2.1.3" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p2.1.3.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. Here, we use domain randomization on our own 3D watermarked models, as there are no such datasets avaliabe, and train a CNN to retrieve watermarks from images of the corresponding 3D printed objects.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we describe the generation of the synthetic training dataset and present the proposed framework for automatic 3D watermark retrieval.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Synthetic Image Generator</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To prevent the model from overfitting to unwanted features such as object colour, surface texture, bump size and shape, and camera view-point, the following aspects were considered when creating the training dataset:</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The 3D shapes.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">To simulate the expected diversity of 3D printed objects in a practical scenario, the generated 3D models should have a wide range of colors and textures, and be embossed with bumps of various sizes and shapes. Eventually, any type of real data would appear as variation of some of the synthetic data, and their bumps will be identified correctly regardless of their peculiarities.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Camera view-point.</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Introducing training images captured from arbitrary view angles increases the robustness of the model, thus we rotate the virtual camera of the simulator around the model.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Scene illumination.</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">In a practical scenario, when we capture images of 3D printed objects we will not be able to control the illumination conditions. Thus we use multiple light sources, eliminating the influence of light intensity and shadow.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Scene background.</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">By padding the captured images onto random backgrounds we reduce the sensitivity to background noise. The background photos are sampled from available public datasets of indoor and outdoor scenes.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Software.</h5>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p1.1" class="ltx_p">For the synthetic data generation task we adopted Pov-Ray <span id="S3.SS1.SSS0.Px5.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S3.SS1.SSS0.Px5.p1.1.2" class="ltx_text ltx_font_bold">?<span id="S3.SS1.SSS0.Px5.p1.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, since it is open-source and supports several geometric primitives for constructive solid geometry, which is a popular modelling method for designing objects for 3D printing.</p>
</div>
<div id="S3.SS1.SSS0.Px5.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p2.1" class="ltx_p">The setup of the synthetic image generator is demonstrated in Figure <a href="#S3.F2" title="Figure 2 ‣ Software. ‣ 3.1 Synthetic Image Generator ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It consists of five light sources, one 3D model, and one camera. When generating data, the camera revolves around the synthetic model to capture images under various angles; simultaneously, the synthetic model orbits light_1, i.e. the primary illuminating source placed in the middle, while rotating around its own axis, to simulate variant beam directions. A random position shift of the camera within a moderate range is introduced to further increase the randomness of view points and scale, while additional illuminating sources lights_2-5 are introduced for simulating a complex illumination scenario. As to the 3D model itself, we introduce color randomness and a wide range of optional textures to simulate the appearance of 3D printed objects. Note that camera’s orbital speed is slightly faster than the angular velocity of the rotating model.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1905.09706/assets/F2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Synthetic image generator.</figcaption>
</figure>
<div id="S3.SS1.SSS0.Px5.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p3.1" class="ltx_p">We set Pov-Ray’s float identifier clock at 120 frames per cycle with the initial frame at clock value 0.0 and the last frame at value 1.0.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Textures and colors.</h5>

<div id="S3.SS1.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px6.p1.3" class="ltx_p">Model textures are randomly picked from Pov-Ray’s files at various scales. The RGB colors are randomized at <math id="S3.SS1.SSS0.Px6.p1.1.m1.3" class="ltx_Math" alttext="\alpha*clock,\beta*clock,\gamma*clock" display="inline"><semantics id="S3.SS1.SSS0.Px6.p1.1.m1.3a"><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.4.cmml"><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.cmml"><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.2.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.1.cmml">∗</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1a" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.4" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1b" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.5" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1c" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.6" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.6.cmml">k</mi></mrow><mo id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.4" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.cmml"><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.2.cmml">β</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.1.cmml">∗</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1a" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.4" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1b" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.5" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1c" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.6" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.6.cmml">k</mi></mrow><mo id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.5" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.cmml"><mrow id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.cmml"><mi id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.2" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.2.cmml">γ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.1.cmml">∗</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.3" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1a" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.4" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1b" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.5" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1c" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.6" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.6.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px6.p1.1.m1.3b"><list id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.4.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3"><apply id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1"><times id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.1"></times><apply id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2"><times id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.1"></times><ci id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.2">𝛼</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.2.3">𝑐</ci></apply><ci id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.3">𝑙</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.4">𝑜</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.5.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.5">𝑐</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.6.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.1.1.1.1.6">𝑘</ci></apply><apply id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2"><times id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.1"></times><apply id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2"><times id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.1"></times><ci id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.2">𝛽</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.2.3">𝑐</ci></apply><ci id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.3">𝑙</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.4.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.4">𝑜</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.5.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.5">𝑐</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.6.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.2.2.2.2.6">𝑘</ci></apply><apply id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3"><times id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.1"></times><apply id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2"><times id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.1.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.1"></times><ci id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.2">𝛾</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.2.3">𝑐</ci></apply><ci id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.3">𝑙</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.4.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.4">𝑜</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.5.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.5">𝑐</ci><ci id="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.6.cmml" xref="S3.SS1.SSS0.Px6.p1.1.m1.3.3.3.3.6">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px6.p1.1.m1.3c">\alpha*clock,\beta*clock,\gamma*clock</annotation></semantics></math>, where <math id="S3.SS1.SSS0.Px6.p1.2.m2.3" class="ltx_Math" alttext="\alpha,\beta,\gamma" display="inline"><semantics id="S3.SS1.SSS0.Px6.p1.2.m2.3a"><mrow id="S3.SS1.SSS0.Px6.p1.2.m2.3.4.2" xref="S3.SS1.SSS0.Px6.p1.2.m2.3.4.1.cmml"><mi id="S3.SS1.SSS0.Px6.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px6.p1.2.m2.1.1.cmml">α</mi><mo id="S3.SS1.SSS0.Px6.p1.2.m2.3.4.2.1" xref="S3.SS1.SSS0.Px6.p1.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px6.p1.2.m2.2.2" xref="S3.SS1.SSS0.Px6.p1.2.m2.2.2.cmml">β</mi><mo id="S3.SS1.SSS0.Px6.p1.2.m2.3.4.2.2" xref="S3.SS1.SSS0.Px6.p1.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px6.p1.2.m2.3.3" xref="S3.SS1.SSS0.Px6.p1.2.m2.3.3.cmml">γ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px6.p1.2.m2.3b"><list id="S3.SS1.SSS0.Px6.p1.2.m2.3.4.1.cmml" xref="S3.SS1.SSS0.Px6.p1.2.m2.3.4.2"><ci id="S3.SS1.SSS0.Px6.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px6.p1.2.m2.1.1">𝛼</ci><ci id="S3.SS1.SSS0.Px6.p1.2.m2.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.2.m2.2.2">𝛽</ci><ci id="S3.SS1.SSS0.Px6.p1.2.m2.3.3.cmml" xref="S3.SS1.SSS0.Px6.p1.2.m2.3.3">𝛾</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px6.p1.2.m2.3c">\alpha,\beta,\gamma</annotation></semantics></math> <math id="S3.SS1.SSS0.Px6.p1.3.m3.2" class="ltx_Math" alttext="\in\left(0,5\right)" display="inline"><semantics id="S3.SS1.SSS0.Px6.p1.3.m3.2a"><mrow id="S3.SS1.SSS0.Px6.p1.3.m3.2.3" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.cmml"><mi id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.2" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.2.cmml"></mi><mo id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.1" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.1.cmml">∈</mo><mrow id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.2" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.1.cmml"><mo id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.2.1" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.1.cmml">(</mo><mn id="S3.SS1.SSS0.Px6.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px6.p1.3.m3.1.1.cmml">0</mn><mo id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.2.2" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S3.SS1.SSS0.Px6.p1.3.m3.2.2" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.2.cmml">5</mn><mo id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.2.3" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px6.p1.3.m3.2b"><apply id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.cmml" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3"><in id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.1.cmml" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.1"></in><csymbol cd="latexml" id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.2.cmml" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.2">absent</csymbol><interval closure="open" id="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.3.3.2"><cn type="integer" id="S3.SS1.SSS0.Px6.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px6.p1.3.m3.1.1">0</cn><cn type="integer" id="S3.SS1.SSS0.Px6.p1.3.m3.2.2.cmml" xref="S3.SS1.SSS0.Px6.p1.3.m3.2.2">5</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px6.p1.3.m3.2c">\in\left(0,5\right)</annotation></semantics></math> are random numbers.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Scene lighting.</h5>

<div id="S3.SS1.SSS0.Px7.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px7.p1.1" class="ltx_p">To approximate a realistic environment, we use five different light sources. The centre (light_1) is a parallel light source and around it in four different orientations we placed: a near point light (light_2), a conical spotlight (light_3), an area light (light_4) and a fading light (light_5).</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px8" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pose and Camera.</h5>

<div id="S3.SS1.SSS0.Px8.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px8.p1.5" class="ltx_p">Within each clock cycle, the synthetic 3D model rotates around its own axis by <math id="S3.SS1.SSS0.Px8.p1.1.m1.1" class="ltx_Math" alttext="360*clock*30" display="inline"><semantics id="S3.SS1.SSS0.Px8.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px8.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.cmml"><mn id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.2" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.2.cmml">360</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.1" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.1.cmml">∗</mo><mi id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.3" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.3" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1a" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.4" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1b" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.5" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1c" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.6" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.6.cmml">k</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.1.cmml">∗</mo><mn id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px8.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1"><times id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.1"></times><apply id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2"><times id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.1"></times><apply id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2"><times id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.1"></times><cn type="integer" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.2">360</cn><ci id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.2.3">𝑐</ci></apply><ci id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.3">𝑙</ci><ci id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.4.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.4">𝑜</ci><ci id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.5.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.5">𝑐</ci><ci id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.6.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.2.6">𝑘</ci></apply><cn type="integer" id="S3.SS1.SSS0.Px8.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.1.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px8.p1.1.m1.1c">360*clock*30</annotation></semantics></math> degrees and revolves around the central light by <math id="S3.SS1.SSS0.Px8.p1.2.m2.1" class="ltx_Math" alttext="360*clock" display="inline"><semantics id="S3.SS1.SSS0.Px8.p1.2.m2.1a"><mrow id="S3.SS1.SSS0.Px8.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.cmml"><mn id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.2" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.2.cmml">360</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.1" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.1.cmml">∗</mo><mi id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.3" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1a" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.4" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1b" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.5" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1c" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.6" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.6.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px8.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1"><times id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.1"></times><apply id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2"><times id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.1"></times><cn type="integer" id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.2">360</cn><ci id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.2.3">𝑐</ci></apply><ci id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.3">𝑙</ci><ci id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.4.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.4">𝑜</ci><ci id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.5.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.5">𝑐</ci><ci id="S3.SS1.SSS0.Px8.p1.2.m2.1.1.6.cmml" xref="S3.SS1.SSS0.Px8.p1.2.m2.1.1.6">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px8.p1.2.m2.1c">360*clock</annotation></semantics></math> degrees. The camera revolves by <math id="S3.SS1.SSS0.Px8.p1.3.m3.1" class="ltx_Math" alttext="360*clock*12" display="inline"><semantics id="S3.SS1.SSS0.Px8.p1.3.m3.1a"><mrow id="S3.SS1.SSS0.Px8.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.cmml"><mn id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.2" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.2.cmml">360</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.1" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.1.cmml">∗</mo><mi id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.3" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.3" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1a" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.4" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1b" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.5" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1c" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.6" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.6.cmml">k</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.1" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.1.cmml">∗</mo><mn id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px8.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1"><times id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.1"></times><apply id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2"><times id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.1"></times><apply id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2"><times id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.1"></times><cn type="integer" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.2">360</cn><ci id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.2.3">𝑐</ci></apply><ci id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.3">𝑙</ci><ci id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.4.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.4">𝑜</ci><ci id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.5.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.5">𝑐</ci><ci id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.6.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.2.6">𝑘</ci></apply><cn type="integer" id="S3.SS1.SSS0.Px8.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.3.m3.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px8.p1.3.m3.1c">360*clock*12</annotation></semantics></math> degrees and its movement along the vertical axis is set to <math id="S3.SS1.SSS0.Px8.p1.4.m4.2" class="ltx_Math" alttext="jStart+jHeight*(1-\cos(4\pi(clock-jStart)))/2" display="inline"><semantics id="S3.SS1.SSS0.Px8.p1.4.m4.2a"><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1a" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.4" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1b" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.5" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1c" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.6" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1d" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.7" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.7.cmml">t</mi></mrow><mo id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.2.cmml">+</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1a" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.4" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1b" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.5" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1c" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.6" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1d" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.7" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1e" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.8" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.8.cmml">t</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.2.cmml">∗</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.cmml"><mn id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.3.cmml">1</mn><mo id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px8.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.1.1.cmml">cos</mi><mo id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1a" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.3.cmml">4</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.4" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.4.cmml">π</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.2a" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1a" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.4" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1b" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.5" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1c" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.6" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.6.cmml">k</mi></mrow><mo id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.4" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1b" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.5" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1c" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.6" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1d" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.7" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.7.cmml">t</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.2" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.2.cmml">/</mo><mn id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.3" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.3.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px8.p1.4.m4.2b"><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2"><plus id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.2"></plus><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3"><times id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.1"></times><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.2">𝑗</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.3">𝑆</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.4.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.4">𝑡</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.5.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.5">𝑎</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.6.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.6">𝑟</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.7.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.3.7">𝑡</ci></apply><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1"><divide id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.2"></divide><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1"><times id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.2"></times><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3"><times id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.2">𝑗</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.3">𝐻</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.4">𝑒</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.5.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.5">𝑖</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.6.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.6">𝑔</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.7.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.7">ℎ</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.8.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.3.8">𝑡</ci></apply><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1"><minus id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.2"></minus><cn type="integer" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.3">1</cn><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1"><cos id="S3.SS1.SSS0.Px8.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.1.1"></cos><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1"><times id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.2"></times><cn type="integer" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.3">4</cn><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.4">𝜋</ci><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑐</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑙</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.4">𝑜</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.5.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.5">𝑐</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.6.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.6">𝑘</ci></apply><apply id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑗</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑆</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.4">𝑡</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.5">𝑎</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.6.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.6">𝑟</ci><ci id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.7.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.7">𝑡</ci></apply></apply></apply></apply></apply></apply><cn type="integer" id="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.4.m4.2.2.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px8.p1.4.m4.2c">jStart+jHeight*(1-\cos(4\pi(clock-jStart)))/2</annotation></semantics></math>. The synthetic image is captured at pixel size <math id="S3.SS1.SSS0.Px8.p1.5.m5.1" class="ltx_Math" alttext="3072*4096" display="inline"><semantics id="S3.SS1.SSS0.Px8.p1.5.m5.1a"><mrow id="S3.SS1.SSS0.Px8.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.cmml"><mn id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.2.cmml">3072</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.1" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.1.cmml">∗</mo><mn id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.3.cmml">4096</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px8.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1"><times id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.1"></times><cn type="integer" id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.2">3072</cn><cn type="integer" id="S3.SS1.SSS0.Px8.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px8.p1.5.m5.1.1.3">4096</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px8.p1.5.m5.1c">3072*4096</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.SSS0.Px8.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px8.p2.1" class="ltx_p">The corresponding annotations are generated simultaneously with the synthetic image, as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Pose and Camera. ‣ 3.1 Synthetic Image Generator ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For generating annotations, we keep the same the camera pose and light parameters and change the watermark bumps to full white while the rest of the model is set to black. The produced synthetic annotation is an exact map of the bump regions.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/clock1.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/clock25.jpg" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/clock63.jpg" id="S3.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/clock113.jpg" id="S3.F3.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/label1.jpg" id="S3.F3.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/label25.jpg" id="S3.F3.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/label63.jpg" id="S3.F3.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/label113.jpg" id="S3.F3.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Synthetic image and corresponding ground truth at different clock values.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Watermark detector</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Retrieving watermark information from 3D bumps is a dot detection problem. We tackle it by combining a deep convolutional neural network named <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">CNN-3DW</span> and an image registration module which processes the output of <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">CNN-3DW</span> to retrieve the watermark bit matrix.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>CNN-3DW</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The CNN-3DW is based on the network model shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2.2 Image registration, processing and matrix retrieval ‣ 3.2 Watermark detector ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. It consists of five convolutional layers with kernel sizes <math id="S3.SS2.SSS1.p1.1.m1.5" class="ltx_Math" alttext="9*9,7*7,7*7,7*7,1*1" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.5a"><mrow id="S3.SS2.SSS1.p1.1.m1.5.5.5" xref="S3.SS2.SSS1.p1.1.m1.5.5.6.cmml"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.2.cmml">9</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml">∗</mo><mn id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3.cmml">9</mn></mrow><mo id="S3.SS2.SSS1.p1.1.m1.5.5.5.6" xref="S3.SS2.SSS1.p1.1.m1.5.5.6.cmml">,</mo><mrow id="S3.SS2.SSS1.p1.1.m1.2.2.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.1" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.1.cmml">∗</mo><mn id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.3" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.3.cmml">7</mn></mrow><mo id="S3.SS2.SSS1.p1.1.m1.5.5.5.7" xref="S3.SS2.SSS1.p1.1.m1.5.5.6.cmml">,</mo><mrow id="S3.SS2.SSS1.p1.1.m1.3.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.1" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.1.cmml">∗</mo><mn id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.3.cmml">7</mn></mrow><mo id="S3.SS2.SSS1.p1.1.m1.5.5.5.8" xref="S3.SS2.SSS1.p1.1.m1.5.5.6.cmml">,</mo><mrow id="S3.SS2.SSS1.p1.1.m1.4.4.4.4" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.2" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.1" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.1.cmml">∗</mo><mn id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.3" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.3.cmml">7</mn></mrow><mo id="S3.SS2.SSS1.p1.1.m1.5.5.5.9" xref="S3.SS2.SSS1.p1.1.m1.5.5.6.cmml">,</mo><mrow id="S3.SS2.SSS1.p1.1.m1.5.5.5.5" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.2" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.1" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.1.cmml">∗</mo><mn id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.3" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.5b"><list id="S3.SS2.SSS1.p1.1.m1.5.5.6.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.5"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1"><times id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.2">9</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3">9</cn></apply><apply id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2"><times id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.2">7</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2.2.2.3">7</cn></apply><apply id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3"><times id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.2">7</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3.3.3.3">7</cn></apply><apply id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4"><times id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.2">7</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.4.4.4.4.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.4.4.3">7</cn></apply><apply id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5"><times id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.2">1</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.5.5.5.5.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.5.5.3">1</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.5c">9*9,7*7,7*7,7*7,1*1</annotation></semantics></math> and feature map channels 48, 96, 48, 24, 1, respectively. The Rectified Linear unit (ReLu) is used as activation function. Each of the first two layers is followed by a standard max pooling layer, increasing the receptive field. For all the convolutional layers, we set the step size as one and do zero padding so that the image size is not altered by the convolutional operations. Since the model is fully convolutional, it can handle different input sizes. The reason we designed our network based on FCN rather than a deeper architecture such as VGG and ResNet is that we found in our experiments that the output of FCN was more stable. Indeed, locating dots on a single colored plate is a relatively simple task and the adoption of deeper models is more resource expensive. Further, as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Pose and Camera. ‣ 3.1 Synthetic Image Generator ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the ground truth image composes of mostly zero value pixels indicating the irrelevant background, and by stacking non-linearity active functions such as ReLU can easily cause the network model to predict all-zero.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Because our training data comes from inexpensive synthetic images with controllable parameters, data augmentation such as mirroring, translational shift, rotation and relighting is not necessary during the training phase. Instead, for time and resource efficiency, since the original images are oversized for graphics cards with small capacity, we randomly cropped each image to smaller patches of uniform size (1024*1024).</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.2" class="ltx_p">We implemented CNN-3DW in Pytorch and adopted the mean squared error (MSE) for computing loss. The model is optimised using Adam <span id="S3.SS2.SSS1.p3.2.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S3.SS2.SSS1.p3.2.2" class="ltx_text ltx_font_bold">?<span id="S3.SS2.SSS1.p3.2.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, the initial learning rate is <math id="S3.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><mrow id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS1.p3.1.m1.1.1.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.cmml"><mn id="S3.SS2.SSS1.p3.1.m1.1.1.2.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.1.m1.1.1.2.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.p3.1.m1.1.1.2.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS2.SSS1.p3.1.m1.1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.1.m1.1.1.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><apply id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1"><minus id="S3.SS2.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1"></minus><apply id="S3.SS2.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.2"><times id="S3.SS2.SSS1.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS2.SSS1.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.2">1</cn><ci id="S3.SS2.SSS1.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS2.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">1e-4</annotation></semantics></math> and is decreased to <math id="S3.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="1e-5" display="inline"><semantics id="S3.SS2.SSS1.p3.2.m2.1a"><mrow id="S3.SS2.SSS1.p3.2.m2.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.cmml"><mrow id="S3.SS2.SSS1.p3.2.m2.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml"><mn id="S3.SS2.SSS1.p3.2.m2.1.1.2.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.2.m2.1.1.2.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.2.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS2.SSS1.p3.2.m2.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p3.2.m2.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.1b"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1"><minus id="S3.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1"></minus><apply id="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2"><times id="S3.SS2.SSS1.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.1"></times><cn type="integer" id="S3.SS2.SSS1.p3.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.2">1</cn><ci id="S3.SS2.SSS1.p3.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.1c">1e-5</annotation></semantics></math> after 150 epochs. We used a single NVIDIA GeForce GTX TITAN X graphics card to train our network with a batch size of 12.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Image registration, processing and matrix retrieval</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.3" class="ltx_p">Using the trained CNN-3DW model we are able to estimate a confidence map <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{Y}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">𝐘</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝐘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\mathbf{Y}</annotation></semantics></math> of the watermark bumps in the image <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mi id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><ci id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">\mathbf{X}</annotation></semantics></math>. To extract the embedded information matrix from the estimated confidence map <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{Y}" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mi id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml">𝐘</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><ci id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">𝐘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">\mathbf{Y}</annotation></semantics></math> is still non-trivial. Indeed, due to variability in the illumination conditions and surface textures inadvertently introduced by the popular, inexpensive printers based on Fused Deposition Modelling (FDM) technology, a large amount of background noise can be introduced, posing great challenges to the automatic watermark region localization. Since a significant bias in the watermark region localization would lead to catastrophic error in the following stages, we decided to tackle this issue by printing four <span id="S3.SS2.SSS2.p1.3.1" class="ltx_text ltx_font_italic">landmarks</span> at the corners of the watermark with distinguishable colours (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.2 Image registration, processing and matrix retrieval ‣ 3.2 Watermark detector ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). During retrieval, the watermark regions are easily located by finding the differently colored <span id="S3.SS2.SSS2.p1.3.2" class="ltx_text ltx_font_italic">landmarks</span> at the four corners.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.3" class="ltx_p">Further, since we do not require from the user to align their camera with the 3D information matrix when capturing images, the generated confidence map needs to be normalized before retrieving the information. We use image registration <span id="S3.SS2.SSS2.p2.3.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S3.SS2.SSS2.p2.3.2" class="ltx_text ltx_font_bold">?<span id="S3.SS2.SSS2.p2.3.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> to transform the quadrilateral watermark region in the confidence map <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{Y}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mi id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">𝐘</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><ci id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">𝐘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\mathbf{Y}</annotation></semantics></math> to a square region (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.2 Image registration, processing and matrix retrieval ‣ 3.2 Watermark detector ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), using Matlab’s built-in tools to map the <span id="S3.SS2.SSS2.p2.3.3" class="ltx_text ltx_font_italic">landmarks</span> to the four corners of a square region. We denote the normalized watermark region of the confidence map as <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{Y}^{r}" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><msup id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">𝐘</mi><mi id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">𝐘</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">\mathbf{Y}^{r}</annotation></semantics></math> and process it to extract the embedded information matrix <math id="S3.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><mi id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><ci id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">\mathbf{M}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.2" class="ltx_p">The registered confidence map <math id="S3.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{Y}^{r}" display="inline"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><msup id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml">𝐘</mi><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">𝐘</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">\mathbf{Y}^{r}</annotation></semantics></math> is first binarized using a threshold value <math id="S3.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><mi id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><ci id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">t</annotation></semantics></math>,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\hat{\mathbf{Y}}^{r}_{ij}=\begin{cases}1\quad\text{if $\mathbf{Y}^{r}_{ij}&gt;t$ ,}\\
0\quad\text{otherwise}.\end{cases}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><msubsup id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mover accent="true" id="S3.E1.m1.2.3.2.2.2" xref="S3.E1.m1.2.3.2.2.2.cmml"><mi id="S3.E1.m1.2.3.2.2.2.2" xref="S3.E1.m1.2.3.2.2.2.2.cmml">𝐘</mi><mo id="S3.E1.m1.2.3.2.2.2.1" xref="S3.E1.m1.2.3.2.2.2.1.cmml">^</mo></mover><mrow id="S3.E1.m1.2.3.2.3" xref="S3.E1.m1.2.3.2.3.cmml"><mi id="S3.E1.m1.2.3.2.3.2" xref="S3.E1.m1.2.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.2.3.1" xref="S3.E1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E1.m1.2.3.2.3.3" xref="S3.E1.m1.2.3.2.3.3.cmml">j</mi></mrow><mi id="S3.E1.m1.2.3.2.2.3" xref="S3.E1.m1.2.3.2.2.3.cmml">r</mi></msubsup><mo id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.3.3.1.cmml"><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.3.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.3.3.1.cmml"><mtr id="S3.E1.m1.2.2.2a" xref="S3.E1.m1.2.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.2.2.2b" xref="S3.E1.m1.2.3.3.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">1</mn><mspace width="1em" id="S3.E1.m1.1.1.1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"></mspace><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1c.cmml"><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1c.cmml">if </mtext><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.2.cmml">𝐘</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.3.cmml">j</mi></mrow><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.3.cmml">r</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">&gt;</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">t</mi></mrow><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.1.1.1.1c.cmml"> ,</mtext></mrow></mrow></mtd><mtd id="S3.E1.m1.2.2.2c" xref="S3.E1.m1.2.3.3.1.1.cmml"></mtd></mtr><mtr id="S3.E1.m1.2.2.2d" xref="S3.E1.m1.2.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.2.2.2e" xref="S3.E1.m1.2.3.3.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.3.3.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1.3.1.2" xref="S3.E1.m1.2.2.2.2.1.1.3.1.1.cmml"><mn id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">0</mn><mspace width="1em" id="S3.E1.m1.2.2.2.2.1.1.3.1.2.1" xref="S3.E1.m1.2.2.2.2.1.1.3.1.1.cmml"></mspace><mtext id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2a.cmml">otherwise</mtext></mrow><mo lspace="0em" id="S3.E1.m1.2.2.2.2.1.1.3.2" xref="S3.E1.m1.2.3.3.1.cmml">.</mo></mrow></mtd><mtd id="S3.E1.m1.2.2.2f" xref="S3.E1.m1.2.3.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2">subscript</csymbol><apply id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.2.1.cmml" xref="S3.E1.m1.2.3.2">superscript</csymbol><apply id="S3.E1.m1.2.3.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2.2"><ci id="S3.E1.m1.2.3.2.2.2.1.cmml" xref="S3.E1.m1.2.3.2.2.2.1">^</ci><ci id="S3.E1.m1.2.3.2.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2.2.2">𝐘</ci></apply><ci id="S3.E1.m1.2.3.2.2.3.cmml" xref="S3.E1.m1.2.3.2.2.3">𝑟</ci></apply><apply id="S3.E1.m1.2.3.2.3.cmml" xref="S3.E1.m1.2.3.2.3"><times id="S3.E1.m1.2.3.2.3.1.cmml" xref="S3.E1.m1.2.3.2.3.1"></times><ci id="S3.E1.m1.2.3.2.3.2.cmml" xref="S3.E1.m1.2.3.2.3.2">𝑖</ci><ci id="S3.E1.m1.2.3.2.3.3.cmml" xref="S3.E1.m1.2.3.2.3.3">𝑗</ci></apply></apply><apply id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.2"><csymbol cd="latexml" id="S3.E1.m1.2.3.3.1.1.cmml" xref="S3.E1.m1.2.2.3">cases</csymbol><list id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4"><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">if </mtext><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1"><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.2">𝐘</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.2">i</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.1" lspace='0px' rspace='0px'></mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.3.3">j</mi></mrow><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.2.2.3">r</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.1">&gt;</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.3">t</mi></mrow><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"> ,</mtext></mrow></ci></list><ci id="S3.E1.m1.2.3.3.1.3a.cmml" xref="S3.E1.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E1.m1.2.3.3.1.3.cmml" xref="S3.E1.m1.2.2.3">otherwise</mtext></ci><list id="S3.E1.m1.2.2.2.2.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.1.2"><cn type="integer" id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1">0</cn><ci id="S3.E1.m1.2.2.2.2.1.1.2a.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"><mtext id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">otherwise</mtext></ci></list><ci id="S3.E1.m1.2.3.3.1.5a.cmml" xref="S3.E1.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E1.m1.2.3.3.1.5.cmml" xref="S3.E1.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\hat{\mathbf{Y}}^{r}_{ij}=\begin{cases}1\quad\text{if $\mathbf{Y}^{r}_{ij}&gt;t$ ,}\\
0\quad\text{otherwise}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p3.8" class="ltx_p">where <math id="S3.SS2.SSS2.p3.3.m1.1" class="ltx_Math" alttext="t=\beta\times Thre" display="inline"><semantics id="S3.SS2.SSS2.p3.3.m1.1a"><mrow id="S3.SS2.SSS2.p3.3.m1.1.1" xref="S3.SS2.SSS2.p3.3.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.3.m1.1.1.2" xref="S3.SS2.SSS2.p3.3.m1.1.1.2.cmml">t</mi><mo id="S3.SS2.SSS2.p3.3.m1.1.1.1" xref="S3.SS2.SSS2.p3.3.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.SSS2.p3.3.m1.1.1.3" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.cmml"><mrow id="S3.SS2.SSS2.p3.3.m1.1.1.3.2" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.cmml"><mi id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.2" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.2.cmml">β</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.1" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.1.cmml">×</mo><mi id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.3" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.3.cmml">T</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.3.m1.1.1.3.1" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p3.3.m1.1.1.3.3" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.3.m1.1.1.3.1a" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p3.3.m1.1.1.3.4" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.3.m1.1.1.3.1b" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p3.3.m1.1.1.3.5" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.5.cmml">e</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m1.1b"><apply id="S3.SS2.SSS2.p3.3.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1"><eq id="S3.SS2.SSS2.p3.3.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.3.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.2">𝑡</ci><apply id="S3.SS2.SSS2.p3.3.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3"><times id="S3.SS2.SSS2.p3.3.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.1"></times><apply id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2"><times id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.1.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.1"></times><ci id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.2.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.2">𝛽</ci><ci id="S3.SS2.SSS2.p3.3.m1.1.1.3.2.3.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.2.3">𝑇</ci></apply><ci id="S3.SS2.SSS2.p3.3.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.3">ℎ</ci><ci id="S3.SS2.SSS2.p3.3.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.4">𝑟</ci><ci id="S3.SS2.SSS2.p3.3.m1.1.1.3.5.cmml" xref="S3.SS2.SSS2.p3.3.m1.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m1.1c">t=\beta\times Thre</annotation></semantics></math>, <math id="S3.SS2.SSS2.p3.4.m2.1" class="ltx_Math" alttext="Thre" display="inline"><semantics id="S3.SS2.SSS2.p3.4.m2.1a"><mrow id="S3.SS2.SSS2.p3.4.m2.1.1" xref="S3.SS2.SSS2.p3.4.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p3.4.m2.1.1.2" xref="S3.SS2.SSS2.p3.4.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.4.m2.1.1.1" xref="S3.SS2.SSS2.p3.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p3.4.m2.1.1.3" xref="S3.SS2.SSS2.p3.4.m2.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.4.m2.1.1.1a" xref="S3.SS2.SSS2.p3.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p3.4.m2.1.1.4" xref="S3.SS2.SSS2.p3.4.m2.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.4.m2.1.1.1b" xref="S3.SS2.SSS2.p3.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p3.4.m2.1.1.5" xref="S3.SS2.SSS2.p3.4.m2.1.1.5.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m2.1b"><apply id="S3.SS2.SSS2.p3.4.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m2.1.1"><times id="S3.SS2.SSS2.p3.4.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m2.1.1.1"></times><ci id="S3.SS2.SSS2.p3.4.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.4.m2.1.1.2">𝑇</ci><ci id="S3.SS2.SSS2.p3.4.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.4.m2.1.1.3">ℎ</ci><ci id="S3.SS2.SSS2.p3.4.m2.1.1.4.cmml" xref="S3.SS2.SSS2.p3.4.m2.1.1.4">𝑟</ci><ci id="S3.SS2.SSS2.p3.4.m2.1.1.5.cmml" xref="S3.SS2.SSS2.p3.4.m2.1.1.5">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m2.1c">Thre</annotation></semantics></math> is the Otsu threshold <span id="S3.SS2.SSS2.p3.8.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S3.SS2.SSS2.p3.8.2" class="ltx_text ltx_font_bold">?<span id="S3.SS2.SSS2.p3.8.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> of <math id="S3.SS2.SSS2.p3.5.m3.1" class="ltx_Math" alttext="\mathbf{Y}^{r}" display="inline"><semantics id="S3.SS2.SSS2.p3.5.m3.1a"><msup id="S3.SS2.SSS2.p3.5.m3.1.1" xref="S3.SS2.SSS2.p3.5.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p3.5.m3.1.1.2" xref="S3.SS2.SSS2.p3.5.m3.1.1.2.cmml">𝐘</mi><mi id="S3.SS2.SSS2.p3.5.m3.1.1.3" xref="S3.SS2.SSS2.p3.5.m3.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m3.1b"><apply id="S3.SS2.SSS2.p3.5.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m3.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m3.1.1.2">𝐘</ci><ci id="S3.SS2.SSS2.p3.5.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.5.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m3.1c">\mathbf{Y}^{r}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p3.6.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.SSS2.p3.6.m4.1a"><mi id="S3.SS2.SSS2.p3.6.m4.1.1" xref="S3.SS2.SSS2.p3.6.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.6.m4.1b"><ci id="S3.SS2.SSS2.p3.6.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.6.m4.1c">\beta</annotation></semantics></math> a user defined parameter. In our experiments, we empirically set <math id="S3.SS2.SSS2.p3.7.m5.1" class="ltx_Math" alttext="\beta=0.35" display="inline"><semantics id="S3.SS2.SSS2.p3.7.m5.1a"><mrow id="S3.SS2.SSS2.p3.7.m5.1.1" xref="S3.SS2.SSS2.p3.7.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p3.7.m5.1.1.2" xref="S3.SS2.SSS2.p3.7.m5.1.1.2.cmml">β</mi><mo id="S3.SS2.SSS2.p3.7.m5.1.1.1" xref="S3.SS2.SSS2.p3.7.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS2.p3.7.m5.1.1.3" xref="S3.SS2.SSS2.p3.7.m5.1.1.3.cmml">0.35</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.7.m5.1b"><apply id="S3.SS2.SSS2.p3.7.m5.1.1.cmml" xref="S3.SS2.SSS2.p3.7.m5.1.1"><eq id="S3.SS2.SSS2.p3.7.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p3.7.m5.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.7.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p3.7.m5.1.1.2">𝛽</ci><cn type="float" id="S3.SS2.SSS2.p3.7.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p3.7.m5.1.1.3">0.35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.7.m5.1c">\beta=0.35</annotation></semantics></math>, which seems to optimize performance. The binary confidence map <math id="S3.SS2.SSS2.p3.8.m6.1" class="ltx_Math" alttext="\hat{\mathbf{Y}}^{r}" display="inline"><semantics id="S3.SS2.SSS2.p3.8.m6.1a"><msup id="S3.SS2.SSS2.p3.8.m6.1.1" xref="S3.SS2.SSS2.p3.8.m6.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p3.8.m6.1.1.2" xref="S3.SS2.SSS2.p3.8.m6.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.8.m6.1.1.2.2" xref="S3.SS2.SSS2.p3.8.m6.1.1.2.2.cmml">𝐘</mi><mo id="S3.SS2.SSS2.p3.8.m6.1.1.2.1" xref="S3.SS2.SSS2.p3.8.m6.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.SSS2.p3.8.m6.1.1.3" xref="S3.SS2.SSS2.p3.8.m6.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.8.m6.1b"><apply id="S3.SS2.SSS2.p3.8.m6.1.1.cmml" xref="S3.SS2.SSS2.p3.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.8.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p3.8.m6.1.1">superscript</csymbol><apply id="S3.SS2.SSS2.p3.8.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p3.8.m6.1.1.2"><ci id="S3.SS2.SSS2.p3.8.m6.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.8.m6.1.1.2.1">^</ci><ci id="S3.SS2.SSS2.p3.8.m6.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.8.m6.1.1.2.2">𝐘</ci></apply><ci id="S3.SS2.SSS2.p3.8.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p3.8.m6.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.8.m6.1c">\hat{\mathbf{Y}}^{r}</annotation></semantics></math> is visualized in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.2 Image registration, processing and matrix retrieval ‣ 3.2 Watermark detector ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (left3). Next, we call Matlab’s <span id="S3.SS2.SSS2.p3.8.3" class="ltx_text ltx_font_italic">regionprops</span> function to detect the connected regions of the binary image and obtain estimates of their centroids and their two semi-axes. If the the sum of the two semi-axes is above a threshold, a bit value 1 will be assigned to the centroid of that region as shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.2 Image registration, processing and matrix retrieval ‣ 3.2 Watermark detector ‣ 3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.14" class="ltx_p">As a final step, we need to extract the information matrix <math id="S3.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS2.p4.1.m1.1a"><mi id="S3.SS2.SSS2.p4.1.m1.1.1" xref="S3.SS2.SSS2.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.1.m1.1b"><ci id="S3.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.1.m1.1c">M</annotation></semantics></math> from the coordinates of the regionprops centroids that have been assigned bit values 1. Ideally, there would exist only <math id="S3.SS2.SSS2.p4.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.SSS2.p4.2.m2.1a"><mi id="S3.SS2.SSS2.p4.2.m2.1.1" xref="S3.SS2.SSS2.p4.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.2.m2.1b"><ci id="S3.SS2.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.2.m2.1c">m</annotation></semantics></math> (the number of rows and columns of <math id="S3.SS2.SSS2.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS2.SSS2.p4.3.m3.1a"><mi id="S3.SS2.SSS2.p4.3.m3.1.1" xref="S3.SS2.SSS2.p4.3.m3.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.3.m3.1b"><ci id="S3.SS2.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.3.m3.1c">\mathbf{M}</annotation></semantics></math>) unique values for the <math id="S3.SS2.SSS2.p4.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS2.p4.4.m4.1a"><mi id="S3.SS2.SSS2.p4.4.m4.1.1" xref="S3.SS2.SSS2.p4.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.4.m4.1b"><ci id="S3.SS2.SSS2.p4.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.4.m4.1c">x</annotation></semantics></math> and <math id="S3.SS2.SSS2.p4.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.SSS2.p4.5.m5.1a"><mi id="S3.SS2.SSS2.p4.5.m5.1.1" xref="S3.SS2.SSS2.p4.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.5.m5.1b"><ci id="S3.SS2.SSS2.p4.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p4.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.5.m5.1c">y</annotation></semantics></math> coordinates, however, due to imperfection in the previous steps, the coordinates have biases. Instead, we use <math id="S3.SS2.SSS2.p4.6.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS2.p4.6.m6.1a"><mi id="S3.SS2.SSS2.p4.6.m6.1.1" xref="S3.SS2.SSS2.p4.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.6.m6.1b"><ci id="S3.SS2.SSS2.p4.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p4.6.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.6.m6.1c">K</annotation></semantics></math>-means clustering on the <math id="S3.SS2.SSS2.p4.7.m7.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS2.p4.7.m7.1a"><mi id="S3.SS2.SSS2.p4.7.m7.1.1" xref="S3.SS2.SSS2.p4.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.7.m7.1b"><ci id="S3.SS2.SSS2.p4.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p4.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.7.m7.1c">x</annotation></semantics></math> and the <math id="S3.SS2.SSS2.p4.8.m8.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.SSS2.p4.8.m8.1a"><mi id="S3.SS2.SSS2.p4.8.m8.1.1" xref="S3.SS2.SSS2.p4.8.m8.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.8.m8.1b"><ci id="S3.SS2.SSS2.p4.8.m8.1.1.cmml" xref="S3.SS2.SSS2.p4.8.m8.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.8.m8.1c">y</annotation></semantics></math> coordinates of the centroids, respectively. We set the number of clusters as <math id="S3.SS2.SSS2.p4.9.m9.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.SSS2.p4.9.m9.1a"><mi id="S3.SS2.SSS2.p4.9.m9.1.1" xref="S3.SS2.SSS2.p4.9.m9.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.9.m9.1b"><ci id="S3.SS2.SSS2.p4.9.m9.1.1.cmml" xref="S3.SS2.SSS2.p4.9.m9.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.9.m9.1c">m</annotation></semantics></math> and rank the <math id="S3.SS2.SSS2.p4.10.m10.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.SSS2.p4.10.m10.1a"><mi id="S3.SS2.SSS2.p4.10.m10.1.1" xref="S3.SS2.SSS2.p4.10.m10.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.10.m10.1b"><ci id="S3.SS2.SSS2.p4.10.m10.1.1.cmml" xref="S3.SS2.SSS2.p4.10.m10.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.10.m10.1c">m</annotation></semantics></math> cluster centers. The point falling into the <math id="S3.SS2.SSS2.p4.11.m11.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS2.p4.11.m11.1a"><mi id="S3.SS2.SSS2.p4.11.m11.1.1" xref="S3.SS2.SSS2.p4.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.11.m11.1b"><ci id="S3.SS2.SSS2.p4.11.m11.1.1.cmml" xref="S3.SS2.SSS2.p4.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.11.m11.1c">i</annotation></semantics></math>-th and <math id="S3.SS2.SSS2.p4.12.m12.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS2.p4.12.m12.1a"><mi id="S3.SS2.SSS2.p4.12.m12.1.1" xref="S3.SS2.SSS2.p4.12.m12.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.12.m12.1b"><ci id="S3.SS2.SSS2.p4.12.m12.1.1.cmml" xref="S3.SS2.SSS2.p4.12.m12.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.12.m12.1c">j</annotation></semantics></math>-th clusters, respectively, will correspond to the <math id="S3.SS2.SSS2.p4.13.m13.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S3.SS2.SSS2.p4.13.m13.2a"><mrow id="S3.SS2.SSS2.p4.13.m13.2.3.2" xref="S3.SS2.SSS2.p4.13.m13.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p4.13.m13.2.3.2.1" xref="S3.SS2.SSS2.p4.13.m13.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS2.p4.13.m13.1.1" xref="S3.SS2.SSS2.p4.13.m13.1.1.cmml">i</mi><mo id="S3.SS2.SSS2.p4.13.m13.2.3.2.2" xref="S3.SS2.SSS2.p4.13.m13.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p4.13.m13.2.2" xref="S3.SS2.SSS2.p4.13.m13.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS2.SSS2.p4.13.m13.2.3.2.3" xref="S3.SS2.SSS2.p4.13.m13.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.13.m13.2b"><interval closure="open" id="S3.SS2.SSS2.p4.13.m13.2.3.1.cmml" xref="S3.SS2.SSS2.p4.13.m13.2.3.2"><ci id="S3.SS2.SSS2.p4.13.m13.1.1.cmml" xref="S3.SS2.SSS2.p4.13.m13.1.1">𝑖</ci><ci id="S3.SS2.SSS2.p4.13.m13.2.2.cmml" xref="S3.SS2.SSS2.p4.13.m13.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.13.m13.2c">(i,j)</annotation></semantics></math> entry of the matrix <math id="S3.SS2.SSS2.p4.14.m14.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS2.SSS2.p4.14.m14.1a"><mi id="S3.SS2.SSS2.p4.14.m14.1.1" xref="S3.SS2.SSS2.p4.14.m14.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.14.m14.1b"><ci id="S3.SS2.SSS2.p4.14.m14.1.1.cmml" xref="S3.SS2.SSS2.p4.14.m14.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.14.m14.1c">\mathbf{M}</annotation></semantics></math>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/x1.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="159" height="159" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/prediction1.jpg" id="S3.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/2_registration.jpg" id="S3.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1905.09706/assets/2_matrix.jpg" id="S3.F4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The test image (left1); the confidence map outputted by CNN-3DW (left2); the binary confidence map obtained after regularisation followed by thresholding (left3); the detected centroids of bit value 1 regions (left4)</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/1905.09706/assets/CNN-3DW.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The architecture of the proposed CNN-3WD.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To assess the effectiveness of training the neural network model with synthetic data, we conducted a series of experiments and analyzed the results, evaluating the performance of proposed technique in various situations.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training datasets: 3DW-real and 3DW-syn</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For our 3DW-syn dataset, we generated 2K images each one carrying a randomly generated <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="20\times 20" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">20\times 20</annotation></semantics></math> watermark. As described in the section <a href="#S3" title="3 Method ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the model of the watermarked object is assigned a random color and texture, scenes are rendered under random camera poses and illuminated from light sources of various types. For comparison, we used a hand-labelled dataset (3DW-real) of 255 images of watermarked 3D printed objects <span id="S4.SS1.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">?<span id="S4.SS1.p1.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. Although we use more synthetic than real images, the cost of creating the 3DW-syn dataset was lower, and moreover 3DW-real was augmented in more ways than 3DW-syn. In particular, we applied random crop, brightness and contrast setting to both datasets, while 3DW-real was also augmented with random flips and random resizing. In all cases, training was stopped when the validation error was stable and the best weights were saved.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For testing, we used 35 images (Test_A) which are taken from same 3D printed models used in 3DW-real, but with different illumination conditions and camera poses, and 20 images (Test_B) taken from other 3D printed models made from materials that were not used in the 3DW-real dataset. Detection performance was evaluated using recall TPR=TP/(TP+FN) and precision PPV=TP/(TP+FP), where in our case TP and FN are the numbers of value 1 bits retrieved correctly and correspondingly incorrectly, while FP is the number of value 0 bits retrieved incorrectly.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate the watermark retrieval capabilities of the proposed technique under the use of various training datasets, and provide an ablation study to further explore the effectiveness of using synthetic data.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Watermark retrieval</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We evaluate the performance of the proposed framework on three scenarios: neural network model trained with real data (3DW-real) only, synthetic data (3DW-syn) only, and the combined dataset (3DW-real + 3DW-syn). Precision and recall are tested separately on Test_A and Test_B and the results are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.2.1 Watermark retrieval ‣ 4.2 Experimental Setup ‣ 4 Experiment ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Note that Test_A consists of images from the same 3D objects as the images of the (3DW-real) training dataset, while Test_B contains images from different objects.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Figure <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">6</span></span> shows indicative outputs of the CNN-3DW on the Test_A and Test_B dataset, after being trained on either of the three training datasets we used. Notice that even though our network has never seen a real image when trained on (3DW-syn) only, it is able to successfully detect most watermark bumps. Moreover, when the network was trained on the combined (3DW-real + 3DW-syn), on the most challenging Test_B it outperformed the network trained on (3DW-real) by a 15% in the recall value and 5% in precision. This surprising result illustrates the power of such a simple technique for bridging the reality gap, and rectifying the human and systematic error disadvantage in manual labeling.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Next, we study the effect of fine-tuning <span id="S4.SS2.SSS1.p3.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S4.SS2.SSS1.p3.1.2" class="ltx_text ltx_font_bold">?<span id="S4.SS2.SSS1.p3.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> the number of real images we add to (3DW-syn) to create a combined training dataset. For fine-tuning, the gradient was allowed to fully flow from end-to-end, the other parameters kept unchanged and the CNN-3DW was trained until convergence. Results with varying percentages of (3DW-real) added to the training dataset are shown in Fig <a href="#S4.F7" title="Figure 7 ‣ 4.2.1 Watermark retrieval ‣ 4.2 Experimental Setup ‣ 4 Experiment ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We note that when a combined dataset is used, recall and precision increase with the number of real-images included. However, when less than 70% of (3DW-real) is included the recall is lower compared to training with (3DW-syn) only. We hypothesize that when (3DW-syn) is mixed with few real images only, the real images confuse the neural network, preventing the learning of features from (3DW-syn).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:138.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.9pt,-24.2pt) scale(1.53887050946928,1.53887050946928) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Test_A</th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Test_B</th>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Recall</th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Precision</th>
<th id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Recall</th>
<th id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Precision</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Real-world dataset</th>
<td id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.1.2.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.1.3.1" class="ltx_text ltx_font_bold">0.89</span></td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.57</td>
<td id="S4.T1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.75</td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Synthetic dataset</th>
<td id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_center">0.65</td>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_center">0.53</td>
<td id="S4.T1.1.1.4.2.4" class="ltx_td ltx_align_center">0.62</td>
<td id="S4.T1.1.1.4.2.5" class="ltx_td ltx_align_center">0.55</td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Combination</th>
<td id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.74</td>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.83</td>
<td id="S4.T1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.5.3.4.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S4.T1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.5.3.5.1" class="ltx_text ltx_font_bold">0.80</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison among training datasets: real-world dataset, synthetic dataset and their combination, on Test_A and Test_B.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1905.09706/assets/F6.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Confidence maps of images in Test_A and Test_B using CNN-3DW trained on: real-world dataset (left2), synthetic dataset (left3) and their combination (left4).</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/1905.09706/assets/fine-tuning.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Percent of real images used in fine-tuning.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Ablation study</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">To study the effect of individual DR parameters we conducted an ablation study. In all cases we train with synthetic data only and since no real images are used we can safely use the larger Test_A for testing.
Figure <a href="#S4.F8" title="Figure 8 ‣ Data augmentation. ‣ 4.2.2 Ablation study ‣ 4.2 Experimental Setup ‣ 4 Experiment ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the results of controlling individual components of the DR data generation procedure as described in more detail below.</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training dataset size.</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.1" class="ltx_p">In this experiment, we train with a percentage only of (3DW-syn), keeping all other parameters unchanged, and we study the effect of the training dataset size upon performance. The results show that recall and precision increase with the size of the dataset, but after a certain level recall and precision rise little.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Texture.</h5>

<div id="S4.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p1.1" class="ltx_p">In this experiment, keeping other parameters unchanged, we create two sets of 1.5K synthetic images each, one utilizing half only of the available texture types, the other utilizing the full set of available textures. When the textures types applied on the watermark bumps were limited, recall and precision dropped to 0.49 and 0.45.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data augmentation.</h5>

<div id="S4.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px3.p1.1" class="ltx_p">In this experiment, during training with the full (3DW-syn), we turned off image brightness and contrast data augmentation. The recall falls to 0.58 while the precision barely drops by 0.02. Note that, in this case, the effect of lighting adjustments on already generated images is smaller than the effect of a changing a simulator parameter such as the number of texture types available.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/1905.09706/assets/ablation.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="360" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The effect of controlling individual components of the DR data generation procedure.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Training strategies</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Finally, we studied the effect of pretrained weights upon the performance of CNN-3DW. In this experiment, we initialized the weights of the CNN-3DW network using random weights, the weights of the (3DW-real) trained network, and the weights of the (3DW-syn) trained network, respectively. In all cases, we retrained on (3DW-real + 3DW-syn) and tested on Test_B. The results are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.2.3 Training strategies ‣ 4.2 Experimental Setup ‣ 4 Experiment ‣ Watermark retrieval from 3D printed objects via synthetic data training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">First, note that the (3DW-real) weights achieve recall and precision of only 0.58 and 0.69 respectively, showing that the real-image weights are not very useful for training a network to operate on the Test_B dataset. This is, in fact, a significant challenge with networks today, namely, that they often fail to transfer from one dataset to another. In contrast, the (3DW-syn) weights achieve better recall and precision rates, indicating that synthetic data enable the network to learn features which are invariant to the specific dataset. Finally, the best rates are achieved with random weights, i.e. when synthetic DR data are mixed with real-image data and train the network without pretrain. Indeed, training with synthetic data only produces inferior results, while training with real data only does cope well with variabiliy in parameters such as printing materials.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:69.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(48.6pt,-7.8pt) scale(1.28904206828289,1.28904206828289) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Random weights</th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(3DW-real) weights</th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(3DW-syn) weights</th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<td id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Recall</td>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.2.2.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.58</td>
<td id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">0.61</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<td id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_bb">Precision</td>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.3.3.2.1" class="ltx_text ltx_font_bold">0.80</span></td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.69</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.75</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Training on (3DW-real + 3DW-syn) with three sets of pretrained weights: random and obtained by training on (3DW-real) and (3DW-syn), respectively. Testing was done on Test_B.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We showed that domain randomization is an effective technique in the context of watermark retrieval from 3D printed objects. Using synthetic DR data, intentionally disclaiming photorealism to force the neural network focus on the most relevant features, we trained the neural network (CNN-3DW) to generate confidence maps for the locations of the watermark bumps. When synthetic DR data were mixed with real image data the retrieval rates were higher than using real or synthetic data alone.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">While the process of inferring information about 3D objects from 2D images of them would always be subject to certain inherent limitations, in some applications recourse to the exact CAD models of those 3D objects can facilitate computer vision tasks. In particular, the annotation of the synthetic images of our training dataset is automatic, eliminating the human error and cost of a hand-labelling, and performed in the 3D domain, eliminating a systematic error that is unavoidable in certain situations where information about 3D objects is directly annotated on 2D images. In the future, we plan to test the technique on more general problems of object recognition, object location and counting problems, working with more complex instances of 3D printed objects and their corresponding CAD models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx1.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Atapour-Abarghouei and Breckon, 2018<span id="bib.bibx1.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Atapour-Abarghouei, A. and Breckon, T. P. (2018).

</span>
<span class="ltx_bibblock">Real-time monocular depth estimation using synthetic data with domain
adaptation via image style transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.3.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, volume 18, page 1.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx2.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Barron et al., 1994<span id="bib.bibx2.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Barron, J. L., Fleet, D. J., and Beauchemin, S. S. (1994).

</span>
<span class="ltx_bibblock">Performance of optical flow techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.3.1" class="ltx_text ltx_font_italic">IJCV</span>, 12(1):43–77.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx3.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Bors, 2006<span id="bib.bibx3.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Bors, A. G. (2006).

</span>
<span class="ltx_bibblock">Watermarking mesh-based representations of 3-d objects using local
moments.

</span>
<span class="ltx_bibblock"><span id="bib.bibx3.3.1" class="ltx_text ltx_font_italic">TIP</span>, 15(3):687–701.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx4.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Brown, 1992<span id="bib.bibx4.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Brown, L. G. (1992).

</span>
<span class="ltx_bibblock">A survey of image registration techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bibx4.3.1" class="ltx_text ltx_font_italic">CSUR</span>, 24(4):325–376.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx5.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Dosovitskiy et al., 2015<span id="bib.bibx5.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V.,
Van Der Smagt, P., Cremers, D., and Brox, T. (2015).

</span>
<span class="ltx_bibblock">Flownet: Learning optical flow with convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.3.1" class="ltx_text ltx_font_italic">Proc. ICCV</span>, pages 2758–2766.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx6.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Gupta et al., 2016<span id="bib.bibx6.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Gupta, A., Vedaldi, A., and Zisserman, A. (2016).

</span>
<span class="ltx_bibblock">Synthetic data for text localisation in natural images.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.3.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, pages 2315–2324.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx7.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Handa et al., 2016<span id="bib.bibx7.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Handa, A., Patraucean, V., Badrinarayanan, V., Stent, S., and Cipolla, R.
(2016).

</span>
<span class="ltx_bibblock">Understanding real world indoor scenes with synthetic data.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx7.3.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, pages 4077–4085.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx8.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hou et al., 2018<span id="bib.bibx8.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Hou, J. U., Kim, D. G., Ahn, W. H., and Lee, H. K. (2018).

</span>
<span class="ltx_bibblock">Copyright protections of digital content in the age of 3d printer:
Emerging issues and survey.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.3.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 6:44082–44093.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx9.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hou et al., 2015<span id="bib.bibx9.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Hou, J. U., Kim, D. G., Choi, S., and Lee, H. K. (2015).

</span>
<span class="ltx_bibblock">3d print-scan resilient watermarking using a histogram-based circular
shift coding structure.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx9.3.1" class="ltx_text ltx_font_italic">Proc. IH &amp; MMSec</span>, pages 115–121. ACM.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx10.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hou et al., 2017<span id="bib.bibx10.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Hou, J. U., Kim, D. G., and Lee, H. K. (2017).

</span>
<span class="ltx_bibblock">Blind 3D mesh watermarking for 3D printed model by analyzing
layering artifact.

</span>
<span class="ltx_bibblock"><span id="bib.bibx10.3.1" class="ltx_text ltx_font_italic">IEEE Tr. on Inf. Forensics and Security</span>, 12(11):2712–2725.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx11.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Jaderberg et al., 2014<span id="bib.bibx11.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. (2014).

</span>
<span class="ltx_bibblock">Synthetic data and artificial neural networks for natural scene text
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx11.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.2227</span>.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx12.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kingma and Ba, 2014<span id="bib.bibx12.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Kingma, D. P. and Ba, J. (2014).

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx13.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Krizhevsky et al., 2012<span id="bib.bibx13.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.3.1" class="ltx_text ltx_font_italic">NIPS</span>, pages 1097–1105.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx14.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Luo and Bors, 2011<span id="bib.bibx14.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Luo, M. and Bors, A. G. (2011).

</span>
<span class="ltx_bibblock">Surface-preserving robust watermarking of 3-d shapes.

</span>
<span class="ltx_bibblock"><span id="bib.bibx14.3.1" class="ltx_text ltx_font_italic">TIP</span>, 20(10):2813–2826.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx15.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Macq et al., 2015<span id="bib.bibx15.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Macq, B., Alface, P. R., and Montanola, M. (2015).

</span>
<span class="ltx_bibblock">Applicability of watermarking for intellectual property rights
protection in a 3d printing scenario.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.3.1" class="ltx_text ltx_font_italic">Proc. Web3D</span>, pages 89–95. ACM.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx16.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Mundhenk et al., 2016<span id="bib.bibx16.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Mundhenk, T. N., Konjevod, G., Sakla, W. A., and Boakye, K. (2016).

</span>
<span class="ltx_bibblock">A large contextual dataset for classification, detection and counting
of cars with deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx16.3.1" class="ltx_text ltx_font_italic">ECCV</span>, pages 785–800. Springer.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx17.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Ohbuchi et al., 2002<span id="bib.bibx17.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ohbuchi, R., Mukaiyama, A., and Takahashi, S. (2002).

</span>
<span class="ltx_bibblock">A frequency-domain approach to watermarking 3d shapes.

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.3.1" class="ltx_text ltx_font_italic">Computer Graphics Forum</span>, 21(3):373–382.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx18.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Ojala et al., 2000<span id="bib.bibx18.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ojala, T., Pietikäinen, M., and Mäenpää, T. (2000).

</span>
<span class="ltx_bibblock">Gray scale and rotation invariant texture classification with local
binary patterns.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx18.3.1" class="ltx_text ltx_font_italic">ECCV</span>, pages 404–420. Springer.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx19.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Otsu, 1979<span id="bib.bibx19.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Otsu, N. (1979).

</span>
<span class="ltx_bibblock">A threshold selection method from gray-level histograms.

</span>
<span class="ltx_bibblock"><span id="bib.bibx19.3.1" class="ltx_text ltx_font_italic">IEEE transactions on systems, man, and cybernetics</span>,
9(1):62–66.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx20.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Pov-Ray, 2018<span id="bib.bibx20.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Pov-Ray (2018).

</span>
<span class="ltx_bibblock">POV-Raypersistence of vision raytracer.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.povray.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.povray.org/</a>.

</span>
<span class="ltx_bibblock">Accessed on 07.10.2018.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx21.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Qiu and Yuille, 2016<span id="bib.bibx21.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Qiu, W. and Yuille, A. (2016).

</span>
<span class="ltx_bibblock">Unrealcv: Connecting computer vision to unreal engine.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx21.3.1" class="ltx_text ltx_font_italic">Proc. ECCV</span>, pages 909–916. Springer.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx22.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Simonyan and Zisserman, 2014<span id="bib.bibx22.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A. (2014).

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx22.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx23.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Szegedy et al., 2015<span id="bib.bibx23.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., and Rabinovich, A. (2015).

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx23.3.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, pages 1–9.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx24.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tobin et al., 2017<span id="bib.bibx24.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
(2017).

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from
simulation to the real world.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.3.1" class="ltx_text ltx_font_italic">IROS</span>, pages 23–30. IEEE.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx25.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tremblay et al., 2018<span id="bib.bibx25.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To,
T., Cameracci, E., Boochoon, S., and Birchfield, S. (2018).

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.3.1" class="ltx_text ltx_font_italic">Proc. CVPRW</span>, pages 969–977.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx26.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Yosinski et al., 2014<span id="bib.bibx26.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014).

</span>
<span class="ltx_bibblock">How transferable are features in deep neural networks?

</span>
<span class="ltx_bibblock">In <span id="bib.bibx26.3.1" class="ltx_text ltx_font_italic">NIPS</span>, pages 3320–3328.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx27.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zhang et al., 2018<span id="bib.bibx27.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Zhang, X., Wang, Q., Breckon, T., and Ivrissimtzis, I. (2018).

</span>
<span class="ltx_bibblock">Watermark retrieval from 3d printed objects via convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bibx27.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.07640</span>.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx28.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zhang et al., 2016a<span id="bib.bibx28.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Zhang, Y., Qiu, W., Chen, Q., Hu, X., and Yuille, A. (2016a).

</span>
<span class="ltx_bibblock">Unrealstereo: A synthetic dataset for analyzing stereo vision.

</span>
<span class="ltx_bibblock"><span id="bib.bibx28.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.04647</span>.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx29.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zhang et al., 2016b<span id="bib.bibx29.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Zhang, Y., Zhou, D., Chen, S., Gao, S., and Ma, Y. (2016b).

</span>
<span class="ltx_bibblock">Single-image crowd counting via multi-column convolutional neural
network.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx29.3.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, pages 589–597.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1905.09705" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1905.09706" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1905.09706">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1905.09706" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1905.09707" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 17:30:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
