<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.12181] Security and Privacy Issues of Federated Learning</title><meta property="og:description" content="Federated Learning (FL) has emerged as a promising approach to address data privacy and confidentiality concerns by allowing multiple participants to construct a shared model without centralizing sensitive data. Howeve…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Security and Privacy Issues of Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Security and Privacy Issues of Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.12181">

<!--Generated on Wed Feb 28 16:32:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  Data privacy and confidentiality,  Machine learning,  Security
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Security and Privacy Issues of Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jahid Hasan 
 
<br class="ltx_break">
</span><span class="ltx_author_notes">Jahid Hasan, Department of Computer Science, Iowa State University, Ames, IA 50011. E-mail: jhasan@iastate.edu</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL) has emerged as a promising approach to address data privacy and confidentiality concerns by allowing multiple participants to construct a shared model without centralizing sensitive data. However, this decentralized paradigm introduces new security challenges, necessitating a comprehensive identification and classification of potential risks to ensure FL’s security guarantees. This paper presents a comprehensive taxonomy of security and privacy challenges in Federated Learning (FL) across various machine learning models, including large language models. We specifically categorize attacks performed by the aggregator and participants, focusing on poisoning attacks, backdoor attacks, membership inference attacks, generative adversarial network (GAN) based attacks, and differential privacy attacks. Additionally, we propose new directions for future research, seeking innovative solutions to fortify FL systems against emerging security risks and uphold sensitive data confidentiality in distributed learning environments.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, Data privacy and confidentiality, Machine learning, Security

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, Federated Learning (FL) has emerged as a promising and transformative paradigm for addressing data privacy and confidentiality concerns in the realm of machine learning. Unlike traditional centralized approaches, FL allows multiple participants to collaboratively construct a shared model without the need to centralize sensitive data. By empowering individual devices or entities to train models locally and share only model updates with a central aggregator, FL offers a privacy-preserving alternative for harnessing the collective intelligence of distributed data sources.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The decentralized nature of FL brings forth a new set of security challenges that demand rigorous investigation and mitigation strategies. Federated Learning (FL) has emerged as a revolutionary approach to uphold user privacy by distributing data from the central server to individual devices, empowering various domains with sensitive data and diverse characteristics to benefit from AI advancements. This novel paradigm gained prominence for two primary reasons<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>: Firstly, it addresses the challenge of inadequate centralized data access in traditional machine learning. Due to direct access restrictions, certain data cannot be stored on the central server. And secondly, it ensures data privacy protection by utilizing local data from edge devices, such as clients, instead of transmitting sensitive information to the server. This way, the network’s asynchronous communication comes into play, preserving the confidentiality of data. By safeguarding data privacy, federated learning allows for the efficient utilization of AI benefits across multiple domains through machine learning models. As participants operate independently, there is an inherent risk of potential security threats that may undermine the integrity of the shared model or compromise the privacy of individual participants’ data. To ensure the viability and reliability of FL, it is imperative to comprehensively identify, categorize, and address these security and privacy issues.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In response to this need, this paper comprehensively explores security and privacy challenges in the context of Federated Learning. The main goal is to provide a comprehensive taxonomy of potential risks that may arise from both the aggregator and participating entities. By categorizing attacks into distinct classes, including poisoning attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, backdoor attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, membership inference attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, generative adversarial network (GAN) based attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and differential privacy attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we aim to shed light on the diverse array of threats faced by FL systems. The scope of this study encompasses various machine learning models, spanning from conventional algorithms to cutting-edge large language models. As FL finds applicability in various domains, such as healthcare, finance, and the Internet of Things (IoT), understanding and addressing the unique security and privacy challenges becomes even more crucial. As part of this investigation, we delve into the methodologies employed by malicious entities to compromise FL systems and intrude upon the privacy of participants’ data. Furthermore, we discuss primary mitigation techniques that have shown promise in countering these security risks and upholding data confidentiality. Notably, we explore the integration of blockchain and Trusted Execution Environments as potential solutions to reinforce the security of FL systems.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper endeavors to present a comprehensive overview of the security and privacy landscape of Federated Learning. By identifying existing threats and highlighting future research directions, we aim to contribute to the ongoing efforts to fortify FL systems against emerging security risks and maintain the utmost protection of sensitive data in distributed learning environments. As FL continues to evolve, this study seeks to foster a more secure and privacy-conscious foundation for this promising approach to machine learning.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper is organized as follows: Section 2 delves into the fundamentals of Federated Learning (FL) and its underlying mechanism, ensuring data privacy and confidentiality. Moving on to Section 3, we explore the various Security and Privacy Challenges associated with FL, analyzing numerous potential attacks on this novel approach. Section 4 presents potential solutions to counteract these attacks and discusses defensive measures to bolster FL’s security and privacy. Section 6 explores related works in this domain, shedding light on previous research and advancements. Finally, in Section 7, we draw the paper to a conclusion, outlining the future directions for enhancing security and privacy in Federated Learning.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Federated Learning Concepts</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Federated Learning is a decentralized machine learning paradigm that facilitates training models across multiple devices while keeping the data on those devices, ensuring user privacy. Instead of sending raw data to a central server for training, FL allows devices, such as smartphones, edge servers, or Internet of Things (IoT) devices, to collaboratively learn from local data while keeping the data localized and secure.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2307.12181/assets/pdf/dd.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="349" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Various Attack Models Within the FL Framework</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Data Privacy and Confidentiality</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">One of the primary motivations behind Federated Learning is to preserve data privacy and confidentiality. In traditional centralized machine learning, sensitive data is often collected and stored on a central server, raising concerns about unauthorized access and potential data breaches. In FL, data remains on the devices where it originates, and only model updates or aggregated information is transmitted to the central server. This approach significantly reduces the risk of exposing raw, sensitive data to potential adversaries.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Security and Privacy Challenges of FL</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Federated Learning presents a novel approach to training models while preserving user privacy, but it also introduces several security and privacy challenges that demand attention. In this section, we discuss some of the critical challenges FL faces, shown in Figure 1:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Poisoning Attacks:</span> Poisoning attacks, also known as data poisoning attacks, involve adversaries injecting malicious data into the local training datasets of participating devices. These adversarial samples can skew the model’s learning process, leading to biased or compromised global models when the updates are aggregated. Robust defenses are essential to detect and mitigate the impact of poisoning attacks on FL.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Backdoor Attacks:</span> Backdoor attacks aim to create a ”backdoor” in the model, allowing an attacker to trigger specific behavior or misclassification when presented with specific input patterns. These backdoors are often injected during training and can pose a significant threat, particularly in scenarios where models are shared across multiple devices and users.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">Membership Inference Attacks:</span> Membership inference attacks focus on inferring whether specific data samples were part of the training dataset used to create the global model. Successful membership inference attacks can compromise user privacy by revealing sensitive information about the data contributors. Developing robust mechanisms to prevent such inference is crucial for maintaining data privacy.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_bold">Generative Adversarial Network (GAN) Based Attacks:</span> Generative Adversarial Networks (GANs) can be leveraged by adversaries to generate synthetic data that closely mimics real data distribution. These synthetic samples may then attack the FL system, potentially leading to data leakage or model manipulation. Detecting and countering GAN-based attacks is a critical challenge.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_bold">Differential Privacy Attacks:</span> Differential privacy is a key technique used to protect individual data privacy in FL. However, FL systems are not immune to differential privacy attacks, where attackers attempt to reverse-engineer the presence of specific data points or learn sensitive information from the differentially private model updates. Strengthening the privacy guarantees against such attacks is essential.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p">Addressing these security and privacy challenges is vital to ensure the success and widespread adoption of Federated Learning. Robust defense mechanisms, privacy-preserving techniques, and continuous research efforts are needed to enhance the security posture of FL systems and protect user data and privacy. As we move forward, exploring innovative solutions and adopting a proactive approach to security will be instrumental in making FL a reliable and privacy-conscious framework for collaborative machine learning.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Security and Privacy Challenges</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Poisoning Attacks</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">A significant and concerning attack prevalent in Federated Learning (FL) context is known as poisoning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Due to the decentralized nature of FL, where each client possesses its training data, the risk of incorporating tampered data weights into the global ML model becomes substantial. This poisoning attack can occur during the training phase and potentially impact both the local models and, consequently, the overall performance and accuracy of the global ML model. In FL, model updates are aggregated from a large group of clients, making the probability of poisoning attacks from one or more clients’ training data quite high. Consequently, the threat posed by poisoning attacks is severe. These attacks specifically target various stages and components within the FL process. Below, we provide a concise overview of the different classifications of poisoning attacks:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Data Poisoning Attack:</span>
The inception of data poisoning attacks against machine learning algorithms dates back to the seminal work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where the researchers introduced exploiting the vulnerabilities of support vector machines by incorporating malicious data points during the training phase to maximize classification errors.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Since then, various approaches have been proposed to counter data poisoning attacks in machine learning algorithms under various settings, including centralized and distributed environments. In Federated Learning (FL), where clients actively participate in the training process by contributing data and sending model parameters to the server, the risk of malicious clients poisoning the global model becomes evident. Data poisoning in FL refers to generating tainted samples to train the global model, aiming to produce falsified model parameters and transmit them to the server.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Another related aspect is data injection, which can be considered a subcategory of data poisoning. In this scenario, a malicious client may inject tainted data into the local model processing, potentially gaining control over multiple clients’ local models and manipulating the global model with their maliciously crafted data. These works collectively highlight the importance of developing robust defense mechanisms to safeguard against data poisoning attacks in the ever-evolving landscape of machine learning algorithms and distributed learning settings.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Figure 2 demonstrates the impact of malicious clients on a CNN system involving 15 clients. These malicious clients upload fake parameter values during each communication round, which can be either opposite to the true value or random numbers within [-1, 1]. The results reveal that malicious clients adversely affect the system’s performance. Moreover, as malicious clients increase, the system’s reliability significantly diminishes, eventually leading to failure.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Model Poisoning Attack:</span>
In model poisoning attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the malicious party can modify the model updates directly before sending them to the central server for aggregation. This enables them to inject malicious parameters into the global model, poisoning its integrity and functionality. The larger the scale of the FL system with numerous clients, the higher the potential effectiveness of model poisoning attacks.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.12181/assets/pdf/acc.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="349" height="331" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Performance Comparison with Varying Number of Malicious Clients.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Backdoor Attacks</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In machine learning security and privacy, transparent attacks like poisoning and inference attacks are known entities. However, lurking in the shadows is a more insidious threat known as backdoor attacks. Unlike their transparent counterparts, backdoor attacks cleverly inject a malicious task into an existing model while preserving its accuracy for the genuine task. This cloak-and-dagger approach makes them difficult to detect promptly, as they may not immediately impact the performance of the original ML task.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the authors experiment with and demonstrate the implementation of backdoor attacks. To mitigate these risks,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> propose model pruning and fine-tuning as potential solutions. However, the severity of backdoor attacks remains high, as their occurrence often goes unnoticed for significant periods, allowing the attacker to maintain covert control. Backdoor attacks can significantly confuse ML models and confidently predict false positives. Trojan threats<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> represent a similar category of backdoor attacks, where the attacker aims to maintain the ML model’s primary task while performing malicious actions in stealth mode.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">In federated learning, backdoor attacks have emerged as potential security threats. The main objective of a backdoor attack in FL is to manipulate local models to compromise the global model. In such attacks, the attacker introduces triggers into one or more local models, causing the global model to exhibit specific behaviors under the presence of these triggers in the inputs. For instance, in autonomous driving<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, an attacker may deploy a backdoored street sign detector that excels at identifying street signs under normal conditions but erroneously identifies stop signs with specific stickers as speed limit signs.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Membership Inference Attacks</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Membership Inference attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> aims to extract information by determining if specific data points are present in a model’s training set. The attacker exploits the global model to gain insights into the training data of other users, potentially compromising their privacy and security.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Training Data Inference Attacks:</span> Membership inference attacks refer to techniques that attempt to deduce details about the training data of a machine learning model. By exploiting the global model, attackers seek to ascertain whether specific data points were used during the model’s training. These attacks rely on guesswork and training a predictive model to infer the original training data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Inference Attacks on Training Data:</span> These attacks uncover information about the training data used to build a machine-learning model. By manipulating the global model, attackers aim to determine the presence or absence of certain data points in the training set. Employing various techniques, they construct predictive models to make educated guesses about the original training data.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Training Data Reconnaissance Attacks:</span> Membership inference attacks are a form of survey aimed at gaining insights into the training data of a machine learning model. Exploiting the global model, attackers attempt to discern whether specific data instances were part of the model’s training set. By training their predictive models, attackers employ educated guesswork to infer details about the original training data. Researchers have demonstrated the potential risks of memorizing neural networks’ training data, exposing them to passive and active inference attacks.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Data Set Inference via Model Exploitation:</span> These attacks involve exploiting a machine learning model to infer details about the training data. Attackers use the global model to check for the presence of specific data in the training set. By training their predictive models, they attempt to deduce information about the original training data through educated guesses.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Training Data Guessing Attacks:</span> Membership inference attacks are akin to ”guessing” the training data used to train a machine learning model. By leveraging the global model, attackers attempt to deduce whether particular data points were present in the training set. Employing various predictive modeling techniques, they try to infer details about the original training data through educated guesses.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Generative Adversarial Network (GAN) Based Attacks</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In the context of security and privacy challenges, the emergence of Generative Adversarial Networks (GANs) poses a new and potent threat. GANs, a powerful development in Deep Learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, continues to be actively researched and refined<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Their primary objective is to generate synthetic samples that closely resemble the distribution of the original training data, even without direct access to the original samples.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The GAN framework sets up a competitive game between two deep learning networks: generative and discriminative networks, akin to game theory. The generative network produces realistic samples, while the discriminative network aims to differentiate between real data samples and those generated by the GAN.
Initially applied to image datasets, GAN attacks have since shown a potential to be extended to diverse types of data, including sensitive records like demographic data. This raises significant concerns regarding privacy and security implications. Malicious agents can exploit GANs to create synthetic data that resembles genuine data, possibly leading to privacy breaches, data falsification, and adversarial manipulation of machine learning models.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Attacks on Client Edge:</span>
The GAN-generated samples aim to closely imitate the distribution of the original training data. Applying record-level differential privacy noise, a technique previously suggested for privacy protection proves ineffective against GAN-based attacks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">The attack primarily relies on an active insider who operates under a white-box access model, gaining access to and using internal model parameters. The attacker participates in the federated deep learning protocol as an honest client but endeavors to extract information about a class of data they do not own (owned by the victim client). Through this active attack, the adversary influences the learning process to force the victim into releasing further details about the targeted class.
However, client-side GAN-based attacks have three main limitations: first, they require altering the distributed model’s architecture to introduce adversarial influence in the learning process; second, the adversarial influence introduced by the malicious client may become insignificant after several iterations of the process; and third, the attack can only imitate input data for training rather than replicating exact samples from the victim side<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Differential Privacy Attacks</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Security and privacy challenges in federated learning have prompted the widespread adoption of Differential Privacy (DP), a popular technique in industry and academia. DP’s core concept revolves around preserving individual privacy by introducing noise to sensitive attributes, ensuring each user’s data remains protected. Despite the addition of noise, the loss of statistical data quality is relatively low compared to the enhanced privacy protection. In federated learning, DP is a crucial defense against inverse data retrieval. By applying DP to participants’ uploaded parameters, frameworks like DPGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and DPFedAvgGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> render GAN-based attacks inefficient in inferring other users’ training data within the deep learning network. DP is versatile and finds application in various scenarios, as demonstrated in multi-agent systems, reinforcement learning, transfer learning, and distributed machine learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Some works combine secure multiparty computation and differential privacy to achieve a secured federated learning model with high accuracy<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Additionally, other approaches improve privacy guarantees by combining DP with shuffling techniques and user data masking using an invisibility cloak algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. However, these solutions introduce uncertainty into the uploaded parameters, potentially compromising training performance. The challenge lies in balancing robust privacy protection and maintaining optimal training performance in federated learning systems. Developing techniques that effectively protect user privacy while preserving model accuracy and server evaluability remains an ongoing area of research to ensure the trustworthiness and reliability of federated learning frameworks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Possible Solutions</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Defense Against Poisoning Attacks</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Poisoning attacks involve injecting malicious data into a machine learning model’s training set to manipulate the model’s behavior during training or deployment. Defense strategies against poisoning attacks typically involve data sanitization techniques, outlier detection, or verification mechanisms. Some popular defense methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> are Byzantine robust aggregation, clustering-based detection, and behavior-based detection methods to enhance the security and robustness of FL systems.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Defense Against Backdoor Attacks</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Backdoor attacks involve adding a hidden pattern or trigger to a machine-learning model that causes it to produce incorrect results when triggered by specific inputs. Defense against backdoor attacks often involves model inspection, identifying and removing suspicious patterns, or techniques like fine-tuning to retrain the model and its parameter without the backdoor trigger.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Defense Against Membership Inference Attacks</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Membership inference attacks attempt to determine whether a specific data point was used in a machine learning model’s training dataset. Defending against such attacks may involve differential privacy techniques, adding noise to the training data, or employing privacy-preserving algorithms. These defense mechanisms are designed to protect the privacy and confidentiality of individual data points within the federated learning setting, thereby enhancing the security and privacy of the overall FL system.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Defense Against Generative Adversarial Network Based Attacks</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">GANs can generate realistic synthetic data, which could be misused to attack machine learning models. Defense strategies against GAN-based attacks might involve:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Adversarial training</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Utilizing GANs for data augmentation</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Employing detection mechanisms to identify fake data</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Defense Against Differential Privacy Attacks</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Differential privacy attacks attempt to infer sensitive information about individuals from a trained model. Defense against such attacks often involves incorporating differential privacy mechanisms during the training process to ensure the privacy of individuals’ data. Some key strategies that can be followed to defend from such attacks include robust identity verification, formal methods, federated averaging with differential privacy, client selections, Byzantine fault tolerance, homomorphic encryption, transfer learning, model distillation, and secure model aggregations.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section reviews existing research and studies related to Federated Learning (FL), specifically focusing on security and privacy aspects. The landscape of FL research has grown rapidly, and numerous studies have contributed valuable insights into addressing the challenges associated with this decentralized learning paradigm.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Privacy-Preserving FL Techniques:</span> Several research efforts have explored novel privacy-preserving techniques in FL. Differential privacy has been a prominent approach to protect individual data privacy during the aggregation of model updates. Various studies have proposed improved differential privacy mechanisms tailored for FL settings, ensuring a balance between privacy guarantees and model accuracy.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Adversarial Attacks and Defenses:</span> Research on adversarial attacks and defenses in FL has gained significant attention. Studies have investigated poisoning attacks, backdoor attacks, membership inference attacks, and other adversarial strategies to compromise FL systems. Researchers have proposed robust defenses to counteract these threats, such as anomaly detection, secure aggregation protocols, and adversarial training mechanisms.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Secure Communication Protocols:</span> Communication security is critical to FL, as model updates are transmitted between devices and the central server. Several studies have focused on developing secure communication protocols, incorporating encryption, authentication, and integrity verification techniques to safeguard against eavesdropping and tampering.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In summary, Federated Learning (FL) has emerged as a promising paradigm to address the challenges of centralized data storage and privacy concerns in machine learning. By decentralizing data and enabling collaborative learning across multiple devices, FL offers a novel approach that preserves user privacy while harnessing the power of AI across diverse domains. Throughout this paper, we have explored the fundamental concepts of Federated Learning, delving into its underlying workings for data privacy and confidentiality. We also identified and discussed FL’s various security and privacy challenges, including poisoning attacks, backdoor attacks, membership inference attacks, GAN-based attacks, and differential privacy attacks.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Researchers and practitioners must develop robust defense mechanisms, secure communication protocols, and privacy-preserving techniques to address these challenges. Techniques such as differential privacy, anomaly detection, and adversarial training can be vital in safeguarding FL systems against adversarial threats and data breaches. As FL gains traction and finds applications in real-world scenarios, regulatory and ethical considerations become paramount. Adherence to data protection regulations, informed consent, and ethical data handling are essential to maintain public trust and confidence in FL technologies.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor
federated learning,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics</em>, ser. Proceedings of
Machine Learning Research, S. Chiappa and R. Calandra, Eds., vol. 108.   PMLR, 26–28 Aug 2020, pp. 2938–2948.
[Online]. Available:
<a target="_blank" href="https://proceedings.mlr.press/v108/bagdasaryan20a.html" title="" class="ltx_ref ltx_url">https://proceedings.mlr.press/v108/bagdasaryan20a.html</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao,
“Neural cleanse: Identifying and mitigating backdoor attacks in neural
networks,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on Security and Privacy (SP)</em>, 2019,
pp. 707–723.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Suri, P. Kanani, V. J. Marathe, and D. W. Peterson, “Subject membership
inference attacks in federated learning,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2206.03317</em>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Zhang, J. Chen, D. Wu, B. Chen, and S. Yu, “Poisoning attack in federated
learning using generative adversarial nets,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2019 18th IEEE
international conference on trust, security and privacy in computing and
communications/13th IEEE international conference on big data science and
engineering (TrustCom/BigDataSE)</em>.   IEEE, 2019, pp. 374–380.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. El Ouadrhiri and A. Abdelhadi, “Differential privacy for deep and federated
learning: A survey,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, vol. 10, pp. 22 359–22 380,
2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector
machines,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1206.6389</em>, 2012.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks to
<math id="bib.bib8.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib8.1.m1.1a"><mo stretchy="false" id="bib.bib8.1.m1.1.1" xref="bib.bib8.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib8.1.m1.1b"><ci id="bib.bib8.1.m1.1.1.cmml" xref="bib.bib8.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib8.1.m1.1c">\{</annotation></semantics></math>Byzantine-Robust<math id="bib.bib8.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib8.2.m2.1a"><mo stretchy="false" id="bib.bib8.2.m2.1.1" xref="bib.bib8.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib8.2.m2.1b"><ci id="bib.bib8.2.m2.1.1.cmml" xref="bib.bib8.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib8.2.m2.1c">\}</annotation></semantics></math> federated learning,” in <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">29th USENIX security
symposium (USENIX Security 20)</em>, 2020, pp. 1605–1622.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can you really backdoor
federated learning?” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against
backdooring attacks on deep neural networks,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International
symposium on research in attacks, intrusions, and defenses</em>.   Springer, 2018, pp. 273–294.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C. Xie, K. Huang, P.-Y. Chen, and B. Li, “Dba: Distributed backdoor attacks
against federated learning,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>, 2020. [Online]. Available:
<a target="_blank" href="https://openreview.net/forum?id=rkgyS0VFvr" title="" class="ltx_ref ltx_url">https://openreview.net/forum?id=rkgyS0VFvr</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Xie, M. Chen, P.-Y. Chen, and B. Li, “Crfl: Certifiably robust federated
learning against backdoor attacks,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>.   PMLR, 2021, pp.
11 372–11 382.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Truex, L. Liu, M. Emre Gursoy, L. Yu, and W. Wei, “Towards
Demystifying Membership Inference Attacks,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, p.
arXiv:1807.09173, June 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial nets,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances
in neural information processing systems</em>, vol. 27, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Hitaj, G. Ateniese, and F. Perez-Cruz, “Deep models under the gan:
information leakage from collaborative deep learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2017 ACM SIGSAC conference on computer and communications security</em>,
2017, pp. 603–618.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security</em>, 2015, pp. 1310–1321.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting unintended
feature leakage in collaborative learning,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on
Security and Privacy (SP)</em>, 2019, pp. 691–706.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Xie, K. Lin, S. Wang, F. Wang, and J. Zhou, “Differentially private
generative adversarial network,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.06739</em>,
2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Augenstein, H. B. McMahan, D. Ramage, S. Ramaswamy, P. Kairouz, M. Chen,
R. Mathews, <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Generative models for effective ml on private,
decentralized datasets,” <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.06679</em>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Zhu, D. Ye, W. Wang, W. Zhou, and S. Y. Philip, “More than privacy:
Applying differential privacy in key areas of artificial intelligence,”
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>, vol. 34, no. 6,
pp. 2824–2843, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Truex, N. Baracaldo, A. Anwar, T. Steinke, H. Ludwig, R. Zhang, and Y. Zhou,
“A hybrid approach to privacy-preserving federated learning,” in
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM workshop on artificial intelligence and
security</em>, 2019, pp. 1–11.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B. Ghazi, R. Pagh, and A. Velingker, “Scalable and differentially private
distributed aggregation in the shuffled model,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1906.08320</em>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Awan, B. Luo, and F. Li, “Contra: Defending against poisoning attacks in
federated learning,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">European Symposium on Research in Computer
Security</em>. [Online]. Available: <a target="_blank" href="https://par.nsf.gov/biblio/10294585" title="" class="ltx_ref ltx_url">https://par.nsf.gov/biblio/10294585</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.12180" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.12181" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.12181">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.12181" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.12182" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 16:32:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
