<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.00789] LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models</title><meta property="og:description" content="Linking information across sources is fundamental to a variety of analyses in social science, business, and government. While large language models (LLMs) offer enormous promise for improving record linkage in noisy da…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.00789">

<!--Generated on Wed Feb 28 08:10:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id3.id1" class="ltx_text ltx_font_typewriter">LinkTransformer</span>: A Unified Package for Record Linkage with Transformer Language Models </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abhishek Arora and Melissa Dell<sup id="id4.3.id1" class="ltx_sup"><span id="id4.3.id1.1" class="ltx_text ltx_font_italic">∗</span></sup> 
<br class="ltx_break">Harvard University, Cambridge, MA, USA 
<br class="ltx_break"><sup id="id5.4.id2" class="ltx_sup">∗</sup>Corresponding author: melissadell@fas.harvard.edu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Linking information across sources is fundamental to a variety of analyses in social science, business, and government. While large language models (LLMs) offer enormous promise for improving record linkage in noisy datasets, in many domains approximate string matching packages in popular softwares such as R and Stata remain predominant. These packages have clean, simple interfaces and can be easily extended to a diversity of languages. Our open-source package <span id="id6.id1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> aims to extend the familiarity and ease-of-use of popular string matching methods to deep learning. It is a general purpose package for record linkage with transformer LLMs that treats record linkage as a text retrieval problem. At its core is an off-the-shelf toolkit for applying transformer models to record linkage with four lines of code. <span id="id6.id1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> contains a rich repository of pre-trained transformer semantic similarity models for multiple languages and supports easy integration of any transformer language model from Hugging Face or OpenAI. It supports standard functionality such as blocking and linking on multiple noisy fields. <span id="id6.id1.3" class="ltx_text ltx_font_typewriter">LinkTransformer</span> APIs also perform other common text data processing tasks, <span id="id6.id1.4" class="ltx_text ltx_font_italic">e.g.</span>, aggregation, noisy de-duplication, and translation-free cross-lingual linkage. Importantly, <span id="id6.id1.5" class="ltx_text ltx_font_typewriter">LinkTransformer</span> also contains comprehensive tools for efficient model tuning, to facilitate different levels of customization when off-the-shelf models do not provide the required accuracy. Finally, to promote reusability, reproducibility, and extensibility, <span id="id6.id1.6" class="ltx_text ltx_font_typewriter">LinkTransformer</span> makes it easy for users to contribute their custom-trained models to its model hub. By combining transformer language models with intuitive APIs that will be familiar to many users of popular string matching packages, <span id="id6.id1.7" class="ltx_text ltx_font_typewriter">LinkTransformer</span> aims to democratize the benefits of LLMs among those who may be less familiar with deep learning frameworks.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Linking information across sources is fundamental to a variety of analyses in social science, business, and government.
A recent literature, focused on matching across e-commerce datasets, shows the promise of transformer large language models (LLMs) for improving record linkage (alternatively termed entity resolution or approximate dictionary matching). Yet these methods have not yet made widespread inroads, with rule-based methods continuing to overwhelmingly predominate in social science and government applications (<span id="S1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, see reviews by <cite class="ltx_cite ltx_citemacro_citet">Binette and Steorts (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>); Abramitzky et al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>). In particular, users commonly employ string-based matching tools available in statistical software packages such as R or Stata.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We suspect that record linkage with large language models has not made further inroads at least in part due to the lack of packages that match the ease of the popular string matching packages, which are intuitive, extensible, and easy-to-use. They require little coding expertise and can easily be applied across different languages and settings.
In contrast, existing tools for large language model matching require considerable technical expertise to implement. This makes sense in the context for which these models were developed - classifying and linking products for e-commerce firms, which employ data scientists - but it is a significant impediment for broader use.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To bridge the gap between the ease-of-use of widely employed string matching packages and the power of modern LLMs, we developed <span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span>, a general purpose, user friendly package for record linkage with transformer LLMs. <span id="S1.p3.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> treats record linkage as a text retrieval problem (See Figure 1). The API can be thought of as a drop-in replacement to popular dataframe manipulation frameworks like pandas or tools like R and Stata, catering to those who lack extensive exposure to coding.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2309.00789/assets/image.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Visualization.</span> This figure shows the <span id="S1.F1.4.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> architecture.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To achieve its objective of democratizing access to the benefits of deep learning amongst those who may lack familiarity with deep learning frameworks, <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> integrates the following features:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">An off-the-shelf toolkit for applying transformer models to record linkage and de-duplication with 4 lines of code</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A rich repository of pre-trained semantic similarity models, supporting multiple languages, that underlies off-the-shelf usage</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Easy integration of any language transformer model on Hugging Face or OpenAI</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">APIs to support related data processing tasks, <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, aggregation, de-duplication, and translation-free cross-lingual linkage</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">Comprehensive tools for efficient model tuning to facilitate different levels of customization</p>
</div>
</li>
<li id="S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S1.I1.i6.p1" class="ltx_para">
<p id="S1.I1.i6.p1.1" class="ltx_p">Easy sharing of models, to promote reusability, reproducibility, and extensibility</p>
</div>
</li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The <span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model zoo currently contains English, Chinese, French, German, Japanese, Spanish, and multilingual pre-trained models.
We initialize with semantic similarity models, <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citet">Reimers and Gurevych (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, which have desirable properties relative to using off-the-shelf embeddings from models like RoBERTa (see Section <a href="#S2" title="2 Relation to the Existing Literature ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
We further tuned these models on a variety of linked datasets.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">While transfer learning can facilitate strong off-the-shelf performance in many scenarios, supporting customization is important. Record linkage applications are extremely diverse in their languages, time periods, and domains, which vary significantly in how out-of-domain they are from the web corpora that underlying LLMs are trained on. Considerable heterogeneity - combined with settings that demand extremely high accuracy - create many scenarios where custom training is useful.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We show that <span id="S1.p7.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> performs well on challenging record linkage tasks. It is equally applicable to record linkage tasks with a single field - <span id="S1.p7.1.2" class="ltx_text ltx_font_italic">e.g.,</span> linking 1940s Mexican tariff product classes across time - and applications that require concatenating an array of noisily measured fields - <span id="S1.p7.1.3" class="ltx_text ltx_font_italic">e.g.,</span> linking 1950s Japanese firms across different large-scale, noisy databases using the firm name, location, products, shareholders, and banks. This type of linkage problem would be highly convoluted with traditional string matching methods, as there are many noisily measured fields of relevance (<span id="S1.p7.1.4" class="ltx_text ltx_font_italic">e.g.,</span> products can be described in different ways, different subsets of managers and shareholders are listed, etc). Using <span id="S1.p7.1.5" class="ltx_text ltx_font_typewriter">LinkTransformer</span> to automatically concatenate the information and feed it to a LLM handles these challenges with ease. A demo is available at <a target="_blank" href="https://www.youtube.com/watch?v=Sn47nmCvV9M" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=Sn47nmCvV9M</a>. More resources are available on our package website <a target="_blank" href="https://linktransformer.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://linktransformer.github.io/</a>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> has a GNU General Public License.
It is being actively maintained, and in the next release we will add support for vision-free and multimodal linkage models, including support to import and customize any timm model <cite class="ltx_cite ltx_citemacro_cite">Wightman (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>. When OCR errors are rampant, vision-only or aligned vision-language transformer models can improve record linkage, relative to string matching or language-only transformer linking <span id="S1.p8.1.2" class="ltx_text ltx_font_italic">e.g.</span>, <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>); Arora et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The rest of the paper is organized as follows. Section <a href="#S2" title="2 Relation to the Existing Literature ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of related work.
The core <span id="S1.p9.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> library, Model Zoo, customized model training, and model sharing are described in Section <a href="#S3" title="3 The LinkTransformer Library ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Section <a href="#S4" title="4 Applications ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> discusses various use cases.
Section <a href="#S5" title="5 Limitations ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> discusses limitations.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Relation to the Existing Literature</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There is a large literature on record linkage spanning social science, statistics, and computer science. Record linkage serves as a prerequisite for many empirical analyses, which often require combining text data from multiple noisy sources. In social science, statistics, and government applications, large language models have made few inroads. A 2022 review of the record linkage literature in <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">Science Advances</span> <cite class="ltx_cite ltx_citemacro_cite">Binette and Steorts (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>, entitled “(Almost) All of Entity Resolution”, concludes that deep neural models are unlikely to be applicable to record linkage using structured data, arguing that training datasets are small and there is unlikely to be much gained from large language models since text fields are often short.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">While there are undoubtedly linking tasks for which LLMs will not be of much use (discussed further in Section <a href="#S5" title="5 Limitations ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> on limitations), an extensive literature on e-commerce applications underscores their utility for linking structured datasets. Benchmarks in this literature (e.g., <cite class="ltx_cite ltx_citemacro_citet">Köpcke et al. (<a href="#bib.bib15" title="" class="ltx_ref">2010</a>); Das et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>); Primpeli et al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>) focus on applications such as matching electronics and software products between Amazon-Google and Walmart-Amazon listings, matching iTunes and Amazon music listings, and matching restaurants between Fodors and Zagats. The focus is on applications in English. Recent studies have used masked language models (e.g., BERT, DistilBERT, RoBERTa) <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Joshi et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>); Brunner and Stockinger (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, GPT <cite class="ltx_cite ltx_citemacro_cite">Peeters and Bizer (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Tang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, or both, significantly outperforming static word embedding and other older linkage methods. <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>, like <span id="S2.p2.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span>, uses Sentence BERT <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The main package in this space to our knowledge, Ditto <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, implements the methods later published in <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>. It requires significant programming expertise to deploy, appropriate for a litearture focused on e-commerce - where data scientists predominate - but a hindrance for broader use.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Most of the literature examining record linkage with LLMs poses record linkage as a classification task, which is appropriate for the e-commerce benchmarks. However, this significantly limits extensibility, as in many social science and government applications the number of entities to be linked numbers in the millions, making it computationally infeasible to compute a softmax over all possible classes (entities).</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> frames record linkage as a knn-retrieval task, in which the nearest neighbor for each entity in a query embedding dataset is retrieved from a key embedding dataset, using cosine similarity implemented with an FAISS backend <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>. <span id="S2.p5.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> includes functionality to tune a no-match threshold - since not all entities in the query need to have a match in the key - and allows for retrieving multiple neighbors, to accommodate many-to-many matches between the query and the key. The <span id="S2.p5.1.3" class="ltx_text ltx_font_typewriter">LinkTransformer</span> architecture was inspired by a variety of bi-encoder applications for unstructured texts, <span id="S2.p5.1.4" class="ltx_text ltx_font_italic">e.g.</span>, passage retrieval <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>, entity disambiguation <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>, and entity co-reference resolution <cite class="ltx_cite ltx_citemacro_cite">Hsu and Horwood (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> departs from much of the literature (with the exception of <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>) in utilizing LLMs trained for semantic similarity for its pre-trained models.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Compare to <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>, <span id="footnote1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> uses a different loss function, supervised contrastive loss <cite class="ltx_cite ltx_citemacro_cite">Khosla et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>. It is well-suited to record linkage as there are multiple positive pairs in the datasets we use for pre-training.</span></span></span> A large literature shows that off-the-shelf LLMs such as BERT have anisotropic geometries <cite class="ltx_cite ltx_citemacro_cite">Ethayarajh (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>: representations of low frequency words are pushed outwards on the hypersphere, the sparsity of low frequency words violates convexity, and the distance between embeddings is correlated with lexical similarity. This leads to poor performance when individual term representations from the transformer model are pooled to create a representation for longer texts - as is often necessary for record linkage - since pooling assumes convexity, and leads to poor alignment between semantically similar texts.
Contrastive training for semantic similarity reduces anisotropy, improving alignment between semantically similar pairs and improving sentence embeddings <cite class="ltx_cite ltx_citemacro_cite">Wang and Isola (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>); Reimers and Gurevych (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>.
<span id="S2.p6.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> builds closely upon Sentence BERT <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, whose excellent semantic similarity library inspired many of the features in <span id="S2.p6.1.3" class="ltx_text ltx_font_typewriter">LinkTransformer</span>.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">The knn retrieval structure of <span id="S2.p7.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> also supports noisy de-duplication, a closely related task that finds noisily duplicated observations within a dataset. <span id="S2.p7.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> follows the methods developed in <cite class="ltx_cite ltx_citemacro_citet">Silcock et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>, who show that de-duplication using a contrastively trained bi-encoder significantly outperforms <math id="S2.p7.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p7.1.m1.1a"><mi id="S2.p7.1.m1.1.1" xref="S2.p7.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p7.1.m1.1b"><ci id="S2.p7.1.m1.1.1.cmml" xref="S2.p7.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.1.m1.1c">n</annotation></semantics></math>-gram and locally sensitive hashing methods and is highly scalable.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The <span id="S3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> Library</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Off-the-shelf Toolkit</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">At the core of <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> is an off-the-shelf toolkit that streamlines record linkage with transformer language models.
The record linkage models enable using pre-trained or self-trained transformer models with just 4 lines of code.
Any Hugging Face or OpenAI model can be used by configuring the <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">model</span> and <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">openai_key</span> arguments. This future-proofs the package, allowing it to take advantage of the open-source revolution that Hugging Face has pioneered.
Here is an example of the core <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold">merge</span> functionality, based on embeddings sourced from an external language model.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<div id="S3.SS1.p2.1" class="ltx_listing ltx_lst_language_python ltx_lst_numbers_left ltx_lstlisting ltx_listing" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,aW1wb3J0IGxpbmt0cmFuc2Zvcm1lciBhcyBsdAojTG9hZCBkYXRhIGZyYW1lCmRmMSA9IHBkLnJlYWRfY3N2KCJkZjEuY3N2IikKZGYxID0gcGQucmVhZF9jc3YoImRmMi5jc3YiKQpkZl9tYXRjaGVkID0gbHQubWVyZ2UoZGYyLCBkZjEsIG1lcmdlX3R5cGU9JzE6bScsIG9uPVsiVmFybmFtZSJdLCBtb2RlbD0ic2VudGVuY2UtdHJhbnNmb3JtZXJzL2FsbC1NaW5pTE0tTDYtdjIiLCBvcGVuYWlfa2V5PU5vbmUp" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span><span id="lstnumberx1.1" class="ltx_text ltx_lst_keyword ltx_font_typewriter"><span id="lstnumberx1.1.1" class="ltx_text" style="color:#FF00FF;">import</span></span><span id="lstnumberx1.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">linktransformer</span><span id="lstnumberx1.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">as</span><span id="lstnumberx1.6" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">lt</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span><span id="lstnumberx2.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#009900;">#Load<span id="lstnumberx2.1.1" class="ltx_text ltx_lst_space"> </span>data<span id="lstnumberx2.1.2" class="ltx_text ltx_lst_space"> </span>frame</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span><span id="lstnumberx3.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df1</span><span id="lstnumberx3.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx3.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx3.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx3.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">pd</span><span id="lstnumberx3.6" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx3.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">read_csv</span><span id="lstnumberx3.8" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx3.9" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"df1.csv"</span><span id="lstnumberx3.10" class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span><span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df1</span><span id="lstnumberx4.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx4.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">pd</span><span id="lstnumberx4.6" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx4.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">read_csv</span><span id="lstnumberx4.8" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx4.9" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"df2.csv"</span><span id="lstnumberx4.10" class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span><span id="lstnumberx5.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df_matched</span><span id="lstnumberx5.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx5.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">lt</span><span id="lstnumberx5.6" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx5.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">merge</span><span id="lstnumberx5.8" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx5.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df2</span><span id="lstnumberx5.10" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx5.11" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.12" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df1</span><span id="lstnumberx5.13" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx5.14" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.15" class="ltx_text ltx_lst_identifier ltx_font_typewriter">merge_type</span><span id="lstnumberx5.16" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx5.17" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">’1:m’</span><span id="lstnumberx5.18" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx5.19" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.20" class="ltx_text ltx_lst_identifier ltx_font_typewriter">on</span><span id="lstnumberx5.21" class="ltx_text ltx_font_typewriter">=[</span><span id="lstnumberx5.22" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"Varname"</span><span id="lstnumberx5.23" class="ltx_text ltx_font_typewriter">],</span><span id="lstnumberx5.24" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.25" class="ltx_text ltx_lst_identifier ltx_font_typewriter">model</span><span id="lstnumberx5.26" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx5.27" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"sentence-transformers/all-MiniLM-L6-v2"</span><span id="lstnumberx5.28" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx5.29" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.30" class="ltx_text ltx_lst_identifier ltx_font_typewriter">openai_key</span><span id="lstnumberx5.31" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx5.32" class="ltx_text ltx_lst_identifier ltx_font_typewriter">None</span><span id="lstnumberx5.33" class="ltx_text ltx_font_typewriter">)</span>
</div>
</div>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> provides a wealth of pre-trained model weights, covering different languages and domains. It currently supports six languages (English, Chinese, French, German, Japanese, and Spanish) with both multilingual and monolingual models.
Training datasets include:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">A novel dataset of firm aliases that we compiled from Wikidata for 6 languages</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">United Nations economic classification schedules (International Standard Industrial Classification, Standard International Trade Classification, and Central Product Classification), which homogenize industry and product classifications across varying national systems. We include models trained on these for 3 of the 6 official languages of the UN.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">A variety of e-commerce linking datasets (see Supplemental Materials)</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">We name these models with a semantic syntax: <code id="S3.SS1.p4.1.1" class="ltx_verbatim ltx_font_typewriter">{org_name}/lt-{data}-{task}-{lang}</code>.
Each model has a detailed model card, with the appropriate tags for quick model discovery. Additionally, for the tasks we trained our models on, we provide a high-level interface to download the right model by task through a wrapper that retrieves the best model for a task chosen by the user.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> makes no compromise in scalability. All functions are vectorized wherever possible and the vector similarity search underlying knn retrieval is accelerated by an FAISS <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> backend that can easily be extended to perform retrieval on GPUs on massive datasets. We also allow “blocking” - running knn-search only within “blocks” that can be defined by the <span id="S3.SS1.p5.1.2" class="ltx_text ltx_font_typewriter">blocking_vars</span> argument.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Record linkage frequently requires matching databases on multiple noisily measured keys. <span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> allows a list of as many variables as needed in the "on" argument. The merge keys specified by the <span id="S3.SS1.p6.1.2" class="ltx_text ltx_font_typewriter">on</span> variable are serialized by concatenating them with a <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="&lt;SEP&gt;" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS1.p6.1.m1.1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1.1.1a" xref="S3.SS1.p6.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.1.1.4" xref="S3.SS1.p6.1.m1.1.1.1.1.4.cmml">P</mi></mrow><mo fence="true" lspace="0em" id="S3.SS1.p6.1.m1.1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p6.1.m1.1.1.2.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1.2">expectation</csymbol><apply id="S3.SS1.p6.1.m1.1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1"><times id="S3.SS1.p6.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1.1"></times><ci id="S3.SS1.p6.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1.2">𝑆</ci><ci id="S3.SS1.p6.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1.3">𝐸</ci><ci id="S3.SS1.p6.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1.4">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">&lt;SEP&gt;</annotation></semantics></math> token, which is based on the underlying tokenizer of the selected base language model. This ensures that the serialization takes advantage of a token already introduced in training and the process is agnostic to the choice of the model.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Since we have designed the API around dataframes - due to their familiarity amongst users of R, Stata or Excel - all import/export formats supported.</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p">The accessible <span id="S3.SS1.p8.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> API supports a plethora of other features that are an essential part of routine data analysis pipelines. These include:</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p"><span id="S3.SS1.p9.1.1" class="ltx_text ltx_font_bold">Aggregation:</span>
Data processing often requires the aggregation of fine descriptions into coarser categories, that are consistent across datasets/time or facilitate interpretation. This problem can be thought of as a merge between finer categories and coarser ones, where <span id="S3.SS1.p9.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> classifies the finer categories by means of finding their nearest coarser neighbor(s). We provide a high-level API for this task, <span id="S3.SS1.p9.1.3" class="ltx_text ltx_font_typewriter">lt.aggregate_rows</span>, with a similar syntax to the main record linkage API.</p>
</div>
<div id="S3.SS1.p10" class="ltx_para">
<p id="S3.SS1.p10.1" class="ltx_p"><span id="S3.SS1.p10.1.1" class="ltx_text ltx_font_bold">Deduplication</span>:
Text datasets can contain noisy duplicates.
Popular libraries like dedupe <cite class="ltx_cite ltx_citemacro_cite">Gregg and Eder (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> only support deduplication using metrics that most closely resemble edit distance. <span id="S3.SS1.p10.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> allows for semantic deduplication with a single, intuitive function call.</p>
</div>
<div id="S3.SS1.p11" class="ltx_para">
<div id="S3.SS1.p11.1" class="ltx_listing ltx_lst_language_python ltx_lst_numbers_left ltx_lstlisting ltx_listing" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,ZGY9cGQucmVhZF9jc3YoImRmMS5jc3YiKSkKZGZfZGVkdXA9bHQuZGVkdXBfcm93cyhkZixvbj0iQ29tcGFueU5hbWUiLG1vZGVsPSJzZW50ZW5jZS10cmFuc2Zvcm1lcnMvYWxsLU1pbmlMTS1MNi12MiIsCiAgICBjbHVzdGVyX3BhcmFtcz0geyd0aHJlc2hvbGQnOiAwLjd9KQ==" download="">⬇</a></div>
<div id="lstnumberx6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span><span id="lstnumberx6.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df</span><span id="lstnumberx6.2" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx6.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">pd</span><span id="lstnumberx6.4" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx6.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">read_csv</span><span id="lstnumberx6.6" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx6.7" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"df1.csv"</span><span id="lstnumberx6.8" class="ltx_text ltx_font_typewriter">))</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span><span id="lstnumberx7.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df_dedup</span><span id="lstnumberx7.2" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx7.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">lt</span><span id="lstnumberx7.4" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx7.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">dedup_rows</span><span id="lstnumberx7.6" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx7.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">df</span><span id="lstnumberx7.8" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx7.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter">on</span><span id="lstnumberx7.10" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx7.11" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"CompanyName"</span><span id="lstnumberx7.12" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx7.13" class="ltx_text ltx_lst_identifier ltx_font_typewriter">model</span><span id="lstnumberx7.14" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx7.15" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"sentence-transformers/all-MiniLM-L6-v2"</span><span id="lstnumberx7.16" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span><span id="lstnumberx8.1" class="ltx_text ltx_lst_space ltx_font_typewriter">    </span><span id="lstnumberx8.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">cluster_params</span><span id="lstnumberx8.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx8.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx8.5" class="ltx_text ltx_font_typewriter">{</span><span id="lstnumberx8.6" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">’threshold’</span><span id="lstnumberx8.7" class="ltx_text ltx_font_typewriter">:</span><span id="lstnumberx8.8" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx8.9" class="ltx_text ltx_font_typewriter">0.7})</span>
</div>
</div>
</div>
<div id="S3.SS1.p12" class="ltx_para">
<p id="S3.SS1.p12.1" class="ltx_p"><span id="S3.SS1.p12.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> de-duplication clusters embeddings under the hood, with embeddings in the same cluster classified as duplicates. <span id="S3.SS1.p12.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> supports several clustering methods like SLINK, DBSCAN, HDBSCAN, and agglomerative clustering.</p>
</div>
<div id="S3.SS1.p13" class="ltx_para">
<p id="S3.SS1.p13.1" class="ltx_p"><span id="S3.SS1.p13.1.1" class="ltx_text ltx_font_bold">Cross-lingual linkage</span>:
Analyses spanning multiple countries often require cross-lingual linkage. This task typically requires machine translation followed by a merge. Edit distance metrics tend to do particularly poorly in this scenario, necessitating costly hand linking. <span id="S3.SS1.p13.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> users can bypass translation by using multilingual transformer models.</p>
</div>
<div id="S3.SS1.p14" class="ltx_para">
<p id="S3.SS1.p14.1" class="ltx_p">We provide helpful notebooks and tutorials on the <span id="S3.SS1.p14.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> website <a target="_blank" href="https://linktransformer.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://linktransformer.github.io/</a> to outline the use of these functionalities and also some toy datasets. We also have a tutorial to help those who are less familiar with language models to select ones that best fit their use case.
More detailed information and additional features that we cannot highlight in the interest of brevity can be found in the online <span id="S3.SS1.p14.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> documentation that is available on our public repo.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/dell-research-harvard/linktransformer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/dell-research-harvard/linktransformer</a></span></span></span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Customized Model Training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Record linkage tasks are highly diverse. Hence, <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> also supports easy model training.
Custom training can be initialized using the weights of any Hugging Face transformer model.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Training data are expected in a <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">pandas</span> data frame, removing entry barriers for the typical social science user who is familiar with other programming languages and statistical packages.
A data frame can include only positive labeled examples (linked observations) as inputs, in which case the model is evaluated using an information retrieval evaluator that measures top-1 retrieval accuracy. Alternately, it can take a list of both positive and negative pairs, in which case the model is evaluated using a binary classification objective.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Only the most important arguments are exposed and the rest have reasonable defaults which can be tweaked by more advanced users. Additionally, <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> supports logging of a training run on Weights and Biases <cite class="ltx_cite ltx_citemacro_cite">Biewald (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<div id="S3.SS2.p4.1" class="ltx_listing ltx_lst_language_python ltx_lst_numbers_left ltx_lstlisting ltx_listing" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,YmVzdF9tb2RlbF9wYXRoPWx0LnRyYWluX21vZGVsKAogICAgICAgIG1vZGVsX3BhdGg9ImhmLXBhdGgtdG8tYmFzZS1tb2RlbCIsCiAgICAgICAgZGF0YT0iZGYxLmNzdiIpLAogICAgICAgIGxlZnRfY29sX25hbWVzPVsibGVmdF92YXIiXSwKICAgICAgICByaWdodF9jb2xfbmFtZXM9WydyaWdodF92YXInXSwKICAgICAgICBsZWZ0X2lkX25hbWU9WydsZWZ0X2lkJ10sCiAgICAgICAgcmlnaHRfaWRfbmFtZT1bJ3JpZ2h0X2lkJ10sCiAgICAgICAgbGFiZWxfY29sX25hbWU9Tm9uZSwKICAgICAgICBsb2dfd2FuZGI9RmFsc2UsCiAgICAgICAgdHJhaW5pbmdfYXJncz17Im51bV9lcG9jaHMiOiAxfQogICAgKQ==" download="">⬇</a></div>
<div id="lstnumberx9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span><span id="lstnumberx9.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">best_model_path</span><span id="lstnumberx9.2" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx9.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">lt</span><span id="lstnumberx9.4" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx9.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">train_model</span><span id="lstnumberx9.6" class="ltx_text ltx_font_typewriter">(</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span><span id="lstnumberx10.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx10.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">model_path</span><span id="lstnumberx10.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx10.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"hf-path-to-base-model"</span><span id="lstnumberx10.5" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span><span id="lstnumberx11.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx11.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">data</span><span id="lstnumberx11.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx11.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"df1.csv"</span><span id="lstnumberx11.5" class="ltx_text ltx_font_typewriter">),</span>
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span><span id="lstnumberx12.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx12.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">left_col_names</span><span id="lstnumberx12.3" class="ltx_text ltx_font_typewriter">=[</span><span id="lstnumberx12.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"left_var"</span><span id="lstnumberx12.5" class="ltx_text ltx_font_typewriter">],</span>
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span><span id="lstnumberx13.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx13.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">right_col_names</span><span id="lstnumberx13.3" class="ltx_text ltx_font_typewriter">=[</span><span id="lstnumberx13.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">’right_var’</span><span id="lstnumberx13.5" class="ltx_text ltx_font_typewriter">],</span>
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span><span id="lstnumberx14.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx14.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">left_id_name</span><span id="lstnumberx14.3" class="ltx_text ltx_font_typewriter">=[</span><span id="lstnumberx14.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">’left_id’</span><span id="lstnumberx14.5" class="ltx_text ltx_font_typewriter">],</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span><span id="lstnumberx15.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx15.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">right_id_name</span><span id="lstnumberx15.3" class="ltx_text ltx_font_typewriter">=[</span><span id="lstnumberx15.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">’right_id’</span><span id="lstnumberx15.5" class="ltx_text ltx_font_typewriter">],</span>
</div>
<div id="lstnumberx16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span><span id="lstnumberx16.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx16.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">label_col_name</span><span id="lstnumberx16.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx16.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">None</span><span id="lstnumberx16.5" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span><span id="lstnumberx17.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx17.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">log_wandb</span><span id="lstnumberx17.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx17.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">False</span><span id="lstnumberx17.5" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span><span id="lstnumberx18.1" class="ltx_text ltx_lst_space ltx_font_typewriter">        </span><span id="lstnumberx18.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">training_args</span><span id="lstnumberx18.3" class="ltx_text ltx_font_typewriter">={</span><span id="lstnumberx18.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="color:#9400D1;">"num_epochs"</span><span id="lstnumberx18.5" class="ltx_text ltx_font_typewriter">:</span><span id="lstnumberx18.6" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx18.7" class="ltx_text ltx_font_typewriter">1}</span>
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span><span id="lstnumberx19.1" class="ltx_text ltx_lst_space ltx_font_typewriter">    </span><span id="lstnumberx19.2" class="ltx_text ltx_font_typewriter">)</span>
</div>
</div>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Default training expects positive pairs. A simple argument that specifies <span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_typewriter">label_col_name</span> switches the dataset format and model evaluation to adapt to positive and negative labels. To make this extensible to most record linkage use-cases, the model can also be trained on a dataset of cluster ids and texts by simply specifying <span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_typewriter">clus_id_col_name</span> and <span id="S3.SS2.p5.1.3" class="ltx_text ltx_font_typewriter">clus_text_col_names</span>.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> is sufficiently sample efficient that most models in the model zoo were trained with a student Google Colab account, an integral feature since the vast majority of potential users have constrained compute budgets.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>User Contributions</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> aims to promote the reusability and reproducibility of record linkage pipelines.
End-users can upload their self-trained models to the <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> Hugging Face hub with a simple
<span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_typewriter">model.save_to_hub</span>
command. Whenever a model is saved, a model card is automatically generated that follows best practices outlined in Hugging Face’s Model Card Guidebook. This process adds a pipeline-tag, supported language(s) (given the base model), and other tags to the model card’s header that facilitate model discovery by other Hugging Face users. Moreover, the automatically generated card contains instructions on how to use the model for record linkage and model-specific architecture and training details.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Applications</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The LLMs in the <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model zoo outperform non-neural methods by a wide margin. We examine applications from both modern web data and historical datasets that we digitized from hard copy historical firm and government records. These tasks are representative of applications in quantitative social science, industry, and government. The results are reported in the supplementary materials.
</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">First, we evaluate in-domain performance on semantic similarity datasets used to pre-train <span id="S4.p2.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models, with the supplementary materials comparing the accuracy of Levenshtein edit distance matching <cite class="ltx_cite ltx_citemacro_cite">Levenshtein et al. (<a href="#bib.bib16" title="" class="ltx_ref">1966</a>)</cite>, semantic similarity models off-the-shelf, and <span id="S4.p2.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> tuned models.
The <span id="S4.p2.1.3" class="ltx_text ltx_font_typewriter">LinkTransformer</span> and OpenAI models tend to outperform edit distance metrics by a wide margin in matching Wikidata firm aliases, with the top-1 retrieval accuracy tending to be 20-30 points higher for <span id="S4.p2.1.4" class="ltx_text ltx_font_typewriter">LinkTransformer</span> linkage than for edit distance based matching. Cases that <span id="S4.p2.1.5" class="ltx_text ltx_font_typewriter">LinkTransformer</span> gets wrong are often impossible even for a skilled human to resolve from the firm names alone, <span id="S4.p2.1.6" class="ltx_text ltx_font_italic">e.g.,</span> in cases where a firm is referred to by two completely disparate acronyms. Off-the-shelf semantic similarity models also tend to outperform string-matching methods, albeit not by as much of a margin as tuned models.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Second, we examine historical applications. The first setting is a concordance between products in two 1940s Mexican tariffs schedules, published by the Mexican government and digitized from the hard copy documents <cite class="ltx_cite ltx_citemacro_cite">Secretaria de Economía de Mexico (<a href="#bib.bib22" title="" class="ltx_ref">1948</a>)</cite>. Tariffs were applied at an extremely disaggregated product level and each of the many thousands of products in the tariff schedule is identified only by a text description, which can change each time the tariff schedule is updated. Around 2,000 products map to different descriptions across the schedules. While there are considerable debates on the role that trade policies have played in long-run development, empirical evidence is limited in part due to the considerable challenges in homogenizing extremely detailed historical tariff schedules across time, as crosswalks do not exist for most tariff schedule changes.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We link the tariff schedules using an off-the-shelf semantic similarity model, as well as a model tuned on the in-domain historical data and the recommended Open AI embeddings (from the model <span id="S4.p4.1.1" class="ltx_text ltx_font_italic">text-embedding-ada-002</span>). All transformer models widely outperform edit distance, with the fine-tuned <span id="S4.p4.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model (at no charge) and purchased GPT hidden states producing similar, near perfect results.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">We also link firms across two different 1950s publications created by different Japanese credit bureaus <cite class="ltx_cite ltx_citemacro_cite">Jinji Koshinjo (<a href="#bib.bib10" title="" class="ltx_ref">1954</a>); Teikoku Koshinjo (<a href="#bib.bib25" title="" class="ltx_ref">1957</a>)</cite>. One has around 7,000 firms and the other has around 70,000, including many small firms. Firm names can be written differently across publications and there are many duplicated or similar firm names.
To make this task feasible, we concatenate information on the firm’s name, prefecture, major products, shareholders, and banks. These variables contain OCR noise and the information included in each publication varies somewhat, <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">e.g.</span> in terms of which products are described, which shareholders are included, etc. This makes rule-based methods quite brittle.
Again, neural models significantly outperform string matching methods, with OpenAI giving the best results. (Due to the cost of labeling, our training dataset may not be large enough to capture the benefits of tuning.)</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Finally, we examine the various e-commerce and industry benchmarks that prevail in this literature. We used exactly the same training procedure for each benchmark, to avoid overfitting, which is often not the case in the literature. We have generally comparable performance, sometimes outperformed by other models (that could be integrated into <span id="S4.p6.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> as well if on Hugging Face) and sometimes outperforming other models.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> is built upon transformer language models, and hence will not be suitable for lower resource languages that lack pre-trained LLMs. <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> will also be less useful in contexts where little language understanding enters record linkage, <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">e.g.</span>, when linking records solely using individual names. For these contexts, the next release of <span id="S5.p1.1.4" class="ltx_text ltx_font_typewriter">LinkTransformer</span> will integrate vision-only transformer models, which the authors have reported on extensively elsewhere <cite class="ltx_cite ltx_citemacro_cite">Arora et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>); Yang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Furthermore, in settings where OCR errors are considerable, too much information may have been destroyed to successfully link entities using garbled texts. In this case, a multimodal framework (<span id="S5.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, <cite class="ltx_cite ltx_citemacro_citet">Arora et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>) that uses aligned language and vision models to incorporate the original image crops or a matching framework that incorporates character visual similarity <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> - as OCR errors tend to confuse visually similar characters - may be required. This again will be incorporated into <span id="S5.p2.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> relies on backends that many end users - accustomed primarily to working with statistical packages like Stata or R - will not be familiar with. We recommend that users new to LLMs deploy the package using a cloud service optimized for deep learning, such as Colab, to avoid the need to resolve dependencies. To reduce startup costs, we will provide detailed tutorials for <span id="S5.p3.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> instillation, inference, and training on Colab.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S5.1" class="ltx_sectional-block">
<section id="Sx1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">Supplementary Materials</h2>

</section>
</div>
</section>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S-1 </span>Training and other details</h2>

<div id="S1a.p1" class="ltx_para">
<p id="S1a.p1.1" class="ltx_p"><span id="S1a.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models use AdamW as the optimizer with a linear schedule with a 100% warm-up with 2e-6 as the max learning rate. We use a batch size of 64 for models trained with Wikidata (companies) and UN data (products). For industry benchmarks, we used a batch size of 128. We trained the models for 150 epochs for industrial benchmarks and 100 epochs for UN/Wikidata/Historic applications. We used Supervised Contrastive loss <cite class="ltx_cite ltx_citemacro_cite">Khosla et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> as the training objective with default hyperparameters. The implementation for the loss was based on the implementation shared on the sentence-transformers repository <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1a.p2" class="ltx_para">
<p id="S1a.p2.1" class="ltx_p"><span id="S1a.p2.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> uses IndexFlatIP from FAISS <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> as the index of choice, allowing an exhaustive search to get <math id="S1a.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1a.p2.1.m1.1a"><mi id="S1a.p2.1.m1.1.1" xref="S1a.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1a.p2.1.m1.1b"><ci id="S1a.p2.1.m1.1.1.cmml" xref="S1a.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.1.m1.1c">k</annotation></semantics></math> nearest neighbours. We use the inner-product as the metric. All embeddings from the encoders are L2-normalized such that the distances (inner-products) given by the FAISS indices are equivalent to cosine similarity.</p>
</div>
<div id="S1a.p3" class="ltx_para">
<p id="S1a.p3.1" class="ltx_p">Code to replicate the below tables and train the models are available on our repository, which also contains links to our training data.</p>
</div>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S-2 </span>Datasets and Results</h2>

<div id="S2a.p1" class="ltx_para">
<p id="S2a.p1.1" class="ltx_p">Table <a href="#S2.T1" title="Table S-1 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-1</span></a> lists the base sentence transformer models that we used to initialize the custom <span id="S2a.p1.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models.
Table <a href="#S2.T2" title="Table S-2 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-2</span></a> describes the datasets used for training the <span id="S2a.p1.1.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model zoo. They are drawn from multilingual UN product and industry concordances, Wikidata company aliases, a 1948 Mexican government concordance between tariff schedules <cite class="ltx_cite ltx_citemacro_cite">Secretaria de Economía de Mexico (<a href="#bib.bib22" title="" class="ltx_ref">1948</a>)</cite>, and a hand-linked dataset between two 1950s Japanese firm-level datasets collected by credit bureaus, one containing around 7,000 firms and the other around 70,000 <cite class="ltx_cite ltx_citemacro_cite">Teikoku Koshinjo (<a href="#bib.bib25" title="" class="ltx_ref">1957</a>); Jinji Koshinjo (<a href="#bib.bib10" title="" class="ltx_ref">1954</a>)</cite>.
Table <a href="#S2.T3" title="Table S-3 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-3</span></a> describes the train-val-test splits for each of these datasets.</p>
</div>
<div id="S2a.p2" class="ltx_para">
<p id="S2a.p2.1" class="ltx_p">Table <a href="#S2.T4" title="Table S-4 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-4</span></a> shows results of fine-tuned models, where the models are evaluated on the test split of the dataset they were trained on.
Table <a href="#S2.T5" title="Table S-5 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-5</span></a> examines performance on two historical linkage tasks: linking historical Mexican tariff schedules and linking historical Japanese firms across large, noisy databases.
Table <a href="#S2.T6" title="Table S-6 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-6</span></a> reports results on standard industry and e-commerce benchmarks for record linkage.</p>
</div>
<div id="S2a.p3" class="ltx_para">
<p id="S2a.p3.1" class="ltx_p">These results - discussed in the main text - show that deep neural record linkage (whether with off-the-shelf semantic similarity models, OpenAI models, or custom <span id="S2a.p3.1.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models) outperforms Levenshtein edit distance by a wide margin.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Base Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">English</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">sentence-transformers/multi-qa-mpnet-base-dot-v1</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Japanese</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center">oshizo/sbert-jsnli-luke-japanese-base-lite</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">French</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center">dangvantuan/sentence-camembert-large</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Chinese</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center">DMetaSoul/sbert-chinese-qmc-domain-v1</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Spanish</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center">hiiamsid/sentence_similarity_spanish_es</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<th id="S2.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">German</th>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_center">Sahajtomar/German-semantic</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<th id="S2.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Multilingual</th>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb">sentence-transformers/paraphrase-multilingual-mpnet-base-v2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table S-1: </span>We used the above sentence-transformers models for different langauges as base models to train <span id="S2.T1.3.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models. They were selected from the Hugging Face model hub and the names correspond to the repo names on the Hub.</figcaption>
</figure>
<figure id="S2.T2" class="ltx_table">
<div id="S2.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:699.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.4pt,19.4pt) scale(0.947386546411009,0.947386546411009) ;">
<table id="S2.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S2.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Training Data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.2.1" class="ltx_tr">
<th id="S2.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">lt-wikidata-comp-en</th>
<td id="S2.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Wikidata English-language</td>
</tr>
<tr id="S2.T2.1.1.3.2" class="ltx_tr">
<th id="S2.T2.1.1.3.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.3.2.2" class="ltx_td ltx_align_center">company names.</td>
</tr>
<tr id="S2.T2.1.1.4.3" class="ltx_tr">
<th id="S2.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-fr</th>
<td id="S2.T2.1.1.4.3.2" class="ltx_td ltx_align_center">Wikidata French-language</td>
</tr>
<tr id="S2.T2.1.1.5.4" class="ltx_tr">
<th id="S2.T2.1.1.5.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.5.4.2" class="ltx_td ltx_align_center">company names.</td>
</tr>
<tr id="S2.T2.1.1.6.5" class="ltx_tr">
<th id="S2.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-de</th>
<td id="S2.T2.1.1.6.5.2" class="ltx_td ltx_align_center">Wikidata German-language</td>
</tr>
<tr id="S2.T2.1.1.7.6" class="ltx_tr">
<th id="S2.T2.1.1.7.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.7.6.2" class="ltx_td ltx_align_center">company names.</td>
</tr>
<tr id="S2.T2.1.1.8.7" class="ltx_tr">
<th id="S2.T2.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-ja</th>
<td id="S2.T2.1.1.8.7.2" class="ltx_td ltx_align_center">Wikidata Japanese-language</td>
</tr>
<tr id="S2.T2.1.1.9.8" class="ltx_tr">
<th id="S2.T2.1.1.9.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.9.8.2" class="ltx_td ltx_align_center">company names.</td>
</tr>
<tr id="S2.T2.1.1.10.9" class="ltx_tr">
<th id="S2.T2.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-zh</th>
<td id="S2.T2.1.1.10.9.2" class="ltx_td ltx_align_center">Wikidata Chinese-language</td>
</tr>
<tr id="S2.T2.1.1.11.10" class="ltx_tr">
<th id="S2.T2.1.1.11.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.11.10.2" class="ltx_td ltx_align_center">company names.</td>
</tr>
<tr id="S2.T2.1.1.12.11" class="ltx_tr">
<th id="S2.T2.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-es</th>
<td id="S2.T2.1.1.12.11.2" class="ltx_td ltx_align_center">Wikidata Spanish-language</td>
</tr>
<tr id="S2.T2.1.1.13.12" class="ltx_tr">
<th id="S2.T2.1.1.13.12.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.13.12.2" class="ltx_td ltx_align_center">company names.</td>
</tr>
<tr id="S2.T2.1.1.14.13" class="ltx_tr">
<th id="S2.T2.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-multi</th>
<td id="S2.T2.1.1.14.13.2" class="ltx_td ltx_align_center">Wikidata multilingual company</td>
</tr>
<tr id="S2.T2.1.1.15.14" class="ltx_tr">
<th id="S2.T2.1.1.15.14.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.15.14.2" class="ltx_td ltx_align_center">names (en, fr, es, de, ja, zh).</td>
</tr>
<tr id="S2.T2.1.1.16.15" class="ltx_tr">
<th id="S2.T2.1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-prod-ind-ja</th>
<td id="S2.T2.1.1.16.15.2" class="ltx_td ltx_align_center">Wikidata Japanese-language</td>
</tr>
<tr id="S2.T2.1.1.17.16" class="ltx_tr">
<th id="S2.T2.1.1.17.16.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.17.16.2" class="ltx_td ltx_align_center">company names and industries.</td>
</tr>
<tr id="S2.T2.1.1.18.17" class="ltx_tr">
<th id="S2.T2.1.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-en</th>
<td id="S2.T2.1.1.18.17.2" class="ltx_td ltx_align_center">UN fine-level product data</td>
</tr>
<tr id="S2.T2.1.1.19.18" class="ltx_tr">
<th id="S2.T2.1.1.19.18.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.19.18.2" class="ltx_td ltx_align_center">in English.</td>
</tr>
<tr id="S2.T2.1.1.20.19" class="ltx_tr">
<th id="S2.T2.1.1.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-en</th>
<td id="S2.T2.1.1.20.19.2" class="ltx_td ltx_align_center">UN coarse-level product data</td>
</tr>
<tr id="S2.T2.1.1.21.20" class="ltx_tr">
<th id="S2.T2.1.1.21.20.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.21.20.2" class="ltx_td ltx_align_center">in English.</td>
</tr>
<tr id="S2.T2.1.1.22.21" class="ltx_tr">
<th id="S2.T2.1.1.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-en</th>
<td id="S2.T2.1.1.22.21.2" class="ltx_td ltx_align_center">UN product data linked</td>
</tr>
<tr id="S2.T2.1.1.23.22" class="ltx_tr">
<th id="S2.T2.1.1.23.22.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.23.22.2" class="ltx_td ltx_align_center">to industries in English.</td>
</tr>
<tr id="S2.T2.1.1.24.23" class="ltx_tr">
<th id="S2.T2.1.1.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-es</th>
<td id="S2.T2.1.1.24.23.2" class="ltx_td ltx_align_center">UN fine-level product data</td>
</tr>
<tr id="S2.T2.1.1.25.24" class="ltx_tr">
<th id="S2.T2.1.1.25.24.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.25.24.2" class="ltx_td ltx_align_center">in Spanish.</td>
</tr>
<tr id="S2.T2.1.1.26.25" class="ltx_tr">
<th id="S2.T2.1.1.26.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-es</th>
<td id="S2.T2.1.1.26.25.2" class="ltx_td ltx_align_center">UN coarse-level product data</td>
</tr>
<tr id="S2.T2.1.1.27.26" class="ltx_tr">
<th id="S2.T2.1.1.27.26.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.27.26.2" class="ltx_td ltx_align_center">in Spanish.</td>
</tr>
<tr id="S2.T2.1.1.28.27" class="ltx_tr">
<th id="S2.T2.1.1.28.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-es</th>
<td id="S2.T2.1.1.28.27.2" class="ltx_td ltx_align_center">UN product data linked</td>
</tr>
<tr id="S2.T2.1.1.29.28" class="ltx_tr">
<th id="S2.T2.1.1.29.28.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.29.28.2" class="ltx_td ltx_align_center">to industries in Spanish.</td>
</tr>
<tr id="S2.T2.1.1.30.29" class="ltx_tr">
<th id="S2.T2.1.1.30.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-fr</th>
<td id="S2.T2.1.1.30.29.2" class="ltx_td ltx_align_center">UN fine-level product data</td>
</tr>
<tr id="S2.T2.1.1.31.30" class="ltx_tr">
<th id="S2.T2.1.1.31.30.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.31.30.2" class="ltx_td ltx_align_center">in French.</td>
</tr>
<tr id="S2.T2.1.1.32.31" class="ltx_tr">
<th id="S2.T2.1.1.32.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-fr</th>
<td id="S2.T2.1.1.32.31.2" class="ltx_td ltx_align_center">UN coarse-level product data</td>
</tr>
<tr id="S2.T2.1.1.33.32" class="ltx_tr">
<th id="S2.T2.1.1.33.32.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.33.32.2" class="ltx_td ltx_align_center">in French.</td>
</tr>
<tr id="S2.T2.1.1.34.33" class="ltx_tr">
<th id="S2.T2.1.1.34.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-fr</th>
<td id="S2.T2.1.1.34.33.2" class="ltx_td ltx_align_center">UN product data linked</td>
</tr>
<tr id="S2.T2.1.1.35.34" class="ltx_tr">
<th id="S2.T2.1.1.35.34.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.35.34.2" class="ltx_td ltx_align_center">to industries in French.</td>
</tr>
<tr id="S2.T2.1.1.36.35" class="ltx_tr">
<th id="S2.T2.1.1.36.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-multi</th>
<td id="S2.T2.1.1.36.35.2" class="ltx_td ltx_align_center">UN fine-level product data</td>
</tr>
<tr id="S2.T2.1.1.37.36" class="ltx_tr">
<th id="S2.T2.1.1.37.36.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.37.36.2" class="ltx_td ltx_align_center">in multiple languages.</td>
</tr>
<tr id="S2.T2.1.1.38.37" class="ltx_tr">
<th id="S2.T2.1.1.38.37.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-multi</th>
<td id="S2.T2.1.1.38.37.2" class="ltx_td ltx_align_center">UN coarse-level product data</td>
</tr>
<tr id="S2.T2.1.1.39.38" class="ltx_tr">
<th id="S2.T2.1.1.39.38.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T2.1.1.39.38.2" class="ltx_td ltx_align_center">in multiple languages.</td>
</tr>
<tr id="S2.T2.1.1.40.39" class="ltx_tr">
<th id="S2.T2.1.1.40.39.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-multi</th>
<td id="S2.T2.1.1.40.39.2" class="ltx_td ltx_align_center">UN product data linked</td>
</tr>
<tr id="S2.T2.1.1.41.40" class="ltx_tr">
<th id="S2.T2.1.1.41.40.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<td id="S2.T2.1.1.41.40.2" class="ltx_td ltx_align_center ltx_border_bb">to industries in multiple languages.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table S-2: </span>Model names and training data sources for various models in the <span id="S2.T2.5.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model zoo. Each of these models is on the Hugging Face hub and can be found by prefixing the organization name <span id="S2.T2.6.2" class="ltx_text ltx_font_italic">dell-research-harvard</span> (for example, <span id="S2.T2.7.3" class="ltx_text ltx_font_italic">dell-research-harvard/lt-wikidata-comp-multi</span>.). Training code can be found on our package Github repo and training configs containing the hyperparameters are available in the model repo on the Hugging Face Hub.</figcaption>
</figure>
<figure id="S2.T3" class="ltx_table">
<div id="S2.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:512.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(27.2pt,-40.1pt) scale(1.18574631892964,1.18574631892964) ;">
<table id="S2.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.1.1.1.1" class="ltx_tr">
<th id="S2.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S2.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Training</span></th>
<th id="S2.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Validation</span></th>
<th id="S2.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.1.1.2.1" class="ltx_tr">
<td id="S2.T3.1.1.2.1.1" class="ltx_td"></td>
<th id="S2.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Size</span></th>
<th id="S2.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Size</span></th>
<th id="S2.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Size</span></th>
</tr>
<tr id="S2.T3.1.1.3.2" class="ltx_tr">
<td id="S2.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">lt-wikidata-comp-es</td>
<td id="S2.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">2252</td>
<td id="S2.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">359</td>
<td id="S2.T3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">345</td>
</tr>
<tr id="S2.T3.1.1.4.3" class="ltx_tr">
<td id="S2.T3.1.1.4.3.1" class="ltx_td ltx_align_left">lt-wikidata-comp-fr</td>
<td id="S2.T3.1.1.4.3.2" class="ltx_td ltx_align_center">6728</td>
<td id="S2.T3.1.1.4.3.3" class="ltx_td ltx_align_center">1034</td>
<td id="S2.T3.1.1.4.3.4" class="ltx_td ltx_align_center">1055</td>
</tr>
<tr id="S2.T3.1.1.5.4" class="ltx_tr">
<td id="S2.T3.1.1.5.4.1" class="ltx_td ltx_align_left">lt-wikidata-comp-ja</td>
<td id="S2.T3.1.1.5.4.2" class="ltx_td ltx_align_center">10120</td>
<td id="S2.T3.1.1.5.4.3" class="ltx_td ltx_align_center">1629</td>
<td id="S2.T3.1.1.5.4.4" class="ltx_td ltx_align_center">1616</td>
</tr>
<tr id="S2.T3.1.1.6.5" class="ltx_tr">
<td id="S2.T3.1.1.6.5.1" class="ltx_td ltx_align_left">lt-wikidata-comp-zh</td>
<td id="S2.T3.1.1.6.5.2" class="ltx_td ltx_align_center">5572</td>
<td id="S2.T3.1.1.6.5.3" class="ltx_td ltx_align_center">1087</td>
<td id="S2.T3.1.1.6.5.4" class="ltx_td ltx_align_center">1014</td>
</tr>
<tr id="S2.T3.1.1.7.6" class="ltx_tr">
<td id="S2.T3.1.1.7.6.1" class="ltx_td ltx_align_left">lt-wikidata-comp-de</td>
<td id="S2.T3.1.1.7.6.2" class="ltx_td ltx_align_center">10131</td>
<td id="S2.T3.1.1.7.6.3" class="ltx_td ltx_align_center">1514</td>
<td id="S2.T3.1.1.7.6.4" class="ltx_td ltx_align_center">1511</td>
</tr>
<tr id="S2.T3.1.1.8.7" class="ltx_tr">
<td id="S2.T3.1.1.8.7.1" class="ltx_td ltx_align_left">lt-wikidata-comp-en</td>
<td id="S2.T3.1.1.8.7.2" class="ltx_td ltx_align_center">28731</td>
<td id="S2.T3.1.1.8.7.3" class="ltx_td ltx_align_center">4132</td>
<td id="S2.T3.1.1.8.7.4" class="ltx_td ltx_align_center">4070</td>
</tr>
<tr id="S2.T3.1.1.9.8" class="ltx_tr">
<td id="S2.T3.1.1.9.8.1" class="ltx_td ltx_align_left">lt-wikidata-comp-multi</td>
<td id="S2.T3.1.1.9.8.2" class="ltx_td ltx_align_center">44660</td>
<td id="S2.T3.1.1.9.8.3" class="ltx_td ltx_align_center">149</td>
<td id="S2.T3.1.1.9.8.4" class="ltx_td ltx_align_center">149</td>
</tr>
<tr id="S2.T3.1.1.10.9" class="ltx_tr">
<td id="S2.T3.1.1.10.9.1" class="ltx_td ltx_align_left">lt-wikidata-comp-prod-ind-ja</td>
<td id="S2.T3.1.1.10.9.2" class="ltx_td ltx_align_center">1190</td>
<td id="S2.T3.1.1.10.9.3" class="ltx_td ltx_align_center">9950</td>
<td id="S2.T3.1.1.10.9.4" class="ltx_td ltx_align_center">9902</td>
</tr>
<tr id="S2.T3.1.1.11.10" class="ltx_tr">
<td id="S2.T3.1.1.11.10.1" class="ltx_td ltx_align_left">lt-un-data-fine-fine-en</td>
<td id="S2.T3.1.1.11.10.2" class="ltx_td ltx_align_center">1252</td>
<td id="S2.T3.1.1.11.10.3" class="ltx_td ltx_align_center">495</td>
<td id="S2.T3.1.1.11.10.4" class="ltx_td ltx_align_center">649</td>
</tr>
<tr id="S2.T3.1.1.12.11" class="ltx_tr">
<td id="S2.T3.1.1.12.11.1" class="ltx_td ltx_align_left">lt-un-data-fine-coarse-en</td>
<td id="S2.T3.1.1.12.11.2" class="ltx_td ltx_align_center">146</td>
<td id="S2.T3.1.1.12.11.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.12.11.4" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S2.T3.1.1.13.12" class="ltx_tr">
<td id="S2.T3.1.1.13.12.1" class="ltx_td ltx_align_left">lt-un-data-fine-industry-en</td>
<td id="S2.T3.1.1.13.12.2" class="ltx_td ltx_align_center">146</td>
<td id="S2.T3.1.1.13.12.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.13.12.4" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S2.T3.1.1.14.13" class="ltx_tr">
<td id="S2.T3.1.1.14.13.1" class="ltx_td ltx_align_left">lt-un-data-fine-fine-es</td>
<td id="S2.T3.1.1.14.13.2" class="ltx_td ltx_align_center">1252</td>
<td id="S2.T3.1.1.14.13.3" class="ltx_td ltx_align_center">274</td>
<td id="S2.T3.1.1.14.13.4" class="ltx_td ltx_align_center">310</td>
</tr>
<tr id="S2.T3.1.1.15.14" class="ltx_tr">
<td id="S2.T3.1.1.15.14.1" class="ltx_td ltx_align_left">lt-un-data-fine-coarse-es</td>
<td id="S2.T3.1.1.15.14.2" class="ltx_td ltx_align_center">146</td>
<td id="S2.T3.1.1.15.14.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.15.14.4" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S2.T3.1.1.16.15" class="ltx_tr">
<td id="S2.T3.1.1.16.15.1" class="ltx_td ltx_align_left">lt-un-data-fine-industry-es</td>
<td id="S2.T3.1.1.16.15.2" class="ltx_td ltx_align_center">144</td>
<td id="S2.T3.1.1.16.15.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.16.15.4" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S2.T3.1.1.17.16" class="ltx_tr">
<td id="S2.T3.1.1.17.16.1" class="ltx_td ltx_align_left">lt-un-data-fine-fine-fr</td>
<td id="S2.T3.1.1.17.16.2" class="ltx_td ltx_align_center">1185</td>
<td id="S2.T3.1.1.17.16.3" class="ltx_td ltx_align_center">210</td>
<td id="S2.T3.1.1.17.16.4" class="ltx_td ltx_align_center">255</td>
</tr>
<tr id="S2.T3.1.1.18.17" class="ltx_tr">
<td id="S2.T3.1.1.18.17.1" class="ltx_td ltx_align_left">lt-un-data-fine-coarse-fr</td>
<td id="S2.T3.1.1.18.17.2" class="ltx_td ltx_align_center">141</td>
<td id="S2.T3.1.1.18.17.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.18.17.4" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S2.T3.1.1.19.18" class="ltx_tr">
<td id="S2.T3.1.1.19.18.1" class="ltx_td ltx_align_left">lt-un-data-fine-industry-fr</td>
<td id="S2.T3.1.1.19.18.2" class="ltx_td ltx_align_center">143</td>
<td id="S2.T3.1.1.19.18.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.19.18.4" class="ltx_td ltx_align_center">18</td>
</tr>
<tr id="S2.T3.1.1.20.19" class="ltx_tr">
<td id="S2.T3.1.1.20.19.1" class="ltx_td ltx_align_left">lt-un-data-fine-fine-multi</td>
<td id="S2.T3.1.1.20.19.2" class="ltx_td ltx_align_center">1252</td>
<td id="S2.T3.1.1.20.19.3" class="ltx_td ltx_align_center">210</td>
<td id="S2.T3.1.1.20.19.4" class="ltx_td ltx_align_center">255</td>
</tr>
<tr id="S2.T3.1.1.21.20" class="ltx_tr">
<td id="S2.T3.1.1.21.20.1" class="ltx_td ltx_align_left">lt-un-data-fine-coarse-multi</td>
<td id="S2.T3.1.1.21.20.2" class="ltx_td ltx_align_center">146</td>
<td id="S2.T3.1.1.21.20.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.21.20.4" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S2.T3.1.1.22.21" class="ltx_tr">
<td id="S2.T3.1.1.22.21.1" class="ltx_td ltx_align_left">lt-un-data-fine-industry-multi</td>
<td id="S2.T3.1.1.22.21.2" class="ltx_td ltx_align_center">143</td>
<td id="S2.T3.1.1.22.21.3" class="ltx_td ltx_align_center">18</td>
<td id="S2.T3.1.1.22.21.4" class="ltx_td ltx_align_center">18</td>
</tr>
<tr id="S2.T3.1.1.23.22" class="ltx_tr">
<td id="S2.T3.1.1.23.22.1" class="ltx_td ltx_align_left">mexicantrade4748</td>
<td id="S2.T3.1.1.23.22.2" class="ltx_td ltx_align_center">1593</td>
<td id="S2.T3.1.1.23.22.3" class="ltx_td ltx_align_center">334</td>
<td id="S2.T3.1.1.23.22.4" class="ltx_td ltx_align_center">334</td>
</tr>
<tr id="S2.T3.1.1.24.23" class="ltx_tr">
<td id="S2.T3.1.1.24.23.1" class="ltx_td ltx_align_left ltx_border_bb">historicjapanesecompanies</td>
<td id="S2.T3.1.1.24.23.2" class="ltx_td ltx_align_center ltx_border_bb">412</td>
<td id="S2.T3.1.1.24.23.3" class="ltx_td ltx_align_center ltx_border_bb">33</td>
<td id="S2.T3.1.1.24.23.4" class="ltx_td ltx_align_center ltx_border_bb">36</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table S-3: </span>Model names and training, validation, and test sizes for various models in the <span id="S2.T3.4.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model zoo. The numbers correspond to the number of classes in each split - not the total number of examples. The data were split into test-train-val at the class level to avoid test set leakage. For <span id="S2.T3.5.2" class="ltx_text ltx_font_italic">historicjapanesecompanies</span> the dataset is of positive and negative pairs - so we report only the counts of positive pairs for consistency with other tasks.</figcaption>
</figure>
<figure id="S2.T4" class="ltx_table">
<div id="S2.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:390.3pt;height:504.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(21.0pt,-27.1pt) scale(1.12034841145974,1.12034841145974) ;">
<table id="S2.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T4.1.1.1.1" class="ltx_tr">
<th id="S2.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S2.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S2.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Edit Distance</span></td>
<td id="S2.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">SBERT</span></td>
<td id="S2.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">LT</span></td>
<td id="S2.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold">OpenAI</span></td>
</tr>
<tr id="S2.T4.1.1.2.2" class="ltx_tr">
<th id="S2.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="5"><span id="S2.T4.1.1.2.2.1.1" class="ltx_text ltx_font_italic">Company Linkage</span></th>
</tr>
<tr id="S2.T4.1.1.3.3" class="ltx_tr">
<th id="S2.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-es</th>
<td id="S2.T4.1.1.3.3.2" class="ltx_td ltx_align_center">0.69</td>
<td id="S2.T4.1.1.3.3.3" class="ltx_td ltx_align_center">0.68</td>
<td id="S2.T4.1.1.3.3.4" class="ltx_td ltx_align_center">0.75</td>
<td id="S2.T4.1.1.3.3.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.3.3.5.1" class="ltx_text ltx_font_bold">0.81</span></td>
</tr>
<tr id="S2.T4.1.1.4.4" class="ltx_tr">
<th id="S2.T4.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-fr</th>
<td id="S2.T4.1.1.4.4.2" class="ltx_td ltx_align_center">0.55</td>
<td id="S2.T4.1.1.4.4.3" class="ltx_td ltx_align_center">0.78</td>
<td id="S2.T4.1.1.4.4.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.4.4.4.1" class="ltx_text ltx_font_bold">0.84</span></td>
<td id="S2.T4.1.1.4.4.5" class="ltx_td ltx_align_center">0.80</td>
</tr>
<tr id="S2.T4.1.1.5.5" class="ltx_tr">
<th id="S2.T4.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-ja</th>
<td id="S2.T4.1.1.5.5.2" class="ltx_td ltx_align_center">0.53</td>
<td id="S2.T4.1.1.5.5.3" class="ltx_td ltx_align_center">0.62</td>
<td id="S2.T4.1.1.5.5.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.5.5.4.1" class="ltx_text ltx_font_bold">0.70</span></td>
<td id="S2.T4.1.1.5.5.5" class="ltx_td ltx_align_center">0.62</td>
</tr>
<tr id="S2.T4.1.1.6.6" class="ltx_tr">
<th id="S2.T4.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-zh</th>
<td id="S2.T4.1.1.6.6.2" class="ltx_td ltx_align_center">0.61</td>
<td id="S2.T4.1.1.6.6.3" class="ltx_td ltx_align_center">0.73</td>
<td id="S2.T4.1.1.6.6.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.6.6.4.1" class="ltx_text ltx_font_bold">0.80</span></td>
<td id="S2.T4.1.1.6.6.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.6.6.5.1" class="ltx_text ltx_font_bold">0.80</span></td>
</tr>
<tr id="S2.T4.1.1.7.7" class="ltx_tr">
<th id="S2.T4.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-de</th>
<td id="S2.T4.1.1.7.7.2" class="ltx_td ltx_align_center">0.63</td>
<td id="S2.T4.1.1.7.7.3" class="ltx_td ltx_align_center">0.69</td>
<td id="S2.T4.1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.7.7.4.1" class="ltx_text ltx_font_bold">0.77</span></td>
<td id="S2.T4.1.1.7.7.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.7.7.5.1" class="ltx_text ltx_font_bold">0.77</span></td>
</tr>
<tr id="S2.T4.1.1.8.8" class="ltx_tr">
<th id="S2.T4.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-en</th>
<td id="S2.T4.1.1.8.8.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S2.T4.1.1.8.8.3" class="ltx_td ltx_align_center">0.72</td>
<td id="S2.T4.1.1.8.8.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.8.8.4.1" class="ltx_text ltx_font_bold">0.74</span></td>
<td id="S2.T4.1.1.8.8.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.8.8.5.1" class="ltx_text ltx_font_bold">0.74</span></td>
</tr>
<tr id="S2.T4.1.1.9.9" class="ltx_tr">
<th id="S2.T4.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-multi</th>
<td id="S2.T4.1.1.9.9.2" class="ltx_td ltx_align_center">0.69</td>
<td id="S2.T4.1.1.9.9.3" class="ltx_td ltx_align_center">0.62</td>
<td id="S2.T4.1.1.9.9.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.9.9.4.1" class="ltx_text ltx_font_bold">0.80</span></td>
<td id="S2.T4.1.1.9.9.5" class="ltx_td ltx_align_center">0.74</td>
</tr>
<tr id="S2.T4.1.1.10.10" class="ltx_tr">
<th id="S2.T4.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-wikidata-comp-prod-ind-ja</th>
<td id="S2.T4.1.1.10.10.2" class="ltx_td ltx_align_center">0.96</td>
<td id="S2.T4.1.1.10.10.3" class="ltx_td ltx_align_center">0.95</td>
<td id="S2.T4.1.1.10.10.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.10.10.4.1" class="ltx_text ltx_font_bold">0.99</span></td>
<td id="S2.T4.1.1.10.10.5" class="ltx_td ltx_align_center">0.98</td>
</tr>
<tr id="S2.T4.1.1.11.11" class="ltx_tr">
<th id="S2.T4.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="5"><span id="S2.T4.1.1.11.11.1.1" class="ltx_text ltx_font_italic">Fine Product Linkage</span></th>
</tr>
<tr id="S2.T4.1.1.12.12" class="ltx_tr">
<th id="S2.T4.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-en</th>
<td id="S2.T4.1.1.12.12.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S2.T4.1.1.12.12.3" class="ltx_td ltx_align_center">0.76</td>
<td id="S2.T4.1.1.12.12.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.12.12.4.1" class="ltx_text ltx_font_bold">0.83</span></td>
<td id="S2.T4.1.1.12.12.5" class="ltx_td ltx_align_center">0.78</td>
</tr>
<tr id="S2.T4.1.1.13.13" class="ltx_tr">
<th id="S2.T4.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-es</th>
<td id="S2.T4.1.1.13.13.2" class="ltx_td ltx_align_center">0.57</td>
<td id="S2.T4.1.1.13.13.3" class="ltx_td ltx_align_center">0.67</td>
<td id="S2.T4.1.1.13.13.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.13.13.4.1" class="ltx_text ltx_font_bold">0.76</span></td>
<td id="S2.T4.1.1.13.13.5" class="ltx_td ltx_align_center">0.68</td>
</tr>
<tr id="S2.T4.1.1.14.14" class="ltx_tr">
<th id="S2.T4.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-fr</th>
<td id="S2.T4.1.1.14.14.2" class="ltx_td ltx_align_center">0.52</td>
<td id="S2.T4.1.1.14.14.3" class="ltx_td ltx_align_center">0.67</td>
<td id="S2.T4.1.1.14.14.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.14.14.4.1" class="ltx_text ltx_font_bold">0.73</span></td>
<td id="S2.T4.1.1.14.14.5" class="ltx_td ltx_align_center">0.68</td>
</tr>
<tr id="S2.T4.1.1.15.15" class="ltx_tr">
<th id="S2.T4.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-fine-multi</th>
<td id="S2.T4.1.1.15.15.2" class="ltx_td ltx_align_center">0.52</td>
<td id="S2.T4.1.1.15.15.3" class="ltx_td ltx_align_center">0.57</td>
<td id="S2.T4.1.1.15.15.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.15.15.4.1" class="ltx_text ltx_font_bold">0.73</span></td>
<td id="S2.T4.1.1.15.15.5" class="ltx_td ltx_align_center">0.68</td>
</tr>
<tr id="S2.T4.1.1.16.16" class="ltx_tr">
<th id="S2.T4.1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="5"><span id="S2.T4.1.1.16.16.1.1" class="ltx_text ltx_font_italic">Product to Industry Linkage</span></th>
</tr>
<tr id="S2.T4.1.1.17.17" class="ltx_tr">
<th id="S2.T4.1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-en</th>
<td id="S2.T4.1.1.17.17.2" class="ltx_td ltx_align_center">0.29</td>
<td id="S2.T4.1.1.17.17.3" class="ltx_td ltx_align_center">0.74</td>
<td id="S2.T4.1.1.17.17.4" class="ltx_td ltx_align_center">0.84</td>
<td id="S2.T4.1.1.17.17.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.17.17.5.1" class="ltx_text ltx_font_bold">0.95</span></td>
</tr>
<tr id="S2.T4.1.1.18.18" class="ltx_tr">
<th id="S2.T4.1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-es</th>
<td id="S2.T4.1.1.18.18.2" class="ltx_td ltx_align_center">0.41</td>
<td id="S2.T4.1.1.18.18.3" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.18.18.3.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S2.T4.1.1.18.18.4" class="ltx_td ltx_align_center">0.89</td>
<td id="S2.T4.1.1.18.18.5" class="ltx_td ltx_align_center">0.89</td>
</tr>
<tr id="S2.T4.1.1.19.19" class="ltx_tr">
<th id="S2.T4.1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-fr</th>
<td id="S2.T4.1.1.19.19.2" class="ltx_td ltx_align_center">0.19</td>
<td id="S2.T4.1.1.19.19.3" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.19.19.3.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S2.T4.1.1.19.19.4" class="ltx_td ltx_align_center">0.67</td>
<td id="S2.T4.1.1.19.19.5" class="ltx_td ltx_align_center">0.61</td>
</tr>
<tr id="S2.T4.1.1.20.20" class="ltx_tr">
<th id="S2.T4.1.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-industry-multi</th>
<td id="S2.T4.1.1.20.20.2" class="ltx_td ltx_align_center">0.19</td>
<td id="S2.T4.1.1.20.20.3" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.20.20.3.1" class="ltx_text ltx_font_bold">0.67</span></td>
<td id="S2.T4.1.1.20.20.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.20.20.4.1" class="ltx_text ltx_font_bold">0.67</span></td>
<td id="S2.T4.1.1.20.20.5" class="ltx_td ltx_align_center">0.61</td>
</tr>
<tr id="S2.T4.1.1.21.21" class="ltx_tr">
<th id="S2.T4.1.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="5"><span id="S2.T4.1.1.21.21.1.1" class="ltx_text ltx_font_italic">Product Aggregation</span></th>
</tr>
<tr id="S2.T4.1.1.22.22" class="ltx_tr">
<th id="S2.T4.1.1.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-en</th>
<td id="S2.T4.1.1.22.22.2" class="ltx_td ltx_align_center">0.61</td>
<td id="S2.T4.1.1.22.22.3" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.22.22.3.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S2.T4.1.1.22.22.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.22.22.4.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S2.T4.1.1.22.22.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.22.22.5.1" class="ltx_text ltx_font_bold">0.95</span></td>
</tr>
<tr id="S2.T4.1.1.23.23" class="ltx_tr">
<th id="S2.T4.1.1.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-es</th>
<td id="S2.T4.1.1.23.23.2" class="ltx_td ltx_align_center">0.47</td>
<td id="S2.T4.1.1.23.23.3" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.23.23.3.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S2.T4.1.1.23.23.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.23.23.4.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S2.T4.1.1.23.23.5" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.23.23.5.1" class="ltx_text ltx_font_bold">0.95</span></td>
</tr>
<tr id="S2.T4.1.1.24.24" class="ltx_tr">
<th id="S2.T4.1.1.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lt-un-data-fine-coarse-fr</th>
<td id="S2.T4.1.1.24.24.2" class="ltx_td ltx_align_center">0.53</td>
<td id="S2.T4.1.1.24.24.3" class="ltx_td ltx_align_center">0.89</td>
<td id="S2.T4.1.1.24.24.4" class="ltx_td ltx_align_center"><span id="S2.T4.1.1.24.24.4.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S2.T4.1.1.24.24.5" class="ltx_td ltx_align_center">0.95</td>
</tr>
<tr id="S2.T4.1.1.25.25" class="ltx_tr">
<th id="S2.T4.1.1.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">lt-un-data-fine-coarse-multi</th>
<td id="S2.T4.1.1.25.25.2" class="ltx_td ltx_align_center ltx_border_bb">0.53</td>
<td id="S2.T4.1.1.25.25.3" class="ltx_td ltx_align_center ltx_border_bb">0.95</td>
<td id="S2.T4.1.1.25.25.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T4.1.1.25.25.4.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S2.T4.1.1.25.25.5" class="ltx_td ltx_align_center ltx_border_bb">0.95</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-4: </span>Performance of various embedding models. Performance is measured by retrieval accuracy at 1 - whether the nearest neighbor in terms of edit distance or cosine similarity is a "relevant" entity. <span id="S2.T4.11.1" class="ltx_text ltx_font_italic">Company linkage</span> links company aliases together, <span id="S2.T4.12.2" class="ltx_text ltx_font_italic">Fine Product Linkage</span> links products from different product classifications together, <span id="S2.T4.13.3" class="ltx_text ltx_font_italic">Product to Industry Linkage</span> links products to their industry classifications, and <span id="S2.T4.14.4" class="ltx_text ltx_font_italic">Product Aggregation</span> links a fine product to its coarser product classification.
<span id="S2.T4.15.5" class="ltx_text ltx_font_italic">LT</span> gives the performance of the trained LinkTransformer model specified in the Model Column (and found on HuggingFace).
<span id="S2.T4.16.6" class="ltx_text ltx_font_italic">Edit Distance</span> gives to linkage accuracy when using Levenshtein distance as the distance metric. <span id="S2.T4.17.7" class="ltx_text ltx_font_italic">SBERT</span> gives linkage accuracy with the base off-the-shelf model used to tune the <span id="S2.T4.18.8" class="ltx_text ltx_font_typewriter">LinkTransformer</span> model (as specified for each language in <a href="#S2.T1" title="Table S-1 ‣ S-2 Datasets and Results ‣ LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-1</span></a>).
<span id="S2.T4.19.9" class="ltx_text ltx_font_italic">OpenAI</span> gives linkage performance when using embeddings from the OpenAI embedding API (text-embedding-ada-002).</figcaption>
</figure>
<figure id="S2.T5" class="ltx_table">
<div id="S2.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:76.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(13.0pt,-2.3pt) scale(1.06366138628581,1.06366138628581) ;">
<table id="S2.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T5.1.1.1.1" class="ltx_tr">
<th id="S2.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S2.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Semantic</span></th>
<th id="S2.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Fine</span></th>
<th id="S2.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Edit</span></th>
<th id="S2.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T5.1.1.1.1.5.1" class="ltx_text ltx_font_bold">OpenAI</span></th>
<th id="S2.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T5.1.1.1.1.6.1" class="ltx_text ltx_font_bold">LT</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T5.1.1.2.1" class="ltx_tr">
<th id="S2.T5.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S2.T5.1.1.2.1.2" class="ltx_td ltx_align_center"><span id="S2.T5.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Sim</span></td>
<td id="S2.T5.1.1.2.1.3" class="ltx_td ltx_align_center"><span id="S2.T5.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Tuned</span></td>
<td id="S2.T5.1.1.2.1.4" class="ltx_td ltx_align_center"><span id="S2.T5.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Distance</span></td>
<td id="S2.T5.1.1.2.1.5" class="ltx_td ltx_align_center"><span id="S2.T5.1.1.2.1.5.1" class="ltx_text ltx_font_bold">ADA</span></td>
<td id="S2.T5.1.1.2.1.6" class="ltx_td ltx_align_center"><span id="S2.T5.1.1.2.1.6.1" class="ltx_text ltx_font_bold">UN/Wiki Model</span></td>
</tr>
<tr id="S2.T5.1.1.3.2" class="ltx_tr">
<th id="S2.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">mexicantrade4748</th>
<th id="S2.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">0.81</th>
<th id="S2.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T5.1.1.3.2.3.1" class="ltx_text ltx_font_bold">0.89</span></th>
<th id="S2.T5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">0.78</th>
<th id="S2.T5.1.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">0.88</th>
<th id="S2.T5.1.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">0.88</th>
</tr>
<tr id="S2.T5.1.1.4.3" class="ltx_tr">
<th id="S2.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">historicjapanesecompanies</th>
<td id="S2.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.66</td>
<td id="S2.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.79</td>
<td id="S2.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.29</td>
<td id="S2.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T5.1.1.4.3.5.1" class="ltx_text ltx_font_bold">0.83</span></td>
<td id="S2.T5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.80</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-5: </span>Historical Linking. We examine the base semantic similarity model off-the-shelf, a fine-tuned <span id="S2.T5.5.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> version, Levenshtein edit distance on the tariff description or company name, OpenAI embeddings and a pre-trained LinkTransformer model - <span id="S2.T5.6.2" class="ltx_text ltx_font_italic">lt-wikidata-comp-prod-ind-ja</span> mexicantrade4748 and <span id="S2.T5.7.3" class="ltx_text ltx_font_italic">lt-un-data-fine-fine-multi</span> for historicjapanesecompanies. The table reports top-1 accuracy. </figcaption>
</figure>
<figure id="S2.T6" class="ltx_table">
<table id="S2.T6.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T6.1.1.1" class="ltx_tr">
<td id="S2.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S2.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S2.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Domain</span></td>
<td id="S2.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S2.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.5.1" class="ltx_text ltx_font_bold"># Pos.</span></td>
<td id="S2.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.6.1" class="ltx_text ltx_font_bold"># Attr.</span></td>
<td id="S2.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.7.1" class="ltx_text ltx_font_bold">Ours (ZS)</span></td>
<td id="S2.T6.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.8.1" class="ltx_text ltx_font_bold">Ours (FT)</span></td>
<td id="S2.T6.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.9.1" class="ltx_text ltx_font_bold">Magellan</span></td>
<td id="S2.T6.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.10.1" class="ltx_text ltx_font_bold">Deep matcher</span></td>
<td id="S2.T6.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.11.1" class="ltx_text ltx_font_bold">Ditto</span></td>
<td id="S2.T6.1.1.1.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T6.1.1.1.12.1" class="ltx_text ltx_font_bold">REMS</span></td>
</tr>
<tr id="S2.T6.1.2.2" class="ltx_tr">
<td id="S2.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="7"><span id="S2.T6.1.2.2.1.1" class="ltx_text">Structured</span></td>
<td id="S2.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">BeerAdvo-RateBeer</td>
<td id="S2.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">beer</td>
<td id="S2.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">450</td>
<td id="S2.T6.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">68</td>
<td id="S2.T6.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">4</td>
<td id="S2.T6.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">82.35</td>
<td id="S2.T6.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">87.5</td>
<td id="S2.T6.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">78.8</td>
<td id="S2.T6.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">72.7</td>
<td id="S2.T6.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">84.59</td>
<td id="S2.T6.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t">96.65</td>
</tr>
<tr id="S2.T6.1.3.3" class="ltx_tr">
<td id="S2.T6.1.3.3.1" class="ltx_td ltx_align_center">iTunes-Amazon1</td>
<td id="S2.T6.1.3.3.2" class="ltx_td ltx_align_center">music</td>
<td id="S2.T6.1.3.3.3" class="ltx_td ltx_align_center">539</td>
<td id="S2.T6.1.3.3.4" class="ltx_td ltx_align_center">132</td>
<td id="S2.T6.1.3.3.5" class="ltx_td ltx_align_center">8</td>
<td id="S2.T6.1.3.3.6" class="ltx_td ltx_align_center">70</td>
<td id="S2.T6.1.3.3.7" class="ltx_td ltx_align_center">80</td>
<td id="S2.T6.1.3.3.8" class="ltx_td ltx_align_center">91.2</td>
<td id="S2.T6.1.3.3.9" class="ltx_td ltx_align_center">88.5</td>
<td id="S2.T6.1.3.3.10" class="ltx_td ltx_align_center">92.28</td>
<td id="S2.T6.1.3.3.11" class="ltx_td ltx_align_center">98.18</td>
</tr>
<tr id="S2.T6.1.4.4" class="ltx_tr">
<td id="S2.T6.1.4.4.1" class="ltx_td ltx_align_center">Fodors-Zagats</td>
<td id="S2.T6.1.4.4.2" class="ltx_td ltx_align_center">restaurant</td>
<td id="S2.T6.1.4.4.3" class="ltx_td ltx_align_center">946</td>
<td id="S2.T6.1.4.4.4" class="ltx_td ltx_align_center">110</td>
<td id="S2.T6.1.4.4.5" class="ltx_td ltx_align_center">6</td>
<td id="S2.T6.1.4.4.6" class="ltx_td ltx_align_center">88</td>
<td id="S2.T6.1.4.4.7" class="ltx_td ltx_align_center">93</td>
<td id="S2.T6.1.4.4.8" class="ltx_td ltx_align_center">100</td>
<td id="S2.T6.1.4.4.9" class="ltx_td ltx_align_center">100</td>
<td id="S2.T6.1.4.4.10" class="ltx_td ltx_align_center">98.14</td>
<td id="S2.T6.1.4.4.11" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="S2.T6.1.5.5" class="ltx_tr">
<td id="S2.T6.1.5.5.1" class="ltx_td ltx_align_center">DBLP-ACM1</td>
<td id="S2.T6.1.5.5.2" class="ltx_td ltx_align_center">citation</td>
<td id="S2.T6.1.5.5.3" class="ltx_td ltx_align_center">12,363</td>
<td id="S2.T6.1.5.5.4" class="ltx_td ltx_align_center">2,220</td>
<td id="S2.T6.1.5.5.5" class="ltx_td ltx_align_center">4</td>
<td id="S2.T6.1.5.5.6" class="ltx_td ltx_align_center">90</td>
<td id="S2.T6.1.5.5.7" class="ltx_td ltx_align_center">97.5</td>
<td id="S2.T6.1.5.5.8" class="ltx_td ltx_align_center">98.4</td>
<td id="S2.T6.1.5.5.9" class="ltx_td ltx_align_center">98.4</td>
<td id="S2.T6.1.5.5.10" class="ltx_td ltx_align_center">98.96</td>
<td id="S2.T6.1.5.5.11" class="ltx_td ltx_align_center">98.18</td>
</tr>
<tr id="S2.T6.1.6.6" class="ltx_tr">
<td id="S2.T6.1.6.6.1" class="ltx_td ltx_align_center">DBLP-Scholar1</td>
<td id="S2.T6.1.6.6.2" class="ltx_td ltx_align_center">citation</td>
<td id="S2.T6.1.6.6.3" class="ltx_td ltx_align_center">28,707</td>
<td id="S2.T6.1.6.6.4" class="ltx_td ltx_align_center">5,347</td>
<td id="S2.T6.1.6.6.5" class="ltx_td ltx_align_center">4</td>
<td id="S2.T6.1.6.6.6" class="ltx_td ltx_align_center">76</td>
<td id="S2.T6.1.6.6.7" class="ltx_td ltx_align_center">91.4</td>
<td id="S2.T6.1.6.6.8" class="ltx_td ltx_align_center">92.3</td>
<td id="S2.T6.1.6.6.9" class="ltx_td ltx_align_center">94.7</td>
<td id="S2.T6.1.6.6.10" class="ltx_td ltx_align_center">95.6</td>
<td id="S2.T6.1.6.6.11" class="ltx_td ltx_align_center">91.74</td>
</tr>
<tr id="S2.T6.1.7.7" class="ltx_tr">
<td id="S2.T6.1.7.7.1" class="ltx_td ltx_align_center">Amazon-Google</td>
<td id="S2.T6.1.7.7.2" class="ltx_td ltx_align_center">software</td>
<td id="S2.T6.1.7.7.3" class="ltx_td ltx_align_center">11,460</td>
<td id="S2.T6.1.7.7.4" class="ltx_td ltx_align_center">1,167</td>
<td id="S2.T6.1.7.7.5" class="ltx_td ltx_align_center">3</td>
<td id="S2.T6.1.7.7.6" class="ltx_td ltx_align_center">39.4</td>
<td id="S2.T6.1.7.7.7" class="ltx_td ltx_align_center">68</td>
<td id="S2.T6.1.7.7.8" class="ltx_td ltx_align_center">49.1</td>
<td id="S2.T6.1.7.7.9" class="ltx_td ltx_align_center">69.3</td>
<td id="S2.T6.1.7.7.10" class="ltx_td ltx_align_center">74.1</td>
<td id="S2.T6.1.7.7.11" class="ltx_td ltx_align_center">65.3</td>
</tr>
<tr id="S2.T6.1.8.8" class="ltx_tr">
<td id="S2.T6.1.8.8.1" class="ltx_td ltx_align_center">Walmart-Amazon1</td>
<td id="S2.T6.1.8.8.2" class="ltx_td ltx_align_center">electronics</td>
<td id="S2.T6.1.8.8.3" class="ltx_td ltx_align_center">10,242</td>
<td id="S2.T6.1.8.8.4" class="ltx_td ltx_align_center">962</td>
<td id="S2.T6.1.8.8.5" class="ltx_td ltx_align_center">5</td>
<td id="S2.T6.1.8.8.6" class="ltx_td ltx_align_center">28</td>
<td id="S2.T6.1.8.8.7" class="ltx_td ltx_align_center">69</td>
<td id="S2.T6.1.8.8.8" class="ltx_td ltx_align_center">71.9</td>
<td id="S2.T6.1.8.8.9" class="ltx_td ltx_align_center">67.6</td>
<td id="S2.T6.1.8.8.10" class="ltx_td ltx_align_center">85.81</td>
<td id="S2.T6.1.8.8.11" class="ltx_td ltx_align_center">71.34</td>
</tr>
<tr id="S2.T6.1.9.9" class="ltx_tr">
<td id="S2.T6.1.9.9.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T6.1.9.9.1.1" class="ltx_text">Textual</span></td>
<td id="S2.T6.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">Abt-Buy</td>
<td id="S2.T6.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">product</td>
<td id="S2.T6.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">9,575</td>
<td id="S2.T6.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t">1,028</td>
<td id="S2.T6.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S2.T6.1.9.9.7" class="ltx_td ltx_align_center ltx_border_t">33.6</td>
<td id="S2.T6.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t">78.4</td>
<td id="S2.T6.1.9.9.9" class="ltx_td ltx_align_center ltx_border_t">33</td>
<td id="S2.T6.1.9.9.10" class="ltx_td ltx_align_center ltx_border_t">55</td>
<td id="S2.T6.1.9.9.11" class="ltx_td ltx_align_center ltx_border_t">88.85</td>
<td id="S2.T6.1.9.9.12" class="ltx_td ltx_align_center ltx_border_t">67.4</td>
</tr>
<tr id="S2.T6.1.10.10" class="ltx_tr">
<td id="S2.T6.1.10.10.1" class="ltx_td"></td>
<td id="S2.T6.1.10.10.2" class="ltx_td ltx_align_center">Company</td>
<td id="S2.T6.1.10.10.3" class="ltx_td ltx_align_center">company</td>
<td id="S2.T6.1.10.10.4" class="ltx_td ltx_align_center">1,12,632</td>
<td id="S2.T6.1.10.10.5" class="ltx_td ltx_align_center">28,200</td>
<td id="S2.T6.1.10.10.6" class="ltx_td ltx_align_center">1</td>
<td id="S2.T6.1.10.10.7" class="ltx_td ltx_align_center">74.07</td>
<td id="S2.T6.1.10.10.8" class="ltx_td ltx_align_center">88</td>
<td id="S2.T6.1.10.10.9" class="ltx_td ltx_align_center">79.8</td>
<td id="S2.T6.1.10.10.10" class="ltx_td ltx_align_center">92.7</td>
<td id="S2.T6.1.10.10.11" class="ltx_td ltx_align_center">41.00</td>
<td id="S2.T6.1.10.10.12" class="ltx_td ltx_align_center">80.73</td>
</tr>
<tr id="S2.T6.1.11.11" class="ltx_tr">
<td id="S2.T6.1.11.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" rowspan="4"><span id="S2.T6.1.11.11.1.1" class="ltx_text">Dirty</span></td>
<td id="S2.T6.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t">iTunes-Amazon2</td>
<td id="S2.T6.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t">music</td>
<td id="S2.T6.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t">539</td>
<td id="S2.T6.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t">132</td>
<td id="S2.T6.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t">8</td>
<td id="S2.T6.1.11.11.7" class="ltx_td ltx_align_center ltx_border_t">74</td>
<td id="S2.T6.1.11.11.8" class="ltx_td ltx_align_center ltx_border_t">81.3</td>
<td id="S2.T6.1.11.11.9" class="ltx_td ltx_align_center ltx_border_t">46.8</td>
<td id="S2.T6.1.11.11.10" class="ltx_td ltx_align_center ltx_border_t">79.4</td>
<td id="S2.T6.1.11.11.11" class="ltx_td ltx_align_center ltx_border_t">92.92</td>
<td id="S2.T6.1.11.11.12" class="ltx_td ltx_align_center ltx_border_t">94.74</td>
</tr>
<tr id="S2.T6.1.12.12" class="ltx_tr">
<td id="S2.T6.1.12.12.1" class="ltx_td ltx_align_center">DBLP-ACM2</td>
<td id="S2.T6.1.12.12.2" class="ltx_td ltx_align_center">citation</td>
<td id="S2.T6.1.12.12.3" class="ltx_td ltx_align_center">12,363</td>
<td id="S2.T6.1.12.12.4" class="ltx_td ltx_align_center">2,220</td>
<td id="S2.T6.1.12.12.5" class="ltx_td ltx_align_center">4</td>
<td id="S2.T6.1.12.12.6" class="ltx_td ltx_align_center">79.6</td>
<td id="S2.T6.1.12.12.7" class="ltx_td ltx_align_center">97.2</td>
<td id="S2.T6.1.12.12.8" class="ltx_td ltx_align_center">91.9</td>
<td id="S2.T6.1.12.12.9" class="ltx_td ltx_align_center">98.1</td>
<td id="S2.T6.1.12.12.10" class="ltx_td ltx_align_center">98.92</td>
<td id="S2.T6.1.12.12.11" class="ltx_td ltx_align_center">98.19</td>
</tr>
<tr id="S2.T6.1.13.13" class="ltx_tr">
<td id="S2.T6.1.13.13.1" class="ltx_td ltx_align_center">DBLP-Scholar2</td>
<td id="S2.T6.1.13.13.2" class="ltx_td ltx_align_center">citation</td>
<td id="S2.T6.1.13.13.3" class="ltx_td ltx_align_center">28,707</td>
<td id="S2.T6.1.13.13.4" class="ltx_td ltx_align_center">5,347</td>
<td id="S2.T6.1.13.13.5" class="ltx_td ltx_align_center">4</td>
<td id="S2.T6.1.13.13.6" class="ltx_td ltx_align_center">75</td>
<td id="S2.T6.1.13.13.7" class="ltx_td ltx_align_center">91.2</td>
<td id="S2.T6.1.13.13.8" class="ltx_td ltx_align_center">82.5</td>
<td id="S2.T6.1.13.13.9" class="ltx_td ltx_align_center">93.8</td>
<td id="S2.T6.1.13.13.10" class="ltx_td ltx_align_center">95.44</td>
<td id="S2.T6.1.13.13.11" class="ltx_td ltx_align_center">91.76</td>
</tr>
<tr id="S2.T6.1.14.14" class="ltx_tr">
<td id="S2.T6.1.14.14.1" class="ltx_td ltx_align_center ltx_border_b">Walmart-Amazon2</td>
<td id="S2.T6.1.14.14.2" class="ltx_td ltx_align_center ltx_border_b">electronics</td>
<td id="S2.T6.1.14.14.3" class="ltx_td ltx_align_center ltx_border_b">10,242</td>
<td id="S2.T6.1.14.14.4" class="ltx_td ltx_align_center ltx_border_b">962</td>
<td id="S2.T6.1.14.14.5" class="ltx_td ltx_align_center ltx_border_b">5</td>
<td id="S2.T6.1.14.14.6" class="ltx_td ltx_align_center ltx_border_b">25</td>
<td id="S2.T6.1.14.14.7" class="ltx_td ltx_align_center ltx_border_b">65</td>
<td id="S2.T6.1.14.14.8" class="ltx_td ltx_align_center ltx_border_b">37.4</td>
<td id="S2.T6.1.14.14.9" class="ltx_td ltx_align_center ltx_border_b">53.8</td>
<td id="S2.T6.1.14.14.10" class="ltx_td ltx_align_center ltx_border_b">82.56</td>
<td id="S2.T6.1.14.14.11" class="ltx_td ltx_align_center ltx_border_b">65.74</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table S-6: </span>Benchmarks. ZS is <span id="S2.T6.4.1" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models zero-shot and FT is <span id="S2.T6.5.2" class="ltx_text ltx_font_typewriter">LinkTransformer</span> models fine-tuned on the benchmark. The remaining columns report comparisons. The metric is F1, as these datasets frame linkage as a binary classification problem.</figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramitzky et al. (2021)</span>
<span class="ltx_bibblock">
Ran Abramitzky, Leah Boustan, Katherine Eriksson, James Feigenbaum, and
Santiago Pérez. 2021.

</span>
<span class="ltx_bibblock">Automated linking of historical data.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Economic Literature</em>, 59(3):865–918.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. (2023)</span>
<span class="ltx_bibblock">
Abhishek Arora, Xinmei Yang, Shao Yu Jheng, and Melissa Dell. 2023.

</span>
<span class="ltx_bibblock">Linking representations with multimodal contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03464</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biewald (2020)</span>
<span class="ltx_bibblock">
Lukas Biewald. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.wandb.com/" title="" class="ltx_ref ltx_href">Experiment tracking with weights and
biases</a>.

</span>
<span class="ltx_bibblock">Software available from wandb.com.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binette and Steorts (2022)</span>
<span class="ltx_bibblock">
Olivier Binette and Rebecca C Steorts. 2022.

</span>
<span class="ltx_bibblock">(almost) all of entity resolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Science Advances</em>, 8(12):eabi8021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brunner and Stockinger (2020)</span>
<span class="ltx_bibblock">
Ursin Brunner and Kurt Stockinger. 2020.

</span>
<span class="ltx_bibblock">Entity matching with transformer architectures-a step forward in data
integration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">23rd International Conference on Extending Database
Technology, Copenhagen, 30 March-2 April 2020</em>, pages 463–473.
OpenProceedings.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2015)</span>
<span class="ltx_bibblock">
Sanjib Das, A Doan, C Gokhale Psgc, Pradap Konda, Yash Govind, and Derek
Paulsen. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sites.%20google.%20com/site/anhaidgroup/useful-stuff/data" title="" class="ltx_ref ltx_href">The magellan data repository</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh. 2019.

</span>
<span class="ltx_bibblock">How contextual are contextualized word representations? comparing the
geometry of bert, elmo, and gpt-2 embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.00512</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gregg and Eder (2022)</span>
<span class="ltx_bibblock">
Forest Gregg and Derek Eder. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/dedupeio/dedupe" title="" class="ltx_ref ltx_href">dedupe</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu and Horwood (2022)</span>
<span class="ltx_bibblock">
Benjamin Hsu and Graham Horwood. 2022.

</span>
<span class="ltx_bibblock">Contrastive representation learning for cross-document coreference
resolution of events and entities.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.11438</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jinji Koshinjo (1954)</span>
<span class="ltx_bibblock">
Jinji Koshinjo. 1954.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Nihon shokuinrokj</em>.

</span>
<span class="ltx_bibblock">Jinji Koshinjo.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, 7(3):535–547.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2021)</span>
<span class="ltx_bibblock">
Salil Rajeev Joshi, Arpan Somani, and Shourya Roy. 2021.

</span>
<span class="ltx_bibblock">Relink: Complete-link industrial record linkage over hybrid feature
spaces.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 37th International Conference on Data Engineering
(ICDE)</em>, pages 2625–2636. IEEE.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu,
Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.04906</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla et al. (2020)</span>
<span class="ltx_bibblock">
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.

</span>
<span class="ltx_bibblock">Supervised contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.11362</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpcke et al. (2010)</span>
<span class="ltx_bibblock">
Hanna Köpcke, Andreas Thor, and Erhard Rahm. 2010.

</span>
<span class="ltx_bibblock">Evaluation of entity resolution approaches on real-world match
problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment</em>, 3(1-2):484–493.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein et al. (1966)</span>
<span class="ltx_bibblock">
Vladimir I Levenshtein et al. 1966.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Soviet physics doklady</em>, volume 10, pages 707–710. Soviet
Union.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yuliang Li, Jinfeng Li, Yoshi Suhara, AnHai Doan, and Wang-Chiew Tan. 2023.

</span>
<span class="ltx_bibblock">Effective entity matching with transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">The VLDB Journal</em>, pages 1–21.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan. 2020.

</span>
<span class="ltx_bibblock">Deep entity matching with pre-trained language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.00584</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peeters and Bizer (2023)</span>
<span class="ltx_bibblock">
Ralph Peeters and Christian Bizer. 2023.

</span>
<span class="ltx_bibblock">Using chatgpt for entity matching.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.03423</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Primpeli et al. (2019)</span>
<span class="ltx_bibblock">
Anna Primpeli, Ralph Peeters, and Christian Bizer. 2019.

</span>
<span class="ltx_bibblock">The wdc training dataset and gold standard for large-scale product
matching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Companion Proceedings of The 2019 World Wide Web
Conference</em>, pages 381–386.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.10084</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Secretaria de Economía de Mexico (1948)</span>
<span class="ltx_bibblock">
Secretaria de Economía de Mexico. 1948.

</span>
<span class="ltx_bibblock">Ajuste de las fracciones de la tarifa arancelaria que rigieron hasta
el año de 1947 con las de la tarifa que entró en vigor por decreto de fecha
13 de diciembre del mismo año y se consideraron a partir de 1948.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Anuario Estadístico del Comercio Exterior de los Estados
Unidos Mexicanos</em>. Gobierno de Mexico.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silcock et al. (2023)</span>
<span class="ltx_bibblock">
Emily Silcock, Luca D’Amico-Wong, Jinglin Yang, and Melissa Dell. 2023.

</span>
<span class="ltx_bibblock">Noise-robust de-duplication at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
Jiawei Tang, Yifei Zuo, Lei Cao, and Samuel Madden. 2022.

</span>
<span class="ltx_bibblock">Generic entity resolution models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">NeurIPS 2022 First Table Representation Workshop</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teikoku Koshinjo (1957)</span>
<span class="ltx_bibblock">
Teikoku Koshinjo. 1957.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Teikoku Ginko Kaisha Yoroku</em>.

</span>
<span class="ltx_bibblock">Teikoku Koshinjo.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Isola (2020)</span>
<span class="ltx_bibblock">
Tongzhou Wang and Phillip Isola. 2020.

</span>
<span class="ltx_bibblock">Understanding contrastive representation learning through alignment
and uniformity on the hypersphere.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, volume 119,
pages 9929–9939. PMLR.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wightman (2019)</span>
<span class="ltx_bibblock">
Ross Wightman. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.4414861" title="" class="ltx_ref ltx_href">Pytorch image
models</a>.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/rwightman/pytorch-image-models" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rwightman/pytorch-image-models</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2019)</span>
<span class="ltx_bibblock">
Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke
Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock">Scalable zero-shot entity linking with dense entity retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.03814</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Xinmei Yang, Abhishek Arora, Shao-Yu Jheng, and Melissa Dell. 2023.

</span>
<span class="ltx_bibblock">Quantifying character similarity with vision transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14672</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Huchen Zhou, Wenfeng Huang, Mohan Li, and Yulin Lai. 2022.

</span>
<span class="ltx_bibblock">Relation-aware entity matching using sentence-bert.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Computers, Materials &amp; Continua</em>, 71(1).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.00788" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.00789" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.00789">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.00789" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.00791" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 08:10:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
