<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2006.03594] From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks</title><meta property="og:description" content="Machine learning (ML) tasks are becoming ubiquitous in today’s network applications.
Federated learning has emerged recently as a technique for training ML models at the network edge by leveraging processing capabiliti…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2006.03594">

<!--Generated on Thu Mar 14 14:09:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seyyedali Hosseinalipour, Christopher G. Brinton, Vaneet Aggarwal, Huaiyu Dai, and Mung Chiang
</span><span class="ltx_author_notes">S. Hosseinalipour, C. G. Brinton, V. Aggarwal, and M. Chiang are with Purdue University, IN, USA e-mail: {hosseina,cgb,vaneet,chiang}@purdue.edu. H. Dai is with NC State University, NC, USA e-mail: hdai@ncsu.edu.
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Machine learning (ML) tasks are becoming ubiquitous in today’s network applications.
Federated learning has emerged recently as a technique for training ML models at the network edge by leveraging processing capabilities across the nodes that collect the data. There are several challenges with employing conventional federated learning in contemporary networks, due to the significant heterogeneity in compute and communication capabilities that exist across devices. To address this, we advocate a new learning paradigm called <span id="id1.id1.1" class="ltx_text ltx_font_italic">fog learning</span> which will intelligently distribute ML model training across the continuum of nodes from edge devices to cloud servers. Fog learning enhances federated learning along three major dimensions: <span id="id1.id1.2" class="ltx_text ltx_font_italic">network</span>, <span id="id1.id1.3" class="ltx_text ltx_font_italic">heterogeneity</span>, and <span id="id1.id1.4" class="ltx_text ltx_font_italic">proximity</span>. It considers a multi-layer hybrid learning framework consisting of heterogeneous devices with various proximities. It accounts for the topology structures of the local networks among the heterogeneous nodes at each network layer, orchestrating them for collaborative/cooperative learning through device-to-device (D2D) communications. This migrates from star network topologies used for parameter transfers in federated learning to more distributed topologies at scale. We discuss several open research directions to realizing fog learning.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The modern era has witnessed an explosion in the number of intelligent wireless devices capable of connecting to the Internet and forming ad-hoc networks. The improved processing capabilities of these Internet of Things (IoT) devices coupled with rising user demands for data-intensive, latency-sensitive tasks has motivated <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">fog computing</span>. Fog computing is an emerging architecture which aims to orchestrate and manage processing resources across nodes in the cloud-to-things continuum, encompassing the cloud, core, metro, edge, clients, and things <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Security and privacy of user data is also an important part of this emerging paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Machine learning (ML) has attracted significant recent attention in networking applications, given its potential to provide fast and autonomous decision-making for 5G, 6G, and future wireless technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
ML techniques generally require large datasets for model training, especially in the newer category of deep learning. This data is generated at end user devices as they interact with applications, and then traditionally is transferred to a central datacenter which carries out the model training. Consider, for example, automated facial recognition carried out by social media platforms today: when a user uploads a photo, a prediction is made of who is in the image by applying a model trained over billions of samples at a datacenter. The user’s feedback on this prediction (e.g., whether it is correct) informs further model refinement.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Centralized ML model training is prohibitive in many emerging network applications, however.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In particular, transferring large volumes of data samples from the end users to the cloud has the following drawbacks:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">For battery-limited devices such as smartphones, unmanned aerial vehicles (UAVs), and wireless sensors, uplink data offloading can consume prohibitive amounts of energy.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">For latency-sensitive applications, the round trip time of data transfer, model training/up-dating, and decision making can be prohibitively long.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">In privacy-sensitive applications, end users may not be willing to share their raw data.</p>
</div>
</li>
</ol>
<p id="S1.p4.2" class="ltx_p">These limitations have motivated work on distributed ML model training, where federated learning has received significant recent attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.4.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.5.2" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The standard operation of federated learning is depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I-A Federated Learning ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. To train an ML model (e.g., a neural network), two steps are repeated in sequence: (i) <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">local learning</span>, in which each worker device updates the parameters of the ML model (e.g., weights on neurons) using its collected dataset, and (ii) <span id="S1.SS1.p1.1.2" class="ltx_text ltx_font_italic">global aggregation</span>, in which a main server determines the new global model from the local updates and synchronizes the devices with this aggregated version. The local learning at each device typically consists of gradient descent iterations to update the model. The global aggregation is typically an averaging of the local parameters, which may be weighted depending on the perceived quality of devices’ updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
A key property of federated learning is that the data itself is never transferred between the devices and the server, which further reduces communication demands, and mitigates privacy concerns associated with data sharing.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2006.03594/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Left: Conventional “star topology” of federated learning. Right: An abstract model of data flow in federated learning.</figcaption>
</figure>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">The standard implementation of federated learning causes performance issues in contemporary fog networking environments, however. Next, we outline the key considerations for developing <span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_italic">network-aware</span> techniques for distributing ML tasks, and initial works that have attempted to address them.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.4.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.5.2" class="ltx_text ltx_font_italic">Design Considerations for Network-Aware ML</span>
</h3>

<section id="S1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS2.SSS1.4.1.1" class="ltx_text">I-B</span>1 </span>Communication heterogeneity</h4>

<div id="S1.SS2.SSS1.p1" class="ltx_para">
<p id="S1.SS2.SSS1.p1.1" class="ltx_p">Most of the IoT devices engaged in ML – cellular phones, smart vehicles, wireless sensors, UAVs, etc. – are mobile, with significant heterogeneity in their communication abilities. Channel qualities will change over time and as devices move through the network.
As the achievable uplink and downlink data rates of the system will vary for each node over time, they must be taken into consideration in the design of distributed ML techniques. These heterogeneous communication characteristics have motivated several recent studies on federated learning for wireless networks, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Additionally, they have motivated studies on communication-efficient federated learning, through the techniques of quantization (i.e., compressing model updates prior to transmission) and sparsification (i.e., transmitting only some elements of the parameter vectors) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
<section id="S1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS2.SSS2.4.1.1" class="ltx_text">I-B</span>2 </span>Computation/storage heterogeneity</h4>

<div id="S1.SS2.SSS2.p1" class="ltx_para">
<p id="S1.SS2.SSS2.p1.1" class="ltx_p">Wireless devices exhibit heterogeneity in their processing equipment and availability of their resources. Thus, the time required to perform a single local update will vary from one device to another.
This has motivated studying the effects of device compute delays and the existence of stragglers on the time required to train ML models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Methods that have been proposed to resolve these effects mostly rely on intelligent selection of device training participation. Techniques for mitigating compute limitations have also been studied more generally, e.g., through model compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
<section id="S1.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS2.SSS3.4.1.1" class="ltx_text">I-B</span>3 </span>Privacy and security</h4>

<div id="S1.SS2.SSS3.p1" class="ltx_para">
<p id="S1.SS2.SSS3.p1.1" class="ltx_p">Although federated learning eliminates the need to transmit raw data, it is possible for sensitive information to be leaked through reverse engineering of model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
This has motivated investigations into adapting well-known privacy and security-preservation techniques – such as differential privacy and functional encryption – to federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S1.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS2.SSS4.4.1.1" class="ltx_text">I-B</span>4 </span>Joint performance metrics</h4>

<div id="S1.SS2.SSS4.p1" class="ltx_para">
<p id="S1.SS2.SSS4.p1.1" class="ltx_p">The performance of an ML task is typically measured through the convergence speed and the accuracy of the resulting model. In network-aware ML, the previous three design considerations suggest additional performance metrics. But these objectives tend to compete with one another: for example, a wireless network device processing more gradient updates may improve resulting model quality, but requires more energy consumption.
Thus, techniques for network-aware ML must consider a joint optimization among the objectives of (i) minimizing network resource costs, (ii) maximizing resulting model quality, and (iii) maximizing privacy/security, with different importance assigned to each objective depending on the application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS3.4.1.1" class="ltx_text">I-C</span> </span><span id="S1.SS3.5.2" class="ltx_text ltx_font_italic">Dimensions of Innovation for Network-Aware ML</span>
</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">Compared with federated learning, fog learning is defined by the three dimensions of <span id="S1.SS3.p1.1.1" class="ltx_text ltx_font_italic">network</span>, <span id="S1.SS3.p1.1.2" class="ltx_text ltx_font_italic">heterogeneity</span>, and <span id="S1.SS3.p1.1.3" class="ltx_text ltx_font_italic">proximity</span>:</p>
<ol id="S1.I2" class="ltx_enumerate">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">It considers the networks and topology structures among the devices and incorporates a collaboration/cooperation among local wired/wireless nodes using device-to-device communications.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">It considers the heterogeneity of nodes through the cloud-to-things continuum, in terms of computation capability and local data distributions.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">It exploits the proximity of resource-limited nodes to resource-abundant nodes to optimize ML training.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2006.03594/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_img_landscape" width="452" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A schematic of model aggregation stages for a large-scale ML task in network-aware learning. The main server aggregates parameter updates from multiple cloud servers. Before reaching these cloud servers, local models trained by devices goes through multiple layers of aggregations. The devices can learn cooperatively via direct D2D communications, through which model parameters, datasets or both are exchanged.</figcaption>
</figure>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Motivating a New Architecture 
<br class="ltx_break">for Network-Aware Learning</span>
</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Conventional federated learning suffers from a series of limitations when implemented over fog networks. In this section, we will explain these limitations, motivating a new paradigm for distributed ML.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Federated Learning: Limitations in Fog Environments</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Consider training and managing a data-intensive, latency-sensitive ML task over a large-scale fog network. We face the following key limitations using federated learning as the solution:</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.4.1.1" class="ltx_text">II-A</span>1 </span>Multi-layer nature of large-scale learning</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Under federated learning, global aggregations would be performed at the main datacenter. When smartphones, smart vehicles, or other connected edge devices perform their local updates, their cellular base stations (BSs), road side units (RSUs), or analogous access points cannot directly transfer these learned parameters to the main server, which will be in a datacenter located possibly thousands of miles away. Instead, one pragmatic approach would be to consider multiple aggregations at different scales, e.g., edge servers in localities, cities, states, etc., before finally reaching the datacenter. Similarly, for a team of data-gathering UAVs in an area with no cellular coverage, the local learning parameters may first be aggregated by a team of miniature UAVs, then multiple heavier UAVs, and then a high altitude platform (HAP). The HAP would transmit the aggregated models to an edge server through a backhaul network. Once at the edge server, these parameters could traverse the aforementioned hierarchy to reach the main server.
This potential multi-layer network structure for model aggregation is depicted in Fig. <a href="#S1.F2" title="Figure 2 ‣ I-C Dimensions of Innovation for Network-Aware ML ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.4.1.1" class="ltx_text">II-A</span>2 </span>Overloading heterogeneous network resources</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">Current cellular BSs and RSUs are not designed to handle model uploads from large numbers of active devices simultaneously. Training deep neural networks (DNNs) with federated learning can require participation from many active devices, as high complexity models require large datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Moreover, given the heterogeneity of IoT devices, each participating device may only be capable of processing a small set of samples for a high dimensional model. This calls for a learning architecture that optimizes the choice of devices participating in model uploading based on current network conditions.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS3.4.1.1" class="ltx_text">II-A</span>3 </span>Device collaboration/cooperation</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">Federated learning ignores the topology structures among the devices and the possibility of collaboration/cooperation among the devices without engaging the main server. Enabling direct communication between devices in local neighborhoods of each network layer could lead to significant power and bandwidth savings by reducing uplink transmissions to nodes in the higher layers. This calls for a framework that explicitly considers device-to-device (D2D) communications being enabled in 5G-and-beyond wireless. We will refer to all communication between devices/nodes within a single network layer as D2D, examples of which are depicted in Fig. <a href="#S1.F2" title="Figure 2 ‣ I-C Dimensions of Innovation for Network-Aware ML ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS4.4.1.1" class="ltx_text">II-A</span>4 </span>Strict privacy assumptions</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">Federated learning guarantees that each device’s local dataset is never transferred over the network. While this is important in privacy-sensitive applications, in many cases users may be willing to share portions of their datasets for ML training, which can be useful when there is a combination of resource-hungry and resource-rich devices. For example, a smart car attempting to train an object classifier with a limited on-board processor is likely willing to offload its sensor data to a more computation powerful car to expedite the training process if the channel conditions are reasonable. This calls for a learning framework which can adapt based on privacy needs.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">From Federated to Fog Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Given these limitations, we propose a new learning paradigm called <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">fog learning</span>. As opposed to federated learning which is based on a star topology of device-server interactions, fog learning will explicitly consider the network and topology structures among the devices and enable intelligent device collaborations/cooperations through data and parameter offloading. This hybrid learning paradigm will exploit the multi-layer structure of fog networks to optimize performance in the presence of heterogeneous network resources.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">There are some recent works on hierarchical federated learning, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. These works are mainly focused on specific two-tiered network structures above wireless cellular devices, e.g., edge clouds connected to a main server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> or small cell and macro cell base stations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Fog learning generalizes this concept to a multi-layer structure that encompasses all IoT elements between the end devices and the main server. Moreover, fog learning introduces collaborative/cooperative model training via D2D communications among the devices at different layers of the network hierarchy.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Fog Learning: a Multi-layer 
<br class="ltx_break">Hybrid Learning Paradigm</span>
</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we define fog learning in terms of its multi-layer structure and hybrid learning characteristics.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Multi-layer Network Architecture</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Fog learning is a multi-layer learning architecture over a fog network. Similar to conventional federated learning, the main server will conduct global aggregations. However, the end users are not directly connected to the main server: instead, the local models learned by end devices may traverse multiple layers of aggregations before reaching the main server. Local aggregations at each layer provide dimensionality reduction, reducing the size of the data being transmitted upstream. Synchronizations at each layer also provide agile responses to any changes in local data distributions.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2006.03594/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="216" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Dimensionality reduction from multi-layer aggregations. The length of the original learning parameter vectors at each end device is <math id="S3.F3.3.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.F3.3.m1.1b"><mi id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><ci id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">G</annotation></semantics></math>. The size of data transmitted upstream from each middle node is also <math id="S3.F3.4.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.F3.4.m2.1b"><mi id="S3.F3.4.m2.1.1" xref="S3.F3.4.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.F3.4.m2.1c"><ci id="S3.F3.4.m2.1.1.cmml" xref="S3.F3.4.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.m2.1d">G</annotation></semantics></math>, reduced by a factor of the number of node inputs.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To see the motivation for dimensionality reduction, consider that any ML model is represented as a vector of its model parameters. For a DNN, this vector can have millions of entries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, where each element requires a certain number of bits for storage and transfer. Depending on the quantization method, then, this parameter vector could require anywhere from a few megabytes to gigabytes. For the hierarchical network structure depicted in Fig. <a href="#S1.F2" title="Figure 2 ‣ I-C Dimensions of Innovation for Network-Aware ML ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, consecutive transmissions of these vectors from millions of edge devices to the main server would lead to large delays, overloaded network infrastructure, and high communication costs.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Each group of devices in Fig. <a href="#S1.F2" title="Figure 2 ‣ I-C Dimensions of Innovation for Network-Aware ML ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> forms a learning cluster which conducts local aggregations of its internal parameters. After each local aggregation, the size of the resulting vector to be transmitted upstream is the same as any one of the input vectors, as illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Multi-layer Network Architecture ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For instance, each UAV in Fig. <a href="#S1.F2" title="Figure 2 ‣ I-C Dimensions of Innovation for Network-Aware ML ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> can aggregate its associated devices’ parameters and send the resulting vector to the upper layer.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Hybrid Learning: Vertical and Horizontal Communications</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The learning architecture in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Multi-layer Network Architecture ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> follows a vertical communication structure, where model parameters are passed only upstream and downstream between the network layers. Fog learning takes this one step further to allow for horizontal communications between devices in the same layer.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Peer-to-peer (P2P) networking has been an area of research, offering on-demand establishment of connectivity and eliminating the requirement of a central module to facilitate communication between peers. 5G-and-beyond wireless technologies are enabling D2D communications between wireless nodes, which is motivating P2P intelligence in fog computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. There is a well-developed body of literature on D2D communication protocols for MANETs, VANETs, FANETs, and wireless sensor networks.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2006.03594/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="456" height="137" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Network representation of fog learning. The root of the tree is the main server, the leaves are the end devices, and the nodes in-between are different intermediate devices. The nodes belonging to the same layer and the same horizontal rectangle form clusters. The patterned rectangles correspond to those clusters that choose to engage in D2D and distributedly learn their model aggregation. The parent nodes of such clusters can then sample one (or a tiny fraction) of their children nodes to obtain the aggregated model. Each yellow block represents a <span id="S3.F4.2.1" class="ltx_text ltx_font_italic">learning block</span>, where the top nodes have a certain clock for transmitting model parameters upstream for global aggregations.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Considering again the structure in Fig. <a href="#S1.F2" title="Figure 2 ‣ I-C Dimensions of Innovation for Network-Aware ML ‣ I Introduction ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, fog learning would intelligently cluster the devices in the bottom-most layer such that each cluster has the potential to form a wireless ad-hoc network for parameter sharing or data offloading. Similarly, the upper layers will be clustered such that the computing nodes in each layer are capable of communicating for parameter sharing, in some cases via low-latency wired connections (e.g., multiple local edge servers connected via fiber in a metropolitan area) and in other cases over the air (e.g., UAVs).</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Hybrid Learning: Vertical and Horizontal Communications ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we represent the fog learning network architecture as a logical tree graph, the leaves of which are the edge devices and the root of which is the main server. Fog learning is a hybrid learning methodology which leverages horizontal communications among nodes in addition to vertical parameter transfers between the layers. In the following, we first discuss a general approach for D2D communications at different network layers, and then discuss two data offloading strategies that can be utilized in the bottom layers of the network.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Distributed aggregations through horizontal communications</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The nodes inside a D2D-enabled cluster are capable of computing the local aggregation of their locally-trained parameters in a distributed manner, through message passing and consensus formation. This approach eliminates the need for the parent node to compute the aggregation, and can be implemented at all the network layers, which has energy efficiency advantages (discussed further in Sec. <a href="#S3.SS3" title="III-C Performance Advantages of Fog Learning ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>). At the bottom-most layer, the datasets of the devices remain local, as in federated learning. In leveraging such horizontal communications, the conventional star topology used in federated learning is transformed to a collaborative/cooperative distributed fog learning topology.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Our recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> studied a realization of horizontal aggregation based on a distributed average consensus formation scheme.
We showed that even with limited amounts of D2D communication enabled, the learning accuracy approaches centralized gradient descent. We demonstrated that using this technique can result in around 50% device energy savings and 80% reduction in the number of parameters transferred over the network compared with conventional federated learning.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>D2D offloading under milder privacy concerns</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">In addition to sharing learning parameters, the proposed D2D communication scheme can also be used for partial dataset offloading among trusted edge devices, for applications with milder privacy concerns. In Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Hybrid Learning: Vertical and Horizontal Communications ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, this is only applicable at the bottom-most layer of the tree where the data is collected. This approach is useful in the presence of heterogeneous computation resources within a cluster (discussed further in Sec. <a href="#S3.SS3" title="III-C Performance Advantages of Fog Learning ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>).</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">Our recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> studied the improvement in network resource costs that intelligent D2D data offloading can provide to distributed learning, finding in particular that up to 50% decrease in the total device processing and transmit resource utilization are possible compared with conventional federated learning. Our results reveal that these gains are consistent over a range of D2D topologies defined by communication restrictions (such as privacy) between nodes.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">and found up to 50% decrease in the total device processing and transmit resource utilization are possible compared with conventional federated learning.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Inter-layer data offloading and caching</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Mobile devices at the bottom-most network layer may move between local topologies rapidly, which presents an opportunity to improve local data distributions. Specifically, if devices offload portions of non-privacy-sensitive data to the next layer up, this data can be cached and broadcasted among a larger number of edge devices. This will increase the similarity of local data to the global distribution and reduce model bias from local updates.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Performance Advantages of Fog Learning</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The local aggregation and D2D offloading features of fog learning will be particularly important for contemporary data-intensive, latency-sensitive applications. These include training ML models for autonomous vehicle navigation, smart factory automation, and augmented/virtual reality (AR/VR) navigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Specifically, the advantages provided are as follows:</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.4.1.1" class="ltx_text">III-C</span>1 </span>Reducing network traffic</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Fog learning employs local aggregations of ML model parameters at different layers of the topology, providing an upstream dimensionality reduction. This results in a significantly reduced network traffic between different network layers.
Reducing data transfer requirements over long distances decreases latency and communication costs. This is particularly important when training high complexity models like DNNs; in these cases, fog learning can leverage asynchronous layer-wise training and parameter update techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for further reductions in upstream traffic.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.4.1.1" class="ltx_text">III-C</span>2 </span>Network power savings</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Horizontal D2D communications allow node clusters to distributedly discover their aggregated models. Thus, the parent node of the cluster can choose one device to upload the aggregated value. Decreasing the number of uplink transmissions by an order of magnitude will reduce energy consumption significantly. For instance, in a cellular network, continuous communication with the BS drains a smartphone’s battery rapidly. With D2D enabled, rather than uploading to the BS at each aggregation, the devices could engage in short-range, low power communications, and only one device will need to transmit the result. Instead of selecting one device, it would also be possible to employ a diversity technique where each device in a cluster engages in short, simultaneous uplink transmissions of only a fraction of the parameters.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS3.4.1.1" class="ltx_text">III-C</span>3 </span>Efficient spectrum usage</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">Devices in a cluster engaged in D2D communications can operate in out-band mode, which does not require utilizing the licensed spectrum of e.g., a cellular BS or a vehicular RSU. Furthermore, when using in-band D2D, the devices can use opportunistic spectrum access methods to exploit the unused licensed spectrum.</p>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS4.4.1.1" class="ltx_text">III-C</span>4 </span>Adaptation to device mobility</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.1" class="ltx_p">Devices may enter/exit a local cluster rapidly. When a device enters a D2D enabled cluster, it can join the learning process quickly through acquisition of the current model parameters from a neighboring node. Also, when a device exits, it can transfer its model/data to a local peer so its locally updated model and data is not negated. This capability, along with the fact that devices in different clusters can perform learning in parallel, can be described as <span id="S3.SS3.SSS4.p1.1.1" class="ltx_text ltx_font_italic">parallel successive learning</span>: nodes can inherit partially-trained models and continue refining the parameters with newly collected data.</p>
</div>
</section>
<section id="S3.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS5.4.1.1" class="ltx_text">III-C</span>5 </span>Leveraging passive and straggler device datasets</h4>

<div id="S3.SS3.SSS5.p1" class="ltx_para">
<p id="S3.SS3.SSS5.p1.1" class="ltx_p">Certain devices may possess valuable data but may have lower computational capabilities or not be engaged in the training process. With D2D-enabled offloading and active inter-layer data caching, these passive datasets can be transferred to resource-abundant active devices.</p>
</div>
</section>
<section id="S3.SS3.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS6.4.1.1" class="ltx_text">III-C</span>6 </span>Faster convergence in fewer global aggregations</h4>

<div id="S3.SS3.SSS6.p1" class="ltx_para">
<p id="S3.SS3.SSS6.p1.1" class="ltx_p">By mitigating the effect of stragglers and enabling more distributed processing on heterogeneous datasets, the global model in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Hybrid Learning: Vertical and Horizontal Communications ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> can be trained faster and with fewer costly global aggregations.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Key Innovations in Fog Learning</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The key innovations of fog learning are as follows:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Establishing multi-stage hierarchical machine learning through space.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Migrating from star to distributed learning topologies via collaboration/cooperation among D2D-enabled devices.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Employing agile network-aware management of heterogeneous nodes and channels.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Distributing task processing based on multi-objective network optimization of latency, cost, and privacy metrics.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Parallel successive learning for rapid refinement of locally trained models.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Open Research Directions</span>
</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In the following, we outline several directions of future research for fog learning:</p>
</div>
<section id="S4.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS1.4.1.1" class="ltx_text">IV-</span>1 </span>Optimizing horizontal/vertical communications</h4>

<div id="S4.SS0.SSS1.p1" class="ltx_para">
<p id="S4.SS0.SSS1.p1.1" class="ltx_p">Performing aggregations via D2D communications may be more resource-efficient, but can also incur more delay compared with the case of vertical aggregations. This delay is a function of data rates among the devices, channel qualities, rounds of D2D communication required, and other factors.
Given the benefits of D2D communications discussed in Sec. <a href="#S3.SS3" title="III-C Performance Advantages of Fog Learning ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>, quantifying the trade-offs and deciding which clusters of devices are suitable to perform the D2D communications deserves further investigation. Also, the potential for model inversion attacks at different network layers caused by horizontal parameter sharing needs to be considered, through effective countermeasures such as functional encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS2.4.1.1" class="ltx_text">IV-</span>2 </span>Multi-layer control and resource allocation</h4>

<div id="S4.SS0.SSS2.p1" class="ltx_para">
<p id="S4.SS0.SSS2.p1.1" class="ltx_p">Fog learning calls for a series of studies on designing control algorithms for orchestrating the nodes at different layers of the network. Along this direction, straggler mitigation in a multi-layer structure must be considered, along with asynchronous management of model aggregations. Additionally, efficient resource allocation along the cloud-to-things continuum must be considered, including congestion-aware distributed flow (load) balancing designs for distributed ML task handling. This may include a dynamic main server selection scheme based on network path resource availability.</p>
</div>
</section>
<section id="S4.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS3.4.1.1" class="ltx_text">IV-</span>3 </span>Error propagation analysis</h4>

<div id="S4.SS0.SSS3.p1" class="ltx_para">
<p id="S4.SS0.SSS3.p1.1" class="ltx_p">Due to communication imperfections and time-varying network topologies, horizontal parameter aggregations of devices in clusters may be noisy versions of the true aggregated values. Such noise will then be propagated and potentially amplified in transmission to upper layers. Modeling these errors, their propagation, and their cumulative effect on training convergence speed and accuracy is an interesting future direction.</p>
</div>
</section>
<section id="S4.SS0.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS4.4.1.1" class="ltx_text">IV-</span>4 </span>Intelligent cluster sampling</h4>

<div id="S4.SS0.SSS4.p1" class="ltx_para">
<p id="S4.SS0.SSS4.p1.1" class="ltx_p">To reduce power consumption and network traffic, the main server in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Hybrid Learning: Vertical and Horizontal Communications ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> can perform cluster sampling, in which only the end devices from certain clusters engage in model training in each round. This requires considering end devices’ data qualities and the characteristics of nodes in different network layers. Also, if nodes in the upper layers have mobile capabilities, this motivates network reconfiguration between global aggregations. For instance, instead of deploying a dedicated set of UAVs for data collection from each cluster of devices, a limited set of UAVs can be utilized, and the optimal trajectory can be obtained to enable the desired cluster sampling.</p>
</div>
</section>
<section id="S4.SS0.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS5.4.1.1" class="ltx_text">IV-</span>5 </span>Block-based learning</h4>

<div id="S4.SS0.SSS5.p1" class="ltx_para">
<p id="S4.SS0.SSS5.p1.1" class="ltx_p">The devices located in different layers of the network can form different <span id="S4.SS0.SSS5.p1.1.1" class="ltx_text ltx_font_italic">learning blocks</span> (see Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B Hybrid Learning: Vertical and Horizontal Communications ‣ III Fog Learning: a Multi-layer Hybrid Learning Paradigm ‣ From Federated to Fog Learning: Distributed Machine Learning over Heterogeneous Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) to further decrease the network traffic and the required number of global aggregations.
In each block, the head (top-most) node(s) have a certain frequency of vertical communication. In-between vertical updates, they can conduct multiple rounds of in-block learning local updates. Studying the trade-offs between the aggregation frequencies of different learning blocks, the computational capabilities of the nodes inside the blocks, model accuracy, and training convergence speed is an open direction.</p>
</div>
</section>
<section id="S4.SS0.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS6.4.1.1" class="ltx_text">IV-</span>6 </span>Modeling of heterogeneous fog networks</h4>

<div id="S4.SS0.SSS6.p1" class="ltx_para">
<p id="S4.SS0.SSS6.p1.1" class="ltx_p">A comprehensive model of the interplay between fog network parameters (e.g., trust levels between users, D2D channel qualities, vertical communication quality variations, heterogeneous data quality, and heterogeneous compute capabilities) can lead to further optimization of fog learning.
Codifying each of these parameters and designing corresponding collaborative/cooperative learning schemes is an open direction.</p>
</div>
</section>
<section id="S4.SS0.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS7.4.1.1" class="ltx_text">IV-</span>7 </span>Smart data sharing</h4>

<div id="S4.SS0.SSS7.p1" class="ltx_para">
<p id="S4.SS0.SSS7.p1.1" class="ltx_p">End users can offload different parts of their datasets to different peers. In acting as helper nodes, devices with higher compute powers can send out requests for specific samples in a dataset that they lack
to maximize the resulting data processing benefit. A similar procedure can be carried out using active inter-layer data caching. This will increase the quality of devices’ datasets and improve the resulting global models.</p>
</div>
</section>
<section id="S4.SS0.SSS8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS8.4.1.1" class="ltx_text">IV-</span>8 </span>Incentivizing end users</h4>

<div id="S4.SS0.SSS8.p1" class="ltx_para">
<p id="S4.SS0.SSS8.p1.1" class="ltx_p">Proper incentive mechanisms are needed to persuade devices to participate in collaborative/cooperative model training. The incentives should consider the parameters of the local datasets (e.g., data quality) and the device’s network-related parameters (e.g., speed of data offloading and computational capabilities).</p>
</div>
</section>
<section id="S4.SS0.SSS9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS9.4.1.1" class="ltx_text">IV-</span>9 </span>Personalized model training</h4>

<div id="S4.SS0.SSS9.p1" class="ltx_para">
<p id="S4.SS0.SSS9.p1.1" class="ltx_p">Training a single global model for an application can lead to poor performance at individual devices in scenarios of extreme data heterogeneity among geographically-distributed nodes. To address this, personalized model training can be investigated for fog learning through frameworks such as multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS10" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS10.4.1.1" class="ltx_text">IV-</span>10 </span>Dynamic networks and mobility models</h4>

<div id="S4.SS0.SSS10.p1" class="ltx_para">
<p id="S4.SS0.SSS10.p1.1" class="ltx_p">D2D data offloading and parameter sharing is only practical when mobile devices are within a certain vicinity. Accurate mobility models of devices could reveal pertinent information regarding the anticipated duration/frequency of contact, the data distributions of the contacting devices, and so forth. This information could be used to develop mobility-aware collaborative model training.</p>
</div>
</section>
<section id="S4.SS0.SSS11" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS11.4.1.1" class="ltx_text">IV-</span>11 </span>Integration with wireless technologies</h4>

<div id="S4.SS0.SSS11.p1" class="ltx_para">
<p id="S4.SS0.SSS11.p1.1" class="ltx_p">Massive MIMO and reconfigurable intelligent surfaces are two radio technologies that will be major drivers of 5G-and-beyond wireless <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. These physical/link-layer technologies can be developed jointly with fog learning to conduct model training over large numbers of users with high data rates and low latency.</p>
</div>
</section>
<section id="S4.SS0.SSS12" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS0.SSS12.4.1.1" class="ltx_text">IV-</span>12 </span>Deep reinforcement learning (DRL) for/via fog learning</h4>

<div id="S4.SS0.SSS12.p1" class="ltx_para">
<p id="S4.SS0.SSS12.p1.1" class="ltx_p">DRL is a useful ML technique when perfect knowledge about the learning environment is not attainable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. This method has the potential to address design problems for wireless learning such as device beamforming, power control, interference management, coordination, and transmission scheduling, all of which can be adapted at different network layers. Decentralized training of DRL in turn requires message passing among the devices, which can be enabled at scale through fog learning via device collaboration, synchronization, and orchestration at different layers.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We introduced <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">fog learning</span>, a new paradigm for distributing ML model training through large-scale networks of heterogeneous devices. We demonstrated that fog learning is inherently a multi-layer collaborative/cooperative hierarchical learning framework that can significantly reduce network resource costs and model training times through local model aggregations at different network layers. We introduced the hybrid property of fog learning, which combines horizontal D2D communications between nodes with vertical communications up the hierarchy. Further, we discussed the distributed topology and multi-objective optimization nature of fog learning that make it network-aware. Finally, we identified several open research directions in this emerging area.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Chiang, S. Ha, F. Risso, T. Zhang, and I. Chih-Lin, “Clarifying
fog computing and networking: 10 questions and answers,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Commun.
Mag.</em>, vol. 55, no. 4, pp. 18–20, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Wu, M. Dong, K. Ota, J. Li, and Z. Guan, “FCSS:
Fog-computing-based content-aware filtering for security services in
information-centric social networks,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Emerg. Topics
Comput.</em>, vol. 7, no. 4, pp. 553–564, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Ali, W. Saad, N. Rajatheva, K. Chang, D. Steinbach, B. Sliwa, C. Wietfeld,
K. Mei, H. Shiri, H.-J. Zepernick <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “6G white paper on
machine learning in wireless communication networks,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2004.13875</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless network
intelligence at the edge,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE</em>, vol. 107, no. 11, pp.
2204–2239, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen, and C. S. Hong,
“Federated learning over wireless networks: Optimization model design and
analysis,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. INFOCOM</em>, 2019, pp. 1387–1395.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W. Wang, Y. Sun, B. Eriksson, W. Wang, and V. Aggarwal, “Wide compression:
Tensor ring nets,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE CVPR</em>, 2018, pp. 9329–9338.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Tu, Y. Ruan, S. Wang, S. Wagle, C. G. Brinton, and C. Joe-Wang,
“Network-aware optimization of distributed learning for fog computing,” in
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. INFOCOM</em>, 2020, pp. 2509–2518.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Xu, N. Baracaldo, Y. Zhou, A. Anwar, and H. Ludwig, “Hybridalpha: An
efficient approach for privacy-preserving federated learning,” in
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Workshop Artif. Intell. Security</em>, 2019, pp. 13–23.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and
K. Chan, “Adaptive federated learning in resource constrained edge
computing systems,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE JSAC</em>, vol. 37, no. 6, pp. 1205–1221, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Luo, X. Chen, Q. Wu, Z. Zhou, and S. Yu, “HFEL: Joint edge
association and resource allocation for cost-efficient hierarchical federated
edge learning,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, pp. 1–1, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. S. H. Abad, E. Ozfatura, D. GUndUz, and O. Ercetin, “Hierarchical
federated learning across heterogeneous cellular networks,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc.
IEEE ICASSP</em>, 2020, pp. 8866–8870.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Hosseinalipour, S. S. Azam, C. G. Brinton, N. Michelusi, V. Aggarwal, D. J.
Love, and H. Dai, “Multi-stage hybrid federated learning over large-scale
wireless fog networks,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.09511</em>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Chen, X. Sun, and Y. Jin, “Communication-efficient federated deep learning
with layerwise asynchronous model update and temporally weighted
aggregation,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Neural Netw. Learn. Syst.</em>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Huang, R. Mo, and C. Yuen, “Reconfigurable intelligent surface
assisted multiuser MISO systems exploiting deep reinforcement learning,”
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE JSAC</em>, vol. 38, no. 8, pp. 1839–1850, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and D. I.
Kim, “Applications of deep reinforcement learning in communications and
networking: A survey,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Surveys Tuts.</em>, vol. 21, no. 4,
pp. 3133–3174, 2019.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Seyyedali Hosseinalipour (M’20)</span>  received B.S. degree from Amirkabir University of Technology in 2015 and Ph.D. degree from NC State University in 2020, both in electrical engineering. He received ECE doctoral scholar of the year award at NC State. He is currently a postdoctoral researcher at Purdue University. His research interests mainly include analysis of modern wireless networks and communication systems.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Christopher G. Brinton (SM’20)</span> 
is an Assistant Professor of ECE at Purdue University. His research interest is at the intersection of network optimization and data science. Since joining Purdue in 2019, he has won a seed for success award and an outstanding faculty mentoring award. He received his Masters and PhD in Electrical Engineering from Princeton University in 2013 and 2016, respectively, where he won the Bede Liu Best Dissertation Award. He is a co-founder of Zoomi Inc., and a co-author of the book <span id="tab2.1.1.1.1.1.2" class="ltx_text ltx_font_italic">The Power of Networks: 6 Principles that Connect our Lives</span>.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Vaneet Aggarwal (SM’15)</span>  received the B.Tech. degree in 2005 from the Indian Institute of Technology Kanpur, and the M.A. and Ph.D. degrees in 2007 and 2010, respectively from Princeton University, all in Electrical Engineering. He is currently an Associate Professor at Purdue University. He received Princeton University’s Porter Ogden Jacobus Honorific Fellowship in 2009, the 2017 IEEE Jack Neubauer Memorial Award, and the 2018 Infocom Workshop Best-Paper Award. His current research interests are in communications and networking, cloud computing, and machine learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td">
<span id="tab4.1.1.1.1" class="ltx_inline-block">
<span id="tab4.1.1.1.1.1" class="ltx_p"><span id="tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Huaiyu Dai (F’17)</span> 
received the B.E. and M.S. degrees in electrical engineering from Tsinghua
University, Beijing, China, in 1996 and 1998, respectively, and the Ph.D. degree in electrical
engineering from Princeton University, Princeton, NJ in 2002. He is currently a Professor of Electrical
and Computer Engineering with NC State University, Raleigh, holding the title of University Faculty
Scholar. His research interests are in the general areas of communication systems and networks,
advanced signal processing for digital communications, communication theory, and information
theory.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab5" class="ltx_float biography">
<table id="tab5.1" class="ltx_tabular">
<tr id="tab5.1.1" class="ltx_tr">
<td id="tab5.1.1.1" class="ltx_td">
<span id="tab5.1.1.1.1" class="ltx_inline-block">
<span id="tab5.1.1.1.1.1" class="ltx_p"><span id="tab5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Mung Chiang (F’12)</span>  is the John A. Edwardson Dean of the College of Engineering at Purdue University. He received his B.S. (Honors), M.S. and Ph.D. from Stanford University in 1999, 2000, and 2003 respectively. Prior to coming to Purdue, he was the Arthur LeGrand Doty Professor of Electrical Engineering at Princeton University. His research on networking received the 2013 Alan T. Waterman Award, the highest honor to US young scientists and engineers. His textbook <span id="tab5.1.1.1.1.1.2" class="ltx_text ltx_font_italic">Networked Life</span> and online course reached 250,000 students since 2012, and the popular science book <span id="tab5.1.1.1.1.1.3" class="ltx_text ltx_font_italic">The Power of Networks</span> was published in 2016. He founded the Princeton EDGE Lab in 2009, which bridges the theory-practice gap in edge networking research by spanning from proofs to prototypes. He co-founded startups in mobile, IoT and big data areas, and co-founded the global nonprofit Open Fog Consortium.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2006.03593" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2006.03594" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2006.03594">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2006.03594" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2006.03597" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 14:09:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
