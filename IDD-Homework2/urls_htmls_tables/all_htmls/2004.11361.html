<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.11361] Enhancing Privacy via Hierarchical Federated Learning</title><meta property="og:description" content="Federated learning suffers from several privacy-related issues that expose the participants to various threats.
A number of these issues are aggravated by the centralized architecture of federated learning.
In this pap…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhancing Privacy via Hierarchical Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhancing Privacy via Hierarchical Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.11361">

<!--Generated on Wed Mar  6 16:18:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  hierarchical architecture,  privacy enhancement
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<section id="id6" class="ltx_glossary ltx_acronym ltx_list_acronym">
<dl id="id6.6" class="ltx_glossarylist">
<dt id="id1.1.id1" class="ltx_glossaryentry">FL</dt>
<dd>Federated Learning</dd>
<dt id="id2.2.id2" class="ltx_glossaryentry">CL</dt>
<dd>Collaborative Learning</dd>
<dt id="id3.3.id3" class="ltx_glossaryentry">HFL</dt>
<dd>Hierarchical Federated Learning</dd>
<dt id="id4.4.id4" class="ltx_glossaryentry">DP</dt>
<dd>Differential Privacy</dd>
<dt id="id5.5.id5" class="ltx_glossaryentry">DOSN</dt>
<dd>Decentralized Online Social Network</dd>
<dt id="id6.6.id6" class="ltx_glossaryentry">OSN</dt>
<dd>Online Social Network</dd>
</dl>
</section>
<h1 class="ltx_title ltx_title_document">Enhancing Privacy via Hierarchical Federated Learning
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aidmar Wainakh
 
<br class="ltx_break">Tim Grube
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_font_italic">Telecooperation Lab</span>
<br class="ltx_break"><span id="id8.2.id2" class="ltx_text ltx_font_italic">Technical University of Darmstadt
<br class="ltx_break"></span>wainakh@tk.tu-darmstadt.de
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id9.3.id1" class="ltx_text ltx_font_italic">Telecooperation Lab</span>
<br class="ltx_break"><span id="id10.4.id2" class="ltx_text ltx_font_italic">Technical University of Darmstadt
<br class="ltx_break"></span>grube@tk.tu-darmstadt.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alejandro Sanchez Guinea
 
<br class="ltx_break">Max Mühlhäuser
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_font_italic">Telecooperation Lab</span>
<br class="ltx_break"><span id="id12.2.id2" class="ltx_text ltx_font_italic">Technical University of Darmstadt
<br class="ltx_break"></span>sanchez@tk.tu-darmstadt.de
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id13.3.id1" class="ltx_text ltx_font_italic">Telecooperation Lab</span>
<br class="ltx_break"><span id="id14.4.id2" class="ltx_text ltx_font_italic">Technical University of Darmstadt
<br class="ltx_break"></span>max@tk.tu-darmstadt.de
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">Federated learning suffers from several privacy-related issues that expose the participants to various threats.
A number of these issues are aggravated by the centralized architecture of federated learning.
In this paper, we discuss applying federated learning on a hierarchical architecture as a potential solution. We introduce the opportunities for more flexible decentralized control over the training process and its impact on the participants’ privacy.
Furthermore, we investigate possibilities to enhance the efficiency and effectiveness of defense and verification methods. </p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, hierarchical architecture, privacy enhancement

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_publicationid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationid: </span>pubid: <span id="id1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">©2020 IEEE. Personal use of this material is permitted.</span> <span id="id1.2" class="ltx_text ltx_inline-block" style="width:433.6pt;"> </span></span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, data breaches have dramatically increased <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
At the same time, more services are making use of large amounts of personal data as part of machine learning implementations to provide value to users.
These two factors have contributed to making users of services increasingly concerned about their privacy.
The concept of  <a href="#id1.1.id1"><span href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Learning</span></span></a> (<a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>) has been proposed to alleviate this issue by allowing multiple users to build a joint model without sharing their data, under the coordination of a central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
However, <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>’s distributed training process, together with its strong dependence on a central server, have increased the attack surface against users yet again <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Fully decentralized learning based on peer-to-peer topology has been proposed to cope with the issues associated with having a central server, such as a performance bottleneck and a single point of failure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
However, the convergence time and robustness against user churn remain open challenges.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A hierarchical architecture lies in between the two extremes, centralized and fully decentralized architectures, as a solution able to cope with complexity, scalability, and failure-related issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Applying <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> based on a hierarchical architecture is referred to as  <a href="#id3.3.id3"><span href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Hierarchical Federated Learning</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>).
<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> naturally matches emerging decentralized infrastructures (e.g., edge and fog computing) and heterogeneous nature of real-world systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Most importantly, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> provides a unique capacity for improving privacy.
As such, it reduces the centralization of power and control in the hands of the central server.
In addition, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> allows flexible placement of defense and verification methods within the hierarchy and enables these methods to be applied more efficiently and effectively.
Furthermore, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> creates the possibility to employ the trust between users to mitigate a number of threats.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we explore the potential benefits of using <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> to address privacy-related issues currently discussed in the <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>-community. The rest of this paper is organized as follows.
In Section <a href="#S2" title="2 Federated Learning and Open Issues ‣ Enhancing Privacy via Hierarchical Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> and three of its privacy-related issues.
Then, we introduce, in Section <a href="#S3" title="3 Hierarchical Federated Learning ‣ Enhancing Privacy via Hierarchical Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the concept of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> and potential applications.
In Section <a href="#S4" title="4 Privacy Implications of ‣ Enhancing Privacy via Hierarchical Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we discuss the privacy implications of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>.
Finally, we wrap up with a conclusion section.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Federated Learning and Open Issues</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><a href="#id1.1.id1"><span href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Learning</span></span></a> (<a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>) is a machine learning setting where multiple entities (users) train a joint model.
Each user trains the model locally on their data and shares only the model updates with a central server.
The central server aggregates the updates from the users into a new global model.
This setting mitigates a number of privacy risks that are typically associated with conventional machine learning, where all training data should be collected, then used to train a model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
In spite of the various benefits of <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, several privacy-related issues are still open.
A number of these issues are mainly derived from the centralized nature of the <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> architecture.
Next, we describe three of these issues, namely centralized control, limited verifiability, and constrained defenses.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">Centralized control</h3>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">The server in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> plays the role of a central coordinator that performs the following core functions:

<span id="S2.I1" class="ltx_inline-enumerate">
<span id="S2.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S2.I1.i1.1" class="ltx_text">sampling users, i.e., selecting which users participate in the training process,
</span></span>
<span id="S2.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S2.I1.i2.1" class="ltx_text">broadcasting the model and training algorithm,
</span></span>
<span id="S2.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(3)</span> <span id="S2.I1.i3.1" class="ltx_text">aggregating the model updates, and
</span></span>
<span id="S2.I1.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(4)</span> <span id="S2.I1.i4.1" class="ltx_text">broadcasting the updated global model.
</span></span>
</span>
Although such a setting limits the computational cost for the users, it places all the control on a single party, the server.
Thus, the server potentially represents a performance bottleneck and a single point of failure.
Furthermore, from a privacy perspective, concentrating such functions on the server leaves the users with limited or no control over the process. Consequently, the users have to rely on trusting the server to perform all the functions correctly while maintaining best privacy practices to protect their model updates.
In this respect, if the server were to be malicious or an adversary were to compromise the server, various attacks could be perpetrated against the users, e.g, reconstruction attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">Limited verifiability</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">In <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the server and users perform several computations locally and share the results with each other.
The users share their updates, while the server shares the aggregated model.
In order to allow the server and users to prove to each other that they perform the expected computations correctly and share a legitimate output, the computations need to be verified.
Multiple approaches based on zero-knowledge proofs have been proposed to apply this verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
However, these approaches suffer from two main limitations.
First, the types of the proofs provided are limited (e.g., range proofs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>).
Second, the time required for verification grows typically exponentially with the number of users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which renders these approaches unscalable, and thus unsuitable for large-scale applications.
Another technique to tackle the verification issue is to use a Trusted Execution Environment to perform the computations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
However, these environments are not widely available on user devices, especially smartphones.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">Constrained defenses</h3>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Although <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> provides improvements for users’ privacy, it opens the door for plenty of attacks, which can be applied by both malicious servers and malicious users.
In this work, we focus on privacy attacks, where the attacker aims at inferring information about users.
These attacks occur in two modes: passive and active.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><em id="S2.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Passive attacks</em>:
The attacker observes the joint model and periodic updates, which can be used to infer information about other users.
Shokri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> introduce a membership inference attack using shadow models, which can be performed in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>.
Melis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> propose a property inference attack, leveraging snapshots of the global model.
Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> obtain the training data of a target user from their shared updates (gradients).
To mitigate such attacks, several defense techniques can be applied by the server and users.
The users can perturb their model updates before sharing them by using one or a combination of the following methods:

<span id="S2.I2.i1.I1" class="ltx_inline-enumerate">
<span id="S2.I2.i1.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S2.I2.i1.I1.i1.1" class="ltx_text">adding noise (e.g., local differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>),
</span></span>
<span id="S2.I2.i1.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S2.I2.i1.I1.i2.1" class="ltx_text">sharing only a fraction of the updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, or
</span></span>
<span id="S2.I2.i1.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(3)</span> <span id="S2.I2.i1.I1.i3.1" class="ltx_text">using the dropout technique in a neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. </span></span>
</span>
However, these techniques usually lead to a substantial loss of the model accuracy.
That is, the trade-off between privacy and accuracy needs to be considered <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
On the server side, protecting user privacy requires to break the linkability of the individual users with their updates.
To achieve this, a secure computation can be performed, e.g, secure aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or secure shuffling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Besides the efficiency issues these techniques suffer from, they hinder the server from detecting malicious updates, creating by that an attack surface for malicious users.
</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><em id="S2.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Active attacks</em>:
The attacker participates in the training process as a user, who maliciously modifies their updates to infer information about other users.
Hitaj et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> propose a reconstruction attack where the attacker provokes the target user to overfit the model on their training data.
Melis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> present a property inference attack based on multi-task learning. Nasr et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> use gradient ascent to perform a membership inference attack.
To detect and mitigate such attacks, several methods can be considered, such as anomaly detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, median-based aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, trimmed mean <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and redundancy-based encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
However, some of these methods require accessing the individual user updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which is impeded by secure aggregation.
In addition, the heavy computations and uncertainty of convergence remain main limitations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Hierarchical Federated Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> is, by definition, coordinated by a central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
We relax this definition to apply <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> within a hierarchical architecture and refer to it as <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic"> <a href="#id3.3.id3"><span href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Hierarchical Federated Learning</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>)</em>.
A restricted version of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>, considering only three layers, has been presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
However, privacy was not considered a prime goal.
In <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>, there is one <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">root server</em>, connected to multiple <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">group servers</em>, which are organized in a tree structure (see Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Hierarchical Federated Learning ‣ Enhancing Privacy via Hierarchical Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
The lowest layer of group servers connects to users (data owners), which are clustered in <em id="S3.p1.1.4" class="ltx_emph ltx_font_italic">user groups</em>.
The hierarchy can contain multiple layers of group servers and can be unbalanced such that different branches vary in their number of layers.
The users send their model updates to the upper layer to be aggregated by the corresponding group servers.
The aggregation process continues in multiple stages (on each layer) towards the root server.
Every two subsequent layers can have a different number of communication (aggregating) rounds before pushing their models to the upper layer.
After aggregation, the global model is forwarded along the hierarchy downwards to the users.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1" class="ltx_p ltx_align_center"><span id="S3.F1.1.1" class="ltx_text">

<img src="/html/2004.11361/assets/images/hfl_arch.png" id="S3.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="281" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of Hierarchical Federated Learning architecture.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">From an abstract perspective, a hierarchy is a hybrid solution between centralized and fully decentralized architectures.
On the one hand, a hierarchy can cope with the scalability and failure tolerance limitations of centralized architectures.
That is evidenced by the existence and active operation of hierarchical architectures within complex, large-scale systems such as the Internet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
On the other hand, a hierarchy helps to tackle the management challenges encountered in fully decentralized architectures.
We can observe this advantage in the organizational structure of companies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> was originally proposed with the intention to be based on a cloud computing infrastructure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Such centralized infrastructure represents a network bottleneck, especially with the increase of resource-constrained devices (e.g., smartphones and IoT sensors).
To alleviate this issue, there is a trend towards decentralized computing infrastructures (e.g., edge and fog computing) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which are introducing hierarchies in communication and computing architectures.
<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> represents a natural step in extending <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> into such emerging infrastructures.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Online social networks represent yet another application where the system architecture matches with <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>. Motivated by privacy-related issues, more decentralized social networks are emerging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
Diaspora and Mastodon are two of the most popular, which leverage the concept of local servers.
Users can choose which local server to connect to and where to store their data.
This architecture represents a hierarchy where some functions are delegated to the higher-layer servers.
In contrast, privacy-related functions (e.g., data storage) are pushed down in the hierarchy to local servers and users.
</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Privacy Implications of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a></span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we discuss the implications of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> in relation to the privacy-related issues of Section <a href="#S2" title="2 Federated Learning and Open Issues ‣ Enhancing Privacy via Hierarchical Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">Centralized control</h3>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">In <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the distribution of functions between the server and users is unbalanced;
the control is concentrated on the server.
In contrast, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> allows different <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> functions to be distributed throughout the hierarchy to the group servers and users, which helps to overcome the performance bottleneck and single point of failure typically found in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>.
This is because the group servers can use the local aggregated models within the hierarchy until they converge on the root server at the top.
Next, we introduce alternative placements for each of the <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> functions.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><em id="S4.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Sampling users</em>:
In <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, the server selects the users to participate in the training process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
A malicious server could misuse this privilege to attack specific users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
In contrast, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> allows sampling to be performed in multiple stages, where a lower layer is sampled by the upper one, till the top.
This can cope with the gap of trust between the users and server in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, since each two subsequent layers in <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> are more likely to have a better base for trust and accountability.
In fog computing, users can be sampled by edge devices, which can be, for instance, a router in the neighborhood.
In decentralized social networks, the local server, which is chosen by the users, can perform sampling.
Also, users can apply privacy-preserving distributed sampling algorithms, such as Anonymous Random Walk used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Broadcasting the training algorithm and model</em>:
Congzheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> have shown that malicious training algorithms can leak information about the training data.
To address this issue in <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>, the group servers can be seen as a protective layer where the algorithms are inspected and verified before being pushed to the users.</p>
</div>
<div id="S4.I1.i2.p2" class="ltx_para">
<p id="S4.I1.i2.p2.1" class="ltx_p">From an efficiency perspective, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> provides the possibility for the group servers to adapt the training process to suit the resources of their users’ devices, e.g., by adjusting the hyper-parameters of the algorithm or by broadcasting a fraction of the model (sub-model).
In <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, training a sub-model by the users can reveal information about their data to the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, while in <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> dividing the model to sub-models can happen in different phases through the layers such that an upper layer is not aware how or whether the lower layer divides the model further.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><em id="S4.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Aggregating the model updates</em>:
As opposed to <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, where the aggregation is conducted only by the server, in <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>, multiple levels of aggregation occur according to the number of layers in the hierarchy.
This aggregation scheme implies that only the lowest layer of group servers receive individual updates from users, while all the upper layers process aggregated updates.
Consequently, the need for secure aggregation in the upper layers is reduced, which can be interpreted to a performance improvement.
In addition, the cascade aggregation process helps to protect the identities of the users from the higher-layer servers.
</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><em id="S4.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Broadcasting the updated global model</em>:
Some privacy attacks in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> require distributing a poisoned model to the target users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
The flat topology at the users level in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> allows this type of attacks easy access.
While <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> provides the possibility to control the model dissemination more effectively through the multiple layers.
In case a poisoned model were to be detected within the hierarchy, the group servers could refrain from pushing it to the users.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">Limited verifiability</h3>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">The poor scalability of the verification methods in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>) hinders their deployment in large-scale applications.
In <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>, verifying the updates of the users can be performed by the corresponding group servers.
Grouping users in comparably small groups reduces the computational overhead on the group servers and makes the deployment of verification methods more feasible.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">Constrained defenses</h3>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">Most of the defense methods in <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a> come with the cost of lower model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, considerable computational overhead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, or both <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> provides the possibility to apply these methods in a more flexible manner.
For instance, applying different methods in different parts of the hierarchy, such that not all the users suffer from a heavy loss in accuracy nor have to perform costly computations.</p>
</div>
<div id="S4.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p2.1" class="ltx_p">As the upper layers in <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> process only aggregated models, not individual users’ updates, the necessity of secure aggregation can be reduced or even eliminated in these layers.
This can lead to a performance improvement and allows applying anomaly detection methods (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>) to detect malicious models received from lower layers.
Detecting a malicious model from a specific group server can be followed by excluding that server from further aggregation rounds while maintaining the training process functioning normally in the rest of the hierarchy.
Furthermore, the affected server can be notified about its malicious model so it can take more computational expensive countermeasures (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>) in their local user group.</p>
</div>
<div id="S4.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p3.1" class="ltx_p">Unlike <a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>, <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> allows leveraging the trust between users as an additional line of defense.
In social networks, grouping users based on trusted graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> can reduce the probability of attacks within groups, as it is assumed to be more difficult for malicious users to break into the groups.
Consequently, users may relax their local defenses (e.g., reduce the noise added locally) to achieve a model with higher accuracy.
At the same time, the group servers can take the responsibility of protecting their groups from attacks across groups by applying specific defense methods (e.g., adding noise to the aggregated updates).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we presented a number of privacy-related issues in  <a href="#id1.1.id1"><span href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Learning</span></span></a> (<a href="#id1.1.id1"><abbr href="#id1.1.id1" title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr></a>), such as centralized control and constrained defenses.
We discussed how  <a href="#id3.3.id3"><span href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Hierarchical Federated Learning</span></span></a> (<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a>) can potentially address these issues.
<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> utilizes an architecture that lies between the centralized (FL) and fully decentralized learning and comes with a series of privacy advantages.
<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> enables high flexibility in functionality distribution, which facilitates known defense and verification methods.
<a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> allows leveraging the trust between the participants in different application scenarios to mitigate several threats.
Based on this contribution, we wish to encourage further research into the direction of <a href="#id3.3.id3"><abbr href="#id3.3.id3" title="Hierarchical Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HFL</span></abbr></a> to unlock its full potential.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 251805230/GRK 2050.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Andrea Bittau, Ulfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth
Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and
Bernhard Seefeld.

</span>
<span class="ltx_bibblock">Prochlo: Strong privacy for analytics in the crowd.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th Symposium on Operating Systems
Principles</span>, pages 441–459, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 1175–1191, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Benedikt Bünz, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter Wuille,
and Greg Maxwell.

</span>
<span class="ltx_bibblock">Bulletproofs: Short proofs for confidential transactions and more.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2018 IEEE Symposium on Security and Privacy (SP)</span>, pages
315–334. IEEE, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Lukas Burhhalter.

</span>
<span class="ltx_bibblock">Robust Secure Aggregation for Privacy-Preserving Federated Learning
with Adversaries.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2019 cheeeeck</span>. ACM, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos.

</span>
<span class="ltx_bibblock">Draco: Byzantine-resilient distributed training via redundant
gradients.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1803.09877</span>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Deepesh Data, Linqi Song, and Suhas Diggavi.

</span>
<span class="ltx_bibblock">Data encoding for byzantine-resilient distributed optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.02664</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Julien Gedeon, Jens Heuschkel, Lin Wang, and Max Mühlhäuser.

</span>
<span class="ltx_bibblock">Fog computing: Current research and future challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">KuVS-Fachgespräch Fog Comput</span>, 1:1–4, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.

</span>
<span class="ltx_bibblock">Deep models under the gan: information leakage from collaborative
deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 603–618, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Elliott Jaques.

</span>
<span class="ltx_bibblock">In praise of hierarchy.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Markets, hierarchies and networks: The coordination of social
life</span>, pages 48–52, 1991.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Wenjun Jiang, Guojun Wang, and Jie Wu.

</span>
<span class="ltx_bibblock">Generating trusted graphs for trust evaluation in online social
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Future generation computer systems</span>, 31:48–58, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova,
and Adam Smith.

</span>
<span class="ltx_bibblock">What can we learn privately?

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">SIAM Journal on Computing</span>, 40(3):793–826, 2011.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Don’t use large mini-batches, use local sgd.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1808.07217</span>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L Liu, J Zhang, S Song, and KB Letaief.

</span>
<span class="ltx_bibblock">Client-edge-cloud hierarchical federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Commun.(ICC), to be published</span>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief.

</span>
<span class="ltx_bibblock">Edge-assisted hierarchical federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.06641</span>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
David McCandless.

</span>
<span class="ltx_bibblock">World’s Biggest Data Breaches &amp; Hacks.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks</a>,
April 2019.

</span>
<span class="ltx_bibblock">Online; accessed 05.03.2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.05629</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Learning differentially private language models without losing
accuracy.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.06963</span>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2019 IEEE Symposium on Security and Privacy (SP)</span>, pages
691–706. IEEE, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri, and Amir Houmansadr.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning: Stand-alone and
federated learning under passive and active white-box inference attacks.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.00910</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Shiqi Shen, Shruti Tople, and Prateek Saxena.

</span>
<span class="ltx_bibblock">Auror: Defending against poisoning attacks in collaborative deep
learning systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the 32nd Annual Conference on Computer
Security Applications</span>, pages 508–519, 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Reza Shokri and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security</span>, pages 1310–1321, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</span>, pages
3–18. IEEE, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Machine learning models that remember too much.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 587–601, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov.

</span>
<span class="ltx_bibblock">Dropout: a simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">The journal of machine learning research</span>, 15(1):1929–1958,
2014.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lakshminarayanan Subramanian, Sharad Agarwal, Jennifer Rexford, and Randy H
Katz.

</span>
<span class="ltx_bibblock">Characterizing the internet hierarchy from multiple vantage points.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings. Twenty-First Annual Joint Conference of the IEEE
Computer and Communications Societies</span>, volume 2, pages 618–627. IEEE, 2002.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu.

</span>
<span class="ltx_bibblock">D <math id="bib.bib27.1.m1.1" class="ltx_Math" alttext="\hat{}2" display="inline"><semantics id="bib.bib27.1.m1.1a"><mrow id="bib.bib27.1.m1.1.1" xref="bib.bib27.1.m1.1.1.cmml"><mover accent="true" id="bib.bib27.1.m1.1.1.2" xref="bib.bib27.1.m1.1.1.2.cmml"><mi id="bib.bib27.1.m1.1.1.2.2" xref="bib.bib27.1.m1.1.1.2.2.cmml"></mi><mo id="bib.bib27.1.m1.1.1.2.1" xref="bib.bib27.1.m1.1.1.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="bib.bib27.1.m1.1.1.1" xref="bib.bib27.1.m1.1.1.1.cmml">​</mo><mn id="bib.bib27.1.m1.1.1.3" xref="bib.bib27.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="bib.bib27.1.m1.1b"><apply id="bib.bib27.1.m1.1.1.cmml" xref="bib.bib27.1.m1.1.1"><times id="bib.bib27.1.m1.1.1.1.cmml" xref="bib.bib27.1.m1.1.1.1"></times><apply id="bib.bib27.1.m1.1.1.2.cmml" xref="bib.bib27.1.m1.1.1.2"><ci id="bib.bib27.1.m1.1.1.2.1.cmml" xref="bib.bib27.1.m1.1.1.2.1">^</ci><csymbol cd="latexml" id="bib.bib27.1.m1.1.1.2.2.cmml" xref="bib.bib27.1.m1.1.1.2.2">absent</csymbol></apply><cn type="integer" id="bib.bib27.1.m1.1.1.3.cmml" xref="bib.bib27.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib27.1.m1.1c">\hat{}2</annotation></semantics></math>: Decentralized training over decentralized data.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1803.07068</span>, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Aidmar Wainakh, Tim Grube, Jörg Daubert, and Max Mühlhäuser.

</span>
<span class="ltx_bibblock">Efficient privacy-preserving recommendations based on social graphs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the 13th ACM Conference on Recommender
Systems</span>, pages 78–86, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Aidmar Wainakh, Tim Grube, and M Max.

</span>
<span class="ltx_bibblock">Tweet beyond the Cage: A Hybrid Solution for the Privacy Dilemma in
Online Social Networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">IEEE Global Communications Conference</span>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level privacy leakage
from federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2019-IEEE Conference on Computer
Communications</span>, pages 2512–2520. IEEE, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin.

</span>
<span class="ltx_bibblock">Verifynet: Secure and verifiable federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:911–926, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards optimal statistical
rates.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1803.01498</span>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
14747–14756, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.11360" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.11361" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.11361">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.11361" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.11362" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 16:18:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
