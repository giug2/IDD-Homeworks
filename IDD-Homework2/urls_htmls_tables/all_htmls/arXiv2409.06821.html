<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts</title>
<!--Generated on Tue Sep 10 19:06:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.06821v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S1" title="In Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S2" title="In Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S2.SS1" title="In 2 Related Works ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Foundation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S2.SS2" title="In 2 Related Works ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Prompt Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S2.SS3" title="In 2 Related Works ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Parameter-efficient Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S2.SS4" title="In 2 Related Works ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Key Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3" title="In Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS1" title="In 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Sam2Rad</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS1.SSS0.Px1" title="In 3.1 Sam2Rad ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title">Image encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS1.SSS0.Px2" title="In 3.1 Sam2Rad ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title">Mask decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS1.SSS0.Px3" title="In 3.1 Sam2Rad ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title">Prompt encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS1.SSS1" title="In 3.1 Sam2Rad ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Prompt predictor network (prompt learning)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS2" title="In 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Losses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4" title="In Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS1" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS2" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS3" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS4" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS5" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Does prompt learning work any better than bounding box prompts?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS6" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Zero-shot segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS7" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Few-shot segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS8" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Robustness to heavy data augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS9" title="In 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.9 </span>Robustness to noisy ground truth masks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S5" title="In Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Assefa Seyoum Wahd
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Banafshe Felfeliyan
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuyue Zhou
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shrimanti Ghosh
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adam McArthur
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiechen Zhang
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jacob L. Jaremko
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abhilash Hareendranathan
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">The use of foundation models like the Segment Anything Model (SAM) for medical image segmentation requires high-quality manual prompts, which are time-consuming to generate and require medical expertise. Even when prompted with sparse prompts, like boxes, points, or text, and dense prompts such as masks SAM or its variants like MedSAM fine-tuned on medical images fail to segment bones in ultrasound (US) images due to significant domain shift.</p>
<p class="ltx_p" id="id1.1">We propose a new prompt learning approach to adapt SAM, its recent variant SAM 2, and other variants to segment bony regions in US images without requiring human prompts. We introduce a prompt predictor network (PPN) with a lightweight cross-attention module to augment the existing prompt encoder, predicting prompt embeddings directly from features extracted by the image encoder. PPN outputs bounding box and mask prompts and <math alttext="N" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mi id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">italic_N</annotation></semantics></math> 256-dimensional embeddings for the regions of interest. Our new framework also allows optional manual prompting which can be concatenated with learned prompts as inputs to the the mask decoder. PPN and the mask decoder can be trained end-to-end using parameter-efficient fine-tuning (PEFT) methods. To preserve SAM’s vast world knowledge, we ideally want to keep all modules in SAM frozen and train only the prompt predictor network. We demonstrate the prompt predictor network’s effectiveness by freezing all parameters in SAM and achieving a Dice score comparable to that of the fine-tuned mask decoder.</p>
<p class="ltx_p" id="id3.id2">Our model can be used autonomously without human supervision, semi-autonomously with <span class="ltx_text ltx_font_italic" id="id3.id2.1">human-in-the-loop</span>, or fully manual where the model uses human prompts only, like the original SAM. In the semi-autonomous mode, the model predicts masks, and if unsatisfactory, users can provide additional prompts (boxes, points, or brushes) to refine the prediction. The autonomous setting can be used for real-time applications while the semi-autonomous is ideal for data labeling, or in active learning frameworks.</p>
<p class="ltx_p" id="id4.id3">We tested the proposed model - Sam2Rad on 3 musculoskeletal US datasets - wrist (3822 images), rotator cuff (1605 images), and hip (4849 images).</p>
<p class="ltx_p" id="id5.id4">Without Sam2Rad, all SAM2 variants failed to segment shoulder US in zero-shot generalization with bounding box prompts, while SAM2 showed better segmentation accuracy across other US datasets vs. SAM. Our model, Sam2Rad, improved the performance of all SAM base networks in all datasets, without requiring manual prompts. The improvement in accuracy ranged from a 2-7% increase in Dice score for hip or wrist, and up to 33% improvement in Dice score (from 49% to 82%) on shoulder data. Notably, Sam2Rad could be trained with as few as 10 labeled images. Sam2Rad is compatible with any SAM architecture and can be utilized for automatic segmentation.</p>
<p class="ltx_p" id="id6.id5">The code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/aswahd/SamRadiology" title="">https://github.com/aswahd/SamRadiology</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Foundation models , Segment anything , Segment anything 2 , Medical imaging , Ultrasound segmentation

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Computers in Biology and Medicine</span></span></span>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\affiliation</span>
<p class="ltx_p" id="p1.2">[uofa]organization=Department of Radiology and Diagnostic Imaging, University of Alberta,
country=Canada</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\affiliation</span>
<p class="ltx_p" id="p2.2">[mcgill]organization=School of Computer Science, McGill University,
country=Canada</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Automated segmentation of structures in medical images is an essential step for various computer-aided diagnoses and treatment planning procedures. Over the past decade, the evolution of deep learning models has significantly accelerated the progress of automatic medical image segmentation. Traditionally, segmentation tasks are approached separately by specialized networks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib8" title="">8</a>]</cite>. As a result, their use in broader clinical contexts is limited as this approach requires substantial amounts of data to train each task and ignores correlations between different segmentation objectives. Meanwhile, deep learning scaling laws suggest that increasing the size of the training dataset and the number of parameters of a model improves accuracy <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib13" title="">13</a>]</cite> — resulting in foundation models. These models, often trained on extensive datasets with a wide array of general information, encapsulate a broad spectrum of world knowledge that can be adapted to specific applications with relatively minimal effort compared to training a model from scratch. Foundation models are shown to generalize well to downstream tasks with few labeled images, are more robust to adversarial attacks, resilient to domain shift, and effectively work in zero-shot settings. Consequently, foundation models have become popular in medical imaging, where large-scale labeled datasets are often scarce.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="622" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.7.3.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Sam2Rad architecture<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.2">. We propose a lightweight two-way attention module to predict prompts for a queried object. Each object (<math alttext="k" class="ltx_Math" display="inline" id="S1.F1.3.1.1.m1.1"><semantics id="S1.F1.3.1.1.m1.1b"><mi id="S1.F1.3.1.1.m1.1.1" xref="S1.F1.3.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.F1.3.1.1.m1.1c"><ci id="S1.F1.3.1.1.m1.1.1.cmml" xref="S1.F1.3.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.1.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.1.1.m1.1e">italic_k</annotation></semantics></math>) is represented by <math alttext="T_{k}\in\mathbb{R}^{N_{i}\times 256}" class="ltx_Math" display="inline" id="S1.F1.4.2.2.m2.1"><semantics id="S1.F1.4.2.2.m2.1b"><mrow id="S1.F1.4.2.2.m2.1.1" xref="S1.F1.4.2.2.m2.1.1.cmml"><msub id="S1.F1.4.2.2.m2.1.1.2" xref="S1.F1.4.2.2.m2.1.1.2.cmml"><mi id="S1.F1.4.2.2.m2.1.1.2.2" xref="S1.F1.4.2.2.m2.1.1.2.2.cmml">T</mi><mi id="S1.F1.4.2.2.m2.1.1.2.3" xref="S1.F1.4.2.2.m2.1.1.2.3.cmml">k</mi></msub><mo id="S1.F1.4.2.2.m2.1.1.1" xref="S1.F1.4.2.2.m2.1.1.1.cmml">∈</mo><msup id="S1.F1.4.2.2.m2.1.1.3" xref="S1.F1.4.2.2.m2.1.1.3.cmml"><mi id="S1.F1.4.2.2.m2.1.1.3.2" xref="S1.F1.4.2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S1.F1.4.2.2.m2.1.1.3.3" xref="S1.F1.4.2.2.m2.1.1.3.3.cmml"><msub id="S1.F1.4.2.2.m2.1.1.3.3.2" xref="S1.F1.4.2.2.m2.1.1.3.3.2.cmml"><mi id="S1.F1.4.2.2.m2.1.1.3.3.2.2" xref="S1.F1.4.2.2.m2.1.1.3.3.2.2.cmml">N</mi><mi id="S1.F1.4.2.2.m2.1.1.3.3.2.3" xref="S1.F1.4.2.2.m2.1.1.3.3.2.3.cmml">i</mi></msub><mo id="S1.F1.4.2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S1.F1.4.2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S1.F1.4.2.2.m2.1.1.3.3.3" xref="S1.F1.4.2.2.m2.1.1.3.3.3.cmml">256</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.4.2.2.m2.1c"><apply id="S1.F1.4.2.2.m2.1.1.cmml" xref="S1.F1.4.2.2.m2.1.1"><in id="S1.F1.4.2.2.m2.1.1.1.cmml" xref="S1.F1.4.2.2.m2.1.1.1"></in><apply id="S1.F1.4.2.2.m2.1.1.2.cmml" xref="S1.F1.4.2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S1.F1.4.2.2.m2.1.1.2.1.cmml" xref="S1.F1.4.2.2.m2.1.1.2">subscript</csymbol><ci id="S1.F1.4.2.2.m2.1.1.2.2.cmml" xref="S1.F1.4.2.2.m2.1.1.2.2">𝑇</ci><ci id="S1.F1.4.2.2.m2.1.1.2.3.cmml" xref="S1.F1.4.2.2.m2.1.1.2.3">𝑘</ci></apply><apply id="S1.F1.4.2.2.m2.1.1.3.cmml" xref="S1.F1.4.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S1.F1.4.2.2.m2.1.1.3.1.cmml" xref="S1.F1.4.2.2.m2.1.1.3">superscript</csymbol><ci id="S1.F1.4.2.2.m2.1.1.3.2.cmml" xref="S1.F1.4.2.2.m2.1.1.3.2">ℝ</ci><apply id="S1.F1.4.2.2.m2.1.1.3.3.cmml" xref="S1.F1.4.2.2.m2.1.1.3.3"><times id="S1.F1.4.2.2.m2.1.1.3.3.1.cmml" xref="S1.F1.4.2.2.m2.1.1.3.3.1"></times><apply id="S1.F1.4.2.2.m2.1.1.3.3.2.cmml" xref="S1.F1.4.2.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S1.F1.4.2.2.m2.1.1.3.3.2.1.cmml" xref="S1.F1.4.2.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S1.F1.4.2.2.m2.1.1.3.3.2.2.cmml" xref="S1.F1.4.2.2.m2.1.1.3.3.2.2">𝑁</ci><ci id="S1.F1.4.2.2.m2.1.1.3.3.2.3.cmml" xref="S1.F1.4.2.2.m2.1.1.3.3.2.3">𝑖</ci></apply><cn id="S1.F1.4.2.2.m2.1.1.3.3.3.cmml" type="integer" xref="S1.F1.4.2.2.m2.1.1.3.3.3">256</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.2.2.m2.1d">T_{k}\in\mathbb{R}^{N_{i}\times 256}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.2.2.m2.1e">italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT × 256 end_POSTSUPERSCRIPT</annotation></semantics></math> learnable prompts. These class prompts are conditioned on features obtained from a pretrained SAM image encoder. During inference, the prompt encoder can be removed to run the model autonomously, or it can be used in combination with the prompt predictor network for human-in-the-loop operation.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="697" id="S1.F2.g1" src="x2.png" width="439"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">
SAM variants show clear limitations when segmenting US images using bounding box prompts. Surprisingly, MedSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib14" title="">14</a>]</cite>, despite being trained on a large-scale medical dataset, performs the worst. In contrast, SAM2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib15" title="">15</a>]</cite>, SAM’s latest iteration, demonstrates the best zero-shot generalization. However, smaller SAM2 variants (such as Sam2-Tiny) still incorrectly include background in the segmentation mask. Notably, all models struggle with shoulder US. These observations suggest that bounding box prompts may not be sufficient, or that the models require fine-tuning. In section <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.SS1.SSS1" title="3.1.1 Prompt predictor network (prompt learning) ‣ 3.1 Sam2Rad ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>, we demonstrate that our proposed prompt learning approach can improve the performance of these models while keeping their pretrained parameters frozen.
</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Foundation Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Foundation models, popularized by OpenAI’s CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib16" title="">16</a>]</cite>, are large-scale deep learning models trained on extensive datasets, enabling them to effectively generalize across various downstream tasks. These models are typically pretrained using self-supervised learning methods on large datasets, allowing them to learn a wide range of patterns without requiring labeled examples. CLIP, for instance, is trained on millions of text-image pairs to create a joint embedding space for images and text. It has been widely used for image classification with text prompts, object detection, image segmentation, and various NLP tasks. Segment Anything Model (SAM) is a foundation model trained on the largest segmentation dataset to date, comprising over 10 million images and 1 billion masks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Segment Anything Model (SAM) has emerged as a foundation model for natural image segmentation, offering one framework for handling multiple segmentation tasks at once, given any segmentation prompt, such as points, bounding boxes, masks or text  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib17" title="">17</a>]</cite>.
SAM has inspired a plethora of follow-up works, leading to the development of universal models for medical image segmentation  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib33" title="">33</a>]</cite>. SAM2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib15" title="">15</a>]</cite>, the latest iteration of SAM, is designed for both image and video segmentation. It can track objects across video frames when given a prompt in any frame. Notably, SAM2 operates <math alttext="6\times" class="ltx_math_unparsed" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1b"><mn id="S2.SS1.p2.1.m1.1.1">6</mn><mo id="S2.SS1.p2.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">6\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">6 ×</annotation></semantics></math> faster than its predecessor. It incorporates a memory attention mechanism, allowing the current frame’s prediction to be conditioned on the previous frames’ image embeddings and predictions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">However, studies that explored SAM’s application in the medical image domain have identified its limitations in specific unseen medical modalities  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib37" title="">37</a>]</cite>. Efforts have been made to address this by fine-tuning SAM on large curated datasets in medical domain, resulting in medical foundation models like  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib38" title="">38</a>]</cite>. Nonetheless, these models’ performance remains limited to the modalities they were trained on and heavily relies on prompt quality. This poses a significant challenge when these models are applied to substantially different data distributions such as ultrasound (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">2</span></a>). As shown in the Figure, SAM and its variants have sub-optimal performance on US data. This could be due to the unique nature of imaging artifacts seen in US, like the speckle interference pattern, which occurs when the distance between adjacent scatters is less than the cell resolution. Addressing such limitations would require domain-specific fine-tuning for optimal performance while maintaining broad applicability. Furthermore, these foundation models require manual prompts, which are practically difficult without the necessary medical expertise. As a result, deep learning methods for automatically generating appropriate prompts have become a significant focus of research.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Prompt Learning</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Traditional approaches to training deep learning models often involve either training the model from scratch or pretraining it with unlabeled data followed by fine-tuning with labeled data for specific tasks. In contrast, prompt-based learning models can autonomously adapt to various tasks by utilizing cues provided by users. These models leverage the domain knowledge introduced through prompts to guide their performance, enabling more efficient and flexible task adaptation without extensive task-specific training.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">SAM demonstrates impressive zero-shot generalization to natural images. However, it requires prompts—such as points, boxes, text, or masks—to segment objects in an image, even during inference. Prompts can be either provided by humans (e.g., drawing boxes around objects or inputting text) or predicted by standalone models. Grounded SAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib39" title="">39</a>]</cite> uses Grounding DINO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib40" title="">40</a>]</cite> to generate the bounding boxes for all objects in a given image, which are then used to prompt SAM. The limitation of this approach is that it requires a model with a separate image encoder trained from scratch.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Recently, automatic techniques like PerSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib41" title="">41</a>]</cite> and Matcher <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib42" title="">42</a>]</cite> have been proposed to generate bounding box prompts for natural images. These approaches assume that foreground and background patches are dissimilar and use patch similarities between a reference image and a test image to generate prompts. However, in US images foreground and background patches are often hard to distinguish, resulting in a high cosine similarity between unrelated patches. We propose a new method to learn prompts using SAM’s image encoder instead of relying on other standalone models like Grounding DINO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib40" title="">40</a>]</cite>. We develop a prompt predictor network (PPN) that takes in image features extracted by SAM’s image encoder and predicts appropriate prompts for regions of interest.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Instead, we propose a lightweight attention mechanism to learn a non-linear function to predict the location of the target regions in the image. The prompt predictor network uses a lightweight attention module that operates on the image features obtained from SAM’s image encoder to predict the prompts for the target regions.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">To the best of our knowledge, this is the first work to report prompt learning for SAM2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib15" title="">15</a>]</cite>, the latest version of SAM. For a comparison between our prompt predictor network and other prompting techniques, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.F4" title="Figure 4 ‣ 3.2 Training Losses ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parameter-efficient Fine-tuning</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Several studies have shown that SAM fails to generalize to medical images even with manual prompts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib37" title="">37</a>]</cite>. We encountered similar issues with ultrasound (US) images, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">2</span></a>. While a straightforward approach to address this issue is fine-tuning all the model’s parameters—vanilla fine-tuning, this method can be suboptimal, often requiring a substantial amount of labeled data and sometimes performing poorly compared to parameter-efficient fine-tuning (PEFT) methods. For example, CoOp <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib43" title="">43</a>]</cite> reported a 40% drop in classification accuracy when vanilla fine-tuning the CLIP image encoder. PEFT is a more efficient way to adapt foundation models to downstream tasks. PEFT methods like adapters <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib27" title="">27</a>]</cite> and low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib44" title="">44</a>]</cite> are designed to adapt foundation models to downstream tasks with minimal changes to the model’s parameters. PEFT methods are particularly useful when labeled data is limited. Adapters <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib27" title="">27</a>]</cite> insert small modules in parallel to the transformer layers of a foundation model and fine-tune only these new parameters. LoRA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib23" title="">23</a>]</cite> is another PEFT method that fine-tunes only a small subset of the model’s parameters. To avoid SAM or SAM2 failure in image segmentation even with manual prompts, the mask decoder can be fine-tuned using LoRA while keeping the image encoder’s parameters frozen. Studies show integrating PEFT with SAM not only reduces computational costs but also mitigates overfitting and improves the overall segmentation accuracy <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Key Contributions</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.3">We propose a new PPN that directly predicts <math alttext="N" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><mi id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">italic_N</annotation></semantics></math> 256-dimensional embeddings alongside bounding box coordinates and mask prompts. This approach allows the network to learn an abstract representation of a prompt instead of a sparse representation like a box. Moreover, <math alttext="N" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1"><semantics id="S2.SS4.p1.2.m2.1a"><mi id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><ci id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.2.m2.1d">italic_N</annotation></semantics></math> can be set to any value, in contrast to other methods that typically predict a single bounding box per object. Studies show that adapting SAM to medical imaging often requires several-point prompts. In PPN, <math alttext="N" class="ltx_Math" display="inline" id="S2.SS4.p1.3.m3.1"><semantics id="S2.SS4.p1.3.m3.1a"><mi id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.1b"><ci id="S2.SS4.p1.3.m3.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.3.m3.1d">italic_N</annotation></semantics></math> can represent any number of points or other object-relevant information. For instance, for objects that are challenging to segment, such as thin overlapping tissues, the network can adapt by predicting multiple point prompts or using alternative representations</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">To the best of our knowledge, this is the first work to report prompt learning for SAM2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib15" title="">15</a>]</cite>, the latest version of SAM.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">The code is available on <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/aswahd/Sam2Radiology" title="">https://github.com/aswahd/Sam2Radiology</a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose a novel framework, SAM2Rad, designed for segmenting medical images without the need for manual prompts. This paper focuses specifically on applying this framework to ultrasound imaging. SAM2Rad incorporates a prompt predictor (generator) network (PPN) that predicts candidate bounding box proposals, mask prompts, and <math alttext="N" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_N</annotation></semantics></math> 256-dimensional prompts. Similar to object detection in Mask R-CNN <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib2" title="">2</a>]</cite>, where bounding box regression heads use features from the backbone network to predict coordinates, PPN’s bounding box regression head can be viewed as object detection. It utilizes features from SAM’s image encoder to generate appropriate prompts for segmenting target regions, functioning analogously to Mask R-CNN’s region proposal network (RPN) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Sam2Rad</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The proposed SAM2Rad framework comprises a pretrained SAM/SAM2 encoder, mask decoder, prompt encoder, and prompt predictor network (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Image encoder</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.2">The image encoder extracts features from the input image. It is based on the vision transformer (ViT) architecture <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib45" title="">45</a>]</cite> and pretrained using masked autoencoders (MAE) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib46" title="">46</a>]</cite>. For an input image of size <math alttext="3\times 1024\times 1024" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">3</mn><mo id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">1024</mn><mo id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.4" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.4.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">3</cn><cn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">1024</cn><cn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.4.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.4">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">3\times 1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">3 × 1024 × 1024</annotation></semantics></math>, the image encoder outputs embeddings of size <math alttext="256\times 64\times 64" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">256</mn><mo id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">64</mn><mo id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.4" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"><times id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1"></times><cn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2">256</cn><cn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3">64</cn><cn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.4.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">256\times 64\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.2.m2.1d">256 × 64 × 64</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Mask decoder</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">The mask decoder transforms the image embeddings and prompt embeddings to a final mask.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Prompt encoder</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.3">The prompt encoder processes both sparse prompts (points and bounding boxes) and dense prompts (masks). Points are encoded into 256-dimensional embeddings, combining positional and learned embeddings. Bounding boxes are similarly encoded, using positional encodings for the top-left corner and learned embeddings for the bottom-right corner. Masks (<math alttext="256\times 256" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">256</mn><mo id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"><times id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1"></times><cn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2">256</cn><cn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.1.m1.1d">256 × 256</annotation></semantics></math>) are transformed into <math alttext="256\times 64\times 64" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><mrow id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml">256</mn><mo id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml">64</mn><mo id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.4" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1"><times id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1"></times><cn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2">256</cn><cn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3">64</cn><cn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.4.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">256\times 64\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.2.m2.1d">256 × 64 × 64</annotation></semantics></math> embeddings using a <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px3.p1.3.m3.1a"><mrow id="S3.SS1.SSS0.Px3.p1.3.m3.1b"><mn id="S3.SS1.SSS0.Px3.p1.3.m3.1.1">4</mn><mo id="S3.SS1.SSS0.Px3.p1.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.3.m3.1d">4 ×</annotation></semantics></math> downsampling convolution block.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">To align the learned prompts with the manual prompts SAM was trained on, our predictor network outputs bounding box coordinates, mask prompts, and <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p2.1.m1.1a"><mi id="S3.SS1.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.1.m1.1b"><ci id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p2.1.m1.1d">italic_N</annotation></semantics></math> 256-dimensional embeddings. For a comparison between our prompt predictor network and other prompting techniques, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.F4" title="Figure 4 ‣ 3.2 Training Losses ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p3.1">In the next section, we outline the design of the prompt predictor.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Prompt predictor network (prompt learning)</h4>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="828" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.8.3.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F3.4.2" style="font-size:90%;">Prompt predictor network architecture.<span class="ltx_text ltx_font_medium" id="S3.F3.4.2.2"> Given image features, our <span class="ltx_text ltx_font_italic" id="S3.F3.4.2.2.1">prompt predictor network</span> predicts prompts for the queried object. The predictor network outputs bounding box coordinates of the target object, an intermediate mask prompt, and <math alttext="N" class="ltx_Math" display="inline" id="S3.F3.3.1.1.m1.1"><semantics id="S3.F3.3.1.1.m1.1b"><mi id="S3.F3.3.1.1.m1.1.1" xref="S3.F3.3.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.F3.3.1.1.m1.1c"><ci id="S3.F3.3.1.1.m1.1.1.cmml" xref="S3.F3.3.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.1.1.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S3.F3.3.1.1.m1.1e">italic_N</annotation></semantics></math> 256-dimensional learnable embeddings (<math alttext="\mathbb{R}^{N\times 256}" class="ltx_Math" display="inline" id="S3.F3.4.2.2.m2.1"><semantics id="S3.F3.4.2.2.m2.1b"><msup id="S3.F3.4.2.2.m2.1.1" xref="S3.F3.4.2.2.m2.1.1.cmml"><mi id="S3.F3.4.2.2.m2.1.1.2" xref="S3.F3.4.2.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S3.F3.4.2.2.m2.1.1.3" xref="S3.F3.4.2.2.m2.1.1.3.cmml"><mi id="S3.F3.4.2.2.m2.1.1.3.2" xref="S3.F3.4.2.2.m2.1.1.3.2.cmml">N</mi><mo id="S3.F3.4.2.2.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.F3.4.2.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.F3.4.2.2.m2.1.1.3.3" xref="S3.F3.4.2.2.m2.1.1.3.3.cmml">256</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.F3.4.2.2.m2.1c"><apply id="S3.F3.4.2.2.m2.1.1.cmml" xref="S3.F3.4.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F3.4.2.2.m2.1.1.1.cmml" xref="S3.F3.4.2.2.m2.1.1">superscript</csymbol><ci id="S3.F3.4.2.2.m2.1.1.2.cmml" xref="S3.F3.4.2.2.m2.1.1.2">ℝ</ci><apply id="S3.F3.4.2.2.m2.1.1.3.cmml" xref="S3.F3.4.2.2.m2.1.1.3"><times id="S3.F3.4.2.2.m2.1.1.3.1.cmml" xref="S3.F3.4.2.2.m2.1.1.3.1"></times><ci id="S3.F3.4.2.2.m2.1.1.3.2.cmml" xref="S3.F3.4.2.2.m2.1.1.3.2">𝑁</ci><cn id="S3.F3.4.2.2.m2.1.1.3.3.cmml" type="integer" xref="S3.F3.4.2.2.m2.1.1.3.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.2.2.m2.1d">\mathbb{R}^{N\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.4.2.2.m2.1e">blackboard_R start_POSTSUPERSCRIPT italic_N × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>). All the predicted (learnable) prompts are then fed to SAM’s mask decoder to generate a mask. The prompt predictor network can handle multiple classes by providing distinct class queries.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.7">Given an image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">italic_I</annotation></semantics></math> of size <math alttext="\mathbb{R}^{3\times H\times W}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><msup id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mn id="S3.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">3</mn><mo id="S3.SS1.SSS1.p1.2.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">×</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml">H</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">×</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.4" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.4.cmml">W</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">ℝ</ci><apply id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3"><times id="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1"></times><cn id="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2">3</cn><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3">𝐻</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.4">𝑊</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">\mathbb{R}^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.1d">blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>, the SAM image encoder provides embeddings <math alttext="E\in\mathbb{R}^{C\times H^{\prime}\times W^{\prime}}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.3.m3.1"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mrow id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml">E</mi><mo id="S3.SS1.SSS1.p1.3.m3.1.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p1.3.m3.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.2.cmml">C</mi><mo id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><msup id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.2.cmml">H</mi><mo id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.3.cmml">′</mo></msup><mo id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><msup id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.2.cmml">W</mi><mo id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.3.cmml">′</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"><in id="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1"></in><ci id="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2">𝐸</ci><apply id="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3"><times id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.2">𝐶</ci><apply id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3">superscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.3.3">′</ci></apply><apply id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4">superscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.3.4.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">E\in\mathbb{R}^{C\times H^{\prime}\times W^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.3.m3.1d">italic_E ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_W start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.
Each point in <math alttext="E" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.4.m4.1"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mi id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><ci id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.4.m4.1d">italic_E</annotation></semantics></math> corresponds to a patch in <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.5.m5.1"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><mi id="S3.SS1.SSS1.p1.5.m5.1.1" xref="S3.SS1.SSS1.p1.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m5.1b"><ci id="S3.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.5.m5.1d">italic_I</annotation></semantics></math>. PerSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib41" title="">41</a>]</cite> and Matcher <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib42" title="">42</a>]</cite> use this feature map to compute the cosine similarity between <math alttext="E" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.6.m6.1"><semantics id="S3.SS1.SSS1.p1.6.m6.1a"><mi id="S3.SS1.SSS1.p1.6.m6.1.1" xref="S3.SS1.SSS1.p1.6.m6.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.6.m6.1b"><ci id="S3.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.6.m6.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.6.m6.1d">italic_E</annotation></semantics></math> and a reference image’s embeddings <math alttext="E_{r}\in\mathbb{R}^{C\times H^{\prime}\times W^{\prime}}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.7.m7.1"><semantics id="S3.SS1.SSS1.p1.7.m7.1a"><mrow id="S3.SS1.SSS1.p1.7.m7.1.1" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml"><msub id="S3.SS1.SSS1.p1.7.m7.1.1.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.1.2.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.2.cmml">E</mi><mi id="S3.SS1.SSS1.p1.7.m7.1.1.2.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.3.cmml">r</mi></msub><mo id="S3.SS1.SSS1.p1.7.m7.1.1.1" xref="S3.SS1.SSS1.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p1.7.m7.1.1.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.1.3.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p1.7.m7.1.1.3.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.2.cmml">C</mi><mo id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.1.cmml">×</mo><msup id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.2.cmml">H</mi><mo id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.3.cmml">′</mo></msup><mo id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.1.cmml">×</mo><msup id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.cmml"><mi id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.2" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.2.cmml">W</mi><mo id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.3" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.3.cmml">′</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.7.m7.1b"><apply id="S3.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1"><in id="S3.SS1.SSS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.1"></in><apply id="S3.SS1.SSS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.2">𝐸</ci><ci id="S3.SS1.SSS1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.2.3">𝑟</ci></apply><apply id="S3.SS1.SSS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3"><times id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.1"></times><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.2">𝐶</ci><apply id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3">superscript</csymbol><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.3.3">′</ci></apply><apply id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4">superscript</csymbol><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.2.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.3.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1.3.3.4.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.7.m7.1c">E_{r}\in\mathbb{R}^{C\times H^{\prime}\times W^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.7.m7.1d">italic_E start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_W start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.5">As mentioned in the introduction<sup class="ltx_sup" id="S3.SS1.SSS1.p2.5.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S1" title="1 Introduction ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">1</span></a></sup>, US images often exhibit significant similarities between foreground and background patches, resulting in a high cosine similarity between unrelated patches. So, instead of using the cosine similarity, we propose a lightweight attention mechanism to learn a non-linear function to predict the location of the target regions in <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.1.m1.1"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.1d">italic_I</annotation></semantics></math>, given the patch embeddings of <math alttext="E" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.2.m2.1"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.1d">italic_E</annotation></semantics></math>. To this end, we represent the learnable positional embeddings (tokens) as <math alttext="T\in\mathbb{R}^{N\times C}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.3.m3.1"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mrow id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.1.1.2" xref="S3.SS1.SSS1.p2.3.m3.1.1.2.cmml">T</mi><mo id="S3.SS1.SSS1.p2.3.m3.1.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p2.3.m3.1.1.3" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.1.1.3.2" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p2.3.m3.1.1.3.3" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.2" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.2.cmml">N</mi><mo id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.3" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><apply id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1"><in id="S3.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.1"></in><ci id="S3.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.2">𝑇</ci><apply id="S3.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3"><times id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.2">𝑁</ci><ci id="S3.SS1.SSS1.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.3.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">T\in\mathbb{R}^{N\times C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.3.m3.1d">italic_T ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.4.m4.1"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mi id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><ci id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.4.m4.1d">italic_N</annotation></semantics></math> is the number of learnable tokens, and <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.5.m5.1"><semantics id="S3.SS1.SSS1.p2.5.m5.1a"><mi id="S3.SS1.SSS1.p2.5.m5.1.1" xref="S3.SS1.SSS1.p2.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.5.m5.1b"><ci id="S3.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.5.m5.1d">italic_C</annotation></semantics></math> is the number of channels.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.4">Since the target regions may appear at arbitrary locations in the image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.1.m1.1"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><mi id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.1b"><ci id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.1.m1.1d">italic_I</annotation></semantics></math>, the learnable position embeddings (<math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.2.m2.1"><semantics id="S3.SS1.SSS1.p3.2.m2.1a"><mi id="S3.SS1.SSS1.p3.2.m2.1.1" xref="S3.SS1.SSS1.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.2.m2.1b"><ci id="S3.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.2.m2.1d">italic_T</annotation></semantics></math>) need to be dynamically updated to incorporate information about the object’s location. First, we add positional encodings to the patch embeddings in <math alttext="E" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.3.m3.1"><semantics id="S3.SS1.SSS1.p3.3.m3.1a"><mi id="S3.SS1.SSS1.p3.3.m3.1.1" xref="S3.SS1.SSS1.p3.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.3.m3.1b"><ci id="S3.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.3.m3.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.3.m3.1d">italic_E</annotation></semantics></math> as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.F3" title="Figure 3 ‣ 3.1.1 Prompt predictor network (prompt learning) ‣ 3.1 Sam2Rad ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">3</span></a>. The updated image embeddings and learnable tokens are then passed to a cross-attention module, where the learnable tokens are the queries and keys/values come from the image embeddings, resulting in updated tokens <math alttext="\hat{T}\in\mathbb{R}^{N\times C}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.4.m4.1"><semantics id="S3.SS1.SSS1.p3.4.m4.1a"><mrow id="S3.SS1.SSS1.p3.4.m4.1.1" xref="S3.SS1.SSS1.p3.4.m4.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.p3.4.m4.1.1.2" xref="S3.SS1.SSS1.p3.4.m4.1.1.2.cmml"><mi id="S3.SS1.SSS1.p3.4.m4.1.1.2.2" xref="S3.SS1.SSS1.p3.4.m4.1.1.2.2.cmml">T</mi><mo id="S3.SS1.SSS1.p3.4.m4.1.1.2.1" xref="S3.SS1.SSS1.p3.4.m4.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS1.SSS1.p3.4.m4.1.1.1" xref="S3.SS1.SSS1.p3.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p3.4.m4.1.1.3" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS1.p3.4.m4.1.1.3.2" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p3.4.m4.1.1.3.3" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.2" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.2.cmml">N</mi><mo id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.3" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.4.m4.1b"><apply id="S3.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1"><in id="S3.SS1.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.1"></in><apply id="S3.SS1.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.2"><ci id="S3.SS1.SSS1.p3.4.m4.1.1.2.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.2.1">^</ci><ci id="S3.SS1.SSS1.p3.4.m4.1.1.2.2.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.2.2">𝑇</ci></apply><apply id="S3.SS1.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3"><times id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.2">𝑁</ci><ci id="S3.SS1.SSS1.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.3.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.4.m4.1c">\hat{T}\in\mathbb{R}^{N\times C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.4.m4.1d">over^ start_ARG italic_T end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.7">The image embeddings are also updated by attending to the updated learnable tokens, resulting in image tokens of shape <math alttext="\hat{E}\in\mathbb{R}^{4096\times 256}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.1.m1.1"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mrow id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.p4.1.m1.1.1.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.2.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.2.cmml">E</mi><mo id="S3.SS1.SSS1.p4.1.m1.1.1.2.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS1.SSS1.p4.1.m1.1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p4.1.m1.1.1.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml"><mn id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.cmml">4096</mn><mo id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.cmml">256</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><apply id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1"><in id="S3.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.1"></in><apply id="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2"><ci id="S3.SS1.SSS1.p4.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.1">^</ci><ci id="S3.SS1.SSS1.p4.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.2">𝐸</ci></apply><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3"><times id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.1"></times><cn id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.2">4096</cn><cn id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.3">256</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">\hat{E}\in\mathbb{R}^{4096\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.1.m1.1d">over^ start_ARG italic_E end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT 4096 × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>. To align the learned prompts with the prompts that SAM was trained on (box, mask, and point), <math alttext="\hat{T}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.2.m2.1"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mover accent="true" id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p4.2.m2.1.1.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.cmml">T</mi><mo id="S3.SS1.SSS1.p4.2.m2.1.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><apply id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1"><ci id="S3.SS1.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.1">^</ci><ci id="S3.SS1.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.2">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">\hat{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.2.m2.1d">over^ start_ARG italic_T end_ARG</annotation></semantics></math> is split into two parts: the first two tokens (<math alttext="\mathbb{R}^{2\times 256}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.3.m3.1"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><msup id="S3.SS1.SSS1.p4.3.m3.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p4.3.m3.1.1.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.3.m3.1.1.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.cmml"><mn id="S3.SS1.SSS1.p4.3.m3.1.1.3.2" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.2.cmml">2</mn><mo id="S3.SS1.SSS1.p4.3.m3.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS1.p4.3.m3.1.1.3.3" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3.cmml">256</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.3.m3.1b"><apply id="S3.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3"><times id="S3.SS1.SSS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.1"></times><cn id="S3.SS1.SSS1.p4.3.m3.1.1.3.2.cmml" type="integer" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.2">2</cn><cn id="S3.SS1.SSS1.p4.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.3.m3.1.1.3.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.3.m3.1c">\mathbb{R}^{2\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.3.m3.1d">blackboard_R start_POSTSUPERSCRIPT 2 × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>) are used to predict bounding box coordinates, and the remaining <math alttext="(N-2)" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.4.m4.1"><semantics id="S3.SS1.SSS1.p4.4.m4.1a"><mrow id="S3.SS1.SSS1.p4.4.m4.1.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.cmml"><mo id="S3.SS1.SSS1.p4.4.m4.1.1.1.2" stretchy="false" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p4.4.m4.1.1.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.2.cmml">N</mi><mo id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.3" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.3.cmml">2</mn></mrow><mo id="S3.SS1.SSS1.p4.4.m4.1.1.1.3" stretchy="false" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.4.m4.1b"><apply id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.1"><minus id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.1"></minus><ci id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.2">𝑁</ci><cn id="S3.SS1.SSS1.p4.4.m4.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.4.m4.1c">(N-2)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.4.m4.1d">( italic_N - 2 )</annotation></semantics></math> tokens are used as high dimensional prompts. The updated image tokens are used to generate a mask prompt of shape <math alttext="\mathbb{R}^{1\times 256\times 256}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.5.m5.1"><semantics id="S3.SS1.SSS1.p4.5.m5.1a"><msup id="S3.SS1.SSS1.p4.5.m5.1.1" xref="S3.SS1.SSS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.SSS1.p4.5.m5.1.1.2" xref="S3.SS1.SSS1.p4.5.m5.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.5.m5.1.1.3" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.cmml"><mn id="S3.SS1.SSS1.p4.5.m5.1.1.3.2" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.2.cmml">1</mn><mo id="S3.SS1.SSS1.p4.5.m5.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS1.p4.5.m5.1.1.3.3" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.3.cmml">256</mn><mo id="S3.SS1.SSS1.p4.5.m5.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS1.p4.5.m5.1.1.3.4" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.4.cmml">256</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.5.m5.1b"><apply id="S3.SS1.SSS1.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3"><times id="S3.SS1.SSS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.1"></times><cn id="S3.SS1.SSS1.p4.5.m5.1.1.3.2.cmml" type="integer" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.2">1</cn><cn id="S3.SS1.SSS1.p4.5.m5.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.3">256</cn><cn id="S3.SS1.SSS1.p4.5.m5.1.1.3.4.cmml" type="integer" xref="S3.SS1.SSS1.p4.5.m5.1.1.3.4">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.5.m5.1c">\mathbb{R}^{1\times 256\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.5.m5.1d">blackboard_R start_POSTSUPERSCRIPT 1 × 256 × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>. The predicted bounding box, mask and <math alttext="(N-2)" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.6.m6.1"><semantics id="S3.SS1.SSS1.p4.6.m6.1a"><mrow id="S3.SS1.SSS1.p4.6.m6.1.1.1" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.cmml"><mo id="S3.SS1.SSS1.p4.6.m6.1.1.1.2" stretchy="false" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p4.6.m6.1.1.1.1" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.2" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.2.cmml">N</mi><mo id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.1" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.3" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.3.cmml">2</mn></mrow><mo id="S3.SS1.SSS1.p4.6.m6.1.1.1.3" stretchy="false" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.6.m6.1b"><apply id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1.1"><minus id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.1"></minus><ci id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.2">𝑁</ci><cn id="S3.SS1.SSS1.p4.6.m6.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.6.m6.1.1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.6.m6.1c">(N-2)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.6.m6.1d">( italic_N - 2 )</annotation></semantics></math> 256-dimensional prompts are then fed to SAM’s mask decoder to generate the final mask. The prompt predictor network can handle multiple classes by providing distinct class queries. <math alttext="\hat{T}\in\mathbb{R}^{(N-2)\times 256}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.7.m7.1"><semantics id="S3.SS1.SSS1.p4.7.m7.1a"><mrow id="S3.SS1.SSS1.p4.7.m7.1.2" xref="S3.SS1.SSS1.p4.7.m7.1.2.cmml"><mover accent="true" id="S3.SS1.SSS1.p4.7.m7.1.2.2" xref="S3.SS1.SSS1.p4.7.m7.1.2.2.cmml"><mi id="S3.SS1.SSS1.p4.7.m7.1.2.2.2" xref="S3.SS1.SSS1.p4.7.m7.1.2.2.2.cmml">T</mi><mo id="S3.SS1.SSS1.p4.7.m7.1.2.2.1" xref="S3.SS1.SSS1.p4.7.m7.1.2.2.1.cmml">^</mo></mover><mo id="S3.SS1.SSS1.p4.7.m7.1.2.1" xref="S3.SS1.SSS1.p4.7.m7.1.2.1.cmml">∈</mo><msup id="S3.SS1.SSS1.p4.7.m7.1.2.3" xref="S3.SS1.SSS1.p4.7.m7.1.2.3.cmml"><mi id="S3.SS1.SSS1.p4.7.m7.1.2.3.2" xref="S3.SS1.SSS1.p4.7.m7.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.7.m7.1.1.1" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.cmml"><mrow id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.cmml"><mo id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.2" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.2.cmml">N</mi><mo id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.1" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.3" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.SSS1.p4.7.m7.1.1.1.2" rspace="0.222em" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.2.cmml">×</mo><mn id="S3.SS1.SSS1.p4.7.m7.1.1.1.3" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.3.cmml">256</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.7.m7.1b"><apply id="S3.SS1.SSS1.p4.7.m7.1.2.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2"><in id="S3.SS1.SSS1.p4.7.m7.1.2.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.1"></in><apply id="S3.SS1.SSS1.p4.7.m7.1.2.2.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.2"><ci id="S3.SS1.SSS1.p4.7.m7.1.2.2.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.2.1">^</ci><ci id="S3.SS1.SSS1.p4.7.m7.1.2.2.2.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.2.2">𝑇</ci></apply><apply id="S3.SS1.SSS1.p4.7.m7.1.2.3.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.7.m7.1.2.3.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.3">superscript</csymbol><ci id="S3.SS1.SSS1.p4.7.m7.1.2.3.2.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.2.3.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.7.m7.1.1.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1.1"><times id="S3.SS1.SSS1.p4.7.m7.1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.2"></times><apply id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1"><minus id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.1"></minus><ci id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.2">𝑁</ci><cn id="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.1.1.1.3">2</cn></apply><cn id="S3.SS1.SSS1.p4.7.m7.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p4.7.m7.1.1.1.3">256</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.7.m7.1c">\hat{T}\in\mathbb{R}^{(N-2)\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.7.m7.1d">over^ start_ARG italic_T end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT ( italic_N - 2 ) × 256 end_POSTSUPERSCRIPT</annotation></semantics></math> may represent any combination of prompts suitable for the task at hand; for example, some shapes are easier to segment with boxes while others are easier with several-point prompts. The high-dimensional prompts will decide whether to use point/box prompts or semantic information based on the properties of the dataset.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Losses</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In some cases, the queried object may not be present in the image, such as in background images or when a specific class is missing in multi-class segmentation. To address this, we introduce a fully connected object prediction head and an output token within the mask decoder. During inference, if the object prediction head outputs a value less than 0, we disregard the segmentation output.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.5">Given an image <math alttext="I\in\mathbb{R}^{3\times H\times W}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mn id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S3.SS2.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml">H</mi><mo id="S3.SS2.p2.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.4" xref="S3.SS2.p2.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><in id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></in><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐼</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.1"></times><cn id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.3.3.2">3</cn><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3">𝐻</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">I\in\mathbb{R}^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_I ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>, a binary ground-truth mask <math alttext="y\in\mathbb{R}^{H\times W}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.2.cmml">H</mi><mo id="S3.SS2.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><in id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></in><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑦</ci><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3"><times id="S3.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.2">𝐻</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">y\in\mathbb{R}^{H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>, we obtain bounding box coordinates <math alttext="\mathbb{R}^{4}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><msup id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">ℝ</mi><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ℝ</ci><cn id="S3.SS2.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathbb{R}^{4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">blackboard_R start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, mask prompts <math alttext="\mathbb{R}^{1\times 256\times 256}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msup id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mn id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">1</mn><mo id="S3.SS2.p2.4.m4.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.4.m4.1.1.3.1.cmml">×</mo><mn id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml">256</mn><mo id="S3.SS2.p2.4.m4.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.4.m4.1.1.3.1.cmml">×</mo><mn id="S3.SS2.p2.4.m4.1.1.3.4" xref="S3.SS2.p2.4.m4.1.1.3.4.cmml">256</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ℝ</ci><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><times id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.1"></times><cn id="S3.SS2.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.3.2">1</cn><cn id="S3.SS2.p2.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.3.3">256</cn><cn id="S3.SS2.p2.4.m4.1.1.3.4.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.3.4">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathbb{R}^{1\times 256\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">blackboard_R start_POSTSUPERSCRIPT 1 × 256 × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>, and a predicted mask <math alttext="\hat{y}\in\mathbb{R}^{H\times W}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mover accent="true" id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">y</mi><mo id="S3.SS2.p2.5.m5.1.1.2.1" xref="S3.SS2.p2.5.m5.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.3.2" xref="S3.SS2.p2.5.m5.1.1.3.3.2.cmml">H</mi><mo id="S3.SS2.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.3.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><in id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></in><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><ci id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2.1">^</ci><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">𝑦</ci></apply><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3"><times id="S3.SS2.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.1"></times><ci id="S3.SS2.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2">𝐻</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\hat{y}\in\mathbb{R}^{H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">over^ start_ARG italic_y end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math> (after sigmoid activation), the training loss function is formulated as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathcal{L}_{\text{Total}}=\lambda_{1}\mathcal{L}_{\text{Mask}}+\lambda_{2}%
\mathcal{L}_{\text{Box}}+\lambda_{3}\mathcal{L}_{\text{Objectness}}," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">ℒ</mi><mtext id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3a.cmml">Total</mtext></msub><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2.cmml">λ</mi><mn id="S3.E1.m1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.3.2.cmml">ℒ</mi><mtext id="S3.E1.m1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.3.3a.cmml">Mask</mtext></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2.cmml">λ</mi><mn id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3a.cmml">Box</mtext></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.3.4.cmml"><msub id="S3.E1.m1.1.1.1.1.3.4.2" xref="S3.E1.m1.1.1.1.1.3.4.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.4.2.2" xref="S3.E1.m1.1.1.1.1.3.4.2.2.cmml">λ</mi><mn id="S3.E1.m1.1.1.1.1.3.4.2.3" xref="S3.E1.m1.1.1.1.1.3.4.2.3.cmml">3</mn></msub><mo id="S3.E1.m1.1.1.1.1.3.4.1" xref="S3.E1.m1.1.1.1.1.3.4.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.3.4.3" xref="S3.E1.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.4.3.2" xref="S3.E1.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mtext id="S3.E1.m1.1.1.1.1.3.4.3.3" xref="S3.E1.m1.1.1.1.1.3.4.3.3a.cmml">Objectness</mtext></msub></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">ℒ</ci><ci id="S3.E1.m1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><mtext id="S3.E1.m1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.2.3">Total</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><times id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1"></times><apply id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2">𝜆</ci><cn id="S3.E1.m1.1.1.1.1.3.2.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.2.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2">ℒ</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.2.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.2.3.3">Mask</mtext></ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2">𝜆</ci><cn id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.3.2.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2">ℒ</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.3.3.3">Box</mtext></ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.4"><times id="S3.E1.m1.1.1.1.1.3.4.1.cmml" xref="S3.E1.m1.1.1.1.1.3.4.1"></times><apply id="S3.E1.m1.1.1.1.1.3.4.2.cmml" xref="S3.E1.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.4.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.4.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.4.2.2">𝜆</ci><cn id="S3.E1.m1.1.1.1.1.3.4.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.4.2.3">3</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.4.3.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.4.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3.2">ℒ</ci><ci id="S3.E1.m1.1.1.1.1.3.4.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.4.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.4.3.3">Objectness</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}_{\text{Total}}=\lambda_{1}\mathcal{L}_{\text{Mask}}+\lambda_{2}%
\mathcal{L}_{\text{Box}}+\lambda_{3}\mathcal{L}_{\text{Objectness}},</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_L start_POSTSUBSCRIPT Total end_POSTSUBSCRIPT = italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT Mask end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT Box end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT Objectness end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.1">where
<math alttext="\mathcal{L}_{\text{mask}}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3a.cmml">mask</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ℒ</ci><ci id="S3.SS2.p3.1.m1.1.1.3a.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><mtext id="S3.SS2.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p3.1.m1.1.1.3">mask</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathcal{L}_{\text{mask}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT</annotation></semantics></math> is the sum of focal loss <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib47" title="">47</a>]</cite> for the mask prompt, and the final mask prediction, where the focal loss is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\mathcal{L}_{\text{Focal}}(\hat{y}_{i},y_{i})=-y_{i}(1-\hat{y}_{i})^{\gamma}%
\log(\hat{y}_{i})-\alpha(1-y_{i}){\hat{y}_{i}}^{\gamma}\log(1-\hat{y}_{i})," class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><msub id="S3.E2.m1.3.3.1.1.2.4" xref="S3.E2.m1.3.3.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.2.4.2" xref="S3.E2.m1.3.3.1.1.2.4.2.cmml">ℒ</mi><mtext id="S3.E2.m1.3.3.1.1.2.4.3" xref="S3.E2.m1.3.3.1.1.2.4.3a.cmml">Focal</mtext></msub><mo id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml"><mo id="S3.E2.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.1.1.2.2.2.4" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.2.cmml">y</mi><mi id="S3.E2.m1.3.3.1.1.2.2.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.7" xref="S3.E2.m1.3.3.1.1.7.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.6" xref="S3.E2.m1.3.3.1.1.6.cmml"><mrow id="S3.E2.m1.3.3.1.1.4.2" xref="S3.E2.m1.3.3.1.1.4.2.cmml"><mo id="S3.E2.m1.3.3.1.1.4.2a" xref="S3.E2.m1.3.3.1.1.4.2.cmml">−</mo><mrow id="S3.E2.m1.3.3.1.1.4.2.2" xref="S3.E2.m1.3.3.1.1.4.2.2.cmml"><msub id="S3.E2.m1.3.3.1.1.4.2.2.4" xref="S3.E2.m1.3.3.1.1.4.2.2.4.cmml"><mi id="S3.E2.m1.3.3.1.1.4.2.2.4.2" xref="S3.E2.m1.3.3.1.1.4.2.2.4.2.cmml">y</mi><mi id="S3.E2.m1.3.3.1.1.4.2.2.4.3" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.1.1.4.2.2.3" xref="S3.E2.m1.3.3.1.1.4.2.2.3.cmml">⁢</mo><msup id="S3.E2.m1.3.3.1.1.3.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.2.cmml">y</mi><mo id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.E2.m1.3.3.1.1.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.3.1.1.1.3.cmml">γ</mi></msup><mo id="S3.E2.m1.3.3.1.1.4.2.2.3a" lspace="0.167em" xref="S3.E2.m1.3.3.1.1.4.2.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.4.2.2.2.1" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">log</mi><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.1a" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.cmml">⁡</mo><mrow id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.cmml"><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.cmml">(</mo><msub id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.2.cmml">y</mi><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.1" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.3" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.6.5" xref="S3.E2.m1.3.3.1.1.6.5.cmml">−</mo><mrow id="S3.E2.m1.3.3.1.1.6.4" xref="S3.E2.m1.3.3.1.1.6.4.cmml"><mi id="S3.E2.m1.3.3.1.1.6.4.4" xref="S3.E2.m1.3.3.1.1.6.4.4.cmml">α</mi><mo id="S3.E2.m1.3.3.1.1.6.4.3" xref="S3.E2.m1.3.3.1.1.6.4.3.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.5.3.1.1" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.5.3.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.5.3.1.1.1" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.cmml"><mn id="S3.E2.m1.3.3.1.1.5.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.3.3.1.1.5.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.2.cmml">y</mi><mi id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.3.3.1.1.5.3.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.cmml">)</mo></mrow><mo id="S3.E2.m1.3.3.1.1.6.4.3a" xref="S3.E2.m1.3.3.1.1.6.4.3.cmml">⁢</mo><mmultiscripts id="S3.E2.m1.3.3.1.1.6.4.5" xref="S3.E2.m1.3.3.1.1.6.4.5.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.6.4.5.2.2" xref="S3.E2.m1.3.3.1.1.6.4.5.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.6.4.5.2.2.2" xref="S3.E2.m1.3.3.1.1.6.4.5.2.2.2.cmml">y</mi><mo id="S3.E2.m1.3.3.1.1.6.4.5.2.2.1" xref="S3.E2.m1.3.3.1.1.6.4.5.2.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.1.1.6.4.5.2.3" xref="S3.E2.m1.3.3.1.1.6.4.5.2.3.cmml">i</mi><mrow id="S3.E2.m1.3.3.1.1.6.4.5a" xref="S3.E2.m1.3.3.1.1.6.4.5.cmml"></mrow><mrow id="S3.E2.m1.3.3.1.1.6.4.5b" xref="S3.E2.m1.3.3.1.1.6.4.5.cmml"></mrow><mi id="S3.E2.m1.3.3.1.1.6.4.5.3" xref="S3.E2.m1.3.3.1.1.6.4.5.3.cmml">γ</mi></mmultiscripts><mo id="S3.E2.m1.3.3.1.1.6.4.3b" lspace="0.167em" xref="S3.E2.m1.3.3.1.1.6.4.3.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.6.4.2.1" xref="S3.E2.m1.3.3.1.1.6.4.2.2.cmml"><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">log</mi><mo id="S3.E2.m1.3.3.1.1.6.4.2.1a" xref="S3.E2.m1.3.3.1.1.6.4.2.2.cmml">⁡</mo><mrow id="S3.E2.m1.3.3.1.1.6.4.2.1.1" xref="S3.E2.m1.3.3.1.1.6.4.2.2.cmml"><mo id="S3.E2.m1.3.3.1.1.6.4.2.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.6.4.2.2.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.cmml"><mn id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.2" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.1" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.cmml"><mi id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.2.cmml">y</mi><mo id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.3.3.1.1.6.4.2.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.6.4.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.7.cmml" xref="S3.E2.m1.3.3.1.1.7"></eq><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><times id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"></times><apply id="S3.E2.m1.3.3.1.1.2.4.cmml" xref="S3.E2.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.4.1.cmml" xref="S3.E2.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.4.2.cmml" xref="S3.E2.m1.3.3.1.1.2.4.2">ℒ</ci><ci id="S3.E2.m1.3.3.1.1.2.4.3a.cmml" xref="S3.E2.m1.3.3.1.1.2.4.3"><mtext id="S3.E2.m1.3.3.1.1.2.4.3.cmml" mathsize="70%" xref="S3.E2.m1.3.3.1.1.2.4.3">Focal</mtext></ci></apply><interval closure="open" id="S3.E2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2"><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2">𝑦</ci></apply><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.2">𝑦</ci><ci id="S3.E2.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S3.E2.m1.3.3.1.1.6.cmml" xref="S3.E2.m1.3.3.1.1.6"><minus id="S3.E2.m1.3.3.1.1.6.5.cmml" xref="S3.E2.m1.3.3.1.1.6.5"></minus><apply id="S3.E2.m1.3.3.1.1.4.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2"><minus id="S3.E2.m1.3.3.1.1.4.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2"></minus><apply id="S3.E2.m1.3.3.1.1.4.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2"><times id="S3.E2.m1.3.3.1.1.4.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.3"></times><apply id="S3.E2.m1.3.3.1.1.4.2.2.4.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.2.4.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.4.2.2.4.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.2">𝑦</ci><ci id="S3.E2.m1.3.3.1.1.4.2.2.4.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1"></minus><cn id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.2">1</cn><apply id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.2.2">𝑦</ci></apply><ci id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.3">𝛾</ci></apply><apply id="S3.E2.m1.3.3.1.1.4.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1"><log id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></log><apply id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2"><ci id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.1">^</ci><ci id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.2.2">𝑦</ci></apply><ci id="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.1.1.1.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E2.m1.3.3.1.1.6.4.cmml" xref="S3.E2.m1.3.3.1.1.6.4"><times id="S3.E2.m1.3.3.1.1.6.4.3.cmml" xref="S3.E2.m1.3.3.1.1.6.4.3"></times><ci id="S3.E2.m1.3.3.1.1.6.4.4.cmml" xref="S3.E2.m1.3.3.1.1.6.4.4">𝛼</ci><apply id="S3.E2.m1.3.3.1.1.5.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.5.3.1.1"><minus id="S3.E2.m1.3.3.1.1.5.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.1"></minus><cn id="S3.E2.m1.3.3.1.1.5.3.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.2">1</cn><apply id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.2">𝑦</ci><ci id="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.5.3.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.6.4.5.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.6.4.5.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.6.4.5.2.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.6.4.5.2.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.6.4.5.2.2.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5.2.2"><ci id="S3.E2.m1.3.3.1.1.6.4.5.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5.2.2.1">^</ci><ci id="S3.E2.m1.3.3.1.1.6.4.5.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5.2.2.2">𝑦</ci></apply><ci id="S3.E2.m1.3.3.1.1.6.4.5.2.3.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5.2.3">𝑖</ci></apply><ci id="S3.E2.m1.3.3.1.1.6.4.5.3.cmml" xref="S3.E2.m1.3.3.1.1.6.4.5.3">𝛾</ci></apply><apply id="S3.E2.m1.3.3.1.1.6.4.2.2.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1"><log id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"></log><apply id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1"><minus id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.1"></minus><cn id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.2">1</cn><apply id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2"><ci id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.2.2">𝑦</ci></apply><ci id="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.6.4.2.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\mathcal{L}_{\text{Focal}}(\hat{y}_{i},y_{i})=-y_{i}(1-\hat{y}_{i})^{\gamma}%
\log(\hat{y}_{i})-\alpha(1-y_{i}){\hat{y}_{i}}^{\gamma}\log(1-\hat{y}_{i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT Focal end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT roman_log ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_α ( 1 - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT roman_log ( 1 - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.3"><math alttext="\mathcal{L}_{\text{Box}}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m1.1"><semantics id="S3.SS2.p3.2.m1.1a"><msub id="S3.SS2.p3.2.m1.1.1" xref="S3.SS2.p3.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.2.m1.1.1.2" xref="S3.SS2.p3.2.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p3.2.m1.1.1.3" xref="S3.SS2.p3.2.m1.1.1.3a.cmml">Box</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m1.1b"><apply id="S3.SS2.p3.2.m1.1.1.cmml" xref="S3.SS2.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m1.1.1.1.cmml" xref="S3.SS2.p3.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m1.1.1.2.cmml" xref="S3.SS2.p3.2.m1.1.1.2">ℒ</ci><ci id="S3.SS2.p3.2.m1.1.1.3a.cmml" xref="S3.SS2.p3.2.m1.1.1.3"><mtext id="S3.SS2.p3.2.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p3.2.m1.1.1.3">Box</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m1.1c">\mathcal{L}_{\text{Box}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m1.1d">caligraphic_L start_POSTSUBSCRIPT Box end_POSTSUBSCRIPT</annotation></semantics></math> is a linear combination of L1 loss and the generalized IoU loss <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib48" title="">48</a>]</cite> for bounding box regression, and <math alttext="\mathcal{L}_{\text{Objectness}}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m2.1"><semantics id="S3.SS2.p3.3.m2.1a"><msub id="S3.SS2.p3.3.m2.1.1" xref="S3.SS2.p3.3.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.3.m2.1.1.2" xref="S3.SS2.p3.3.m2.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p3.3.m2.1.1.3" xref="S3.SS2.p3.3.m2.1.1.3a.cmml">Objectness</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m2.1b"><apply id="S3.SS2.p3.3.m2.1.1.cmml" xref="S3.SS2.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m2.1.1.1.cmml" xref="S3.SS2.p3.3.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m2.1.1.2.cmml" xref="S3.SS2.p3.3.m2.1.1.2">ℒ</ci><ci id="S3.SS2.p3.3.m2.1.1.3a.cmml" xref="S3.SS2.p3.3.m2.1.1.3"><mtext id="S3.SS2.p3.3.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p3.3.m2.1.1.3">Objectness</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m2.1c">\mathcal{L}_{\text{Objectness}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m2.1d">caligraphic_L start_POSTSUBSCRIPT Objectness end_POSTSUBSCRIPT</annotation></semantics></math> is the binary cross-entropy loss to predict the presence of the queried object in the image.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="662" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.11.3.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F4.4.2" style="font-size:90%;">Comparison between our prompt predictor network and others.<span class="ltx_text ltx_font_medium" id="S3.F4.4.2.3"> PerSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib41" title="">41</a>]</cite> and Matcher <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib42" title="">42</a>]</cite> use learning-free cosine similarity to predict box and point coordinates by comparing image patches between a reference image and a test image </span>(a)<span class="ltx_text ltx_font_medium" id="S3.F4.4.2.2">. Given image features, our <span class="ltx_text ltx_font_italic" id="S3.F4.4.2.2.1">prompt predictor network</span> predicts prompts for the queried object. The predictor network outputs bounding box coordinates of target regions, an intermediate mask prompt, and <math alttext="N" class="ltx_Math" display="inline" id="S3.F4.3.1.1.m1.1"><semantics id="S3.F4.3.1.1.m1.1b"><mi id="S3.F4.3.1.1.m1.1.1" xref="S3.F4.3.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.F4.3.1.1.m1.1c"><ci id="S3.F4.3.1.1.m1.1.1.cmml" xref="S3.F4.3.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.3.1.1.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S3.F4.3.1.1.m1.1e">italic_N</annotation></semantics></math> high-dimensional learnable embeddings (<math alttext="\mathbb{R}^{N\times 256}" class="ltx_Math" display="inline" id="S3.F4.4.2.2.m2.1"><semantics id="S3.F4.4.2.2.m2.1b"><msup id="S3.F4.4.2.2.m2.1.1" xref="S3.F4.4.2.2.m2.1.1.cmml"><mi id="S3.F4.4.2.2.m2.1.1.2" xref="S3.F4.4.2.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S3.F4.4.2.2.m2.1.1.3" xref="S3.F4.4.2.2.m2.1.1.3.cmml"><mi id="S3.F4.4.2.2.m2.1.1.3.2" xref="S3.F4.4.2.2.m2.1.1.3.2.cmml">N</mi><mo id="S3.F4.4.2.2.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.F4.4.2.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.F4.4.2.2.m2.1.1.3.3" xref="S3.F4.4.2.2.m2.1.1.3.3.cmml">256</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.F4.4.2.2.m2.1c"><apply id="S3.F4.4.2.2.m2.1.1.cmml" xref="S3.F4.4.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F4.4.2.2.m2.1.1.1.cmml" xref="S3.F4.4.2.2.m2.1.1">superscript</csymbol><ci id="S3.F4.4.2.2.m2.1.1.2.cmml" xref="S3.F4.4.2.2.m2.1.1.2">ℝ</ci><apply id="S3.F4.4.2.2.m2.1.1.3.cmml" xref="S3.F4.4.2.2.m2.1.1.3"><times id="S3.F4.4.2.2.m2.1.1.3.1.cmml" xref="S3.F4.4.2.2.m2.1.1.3.1"></times><ci id="S3.F4.4.2.2.m2.1.1.3.2.cmml" xref="S3.F4.4.2.2.m2.1.1.3.2">𝑁</ci><cn id="S3.F4.4.2.2.m2.1.1.3.3.cmml" type="integer" xref="S3.F4.4.2.2.m2.1.1.3.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.2.2.m2.1d">\mathbb{R}^{N\times 256}</annotation><annotation encoding="application/x-llamapun" id="S3.F4.4.2.2.m2.1e">blackboard_R start_POSTSUPERSCRIPT italic_N × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>). All the predicted prompts (learnable prompts) are then fed to SAM’s mask decoder to generate a mask. The prompt predictor network can handle multiple classes by providing learnable class queries </span>(c)<span class="ltx_text ltx_font_medium" id="S3.F4.4.2.4">. In contrast, <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib50" title="">50</a>]</cite> predict only bounding boxes or intermediate masks, while GroundedSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib39" title="">39</a>]</cite> and AutoSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib51" title="">51</a>]</cite> predict prompts using a standalone image encoder </span>(b)<span class="ltx_text ltx_font_medium" id="S3.F4.4.2.5">.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The datasets were curated from the US scans of different body parts including wrist, elbow, hip, and shoulder (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.T1" title="Table 1 ‣ 4.1 Datasets ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">1</span></a>) obtained with institutional ethics board approval. Data was presented to the networks in anonymized form. After splitting the subjects into training/test sets based on subject study ID, we extracted 2D frames from each volume and filtered out frames with no foreground objects.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Number of images in the train/test sets.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.4.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.2"># of training images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.3"># of test images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.4"># of subjects</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.4.2.1.1">Hip</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.2.1.2">4987</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.2.1.3">4849</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.2.1.4">50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.4.3.2.1">Wrist</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.2">3738</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.3">3822</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.4">101</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.4.4.3.1">Shoulder (Bursa)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.2">7364</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.3">1605</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.4">206</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Augmentation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">Overparametrized networks come with the risk of overfitting, especially when working with limited training data. To enhance the model’s generalization and prevent overfitting, we apply strong augmentations: random horizontal/vertical flip, random 20% horizontal/vertical translations, a random rotation in the range [-90, 90] degrees, and a random resized crop in the range of <math alttext="[0.8,1.0]" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.2"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.3.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml"><mo id="S4.SS2.p1.1.m1.2.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">0.8</mn><mo id="S4.SS2.p1.1.m1.2.3.2.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">1.0</mn><mo id="S4.SS2.p1.1.m1.2.3.2.3" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><interval closure="closed" id="S4.SS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.2"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="float" xref="S4.SS2.p1.1.m1.1.1">0.8</cn><cn id="S4.SS2.p1.1.m1.2.2.cmml" type="float" xref="S4.SS2.p1.1.m1.2.2">1.0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">[0.8,1.0]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.2d">[ 0.8 , 1.0 ]</annotation></semantics></math>. Each augmentation is applied with a probability of <math alttext="0.5" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn id="S4.SS2.p1.2.m2.1.1.cmml" type="float" xref="S4.SS2.p1.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">0.5</annotation></semantics></math>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.F5" title="Figure 5 ‣ 4.2 Data Augmentation ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates an example of such augmentation and demonstrates the model’s robustness to these augmentations.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="273" id="S4.F5.g1" src="extracted/5846389/images/data_augmentation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">The model is robust to strong augmentations. For details on the data augmentation techniques applied during training, see section <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.SS2" title="4.2 Data Augmentation ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.5">The code is implemented using PyTorch <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib52" title="">52</a>]</cite>. We used the AdamW optimizer<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib53" title="">53</a>]</cite> with a weight decay of 0.1, batch size of 4, learning rate of <math alttext="1e-4" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mrow id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml">1</mn><mo id="S4.SS3.p1.1.m1.1.1.2.1" xref="S4.SS3.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.SS3.p1.1.m1.1.1.2.3" xref="S4.SS3.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><minus id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></minus><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><times id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2.1"></times><cn id="S4.SS3.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.2.2">1</cn><ci id="S4.SS3.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn id="S4.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">1 italic_e - 4</annotation></semantics></math> for the prompt predictor network, and <math alttext="1e-5" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mrow id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2.2" xref="S4.SS3.p1.2.m2.1.1.2.2.cmml">1</mn><mo id="S4.SS3.p1.2.m2.1.1.2.1" xref="S4.SS3.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS3.p1.2.m2.1.1.2.3" xref="S4.SS3.p1.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><minus id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></minus><apply id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><times id="S4.SS3.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.p1.2.m2.1.1.2.1"></times><cn id="S4.SS3.p1.2.m2.1.1.2.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.2.2">1</cn><ci id="S4.SS3.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.p1.2.m2.1.1.2.3">𝑒</ci></apply><cn id="S4.SS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">1e-5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">1 italic_e - 5</annotation></semantics></math> for the mask decoder parameters.
<math alttext="\lambda_{1},\lambda_{2},\lambda_{3}" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.3"><semantics id="S4.SS3.p1.3.m3.3a"><mrow id="S4.SS3.p1.3.m3.3.3.3" xref="S4.SS3.p1.3.m3.3.3.4.cmml"><msub id="S4.SS3.p1.3.m3.1.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.1.cmml"><mi id="S4.SS3.p1.3.m3.1.1.1.1.2" xref="S4.SS3.p1.3.m3.1.1.1.1.2.cmml">λ</mi><mn id="S4.SS3.p1.3.m3.1.1.1.1.3" xref="S4.SS3.p1.3.m3.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS3.p1.3.m3.3.3.3.4" xref="S4.SS3.p1.3.m3.3.3.4.cmml">,</mo><msub id="S4.SS3.p1.3.m3.2.2.2.2" xref="S4.SS3.p1.3.m3.2.2.2.2.cmml"><mi id="S4.SS3.p1.3.m3.2.2.2.2.2" xref="S4.SS3.p1.3.m3.2.2.2.2.2.cmml">λ</mi><mn id="S4.SS3.p1.3.m3.2.2.2.2.3" xref="S4.SS3.p1.3.m3.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS3.p1.3.m3.3.3.3.5" xref="S4.SS3.p1.3.m3.3.3.4.cmml">,</mo><msub id="S4.SS3.p1.3.m3.3.3.3.3" xref="S4.SS3.p1.3.m3.3.3.3.3.cmml"><mi id="S4.SS3.p1.3.m3.3.3.3.3.2" xref="S4.SS3.p1.3.m3.3.3.3.3.2.cmml">λ</mi><mn id="S4.SS3.p1.3.m3.3.3.3.3.3" xref="S4.SS3.p1.3.m3.3.3.3.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.3b"><list id="S4.SS3.p1.3.m3.3.3.4.cmml" xref="S4.SS3.p1.3.m3.3.3.3"><apply id="S4.SS3.p1.3.m3.1.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.2">𝜆</ci><cn id="S4.SS3.p1.3.m3.1.1.1.1.3.cmml" type="integer" xref="S4.SS3.p1.3.m3.1.1.1.1.3">1</cn></apply><apply id="S4.SS3.p1.3.m3.2.2.2.2.cmml" xref="S4.SS3.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.2.2.2.2.1.cmml" xref="S4.SS3.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S4.SS3.p1.3.m3.2.2.2.2.2.cmml" xref="S4.SS3.p1.3.m3.2.2.2.2.2">𝜆</ci><cn id="S4.SS3.p1.3.m3.2.2.2.2.3.cmml" type="integer" xref="S4.SS3.p1.3.m3.2.2.2.2.3">2</cn></apply><apply id="S4.SS3.p1.3.m3.3.3.3.3.cmml" xref="S4.SS3.p1.3.m3.3.3.3.3"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.3.3.3.3.1.cmml" xref="S4.SS3.p1.3.m3.3.3.3.3">subscript</csymbol><ci id="S4.SS3.p1.3.m3.3.3.3.3.2.cmml" xref="S4.SS3.p1.3.m3.3.3.3.3.2">𝜆</ci><cn id="S4.SS3.p1.3.m3.3.3.3.3.3.cmml" type="integer" xref="S4.SS3.p1.3.m3.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.3c">\lambda_{1},\lambda_{2},\lambda_{3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.3d">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math> in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.E1" title="In 3.2 Training Losses ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">1</span></a>) are set to 10, 1, 1, respectively. The focal loss has parameters <math alttext="\gamma=3" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.1"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mi id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">γ</mi><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><eq id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1"></eq><ci id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">𝛾</ci><cn id="S4.SS3.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">\gamma=3</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.1d">italic_γ = 3</annotation></semantics></math> and <math alttext="\alpha=0.7" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mi id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">α</mi><mo id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><eq id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1"></eq><ci id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">𝛼</ci><cn id="S4.SS3.p1.5.m5.1.1.3.cmml" type="float" xref="S4.SS3.p1.5.m5.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">\alpha=0.7</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">italic_α = 0.7</annotation></semantics></math> in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S3.E2" title="In 3.2 Training Losses ‣ 3 Methodology ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">2</span></a>). To maintain the aspect ratio, we resized the longer side of each image to 1024 pixels and padded the shorter side with zeros.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Models’ performance was evaluated using Intersection over Union (IoU) and the Dice similarity coefficient on a held-out test set. <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">Table</span> <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.T1" title="Table 1 ‣ 4.1 Datasets ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of test images for each dataset used in this study.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:390.3pt;height:143.9pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.9pt,9.5pt) scale(0.882651306962547,0.882651306962547) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.2">Hip</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.3">Wrist</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.4">Shoulder</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.1.2.1.1">MobileSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib54" title="">54</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.2">68.2/54.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.3">50.8/35.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.2.1.4"><span class="ltx_text" id="S4.T2.2.1.2.1.4.1" style="color:#FF0000;">18.1/10.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.1.3.2.1">SAM ViTB</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.2">76.7/62.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.3">59.3/43.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.3.2.4"><span class="ltx_text" id="S4.T2.2.1.3.2.4.1" style="color:#FF0000;">22.2/12.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.1.4.3.1">MedSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib14" title="">14</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.3.2">14.5/8.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.3.3">20.5/11.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.3.4"><span class="ltx_text" id="S4.T2.2.1.4.3.4.1" style="color:#FF0000;">13.1/7.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.1.5.4.1">SAM ViTH</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.4.2">75.4/62.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.4.3">54.7/39.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.4.4"><span class="ltx_text" id="S4.T2.2.1.5.4.4.1" style="color:#FF0000;">18.5/10.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.6.5" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.1.6.5.1"><span class="ltx_text" id="S4.T2.2.1.6.5.1.1" style="background-color:#DFDFDF;">SAM2 (Hiera-Tiny)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.6.5.2"><span class="ltx_text" id="S4.T2.2.1.6.5.2.1" style="background-color:#DFDFDF;">75.1/61.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.6.5.3"><span class="ltx_text" id="S4.T2.2.1.6.5.3.1" style="background-color:#DFDFDF;">49.9/35.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.6.5.4"><span class="ltx_text" id="S4.T2.2.1.6.5.4.1" style="color:#FF0000;background-color:#DFDFDF;">15.3/8.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.7.6" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.1.7.6.1"><span class="ltx_text" id="S4.T2.2.1.7.6.1.1" style="background-color:#DFDFDF;">SAM2 (Hiera-Small)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.6.2"><span class="ltx_text" id="S4.T2.2.1.7.6.2.1" style="background-color:#DFDFDF;">74.7/60.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.6.3"><span class="ltx_text" id="S4.T2.2.1.7.6.3.1" style="background-color:#DFDFDF;">45.6/31.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.6.4"><span class="ltx_text" id="S4.T2.2.1.7.6.4.1" style="color:#FF0000;background-color:#DFDFDF;">13.0/7.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.8.7" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.1.8.7.1"><span class="ltx_text" id="S4.T2.2.1.8.7.1.1" style="background-color:#DFDFDF;">SAM2 (Hiera-Large)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.7.2"><span class="ltx_text" id="S4.T2.2.1.8.7.2.1" style="background-color:#DFDFDF;">75.2/61.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.7.3"><span class="ltx_text" id="S4.T2.2.1.8.7.3.1" style="background-color:#DFDFDF;">59.5/44.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.7.4"><span class="ltx_text" id="S4.T2.2.1.8.7.4.1" style="color:#FF0000;background-color:#DFDFDF;">25.2/15.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.9.8" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.2.1.9.8.1"><span class="ltx_text" id="S4.T2.2.1.9.8.1.1" style="background-color:#DFDFDF;">SAM2 (Hiera-Base+)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.9.8.2"><span class="ltx_text" id="S4.T2.2.1.9.8.2.1" style="background-color:#DFDFDF;">78.2/65.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.9.8.3"><span class="ltx_text" id="S4.T2.2.1.9.8.3.1" style="background-color:#DFDFDF;">63.5/47.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.9.8.4"><span class="ltx_text" id="S4.T2.2.1.9.8.4.1" style="color:#FF0000;background-color:#DFDFDF;">25.3/15.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.4.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.5.2" style="font-size:90%;">Zero-shot<span class="ltx_text ltx_font_medium" id="S4.T2.5.2.1"> Dice/IoU scores of SAM variants on three US datasets. </span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:390.3pt;height:132.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(35.6pt,-12.0pt) scale(1.22294242279199,1.22294242279199) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.2.1.1.1.2">Bounding Box</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.2.1.1.1.3" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.1.1.3.1" style="background-color:#DFDFDF;">Learned Prompts</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.2.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.2.2.2">Hip</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.2.2.3">Wrist</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.2.2.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.2.2.4.1" style="background-color:#DFDFDF;">Hip</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.2.2.5" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.2.2.5.1" style="background-color:#DFDFDF;">Wrist</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.1.3.1.1">SAM2 (Hiera-Tiny)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.1.2">75.1/61.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.1.3">49.9/35.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.1.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.3.1.4.1" style="background-color:#DFDFDF;">81.2/69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.3.1.5" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.3.1.5.1" style="background-color:#DFDFDF;">82.7/71.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.1.4.2.1">SAM2 (Hiera-Small)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4.2.2">74.7/60.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4.2.3">45.6/31.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4.2.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.4.2.4.1" style="background-color:#DFDFDF;">80.5/68.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4.2.5" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.4.2.5.1" style="background-color:#DFDFDF;">83.1/71.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.1.5.3.1">SAM2 (Hiera-Large)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.5.3.2">75.2/61.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.5.3.3">59.5/44.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.5.3.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.5.3.4.1" style="background-color:#DFDFDF;">81.6/70.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.5.3.5" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.5.3.5.1" style="background-color:#DFDFDF;">83.1/71.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.2.1.6.4.1">SAM2 (Hiera-Base+)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.6.4.2">78.2/65.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.6.4.3">63.5/47.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.6.4.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.6.4.4.1" style="background-color:#DFDFDF;">80.4/68.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.6.4.5" style="background-color:#DFDFDF;"><span class="ltx_text" id="S4.T3.2.1.6.4.5.1" style="background-color:#DFDFDF;">83.1/71.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.5.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.6.2" style="font-size:90%;">Comparison of bounding box and learned prompts on hip and wrist datasets using a <span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1">frozen SAM2</span>. The results demonstrate that learned prompts perform better than bounding box prompts without fine-tuning any SAM2 parameters. <span class="ltx_text ltx_font_italic" id="S4.T3.6.2.2">Our results are highlighted in gray. Scores are presented as Dice/IoU</span>.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.2" style="width:433.6pt;height:143.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.6pt,0.9pt) scale(0.988180386529847,0.988180386529847) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T4.2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.2.1.1.1.2">Hip</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.2.1.1.1.3">Wrist</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.2.1.1.1.4">Shoulder (bursa)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.2.2.1">Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.2">LP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.3">BB+LP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.4">LP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.5">BB+LP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.6">LP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.2.7">BB+LP</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.2.1.3.3.1">MobileSAM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.2">78.4/65.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.3">81.4/69.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.4">79.8/67.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.5">81.4/69.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.6">73.7/60.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.3.7">75.0/62.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.4.4.1">SAM-ViTB</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.4.2">81.9/70.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.4.3">84.6/73.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.4.4">82.6/71.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.4.5">83.9/72.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.4.6">75.4/62.9</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.4.7">76.0/63.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.5.5" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.2.1.5.5.1"><span class="ltx_text" id="S4.T4.2.1.5.5.1.1" style="background-color:#DFDFDF;">SAM2Rad (Hiera-Tiny)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.5.5.2"><span class="ltx_text" id="S4.T4.2.1.5.5.2.1" style="background-color:#DFDFDF;">82.4/70.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.5.5.3"><span class="ltx_text" id="S4.T4.2.1.5.5.3.1" style="background-color:#DFDFDF;">84.0/73.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.5.5.4"><span class="ltx_text" id="S4.T4.2.1.5.5.4.1" style="background-color:#DFDFDF;">83.6/72.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.5.5.5"><span class="ltx_text" id="S4.T4.2.1.5.5.5.1" style="background-color:#DFDFDF;">84.6/73.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.5.5.6"><span class="ltx_text" id="S4.T4.2.1.5.5.6.1" style="background-color:#DFDFDF;">76.3/63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.5.5.7"><span class="ltx_text" id="S4.T4.2.1.5.5.7.1" style="background-color:#DFDFDF;">77.6/65.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.6.6" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.6.6.1"><span class="ltx_text" id="S4.T4.2.1.6.6.1.1" style="background-color:#DFDFDF;">SAM2Rad (Hiera-Small)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.6.2"><span class="ltx_text" id="S4.T4.2.1.6.6.2.1" style="background-color:#DFDFDF;">82.6/71.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.6.3"><span class="ltx_text" id="S4.T4.2.1.6.6.3.1" style="background-color:#DFDFDF;">84.7/73.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.6.4"><span class="ltx_text" id="S4.T4.2.1.6.6.4.1" style="background-color:#DFDFDF;">84.5/73.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.6.5"><span class="ltx_text" id="S4.T4.2.1.6.6.5.1" style="background-color:#DFDFDF;">85.5/75.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.6.6"><span class="ltx_text" id="S4.T4.2.1.6.6.6.1" style="background-color:#DFDFDF;">74.8/61.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.6.7"><span class="ltx_text" id="S4.T4.2.1.6.6.7.1" style="background-color:#DFDFDF;">75.6/62.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.7.7" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.7.7.1"><span class="ltx_text" id="S4.T4.2.1.7.7.1.1" style="background-color:#DFDFDF;">SAM2Rad (Hiera-Large)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.7.2"><span class="ltx_text" id="S4.T4.2.1.7.7.2.1" style="background-color:#DFDFDF;">81.6/69.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.7.3"><span class="ltx_text" id="S4.T4.2.1.7.7.3.1" style="background-color:#DFDFDF;">84.3/73.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.7.4"><span class="ltx_text" id="S4.T4.2.1.7.7.4.1" style="background-color:#DFDFDF;">76.5/64.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.7.5"><span class="ltx_text" id="S4.T4.2.1.7.7.5.1" style="background-color:#DFDFDF;">77.2/64.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.7.6"><span class="ltx_text" id="S4.T4.2.1.7.7.6.1" style="background-color:#DFDFDF;">76.5/64.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.7.7"><span class="ltx_text" id="S4.T4.2.1.7.7.7.1" style="background-color:#DFDFDF;">77.2/64.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.8.8" style="background-color:#DFDFDF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.2.1.8.8.1"><span class="ltx_text" id="S4.T4.2.1.8.8.1.1" style="background-color:#DFDFDF;">SAM2Rad (Hiera-Base+)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.8.8.2"><span class="ltx_text" id="S4.T4.2.1.8.8.2.1" style="background-color:#DFDFDF;">82.9/71.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.8.8.3"><span class="ltx_text" id="S4.T4.2.1.8.8.3.1" style="background-color:#DFDFDF;">85.5/75.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.8.8.4"><span class="ltx_text" id="S4.T4.2.1.8.8.4.1" style="background-color:#DFDFDF;">83.7/72.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.8.8.5"><span class="ltx_text" id="S4.T4.2.1.8.8.5.1" style="background-color:#DFDFDF;">85.1/75.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.8.8.6"><span class="ltx_text" id="S4.T4.2.1.8.8.6.1" style="background-color:#DFDFDF;">75.0/62.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.8.8.7"><span class="ltx_text" id="S4.T4.2.1.8.8.7.1" style="background-color:#DFDFDF;">75.6/62.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.6.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text ltx_font_italic" id="S4.T4.7.2" style="font-size:90%;">Sam2Rad Dice/IoU results<span class="ltx_text ltx_font_upright" id="S4.T4.7.2.1">. <span class="ltx_text ltx_font_bold" id="S4.T4.7.2.1.1">LP</span> - Learned Prompts, <span class="ltx_text ltx_font_bold" id="S4.T4.7.2.1.2">BB + LP</span> - Bounding Box plus Learned Prompts.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6"><svg class="ltx_picture ltx_centering" height="386.04" id="S4.F6.pic1" overflow="visible" version="1.1" width="349.21"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,386.04) matrix(1 0 0 -1 0 0) translate(41.34,0) translate(0,124.28) matrix(1.0 0.0 0.0 1.0 -41.34 -124.28)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(61.02,0) translate(0,124.28)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 -5.91 L 0 0 M 86.12 -5.91 L 86.12 0 M 172.24 -5.91 L 172.24 0 M 258.37 -5.91 L 258.37 0 M 0 258.6 L 0 252.69 M 86.12 258.6 L 86.12 252.69 M 172.24 258.6 L 172.24 252.69 M 258.37 258.6 L 258.37 252.69" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M -19.69 0 L -13.78 0 M -19.69 50.54 L -13.78 50.54 M -19.69 101.08 L -13.78 101.08 M -19.69 151.62 L -13.78 151.62 M -19.69 202.15 L -13.78 202.15 M -19.69 252.69 L -13.78 252.69 M 278.05 0 L 272.15 0 M 278.05 50.54 L 272.15 50.54 M 278.05 101.08 L 272.15 101.08 M 278.05 151.62 L 272.15 151.62 M 278.05 202.15 L 272.15 202.15 M 278.05 252.69 L 272.15 252.69" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M -19.69 0 L -19.69 252.69 L 278.05 252.69 L 278.05 0 L -19.69 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 -47.81 -58.5)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="66.11"><span class="ltx_text" id="S4.F6.pic1.15.15.15.15.15.1.1">Sam2-Tiny</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 32.25 -66.57)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="76.1"><span class="ltx_text" id="S4.F6.pic1.16.16.16.16.16.1.1">SAM2-Small</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 117.2 -65.73)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="76.33"><span class="ltx_text" id="S4.F6.pic1.17.17.17.17.17.1.1">SAM2-Large</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.7071 0.7071 -0.7071 0.7071 200.31 -69.83)"><foreignobject height="10.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="81.37"><span class="ltx_text" id="S4.F6.pic1.18.18.18.18.18.1.1">SAM2-Base+</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.49 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><math alttext="0" class="ltx_Math" display="inline" id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S4.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -44.56 46.08)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.2" class="ltx_Math" display="inline" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">0.2</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">0.2</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -44.56 96.62)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.4" class="ltx_Math" display="inline" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">0.4</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">0.4</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -44.56 147.16)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.6" class="ltx_Math" display="inline" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">0.6</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">0.6</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -44.56 197.7)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.8" class="ltx_Math" display="inline" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1d">0.8</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.49 248.24)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><math alttext="1" class="ltx_Math" display="inline" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1d">1</annotation></semantics></math></foreignobject></g><clippath id="pgfcp1"><path d="M -19.69 0 L 278.05 0 L 278.05 252.69 L -19.69 252.69 Z"></path></clippath><g clip-path="url(#pgfcp1)"><g color="#0000FF" fill="#B3B3FF" stroke="#0000FF"><path d="M -22.14 0 h 20.76 v 197.1 h -20.76 Z M 63.98 0 h 20.76 v 197.1 h -20.76 Z M 150.11 0 h 20.76 v 189.52 h -20.76 Z M 236.23 0 h 20.76 v 186.99 h -20.76 Z"></path></g><g></g><g color="#FF0000" fill="#FFB3B3" stroke="#FF0000"><path d="M 1.38 0 h 20.76 v 164.25 h -20.76 Z M 87.51 0 h 20.76 v 164.25 h -20.76 Z M 173.63 0 h 20.76 v 159.2 h -20.76 Z M 259.75 0 h 20.76 v 154.14 h -20.76 Z"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 -25.21 201.99)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.78" class="ltx_Math" display="inline" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.78</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">0.78</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">0.78</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 60.91 201.99)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.78" class="ltx_Math" display="inline" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.78</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">0.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">0.78</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">0.78</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 147.03 194.41)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.75" class="ltx_Math" display="inline" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.75</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">0.75</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">0.75</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">0.75</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 233.15 191.88)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.74" class="ltx_Math" display="inline" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.74</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">0.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">0.74</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">0.74</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 -1.69 169.14)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.65" class="ltx_Math" display="inline" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.65</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">0.65</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">0.65</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 84.43 169.14)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.65" class="ltx_Math" display="inline" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.65</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">0.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">0.65</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">0.65</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 170.55 164.09)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.63" class="ltx_Math" display="inline" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.63</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">0.63</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">0.63</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">0.63</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 256.68 159.03)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.61" class="ltx_Math" display="inline" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.61</mn><annotation-xml encoding="MathML-Content" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">0.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">0.61</annotation><annotation encoding="application/x-llamapun" id="S4.F6.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">0.61</annotation></semantics></math></foreignobject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 81.99 -124 h 94.39 v 22.65 h -94.39 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 86.14 -121.23)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.555)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.58)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#0000FF" fill="#B3B3FF" stroke="#0000FF" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.01 0) translate(16.12,0) matrix(1.0 0.0 0.0 1.0 -13.36 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.71"><span class="ltx_text" id="S4.F6.pic1.19.19.19.19.19.1.1.1.1.1">Dice</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FFB3B3" stroke="#FF0000" transform="matrix(1 0 0 -1 45.25 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 58.26 0) translate(13.91,0) matrix(1.0 0.0 0.0 1.0 -11.15 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="22.29"><span class="ltx_text" id="S4.F6.pic1.20.20.20.20.20.2.2.2.1.1">IoU</span></foreignobject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_italic" id="S4.F6.4.2" style="font-size:90%;">Prompting SAM2 with few-shot images<span class="ltx_text ltx_font_upright" id="S4.F6.4.2.1">. The figure shows the performance of various SAM variants trained on just 10 images and evaluated on the entire test dataset. The proposed prompting strategy demonstrates data efficiency across all SAM variants. </span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Does prompt learning work any better than bounding box prompts?</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To evaluate the effectiveness of learned prompts versus bounding box prompts, we conducted an experiment using frozen SAM2. The results, presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.T3" title="Table 3 ‣ 4.4 Evaluation Metrics ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">3</span></a>, demonstrate that learned prompts consistently outperform bounding box prompts across all SAM2 variants. For instance, on the wrist dataset, SAM2 (Hiera-Base+) achieved a Dice score of 83.5 with learned prompts, compared to 63.5 with bounding box prompts. These findings suggest that the prompt learning strategy not only addresses SAM’s need for manual prompts but also introduces additional information not present in manual bounding box prompts. The full results, including LoRA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib44" title="">44</a>]</cite> fine-tuning the mask decoder, are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.T4" title="Table 4 ‣ 4.4 Evaluation Metrics ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Zero-shot segmentation</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">First, we evaluate the zero-shot generalizability of frozen SAM, SAM2, and MedSAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#bib.bib14" title="">14</a>]</cite> (fine-tuned on several medical imaging modalities) on three US datasets. We prompt SAM with the ground truth bounding box. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.T2" title="Table 2 ‣ 4.4 Evaluation Metrics ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">2</span></a> show that SAM2 outperforms SAM on all datasets, with the most significant improvement observed on the wrist dataset—although it produces unpredictable results in edge cases. These findings demonstrate that SAM2 aligns better with US images, even without fine-tuning. However, on more challenging datasets such as the shoulder dataset, both SAM and SAM2 perform poorly, indicating the need for fine-tuning. Surprisingly, MedSAM, despite being fine-tuned on medical images, underperforms compared to SAM and SAM2 on US images, even though the latter two were trained solely on natural images.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Few-shot segmentation</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">To further validate the effectiveness of our approach, we trained the model with only 10 images and evaluated it on the corresponding test dataset. The results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.F6" title="Figure 6 ‣ 4.4 Evaluation Metrics ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">6</span></a> show that the proposed prompting strategy works well across all SAM variants, with SAM2 having superior performance. The results demonstrate that our approach is effective in the low data regime, making it suitable for medical imaging applications where expert-labeled data is scarce.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Robustness to heavy data augmentation</h3>
<div class="ltx_para" id="S4.SS8.p1">
<p class="ltx_p" id="S4.SS8.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.F5" title="Figure 5 ‣ 4.2 Data Augmentation ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the models’ robustness to heavy data augmentation on the test set. This is crucial for generalization to unseen data, especially in overparameterized models like many foundation models. The results suggest that the model is learning the underlying structure of the data rather than simply memorizing the training set.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.9 </span>Robustness to noisy ground truth masks</h3>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="199" id="S4.F7.g1" src="extracted/5846389/images/noisy_labels.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_italic" id="S4.F7.4.2" style="font-size:90%;">Sam2-Tiny with PPN predictions on test images<span class="ltx_text ltx_font_upright" id="S4.F7.4.2.1">. The model provides complete bone segmentation, in contrast to the partial masks present in the ground truth labels.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS9.p1">
<p class="ltx_p" id="S4.SS9.p1.1">Our labels contained various inconsistencies, for instance, partial masks. Despite these imperfections, the model effectively averaged out the noise, producing accurate segmentations on test images. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06821v1#S4.F7" title="Figure 7 ‣ 4.9 Robustness to noisy ground truth masks ‣ 4 Experiments ‣ Sam2Rad: A Segmentation Model for Medical Images with Learnable Prompts"><span class="ltx_text ltx_ref_tag">7</span></a> provides visual examples of this robustness.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we presented Sam2Rad, a novel prompt learning approach that significantly enhances the performance of SAM/SAM2 and its variants for ultrasound image segmentation. Our method addresses key limitations of promptable models by eliminating the need for manual prompts and improving segmentation accuracy across various datasets. Sam2Rad outperformed all SAM and SAM2 variants in zero-shot generalization, particularly for challenging cases like shoulder ultrasound. The model demonstrated remarkable improvements, with Dice score increases ranging from 2-7% for hip and wrist datasets, and up to 33% (from 49% to 82%) for shoulder data. Notably, Sam2Rad can be trained with as few as 10 labeled images. By leveraging SAM’s image encoder features, our network seamlessly integrates with SAM’s architecture to produce high-quality prompts. Our model offers three modes: autonomous, semi-autonomous with human-in-the-loop, and fully manual. The seamless integration with SAM’s architecture and compatibility with any SAM variant make Sam2Rad a versatile and powerful tool for automatic segmentation in medical imaging.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Dr. Jacob L. Jaremko is a Canada CIFAR AI Chair, and his academic time is made available by Medical Imaging Consultants (MIC), Edmonton, Canada. We acknowledge the support of TD Ready Grant, IC-IMPACTS, One Child Every Child, Arthritis Society, and Alberta Innovates AICE Concepts for financial support, the Alberta Emergency Strategic Clinical Network for clinical scanning, and Compute Canada in providing us with computational resources including high-power GPU that were used for training and testing our deep learning models.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, T. Brox, U-Net: Convolutional Networks for Biomedical Image Segmentation, https://arxiv.org/abs/1505.04597v1 (May 2015).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, R. Girshick, Mask R-CNN (Jan. 2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1703.06870" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1703.06870</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1703.06870" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1703.06870</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl, J. Wasserthal, G. Koehler, T. Norajitra, S. Wirkert, K. H. Maier-Hein, nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation (Sep. 2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1809.10486" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1809.10486</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1809.10486" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1809.10486</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, J. Liang, UNet++: A Nested U-Net Architecture for Medical Image Segmentation (Jul. 2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1807.10165" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1807.10165</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1807.10165" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1807.10165</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker, D. Rueckert, Attention U-Net: Learning Where to Look for the Pancreas (May 2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1804.03999" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1804.03999</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1804.03999" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1804.03999</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, Y. Zhou, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation (Feb. 2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2102.04306" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2102.04306</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2102.04306" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2102.04306</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. Roth, D. Xu, UNETR: Transformers for 3D Medical Image Segmentation (Oct. 2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2103.10504" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2103.10504</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2103.10504" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2103.10504</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, M. Wang, Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation (May 2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2105.05537" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2105.05537</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2105.05537" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2105.05537</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, D. Amodei, Scaling Laws for Neural Language Models (Jan. 2020).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2001.08361" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2001.08361</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. S. Rosenfeld, A. Rosenfeld, Y. Belinkov, N. Shavit, A Constructive Prediction of the Generalization Error Across Scales (Dec. 2019).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1909.12673" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1909.12673</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1909.12673" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1909.12673</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Bahri, E. Dyer, J. Kaplan, J. Lee, U. Sharma, Explaining Neural Scaling Laws (Apr. 2024).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2102.06701" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2102.06701</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.1073/pnas.2311878121" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1073/pnas.2311878121</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Hernandez, J. Kaplan, T. Henighan, S. McCandlish, Scaling Laws for Transfer (Feb. 2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2102.01293" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2102.01293</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2102.01293" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2102.01293</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, L. Beyer, LiT: Zero-shot transfer with locked-image text tuning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 18123–18133.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Ma, Y. He, F. Li, L. Han, C. You, B. Wang, Segment Anything in Medical Images (Jul. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.12306" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.12306</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dollár, C. Feichtenhofer, SAM 2: Segment Anything in Images and Videos.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Wang, Z. Wu, D. Agarwal, J. Sun, MedCLIP: Contrastive Learning from Unpaired Medical Images and Text (Oct. 2022).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.10163" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2210.10163</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2210.10163" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2210.10163</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollár, R. Girshick, Segment Anything (Apr. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.02643" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.02643</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.02643" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2304.02643</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Zhang, S. Hu, C. Jiang, Y. Cheng, Y. Qi, Segment Anything Model with Uncertainty Rectification for Auto-Prompting Medical Image Segmentation (Dec. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.10529" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2311.10529</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R. Deng, C. Cui, Q. Liu, T. Yao, L. W. Remedios, S. Bao, B. A. Landman, L. E. Wheless, L. A. Coburn, K. T. Wilson, Y. Wang, S. Zhao, A. B. Fogo, H. Yang, Y. Tang, Y. Huo, Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging (Apr. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.04155" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.04155</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. A. Mazurowski, H. Dong, H. Gu, J. Yang, N. Konz, Y. Zhang, Segment Anything Model for Medical Image Analysis: An Experimental Study, Medical Image Analysis 89 (2023) 102918.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.10517" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.10517</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.media.2023.102918" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1016/j.media.2023.102918</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Hu, Y. Li, X. Yang, BreastSAM: A Study of Segment Anything Model for Breast Tumor Detection in Ultrasound Images (May 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.12447" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2305.12447</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model, in: The Twelfth International Conference on Learning Representations, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. Zhang, D. Liu, Customized segment anything model for medical image segmentation, arXiv.org (2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.13785" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/ARXIV.2304.13785</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Zhang, R. Jiao, How segment anything model (SAM) boost medical image segmentation?, arXiv.org (2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.03678" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/ARXIV.2305.03678</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Gu, H. Dong, J. Yang, M. A. Mazurowski, How to build the best medical image segmentation algorithm using foundation models: A comprehensive empirical study with Segment Anything Model (May 2024).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2404.09957" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2404.09957</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
W. Lei, X. Wei, X. Zhang, K. Li, S. Zhang, MedLSAM: Localize and Segment Anything Model for 3D Medical Images (Jun. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.14752" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2306.14752</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Wu, Y. Zhang, R. Fu, H. Fang, Y. Liu, Z. Wang, Y. Xu, Y. Jin, Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation (May 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.12620" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.12620</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.12620" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2304.12620</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Ye, J. Cheng, J. Chen, Z. Deng, T. Li, H. Wang, Y. Su, Z. Huang, J. Chen, L. Jiang, H. Sun, M. Zhu, S. Zhang, J. He, Y. Qiao, SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20 Million masks.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T. Chen, L. Zhu, C. Ding, R. Cao, S.-W. Zhang, Y. Wang, Z. Li, L. Sun, P. Mao, Y.-D. Zang, SAM fails to segment anything? - SAM-Adapter: Adapting SAM in underperformed scenes: Camouflage, shadow, and more, arXiv.org (2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.09148" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/ARXIV.2304.09148</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
T. Chen, L. Zhu, C. Ding, R. Cao, Y. Wang, S. Zhang, Z. Li, L. Sun, Y. Zang, P. Mao, SAM-Adapter: Adapting Segment Anything in Underperformed Scenes, in: 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), IEEE, Paris, France, 2023, pp. 3359–3367.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCVW60793.2023.00361" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICCVW60793.2023.00361</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
F. Putz, J. Grigo, T. Weissmann, P. Schubert, D. Hoefler, A. Gomaa, H. B. Tkhayat, A. Hagag, S. Lettmaier, B. Frey, U. S. Gaipl, L. V. Distel, S. Semrau, C. Bert, R. Fietkau, Y. Huang, The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning (Apr. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.07875" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.07875</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.07875" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2304.07875</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
G. Ning, H. Liang, Z. Jiang, H. Zhang, H. Liao, The potential of ’Segment Anything’ (SAM) for universal intelligent ultrasound image guidance, BioScience Trends 17 (3) (2023) 230–233.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5582/bst.2023.01119" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.5582/bst.2023.01119</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
X. Lin, Y. Xiang, L. Zhang, X. Yang, Z. Yan, L. Yu, SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation (Sep. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.06824" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2309.06824</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T. Chen, L. Zhu, C. Ding, R. Cao, Y. Wang, Z. Li, L. Sun, P. Mao, Y. Zang, SAM Fails to Segment Anything? – SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More, https://arxiv.org/abs/2304.09148v3 (Apr. 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
P. Shi, J. Qiu, S. M. D. Abaxi, H. Wei, F. P.-W. Lo, W. Yuan, Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation, Diagnostics 13 (11) (2023) 1947.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.12637" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.12637</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/diagnostics13111947" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.3390/diagnostics13111947</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. Roy, T. Wald, G. Koehler, M. R. Rokuss, N. Disch, J. Holzschuh, D. Zimmerer, K. H. Maier-Hein, SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model (Apr. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.05396" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2304.05396</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Huang, X. Yang, L. Liu, H. Zhou, A. Chang, X. Zhou, R. Chen, J. Yu, J. Chen, C. Chen, S. Liu, H. Chi, X. Hu, K. Yue, L. Li, V. Grau, D.-P. Fan, F. Dong, D. Ni, Segment anything model for medical images?, Medical Image Analysis 92 (2024) 103061.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.media.2023.103061" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1016/j.media.2023.103061</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Cheng, J. Ye, Z. Deng, J. Chen, T. Li, H. Wang, Y. Su, Z. Huang, J. Chen, L. Jiang, H. Sun, J. He, S. Zhang, M. Zhu, Y. Qiao, SAM-Med2D (Aug. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.16184" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2308.16184</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.16184" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2308.16184</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, Z. Zeng, H. Zhang, F. Li, J. Yang, H. Li, Q. Jiang, L. Zhang, Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks (Jan. 2024).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.14159" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2401.14159</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, L. Zhang, Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, https://arxiv.org/abs/2303.05499v4 (Mar. 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
R. Zhang, Z. Jiang, Z. Guo, S. Yan, J. Pan, X. Ma, H. Dong, P. Gao, H. Li, Personalize Segment Anything Model with One Shot (Oct. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.03048" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2305.03048</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.03048" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2305.03048</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y. Liu, M. Zhu, H. Li, H. Chen, X. Wang, C. Shen, Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching, https://arxiv.org/abs/2305.13310v1 (May 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
K. Zhou, J. Yang, C. C. Loy, Z. Liu, Conditional Prompt Learning for Vision-Language Models (Oct. 2022).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2203.05557" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2203.05557</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, LoRA: Low-Rank Adaptation of Large Language Models (Oct. 2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2106.09685" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2106.09685</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2106.09685" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2106.09685</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929v2 (Oct. 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, R. Girshick, Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377v3 (Nov. 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollár, Focal Loss for Dense Object Detection (Feb. 2018).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1708.02002" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1708.02002</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1708.02002" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1708.02002</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, S. Savarese, Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression (Apr. 2019).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1902.09630" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1902.09630</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1902.09630" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1902.09630</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Li, Z. Zhang, H. Zhao, Self-Prompting Large Language Models for Zero-Shot Open-Domain QA (May 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.08635" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2212.08635</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
B. Xie, H. Tang, B. Duan, D. Cai, Y. Yan, MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation (Mar. 2024).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.14103" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2403.14103</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
T. Shaharabany, A. Dahan, R. Giryes, L. Wolf, AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder (Jun. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.06370" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2306.06370</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.06370" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2306.06370</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, PyTorch: An Imperative Style, High-Performance Deep Learning Library (Dec. 2019).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1912.01703" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1912.01703</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
I. Loshchilov, F. Hutter, Decoupled Weight Decay Regularization (Jan. 2019).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1711.05101" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1711.05101</span></a>, <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1711.05101" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.48550/arXiv.1711.05101</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
C. Zhang, D. Han, Y. Qiao, J. U. Kim, S.-H. Bae, S. Lee, C. S. Hong, Faster Segment Anything: Towards Lightweight SAM for Mobile Applications (Jul. 2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.14289" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2306.14289</span></a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 19:06:54 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
