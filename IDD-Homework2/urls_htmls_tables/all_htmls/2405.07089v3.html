<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SonifyAR: Context-Aware Sound Generation in Augmented Reality</title>
<!--Generated on Mon Aug 12 02:43:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Mixed Reality,  Sound,  Augmented Reality,  Authoring Tool" lang="en" name="keywords"/>
<base href="/html/2405.07089v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S1" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S2" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S2.SS1" title="In 2. Related Work ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>AR Sound Authoring</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S2.SS2" title="In 2. Related Work ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Sound Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S2.SS3" title="In 2. Related Work ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Context-awareness in AR</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S3" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The Design of SonifyAR</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S3.SS1" title="In 3. The Design of SonifyAR ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>The Triad of User, Virtuality, and Reality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S3.SS2" title="In 3. The Design of SonifyAR ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Acquisition of AR Sound</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S3.SS3" title="In 3. The Design of SonifyAR ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Event Representation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>SonifyAR Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS1" title="In 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Event Textualization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS1.SSS0.Px1" title="In 4.1. Event Textualization ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title">Event Types.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS1.SSS0.Px2" title="In 4.1. Event Textualization ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title">Scene Context Understanding.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS1.SSS0.Px3" title="In 4.1. Event Textualization ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title">Virtual Object Understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS1.SSS0.Px4" title="In 4.1. Event Textualization ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title">Event-to-text</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS2" title="In 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Sound Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS3" title="In 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>User Interface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS4" title="In 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Technical Implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S5" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>User Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S5.SS1" title="In 5. User Study ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S5.SS2" title="In 5. User Study ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S5.SS3" title="In 5. User Study ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S6" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S6.SS1" title="In 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Education</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S6.SS2" title="In 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Accessibility</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S6.SS3" title="In 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Sonify Existing Apps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S6.SS4" title="In 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Using SonifyAR on an MR Headset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S6.SS5" title="In 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Augmenting AR Authoring Processes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S7" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S7.SS1" title="In 7. Discussion and Future Work ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Limitations and Failure Cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S7.SS2" title="In 7. Discussion and Future Work ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S8" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#A1" title="In SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="color:#000000;">A</span> </span><span class="ltx_text" style="color:#000000;">Task Prompts</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">SonifyAR: Context-Aware Sound Generation in Augmented Reality</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xia Su
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Washington</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Seattle</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Washington</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xiasu@cs.washington.edu">xiasu@cs.washington.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jon E. Froehlich
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">University of Washington</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Seattle</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">Washington</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jonf@cs.washington.edu">jonf@cs.washington.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eunyee Koh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Adobe Research</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:eunyee@adobe.com">eunyee@adobe.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chang Xiao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Adobe Research</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:cxiao@adobe.com">cxiao@adobe.com</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id17.id1">Sound plays a crucial role in enhancing user experience and immersiveness in Augmented Reality (AR). However, current platforms lack support for AR sound authoring due to limited interaction types, challenges in collecting and specifying context information, and difficulty in acquiring matching sound assets. We present SonifyAR, an LLM-based AR sound authoring system that generates context-aware sound effects for AR experiences. SonifyAR expands the current design space of AR sound and implements a <span class="ltx_text ltx_font_italic" id="id17.id1.1">Programming by Demonstration</span> (PbD) pipeline to automatically collect contextual information of AR events, including virtual-content-semantics and real-world context. This context information is then processed by a large language model to acquire sound effects with <span class="ltx_text ltx_font_italic" id="id17.id1.2">Recommendation</span>, <span class="ltx_text ltx_font_italic" id="id17.id1.3">Retrieval</span>, <span class="ltx_text ltx_font_italic" id="id17.id1.4">Generation</span>, and <span class="ltx_text ltx_font_italic" id="id17.id1.5">Transfer</span> methods. To evaluate the usability and performance of our system, we conducted a user study with eight participants and created five example applications, including an AR-based science experiment, and an assistive application for low-vision AR users.</p>
</div>
<div class="ltx_keywords">Mixed Reality, Sound, Augmented Reality, Authoring Tool
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>The ACM Symposium on User Interface Software and Technology; Oct 13–16,
2024; Pittsburgh, PA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interaction design</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interactive systems and tools</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="250" id="S0.F1.g1" src="x1.png" width="830"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span class="ltx_text" id="S0.F1.2.1" style="color:#000000;">SonifyAR is a custom AR sound authoring pipeline that generates context-matching sounds for AR events <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.1">in situ</span> using generative AI. For example, imagine sliding an AR tea cup across a real-world surface such as a wood table. SonifyAR observes this user action (a slide gesture), the action source (the user), and the action target (a virtual ceramic teacup), infers scene information such as the surface material (a wood table), and uses a custom AI backend to <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.2">recommend</span>, <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.3">retrieve</span>, <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.4">generate</span>, or <span class="ltx_text ltx_font_italic" id="S0.F1.2.1.5">sound-style transfer</span> sound effects.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S0.F1.3">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S0.F1.4">SonifyAR pipeline overview. From left to right: an AR sliding operation on a virtual ceramic cup on a real-world table; A text box containing the context information of this interaction; SonifyAR use retrieve-based and generation-based methods to generate sound; The sound can be tested and experienced in AR.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Sound is a critical but often overlooked element in Augmented Reality (AR). AR-based sound can improve immersion and overall user experience <cite class="ltx_cite ltx_citemacro_citep">(Roginska and Geluso, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib51" title="">2017</a>)</cite> and support depth perception and task completion <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib67" title="">2007</a>)</cite>, search and navigation <cite class="ltx_cite ltx_citemacro_citep">(Rumiński, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib53" title="">2015</a>)</cite>, and even assistance for people with low vision  <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib50" title="">2012b</a>)</cite>. Despite its importance, current AR authoring platforms like <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Reality Composer</span> <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib10" title="">2023c</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">Adobe Aero</span> <cite class="ltx_cite ltx_citemacro_citep">(Adobe, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib6" title="">2023a</a>)</cite>, and <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">Unity Mars</span> <cite class="ltx_cite ltx_citemacro_citep">(Unity Technologies, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib61" title="">2023</a>)</cite> provide only rudimentary sound support. Specifically, we identified three critical gaps with existing AR authoring tools:</p>
</div>
<div class="ltx_para" id="S1.p2">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Limited real-world context.</span> Existing systems typically support action triggers linked to virtual objects in AR but lack support for real-world contextual information (<span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.2">e.g.</span>, a virtual toy robot traversing diverse indoor surfaces like wood, carpet, or glass.).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Limited interaction specification.</span> Existing systems provide only pre-defined interaction triggers like <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.2">“Tap”</span> and <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.3">“Proximity Enter”</span>. This limits a creator’s ability to specify interactions outside the provided options, especially those that involve environmental context (<span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.4">e.g.,</span> the user “slides” virtual chalk on a real-world blackboard).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Limited sound sources.</span> Existing systems are limited by the sound assets available in their libraries and the scarcity of suitable sound resources online. Thus, AR authors struggle to find appropriate sounds for distinct AR events (<span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.2">e.g.,</span> reproducing the wing flutter of a virtual dragonfly or simulating the eating sound of a virtual dinosaur).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these challenges, we present <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">SonifyAR</span>: a context-aware AR sound authoring system using <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib43" title="">2024</a>)</cite> and a <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">text2audio</span> diffusion model called <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">AudioLDM</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>)</cite>. SonifyAR makes the following key technical advancements:</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.p4.1.1">First, context collection using PbD.</span> SonifyAR adopts a <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">Programming by Demonstration</span> (PbD) <cite class="ltx_cite ltx_citemacro_citep">(Lau and Weld, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib34" title="">1998</a>)</cite> pipeline to simplify the specification of complex AR interactions. The PbD pipeline enables users to demonstrate AR sound interactions while the system automatically detects the action and collects context information. For example, if a creator wants to sonify the stomping of a walking robot, they can position the virtual robot on the target (physical) surface. As the robot walks, the collision between robot’s feet and the surface, as well as the context information like the robot’s attributes and the surface’s material, is captured by SonifyAR .</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Second, context as text.</span> To utilize AR context information stemming from multiple sources (<span class="ltx_text ltx_font_italic" id="S1.p5.1.2">e.g.</span> user action, virtual object, real-world environment) and in different formats (<span class="ltx_text ltx_font_italic" id="S1.p5.1.3">e.g.</span> categorical, 3D shape, image), we use text as the universal medium to encompass all context information. For example, we generate an LLM prompt using the following template: “<span class="ltx_text ltx_font_italic" id="S1.p5.1.4">This event is [Event Type], caused by [Source]. This event casts on [Target Object]. [Additional Information on Involved Entities]</span>”.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Finally, LLM-based sound acquisition.</span> We integrate a suite of four sound acquisition methods: <span class="ltx_text ltx_font_italic" id="S1.p6.1.2">recommend</span>, <span class="ltx_text ltx_font_italic" id="S1.p6.1.3">retrieve</span>, <span class="ltx_text ltx_font_italic" id="S1.p6.1.4">generate</span>, and <span class="ltx_text ltx_font_italic" id="S1.p6.1.5">transfer</span>, all controlled by the underlying LLM. For each AR sound interaction, the context information as text is fed to the LLM for processing. The LLM then provides text prompts that control the suite of four sound acquisition methods, which automatically provide matching sound assets for the AR interaction.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To author AR sound with SonifyAR, the creator initiates AR interactions and the system automatically lists sound options based on context. For example, imagine trying to make a virtual tea cup chime accordingly when being slid on a table (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S0.F1" title="Figure 1 ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>). In traditional approaches, the creator would need to manually specify this sliding action and find a sound asset to match the chiming ceramic cup. But with SonifyAR, the creator could demonstrate this sliding action and the chime sounds are generated automatically for the creator to choose from.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To explore the potential of SonifyAR, we conducted two evaluations: first, a qualitative user study with eight designers to examine the usability of and reactions to the SonifyAR prototype; second, we apply SonifyAR across five key user scenarios: from AR education to improving AR headset safety. Our user study highlights the potential of SonifyAR and the generated sounds while identifying key areas of improvement such as sound quality and interface design. Our application scenarios demonstrate the breadth and potential of SonifyAR incorporating automatically acquired sounds seamlessly into various AR experiences—a practice that would otherwise take significant manual time and effort.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Our work makes three primary contributions: (1) a design space for AR sound authoring tools, which highlights existing gaps in the literature; (2) a novel context-aware AR sound authoring pipeline using generative AI, called SonifyAR, that incorporates sensed environmental cues like surface material, virtual object semantics, and user action; (3) findings from a user study with eight designers and five application examples that demonstrate SonifyAR’s potential and key advancements in this space. To our knowledge, we are the first system to offer automatic, in-context sound authoring for AR.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We cover prior work in AR sound authoring, AI-based sound generation techniques, and context-awareness in AR.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>AR Sound Authoring</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Sound in AR provides many benefits from directing the user’s attention and enhancing immersion <cite class="ltx_cite ltx_citemacro_citep">(Roginska and Geluso, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib51" title="">2017</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib67" title="">2007</a>)</cite> to creating interactive time-varying experiences and increasing accessibility <cite class="ltx_cite ltx_citemacro_citep">(Serafin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib54" title="">2018</a>; Rumiński, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib53" title="">2015</a>)</cite>. However, as mentioned, the current landscape of AR authoring tools reveals deficiencies in how sound is supported.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Current tools highlight a variety of AR sound authoring methods. <cite class="ltx_cite ltx_citemacro_citep">(Monteiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib40" title="">2023</a>; Nebeling and Speicher, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib41" title="">2018</a>)</cite>. Some—like <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">Adobe Aero</span> <cite class="ltx_cite ltx_citemacro_citep">(Adobe, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib6" title="">2023a</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">Apple Reality Composer</span> <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib10" title="">2023c</a>)</cite>—are easy to use, require no coding skills, and can only attach user-provided sound with specific AR event triggers <cite class="ltx_cite ltx_citemacro_citep">(hal, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib4" title="">[n. d.]</a>; arv, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib2" title="">[n. d.]</a>; Adobe, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib6" title="">2023a</a>; Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib10" title="">2023c</a>)</cite>. Others—like <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">Unity</span> <cite class="ltx_cite ltx_citemacro_citep">(Technologies, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib60" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.4">Unreal Engine</span> <cite class="ltx_cite ltx_citemacro_citep">(Games, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib19" title="">2023</a>)</cite>—are heavily code-based and require significant technical skills, but provide substantial flexibility in designing AR sound interactions, including the ability to set parameters like sound decay and code-specific conditions for activating sound assets. One common unifying approach <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.5">Programming by Specification</span> (PbS) methodology <cite class="ltx_cite ltx_citemacro_citep">(Monteiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib40" title="">2023</a>)</cite>, where creators define AR sound content and play conditions during the design phase. This allows creators to precisely define the conditions for triggers and the consequent sound effects.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">As mentioned in introduction, however, critical gaps exist. These gaps are partially due to the limitation of the PbS methodology, since the triggers specified in the design stage naturally lack richness in real-world context, which is crucial for AR sound realism. Also, the trade-offs between supported trigger types and the tool’s usability make most tools lack coverage for many sound-producing AR events. Additionally, all sound assets need to be manually selected or created, tasking creators with sourcing suitable sound effects to match the specific triggers. The SonifyAR system aims to address these gaps.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Sound Acquisition</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Although no work has explored sound generation with AR events as input, sound generation conditioned from other input modalities like images, 3D models, text, and videos has been widely explored. Various cross-modal generative models like RNN <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib66" title="">2018</a>)</cite>, GAN <cite class="ltx_cite ltx_citemacro_citep">(Engel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib17" title="">2019</a>; Kumar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib32" title="">2019</a>; Ghose and Prevost, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib20" title="">2022</a>)</cite>, VAE <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib15" title="">2020</a>)</cite>, and most recently diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Kong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib30" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib65" title="">2023</a>)</cite>, are utilized to generate sound. Despite different model architectures, one common training goal is the mapping between latent representations of input modalities and output sound. Thus, sounds can be generated from prompts like images <cite class="ltx_cite ltx_citemacro_citep">(Sheffer and Adi, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib55" title="">2023</a>)</cite>, videos <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib66" title="">2018</a>; Ghose and Prevost, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib20" title="">2022</a>)</cite>, and text <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib65" title="">2023</a>)</cite>. For example, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">FoleyGAN</span> <cite class="ltx_cite ltx_citemacro_citep">(Ghose and Prevost, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib20" title="">2022</a>)</cite> conditions action sequences of input videos to generate visually aligned, realistic soundtracks. <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">AudioLDM</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>)</cite> utilizes the CLAP-based <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib64" title="">2023a</a>)</cite> latent space to embed input text and generate matching sound with a diffusion model. Unlike cross-modal generative methods, another promising approach is physics-based sound synthesis <cite class="ltx_cite ltx_citemacro_citep">(Raghuvanshi and Lin, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib46" title="">2006</a>; Roodaki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib52" title="">2017</a>; Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib48" title="">2013</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib24" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib25" title="">2022</a>; Liu and Manocha, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib39" title="">2021</a>; Diaz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib16" title="">2022</a>)</cite>. For example, the spring-mass model <cite class="ltx_cite ltx_citemacro_citep">(Raghuvanshi and Lin, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib46" title="">2006</a>)</cite>—recently improved with deep learning <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib24" title="">2020</a>)</cite>—can provide realistic sound simulation given a 3D model and material properties.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Besides synthesis, retrieval methods can also acquire matching sound for input conditions. For example, Koepke <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Koepke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib29" title="">2022</a>)</cite> trained cross-modal embedding models to support text-to-audio retrieval. Multi-modal models like VAST  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib11" title="">2023</a>)</cite> also support text-to-audio retrieval that could potentially be used to provide sound assets for AR experiences. Recent work, called <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">Soundify</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib36" title="">2023</a>)</cite>, utilizes retrieved sound assets to sonify videos. The authors claim that although sound retrieval does not ensure coverage for all input descriptions, the retrieved sound assets usually surpass the synthesis results in terms of quality. In this case, combining generation and retrieval in the sound acquisition pipeline seems to be a natural method that integrates the merits of both. We explore this possibility with SonifyAR.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Unlike other existing sound acquisition methods, AR poses unique challenges in the matching of input condition and output sound. The multi-modal nature of AR interactions, including the information of the virtual object (<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.1">e.g.</span> a toy robot with a walking animation), real environment (<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">e.g.</span> a living room with a wooden table), and user action (<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.3">e.g.</span> user taps on a virtual model), needs to be carefully processed into the sound acquisition pipeline to result in matching sound assets.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Context-awareness in AR</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">In AR, context-awareness is an important property that allows AR content to be dynamically adjusted according to context information, such as location, scene semantics, and human factors. Context-awareness aims to adapt an application not only to the user but also to their environment and specific needs with the goal of improved usability and immersive <cite class="ltx_cite ltx_citemacro_citep">(Krings et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib31" title="">2020</a>)</cite>. Previous research has explored various aspects of context-awareness in AR, highlighting its importance across different applications.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">For example, the popular AR game <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.1">Pokemón Go<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_upright" id="S2.SS3.p2.1.1.1.1">(</span>Niantic<span class="ltx_text ltx_font_upright" id="S2.SS3.p2.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib42" title="">2024</a><span class="ltx_text ltx_font_upright" id="S2.SS3.p2.1.1.3.3">)</span></cite></span> shows game contents based on user’s physical location. A more common practice in AR research is adapting virtual contents to scene semantics—essentially understanding the objects and their locations in the user’s environment. Qian <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Qian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib44" title="">2022</a>)</cite> introduced an authoring system that helps user create adaptive AR experiences to surrounding scene semantics. Lang <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib33" title="">2019</a>)</cite> analyzes scene semantics to guide the positioning of virtual agents. Similarly, Liang <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.4">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib35" title="">2021</a>)</cite> places virtual pets into AR based on scene geometry and semantics. This realm of similar work <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib21" title="">2020</a>; Tahara et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib59" title="">2020</a>)</cite> usually captures and analyzes the surrounding environment and designs algorithms that determines possible interaction between virtual content and real-world scene. Lindbauer <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.5">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lindlbauer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib37" title="">2019</a>)</cite> push the AR context-awareness further by including task and cognitive load as context to adjust level of detail of AR virtual interface.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Although there has been substantial discussion in the realm of AR context-awareness, we observed that prior work generally focuses on the visual modality of AR, neglecting sound. Additionally, existing work primarily concentrates on adjusting and arranging virtual content rather than generating new content. We aim to fill this gap by designing a sound generation pipeline that processes context information to produce matching sound assets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>The Design of SonifyAR</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To design an effective sound authoring system that generates matching sounds for AR interaction, three main research questions need to be answered.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">What are the target AR interactions that should produce sound effects?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">How can we efficiently acquire sound assets for these AR interactions?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">How can these AR interactions be specified and represented in the authoring process?</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.p1.2">We aim to elaborate on these questions and the respective design rationale in this section.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>The Triad of User, Virtuality, and Reality</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We began by examining the design space of AR interaction sounds. Drawing on Jain <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">et al.’s</span> <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib23" title="">2021</a>)</cite> sound taxonomy in VR, we attempted a similar analysis for AR sound interaction. We first investigated several widely used code-free AR authoring tools for their AR sound capability, including <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">Adobe Aero</span> <cite class="ltx_cite ltx_citemacro_citep">(Adobe, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib6" title="">2023a</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">Apple Reality Composer</span> <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib10" title="">2023c</a>)</cite>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.4">Halo AR</span> <cite class="ltx_cite ltx_citemacro_citep">(hal, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib4" title="">[n. d.]</a>)</cite>. We found that these tools support AR sound related to either <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.5">virtual content interaction</span> or <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.6">AR-based spatial anchors</span> (<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.7">e.g.,</span>play a sound when a user gets close to a certain position, scans a QR code, or taps on a virtual model).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The AR sound research literature <cite class="ltx_cite ltx_citemacro_citep">(Dam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib14" title="">2024</a>; Rakkolainen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib47" title="">2021</a>; Filus and Rambli, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib18" title="">2012</a>)</cite>, however, envisions a much broader scope. Rakkolainen <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Rakkolainen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib47" title="">2021</a>)</cite> classified AR audio into five classes: <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">ambient</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">directional</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">musical</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.5">speech</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.6">noise</span>.
Dam <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.7">et al.’s</span> <cite class="ltx_cite ltx_citemacro_citep">(Dam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib14" title="">2024</a>)</cite> taxonomy of <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.8">Audio Augmented Reality</span> (AAR), defines AR-based sound across <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.9">Environment Connected</span> (sound maintaining awareness and interaction with physical environment), <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.10">Goal Directed</span> (sound assisting users’ primary goal), and <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.11">Context Adapted</span> (sound adaptive to users’ immediate reality), as well as their intersection. This taxonomy emphasizes that AR sound is often rooted in contextual information from the real world environment and user intention.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Drawing on this prior work, we introduce a novel AR sound design space that helps emphasize the sonic interplay between three key factors of AR experiences: <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1">reality</span>, denoting the physical environment in the real world that hosts the AR experience; <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.2">virtuality</span>, referring to the virtual object(s) placed in the AR experience; and <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.3">the user</span>, representing the individual interacting with the AR experience. Below, we expand on each element of this proposed design space and indicate current levels of support in the existing code-free AR sound authoring tools <cite class="ltx_cite ltx_citemacro_citep">(Adobe, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib6" title="">2023a</a>; Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib10" title="">2023c</a>; hal, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib4" title="">[n. d.]</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="409" id="S3.F2.g1" src="x2.png" width="415"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The sound-producing opportunities in the triad of User, Virtuality and Reality.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F2.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F2.2">Sound design space shown as a hexagon between User, Virtuality and Reality. SonifyAR can support all listed sound design cases, while the existing work can only support the User-Virtuality and Virtuality cases.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Virtuality</span> (<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">partially supported</span>): <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.3">Virtuality</span> refers to sounds that accompany AR events involving only <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.4">virtual objects</span>. This could be bound to an object’s change of status (<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.5">e.g.,</span> a notifying sound when a virtual object shows up), from an object’s animated behavior (<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.6">e.g.,</span> the mechanical noise made by a virtual dinosaur roaring), or the interaction between multiple virtual objects (<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.7">e.g.,</span> two virtual balls clacking when they collide). Current authoring tools support status changes and animation playing as sound-initiating triggers, but do not support sound for interactions between virtual objects.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">User-Virtuality</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.2">(well supported)</span>: <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.3">User-Virtuality</span> are the corresponding sounds that react to the users’ actions with virtual objects (<span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.4">e.g.,</span> a virtual dog barks when users tap on or gets close to it). In all of our investigated AR creation tools, these user-initiated triggers like <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.5">“Tap”</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.6">“Proximity Enter”</span> are well supported.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">User-Reality</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">(not well supported)</span>: <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.3">User-Reality</span> refers to sounds that accompany user interactions with the physical environment via their AR devices. This sound feedback can improve users’ understanding of the surrounding environment (<span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.4">e.g.,</span> an appropriate tap sound when users tap on a real-world table surface via their phone screen). None of the AR authoring tools we investigated support user-reality actions as triggers.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Virtuality-Reality</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.2">(not well supported)</span>: <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.3">Virtuality-Reality</span> denotes sound feedback that plays a crucial role in enhancing realism when virtual objects interact with the real-world environment (<span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.4">e.g.,</span> the crisp stomping sound of a virtual robot when it walks on a real-world glass surface). Although physics simulations between virtual models and real-world surfaces have been supported, their sound feedback remains unexplored in AR authoring tools.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">User-Virtuality-Reality</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.2">(not well supported)</span>:
<span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.3">User-Virtuality-Reality</span> are the sounds that accompany user actions involving both virtual and real elements. For example, the material-aware sliding sounds when users apply a virtual scraper on different real-world surfaces (<span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.4">e.g.,</span> a wooden table, painted wall, or glass window). None of our investigated authoring tools can support the specification of this complex interaction.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p9.1.1">Others</span>: We exclude the domains of <span class="ltx_text ltx_font_bold" id="S3.SS1.p9.1.2">Reality</span> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p9.1.3">User</span> from the enumeration since our interest lies in events where the AR system (<span class="ltx_text ltx_font_italic" id="S3.SS1.p9.1.4">e.g.</span> smartphone or AR headset) generate the sound. Thus, sounds naturally generated by user and reality (<span class="ltx_text ltx_font_italic" id="S3.SS1.p9.1.5">e.g.,</span> a user physically knocking on a wooden table) are excluded from our discussion.</p>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<p class="ltx_p" id="S3.SS1.p10.1">Based on the above analysis and as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S3.F2" title="Figure 2 ‣ 3.1. The Triad of User, Virtuality, and Reality ‣ 3. The Design of SonifyAR ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">2</span></a>, existing AR authoring tools fail to offer functional AR sound authoring support for many of the identified dimensions, especially those involving <span class="ltx_text ltx_font_italic" id="S3.SS1.p10.1.1">Reality</span>. SonifyAR aims to address this gap.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Acquisition of AR Sound</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">While the above design space helps highlight the dimensions of AR sound and current authoring support, below we reflect on key challenges related to the limitations of sound acquisition methods in terms of handling these different dimensions. For example, a mechanical noise of a virtual robot (virtuality) can be easily sourced from local or online sound databases. However, more specific sounds, like a virtual steel ball hitting a physical concrete wall surface or a virtual racecar jumping into a backyard pool (virtuality-reality) requires physical world understanding and material awareness. The space of possible sound effects here is near infinite. Thus, in such cases, generating sounds using text-to-sound models is preferable. Conversely, certain highly recognizable sounds, like a dog barking, pose difficulties for current generative models, often resulting in outputs that are unsatisfactory and noisy.
The complexity in user needs and trade-offs in different methods suggests that the combination of both generation and retrieval could serve as a practical solution. </p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Event Representation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To combine generation and retrieval methods, one challenge is how to condition this suite of different sound acquisition methods.
While visual formats such as images <cite class="ltx_cite ltx_citemacro_citep">(Sheffer and Adi, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib55" title="">2023</a>)</cite> and videos <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib66" title="">2018</a>; Ghose and Prevost, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib20" title="">2022</a>)</cite> have been widely used as inputs for sound generation models, they fall short in fully capturing AR context information, particularly user actions. To overcome this, we explore the use of text as a universal representation for sound acquisition inputs, which has the following benefits. First, text can sufficiently and precisely convey necessary context information and the specifics of sound-generating events (<span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">e.g.,“User slides a ceramic cup on a wooden surface”</span>). Second, given that the AR experience operates within a hardware-software system with multi-modal sensors (<span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">e.g.,</span> camera, IMU, GPS), we can leverage this system to monitor and log events within the AR experience and summarize them into descriptive text. Third, recent advancements, as demonstrated by works like <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.3">HuggingGPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib56" title="">2023</a>)</cite> which uses a LLM to control AI tasks, showcase the practicality of using LLMs as controllers to process text for sound generation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>SonifyAR Implementation</h2>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="291" id="S4.F3.g1" src="x3.png" width="831"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Overview of the pipeline of SonifyAR. Our system monitors and logs context information of AR events, which includes the event type, the subjects and objects (virtual or real-world), and the attributes of the involved elements like their materials. This information is compiled into a text template and then processed by our LLM controller to acquire sound assets. The results are subsequently presented in our selection panel. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F3.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F3.2">A pipeline figure showing SonifyAR’s technical process. From left to right: a virtual toy robot on a wooden desk; several smaller figures showing the context information; a text box containing the context; three icons showing the LLM-controlled sound acquisition process, including openAI logo, a search icon, and a gear icon; a simplified UI panel for sound assets selection.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Building on the above design space, we introduce SonifyAR, a novel PbD sound authoring framework that uses context recognition and generative AI to create personalized, context-sensitive sounds for AR interactions (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F3" title="Figure 3 ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>). SonifyAR consists of three primary components : (1) <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Event Textualization</span>: Every user action is recognized as an AR event, and the context of these events is transformed into textual descriptions. (2) <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">Sound Acquisition</span>: An LLM-controlled sound acquisition process that utilize four methods to produces sound effects based on the context. (3) <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">User Interface</span>: An interface that allows users to experience the AR scene and provides the capability to view, modify, and test sound effects for AR events.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Event Textualization</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In SonifyAR, we utilize a PbD authoring framework to capture potential sound-producing AR events and compile the context information into text. When users interact with the AR space, SonifyAR captures AR events that can lead to sound feedback. Here we define an AR event as a user action and its subsequent result (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">e.g.,</span> when a user taps a virtual object to trigger its animation). For each AR event, we describe it with an event type (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">e.g.,</span> tapping an object ), action source (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">e.g.,</span> user or virtual object), and action target (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.4">e.g.,</span> virtual object or real-world plane). Information about the involved parties, such as virtual objects or real-world planes, are also included in the context information. This collected event data is then transformed into text with a template.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="236" id="S4.F4.g1" src="x4.png" width="416"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>SonifyAR’s event textualization. Left: a user tapping on a virtual cup through the SonifyAR interface; Right: the textual context information extracted by SonifyAR’s internal PbD framework.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F4.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F4.2">Left: a human user holding a phone, while tapping on the screen with another hand. There is a virtual ceramic tea cup on the screen; Right: a text box containing the context information.</p>
</div>
</div>
</figure>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Event Types.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">To support a broad range of AR sound interactions, we implemented six event types:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">Tap Real World Structure:</span> A user taps on a real-world structure through the AR interface.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">Slide: </span>A user holds the virtual object and slides it on a real-world surface.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.1">Collide:</span> A virtual object collides with another virtual object or with a real-world surface. We implemented a specialized collision detection mechanism as introduced in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.SS4" title="4.4. Technical Implementation ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">subsection 4.4</span></a></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i4.p1.1.1">Show Up: </span>A virtual object shows up in the AR experience.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i5.p1.1.1">Tap Virtual Objects:</span> A user taps on a virtual object through the AR interface.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(6)</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i6.p1.1.1">Play Animation:</span> A virtual object plays an animation.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Scene Context Understanding.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">SonifyAR utilizes ARKit <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib7" title="">2023a</a>)</cite>’s plane detection functionality <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib8" title="">2023b</a>)</cite> to assess the surrounding environment. Since most AR experiences are anchored to planes, the detected plane information serves as a crucial context. To enrich such information, we employ the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.1">Deep Material Segmentation</span> model <cite class="ltx_cite ltx_citemacro_citep">(Upchurch and Niu, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib62" title="">2022</a>)</cite> to segment the scene and identify the material of planes (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F3" title="Figure 3 ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>), which helps produce realistic sound effects when an AR event involves this plane. Currently, we support six materials: wood, carpet, concrete, paper, metal, and glass. Surfaces that do not fall into one of these six categories are labeled as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.2">unknown surface</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Virtual Object Understanding</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">Virtual object semantics are also crucial for sound generation. To ensure sound assets align with the virtual object’s material and its state, we collect a text description for all the virtual objects (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.1">e.g.,</span> “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.2">This model is a toy robot made of metal.</span>”) and their animations (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.3">e.g.,</span> “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.4">A toy robot walks.</span>”). These descriptions can be provided by the asset creator or, if necessary, we prompt the user to add relevant details. Any AR event involving the virtual object or its animation will incorporate these text descriptions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Event-to-text</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">To aggregate the multi-source context information described above, we employ a simple text template that will feed into an LLM-based backend: “<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.1">This event is [Event Type], caused by [Source]. This event casts on [Target Object]. [Additional Information on Involved Entities].</span>” <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.2">Event Type</span> is the type of event described by the aforementioned type names; <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.3">Source</span> refers to the subject of the event, which could be the user when the event is directly triggered by user, or virtual object when it interacts with real-world environment; <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.4">Target Object</span> is the object of the event (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.5">e.g.,</span> the plane that gets tapped on or the animation that gets played.). Finally, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.6">Additional Information on Involved Entities</span> include details that further elucidate the source object, the triggered event, and the target object. Examples include material descriptions and animation details. We leave the corresponding entity field blank when events are missing a target object or additional information.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p2.1">During user interactions, the system actively monitors and logs events happening in AR space. The context information is fetched and plugged into the text template, crafting a coherent description that reflects the user’s interaction within the AR environment.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Sound Acquisition</h3>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="768" id="S4.F5.g1" src="x5.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>SonifyAR’s sound acquisition pipeline.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F5.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F5.2">A detailed diagram showing SonifyAR’s sound acquisition process. Top shows the context information fed to the LLM; Middle shows a big openAI logo, indicating the LLM; Bottom shows four sound acquisition pipeline and their respective outputs.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">SonifyAR utilizes GPT4<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib43" title="">2024</a>)</cite> to automatically retrieve or generate context-matching sound assets of an AR event. Inspired by <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">HuggingGPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib56" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">Visual ChatGPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib63" title="">2023b</a>)</cite>, we utilize the LLM as a controller of multiple sound authoring methods. The LLM takes the text description of the event as input and replies with commands for multiple sound acquisition methods (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F5" title="Figure 5 ‣ 4.2. Sound Acquisition ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>). At our current stage, we support four major sound authoring methods: local recommendation, online retrieval, <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">text2sound</span> generation, and text-guided sound transfer. All sound authoring processes listed below operate concurrently in the backend. This ensures an uninterrupted AR experience.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Local Recommendation.</span> The LLM can recommend sound assets stored in the local database based on semantics in the event description. Similar to other AR authoring tools, we collect a set of sound effects from <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">Adobe Audition</span>’s library<cite class="ltx_cite ltx_citemacro_citep">(Adobe, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib5" title="">2023b</a>)</cite>, each labeled with a descriptive filename, like “<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">Crash Aluminum Tray Bang</span>” or “<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.4">Liquid Mud Suction</span>”. The entire catalog of sound filenames is provided to LLM. When given the event context, the LLM recommends the top five most suitable local sound effects based on their filenames. It then returns the selected sound effect in the format of <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.5">method1recommend:FILENAME</span>, where <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.6">FILENAME</span> is
replaced by the actual filename. Upon receiving, SonifyAR parses the filename and adds the top corresponding sound as one of the sound options for the event. Users can also long-hold options in the UI to reveal other top recommendation results.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Online Retrieval.</span> We also expand our retrieval capability with an online sound asset database called <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">FreeSound</span> <cite class="ltx_cite ltx_citemacro_citep">(fre, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib3" title="">[n. d.]</a>)</cite>. We use the FreeSound API, which returns an N-best list of matching sound effects based on a given query.
The queries are condensed versions of full event descriptions generated by the LLM, returned in the format <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.3">method2retrieval:PROMPT</span>, with <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.4">PROMPT</span> replaced by the specific search query. The returned results are JSON strings. We select the top five matches from the entire set of sound effects returned by the API, which are then downloaded and presented to the user as sound options.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Sound Generation.</span> Beyond recommendation and retrieval methods, we also use the text prompt to generate custom sound effects using an audio diffusion model <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.2">AudioLDM</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>)</cite>. We ask the LLM to compress the event text description into a shortened generation prompt: using the format <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p4.1.3">method3generation:PROMPT</span>, where the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p4.1.4">PROMPT</span> would be replaced with the generation prompt. Upon receiving such a command from the LLM, SonifyAR sends the prompt to the AudioLDM model, requesting <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.5">text2sound</span> generation. These newly generated sounds are then shown in the UI as sound options.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Sound-style Transfer.</span> Besides text2audio generation, <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.2">AudioLDM</span> is also capable of performing text-based sound style transfer. Specifically, for events like tapping, sliding, or colliding, instead of generating a new sound from scratch, SonifyAR uses a default sound effect and initiates a style transfer operation with a text prompt provided by the LLM (<span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.3">e.g.,</span> transfer a general tap sound to tapping on glass). This approach allows the output sounds to match the length and rhythm of the input sounds, so that they can be well-timed with actions. Furthermore, when users wish to further modify any provided sound assets, SonifyAR offers text-based style transfer as a fine-tuning and customizing option.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>User Interface</h3>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="517" id="S4.F6.g1" src="x6.png" width="831"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>SonifyAR’s authoring interface. Left: SonifyAR’s phone-based AR interface. Right: SonifyAR’s authoring panel.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F6.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F6.2">Two UI screensots. Left shows a virtual toy plane placed on a real-world wooden desk; Right shows the same interface with a half-transparent call-out panel on the lower half of screen, containing the acquired sound assets for AR events.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As a PbD authoring framework, the SonifyAR application invites users to explore freely with an interactive AR experience (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F6" title="Figure 6 ‣ 4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> left). Users can interact with the scene, performing actions like moving virtual objects or interacting with the real-world surfaces. When an AR event is detected, a text label appears on the top-left screen to inform the user (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F6" title="Figure 6 ‣ 4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>A) and the sound acquisition component is activated to generate candidate sound effects for the detected events. By clicking the “<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">authoring panel</span>” button (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F6" title="Figure 6 ‣ 4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>D), users can prompt an editing interface (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F6" title="Figure 6 ‣ 4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>F) that overlays the AR scene. The UI includes all detected AR events (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F6" title="Figure 6 ‣ 4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>G) and the corresponding generated sound assets (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S4.F6" title="Figure 6 ‣ 4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>H). Users can click on a sound effect to preview and double click to select and activate it. After confirming their choices, users can hide the authoring panel to resume the AR experience and test the selected sounds. The editing interface can always be re-activated to modify choices.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Additionally, users can long press a sound asset to call out a menu with a suite of exploratory options. For recommended or retrieved sounds, the menu includes an option to list all other sound assets in the top five recommendations. For all sound assets, the menu provides a “<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">style transfer</span>” and a “<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">generate similar sounds</span>” feature, enabling users to style transfer audio effects or to generate similar sounds based on the selected sound. When this feature is selected, users can type a simple text prompt to guide the sound generation process. These choices can be iterated upon or used to explore new sound variations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Technical Implementation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">SonifyAR is built with Apple’s ARKit in Swift 5.7 and Xcode 14, and runs as an iOS application on iPhones. Due to the enhanced performance of plane detection on devices with LiDAR (available in iPhone’s Pro lineup starting from iPhone Pro 12), we developed and tested the app on an iPhone 13 Pro Max running iOS 16. We implemented the collision detection between virtual objects and planes with ARKit’s physics simulation <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib9" title="">2023d</a>)</cite>. To enable collision detection for animated virtual object parts (<span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">e.g.</span> the stomping of a robot foot on the plane), we bind colliders to the joints of the virtual object’s animated bones.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">For the LLM-based backend, we use GPT-4.0 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib43" title="">2024</a>)</cite>. The text prompt is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#A1" title="Appendix A Task Prompts ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>. For material segmentation, we use the <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.1">Dense Material Segmentation</span> (DMS) model
<cite class="ltx_cite ltx_citemacro_citep">(Upchurch and Niu, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib62" title="">2022</a>)</cite> for its detailed material labeling, fast processing time, and high accuracy. For sound generation, we use the <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.2">AudioLDM</span> model <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>)</cite> due to its state-of-the-art performance and versatility in text2sound generation and text-guided sound2sound transfer. All models are hosted on a server with an NVIDIA RTX 4080 GPU and CUDA 12.2.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>User Study</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To examine the usability and authoring performance of the SonifyAR system, we conducted a usability study across eight participants, who used the SonifyAR application in a fixed experimental setting and provided ratings for the system.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Participants</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We recruited eight participants (six male and two female, aged between 26 and 33)<span class="ltx_text" id="S5.SS1.p1.1.1" style="color:#000000;"> via snowball sampling at Adobe with varying experience in AR. </span>Out of the eight participants, six had prior AR experience, and three had specific experience in AR authoring tools.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Procedure</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The usability study was conducted in a small meeting room and had four parts: (1) We first introduced our study goal, collected demographic and background information. (2) We then demonstrated SonifyAR with an example AR experience. During this phase, participants had the opportunity to ask any questions regarding SonifyAR’s functionalities. (3) Afterwards, participants were asked to independently explore the SonifyAR app. We provided a walking robot model which could be added to indoor surfaces. This model starts walking when tapped. For consistency, we asked all participants to explore this same AR asset and use our automatic sound authoring pipeline to create sound effects with the model until they were satisfied with the results. The entire usage process was screen-recorded and the user operations were logged. (4) Finally, we sought participant feedback regarding the usability, helpfulness and technical performance of SonifyAR. Participants provided reasoning and insights to support their answers. The questions and rating results are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S5.F7" title="Figure 7 ‣ 5.3. Results ‣ 5. User Study ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Results</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">All participants were able to complete the AR sound authoring task using SonifyAR without difficulty. On average, the authoring process took 406 seconds (<span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">SD=</span>137s),<span class="ltx_text" id="S5.SS3.p1.1.2" style="color:#000000;"> and the entire study took 35 minutes</span>. Participants tested an average of 56 (<span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.3">SD=</span>10.5) sound assets, with an average of 6.4 sounds (<span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.4">SD=</span>1.7) assigned to the authoring results.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Participant feedback was also largely positive. All eight participants expressed favorable impressions of the tool. They highly agreed that the SonifyAR tool would be helpful to AR authoring process (Q1, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">avg=</span>6 out of 7, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">SD=</span>0.93). There was also a general willingness for using SonifyAR in their own AR creation practice (Q2, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.3">avg=</span>6.3, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.4">SD=</span>1.16). Participants agreed that the sound interaction involving real-world surfaces will improve the immersiveness of AR experiences (Q3, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.5">avg=</span>6.4, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.6">SD=</span>1.1). Additionally, they preferred the automated sound authoring process over their prior manual search experiences for sound assets (Q5, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.7">avg=</span>6.1, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.8">SD=</span>1.0). The only area for improvement was the quality of the generated sound (Q4, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.9">avg=</span>4.75, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.10">SD=</span>1.39). However, this could be enhanced with the introduction of a more advanced sound generative model, which can be easily integrated into our system. The full list of questions and the respective Likert results can be viewed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S5.F7" title="Figure 7 ‣ 5.3. Results ‣ 5. User Study ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text" id="S5.SS3.p3.1.1" style="color:#000000;">Besides the ratings, participants also provided suggestions, especially on the UI and usability. For example, <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.1.1">“I really want more usability features that can help communicate these events”</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.1.2">“I hope there are more associations between the AR events and the sound generation happening in the backend, like some real-time audio hints”</span>. </span></p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="507" id="S5.F7.g1" src="x7.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Likert questions and results of our usability study</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F7.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F7.2">A horizontal chart showing the likert question results with color and numbers.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Applications</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">To further explore and demonstrate the potential of our approach, we authored five AR scenarios with SonifyAR. See the supplementary video for additional details and an audio-visual demonstration of the resulting SonifyAR sound effects.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Education</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">AR has been widely explored in STEM education (<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.1">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citep">(Sırakaya and Alsancak Sırakaya, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib57" title="">2022</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib28" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib26" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib27" title="">2016</a>; Suzuki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib58" title="">2020</a>; Chulpongsatorn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib12" title="">2023</a>)</cite>). As existing educational applications try to simulate and visualize physical or chemical phenomenon in AR <cite class="ltx_cite ltx_citemacro_citep">(Radu and Schneider, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib45" title="">2019</a>; Irwansyah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib22" title="">2018</a>)</cite>, SonifyAR can improve the blending between the real environment and virtual content through immersive, appropriate sound effects. To showcase such a possibility, we implemented an AR physics experiment using SonifyAR. To help illustrate one of Newton’s laws of motion–the conservation of momentum—we created an AR scene with a downward ramp and two metal balls: one at the top and the other at the bottom (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S6.F8" title="Figure 8 ‣ 6.1. Education ‣ 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>). Upon initiating the experiment, the top ball rolls down the slope and collides with the bottom ball. This collision causes the bottom ball to move forward and eventually fall, while the top ball comes to a stop. With SonifyAR, collision events are automatically detected and material-specific sound effects, such as the clashing of two metal balls or a metal ball dropping onto table, are seamlessly generated.</p>
</div>
<figure class="ltx_figure" id="S6.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="523" id="S6.F8.g1" src="x8.png" width="789"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>An AR physics experiment sonified by SonifyAR. (A) The slope and metal ball model and simulation process. (B), (C) Context information of two collisions in the simulation. (D) The acquired sound assets for the collisions.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F8.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.F8.2">A figure showing an AR physics experiment and SonifyAR’s authoring results. A: a virtual slope with two metal balls on it. The top ball slide down and collide with the lower one. Then the lower one drop down to the table while the top one remain on slope. B and C: the context information of the two collide events; D: the resulted sound assets shown in UI.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Accessibility</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">AAR (Audio Augmented Reality) has been explored as accessibility assistance for blind or low vision people <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib49" title="">2012a</a>; Coughlan and Miele, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib13" title="">2017</a>)</cite>. By sonifying real world environments and virtual contents, blind or low vision (BLV) people can better interpret visual information in both reality and virtuality. We envision SonifyAR assisting AR accessibility in three ways. First, by supporting AR sound authoring, SonifyAR encourages creators to add sound effects in AR experiences, which can help BLV users interpret visual content. Second, by providing context-aware AR sound in 3D audio, people with low-vision can better navigate virtual objects in the AR environment <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib49" title="">2012a</a>)</cite>. Third, by enabling user to interact with real-world surfaces via AR (<span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">e.g.</span> tapping on a real world surface), user can explore the surrounding space via the AR interface.</p>
</div>
<figure class="ltx_figure" id="S6.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="663" id="S6.F9.g1" src="x9.png" width="497"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Robot walking on different surface with different sound. (A), (B) Robot walking on a wooden surface, and the corresponding context information. (C), (D) Robot walking on carpet surface, and the corresponding context information. (E), (F) The acquired sound assets.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F9.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.F9.2">A figure showing an AR robot walking and SonifyAR’s authoring results. A: a virtual robot walk on wooden table. B: the context information of the walking; C: a virtual robot walk on carpet surface. D: the context information of the walking; E and F: the resulted sound assets shown in UI.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">To showcase potential accessibility benefits, we use a toy robot 3D model (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S6.F9" title="Figure 9 ‣ 6.2. Accessibility ‣ 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>). As the robot virtually walks from one physical surface to another (<span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">e.g.,</span> from a wood floor to a carpeted floor), the auto-generated sound effects change appropriately. In this way, a BLV user can better perceive the location and activity of the robot as it is walking.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Sonify Existing Apps</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Due to the extensibility of the text template used in SonifyAR’s sound acquisition pipeline, it can be easily implemented into existing AR apps. For example, if implemented at the SDK level (<span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.1">e.g.</span> ARKit and ARCore), SonifyAR can automatically capture textual description of AR events and provide sonification using the automatic sound acquisition results.</p>
</div>
<figure class="ltx_figure" id="S6.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="396" id="S6.F10.g1" src="x10.png" width="582"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>AR dinosaur sonified by SonifyAR. (A) AR Camera app interface with a virtual dinosaur playing eating animation in the scene. (B) The context information of the scene. (C) Text prompt and sound output from SonifyAR’s pipeline.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F10.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.F10.2">A figure showing an dinosaur roaring and SonifyAR’s authoring results. A: a virtual dinosaur roar in an AR scene; B: the context information of the roaring; C: the resulted sound asset.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">We explore this application possibility with a Wizard of Oz (WoZ) prototype on a phone-based AR application called <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.1">ARVid</span> <cite class="ltx_cite ltx_citemacro_citep">(arv, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib2" title="">[n. d.]</a>)</cite>, which is a downloadable AR app that puts 3D animated objects into real world environment. We selected a dinosaur animation as an example (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S6.F10" title="Figure 10 ‣ 6.3. Sonify Existing Apps ‣ 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>) and use WoZ to simulate the direct integration of the SonifyAR pipeline into ARVid app. By manually feeding the context information (<span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.2">e.g. “The dinosaur is eating”</span>) into the SonifyAR pipeline, we generated sound assets that well-match the 3D animations.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Using SonifyAR on an MR Headset</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">As recent AR/MR headsets become increasingly popular, manufacturers have established safety guidelines, such as setting up safety zone and clear up indoor spaces. We envision SonifyAR contributing to this topic by hinting users of the existence of real-world entities with material-aware sound effects when application windows or virtual object intersect with real world surfaces or objects.</p>
</div>
<figure class="ltx_figure" id="S6.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="354" id="S6.F11.g1" src="x11.png" width="663"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Sonification of Vision Pro’s window operation. (A) User place a virtual window on sofa. (B) Context information of this action. (C) Text prompt and sound output from SonifyAR’s pipeline.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F11.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.F11.2">A figure showing Vision Pro’s interface and SonifyAR’s authoring results. A: a virtual browser window rests on a sofa in Vision Pro’s interface; B: the context information of the collision; C: the resulted sound asset.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">We implement a WoZ prototype based on Apple Vision Pro’s user interaction (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S6.F11" title="Figure 11 ‣ 6.4. Using SonifyAR on an MR Headset ‣ 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>). When using the Vision Pro, a user can actively adjust the position of application windows by hand gestures. SonifyAR can enhance this interaction by generating AR sounds when virtual windows are placed on or intersect with indoor surfaces like walls, desks, and floors. By processing these interactions and applying sound transfers with text prompts such as <span class="ltx_text ltx_font_italic" id="S6.SS4.p2.1.1">“a virtual window bumps into a wall”</span> and <span class="ltx_text ltx_font_italic" id="S6.SS4.p2.1.2">“a virtual window collides with a carpet”</span>, SonifyAR aims to improve the permanence of virtual windows and increase awareness of surrounding objects as users navigate through virtual windows.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5. </span>Augmenting AR Authoring Processes</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">Besides serving as a standalone PbD authoring experience, SonifyAR could improve existing AR authoring tools. As mainstream AR authoring tools like Adobe Aero and Apple Reality Composer already represent their AR interaction as text like <span class="ltx_text ltx_font_italic" id="S6.SS5.p1.1.1">“Tap &amp; Play Sound”</span> and <span class="ltx_text ltx_font_italic" id="S6.SS5.p1.1.2">“Proximity &amp; Jiggle”</span>, these specifications can be fed into SonifyAR’s text-based sound authoring pipeline. In this case, SonifyAR works as a plugin and provides matching sound assets for AR interactions.</p>
</div>
<figure class="ltx_figure" id="S6.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="536" id="S6.F12.g1" src="x12.png" width="829"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>WoZ example of SonifyAR being applied in Reality Composer’s authoring process. (A) A machine model in Reality Composer’s authoring interface. (B) Reality Composer’s interface for AR sound. (C) Context information of the event in B. (D), (E) Two sound assets generated and retrieved with SonifyAR’s pipeline. (F) The acquired sound being used as recommended options.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F12.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S6.F12.2">A figure showing Reality Composer’s interface and SonifyAR’s authoring results. A: Reality Composer’s mobile interface with a sewing machine in the canvas; B: Reality Composer’s UI for authoring the ”play sound” action of the virtual object; C: the context information of the ”play sound” action; D: SonifyAR’s generation result as a sound asset; E: SonifyAR’s retrieval result as a sound asset; F: the WoZ interface of Reality Composer showing SonifyAR’s sound assets.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">We implemented an additional WoZ prototype using Apple’s Reality Composer and a 3D model of an animated machine (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.07089v3#S6.F12" title="Figure 12 ‣ 6.5. Augmenting AR Authoring Processes ‣ 6. Applications ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>). When authoring the <span class="ltx_text ltx_font_italic" id="S6.SS5.p2.1.1">“Play Sound”</span> behavior of this model, context information like the model description, triggering condition, and animation description, can be fed into SonifyAR’s text-based authoring pipeline. The resulting retrieved/generated sound assets appear at the top of the authoring UI as <span class="ltx_text ltx_font_italic" id="S6.SS5.p2.1.2">“SonifyAR Recommended”</span>. See supplementary video for more details.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Discussion and Future Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we designed and implemented an AR sound authoring pipeline that automatically generates sound assets based on context information. We evaluated and identified opportunities in the AR sound interaction space, and implemented a custom LLM+Generative AI pipeline for sound acquisition. We tested the pipeline with a usability study and through a ”proof-by-demonstration” across five application scenarios.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Limitations and Failure Cases</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">While SonifyAR was positively evaluated in our user study and demonstrated broad application potential, we observed limitations in the system adaptability and encountered some failure cases.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">Firstly, since the sound acquisition pipeline relies on an LLM as both the context collector and overall controller, LLM errors can lead to failures. For example, the LLM can hallucinate about the context information and prompt sound generation model with inaccruate text input (<span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1">e.g.,</span> LLM provides prompt “sliding plastic on carpet” when the provided context information is the virtual model, a metal toy robot, slides on wooden floor) that lead to non-matching outputs.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">Secondly, although AudioLDM <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#bib.bib38" title="">2023</a>)</cite> is a state-of-the-art text-to-audio generator and performs well in most cases, it can sometimes produce subpar outputs that are noisy or do not match prompts. Based on our experiences, such performance inconsistency can be hard to predict thus cannot be completely solved by prompt engineering.</p>
</div>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">Lastly, some AR interactions can be highly time-sensitive (<span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.1">e.g.,</span> objects colliding, playing animations that show clear action sequences) and require sound assets to be precisely synchronized with visual content. <span class="ltx_text" id="S7.SS1.p4.1.2" style="color:#000000;">Currently, we use two methods to synchronize sounds with AR events. First, we provide example sound assets for AR interactions of <span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.2.1">“Colliding”, “Tapping”</span> and <span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.2.2">“’Sliding’</span>. By conducting sound style transfer using well-timed example sounds as inputs, the output sounds can be formatted to be in sync with the AR interaction. Secondly, we bind colliders to 3D model skeleton joints, ensuring that the generated sound is played only when a collision is detected between an animated model and real-world surfaces. However, these synchronization method do not apply to 3D assets without skeletons or sounds unrelated to collisions, such as dinosaur roaring and mechanical arm unfolding. Fully addressing this 3D animation-to-audio synchronization challenge is beyond our current scope.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Future Work</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">We plan to address the following improvements for future work.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">First, our current SonifyAR app is a simplified tool that exclusively supports the authoring of AR sound and uses pre-crafted AR assets as input. For wider applicability that reflects the real-world needs of AR content creation, we recognize the need to incorporate the AR sound authoring pipeline into the entire AR content creation process. Our future goals include integrating the SonifyAR framework with established tools like Reality Composer and Adobe Aero, <span class="ltx_text" id="S7.SS2.p2.1.1" style="color:#000000;"> and also building a Unity SDK to enable usage of our system among professional AR developers. We will also expand the list of AR event types supported by SonifyAR, adding categories such as <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.1">“Spin”</span>, <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.2">“Emphasize”</span>, <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.3">“Expand”</span>, or <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1.4">“Jump”</span>—all of which can easily be incorporated into the text template of SonifyAR.</span></p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">Secondly, SonifyAR adopts a straightforward process of using a text template to compile context information collected from multiple sources, including virtual object semantics and real world semantics. Currently, we use one material segmentation model to perceive real-world context information. In future developments, we aspire to incorporate more sophisticated computer vision models or multi-modal foundational models, for deeper understanding of both virtual objects and real world environment, enhancing the context acquisition process with greater precision and broader adaptability.
<span class="ltx_text" id="S7.SS2.p3.1.1" style="color:#000000;">
Thirdly, SonifyAR has some basic error-handling features, such as the automatic removal of failed retrieval results and editing options for subpar sound assets (<a class="ltx_ref" href="https://arxiv.org/html/2405.07089v3#S4.SS3" title="4.3. User Interface ‣ 4. SonifyAR Implementation ‣ SonifyAR: Context-Aware Sound Generation in Augmented Reality"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>). Additionally, users can correct recognition errors, such as incorrectly recognized materials, by specifying the correct material with text. In future versions of SonifyAR, we plan to incorporate more advanced error-handling mechanisms, such as deploying a sound quality inference model to validate the output and automatically re-prompt the sound generation model when subpar sounds are generated.</span></p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1">Lastly, the current sound generation output of SonifyAR presents opportunities for further refinement. Recognizing the rapid advancements in AI, our system has been designed as a modular framework, ensuring that as more advanced models emerge, they can be seamlessly integrated. Every model (<span class="ltx_text ltx_font_italic" id="S7.SS2.p4.1.1">e.g.</span>, LLM, CV, Sound Generation) within SonifyAR is replaceable, allowing SonifyAR to easily adapt and stay at the forefront of innovation in this field.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we present SonifyAR, a context-aware sound authoring system designed for sonifying AR events. Our work implements a custom PbD authoring pipeline which enables sound asset acquisition using context information and AI. With SonifyAR, users demonstrate AR interactions and have AR sound assets automatically acquired. Our studies validate the usability and overall performance of our system. Our five application examples further supports the potential of our approach. This research advances literature in AR sound authoring, while also opening up new research avenues for the application of LLM and generative models in AR systems.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research was mainly conducted during an internship at Adobe Research and also supported by NSF award #1763199. We thank Dr. Cuong Nguyen for his suggestions to our research and our user study participants for their participation.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">arv ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ARVid - Augmented Reality</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://apps.apple.com/us/app/arvid-augmented-reality/id1276546297" title="">https://apps.apple.com/us/app/arvid-augmented-reality/id1276546297</a>
</span>
<span class="ltx_bibblock">Accessed on September 24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">fre ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Freesound</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://freesound.org/" title="">https://freesound.org/</a>
</span>
<span class="ltx_bibblock">Accessed on September 24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">hal ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Halo AR</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://haloar.app/" title="">https://haloar.app/</a>
</span>
<span class="ltx_bibblock">Accessed on September 24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adobe (2023b)</span>
<span class="ltx_bibblock">
Adobe. 2023b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Adobe Audition Sound Effects Download Page</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adobe.com/products/audition/offers/AdobeAuditionDLCSFX.html" title="">https://www.adobe.com/products/audition/offers/AdobeAuditionDLCSFX.html</a>
</span>
<span class="ltx_bibblock">Accessed on Date of Access.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adobe (2023a)</span>
<span class="ltx_bibblock">
Adobe. Accessed September 11, 2023a.

</span>
<span class="ltx_bibblock">Adobe Aero.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adobe.com/products/aero.html" title="">https://www.adobe.com/products/aero.html</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2023a)</span>
<span class="ltx_bibblock">
Apple. 2023a.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Apple ARKit Documentation</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.apple.com/documentation/arkit/" title="">https://developer.apple.com/documentation/arkit/</a>
</span>
<span class="ltx_bibblock">Accessed on Oct 9th, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2023b)</span>
<span class="ltx_bibblock">
Apple. 2023b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ARKit - Tracking and Visualizing Planes</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_visualizing_planes" title="">https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_visualizing_planes</a>
</span>
<span class="ltx_bibblock">Accessed on Oct 9th, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2023d)</span>
<span class="ltx_bibblock">
Apple. 2023d.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">SceneKit - Physics Simulation</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.apple.com/documentation/scenekit/physics_simulation" title="">https://developer.apple.com/documentation/scenekit/physics_simulation</a>
</span>
<span class="ltx_bibblock">Accessed on Oct 9th, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2023c)</span>
<span class="ltx_bibblock">
Apple. Accessed September 11, 2023c.

</span>
<span class="ltx_bibblock">Reality Composer.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://apps.apple.com/us/app/reality-composer/id1462358802" title="">https://apps.apple.com/us/app/reality-composer/id1462358802</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. 2023.

</span>
<span class="ltx_bibblock">VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.18500 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chulpongsatorn et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Neil Chulpongsatorn, Mille Skovhus Lunding, Nishan Soni, and Ryo Suzuki. 2023.

</span>
<span class="ltx_bibblock">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em> (San Francisco, CA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.2">(UIST ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 92, 16 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3586183.3606827" title="">https://doi.org/10.1145/3586183.3606827</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coughlan and Miele (2017)</span>
<span class="ltx_bibblock">
James M Coughlan and Joshua Miele. 2017.

</span>
<span class="ltx_bibblock">AR4VI: AR as an accessibility tool for people with visual impairments. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)</em>. IEEE, 288–292.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dam et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Abhraneil Dam, Arsh Siddiqui, Charles Leclercq, and Myounghoon Jeon. 2024.

</span>
<span class="ltx_bibblock">Taxonomy and definition of audio augmented reality (AAR): A grounded theory study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">International Journal of Human-Computer Studies</em> 182 (2024), 103179.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. 2020.

</span>
<span class="ltx_bibblock">Jukebox: A generative model for music.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2005.00341</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diaz et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Rodrigo Diaz, Ben Hayes, Charalampos Saitis, György Fazekas, and Mark Sandler. 2022.

</span>
<span class="ltx_bibblock">Rigid-Body Sound Synthesis with Differentiable Modal Resonators.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.15306" title="">http://arxiv.org/abs/2210.15306</a>
</span>
<span class="ltx_bibblock">arXiv:2210.15306 [cs, eess].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engel et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. 2019.

</span>
<span class="ltx_bibblock">Gansynth: Adversarial neural audio synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:1902.08710</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Filus and Rambli (2012)</span>
<span class="ltx_bibblock">
Mohd Ihsan Alimi Mohd Filus and Dayang Rohaya Awang Rambli. 2012.

</span>
<span class="ltx_bibblock">Using non-speech sound as acoustic modality in Augmented Reality environment. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">2012 International Symposium on Computer Applications and Industrial Electronics (ISCAIE)</em>. IEEE, 79–82.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Games (2023)</span>
<span class="ltx_bibblock">
Epic Games. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Unreal Engine</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unrealengine.com/en-US" title="">https://www.unrealengine.com/en-US</a>
</span>
<span class="ltx_bibblock">Accessed on Oct 9th, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghose and Prevost (2022)</span>
<span class="ltx_bibblock">
Sanchita Ghose and John J Prevost. 2022.

</span>
<span class="ltx_bibblock">Foleygan: Visually guided generative adversarial network-based synchronous sound generation in silent videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">IEEE Transactions on Multimedia</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lei Han, Tian Zheng, Yinheng Zhu, Lan Xu, and Lu Fang. 2020.

</span>
<span class="ltx_bibblock">Live semantic 3d perception for immersive augmented reality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">IEEE transactions on visualization and computer graphics</em> 26, 5 (2020), 2012–2022.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irwansyah et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ferli Septi Irwansyah, YM Yusuf, Ida Farida, and Muhammad Ali Ramdhani. 2018.

</span>
<span class="ltx_bibblock">Augmented reality (AR) technology on the android operating system in chemistry learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">IOP conference series: Materials science and engineering</em>, Vol. 288. IOP Publishing, 012068.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dhruv Jain, Sasa Junuzovic, Eyal Ofek, Mike Sinclair, John R. Porter, Chris Yoon, Swetha Machanavajhala, and Meredith Ringel Morris. 2021.

</span>
<span class="ltx_bibblock">A Taxonomy of Sounds in Virtual Reality. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 2021 International Conference on Multimodal Interaction</em>. ACM, Montréal QC Canada, 80–91.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3462244.3479946" title="">https://doi.org/10.1145/3462244.3479946</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xutong Jin, Sheng Li, Tianshu Qu, Dinesh Manocha, and Guoping Wang. 2020.

</span>
<span class="ltx_bibblock">Deep-Modal: Real-Time Impact Sound Synthesis for Arbitrary Shapes. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 28th ACM International Conference on Multimedia</em> <em class="ltx_emph ltx_font_italic" id="bib.bib24.4.2">(MM ’20)</em>. Association for Computing Machinery, New York, NY, USA, 1171–1179.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3394171.3413572" title="">https://doi.org/10.1145/3394171.3413572</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xutong Jin, Sheng Li, Guoping Wang, and Dinesh Manocha. 2022.

</span>
<span class="ltx_bibblock">NeuralSound: learning-based modal sound synthesis with acoustic transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">ACM Transactions on Graphics</em> 41, 4 (July 2022), 1–15.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3528223.3530184" title="">https://doi.org/10.1145/3528223.3530184</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Seokbin Kang, Leyla Norooz, Elizabeth Bonsignore, Virginia Byrne, Tamara Clegg, and Jon E. Froehlich. 2019.

</span>
<span class="ltx_bibblock">PrototypAR: Prototyping and Simulating Complex Systems with Paper Craft and Augmented Reality. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 18th ACM International Conference on Interaction Design and Children</em> (Boise, ID, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib26.4.2">(IDC ’19)</em>. Association for Computing Machinery, New York, NY, USA, 253–266.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3311927.3323135" title="">https://doi.org/10.1145/3311927.3323135</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Seokbin Kang, Leyla Norooz, Vanessa Oguamanam, Angelisa C. Plane, Tamara L. Clegg, and Jon E. Froehlich. 2016.

</span>
<span class="ltx_bibblock">SharedPhys: Live Physiological Sensing, Whole-Body Interaction, and Large-Screen Visualizations to Support Shared Inquiry Experiences. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the The 15th International Conference on Interaction Design and Children</em> (Manchester, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib27.4.2">(IDC ’16)</em>. Association for Computing Machinery, New York, NY, USA, 275–287.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2930674.2930710" title="">https://doi.org/10.1145/2930674.2930710</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Seokbin Kang, Ekta Shokeen, Virginia L. Byrne, Leyla Norooz, Elizabeth Bonsignore, Caro Williams-Pierce, and Jon E. Froehlich. 2020.

</span>
<span class="ltx_bibblock">ARMath: Augmenting Everyday Life with Math Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib28.4.2">(CHI ’20)</em>. Association for Computing Machinery, New York, NY, USA, 1–15.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3313831.3376252" title="">https://doi.org/10.1145/3313831.3376252</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koepke et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
A Sophia Koepke, Andreea-Maria Oncescu, João F Henriques, Zeynep Akata, and Samuel Albanie. 2022.

</span>
<span class="ltx_bibblock">Audio retrieval with natural language queries: A benchmark study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">IEEE Transactions on Multimedia</em> 25 (2022), 2675–2685.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2020.

</span>
<span class="ltx_bibblock">Diffwave: A versatile diffusion model for audio synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2009.09761</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krings et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sarah Krings, Enes Yigitbas, Ivan Jovanovikj, Stefan Sauer, and Gregor Engels. 2020.

</span>
<span class="ltx_bibblock">Development framework for context-aware augmented reality applications. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Companion Proceedings of the 12th ACM SIGCHI Symposium on Engineering Interactive Computing Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib31.4.2">(EICS ’20 Companion)</em>. Association for Computing Machinery, New York, NY, USA, 1–6.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3393672.3398640" title="">https://doi.org/10.1145/3393672.3398640</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre De Brebisson, Yoshua Bengio, and Aaron C Courville. 2019.

</span>
<span class="ltx_bibblock">Melgan: Generative adversarial networks for conditional waveform synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lang et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yining Lang, Wei Liang, and Lap-Fai Yu. 2019.

</span>
<span class="ltx_bibblock">Virtual agent positioning driven by scene semantics in mixed reality. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</em>. IEEE, 767–775.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lau and Weld (1998)</span>
<span class="ltx_bibblock">
Tessa A. Lau and Daniel S. Weld. 1998.

</span>
<span class="ltx_bibblock">Programming by Demonstration: An Inductive Learning Formulation. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 4th International Conference on Intelligent User Interfaces</em> (Los Angeles, California, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">(IUI ’99)</em>. Association for Computing Machinery, New York, NY, USA, 145–152.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/291080.291104" title="">https://doi.org/10.1145/291080.291104</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei Liang, Xinzhe Yu, Rawan Alghofaili, Yining Lang, and Lap-Fai Yu. 2021.

</span>
<span class="ltx_bibblock">Scene-aware behavior synthesis for virtual pets in mixed reality. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
David Chuan-En Lin, Anastasis Germanidis, Cristóbal Valenzuela, Yining Shi, and Nikolas Martelaro. 2023.

</span>
<span class="ltx_bibblock">Soundify: Matching sound effects to video. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lindlbauer et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
David Lindlbauer, Anna Maria Feit, and Otmar Hilliges. 2019.

</span>
<span class="ltx_bibblock">Context-aware online adaptation of mixed reality interfaces. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the 32nd annual ACM symposium on user interface software and technology</em>. 147–160.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. 2023.

</span>
<span class="ltx_bibblock">Audioldm: Text-to-audio generation with latent diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:2301.12503</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Manocha (2021)</span>
<span class="ltx_bibblock">
Shiguang Liu and Dinesh Manocha. 2021.

</span>
<span class="ltx_bibblock">Sound Synthesis, Propagation, and Rendering: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2011.05538" title="">http://arxiv.org/abs/2011.05538</a>
</span>
<span class="ltx_bibblock">arXiv:2011.05538 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monteiro et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kyzyl Monteiro, Ritik Vatsal, Neil Chulpongsatorn, Aman Parnami, and Ryo Suzuki. 2023.

</span>
<span class="ltx_bibblock">Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nebeling and Speicher (2018)</span>
<span class="ltx_bibblock">
Michael Nebeling and Maximilian Speicher. 2018.

</span>
<span class="ltx_bibblock">The Trouble with Augmented Reality/Virtual Reality Authoring Tools. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>. IEEE, Munich, Germany, 333–337.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ISMAR-Adjunct.2018.00098" title="">https://doi.org/10.1109/ISMAR-Adjunct.2018.00098</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niantic (2024)</span>
<span class="ltx_bibblock">
Inc. Niantic. 2024.

</span>
<span class="ltx_bibblock">Pokémon GO.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pokemongolive.com/?hl=en" title="">https://pokemongolive.com/?hl=en</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-07-22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xun Qian, Fengming He, Xiyun Hu, Tianyi Wang, Ananya Ipsita, and Karthik Ramani. 2022.

</span>
<span class="ltx_bibblock">Scalar: Authoring semantically adaptive augmented reality experiences in virtual reality. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>. 1–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radu and Schneider (2019)</span>
<span class="ltx_bibblock">
Iulian Radu and Bertrand Schneider. 2019.

</span>
<span class="ltx_bibblock">What can we learn from augmented reality (AR)? Benefits and drawbacks of AR for inquiry-based learning of physics. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghuvanshi and Lin (2006)</span>
<span class="ltx_bibblock">
Nikunj Raghuvanshi and Ming C. Lin. 2006.

</span>
<span class="ltx_bibblock">Interactive sound synthesis for large scale environments. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 2006 symposium on Interactive 3D graphics and games</em> <em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2">(I3D ’06)</em>. Association for Computing Machinery, New York, NY, USA, 101–108.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1111411.1111429" title="">https://doi.org/10.1145/1111411.1111429</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rakkolainen et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ismo Rakkolainen, Ahmed Farooq, Jari Kangas, Jaakko Hakulinen, Jussi Rantala, Markku Turunen, and Roope Raisamo. 2021.

</span>
<span class="ltx_bibblock">Technologies for multimodal interaction in extended reality—a scoping review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Multimodal Technologies and Interaction</em> 5, 12 (2021), 81.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Zhimin Ren, Hengchin Yeh, and Ming C. Lin. 2013.

</span>
<span class="ltx_bibblock">Example-guided physically based modal sound synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">ACM Transactions on Graphics</em> 32, 1 (Feb. 2013), 1:1–1:16.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2421636.2421637" title="">https://doi.org/10.1145/2421636.2421637</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2012a)</span>
<span class="ltx_bibblock">
Flavio Ribeiro, Dinei Florencio, Philip A Chou, and Zhengyou Zhang. 2012a.

</span>
<span class="ltx_bibblock">Auditory augmented reality: Object sonification for the visually impaired. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">2012 IEEE 14th international workshop on multimedia signal processing (MMSP)</em>. IEEE, 319–324.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2012b)</span>
<span class="ltx_bibblock">
Flávio Ribeiro, Dinei Florêncio, Philip A. Chou, and Zhengyou Zhang. 2012b.

</span>
<span class="ltx_bibblock">Auditory augmented reality: Object sonification for the visually impaired. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">2012 IEEE 14th International Workshop on Multimedia Signal Processing (MMSP)</em>. 319–324.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/MMSP.2012.6343462" title="">https://doi.org/10.1109/MMSP.2012.6343462</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roginska and Geluso (2017)</span>
<span class="ltx_bibblock">
Agnieszka Roginska and Paul Geluso. 2017.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Immersive Sound</em>.

</span>
<span class="ltx_bibblock">Focal Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roodaki et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Hessam Roodaki, Navid Navab, Abouzar Eslami, Christopher Stapleton, and Nassir Navab. 2017.

</span>
<span class="ltx_bibblock">SonifEye: Sonification of Visual Information Using Physical Modeling Sound Synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">IEEE Transactions on Visualization and Computer Graphics</em> 23, 11 (Nov. 2017), 2366–2371.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TVCG.2017.2734327" title="">https://doi.org/10.1109/TVCG.2017.2734327</a>
</span>
<span class="ltx_bibblock">Conference Name: IEEE Transactions on Visualization and Computer Graphics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rumiński (2015)</span>
<span class="ltx_bibblock">
Dariusz Rumiński. 2015.

</span>
<span class="ltx_bibblock">An experimental study of spatial sound usefulness in searching and navigating through AR environments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Virtual Reality</em> 19, 3-4 (2015), 223–233.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serafin et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Stefania Serafin, Michele Geronazzo, Cumhur Erkut, Niels C. Nilsson, and Rolf Nordahl. 2018.

</span>
<span class="ltx_bibblock">Sonic Interactions in Virtual Reality: State of the Art, Current Challenges, and Future Directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">IEEE Computer Graphics and Applications</em> 38, 2 (March 2018), 31–43.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/MCG.2018.193142628" title="">https://doi.org/10.1109/MCG.2018.193142628</a>
</span>
<span class="ltx_bibblock">Conference Name: IEEE Computer Graphics and Applications.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheffer and Adi (2023)</span>
<span class="ltx_bibblock">
Roy Sheffer and Yossi Adi. 2023.

</span>
<span class="ltx_bibblock">I Hear Your True Colors: Image Guided Audio Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. 1–5.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICASSP49357.2023.10096023" title="">https://doi.org/10.1109/ICASSP49357.2023.10096023</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">arXiv preprint arXiv:2303.17580</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sırakaya and Alsancak Sırakaya (2022)</span>
<span class="ltx_bibblock">
Mustafa Sırakaya and Didem Alsancak Sırakaya. 2022.

</span>
<span class="ltx_bibblock">Augmented reality in STEM education: A systematic review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Interactive Learning Environments</em> 30, 8 (2022), 1556–1569.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzuki et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ryo Suzuki, Rubaiat Habib Kazi, Li-yi Wei, Stephen DiVerdi, Wilmot Li, and Daniel Leithinger. 2020.

</span>
<span class="ltx_bibblock">RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</em> (Virtual Event, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib58.4.2">(UIST ’20)</em>. Association for Computing Machinery, New York, NY, USA, 166–181.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3379337.3415892" title="">https://doi.org/10.1145/3379337.3415892</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tahara et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tomu Tahara, Takashi Seno, Gaku Narita, and Tomoya Ishikawa. 2020.

</span>
<span class="ltx_bibblock">Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>. 249–255.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00072" title="">https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00072</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Technologies (2023)</span>
<span class="ltx_bibblock">
Unity Technologies. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Unity</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unity.com/" title="">https://unity.com/</a>
</span>
<span class="ltx_bibblock">Accessed on Oct 9th, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unity Technologies (2023)</span>
<span class="ltx_bibblock">
Unity Technologies. 2023.

</span>
<span class="ltx_bibblock">Getting Started with Unity MARS.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unity.com/products/mars/get-started" title="">https://unity.com/products/mars/get-started</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-02.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Upchurch and Niu (2022)</span>
<span class="ltx_bibblock">
Paul Upchurch and Ransen Niu. 2022.

</span>
<span class="ltx_bibblock">A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2207.10614" title="">http://arxiv.org/abs/2207.10614</a>
</span>
<span class="ltx_bibblock">arXiv:2207.10614 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023b.

</span>
<span class="ltx_bibblock">Visual chatgpt: Talking, drawing and editing with visual foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2303.04671</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023a.

</span>
<span class="ltx_bibblock">Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. 2023.

</span>
<span class="ltx_bibblock">Diffsound: Discrete diffusion model for text-to-sound generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L. Berg. 2018.

</span>
<span class="ltx_bibblock">Visual to Sound: Generating Natural Sound for Videos in the Wild. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
ZhiYing Zhou, Adrian David Cheok, Yan Qiu, and Xubo Yang. 2007.

</span>
<span class="ltx_bibblock">The Role of 3-D Sound in Human Reaction and Performance in Augmented Reality Environments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans</em> 37, 2 (March 2007), 262–272.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TSMCA.2006.886376" title="">https://doi.org/10.1109/TSMCA.2006.886376</a>
</span>
<span class="ltx_bibblock">Conference Name: IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix" style="color:#000000;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Task Prompts</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text" id="A1.p1.1.1" style="color:#000000;">Here we provide text prompts for tuning a GPT4.0 model into our sound acquisition process controller.
</span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text" id="A1.p1.1.2" style="color:#000000;">
</span>
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.p1.1.3" style="width:433.6pt;">
<span class="ltx_p" id="A1.p1.1.3.1"><span class="ltx_text ltx_font_bold" id="A1.p1.1.3.1.1" style="color:#000000;">System Context:</span><span class="ltx_text" id="A1.p1.1.3.1.2" style="color:#000000;"></span></span>
<span class="ltx_p" id="A1.p1.1.3.2"><span class="ltx_text" id="A1.p1.1.3.2.1" style="color:#000000;">You are an assistant helping user generate or retrieve matching sound assets for augmented reality events. You will be provided with the description of the event, including: who caused this event, what is this event, and what is the subject of this event, which can be null for some cases. Note that the description of the causer and subject of event can sometimes be long and you need to extract the important part of it to create concise replies. For example, when being told that the causing entity of the event is “metalball of a plastic slope with balls running on it” you should know that the only mattering keyword here is the metalball.</span></span>
<span class="ltx_p" id="A1.p1.1.3.3"><span class="ltx_text" id="A1.p1.1.3.3.1" style="color:#000000;">You have three methods to provide sound assets. Method 1 is recommending from a predefined list of sound assets, each with a name describing the content. You will recommend the best matching name based on event description. The full list of sound asset will be provided at the end of this context; Method 2 is an online sound retrieval API, for which you should generate a simplified text prompt to search with, preferably within 4 words; Method 3 is a diffusion model that takes text prompt and generate sound assets. This method has three functions: generating new sound file with text prompt, generating similar sound file based on an existing sound, and transferring an existing sound with a text prompt.</span></span>
<span class="ltx_p" id="A1.p1.1.3.4"><span class="ltx_text" id="A1.p1.1.3.4.1" style="color:#000000;">For each conversation, you will first provide a response about all three sound sources. User may ask following up questions in reply, then you just reply based on specific feedback.
When providing results, you should always follow a strict format starting and end with #. For method 1, you should reply the best matching sound asset in #method1:FILENAME# format. For method 2, you should reply in format of #method2:PROMPT#, please replace the PROMPT with your generated prompt. For method 3, when generating new sound, you should reply with #method3generation:PROMPT#, please replace the PROMPT with your generated prompt; when generating similar sound of a existing sound, reply with #method3similar#, when transferring an existing sound, reply with #method3transfer:PROMPT#, please replace the PROMPT with your generated prompt. Note that the hashtags need to be kept at both start and end of the result, the FILENAME need to be replaced by the exact name provided in the asset list, with no exception. The PROMPT should be replaced with the prompt you generate for these cases. Don’t explain why you have these prompts, just stricktly follow the formats.</span></span>
<span class="ltx_p" id="A1.p1.1.3.5"><span class="ltx_text" id="A1.p1.1.3.5.1" style="color:#000000;">Here I list names of sound assets to recommend from, each filename represents its content:</span></span>
<span class="ltx_p" id="A1.p1.1.3.6"><span class="ltx_text" id="A1.p1.1.3.6.1" style="color:#000000;">1. Knock surface</span></span>
<span class="ltx_p" id="A1.p1.1.3.7"><span class="ltx_text" id="A1.p1.1.3.7.1" style="color:#000000;">2. Sliding sound</span></span>
<span class="ltx_p" id="A1.p1.1.3.8"><span class="ltx_text" id="A1.p1.1.3.8.1" style="color:#000000;">3. Crash Bulb Break</span></span>
<span class="ltx_p" id="A1.p1.1.3.9"><span class="ltx_text ltx_font_italic" id="A1.p1.1.3.9.1" style="color:#000000;">¡… 32 assets in total…¿</span><span class="ltx_text" id="A1.p1.1.3.9.2" style="color:#000000;"></span></span>
</span><span class="ltx_text" id="A1.p1.1.4" style="color:#000000;">
</span><span class="ltx_text" id="A1.p1.1.5" style="color:#000000;">
</span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text" id="A1.p1.1.6" style="color:#000000;">We also prompt GPT4.0 to better parse the context information of specific AR events. We show one example event in the following prompt:
</span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text" id="A1.p1.1.7" style="color:#000000;">
</span>
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.p1.1.8" style="width:433.6pt;">
<span class="ltx_p" id="A1.p1.1.8.1"><span class="ltx_text ltx_font_bold" id="A1.p1.1.8.1.1" style="color:#000000;">Conversation Prompt:</span><span class="ltx_text" id="A1.p1.1.8.1.2" style="color:#000000;"></span></span>
<span class="ltx_p" id="A1.p1.1.8.2"><span class="ltx_text" id="A1.p1.1.8.2.1" style="color:#000000;">Please give me the results as hinted by context. For this specific event, the description of the AR event is:</span></span>
<span class="ltx_p" id="A1.p1.1.8.3"><span class="ltx_text" id="A1.p1.1.8.3.1" style="color:#000000;">The sound-producing AR event is </span><span class="ltx_text ltx_font_italic" id="A1.p1.1.8.3.2" style="color:#000000;">TapSurface</span><span class="ltx_text" id="A1.p1.1.8.3.3" style="color:#000000;">. The causing entity of this event is </span><span class="ltx_text ltx_font_italic" id="A1.p1.1.8.3.4" style="color:#000000;">a metal robot</span><span class="ltx_text" id="A1.p1.1.8.3.5" style="color:#000000;">. The target entity is </span><span class="ltx_text ltx_font_italic" id="A1.p1.1.8.3.6" style="color:#000000;">a wooden surface</span><span class="ltx_text" id="A1.p1.1.8.3.7" style="color:#000000;">.</span></span>
<span class="ltx_p" id="A1.p1.1.8.4"><span class="ltx_text" id="A1.p1.1.8.4.1" style="color:#000000;">You should reply top5 results with method 1, ensuring that the filename provided is exactly from the asset list. You should also give one result with method 3 transfer, with a reasonable prompt that could transfer an existing tapping sound to a more material-aware sound. For example, for a tapping on a wooden surface, you could use tapping wood as the prompt. Reply with the format of #method3transfer:PROMPT#, replacing the PROMPT with your generated prompt. You can add a ’high quality’ tag in the prompt to help improve the sound quality.</span></span>
</span><span class="ltx_text" id="A1.p1.1.9" style="color:#000000;">
</span><span class="ltx_text" id="A1.p1.1.10"></span></p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 12 02:43:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
