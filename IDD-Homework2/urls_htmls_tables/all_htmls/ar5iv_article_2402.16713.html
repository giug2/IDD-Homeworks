<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Sumedh Rasal
    <br class="ltx_break"/>
    Georgia Institute of Technology
    <br class="ltx_break"/>
    Chicago, IL
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id1.1.id1">
     srasal3@gatech.edu
    </span>
    <br class="ltx_break"/>
    <span class="ltx_ERROR undefined" id="id2.2.id2">
     \And
    </span>
    E. J. Hauer
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id3.3.id3">
     EJ@ThoughtCrafters.com
    </span>
    <br class="ltx_break"/>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id4.id1">
   Large Language Models (LLMs) have demonstrated remarkable capabilities in solving various tasks, yet they often struggle with comprehensively addressing complex and vague problems. Existing approaches, including multi-agent LLM systems, offer solutions to certain challenges but still require manual setup and lack scalability. To address this gap, we propose a novel approach leveraging decomposition to enable LLMs to tackle vague problems effectively.
  </p>
  <p class="ltx_p" id="id5.id2">
   Our approach involves an orchestrating LLM that interacts with users to understand the problem and then decomposes it into tangible sub-problems. Instead of expecting the LLM to solve the entire problem in one go, we train it to ask follow-up questions to gain a deeper understanding of the user’s requirements. Once the problem is adequately understood, the orchestrating LLM divides it into smaller, manageable sub-problems. Each sub-problem is then assigned to specialized LLM agents or non-LLM functions for resolution. These agents work in parallel to solve their respective sub-problems, with the orchestrating LLM overseeing the process and compiling the solutions into a comprehensive answer for the user. By adopting this decomposition approach, we alleviate the constraints imposed by token limitations on LLM outputs and empower them to provide nuanced solutions to complex and ambiguous problems.
  </p>
  <p class="ltx_p" id="id6.id3">
   Through our approach, we aim to enable LLMs to think and operate more like humans, breaking down complex problems into manageable parts and collaboratively solving them. This not only enhances the problem-solving capabilities of LLMs but also offers a scalable and efficient method for addressing a wide range of real-world challenges.
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <p class="ltx_p" id="p1.1">
   <em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.1.1">
    K
   </em>
   <span class="ltx_text ltx_font_bold" id="p1.1.2">
    eywords
   </span>
   Decomposition
   <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.1.m1.1">
    <semantics id="p1.1.m1.1a">
     <mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">
      ⋅
     </mo>
     <annotation-xml encoding="MathML-Content" id="p1.1.m1.1b">
      <ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">
       ⋅
      </ci>
     </annotation-xml>
     <annotation encoding="application/x-tex" id="p1.1.m1.1c">
      \cdot
     </annotation>
    </semantics>
   </math>
   Multi-Agents
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    In recent years, the proliferation of Large Language Models (LLMs) has revolutionized problem-solving across various domains
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx11" title="">
      Devlin et al., 2018
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx25" title="">
      Patil et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx32" title="">
      Touvron et al., 2023
     </a>
     ]
    </cite>
    . These models possess the ability to comprehend user queries and generate responses, often providing novel insights and solutions
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx33" title="">
      Wei et al., 2022a
     </a>
     ]
    </cite>
    . Notably, LLMs exhibit exceptional performance in tasks that can be tackled through zero-shot or few-shot training techniques
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx2" title="">
      Brown et al., 2020
     </a>
     ]
    </cite>
    , such as transforming questions into familiar formats or solving coding problems based on analogous examples
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx5" title="">
      Chen et al., 2021
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx29" title="">
      Sanh et al., 2021
     </a>
     ]
    </cite>
    . As such, they hold immense potential for automating numerous tasks and streamlining workflows across diverse professional domains
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx22" title="">
      Liu et al., 2023b
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx6" title="">
      Chiang and Lee, 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx13" title="">
      Gao et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx31" title="">
      Shen et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx37" title="">
      Zheng et al., 2023
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    However, despite their remarkable capabilities, LLMs are not immune to limitations. Instances of hallucination, contradictory responses, or misinterpretation of user queries highlight the challenges inherent in relying solely on these models for problem-solving
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx1" title="">
      Azamfirei et al., 2023
     </a>
     ]
    </cite>
    . Existing frameworks designed to aid users in systematically addressing novel problems are constrained by token limits imposed by LLM models
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx15" title="">
      Gur et al., 2023
     </a>
     ]
    </cite>
    . These limits restrict the number of words or tokens that an LLM can process in a single message or query, often hindering the comprehensiveness of responses and impeding the continuity of conversations. While approaches such as chaining previous conversations or employing semantic search techniques mitigate some of these challenges, they are susceptible to errors, including redundancy or irrelevance in retrieved information
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx24" title="">
      Ouyang et al., 2022
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx30" title="">
      Saunders et al., 2022
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx21" title="">
      Liu et al., 2023a
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To address these limitations, a promising avenue lies in leveraging multi-agent frameworks to tackle lengthy or complex problems
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx12" title="">
      Du et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx8" title="">
      Cohen et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx9" title="">
      Dafoe et al., 2021
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx10" title="">
      Dafoe et al., 2020
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx16" title="">
      Kramár et al., 2022
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx36" title="">
      Yao et al., 2022
     </a>
     ]
    </cite>
    . Such frameworks distribute problem-solving tasks among multiple LLM agents, offering a more robust approach compared to heuristic methods
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx19" title="">
      Li et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx20" title="">
      Liang et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx26" title="">
      Qian et al., 2023
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx17" title="">
      Lazaridou et al., 2020
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx14" title="">
      Graesser et al., 2020
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx18" title="">
      Lee et al., 2018
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx35" title="">
      Wu et al., 2023
     </a>
     ]
    </cite>
    . However, the manual assignment of personas or roles to each LLM agent presents a bottleneck, particularly when dealing with novel problems lacking predefined roles
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx28" title="">
      Rasal, 2024
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    In this paper, we propose a novel solution that integrates the strengths of existing approaches while addressing their limitations. Our approach centers on an orchestration strategy that employs problem decomposition to facilitate the seamless resolution of lengthy or complex problems. Inspired by human problem-solving approaches, our framework involves an orchestration LLM that engages in iterative conversations with users to ascertain problem requirements. Once the requirements are elucidated, the orchestration LLM decomposes the problem into manageable subproblems and assigns them to specialized agents, each tailored to address specific aspects of the problem. Notably, our approach incorporates non-LLM functionalities where necessary, enhancing robustness and adaptability to real-world tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx27" title="">
      Qin et al., 2023
     </a>
     ]
    </cite>
    . By leveraging problem decomposition and distributed task assignment, our framework offers a comprehensive and scalable solution for problem-solving with LLMs.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    This is what we propose:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       Our proposed solution integrates problem decomposition and multi-agent orchestration, inspired by human problem-solving approaches, to address the limitations of existing frameworks.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       By leveraging problem decomposition and distributed task assignment, our framework offers a comprehensive and scalable solution for problem-solving with LLMs, enhancing adaptability to real-world tasks.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Methodology
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    In response to these challenges, our methodology adopts a novel approach that seeks to address the token limit issue through algorithmic enhancements rather than relying solely on memory expansion. At the core of our methodology lies the development of an Orchestration LLM, tasked with the responsibility of comprehending user problem statements, decomposing them into manageable subproblems, and orchestrating interactions among specialized LLM agents.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    Inspired by human problem-solving strategies, our methodology mirrors the process of breaking down complex tasks into smaller, more digestible components. We employ advanced natural language processing (NLP) techniques to analyze and dissect complex queries, identifying distinct subproblems across various domains. These subproblems are then mapped to specialized LLM agents, each pre-trained on specific tasks such as data analysis, creative writing, or technical problem-solving, to ensure precision and relevance in their responses.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    Analogous to seeking expertise in real-life scenarios, our methodology leverages collaboration with domain experts to optimize problem-solving outcomes. For instance, just as individuals seek tax experts for efficient tax filing, our approach integrates specialized agents to tackle domain-specific challenges effectively.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    Furthermore, we develop an algorithmic framework that enables the Orchestration LLM to decompose complex problems based on predefined criteria such as problem domain, required expertise, and task complexity. Finally, we implement a task assignment system to allocate subproblems among selected specialized agents, considering factors such as availability, expertise, and workload to streamline the problem-solving process and optimize resource utilization.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p5">
   <p class="ltx_p" id="S2.p5.1">
    The high-level algorithm to our framework can be found
    <a class="ltx_ref" href="#alg1" title="Algorithm 1 ‣ 2 Methodology ‣ Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
   </p>
  </div>
  <figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_float">
     <span class="ltx_text ltx_font_bold" id="alg1.2.1.1">
      Algorithm 1
     </span>
    </span>
    Orchestration of Problem Solving with LLMs
   </figcaption>
   <div class="ltx_listing ltx_listing" id="alg1.3">
    <div class="ltx_listingline" id="alg1.l0">
     <span class="ltx_tag ltx_tag_listingline">
      0:
     </span>
     A complex problem statement
    </div>
    <div class="ltx_listingline" id="alg1.l0a">
     <span class="ltx_tag ltx_tag_listingline">
      0:
     </span>
     A comprehensive solution to the problem
    </div>
    <div class="ltx_listingline" id="alg1.l1">
     <span class="ltx_tag ltx_tag_listingline">
      1:
     </span>
     Begin
    </div>
    <div class="ltx_listingline" id="alg1.l2">
     <span class="ltx_tag ltx_tag_listingline">
      2:
     </span>
     Input the complex problem statement
    </div>
    <div class="ltx_listingline" id="alg1.l3">
     <span class="ltx_tag ltx_tag_listingline">
      3:
     </span>
     Decompose the problem into subproblems
    </div>
    <div class="ltx_listingline" id="alg1.l4">
     <span class="ltx_tag ltx_tag_listingline">
      4:
     </span>
     <span class="ltx_text ltx_font_bold" id="alg1.l4.1">
      for
     </span>
     each subproblem
     <span class="ltx_text ltx_font_bold" id="alg1.l4.2">
      do
     </span>
    </div>
    <div class="ltx_listingline" id="alg1.l5">
     <span class="ltx_tag ltx_tag_listingline">
      5:
     </span>
     Select a specialized LLM agent based on the subproblem’s domain and requirements
    </div>
    <div class="ltx_listingline" id="alg1.l6">
     <span class="ltx_tag ltx_tag_listingline">
      6:
     </span>
     Assign the subproblem to the selected agent
    </div>
    <div class="ltx_listingline" id="alg1.l7">
     <span class="ltx_tag ltx_tag_listingline">
      7:
     </span>
     Agent solves the subproblem and stores the solution
    </div>
    <div class="ltx_listingline" id="alg1.l8">
     <span class="ltx_tag ltx_tag_listingline">
      8:
     </span>
     <span class="ltx_text ltx_font_bold" id="alg1.l8.1">
      end
     </span>
     <span class="ltx_text ltx_font_bold" id="alg1.l8.2">
      for
     </span>
    </div>
    <div class="ltx_listingline" id="alg1.l9">
     <span class="ltx_tag ltx_tag_listingline">
      9:
     </span>
     Aggregate the solutions of subproblems into a coherent overall solution
    </div>
    <div class="ltx_listingline" id="alg1.l10">
     <span class="ltx_tag ltx_tag_listingline">
      10:
     </span>
     Output the comprehensive solution
    </div>
    <div class="ltx_listingline" id="alg1.l11">
     <span class="ltx_tag ltx_tag_listingline">
      11:
     </span>
     End
    </div>
   </div>
  </figure>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Decomposition Approach
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     The implementation of our framework initiates with the user’s submission of a complex problem statement, facilitated through a conversational interface within the LangChain framework
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bibx4" title="">
       Chase, 2022
      </a>
      ]
     </cite>
     . Within the Orchestration LLM, this input undergoes meticulous processing, wherein key information and requirements are extracted and analyzed with precision. Subsequently, the Orchestration LLM seamlessly applies a sophisticated decomposition algorithm, systematically breaking down the problem into manageable subproblems, thereby ensuring clarity and precision throughout the process.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     The decomposition approach was crafted through a series of few-shot training iterations employing the GPT4 model
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bibx23" title="">
       OpenAI, 2023
      </a>
      ]
     </cite>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bibx3" title="">
       Bubeck et al., 2023
      </a>
      ]
     </cite>
     . Initially, the LLM model decomposed the main problem into vague subproblems, which necessitated further refinement to ensure alignment with the expected decomposition format. Utilizing few-shot training, we provided the model with chain-of-thought type reasoning to guide its decomposition process effectively.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     Upon successful decomposition, the Orchestration LLM assumes the pivotal role of assigning these subproblems to the most suitable specialized LLM agents. This assignment process is meticulously orchestrated to ensure that each subproblem is delegated to an agent possessing the requisite expertise and capabilities to effectively address it.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p4">
    <p class="ltx_p" id="S2.SS1.p4.1">
     Central to the framework’s efficacy is the decomposition phase, which serves as its cornerstone. We deliberately crafted the initial design to be adaptable and generalizable across diverse problem domains, prioritizing the demonstration of decomposition capabilities over specialized use-case system development.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Experiments
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Our experiments center around the utilization of an orchestration LLM agent, employing OpenAI’s gpt-4 model
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx23" title="">
      OpenAI, 2023
     </a>
     ]
    </cite>
    , which adeptly engages with users to elucidate requirements and subsequently decomposes problems into manageable subproblems. This orchestration LLM also determines the most suitable specialized LLM agent to address each subproblem. Each specialized LLM agent employed in our experiments is based on OpenAI’s gpt3.5-turbo model, configured with a temperature parameter set to 0.0, reflecting the balance between coherence and creativity in responses. Facilitating communication among LLM agents is the LangChain framework, enabling seamless multi-agent interaction and facilitating focus on problem-solving.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Experiment 1: Travel Planning with LLM Agents
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     In this experiment, users present their travel plans to the framework, initiating a dialogue with the orchestration LLM to refine requirements and subsequently decompose the problem into subproblems. The orchestration LLM allocates tasks to specialized agents, which upon completion, contribute their findings for collation into a final course of action. Before execution, the orchestration LLM verifies details with the user to ensure accuracy.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     Here’s the conversational output of the framework.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <blockquote class="ltx_quote" id="S3.SS1.p3.1">
     <p class="ltx_p" id="S3.SS1.p3.1.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1.1">
       Initial Interaction: Gathering Requirements
      </span>
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.2">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.2.1">
       User
      </span>
      : I need to book a return flight from Chicago Airport to Newark Airport from 3/22/24 to 3/26/24. I want to fly in luxury and travel between the hours of 10 am and 4 pm.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.3">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.3.1">
       Orchestration LLM Agent
      </span>
      : Got it. To ensure I find the best options for you, could you specify your preferences for seating class and any additional services you might require during your flight?
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.4">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.4.1">
       User
      </span>
      : I prefer business class and would like access to in-flight Wi-Fi and a window seat.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.5">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.5.1">
       Orchestration LLM Agent Decomposes the Problem
      </span>
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.6">
      The Orchestration LLM Agent processes the user’s request and decomposes it into subproblems to be handled by specialized agents:
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.7">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.7.1">
       Flight Search Agent
      </span>
      : Find business class flights from Chicago to Newark that depart between 10 am and 4 pm on 3/22/24 and return flights on 3/26/24.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.8">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.8.1">
       Amenity Preferences Agent
      </span>
      : Identify airlines offering in-flight Wi-Fi and window seat availability in business class.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.9">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.9.1">
       Booking Agent
      </span>
      : Handle the booking process once the flight options are selected.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.10">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.10.1">
       Interaction with Specialized Agents
      </span>
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.11">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.11.1">
       Orchestration LLM Agent to Flight Search Agent
      </span>
      : Find business class flights from Chicago to Newark departing between 10 am and 4 pm on 3/22/24 and returning on 3/26/24.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.12">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.12.1">
       Flight Search Agent
      </span>
      : Found several options.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.13">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.13.1">
       Orchestration LLM Agent to Amenity Preferences Agent
      </span>
      : Which of these options offers in-flight Wi-Fi and window seats in business class?
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.14">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.14.1">
       Amenity Preferences Agent
      </span>
      : [Lists airlines that meet the criteria]
Finalizing the Option
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.15">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.15.1">
       Orchestration LLM Agent to User
      </span>
      : Based on your preferences, here are the best flight options that offer business class, in-flight Wi-Fi, and window seats. [Presents options]
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.16">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.16.1">
       User
      </span>
      : I’ll take the option with Airline X on the 3/22 at 2 pm.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.17">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.17.1">
       Orchestration LLM Agent to Booking Agent
      </span>
      : Please book the flight with Airline X for 3/22, departing at 2 pm in business class, with a window seat and in-flight Wi-Fi.
     </p>
     <p class="ltx_p" id="S3.SS1.p3.1.18">
      <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.18.1">
       Booking Agent
      </span>
      : Your flight is booked. Here are your confirmation details.
     </p>
    </blockquote>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Experiment 2: Exploratory Research and Summarization
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Users pose exploratory research queries to the framework, prompting the orchestration LLM to segment the problem into subproblems and assign tasks to specialized agents. Following the completion of tasks, the specialized agents provide their outputs, which are synthesized by the orchestration LLM into a comprehensive summary. Feedback from the user is sought to refine the final output as necessary.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     Here’s the conversational output of the framework.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p3">
    <blockquote class="ltx_quote" id="S3.SS2.p3.1">
     <p class="ltx_p" id="S3.SS2.p3.1.1">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1.1">
       Initial Interaction: Gathering Requirements
      </span>
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.2">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.2.1">
       User
      </span>
      : I need to research topics about long-term memory management in LLMs, focusing on what’s currently possible and what’s not. Then, I’d like a 5-page article summarizing these findings and suggesting future research directions.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.3">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.3.1">
       Orchestration LLM Agent Decomposes the Problem
      </span>
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.4">
      The Orchestration LLM Agent analyzes the request and identifies several key tasks:
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.5">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.5.1">
       Literature Review Agent
      </span>
      : Conduct a comprehensive review of existing research on long-term memory management in LLMs.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.6">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.6.1">
       Analysis Agent
      </span>
      : Analyze the collected research to distinguish between solved problems, ongoing challenges, and unexplored areas.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.7">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.7.1">
       Writing Agent
      </span>
      : Synthesize the findings into a coherent 5-page article, highlighting current capabilities, and limitations, and suggesting future research directions.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.8">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.8.1">
       Interaction with Specialized Agents
      </span>
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.9">
      Task 1: Conduct Literature Review
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.10">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.10.1">
       Orchestration LLM Agent to Literature Review Agent
      </span>
      : Identify and summarize key research articles on long-term memory management in LLMs, focusing on methodologies, findings, and limitations.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.11">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.11.1">
       Literature Review Agent
      </span>
      : [Returns a summary of articles, including key methodologies, findings, challenges, and limitations related to long-term memory management.]
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.12">
      Task 2: Analyze Research
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.13">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.13.1">
       Orchestration LLM Agent to Analysis Agent
      </span>
      : Based on the literature review, analyze the current state of long-term memory management in LLMs, identifying what is possible, what remains challenging, and potential areas for future research.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.14">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.14.1">
       Analysis Agent
      </span>
      : [Provides an analysis distinguishing between the current capabilities and limitations of long-term memory management in LLMs, along with suggestions for future research directions.]
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.15">
      Task 3: Write Article
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.16">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.16.1">
       Orchestration LLM Agent to Writing Agent
      </span>
      : Using the information collected and analyzed, write a 5-page article summarizing the findings on long-term memory management in LLMs. Highlight what is currently possible, and the limitations, and suggest ideas for future research.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.17">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.17.1">
       Writing Agent
      </span>
      : [Drafts a 5-page article that summarizes the state of long-term memory management in LLMs, including an overview of existing research, current limitations, and proposed future directions.]
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.18">
      Finalizing the Article
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.19">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.19.1">
       Orchestration LLM Agent to User
      </span>
      : Based on the comprehensive research and analysis conducted by specialized agents, we have compiled a 5-page article summarizing the current state of long-term memory management in LLMs. The article discusses what is currently possible, outlines the main challenges and limitations, and suggests potential areas for future research.
     </p>
     <p class="ltx_p" id="S3.SS2.p3.1.20">
      <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.20.1">
       User
      </span>
      : Reviews the article and provides feedback or requests revisions if necessary.
     </p>
    </blockquote>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Results
  </h2>
  <div class="ltx_para ltx_noindent" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    Rather than relying solely on conventional datasets typically utilized in research, we opted to assess the efficacy of our framework using the GSM8K dataset, chosen to specifically demonstrate the capabilities of our approach. We employed four distinct types of agents in our experimentation: a single LLM agent powered by the GPT3.5-turbo model, two LLM agents supported by GPT3.5-turbo models, two LLM agents utilizing the GPT3.5-turbo model combined with the chain-of-thought and persona approach
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx34" title="">
      Wei et al., 2022b
     </a>
     ]
    </cite>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx28" title="">
      Rasal, 2024
     </a>
     ]
    </cite>
    , and finally, two LLM agents operating with the GPT4 model
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bibx23" title="">
      OpenAI, 2023
     </a>
     ]
    </cite>
    for orchestration and decomposition methodology and GPT3.5-turbo model for the specialized LLM agents, which involves breaking down the primary problem into subproblems. Our orchestration framework represents a notable advancement over existing methodologies, with detailed results presented in Table
    <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S4.p2">
   <blockquote class="ltx_quote" id="S4.p2.1">
    <p class="ltx_p" id="S4.p2.1.1">
     "GSM8K consists of 8.5K high-quality grade school math problems created by human problem writers. We segmented these into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ - / *) to reach the final answer. A bright middle school student should be able to solve every problem."
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bibx7" title="">
       Cobbe et al., 2021
      </a>
      ]
     </cite>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para ltx_noindent" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    Furthermore, our observations indicated a significant enhancement in accuracy when employing a calculator function as opposed to a math-specialized LLM agent. This improvement stems from the inherent limitations of LLMs in executing mathematical operations, despite their versatility across various domains. We contend that rather than training LLMs for math operations, our framework facilitates the orchestration LLM in discerning whether a subproblem necessitates an LLM agent or an alternative non-LLM approach.
   </p>
  </div>
  <figure class="ltx_table" id="S4.T1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_table">
     Table 1:
    </span>
    GSM8K Data set Evaluation
   </figcaption>
   <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="S4.T1.1.1.1">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1">
       Agent
      </th>
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">
       Solve Rate
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="S4.T1.1.2.1">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.1">
       Single GPT3.5-turbo
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.2">
       50%
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.3.2">
      <td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.1">
       Multi-Agent GPT3.5-turbo
      </td>
      <td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.2">
       55%
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.4.3">
      <td class="ltx_td ltx_align_left" id="S4.T1.1.4.3.1">
       Multi-Agent GPT3.5-turbo (LLM-Harmony)
      </td>
      <td class="ltx_td ltx_align_left" id="S4.T1.1.4.3.2">
       65%
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.5.4">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.5.4.1">
       Multi-Agent GPT4
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.1.1">
        (Our Approach)
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.5.4.2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.2.1">
        73%
       </span>
      </td>
     </tr>
    </tbody>
   </table>
  </figure>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Limitations
  </h2>
  <div class="ltx_para ltx_noindent" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    While our decomposition framework represents a significant advancement, it possesses inherent limitations that warrant consideration. Foremost among these limitations is the framework’s lack of generalization to address all conceivable problem scenarios, both novel and otherwise. We recognize the necessity of further training the orchestration model, essentially serving as a foundational Large Language Model (LLM), to acquire a comprehensive understanding of diverse problem-solving approaches. Presently, the model exhibits proficiency in decomposing various problems, yet its scope may not extend to accommodate the complexities of future challenges.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    For instance, consider a user’s request for transportation. While the current model adeptly decomposes the problem into a specialized agent capable of booking a ride, scenarios may arise where multiple specialized agents possess this capability. In such instances, the orchestration LLM faces the challenge of determining the most appropriate course of action. Additionally, the framework lacks a mechanism to assimilate information about new specialized agents, highlighting the need for a retraining mechanism to continually update the orchestration LLM’s knowledge base.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    Furthermore, while the current model effectively decomposes problems, it lacks the capability to iteratively learn and refine its problem-solving strategies based on the outcomes of decomposition steps. Addressing this limitation represents a key focus of our future research endeavors. Nonetheless, the primary objective of this paper is to disseminate our decomposition approach to the readership, offering insights into its application in problem-solving contexts.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para ltx_noindent" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In conclusion, our paper introduces a novel approach to problem-solving using a decomposition framework orchestrated by Large Language Models (LLMs). Through meticulous decomposition of complex problem statements into manageable subproblems, our framework demonstrates the potential to streamline problem-solving processes and mitigate the limitations inherent in current LLM-based solutions. By leveraging the inherent capabilities of LLMs to understand user queries and decompose problems, our approach offers a promising avenue for addressing a wide range of challenges across various domains.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    Our implementation of the framework highlights its efficacy in breaking down intricate problem statements and assigning tasks to specialized LLM agents, resulting in coherent and comprehensive solutions. Moreover, our experiments underscore the versatility of the framework in facilitating tasks such as travel planning and research summary generation, showcasing its applicability in real-world scenarios.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    However, our work also reveals several limitations that necessitate further research and development. While the decomposition framework represents a significant advancement, its generalization to novel problem scenarios remains an ongoing challenge. Additionally, the framework lacks mechanisms for adaptive learning and integration of new specialized agents, indicating avenues for future improvement.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p4">
   <p class="ltx_p" id="S6.p4.1">
    Despite these limitations, our paper contributes to the growing body of literature on LLM-based problem-solving approaches by introducing a systematic and scalable framework. By sharing our approach and insights, we aim to stimulate further research and innovation in this domain, paving the way for enhanced problem-solving capabilities and the realization of artificial general intelligence.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bibx1">
    <span class="ltx_tag ltx_tag_bibitem">
     [Azamfirei et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Azamfirei, R., Kudchadkar, S. R., and Fackler, J. (2023).
    </span>
    <span class="ltx_bibblock">
     Large language models and the perils of their hallucinations.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">
      Critical Care
     </span>
     , 27(1):1–2.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx2">
    <span class="ltx_tag ltx_tag_bibitem">
     [Brown et al., 2020]
    </span>
    <span class="ltx_bibblock">
     Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020).
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">
      Advances in neural information processing systems
     </span>
     , 33:1877–1901.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx3">
    <span class="ltx_tag ltx_tag_bibitem">
     [Bubeck et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. (2023).
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with gpt-4.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">
      arXiv preprint arXiv:2303.12712
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx4">
    <span class="ltx_tag ltx_tag_bibitem">
     [Chase, 2022]
    </span>
    <span class="ltx_bibblock">
     Chase, H. (2022).
    </span>
    <span class="ltx_bibblock">
     Langchain, october 2022.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">
      URL https://github. com/langchain-ai/langchain
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx5">
    <span class="ltx_tag ltx_tag_bibitem">
     [Chen et al., 2021]
    </span>
    <span class="ltx_bibblock">
     Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021).
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">
      arXiv preprint arXiv:2107.03374
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx6">
    <span class="ltx_tag ltx_tag_bibitem">
     [Chiang and Lee, 2023]
    </span>
    <span class="ltx_bibblock">
     Chiang, C.-H. and Lee, H.-y. (2023).
    </span>
    <span class="ltx_bibblock">
     Can large language models be an alternative to human evaluations?
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">
      arXiv preprint arXiv:2305.01937
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx7">
    <span class="ltx_tag ltx_tag_bibitem">
     [Cobbe et al., 2021]
    </span>
    <span class="ltx_bibblock">
     Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. (2021).
    </span>
    <span class="ltx_bibblock">
     Training verifiers to solve math word problems.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">
      arXiv preprint arXiv:2110.14168
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx8">
    <span class="ltx_tag ltx_tag_bibitem">
     [Cohen et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Cohen, R., Hamri, M., Geva, M., and Globerson, A. (2023).
    </span>
    <span class="ltx_bibblock">
     Lm vs lm: Detecting factual errors via cross examination.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">
      arXiv preprint arXiv:2305.13281
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx9">
    <span class="ltx_tag ltx_tag_bibitem">
     [Dafoe et al., 2021]
    </span>
    <span class="ltx_bibblock">
     Dafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., and Graepel, T. (2021).
    </span>
    <span class="ltx_bibblock">
     Cooperative ai: machines must learn to find common ground.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">
      Nature
     </span>
     , 593(7857):33–36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx10">
    <span class="ltx_tag ltx_tag_bibitem">
     [Dafoe et al., 2020]
    </span>
    <span class="ltx_bibblock">
     Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee, K. R., Leibo, J. Z., Larson, K., and Graepel, T. (2020).
    </span>
    <span class="ltx_bibblock">
     Open problems in cooperative ai.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">
      arXiv preprint arXiv:2012.08630
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx11">
    <span class="ltx_tag ltx_tag_bibitem">
     [Devlin et al., 2018]
    </span>
    <span class="ltx_bibblock">
     Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
    </span>
    <span class="ltx_bibblock">
     Bert: Pre-training of deep bidirectional transformers for language understanding.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">
      arXiv preprint arXiv:1810.04805
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx12">
    <span class="ltx_tag ltx_tag_bibitem">
     [Du et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. (2023).
    </span>
    <span class="ltx_bibblock">
     Improving factuality and reasoning in language models through multiagent debate.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">
      arXiv preprint arXiv:2305.14325
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx13">
    <span class="ltx_tag ltx_tag_bibitem">
     [Gao et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Gao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan, X. (2023).
    </span>
    <span class="ltx_bibblock">
     Human-like summarization evaluation with chatgpt.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">
      arXiv preprint arXiv:2304.02554
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx14">
    <span class="ltx_tag ltx_tag_bibitem">
     [Graesser et al., 2020]
    </span>
    <span class="ltx_bibblock">
     Graesser, L., Cho, K., and Kiela, D. (2020).
    </span>
    <span class="ltx_bibblock">
     Emergent linguistic phenomena in multi-agent communication games.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx15">
    <span class="ltx_tag ltx_tag_bibitem">
     [Gur et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Gur, I., Furuta, H., Huang, A., Safdari, M., Matsuo, Y., Eck, D., and Faust, A. (2023).
    </span>
    <span class="ltx_bibblock">
     A real-world webagent with planning, long context understanding, and program synthesis.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">
      arXiv preprint arXiv:2307.12856
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx16">
    <span class="ltx_tag ltx_tag_bibitem">
     [Kramár et al., 2022]
    </span>
    <span class="ltx_bibblock">
     Kramár, J., Eccles, T., Gemp, I., Tacchetti, A., McKee, K. R., Malinowski, M., Graepel, T., and Bachrach, Y. (2022).
    </span>
    <span class="ltx_bibblock">
     Negotiation and honesty in artificial intelligence methods for the board game of diplomacy.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">
      Nature Communications
     </span>
     , 13(1):7214.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx17">
    <span class="ltx_tag ltx_tag_bibitem">
     [Lazaridou et al., 2020]
    </span>
    <span class="ltx_bibblock">
     Lazaridou, A., Potapenko, A., and Tieleman, O. (2020).
    </span>
    <span class="ltx_bibblock">
     Multi-agent communication meets natural language: Synergies between functional and structural language learning.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx18">
    <span class="ltx_tag ltx_tag_bibitem">
     [Lee et al., 2018]
    </span>
    <span class="ltx_bibblock">
     Lee, J., Cho, K., Weston, J., and Kiela, D. (2018).
    </span>
    <span class="ltx_bibblock">
     Emergent translation in multi-agent communication.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx19">
    <span class="ltx_tag ltx_tag_bibitem">
     [Li et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. (2023).
    </span>
    <span class="ltx_bibblock">
     Camel: Communicative agents for" mind" exploration of large scale language model society.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">
      arXiv preprint arXiv:2303.17760
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx20">
    <span class="ltx_tag ltx_tag_bibitem">
     [Liang et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S. (2023).
    </span>
    <span class="ltx_bibblock">
     Encouraging divergent thinking in large language models through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">
      arXiv preprint arXiv:2305.19118
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx21">
    <span class="ltx_tag ltx_tag_bibitem">
     [Liu et al., 2023a]
    </span>
    <span class="ltx_bibblock">
     Liu, J., Xia, C. S., Wang, Y., and Zhang, L. (2023a).
    </span>
    <span class="ltx_bibblock">
     Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">
      arXiv preprint arXiv:2305.01210
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx22">
    <span class="ltx_tag ltx_tag_bibitem">
     [Liu et al., 2023b]
    </span>
    <span class="ltx_bibblock">
     Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C. (2023b).
    </span>
    <span class="ltx_bibblock">
     Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">
      arXiv preprint arXiv:2303.16634
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx23">
    <span class="ltx_tag ltx_tag_bibitem">
     [OpenAI, 2023]
    </span>
    <span class="ltx_bibblock">
     OpenAI (2023).
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report. arxiv 2303.08774.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">
      View in Article
     </span>
     , 2:13.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx24">
    <span class="ltx_tag ltx_tag_bibitem">
     [Ouyang et al., 2022]
    </span>
    <span class="ltx_bibblock">
     Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022).
    </span>
    <span class="ltx_bibblock">
     Training language models to follow instructions with human feedback, 2022.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">
      URL https://arxiv. org/abs/2203.02155
     </span>
     , 13.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx25">
    <span class="ltx_tag ltx_tag_bibitem">
     [Patil et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. (2023).
    </span>
    <span class="ltx_bibblock">
     Gorilla: Large language model connected with massive apis.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">
      arXiv preprint arXiv:2305.15334
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx26">
    <span class="ltx_tag ltx_tag_bibitem">
     [Qian et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu, Z., and Sun, M. (2023).
    </span>
    <span class="ltx_bibblock">
     Communicative agents for software development.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">
      arXiv preprint arXiv:2307.07924
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx27">
    <span class="ltx_tag ltx_tag_bibitem">
     [Qin et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. (2023).
    </span>
    <span class="ltx_bibblock">
     Toolllm: Facilitating large language models to master 16000+ real-world apis.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">
      arXiv preprint arXiv:2307.16789
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx28">
    <span class="ltx_tag ltx_tag_bibitem">
     [Rasal, 2024]
    </span>
    <span class="ltx_bibblock">
     Rasal, S. (2024).
    </span>
    <span class="ltx_bibblock">
     Llm harmony: Multi-agent communication for problem solving.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">
      arXiv preprint arXiv:2401.01312
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx29">
    <span class="ltx_tag ltx_tag_bibitem">
     [Sanh et al., 2021]
    </span>
    <span class="ltx_bibblock">
     Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. (2021).
    </span>
    <span class="ltx_bibblock">
     Multitask prompted training enables zero-shot task generalization.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx29.1.1">
      arXiv preprint arXiv:2110.08207
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx30">
    <span class="ltx_tag ltx_tag_bibitem">
     [Saunders et al., 2022]
    </span>
    <span class="ltx_bibblock">
     Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. (2022).
    </span>
    <span class="ltx_bibblock">
     Self-critiquing models for assisting human evaluators.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">
      arXiv preprint arXiv:2206.05802
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx31">
    <span class="ltx_tag ltx_tag_bibitem">
     [Shen et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Shen, C., Cheng, L., You, Y., and Bing, L. (2023).
    </span>
    <span class="ltx_bibblock">
     Are large language models good evaluators for abstractive summarization?
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">
      arXiv preprint arXiv:2305.13091
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx32">
    <span class="ltx_tag ltx_tag_bibitem">
     [Touvron et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023).
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">
      arXiv preprint arXiv:2302.13971
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx33">
    <span class="ltx_tag ltx_tag_bibitem">
     [Wei et al., 2022a]
    </span>
    <span class="ltx_bibblock">
     Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. (2022a).
    </span>
    <span class="ltx_bibblock">
     Emergent abilities of large language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx33.1.1">
      arXiv preprint arXiv:2206.07682
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx34">
    <span class="ltx_tag ltx_tag_bibitem">
     [Wei et al., 2022b]
    </span>
    <span class="ltx_bibblock">
     Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022b).
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx34.1.1">
      Advances in Neural Information Processing Systems
     </span>
     , 35:24824–24837.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx35">
    <span class="ltx_tag ltx_tag_bibitem">
     [Wu et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Wu, N., Gong, M., Shou, L., Liang, S., and Jiang, D. (2023).
    </span>
    <span class="ltx_bibblock">
     Large language models are diverse role-players for summarization evaluation.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx35.1.1">
      arXiv preprint arXiv:2303.15078
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx36">
    <span class="ltx_tag ltx_tag_bibitem">
     [Yao et al., 2022]
    </span>
    <span class="ltx_bibblock">
     Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2022).
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx36.1.1">
      arXiv preprint arXiv:2210.03629
     </span>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bibx37">
    <span class="ltx_tag ltx_tag_bibitem">
     [Zheng et al., 2023]
    </span>
    <span class="ltx_bibblock">
     Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023).
    </span>
    <span class="ltx_bibblock">
     Judging llm-as-a-judge with mt-bench and chatbot arena.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bibx37.1.1">
      arXiv preprint arXiv:2306.05685
     </span>
     .
    </span>
   </li>
  </ul>
 </section>
</article>
