<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis</title>
<!--Generated on Wed Sep 25 12:15:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Biomedical image segmentation Image analysis Transformers Deep Learning" lang="en" name="keywords"/>
<base href="/html/2409.16940v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S1" title="In Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S2" title="In Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3" title="In Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS1" title="In 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS2" title="In 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Segmentation Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS2.SSS1" title="In 3.2 Segmentation Models ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>U-Net</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS2.SSS2" title="In 3.2 Segmentation Models ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>UNETR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS2.SSS3" title="In 3.2 Segmentation Models ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Segment Anything Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS2.SSS4" title="In 3.2 Segmentation Models ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Swin-UPerNet</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS3" title="In 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Swin-UPerNet Modifications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS3.SSS1" title="In 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Swin-S-PS2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS3.SSS2" title="In 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Swin-S-Conv</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS3.SSS3" title="In 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Swin-S-TB</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS3.SSS4" title="In 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Swin-S-TB-Skip</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS3.SSS5" title="In 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.5 </span>Swin-S-Pyramid</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS4" title="In 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Training Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS4.SSS1" title="In 3.4 Training Pipeline ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Data Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS4.SSS2" title="In 3.4 Training Pipeline ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS4.SSS3" title="In 3.4 Training Pipeline ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS5" title="In 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Computational Resourses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4" title="In Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.SS1" title="In 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Comparison of Transformer-based Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.SS2" title="In 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comparison of Swin-UPerNet Modifications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.SS3" title="In 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>FLOPs and Parameters of the Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S5" title="In Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S6" title="In Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
<sup class="ltx_sup" id="id1.1">1</sup>Institute of Computer Science, University of Tartu
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{illia.tsiporenko, pavel.chizhov, dmytro.fishman}@ut.ee</span></span></span>
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ut.ee" title="">https://ut.ee</a>
<br class="ltx_break"/><sup class="ltx_sup" id="id1.3">2</sup>Center for Artificial Intelligence, Technical University of Applied Sciences Würzburg-Schweinfurt
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.4"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>pavel.chizhov@thws.de</span></span></span>
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://thws.de" title="">https://thws.de</a>
<br class="ltx_break"/><sup class="ltx_sup" id="id1.5">3</sup>STACC, Estonia, Tartu 
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stacc.ee/" title="">https://stacc.ee/</a>
<br class="ltx_break"/></span></span></span>
<h1 class="ltx_title ltx_title_document">Going Beyond U-Net: Assessing Vision
Transformers for Semantic Segmentation
in Microscopy Image Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Illia Tsiporenko<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0009-0009-0404-0679
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pavel Chizhov<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0009-0001-7329-6899
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dmytro Fishman<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-4644-8893
</span><span class="ltx_author_notes">11 3 3</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Segmentation is a crucial step in microscopy image analysis. Numerous approaches have been developed over the past years, ranging from classical segmentation algorithms to advanced deep learning models. While U-Net remains one of the most popular and well-established models for biomedical segmentation tasks, recently developed transformer-based models promise to enhance the segmentation process of microscopy images. In this work, we assess the efficacy of transformers, including UNETR, the Segment Anything Model, and Swin-UPerNet, and compare them with the well-established U-Net model across various image modalities such as electron microscopy, brightfield, histopathology, and phase-contrast. Our evaluation identifies several limitations in the original Swin Transformer model, which we address through architectural modifications to optimise its performance. The results demonstrate that these modifications improve segmentation performance compared to the classical U-Net model and the unmodified Swin-UPerNet. This comparative analysis highlights the promise of transformer models for advancing biomedical image segmentation. It demonstrates that their efficiency and applicability can be improved with careful modifications, facilitating their future use in microscopy image analysis tools.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Biomedical image segmentation Image analysis Transformers Deep Learning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Identifying objects in microscopy images is a crucial first step in successful image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib11" title="">11</a>]</cite>. Precise segmentation of various cellular structures, including nuclei, enables the extraction and analysis of vital morphological features. However, achieving accurate and efficient segmentation remains challenging due to the complex and heterogeneous nature of microscopy data.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Deep learning algorithms are powerful tools for segmentation tasks, given their ability to generalise and understand underlying image structures. For a long time, the traditional Convolutional Neural Network (CNN) U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib24" title="">24</a>]</cite> has been one of the most popular and well-established models in this field, demonstrating notable results in various microscopy image segmentation tasks. However, many new deep learning models have been developed over the past few years, with transformers among the most promising. Transformers use the attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib28" title="">28</a>]</cite> at their core, allowing them to capture complex image structures, provide an unlimited receptive field, and incorporate more local context than traditional CNNs. These features are particularly advantageous for enhancing the segmentation process of microscopy images, where capturing local context is essential for improving the finer details in segmentation masks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper provides an assessment of segmentation models, which utilise some of the most popular vision transformers as image encoders — Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib8" title="">8</a>]</cite> (ViT), present in the UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib13" title="">13</a>]</cite> model, and Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite>, present in the Swin-UPerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite> model. Additionally, we assess the novel foundational Segment Anything Model (SAM), which uses user-defined prompts to enhance the segmentation process. We compare these models to the robust and lightweight U-Net model, which serves as our baseline.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Swin Transformer in combination with UPerNet-based decoder demonstrated promising performance in semantic image segmenation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite>. However, the model’s reliance on processing image patches of size 4 inevitably leads to the loss of fine-grained details. This loss of low-level details, coupled with the use of bilinear interpolation in the decoder, may compromise the overall performance of the model by reducing the precision of the segmentation boundaries and affecting the accuracy of object delineation. In our work, we aim to address this issue by designing encoder and decoder enhancements to introduce local context and improve the flexibility of mask generation, thereby improving detail capture and segmentation accuracy.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">By studying the capabilities of these transformer-based models, we aim to highlight their potential advantages and drawbacks compared to the traditional U-Net model. In our comparative analysis, we seek to demonstrate the promise of transformers in advancing microscopy image segmentation.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">While U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib24" title="">24</a>]</cite> remains one of the most popular models for segmentation tasks in the biomedical domain, recent years have seen the development of many new transformer-based models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib25" title="">25</a>]</cite>. These models can be roughly divided into two broad categories: transformer-CNN and hybrid models. Transformer-CNN models use transformers as the primary image encoder, while CNN layers in the decoder generate the segmentation masks. Examples include UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib13" title="">13</a>]</cite>, Swin UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib12" title="">12</a>]</cite>, Swin-UPerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite>, and SETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib31" title="">31</a>]</cite>. On the other hand, hybrid models utilize both CNN and transformer layers in the encoder but retain CNN layers in the decoder. An example of this model type are TransUNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib5" title="">5</a>]</cite>, SU-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib10" title="">10</a>]</cite>, and CS-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Even though hybrid models are more flexible in design and allow for more architectural experiments, one major advantage of Transformer-CNN models is the use of intact transformer encoders pre-trained on large datasets like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib7" title="">7</a>]</cite>. Such models generally show superior performance over hybrid ones, as the improvement coming from transformers is mostly related to large and diverse pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite>. This difference renders hybrid models less preferable, thus we decided to omit them in our experiments.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">There is also a separate category of models that have been recently gaining popularity — foundational models. These models are typically trained on massive datasets and offer zero-shot generalisation. Such a model was recently introduced for image segmentation — Segment Anything Model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib17" title="">17</a>]</cite> (SAM). SAM uses user-defined prompts, such as bounding boxes or points, to guide and improve the segmentation process.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">As the Swin Transformer has demonstrated superior performance in many imaging tasks, numerous new re developed using Swin as a basis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib4" title="">4</a>]</cite>. Swin-UPerNet was the first model that used Swin as the encoder in combination with the UPerNet decoder. Following its success, many other segmentation models that employ Swin as the backbone were developed. Some propose different types of decoders compared to the original Swin-UPerNet, such as Swin UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib12" title="">12</a>]</cite> and SSformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib25" title="">25</a>]</cite>, while others follow the idea of hybrid models such as CS-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib1" title="">1</a>]</cite>, where both the encoder and decoder are revised. However, to the best of our knowledge, there is a notable gap in research on the original Swin-UPerNet, with opportunities for enhancing this model. Thus, in this work, we explore Swin-UPerNet, identify its potential limitations, and address them through custom modifications. These modifications, while greatly improving its performance, preserve the original architecture of the model, enabling the reuse of the pre-trained weights, which improves the convergence of the loss during training and enhances the overall performance of the model.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our work aims to comprehensively compare the well-established U-Net model and notable transformer-based models, including UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib13" title="">13</a>]</cite>, Swin-UPerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite>, and the Segment Anything Model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib17" title="">17</a>]</cite>, specifically within the microscopy domain. Additionally, we design custom modifications for Swin-UPerNet to enhance its performance in microscopy image segmentation tasks. In this section, we will describe the datasets, the configuration of the models, and the approaches for training and evaluation.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.5.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.6.2" style="font-size:90%;">Detailed overview of datasets used in the study. Here, we detail the number of images present in each dataset, the resolution and the number of channels of each image in the dataset, the segmentation target and the modality of the images.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.4.1.1" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.4.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.4.1.2" style="padding-left:7.1pt;padding-right:7.1pt;">Images</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.4.1.3" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.4.1.3.1">Resolution</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.4.1.4" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.4.1.4.1">Channels</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.4.1.5" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.4.1.5.1">Target</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.4.1.6" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.4.1.6.1">Modality</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5.2">
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.2.1" style="padding-left:7.1pt;padding-right:7.1pt;">(Train / Test)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2" style="padding-left:7.1pt;padding-right:7.1pt;">Seven</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.3" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.1.1.3.1">2016 / 504</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.1.1.1.1"><math alttext="1080\times 1080" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml">1080</mn><mo id="S3.T1.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><times id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.1"></times><cn id="S3.T1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.1.1.2">1080</cn><cn id="S3.T1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">1080\times 1080</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">1080 × 1080</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.4" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.1.1.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.5" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.1.1.5.1">Nuclei</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.6" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.1.1.6.1">Brightfield</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.6.3">
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.3.1" style="padding-left:7.1pt;padding-right:7.1pt;">Cell Lines</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.7.4.1" style="padding-left:7.1pt;padding-right:7.1pt;">Electron</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.7.4.2" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.7.4.2.1">366 / 99</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.7.4.3" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.7.4.3.1">Varies</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.7.4.4" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.7.4.4.1">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.7.4.5" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.3.7.4.5.1">Varies</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.7.4.6" style="padding-left:7.1pt;padding-right:7.1pt;">Electron</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.8.5">
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.5.1" style="padding-left:7.1pt;padding-right:7.1pt;">Microscopy</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.5.2" style="padding-left:7.1pt;padding-right:7.1pt;">Microscopy</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.2.2.2.1">LIVECell</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.3" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.2.2.3.1">3253 / 1986</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.1" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.2.2.1.1"><math alttext="768\times 512" class="ltx_Math" display="inline" id="S3.T1.2.2.1.1.m1.1"><semantics id="S3.T1.2.2.1.1.m1.1a"><mrow id="S3.T1.2.2.1.1.m1.1.1" xref="S3.T1.2.2.1.1.m1.1.1.cmml"><mn id="S3.T1.2.2.1.1.m1.1.1.2" xref="S3.T1.2.2.1.1.m1.1.1.2.cmml">768</mn><mo id="S3.T1.2.2.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.2.2.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.2.2.1.1.m1.1.1.3" xref="S3.T1.2.2.1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.1.m1.1b"><apply id="S3.T1.2.2.1.1.m1.1.1.cmml" xref="S3.T1.2.2.1.1.m1.1.1"><times id="S3.T1.2.2.1.1.m1.1.1.1.cmml" xref="S3.T1.2.2.1.1.m1.1.1.1"></times><cn id="S3.T1.2.2.1.1.m1.1.1.2.cmml" type="integer" xref="S3.T1.2.2.1.1.m1.1.1.2">768</cn><cn id="S3.T1.2.2.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.2.2.1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.1.m1.1c">768\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.1.m1.1d">768 × 512</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.4" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T1.2.2.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.5" style="padding-left:7.1pt;padding-right:7.1pt;">Individual</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.6" style="padding-left:7.1pt;padding-right:7.1pt;">Phase</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.9.6">
<td class="ltx_td ltx_align_center" id="S3.T1.3.9.6.1" style="padding-left:7.1pt;padding-right:7.1pt;">Cells</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.9.6.2" style="padding-left:7.1pt;padding-right:7.1pt;">Contrast</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.3.2" style="padding-left:7.1pt;padding-right:7.1pt;">MoNuSeg</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.3.3" style="padding-left:7.1pt;padding-right:7.1pt;">250 / 140</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.3.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mrow id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml"><mn id="S3.T1.3.3.1.m1.1.1.2" xref="S3.T1.3.3.1.m1.1.1.2.cmml">512</mn><mo id="S3.T1.3.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.3.3.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.3.3.1.m1.1.1.3" xref="S3.T1.3.3.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><apply id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"><times id="S3.T1.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1.1"></times><cn id="S3.T1.3.3.1.m1.1.1.2.cmml" type="integer" xref="S3.T1.3.3.1.m1.1.1.2">512</cn><cn id="S3.T1.3.3.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.3.3.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">512 × 512</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.3.4" style="padding-left:7.1pt;padding-right:7.1pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.3.5" style="padding-left:7.1pt;padding-right:7.1pt;">Nuclei</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.3.6" style="padding-left:7.1pt;padding-right:7.1pt;">Histopathology</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">To assess the performance and capabilities of those models, we chose four different datasets, each representing a distinct image modality, offering unique segmentation challenges. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.T1" style="color:#FF0000;" title="Table 1 ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">1</span></a> provides a detailed overview of the chosen datasets and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.F1" style="color:#FF0000;" title="Figure 1 ‣ 3.1 Datasets ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">1</span></a> provides an example image from each dataset. The Electron Microscopy dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib30" title="">30</a>]</cite> contains images of various resolutions, focusing on electron microscopy image modality. The Seven Cell Lines dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib11" title="">11</a>]</cite> contains brightfield images with a resolution of <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">1024</mn><mo id="S3.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn id="S3.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.2">1024</cn><cn id="S3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">1024 × 1024</annotation></semantics></math>, targeting nuclei of cells. The LIVECell dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib9" title="">9</a>]</cite> consists of phase-contrast images of <math alttext="768\times 512" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">768</mn><mo id="S3.SS1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn id="S3.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.2">768</cn><cn id="S3.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">768\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">768 × 512</annotation></semantics></math>, mainly focusing on individual cells. MoNuSeg dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib18" title="">18</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib19" title="">19</a>]</cite> includes whole-slide histopathology images, which we tiled into smaller images of size <math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">512</mn><mo id="S3.SS1.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><cn id="S3.SS1.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2">512</cn><cn id="S3.SS1.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">512 × 512</annotation></semantics></math>, with the main target being nuclei of tissue cells. This thorough collection of datasets allows us to fairly evaluate the capabilities of each of the segmentation models in various segmentation scenarios, ensuring an in-depth assessment.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S3.F1.sf1.g1" src="extracted/5878722/imgs/EM.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S3.F1.sf2.g1" src="extracted/5878722/imgs/MONUSEG.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S3.F1.sf3.g1" src="extracted/5878722/imgs/SEVENLINES.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S3.F1.sf4.g1" src="extracted/5878722/imgs/LIVECELL.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.sf4.2.1.1" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">We present example crops of the images from (a) Electron Microscopy dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib30" title="">30</a>]</cite>, (b) MoNuSeg dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib19" title="">19</a>]</cite>, (c) Seven Cell Lines dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib11" title="">11</a>]</cite>, and (d) LIVECell dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib9" title="">9</a>]</cite></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Segmentation Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib24" title="">24</a>]</cite> model serves as our practical baseline for this study as it is known for its robust and notable performance in microscopy image segmentation tasks. Its architecture was specifically designed for biomedical imaging tasks, featuring a symmetric encoder-decoder structure with skip connections.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For transformer-based models, we chose the models that employ different types of vision transformer encoders and different approaches for segmentation to assess their advantages and limitations. Among many transformer-based image encoders, two are well-established in the field — the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib8" title="">8</a>]</cite> and the Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite>.
The first transformer model we chose for assessment was UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib13" title="">13</a>]</cite>, which was initially designed for 3D biomedical image segmentation. We adapted UNETR for 2D image segmentation to use in our experiments. UNETR utilises ViT at its core to encode the images. The decoder part is similar to the U-Net model, consisting of a series of convolutional layers and transposed convolutions to upscale the feature maps produced by the encoder. On the other hand, Swin-UPerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite> utilises a different type of encoder — Swin Transformer, with its unique windowed attention mechanism and patch merging operations making it possible to extract the features from the input image on different scales. UPerNet serves as the decoder of the network, consisting of a Feature Pyramid Network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib20" title="">20</a>]</cite> (FPN), a Pyramid Pooling Module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib14" title="">14</a>]</cite> (PPM), and the final upscaling layer — bilinear interpolation. Lastly, the Segment Anything Model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib17" title="">17</a>]</cite> presents a unique approach to the segmentation tasks by utilising user prompts, such as points or bounding boxes, enhancing the performance of the model in complex microscopy image segmentation tasks.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>U-Net</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">We utilised the Segmentation Models Pytorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib15" title="">15</a>]</cite> (SMP) framework to construct the U-Net model. ResNet34, pre-trained on the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib7" title="">7</a>]</cite>, was used as the backbone for the network. We kept the network parameters and configuration as predefined in the framework. The depth of the encoder was set to 5 stages, where each stage generates feature maps two times smaller in spatial dimension than the previous one.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>UNETR</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">We adapted the original version of UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib13" title="">13</a>]</cite>, designed for biomedical tasks, to handle 2D microscopy images. We followed the same original architectural ideas of the model with slight adjustments — all of Conv3D layers in the decoder part of the network were replaced with Conv2D. We utilised the base version of ViT, pre-trained on ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib7" title="">7</a>]</cite>, with the patch size of <math alttext="16\times 16" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mrow id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">16</mn><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><times id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1"></times><cn id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">16</cn><cn id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">16\times 16</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">16 × 16</annotation></semantics></math></p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Segment Anything Model</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.3">We utilised SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib17" title="">17</a>]</cite> out of the box, pre-trained on the SA-1B dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib17" title="">17</a>]</cite>. We assessed all of the ways to segment images with SAM: automatic segmentation, providing user-defined point or bounding box prompts. The bounding boxes represent the highest degree of user interaction with the model and, thus — the highest degree of effort compared to the point prompting. The model expects bounding boxes as input in the [<math alttext="\text{B}\times 4" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mtext id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2a.cmml">B</mtext><mo id="S3.SS2.SSS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><times id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS3.p1.1.m1.1.1.2a.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2"><mtext id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">B</mtext></ci><cn id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">\text{B}\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">B × 4</annotation></semantics></math>] format, where <span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS3.p1.3.1">B</span> represents the number of output masks. Similarly, the input format for point prompts is [<math alttext="\text{B}\times\text{N}\times 2" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.3.m3.1"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mrow id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml"><mtext id="S3.SS2.SSS3.p1.3.m3.1.1.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.2a.cmml">B</mtext><mo id="S3.SS2.SSS3.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.cmml">×</mo><mtext id="S3.SS2.SSS3.p1.3.m3.1.1.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.3a.cmml">N</mtext><mo id="S3.SS2.SSS3.p1.3.m3.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p1.3.m3.1.1.4" xref="S3.SS2.SSS3.p1.3.m3.1.1.4.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><apply id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1"><times id="S3.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1"></times><ci id="S3.SS2.SSS3.p1.3.m3.1.1.2a.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.2"><mtext id="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.2">B</mtext></ci><ci id="S3.SS2.SSS3.p1.3.m3.1.1.3a.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3"><mtext id="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3">N</mtext></ci><cn id="S3.SS2.SSS3.p1.3.m3.1.1.4.cmml" type="integer" xref="S3.SS2.SSS3.p1.3.m3.1.1.4">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">\text{B}\times\text{N}\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.3.m3.1d">B × N × 2</annotation></semantics></math>], where B is the number of output masks, and N represents the number of points per object. On the other hand, automatic segmentation requires no interaction with the model from the user side, segmenting all potential objects and structures in the image. To assess the performance of the model, we used the OpenCV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib16" title="">16</a>]</cite> framework to generate relevant points and bounding boxes from the ground truth masks, which were provided in the datasets. These prompts served as the input to the model alongside the corresponding image to obtain final results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Swin-UPerNet</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">We used the Swin-UPerNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib22" title="">22</a>]</cite> model, pre-trained on the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib7" title="">7</a>]</cite>. We utilised the small (Swin-S) and base (Swin-B) versions of the Swin Transformer for the encoder. The decoder remained the same, consisting of FPN, PPM, and final linear interpolation. The default configuration of Swin-UPerNet uses a patch size of <math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.1.m1.1"><semantics id="S3.SS2.SSS4.p1.1.m1.1a"><mrow id="S3.SS2.SSS4.p1.1.m1.1.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS4.p1.1.m1.1.1.2" xref="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml">4</mn><mo id="S3.SS2.SSS4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS4.p1.1.m1.1.1.3" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.1.m1.1b"><apply id="S3.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1"><times id="S3.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.1"></times><cn id="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS4.p1.1.m1.1.1.2">4</cn><cn id="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS4.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.1.m1.1d">4 × 4</annotation></semantics></math> with a window of size 7 (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.T2" style="color:#FF0000;" title="Table 2 ‣ 3.2.4 Swin-UPerNet ‣ 3.2 Segmentation Models ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">2</span></a> provides a detailed overview of configuration).</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.4.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.5.2" style="font-size:90%;">Detailed overview of parameters of the small (Swin-S) and base (Swin-B) versions of Swin-UPerNet.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T2.2.3.1.1" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.3.1.1.1">Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.3.1.2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.3.1.2.1">Swin-S</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.3.1.3" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.3.1.3.1">Swin-B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.2.2.3" style="padding-left:7.1pt;padding-right:7.1pt;">Patch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><mn id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml">4</mn><mo id="S3.T2.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T2.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.T2.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><times id="S3.T2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1.1"></times><cn id="S3.T2.1.1.1.m1.1.1.2.cmml" type="integer" xref="S3.T2.1.1.1.m1.1.1.2">4</cn><cn id="S3.T2.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T2.1.1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">4 × 4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.2" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.T2.2.2.2.m1.1"><semantics id="S3.T2.2.2.2.m1.1a"><mrow id="S3.T2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.m1.1.1.cmml"><mn id="S3.T2.2.2.2.m1.1.1.2" xref="S3.T2.2.2.2.m1.1.1.2.cmml">4</mn><mo id="S3.T2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S3.T2.2.2.2.m1.1.1.3" xref="S3.T2.2.2.2.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><apply id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1"><times id="S3.T2.2.2.2.m1.1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1.1"></times><cn id="S3.T2.2.2.2.m1.1.1.2.cmml" type="integer" xref="S3.T2.2.2.2.m1.1.1.2">4</cn><cn id="S3.T2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S3.T2.2.2.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.m1.1d">4 × 4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.2.4.2.1" style="padding-left:7.1pt;padding-right:7.1pt;">Embedding dimension</th>
<td class="ltx_td ltx_align_left" id="S3.T2.2.4.2.2" style="padding-left:7.1pt;padding-right:7.1pt;">96</td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.4.2.3" style="padding-left:7.1pt;padding-right:7.1pt;">128</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.2.5.3.1" style="padding-left:7.1pt;padding-right:7.1pt;">Window size</th>
<td class="ltx_td ltx_align_left" id="S3.T2.2.5.3.2" style="padding-left:7.1pt;padding-right:7.1pt;">7</td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.5.3.3" style="padding-left:7.1pt;padding-right:7.1pt;">12</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.2.6.4.1" style="padding-left:7.1pt;padding-right:7.1pt;">Depth of transformer</th>
<td class="ltx_td ltx_align_left" id="S3.T2.2.6.4.2" style="padding-left:7.1pt;padding-right:7.1pt;">2, 2, 18, 2</td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.6.4.3" style="padding-left:7.1pt;padding-right:7.1pt;">2, 2, 18, 2</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.2.7.5.1" style="padding-left:7.1pt;padding-right:7.1pt;">Heads in each stage</th>
<td class="ltx_td ltx_align_left" id="S3.T2.2.7.5.2" style="padding-left:7.1pt;padding-right:7.1pt;">3, 6, 12, 24</td>
<td class="ltx_td ltx_align_left" id="S3.T2.2.7.5.3" style="padding-left:7.1pt;padding-right:7.1pt;">4, 8, 16, 32</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.2.8.6.1" style="padding-left:7.1pt;padding-right:7.1pt;">Hidden size in MLP</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.8.6.2" style="padding-left:7.1pt;padding-right:7.1pt;">768</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.8.6.3" style="padding-left:7.1pt;padding-right:7.1pt;">1024</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Swin-UPerNet Modifications</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="599" id="S3.F2.g1" src="extracted/5878722/imgs/swin_mods_eccv_vertical.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.4.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.2.1" style="font-size:90%;">Representation of Swin-UPerNet architecture, which consists of Swin Transformer (blue blocks) and the UPerNet decoder (green blocks). Orange dotted rectangles provide an overview of our proposed modifications to the architecture of the model. Conv denotes a convolutional block, which is made of a convolutional layer, batch normalisation, and ReLU activation. Deconv denotes transposed convolutional operation. The circle with a line denotes an addition operation, followed by a convolutional operation with kernel size <math alttext="3\times 3" class="ltx_Math" display="inline" id="S3.F2.2.1.m1.1"><semantics id="S3.F2.2.1.m1.1b"><mrow id="S3.F2.2.1.m1.1.1" xref="S3.F2.2.1.m1.1.1.cmml"><mn id="S3.F2.2.1.m1.1.1.2" xref="S3.F2.2.1.m1.1.1.2.cmml">3</mn><mo id="S3.F2.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.F2.2.1.m1.1.1.1.cmml">×</mo><mn id="S3.F2.2.1.m1.1.1.3" xref="S3.F2.2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.1.m1.1c"><apply id="S3.F2.2.1.m1.1.1.cmml" xref="S3.F2.2.1.m1.1.1"><times id="S3.F2.2.1.m1.1.1.1.cmml" xref="S3.F2.2.1.m1.1.1.1"></times><cn id="S3.F2.2.1.m1.1.1.2.cmml" type="integer" xref="S3.F2.2.1.m1.1.1.2">3</cn><cn id="S3.F2.2.1.m1.1.1.3.cmml" type="integer" xref="S3.F2.2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.1.m1.1d">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.1.m1.1e">3 × 3</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">While exploring the Swin-UPerNet, we identified several issues in the network. As the original model uses a patch size of <math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">4</mn><mo id="S3.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn id="S3.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.2">4</cn><cn id="S3.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">4 × 4</annotation></semantics></math>, the input size reduces by 4 times after the patch partitioning operation. This causes the misalignment in the decoder part of the network. In order to align the dimension of the final segmentation mask with the input image, bilinear interpolation is used in the original implementation of the model. While this approach provides a clear and lightweight solution, it has some drawbacks. Bilinear interpolation does not have learnable parameters and can introduce artefacts in the segmentation mask, potentially decreasing the performance of the model. To address this issue, we propose an architectural improvement by replacing the bilinear interpolation with a series of convolutional and transposed convolutional layers, introducing more learnable parameters and enhancing the quality of the segmentation mask.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">As the segmentation tasks can be challenging when dealing with microscopy images, and the size of the objects may greatly vary, ranging from tiny nuclei to whole cells, it is necessary to induce more local information. We proposed different ways to achieve it and enhance the performance of Swin-UPerNet, specifically in microscopy image segmentation tasks:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Decreasing the patch size to induce more local context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Adding a skip connection with a convolutional block from the input image to the decoder part of the network.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Adding additional Swin Transformer stages into the backbone.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To address these issues, we designed several architectural and configurational improvements.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.F2" style="color:#FF0000;" title="Figure 2 ‣ 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the main ideas of our designed modifications, and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.T3" style="color:#FF0000;" title="Table 3 ‣ 3.3.5 Swin-S-Pyramid ‣ 3.3 Swin-UPerNet Modifications ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of modification present in different types of proposed architectures.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">These modifications aim to induce more local context in the model and potentially increase its performance. We present a detailed overview and explanation of each designed modfication below.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Swin-S-PS2</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.2">We changed the patch size from <math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mrow id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml">4</mn><mo id="S3.SS3.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS1.p1.1.m1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><times id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.2">4</cn><cn id="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">4 × 4</annotation></semantics></math> to <math alttext="2\times 2" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.2.m2.1"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mrow id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mn id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">2</mn><mo id="S3.SS3.SSS1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS1.p1.2.m2.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><times id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1"></times><cn id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">2</cn><cn id="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.2.m2.1d">2 × 2</annotation></semantics></math>, increasing the ability of the model to capture finer details. We kept all other parameters the same.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Swin-S-Conv</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">We replaced the bilinear interpolation, which has no learnable parameters and sometimes produces artefacts in the final segmentation mask, with a series of convolutional blocks alongside transposed convolutions. Each convolutional block consists of a Conv2D layer with a kernel size of <math alttext="3\times 3" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><mrow id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS2.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml">3</mn><mo id="S3.SS3.SSS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1"><times id="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS2.p1.1.m1.1.1.2">3</cn><cn id="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.1d">3 × 3</annotation></semantics></math> and padding of 1, Batch normalisation, and ReLU activation function. We further enhanced the model by adding a skip connection with a convolutional block and merging its feature maps with those generated by the decoder. This set of modifications aims to enhance the quality of the final segmentation mask and increase the performance of the model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Swin-S-TB</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">We decreased the patch size of <math alttext="2\times 2" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.1.m1.1"><semantics id="S3.SS3.SSS3.p1.1.m1.1a"><mrow id="S3.SS3.SSS3.p1.1.m1.1.1" xref="S3.SS3.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS3.p1.1.m1.1.1.2" xref="S3.SS3.SSS3.p1.1.m1.1.1.2.cmml">2</mn><mo id="S3.SS3.SSS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS3.p1.1.m1.1.1.3" xref="S3.SS3.SSS3.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.1.m1.1b"><apply id="S3.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1"><times id="S3.SS3.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS3.p1.1.m1.1.1.2">2</cn><cn id="S3.SS3.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.1.m1.1c">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.1.m1.1d">2 × 2</annotation></semantics></math> and kept the convolutional and transposed convolutions in the decoder part. Additionally, we integrated one more stage in the backbone of the network, consisting of two consecutive Swin Transformer blocks. All of those changes aim to increase the ability of the model to process complex features and induce more of the local context.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Swin-S-TB-Skip</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">We kept the previous ideas of Swin-S-TB. Additionally, we added a skip connection in order to introduce more low-level information and see how much it contributes to the quality of the final segmentation mask.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.5 </span>Swin-S-Pyramid</h4>
<div class="ltx_para" id="S3.SS3.SSS5.p1">
<p class="ltx_p" id="S3.SS3.SSS5.p1.1">We decided to decrease the patch size even more — to <math alttext="1\times 1" class="ltx_Math" display="inline" id="S3.SS3.SSS5.p1.1.m1.1"><semantics id="S3.SS3.SSS5.p1.1.m1.1a"><mrow id="S3.SS3.SSS5.p1.1.m1.1.1" xref="S3.SS3.SSS5.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS5.p1.1.m1.1.1.2" xref="S3.SS3.SSS5.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS3.SSS5.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS5.p1.1.m1.1.1.3" xref="S3.SS3.SSS5.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS5.p1.1.m1.1b"><apply id="S3.SS3.SSS5.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS5.p1.1.m1.1.1"><times id="S3.SS3.SSS5.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS5.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS5.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS5.p1.1.m1.1.1.2">1</cn><cn id="S3.SS3.SSS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS5.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS5.p1.1.m1.1c">1\times 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS5.p1.1.m1.1d">1 × 1</annotation></semantics></math>. We extended the backbone of the network, adding two additional stages. We changed the embedding dimension to 24 in order to align with the desired input of the rest of the backbone, keeping the pre-trained weights. The architecture of the decoder was adjusted so that the output of two additional stages in the backbone aligns with the FPN, yielding the final segmentation mask with the same dimensions as the input image. With these changes, we do not need to have any additional convolutional layers or interpolation in the decoder.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.12.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.13.2" style="font-size:90%;">Detailed overview of Swin-UPerNet modifications. Each row represents the Swin-UPerNet modification. The checkmarks show modifications present in the architecture. <span class="ltx_text ltx_font_bold" id="S3.T3.13.2.1">Deconv2D</span> denotes a series of convolutional blocks and transposed convolutional layers. <span class="ltx_text ltx_font_bold" id="S3.T3.13.2.2">Skip</span> denotes the presence of a skip connection from the input image to the decoder part of the network. <span class="ltx_text ltx_font_bold" id="S3.T3.13.2.3">Extra Stage</span> denotes the additional Swin stages in the encoder of the network. <span class="ltx_text ltx_font_bold" id="S3.T3.13.2.4">Pyramid</span> denotes the modification with an extended encoder and decoder.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.6.7.1.1" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.6.7.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="4" id="S3.T3.6.7.1.2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.6.7.1.2.1">Modifications</span></th>
<td class="ltx_td ltx_border_tt" id="S3.T3.6.7.1.3" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.8.2">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T3.6.8.2.1" style="padding-left:7.1pt;padding-right:7.1pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T3.6.8.2.2" style="padding-left:7.1pt;padding-right:7.1pt;">Patch</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.6.8.2.3" rowspan="2" style="padding-left:7.1pt;padding-right:7.1pt;"><span class="ltx_text" id="S3.T3.6.8.2.3.1">Deconv2D</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.6.8.2.4" style="padding-left:7.1pt;padding-right:7.1pt;">Skip</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.6.8.2.5" style="padding-left:7.1pt;padding-right:7.1pt;">Extra</th>
<td class="ltx_td" id="S3.T3.6.8.2.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.9.3">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T3.6.9.3.1" style="padding-left:7.1pt;padding-right:7.1pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T3.6.9.3.2" style="padding-left:7.1pt;padding-right:7.1pt;">Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.6.9.3.3" style="padding-left:7.1pt;padding-right:7.1pt;">Connection</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.6.9.3.4" style="padding-left:7.1pt;padding-right:7.1pt;">Stage</th>
<td class="ltx_td" id="S3.T3.6.9.3.5" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.2" style="padding-left:7.1pt;padding-right:7.1pt;">Swin-UPerNet</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.T3.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mn id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2.cmml">4</mn><mo id="S3.T3.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><times id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1.1"></times><cn id="S3.T3.1.1.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.1.1.1.m1.1.1.2">4</cn><cn id="S3.T3.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.1.1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.m1.1d">4 × 4</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.5" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.2.2.2" style="padding-left:7.1pt;padding-right:7.1pt;">Swin-S-PS2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.2.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="2\times 2" class="ltx_Math" display="inline" id="S3.T3.2.2.1.m1.1"><semantics id="S3.T3.2.2.1.m1.1a"><mrow id="S3.T3.2.2.1.m1.1.1" xref="S3.T3.2.2.1.m1.1.1.cmml"><mn id="S3.T3.2.2.1.m1.1.1.2" xref="S3.T3.2.2.1.m1.1.1.2.cmml">2</mn><mo id="S3.T3.2.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.2.2.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.2.2.1.m1.1.1.3" xref="S3.T3.2.2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.m1.1b"><apply id="S3.T3.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1"><times id="S3.T3.2.2.1.m1.1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1.1"></times><cn id="S3.T3.2.2.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.2.2.1.m1.1.1.2">2</cn><cn id="S3.T3.2.2.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.2.2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.m1.1c">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.1.m1.1d">2 × 2</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.3" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.4" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td" id="S3.T3.2.2.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.3.3.2" style="padding-left:7.1pt;padding-right:7.1pt;">Swin-S-Conv</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.3.3.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="4\times 4" class="ltx_Math" display="inline" id="S3.T3.3.3.1.m1.1"><semantics id="S3.T3.3.3.1.m1.1a"><mrow id="S3.T3.3.3.1.m1.1.1" xref="S3.T3.3.3.1.m1.1.1.cmml"><mn id="S3.T3.3.3.1.m1.1.1.2" xref="S3.T3.3.3.1.m1.1.1.2.cmml">4</mn><mo id="S3.T3.3.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.3.3.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.3.3.1.m1.1.1.3" xref="S3.T3.3.3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.m1.1b"><apply id="S3.T3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1"><times id="S3.T3.3.3.1.m1.1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1.1"></times><cn id="S3.T3.3.3.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.3.3.1.m1.1.1.2">4</cn><cn id="S3.T3.3.3.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.3.3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.1.m1.1d">4 × 4</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.4" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.5" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td" id="S3.T3.3.3.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.4.2" style="padding-left:7.1pt;padding-right:7.1pt;">Swin-S-Pyramid</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.4.4.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="1\times 1" class="ltx_Math" display="inline" id="S3.T3.4.4.1.m1.1"><semantics id="S3.T3.4.4.1.m1.1a"><mrow id="S3.T3.4.4.1.m1.1.1" xref="S3.T3.4.4.1.m1.1.1.cmml"><mn id="S3.T3.4.4.1.m1.1.1.2" xref="S3.T3.4.4.1.m1.1.1.2.cmml">1</mn><mo id="S3.T3.4.4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.4.4.1.m1.1.1.3" xref="S3.T3.4.4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><apply id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1"><times id="S3.T3.4.4.1.m1.1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1.1"></times><cn id="S3.T3.4.4.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.4.4.1.m1.1.1.2">1</cn><cn id="S3.T3.4.4.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.4.4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">1\times 1</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.1.m1.1d">1 × 1</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.3" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.5" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td" id="S3.T3.4.4.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.5.2" style="padding-left:7.1pt;padding-right:7.1pt;">Swin-S-TB</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.5.5.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="2\times 2" class="ltx_Math" display="inline" id="S3.T3.5.5.1.m1.1"><semantics id="S3.T3.5.5.1.m1.1a"><mrow id="S3.T3.5.5.1.m1.1.1" xref="S3.T3.5.5.1.m1.1.1.cmml"><mn id="S3.T3.5.5.1.m1.1.1.2" xref="S3.T3.5.5.1.m1.1.1.2.cmml">2</mn><mo id="S3.T3.5.5.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.5.5.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.5.5.1.m1.1.1.3" xref="S3.T3.5.5.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.1.m1.1b"><apply id="S3.T3.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1"><times id="S3.T3.5.5.1.m1.1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1.1"></times><cn id="S3.T3.5.5.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.5.5.1.m1.1.1.2">2</cn><cn id="S3.T3.5.5.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.5.5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.1.m1.1c">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.1.m1.1d">2 × 2</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.3" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.4" style="padding-left:7.1pt;padding-right:7.1pt;">—</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td" id="S3.T3.5.5.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.6.6.2" style="padding-left:7.1pt;padding-right:7.1pt;">Swin-S-TB-Skip</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.6.6.1" style="padding-left:7.1pt;padding-right:7.1pt;"><math alttext="2\times 2" class="ltx_Math" display="inline" id="S3.T3.6.6.1.m1.1"><semantics id="S3.T3.6.6.1.m1.1a"><mrow id="S3.T3.6.6.1.m1.1.1" xref="S3.T3.6.6.1.m1.1.1.cmml"><mn id="S3.T3.6.6.1.m1.1.1.2" xref="S3.T3.6.6.1.m1.1.1.2.cmml">2</mn><mo id="S3.T3.6.6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.6.6.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.6.6.1.m1.1.1.3" xref="S3.T3.6.6.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.m1.1b"><apply id="S3.T3.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1"><times id="S3.T3.6.6.1.m1.1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1.1"></times><cn id="S3.T3.6.6.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.6.6.1.m1.1.1.2">2</cn><cn id="S3.T3.6.6.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.6.6.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.m1.1c">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.1.m1.1d">2 × 2</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.3" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.4" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.5" style="padding-left:7.1pt;padding-right:7.1pt;">✓</td>
<td class="ltx_td ltx_border_bb" id="S3.T3.6.6.6" style="padding-left:7.1pt;padding-right:7.1pt;"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training Pipeline</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We designed our custom training pipeline to effectively train and switch between different deep learning models and their modifications. We utilised Pytorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib23" title="">23</a>]</cite>, Weights and Biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib2" title="">2</a>]</cite>, and Hydra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib29" title="">29</a>]</cite> frameworks for flexible training, configuration management, tracking and logging the experiments.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Data Preprocessing</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Input images are normalised and transformed using the Albumentations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib3" title="">3</a>]</cite> framework. We apply horizontal and vertical flips and random rotation to the input. This choice enhances the ability of the model to understand the structure of the images across different orientations and scales of the input images, which often can be the case in microscopy images.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">Additionally, we use the random cropping of size <math alttext="224\times 224" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.1.m1.1"><semantics id="S3.SS4.SSS1.p2.1.m1.1a"><mrow id="S3.SS4.SSS1.p2.1.m1.1.1" xref="S3.SS4.SSS1.p2.1.m1.1.1.cmml"><mn id="S3.SS4.SSS1.p2.1.m1.1.1.2" xref="S3.SS4.SSS1.p2.1.m1.1.1.2.cmml">224</mn><mo id="S3.SS4.SSS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.SSS1.p2.1.m1.1.1.3" xref="S3.SS4.SSS1.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.1.m1.1b"><apply id="S3.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1"><times id="S3.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1.1"></times><cn id="S3.SS4.SSS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS4.SSS1.p2.1.m1.1.1.2">224</cn><cn id="S3.SS4.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.SSS1.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p2.1.m1.1d">224 × 224</annotation></semantics></math> during the training process. We found this crop size beneficial as it allows us to avoid unnecessary padding during training Swin-UPerNet and other transformer-based models. If the height and the width of the input image were not multiple of the product of window size and scaling factors across the layers of the network, the additional padding is applied to process the image. This padding can lead to artefacts in the final segmentation mask and potentially affect the performance of the model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Training</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Each model was trained for 150 epochs, which we found optimal for convergence. The batch size of 16 images was used — the maximum that could fit into our GPU memory. We sampled 500 images from the dataset during training each epoch to provide diverse examples and enhance the robustness of the model. We chose the combination of Dice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib26" title="">26</a>]</cite> and Focal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib21" title="">21</a>]</cite> losses for training with weight coefficients set to 0.9 and 0.1, serving as the standard ratio. We compute the loss as follows:</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{total}=\alpha\times\mathcal{L}_{dice}(Y,\hat{Y})+\beta\times%
\mathcal{L}_{focal}(Y,\hat{Y})" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.5" xref="S3.E1.m1.4.5.cmml"><msub id="S3.E1.m1.4.5.2" xref="S3.E1.m1.4.5.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.5.2.2" xref="S3.E1.m1.4.5.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.4.5.2.3" xref="S3.E1.m1.4.5.2.3.cmml"><mi id="S3.E1.m1.4.5.2.3.2" xref="S3.E1.m1.4.5.2.3.2.cmml">t</mi><mo id="S3.E1.m1.4.5.2.3.1" xref="S3.E1.m1.4.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.2.3.3" xref="S3.E1.m1.4.5.2.3.3.cmml">o</mi><mo id="S3.E1.m1.4.5.2.3.1a" xref="S3.E1.m1.4.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.2.3.4" xref="S3.E1.m1.4.5.2.3.4.cmml">t</mi><mo id="S3.E1.m1.4.5.2.3.1b" xref="S3.E1.m1.4.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.2.3.5" xref="S3.E1.m1.4.5.2.3.5.cmml">a</mi><mo id="S3.E1.m1.4.5.2.3.1c" xref="S3.E1.m1.4.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.2.3.6" xref="S3.E1.m1.4.5.2.3.6.cmml">l</mi></mrow></msub><mo id="S3.E1.m1.4.5.1" xref="S3.E1.m1.4.5.1.cmml">=</mo><mrow id="S3.E1.m1.4.5.3" xref="S3.E1.m1.4.5.3.cmml"><mrow id="S3.E1.m1.4.5.3.2" xref="S3.E1.m1.4.5.3.2.cmml"><mrow id="S3.E1.m1.4.5.3.2.2" xref="S3.E1.m1.4.5.3.2.2.cmml"><mi id="S3.E1.m1.4.5.3.2.2.2" xref="S3.E1.m1.4.5.3.2.2.2.cmml">α</mi><mo id="S3.E1.m1.4.5.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.4.5.3.2.2.1.cmml">×</mo><msub id="S3.E1.m1.4.5.3.2.2.3" xref="S3.E1.m1.4.5.3.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.5.3.2.2.3.2" xref="S3.E1.m1.4.5.3.2.2.3.2.cmml">ℒ</mi><mrow id="S3.E1.m1.4.5.3.2.2.3.3" xref="S3.E1.m1.4.5.3.2.2.3.3.cmml"><mi id="S3.E1.m1.4.5.3.2.2.3.3.2" xref="S3.E1.m1.4.5.3.2.2.3.3.2.cmml">d</mi><mo id="S3.E1.m1.4.5.3.2.2.3.3.1" xref="S3.E1.m1.4.5.3.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.2.2.3.3.3" xref="S3.E1.m1.4.5.3.2.2.3.3.3.cmml">i</mi><mo id="S3.E1.m1.4.5.3.2.2.3.3.1a" xref="S3.E1.m1.4.5.3.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.2.2.3.3.4" xref="S3.E1.m1.4.5.3.2.2.3.3.4.cmml">c</mi><mo id="S3.E1.m1.4.5.3.2.2.3.3.1b" xref="S3.E1.m1.4.5.3.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.2.2.3.3.5" xref="S3.E1.m1.4.5.3.2.2.3.3.5.cmml">e</mi></mrow></msub></mrow><mo id="S3.E1.m1.4.5.3.2.1" xref="S3.E1.m1.4.5.3.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.4.5.3.2.3.2" xref="S3.E1.m1.4.5.3.2.3.1.cmml"><mo id="S3.E1.m1.4.5.3.2.3.2.1" stretchy="false" xref="S3.E1.m1.4.5.3.2.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Y</mi><mo id="S3.E1.m1.4.5.3.2.3.2.2" xref="S3.E1.m1.4.5.3.2.3.1.cmml">,</mo><mover accent="true" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">Y</mi><mo id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml">^</mo></mover><mo id="S3.E1.m1.4.5.3.2.3.2.3" stretchy="false" xref="S3.E1.m1.4.5.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.5.3.1" xref="S3.E1.m1.4.5.3.1.cmml">+</mo><mrow id="S3.E1.m1.4.5.3.3" xref="S3.E1.m1.4.5.3.3.cmml"><mrow id="S3.E1.m1.4.5.3.3.2" xref="S3.E1.m1.4.5.3.3.2.cmml"><mi id="S3.E1.m1.4.5.3.3.2.2" xref="S3.E1.m1.4.5.3.3.2.2.cmml">β</mi><mo id="S3.E1.m1.4.5.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.4.5.3.3.2.1.cmml">×</mo><msub id="S3.E1.m1.4.5.3.3.2.3" xref="S3.E1.m1.4.5.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.5.3.3.2.3.2" xref="S3.E1.m1.4.5.3.3.2.3.2.cmml">ℒ</mi><mrow id="S3.E1.m1.4.5.3.3.2.3.3" xref="S3.E1.m1.4.5.3.3.2.3.3.cmml"><mi id="S3.E1.m1.4.5.3.3.2.3.3.2" xref="S3.E1.m1.4.5.3.3.2.3.3.2.cmml">f</mi><mo id="S3.E1.m1.4.5.3.3.2.3.3.1" xref="S3.E1.m1.4.5.3.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.3.2.3.3.3" xref="S3.E1.m1.4.5.3.3.2.3.3.3.cmml">o</mi><mo id="S3.E1.m1.4.5.3.3.2.3.3.1a" xref="S3.E1.m1.4.5.3.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.3.2.3.3.4" xref="S3.E1.m1.4.5.3.3.2.3.3.4.cmml">c</mi><mo id="S3.E1.m1.4.5.3.3.2.3.3.1b" xref="S3.E1.m1.4.5.3.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.3.2.3.3.5" xref="S3.E1.m1.4.5.3.3.2.3.3.5.cmml">a</mi><mo id="S3.E1.m1.4.5.3.3.2.3.3.1c" xref="S3.E1.m1.4.5.3.3.2.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.5.3.3.2.3.3.6" xref="S3.E1.m1.4.5.3.3.2.3.3.6.cmml">l</mi></mrow></msub></mrow><mo id="S3.E1.m1.4.5.3.3.1" xref="S3.E1.m1.4.5.3.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.4.5.3.3.3.2" xref="S3.E1.m1.4.5.3.3.3.1.cmml"><mo id="S3.E1.m1.4.5.3.3.3.2.1" stretchy="false" xref="S3.E1.m1.4.5.3.3.3.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">Y</mi><mo id="S3.E1.m1.4.5.3.3.3.2.2" xref="S3.E1.m1.4.5.3.3.3.1.cmml">,</mo><mover accent="true" id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mi id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml">Y</mi><mo id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml">^</mo></mover><mo id="S3.E1.m1.4.5.3.3.3.2.3" stretchy="false" xref="S3.E1.m1.4.5.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.5.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.1.cmml" xref="S3.E1.m1.4.5.1"></eq><apply id="S3.E1.m1.4.5.2.cmml" xref="S3.E1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.2.1.cmml" xref="S3.E1.m1.4.5.2">subscript</csymbol><ci id="S3.E1.m1.4.5.2.2.cmml" xref="S3.E1.m1.4.5.2.2">ℒ</ci><apply id="S3.E1.m1.4.5.2.3.cmml" xref="S3.E1.m1.4.5.2.3"><times id="S3.E1.m1.4.5.2.3.1.cmml" xref="S3.E1.m1.4.5.2.3.1"></times><ci id="S3.E1.m1.4.5.2.3.2.cmml" xref="S3.E1.m1.4.5.2.3.2">𝑡</ci><ci id="S3.E1.m1.4.5.2.3.3.cmml" xref="S3.E1.m1.4.5.2.3.3">𝑜</ci><ci id="S3.E1.m1.4.5.2.3.4.cmml" xref="S3.E1.m1.4.5.2.3.4">𝑡</ci><ci id="S3.E1.m1.4.5.2.3.5.cmml" xref="S3.E1.m1.4.5.2.3.5">𝑎</ci><ci id="S3.E1.m1.4.5.2.3.6.cmml" xref="S3.E1.m1.4.5.2.3.6">𝑙</ci></apply></apply><apply id="S3.E1.m1.4.5.3.cmml" xref="S3.E1.m1.4.5.3"><plus id="S3.E1.m1.4.5.3.1.cmml" xref="S3.E1.m1.4.5.3.1"></plus><apply id="S3.E1.m1.4.5.3.2.cmml" xref="S3.E1.m1.4.5.3.2"><times id="S3.E1.m1.4.5.3.2.1.cmml" xref="S3.E1.m1.4.5.3.2.1"></times><apply id="S3.E1.m1.4.5.3.2.2.cmml" xref="S3.E1.m1.4.5.3.2.2"><times id="S3.E1.m1.4.5.3.2.2.1.cmml" xref="S3.E1.m1.4.5.3.2.2.1"></times><ci id="S3.E1.m1.4.5.3.2.2.2.cmml" xref="S3.E1.m1.4.5.3.2.2.2">𝛼</ci><apply id="S3.E1.m1.4.5.3.2.2.3.cmml" xref="S3.E1.m1.4.5.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.3.2.2.3.1.cmml" xref="S3.E1.m1.4.5.3.2.2.3">subscript</csymbol><ci id="S3.E1.m1.4.5.3.2.2.3.2.cmml" xref="S3.E1.m1.4.5.3.2.2.3.2">ℒ</ci><apply id="S3.E1.m1.4.5.3.2.2.3.3.cmml" xref="S3.E1.m1.4.5.3.2.2.3.3"><times id="S3.E1.m1.4.5.3.2.2.3.3.1.cmml" xref="S3.E1.m1.4.5.3.2.2.3.3.1"></times><ci id="S3.E1.m1.4.5.3.2.2.3.3.2.cmml" xref="S3.E1.m1.4.5.3.2.2.3.3.2">𝑑</ci><ci id="S3.E1.m1.4.5.3.2.2.3.3.3.cmml" xref="S3.E1.m1.4.5.3.2.2.3.3.3">𝑖</ci><ci id="S3.E1.m1.4.5.3.2.2.3.3.4.cmml" xref="S3.E1.m1.4.5.3.2.2.3.3.4">𝑐</ci><ci id="S3.E1.m1.4.5.3.2.2.3.3.5.cmml" xref="S3.E1.m1.4.5.3.2.2.3.3.5">𝑒</ci></apply></apply></apply><interval closure="open" id="S3.E1.m1.4.5.3.2.3.1.cmml" xref="S3.E1.m1.4.5.3.2.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑌</ci><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><ci id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1">^</ci><ci id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2">𝑌</ci></apply></interval></apply><apply id="S3.E1.m1.4.5.3.3.cmml" xref="S3.E1.m1.4.5.3.3"><times id="S3.E1.m1.4.5.3.3.1.cmml" xref="S3.E1.m1.4.5.3.3.1"></times><apply id="S3.E1.m1.4.5.3.3.2.cmml" xref="S3.E1.m1.4.5.3.3.2"><times id="S3.E1.m1.4.5.3.3.2.1.cmml" xref="S3.E1.m1.4.5.3.3.2.1"></times><ci id="S3.E1.m1.4.5.3.3.2.2.cmml" xref="S3.E1.m1.4.5.3.3.2.2">𝛽</ci><apply id="S3.E1.m1.4.5.3.3.2.3.cmml" xref="S3.E1.m1.4.5.3.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.3.3.2.3.1.cmml" xref="S3.E1.m1.4.5.3.3.2.3">subscript</csymbol><ci id="S3.E1.m1.4.5.3.3.2.3.2.cmml" xref="S3.E1.m1.4.5.3.3.2.3.2">ℒ</ci><apply id="S3.E1.m1.4.5.3.3.2.3.3.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3"><times id="S3.E1.m1.4.5.3.3.2.3.3.1.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3.1"></times><ci id="S3.E1.m1.4.5.3.3.2.3.3.2.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3.2">𝑓</ci><ci id="S3.E1.m1.4.5.3.3.2.3.3.3.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3.3">𝑜</ci><ci id="S3.E1.m1.4.5.3.3.2.3.3.4.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3.4">𝑐</ci><ci id="S3.E1.m1.4.5.3.3.2.3.3.5.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3.5">𝑎</ci><ci id="S3.E1.m1.4.5.3.3.2.3.3.6.cmml" xref="S3.E1.m1.4.5.3.3.2.3.3.6">𝑙</ci></apply></apply></apply><interval closure="open" id="S3.E1.m1.4.5.3.3.3.1.cmml" xref="S3.E1.m1.4.5.3.3.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑌</ci><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><ci id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1">^</ci><ci id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2">𝑌</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\mathcal{L}_{total}=\alpha\times\mathcal{L}_{dice}(Y,\hat{Y})+\beta\times%
\mathcal{L}_{focal}(Y,\hat{Y})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">caligraphic_L start_POSTSUBSCRIPT italic_t italic_o italic_t italic_a italic_l end_POSTSUBSCRIPT = italic_α × caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_c italic_e end_POSTSUBSCRIPT ( italic_Y , over^ start_ARG italic_Y end_ARG ) + italic_β × caligraphic_L start_POSTSUBSCRIPT italic_f italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT ( italic_Y , over^ start_ARG italic_Y end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.SSS2.p2.4">Here <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.1.m1.1"><semantics id="S3.SS4.SSS2.p2.1.m1.1a"><mi id="S3.SS4.SSS2.p2.1.m1.1.1" xref="S3.SS4.SSS2.p2.1.m1.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.1.m1.1b"><ci id="S3.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p2.1.m1.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.1.m1.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p2.1.m1.1d">italic_Y</annotation></semantics></math> is the ground truth mask, <math alttext="\hat{Y}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.2.m2.1"><semantics id="S3.SS4.SSS2.p2.2.m2.1a"><mover accent="true" id="S3.SS4.SSS2.p2.2.m2.1.1" xref="S3.SS4.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS4.SSS2.p2.2.m2.1.1.2" xref="S3.SS4.SSS2.p2.2.m2.1.1.2.cmml">Y</mi><mo id="S3.SS4.SSS2.p2.2.m2.1.1.1" xref="S3.SS4.SSS2.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.2.m2.1b"><apply id="S3.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS2.p2.2.m2.1.1"><ci id="S3.SS4.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS4.SSS2.p2.2.m2.1.1.1">^</ci><ci id="S3.SS4.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS4.SSS2.p2.2.m2.1.1.2">𝑌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.2.m2.1c">\hat{Y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p2.2.m2.1d">over^ start_ARG italic_Y end_ARG</annotation></semantics></math> is the predicted mask, and <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.3.m3.1"><semantics id="S3.SS4.SSS2.p2.3.m3.1a"><mi id="S3.SS4.SSS2.p2.3.m3.1.1" xref="S3.SS4.SSS2.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.3.m3.1b"><ci id="S3.SS4.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS4.SSS2.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p2.3.m3.1d">italic_α</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.4.m4.1"><semantics id="S3.SS4.SSS2.p2.4.m4.1a"><mi id="S3.SS4.SSS2.p2.4.m4.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.4.m4.1b"><ci id="S3.SS4.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.4.m4.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p2.4.m4.1d">italic_β</annotation></semantics></math> are the weight coefficients.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Evaluation</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">For the evaluation, we used the F1 score and IoU score as our primary metrics to thoroughly assess the performance of all models. The evaluation itself was done on separate test sets of full-size images. For the UNETR and U-Net models, we applied a custom tiling algorithm — the input image was divided into tiles, and the model predicted the segmentation mask for each of the tiles. Those segmentation masks of tiles were merged back to obtain the final full-size segmentation mask.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Computational Resourses</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">We trained all of the models on the High-Performance Computing Cluster of the University of Tartu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib27" title="">27</a>]</cite>, which has Nvidia Tesla V100 GPUs with 32 gigabytes of VRAM and Nvidia Tesla A100 GPUs with 40 and 80 gigabytes of VRAM running CUDA 12.3 with Driver version 545.23.08.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="666" id="S3.F3.g1" src="extracted/5878722/imgs/preds.png" width="395"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Predicted segmentation masks of Swin-S-TB-Skip, UNETR, U-Net, and Segment Anything Model (utilising bounding box and point prompts and enabling automatic segmentation). The white contour represents the outline of the ground truth mask. The colour overlay represents the predicted segmentation mask of the model: green colour for Swin-S-TB-Skip, red colour for UNETR, blue colour for U-Net, and purple colour for SAM. We made the image from MoNuSeg dataset grayscale for the purpose of better visibility of predicted segmentation masks.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Here, we present the results of our experiments. Firstly, we compare the U-Net model against the chosen transformer-based models — UNETR, Swin-S and Swin-B, and SAM. Next, we will present and detail the results of our designed modifications for Swin-S, specifically designed to enhance its performance in microscopy image segmentation tasks. These modifications seek to induce much more local context and finer details, which is necessary when dealing with microscopy images.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison of Transformer-based Models</h3>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.4.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.5.2" style="font-size:90%;">Performance results of UNETR, Swin-UPerNet with Swin-S and Swin-B as the backbones, and Segment Anything Model operating in three different modes (the number of point and bounding box prompts are equal to the number of ground truth instances in each test image) compared to U-Net across datasets. Each row represents the model, while each column represents the obtained F1 and IoU values on each dataset. The best scores are highlighted in <span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1">bold</span>, and the second best scores are <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.5.2.2">underlined</span>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.6.1.1.1" rowspan="3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text" id="S4.T4.6.1.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.6.1.1.2" rowspan="2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text" id="S4.T4.6.1.1.2.1">LIVECell</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.6.1.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">Seven</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.6.1.1.4" rowspan="2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text" id="S4.T4.6.1.1.4.1">MoNuSeg</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T4.6.1.1.5" style="padding-left:5.7pt;padding-right:5.7pt;">Electron</th>
</tr>
<tr class="ltx_tr" id="S4.T4.6.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="S4.T4.6.2.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">Cell Lines</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="S4.T4.6.2.2.2" style="padding-left:5.7pt;padding-right:5.7pt;">Microscopy</th>
</tr>
<tr class="ltx_tr" id="S4.T4.6.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.1.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.2.1">IoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.3.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.4.1">IoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.5.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.6.1">IoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.7.1">F1</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S4.T4.6.3.3.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.3.3.8.1">IoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.6.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.6.4.1.1" style="padding-left:5.7pt;padding-right:5.7pt;">U-Net</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.4.1.2.1">0.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.4.1.3.1">0.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.4.1.4.1">0.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.4.1.5.1">0.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.4.1.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.4.1.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.4.1.9.1">0.88</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.6.5.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">UNETR</th>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.5.2.2.1">0.93</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.5.2.3.1">0.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.5.2.4.1">0.80</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.5.2.5.1">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.5.2.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.6.5.2.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.75</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.6.6.3.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S</th>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.6.3.2.1">0.92</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.61</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.6.3.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.6.3.8.1">0.93</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.6.6.3.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.6.3.9.1">0.88</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.6.7.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-B</th>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.7.4.2.1">0.92</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.7.4.3.1">0.86</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.7.4.6.1">0.83</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.7.4.7.1">0.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.7.4.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.7.4.8.1">0.93</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.6.7.4.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.7.4.9.1">0.88</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.6.8.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">SAM (Bounding Box)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.8.5.6.1">0.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.8.5.7.1">0.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.8.5.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.87</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.6.8.5.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.80</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.6.9.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">SAM (Point Prompts)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.57</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.46</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.27</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.16</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.57</td>
<td class="ltx_td ltx_align_center" id="S4.T4.6.9.6.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.61</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.6.9.6.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.52</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.6.10.7.1" style="padding-left:5.7pt;padding-right:5.7pt;">SAM (Automatic Mode)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.46</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.17</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.77</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.6.10.7.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.67</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We fine-tuned and evaluated U-Net, UNETR, and Swin-UPerNet on each dataset separately, following our pipeline outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS4" style="color:#FF0000;" title="3.4 Training Pipeline ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">3.4</span></a>. In contrast, we assessed SAM’s out-of-the-box performance without fine-tuning to evaluate its immediate usability. We provided the bounding boxes and point prompts equal to the number of instances in each test image for a fair comparison. The results, detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.T4" style="color:#FF0000;" title="Table 4 ‣ 4.1 Comparison of Transformer-based Models ‣ 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">4</span></a>, show that U-Net consistently performs well across all datasets, achieving the highest IoU of 0.88 on the Electron Microscopy dataset, setting a strong baseline for other models. UNETR generally matches the performance of U-Net but lags on the Electron Microscopy dataset with the 0.75 IoU score. Both small and basic versions of Swin-UPerNet are behind U-Net and UNETR across almost all of the datasets, except for the Electron Microscopy, showing the same results as the U-Net. These observations highlight that the traditional CNN approach remains good and robust. Segment Anything Model, when using bounding box prompts, shows reasonable performance but does not achieve the same levels as fine-tuned models. Although bounding box prompts provide decent test scores, performance decreases when switching to point prompts or using automatic mode, particularly on the Seven Cell Lines dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison of Swin-UPerNet Modifications</h3>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.4.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.5.2" style="font-size:90%;">Performance results of Swin-UPerNet (Swin-S) modifications compared to U-Net and original Swin-UPerNet (Swin-S and Swin-B) across datasets. Each row represents the model or the modification, while each column represents the obtained F1 and IoU values on each dataset. The best scores are highlighted in <span class="ltx_text ltx_font_bold" id="S4.T5.5.2.1">bold</span>, and the second best scores are <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.5.2.2">underlined</span>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T5.6.1.1.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.1.1.1.1">Models</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.6.1.1.2" rowspan="2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text" id="S4.T5.6.1.1.2.1">LIVECell</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.6.1.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">Seven</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.6.1.1.4" rowspan="2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text" id="S4.T5.6.1.1.4.1">MoNuSeg</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.6.1.1.5" style="padding-left:5.7pt;padding-right:5.7pt;">Electron</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T5.6.2.2.1" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T5.6.2.2.2" style="padding-left:5.7pt;padding-right:5.7pt;">Cell Lines</td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T5.6.2.2.3" style="padding-left:5.7pt;padding-right:5.7pt;">Microscopy</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.3.3">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T5.6.3.3.1" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.2.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.3.1">IoU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.4.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.5.1">IoU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.6.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.7.1">IoU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.8.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.3.3.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.3.3.9.1">IoU</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.6.4.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">U-Net</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.4.4.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.88</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.6.5.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S</th>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.61</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.5.5.6.1">0.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.93</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.5.5.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.88</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.6.6.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-B</th>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.6.6.6.1">0.83</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.6.6.7.1">0.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.93</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.6.6.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.88</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.6.7.7.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-PS2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.7.7.2.1">0.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.7.7.3.1">0.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.7.7.6.1">0.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.7.7.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.89</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.6.8.8.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-Conv</th>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.81</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.8.8.8.1">0.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.8.8.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.8.8.9.1">0.90</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.6.9.9.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-TB</th>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.9.9.2.1">0.93</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.9.9.3.1">0.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.9.9.4.1">0.83</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.9.9.5.1">0.72</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.9.9.6.1">0.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.91</td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.9.9.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.86</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.6.10.10.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-TB-Skip</th>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.2.1">0.93</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.3.1">0.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.4.1">0.84</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.5.1">0.74</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.6.10.10.6.1">0.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.7.1">0.71</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.8.1">0.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.6.10.10.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.6.10.10.9.1">0.91</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.6.11.11.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-Pyramid</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.2" style="padding-left:5.7pt;padding-right:5.7pt;">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.3" style="padding-left:5.7pt;padding-right:5.7pt;">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.4" style="padding-left:5.7pt;padding-right:5.7pt;">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.5" style="padding-left:5.7pt;padding-right:5.7pt;">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.6" style="padding-left:5.7pt;padding-right:5.7pt;">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.7" style="padding-left:5.7pt;padding-right:5.7pt;">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.8" style="padding-left:5.7pt;padding-right:5.7pt;">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.6.11.11.9" style="padding-left:5.7pt;padding-right:5.7pt;">0.84</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We fine-tuned all of the designed modifications on each dataset separately, utilising the proposed train pipeline, described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.SS4" style="color:#FF0000;" title="3.4 Training Pipeline ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">3.4</span></a> and draw a comparison between the original U-Net and Swin-UPerNet proposed architectures, aiming to increase its performance in microscopy image segmentation. All of the presented modifications were based on the Swin-S architecture. From Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.T5" style="color:#FF0000;" title="Table 5 ‣ 4.2 Comparison of Swin-UPerNet Modifications ‣ 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">5</span></a>, we can see that our Swin-S-TB-Skip modification excels across almost all datasets, surpassing U-Net and the original Swin-UPerNet models, both small (Swin-S) and basic (Swin-B) versions, achieving higher IoU score. Apart from this, we can see a notable increase in the performance of Swin-S-TB-Skip compared to Swin-UPerNet on the Seven Cell Lines dataset, which contains brightfield images with the nuclei as a target. We consider this modification to be our best among the others.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>FLOPs and Parameters of the Models</h3>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.6.3.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.4.2" style="font-size:90%;">Model Parameters and FLOPs. We calculated the number of FLOPs by passing the 3-channel image of size <math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.T6.3.1.m1.1"><semantics id="S4.T6.3.1.m1.1b"><mrow id="S4.T6.3.1.m1.1.1" xref="S4.T6.3.1.m1.1.1.cmml"><mn id="S4.T6.3.1.m1.1.1.2" xref="S4.T6.3.1.m1.1.1.2.cmml">224</mn><mo id="S4.T6.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T6.3.1.m1.1.1.1.cmml">×</mo><mn id="S4.T6.3.1.m1.1.1.3" xref="S4.T6.3.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.1.m1.1c"><apply id="S4.T6.3.1.m1.1.1.cmml" xref="S4.T6.3.1.m1.1.1"><times id="S4.T6.3.1.m1.1.1.1.cmml" xref="S4.T6.3.1.m1.1.1.1"></times><cn id="S4.T6.3.1.m1.1.1.2.cmml" type="integer" xref="S4.T6.3.1.m1.1.1.2">224</cn><cn id="S4.T6.3.1.m1.1.1.3.cmml" type="integer" xref="S4.T6.3.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.1.m1.1d">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.1.m1.1e">224 × 224</annotation></semantics></math> to the model. Swin-S-PS2 denotes our modification of Swin-UPerNet with decreased patch size to <math alttext="2\times 2" class="ltx_Math" display="inline" id="S4.T6.4.2.m2.1"><semantics id="S4.T6.4.2.m2.1b"><mrow id="S4.T6.4.2.m2.1.1" xref="S4.T6.4.2.m2.1.1.cmml"><mn id="S4.T6.4.2.m2.1.1.2" xref="S4.T6.4.2.m2.1.1.2.cmml">2</mn><mo id="S4.T6.4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T6.4.2.m2.1.1.1.cmml">×</mo><mn id="S4.T6.4.2.m2.1.1.3" xref="S4.T6.4.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.4.2.m2.1c"><apply id="S4.T6.4.2.m2.1.1.cmml" xref="S4.T6.4.2.m2.1.1"><times id="S4.T6.4.2.m2.1.1.1.cmml" xref="S4.T6.4.2.m2.1.1.1"></times><cn id="S4.T6.4.2.m2.1.1.2.cmml" type="integer" xref="S4.T6.4.2.m2.1.1.2">2</cn><cn id="S4.T6.4.2.m2.1.1.3.cmml" type="integer" xref="S4.T6.4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.2.m2.1d">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S4.T6.4.2.m2.1e">2 × 2</annotation></semantics></math>. Swin-S-TB-Skip denotes our best modification, with the extension of the encoder, decrease in patch size, replacement of interpolation and addition of skip connection. We cannot provide FLOPs for SAM as it depends on the amount of prompts provided to the model, which can greatly vary.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T6.7.1.1.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.7.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.7.1.1.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.7.1.1.2.1">Params (M)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.7.1.1.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T6.7.1.1.3.1">FLOPs (G)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.7.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.7.2.1.1" style="padding-left:5.7pt;padding-right:5.7pt;">U-Net</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.7.2.1.2" style="padding-left:5.7pt;padding-right:5.7pt;">24.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.7.2.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">12</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.3.2">
<td class="ltx_td ltx_align_left" id="S4.T6.7.3.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">UNETR</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.3.2.2" style="padding-left:5.7pt;padding-right:5.7pt;">111.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.3.2.3" style="padding-left:5.7pt;padding-right:5.7pt;">234</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.4.3">
<td class="ltx_td ltx_align_left" id="S4.T6.7.4.3.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-B</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.4.3.2" style="padding-left:5.7pt;padding-right:5.7pt;">121.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.4.3.3" style="padding-left:5.7pt;padding-right:5.7pt;">128</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.5.4">
<td class="ltx_td ltx_align_left" id="S4.T6.7.5.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.5.4.2" style="padding-left:5.7pt;padding-right:5.7pt;">81.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.5.4.3" style="padding-left:5.7pt;padding-right:5.7pt;">98</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.6.5">
<td class="ltx_td ltx_align_left" id="S4.T6.7.6.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-PS2 (ours)</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.6.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">81.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.6.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">390</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.7.6">
<td class="ltx_td ltx_align_left" id="S4.T6.7.7.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">Swin-S-TB-Skip (ours)</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">82.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.7.7.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">452</td>
</tr>
<tr class="ltx_tr" id="S4.T6.7.8.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.7.8.7.1" style="padding-left:5.7pt;padding-right:5.7pt;">SAM</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.7.8.7.2" style="padding-left:5.7pt;padding-right:5.7pt;">93.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.7.8.7.3" style="padding-left:5.7pt;padding-right:5.7pt;">—</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Here, we provide an overview of the FLOPs and parameters for UNETR, U-Net, SAM, Swin-UPerNet, and its best modification — Swin-S-TB-Skip. The FLOPs were calculated using a 3-channel image of <math alttext="224\times 224" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">224</mn><mo id="S4.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn id="S4.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.2">224</cn><cn id="S4.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">224 × 224</annotation></semantics></math> as the input to each model. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.T6" style="color:#FF0000;" title="Table 6 ‣ 4.3 FLOPs and Parameters of the Models ‣ 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">6</span></a> shows that U-Net has the least amount of FLOPs among all other models.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Another notable observation is that Swin-S-TB has almost 4.5 times more FLOPs than the original Swin-UPerNet. While this may sound alarming, the reason behind it is simple. As the patch size decreases to <math alttext="2\times 2" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">2</cn><cn id="S4.SS3.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">2\times 2</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">2 × 2</annotation></semantics></math>, the number of patches in the image increases, leading to a fourfold increase in the size of the attention matrix. Although this modification requires many more FLOPs to run, it still fits within the memory constraints of the same GPU. We could not provide FLOPs for the SAM model, as it depends on the number of prompts passed to the model by the user, which can greatly vary.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our experimental results offer an overview of the capabilities of modern transformer-based models — UNETR, Swin-UPerNet, and SAM in recognizing and segmenting various objects and structures within microscopy images across different modalities. We compared these models to the established U-Net model and evaluated their performance. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S3.F3" style="color:#FF0000;" title="Figure 3 ‣ 3.5 Computational Resourses ‣ 3 Methods ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">3</span></a> provides examples of the predictions of our best modification - Swin-TB-Skip compared to the UNETR, U-Net, and Segment Anything Model across all of the datasets. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.T4" style="color:#FF0000;" title="Table 4 ‣ 4.1 Comparison of Transformer-based Models ‣ 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">4</span></a> show that U-Net remains a strong contender for semantic segmentation in microscopy images. While UNETR and Swin-UPerNet generally match U-Net’s performance, their significant computational demands make them less practical for real-world applications.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The original Swin-UPerNet slightly falls behind the U-Net across most of the datasets, but several innovative modifications we introduced greatly enhanced its performance. Modifications such as extending the encoder, replacing interpolation layers in the decoder, adding an extra skip connection, and reducing patch size aim to improve local context modelling. This is crucial for better segmentation quality in microscopy images, where object and structure variability is high. Our top-performing modification, Swin-S-TB-Skip, showed notable improvements across all datasets. It is also notable, that our best modification surpassed the basic version of the original Swin-UperNet, which has more parameters, highlighting the value and the relevance of our architectural improvements. We emphasize the substantial performance boost on the Seven Cell Lines dataset compared to the original Swin-UPerNet. The brightfield modality and the challenge of segmenting cell nuclei make this dataset especially difficult.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Segment Anything Model, the first foundational segmentation model, has delivered mixed results across various datasets. The model heavily depends on user-defined prompts to achieve improvements in performance, particularly noticeable when bounding boxes are employed in contrast to its baseline automatic segmentation capabilities. This reliance on user input for optimal performance significantly diminishes its utility compared to other models. Without user interaction, SAM’s segmentations are often suboptimal, limiting its direct comparability and competitiveness with automated models that do not require such inputs. Moreover, SAM lacks class awareness, indiscriminately segmenting all detectable objects and structures. This restricts its applicability for specialized tasks, such as cell segmentation, where targeted recognition of specific classes is crucial. Nonetheless, with further developments, such as user interface, SAM could evolve into a valuable tool for interactive annotation.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">These findings demonstrate that there is still potential for advancement in transformer-based models. Their unique attention mechanisms hold promise for achieving cutting-edge performance in segmentation tasks. By continuously refining and improving these architectures, we can unlock their full potential and establish new benchmarks in the field.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we make two major contributions to the field of microscopy image segmentation. Our first contribution is a detailed comparison between the well-established and popular U-Net model and several innovative transformer-based deep learning models. These include Swin-UPerNet, which features a unique windowed attention mechanism, the Segment Anything Model with its interactive prompt segmentation approach, and UNETR, which blends a traditional U-Net-like decoder with a modern vision transformer encoder. Our evaluations reveal that while these modern transformer-based models perform comparably to U-Net, there is still room for improvement.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our second major contribution focuses on enhancing the performance of the Swin-UPerNet model. We conducted a series of experiments aimed at increasing its robustness and performance across various microscopy images. The modifications we implemented greatly improved the performance of the model. Our revised version, Swin-S-TB-Skip, on average, outperformed the original Swin-UPerNet and U-Net across all tested microscopy datasets, achieving a higher IoU score.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">However, these performance gains come with increased computational demand (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#S4.T6" style="color:#FF0000;" title="Table 6 ‣ 4.3 FLOPs and Parameters of the Models ‣ 4 Results ‣ Going Beyond U-Net: Assessing Vision Transformers for Semantic Segmentation in Microscopy Image Analysis"><span class="ltx_text ltx_ref_tag">6</span></a>). Future research should, therefore, concentrate on optimising these architectural enhancements for practical applications and integrating them into diverse microscopy image analysis workflows and tools.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by Revvity <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.revvity.com/" title="">https://www.revvity.com/</a></span></span></span>. Computational resources were provided by the High-Performance Computing Cluster of the University of Tartu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16940v1#bib.bib27" title="">27</a>]</cite>. A big thank you to all the members of the Biomedical Computer Vision Lab at the University of Tartu for their continuous support.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alrfou, K., Zhao, T., Kordijazi, A.: Cs-unet: A generalizable and flexible segmentation algorithm (Apr 2024). https://doi.org/10.1007/s11042-024-19242-4, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s11042-024-19242-4" title="">http://dx.doi.org/10.1007/s11042-024-19242-4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Biewald, L.: Experiment tracking with weights and biases (2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wandb.com/" title="">https://www.wandb.com/</a>, software available from wandb.com

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov, A., Druzhinin, M., Kalinin, A.A.: Albumentations: Fast and flexible image augmentations. Information <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">11</span>(2) (2020). https://doi.org/10.3390/info11020125, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2078-2489/11/2/125" title="">https://www.mdpi.com/2078-2489/11/2/125</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swin-unet: Unet-like pure transformer for medical image segmentation. In: Computer Vision – ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part III. p. 205–218. Springer-Verlag, Berlin, Heidelberg (2023). https://doi.org/10.1007/978-3-031-25066-8_9, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-25066-8_9" title="">https://doi.org/10.1007/978-3-031-25066-8_9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chi, J., Li, Z., Sun, Z., Yu, X., Wang, H.: Hybrid transformer unet for thyroid segmentation from ultrasound scans. Computers in Biology and Medicine <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">153</span>, 106453 (2023). https://doi.org/https://doi.org/10.1016/j.compbiomed.2022.106453, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0010482522011611" title="">https://www.sciencedirect.com/science/article/pii/S0010482522011611</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248–255 (2009). https://doi.org/10.1109/CVPR.2009.5206848

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=YicbFdNTTy" title="">https://openreview.net/forum?id=YicbFdNTTy</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Edlund, C., Jackson, T.R., Khalid, N., Bevan, N., Dale, T., Dengel, A., Ahmed, S., Trygg, J., Sjögren, R.: Livecell—a large-scale dataset for label-free live cell segmentation (Aug 2021). https://doi.org/10.1038/s41592-021-01249-6, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41592-021-01249-6" title="">http://dx.doi.org/10.1038/s41592-021-01249-6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Fan, C.M., Liu, T.J., Liu, K.H.: Sunet: swin transformer unet for image denoising. In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). pp. 2333–2337. IEEE (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Fishman, D., Salumaa, S., Majoral, D., Laasfeld, T., Peel, S., Wildenhain, J., Schreiner, A., Palo, K., Parts, L.: Practical segmentation of nuclei in brightfield cell images with neural networks trained on fluorescently labelled samples (Jun 2021). https://doi.org/10.1111/jmi.13038, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1111/jmi.13038" title="">http://dx.doi.org/10.1111/jmi.13038</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In: Crimi, A., Bakas, S. (eds.) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. pp. 272–284. Springer International Publishing, Cham (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In: 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 1748–1758 (2022). https://doi.org/10.1109/WACV51458.2022.00181

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks for visual recognition. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) Computer Vision – ECCV 2014. pp. 346–361. Springer International Publishing, Cham (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Iakubovskii, P.: Segmentation models pytorch (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/qubvel/segmentation_models.pytorch" title="">https://github.com/qubvel/segmentation_models.pytorch</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Itseez: Open source computer vision library. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/itseez/opencv" title="">https://github.com/itseez/opencv</a> (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. In: 2023 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 3992–4003 (2023). https://doi.org/10.1109/ICCV51070.2023.00371

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kumar, N., Verma, R., Anand, D., Zhou, Y., Onder, O.F., Tsougenis, E., Chen, H., Heng, P.A., Li, J., Hu, Z., Wang, Y., Koohbanani, N.A., Jahanifar, M., Tajeddin, N.Z., Gooya, A., Rajpoot, N., Ren, X., Zhou, S., Wang, Q., Shen, D., Yang, C.K., Weng, C.H., Yu, W.H., Yeh, C.Y., Yang, S., Xu, S., Yeung, P.H., Sun, P., Mahbod, A., Schaefer, G., Ellinger, I., Ecker, R., Smedby, O., Wang, C., Chidester, B., Ton, T.V., Tran, M.T., Ma, J., Do, M.N., Graham, S., Vu, Q.D., Kwak, J.T., Gunda, A., Chunduri, R., Hu, C., Zhou, X., Lotfi, D., Safdari, R., Kascenas, A., O’Neil, A., Eschweiler, D., Stegmaier, J., Cui, Y., Yin, B., Chen, K., Tian, X., Gruening, P., Barth, E., Arbel, E., Remer, I., Ben-Dor, A., Sirazitdinova, E., Kohl, M., Braunewell, S., Li, Y., Xie, X., Shen, L., Ma, J., Baksi, K.D., Khan, M.A., Choo, J., Colomer, A., Naranjo, V., Pei, L., Iftekharuddin, K.M., Roy, K., Bhattacharjee, D., Pedraza, A., Bueno, M.G., Devanathan, S., Radhakrishnan, S., Koduganty, P., Wu, Z., Cai, G., Liu, X., Wang, Y., Sethi, A.:
A multi-organ nucleus segmentation challenge. IEEE Transactions on Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">39</span>(5), 1380–1391 (2020). https://doi.org/10.1109/TMI.2019.2947628

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.: A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE Transactions on Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">36</span>(7), 1550–1560 (2017). https://doi.org/10.1109/TMI.2017.2677499

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 936–944 (2017). https://doi.org/10.1109/CVPR.2017.106

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection (2017). https://doi.org/10.48550/ARXIV.1708.02002, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1708.02002" title="">https://arxiv.org/abs/1708.02002</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 9992–10002 (2021). https://doi.org/10.1109/ICCV48922.2021.00986

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc. (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. pp. 234–241. Springer International Publishing, Cham (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Shi, W., Xu, J., Gao, P.: Ssformer: A lightweight transformer for semantic segmentation. In: 2022 IEEE 24th International Workshop on Multimedia Signal Processing (MMSP). pp. 1–5 (2022). https://doi.org/10.1109/MMSP55362.2022.9949177

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Jorge Cardoso, M.: Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In: Cardoso, M.J., Arbel, T., Carneiro, G., Syeda-Mahmood, T., Tavares, J.M.R., Moradi, M., Bradley, A., Greenspan, H., Papa, J.P., Madabhushi, A., Nascimento, J.C., Cardoso, J.S., Belagiannis, V., Lu, Z. (eds.) Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. pp. 240–248. Springer International Publishing, Cham (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
University of Tartu: High performance computing center, institute of computer science (2018). https://doi.org/10.23673/PH6N-0144

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yadan, O.: Hydra - a framework for elegantly configuring complex applications. Github (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/hydra" title="">https://github.com/facebookresearch/hydra</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Yildirim, B., Cole, J.M.: Bayesian particle instance segmentation for electron microscopy image quantification (Mar 2021). https://doi.org/10.1021/acs.jcim.0c01455, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/acs.jcim.0c01455" title="">http://dx.doi.org/10.1021/acs.jcim.0c01455</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P.H., Zhang, L.: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In: CVPR (2021)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 12:15:56 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
