<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Design of an Open-Source Architecture for Neural Machine Translation</title>
<!--Generated on Wed Mar  6 09:48:52 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.03582v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S1" title="1 Credits ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Credits</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S2" title="2 Introduction ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S3" title="3 Related Work ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S3.SS1" title="3.1 NMT ‣ 3 Related Work ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S3.SS2" title="3.2 NMT Tools ‣ 3 Related Work ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>NMT Tools</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4" title="4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Architecture of adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS1" title="4.1 adaptNMT ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>adaptNMT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS1.SSS1" title="4.1.1 Initialization and logging ‣ 4.1 adaptNMT ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Initialization and logging</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS1.SSS2" title="4.1.2 Modes of operation ‣ 4.1 adaptNMT ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Modes of operation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS1.SSS3" title="4.1.3 Customization of models ‣ 4.1 adaptNMT ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Customization of models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS1.SSS4" title="4.1.4 Use of subword segmentation ‣ 4.1 adaptNMT ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Use of subword segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS1.SSS5" title="4.1.5 Translation and evaluation ‣ 4.1 adaptNMT ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.5 </span>Translation and evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.SS2" title="4.2 Infrastructure ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S5" title="5 Discussion ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S6" title="6 Conclusion and Future Work ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.03582v1 [cs.CL] 06 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Design of an Open-Source Architecture for Neural Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Séamus Lankford
<br class="ltx_break"/>ADAPT Centre, 
<br class="ltx_break"/>MTU, Cork, Ireland
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">seamus.lankford@mtu.ie</span> <span class="ltx_ERROR undefined" id="id2.2.id2">\And</span>Haithem Afli
<br class="ltx_break"/>ADAPT Centre, 
<br class="ltx_break"/>MTU, Cork, Ireland
<br class="ltx_break"/>          <span class="ltx_text ltx_font_typewriter" id="id3.3.id3">haithem.afli@mtu.ie</span>
<span class="ltx_ERROR undefined" id="id4.4.id4">\And</span>Andy Way
<br class="ltx_break"/>ADAPT Centre, 
<br class="ltx_break"/>DCU, Dublin, Ireland
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">andy.way@dcu.ie</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO<sub class="ltx_sub" id="id6.id1.1">2</sub> emissions generated during model development. The application is freely available.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://github.com/adaptNMT" title="">http://github.com/adaptNMT</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Credits</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">This research is supported by Science Foundation Ireland through the ADAPT Centre (Grant 13/RC/2106) (www.adaptcentre.ie) at Dublin City University. This research was also funded by the Munster Technological University.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Introduction</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Explainable Artificial Intelligence (XAI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx1" title="">Arrieta et al., 2020</a>]</cite> aims to ensure that the outcomes of AI solutions are easily comprehensible to humans. In light of this goal, adaptNMT has been developed to provide users with a form of Explainable Neural Machine Translation (XNMT). The typical NMT process comprises several independent stages, including setting up the environment, preparing the dataset, training subword models, parameterizing and training the main models, evaluating and deploying them. By adopting a modular approach, this framework has established an effective NMT model development process that caters to both technical and non-technical practitioners in the field. To address the environmental impact of building and running large AI models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx10" title="">Henderson et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx12" title="">Jooste et al., 2022b</a>]</cite>, we have also produced a “green report” that calculates carbon emissions. While primarily intended as an information aid, this report will hopefully encourage the development of reusable and sustainable models.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">This research endeavors to create models and applications that address the challenges of language technology, which will be particularly beneficial for those new to the field of Machine Translation (MT) and those seeking to learn more about NMT.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The application is built on OpenNMT<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opennmt.net" title="">https://opennmt.net</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx14" title="">Klein et al., 2017</a>]</cite> and thus inherits all of its features. Unlike many NMT toolkits, a command line interface (CLI) is not used, and the interface is designed and fully implemented in Google Colab.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="colab.research.google.com" title="">colab.research.google.com</a></span></span></span> For both educational and research purposes, a cloud-hosted solution like Colab is often more user-friendly. Additionally, the training of models can be monitored and controlled via a Google Colab mobile app, which is useful for long-run builds. The adaptNMT framework also includes GUI controls that allow for the customization of all crucial parameters needed for NMT model training.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The application can be run in local mode to utilize existing infrastructure or hosted mode for rapid infrastructure scaling. A deploy function is also included to allow for the immediate deployment of trained models.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">This paper begins by presenting background information on NMT and NMT tools in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S3" title="3 Related Work ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, followed by a detailed description of the adaptNMT architecture and its key features in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4" title="4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>. The system is discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S5" title="5 Discussion ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> before concluding with a discussion of future work in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S6" title="6 Conclusion and Future Work ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>. A more in-depth system description, coupled with an empirical evaluation
of models developed using the application, is outlined in a separate paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx20" title="">Lankford et al., 2023a</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>NMT</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In addition to the ongoing research dedicated to developing state-of-the-art (SOTA) NMT models, comprehensive descriptions of this technology are readily available in the literature, making it accessible to individuals who are new to the field or have limited technical expertise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx31" title="">Way, 2019</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">NMT has benefitted from the availability of large parallel corpora, leading to the development of high-performing MT models. The field of MT has experienced significant advancements through the application of NMT, particularly after the introduction of the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx29" title="">Vaswani et al., 2017</a>]</cite> architecture, which has resulted in SOTA performance across multiple language pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx5" title="">Bojar et al., 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx6" title="">Bojar et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx16" title="">Lankford et al., 2021a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx17" title="">Lankford et al., 2021b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx18" title="">Lankford et al., 2022a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx19" title="">Lankford et al., 2022b</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>NMT Tools</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In essence, adaptNMT is an IPython wrapper built on OpenNMT, enabling it to benefit from OpenNMT’s extensive feature set and continuous code maintenance. However, adaptNMT takes abstraction to a higher level than OpenNMT, with greater focus on usability, particularly for newcomers. As a result, adaptNMT facilitates easy and fast deployment, offering features such as more pre-processing, as well as GUI control over model creation. Moreover, it incorporates green features in line with current research efforts towards smaller models with reduced carbon footprints, making it suitable for educational and research environments alike.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Other commonly used frameworks for developing NMT systems include FAIRSEQ<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairseq" title="">https://github.com/facebookresearch/fairseq</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx24" title="">Ott et al., 2019</a>]</cite>, an open-source sequence modelling toolkit based on PyTorch that allows for training models for translation, summarization, and language modelling. Marian<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://marian-nmt.github.io" title="">https://marian-nmt.github.io</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx13" title="">Junczys-Dowmunt et al., 2018</a>]</cite>, on the other hand, is an NMT framework based on dynamic computation graphs and developed using C++. OpenNMT is an open-source NMT framework that has been widely adopted in the research community and covers the entire MT workflow from data preparation to live inference.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Architecture of adaptNMT</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">After providing a general overview of NMT and NMT development systems, we introduce the adaptNMT tool, which enables users to configure the components of the NMT development process. The platform’s system architecture is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.F1" title="Figure 1 ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. The tool is built as an IPython notebook and leverages the Pytorch implementation of OpenNMT for training models. Additionally, SentencePiece is used to train subword models. Using a Jupyter notebook facilitates sharing the application with other members of the MT community, and the application’s setup is simplified since all necessary packages are downloaded dynamically as the application runs.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S4.F1.g1" src="extracted/5452427/autonmt_arch.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Proposed architecture for adaptNMT: a language-agnostic NMT development environment. The system is designed to run either in the cloud or using local infrastructure. Models are trained using parallel corpora. Visualization and extensive logging enable real-time monitoring. Models are developed using vanilla RNN-based NMT, Transformer-based approaches or transfer learning using a fine-tuning approach. Translation and evaluation can be carried out using either single models or ensembles.</figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The system has two deployment options: running it locally or as a Colab instance via Google Cloud. In order to build translation models, the system requires parallel text corpora for both the source and target languages. A Tensorboard visualization allows for real-time monitoring of the model training process. At runtime, users can select to use the system for either model building or translation services, or both. Additionally, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#S4.F1" title="Figure 1 ‣ 4 Architecture of adaptNMT ‣ Design of an Open-Source Architecture for Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, the system enables the generation of an ensemble output during translation. Finally, trained models can be easily deployed to a pre-configured location.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>adaptNMT</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The application may be run as an IPython Jupyter notebook or as a Google Colab application. Given the ease of integrating large Google drive storage into Colab, the application has been used exclusively as a Google Colab application for our own experiments.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Initialization and logging</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Initialization enables connection to Google Drive to run experiments, automatic installation of Python, OpenNMT,<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opennmt.net" title="">https://opennmt.net</a></span></span></span> SentencePiece,<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/sentencepiece" title="">https://github.com/google/sentencepiece</a></span></span></span> Pytorch and other applications. The visualization section enables real-time graphing of model development. All log files are stored and can be viewed to inspect training convergence, the model’s training and validation accuracy and changes in learning rates.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Modes of operation</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">There are two modes of operation: local or cloud. In local mode, the application is run so that models are built using the user’s local GPU resources. The option to use cloud mode enables users to develop models using Google’s GPU clusters. For shorter training times, the unpaid Colab option is adequate. However, for a small monthly subscription, the Google Colab Pro option is worthwhile since users have access to improved GPU and compute resources. Furthermore, using Google Cloud may be considered as the “green option” since its platform uses 100% renewables <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx15" title="">Lacoste et al., 2019</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Customization of models</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">The system has been developed to allow users to select variations to the underlying model architecture. A vanilla RNN or Transformer approach may be selected to develop the NMT model. The customization mode enables users to specify the exact parameters required for the chosen approach. One of the features, AutoBuild, enables a user to build an NMT model in three simple steps: (i) upload source and target files, (ii) select RNN or Transformer, and (iii) click AutoBuild.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Use of subword segmentation</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">In the NMT development process, users can specify the type of optimizer for learning and choose from different subword models. The subword model functionality allows for the selection of a subword model type and the choice of vocabulary size, currently offering either a SentencePiece unigram or a SentencePiece BPE model.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1">A user may upload a dataset which includes the train, validation and test splits for both source and target languages. In cases where a user has not already created the required splits for model training, single source and target files may be uploaded. Automated splitting of the uploaded dataset into train, validation, and test files is then performed based on the user’s chosen split ratio.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p3">
<p class="ltx_p" id="S4.SS1.SSS4.p3.1">Given that building NMT models typically demands long training times, an automatic notification feature is incorporated that informs the user by email when model training has been completed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5 </span>Translation and evaluation</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">The application supports not only the training of models but also the translation and evaluation of model performance. For translation using pre-built models, users can specify the model name as a hyperparameter which is subsequently used to translate and evaluate the test files. The option for creating an ensemble output is also available, with users simply naming the models to be used in generating the ensemble output.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.1">Once the system has been built, the user can select the model to be used for translating the test set. While human evaluation is often considered the most insightful approach for evaluating translation quality, it can be limited by factors such as availability, cost, and subjectivity. Thus, automatic evaluation metrics are frequently employed, particularly by developers monitoring incremental progress of systems. A further discussion on the advantages and disadvantages of human and automatic evaluation is available in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx30" title="">Way, 2018</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p3">
<p class="ltx_p" id="S4.SS1.SSS5.p3.1">Several automatic evaluation metrics provided by SacreBleu<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mjpost/sacrebleu" title="">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx27" title="">Post, 2018</a>]</cite> are used: BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx25" title="">Papineni et al., 2002</a>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx28" title="">Snover et al., 2006</a>]</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx26" title="">Popović, 2015</a>]</cite>. Translation quality can also be evaluated using Meteor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx9" title="">Denkowski and Lavie, 2014</a>]</cite> and F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx23" title="">Melamed et al., 2003</a>]</cite>. Note that BLEU, ChrF, Meteor and F1 are precision-based metrics, so higher scores are better, whereas TER is an error-based metric and lower scores indicate better translation quality. Evaluation options available include standard (truecase) and lowercase BLEU scores, a sentence-level BLEU score option, ChrF1 and ChrF3.
</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p4">
<p class="ltx_p" id="S4.SS1.SSS5.p4.1">There are three levels of logging: model development logs for graphing, training console output and experimental results. A references section outlines resources which are relevant to developing, using and understanding adaptNMT. Validation during training is currently conducted using model accuracy and perplexity (PPL).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Infrastructure</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Rapid prototype development is possible through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16GB graphic cards and up to 27GB of memory when available.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Numerous tools have been developed to assess the carbon footprint of NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx3" title="">Bannour et al., 2021</a>]</cite>. The notion of sustainable NLP has also gained momentum as an independent research track, with high-profile conferences such as the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">EACL 2021 Green and Sustainable NLP</span> track dedicating resources to this area.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://2021.eacl.org/news/green-and-sustainable-nlp" title="">https://2021.eacl.org/news/green-and-sustainable-nlp</a></span></span></span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Given these developments, we have incorporated a ”green report” into adaptNMT that logs the kgCO<sub class="ltx_sub" id="S5.p2.1.1">2</sub> generated during model development. This aligns with the industry’s increasing focus on quantifying the environmental impact of NLP. In fact, it has been demonstrated that high-performing MT systems can be developed with much lower carbon footprints, leading to significant energy cost savings for a real translation company <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx11" title="">Jooste et al., 2022a</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The risks associated with relying on Large Language Models (LLMs) have been well-documented in the literature. The discussion surrounding these models emphasizes not only their environmental impact but also the inherent biases and dangers they pose for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx4" title="">Bender et al., 2021</a>]</cite>. It is important to note that smaller, in-domain datasets can yield high-performing NMT models, and the adaptNMT framework makes this approach easily accessible and understandable.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We have introduced adaptNMT, an application that manages the entire NMT model development, evaluation, and deployment workflow.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">As for future work, our development efforts will be directed towards incorporating new transfer learning methods and improving our ability to track environmental costs. We will integrate modern zero-shot and few-shot approaches, as seen in the GPT3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx7" title="">Brown et al., 2020</a>]</cite> and Facebook LASER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx2" title="">Artetxe and Schwenk, 2019</a>]</cite> frameworks. While the existing adaptNMT application is focused on customizing NMT models, we will also develop a separate application, adaptLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx21" title="">Lankford et al., 2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx22" title="">Lankford et al., 2023c</a>]</cite>, for fine-tuning LLMs. In particular, adaptLLM will cater for low-resource language pairs such as NLLB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03582v1#bib.bibx8" title="">Costa-jussà et al., 2022</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The green report integrated into the application represents our first implementation of a sustainable NLP feature within adaptNMT. We plan to enhance this feature by improving the user interface and providing recommendations on how to develop greener models. As an open-source project, we invite the community to contribute new ideas and improvements to the development of this feature.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Arrieta et al., 2020] </span>
<span class="ltx_bibblock">
Arrieta, Alejandro Barredo, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">Information Fusion</span>, 58:82–115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Artetxe and Schwenk, 2019] </span>
<span class="ltx_bibblock">
Artetxe, Mikel and Holger Schwenk.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">Transactions of the Association for Computational Linguistics</span>, 7:597–610.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Bannour et al., 2021] </span>
<span class="ltx_bibblock">
Bannour, Nesrine, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</span>, pages 11–21, Virtual, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Bender et al., 2021] </span>
<span class="ltx_bibblock">
Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</span>, FAccT ’21, page 610–623, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Bojar et al., 2017] </span>
<span class="ltx_bibblock">
Bojar, Ondřej, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Findings of the 2017 conference on machine translation (WMT17).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">Proceedings of the Second Conference on Machine Translation</span>, pages 169–214, Copenhagen, Denmark, September. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Bojar et al., 2018] </span>
<span class="ltx_bibblock">
Bojar, Ondřej, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, and Christof Monz.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Findings of the 2018 conference on machine translation (WMT18).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</span>, pages 272–303, Belgium, Brussels, October. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Brown et al., 2020] </span>
<span class="ltx_bibblock">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">Advances in neural information processing systems</span>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Costa-jussà et al., 2022] </span>
<span class="ltx_bibblock">
Costa-jussà, Marta R, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">arXiv preprint arXiv:2207.04672</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Denkowski and Lavie, 2014] </span>
<span class="ltx_bibblock">
Denkowski, Michael and Alon Lavie.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Meteor universal: Language specific translation evaluation for any target language.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">Proceedings of the Ninth Workshop on Statistical Machine Translation</span>, pages 376–380, Baltimore, Maryland, USA, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Henderson et al., 2020] </span>
<span class="ltx_bibblock">
Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Towards the systematic reporting of the energy and carbon footprints of machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">Journal of Machine Learning Research</span>, 21(248):1–43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Jooste et al., 2022a] </span>
<span class="ltx_bibblock">
Jooste, Wandri, Rejwanul Haque, and Andy Way.

</span>
<span class="ltx_bibblock">2022a.

</span>
<span class="ltx_bibblock">Knowledge distillation: A method for making neural machine translation more efficient.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Information</span>, 13(2).

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Jooste et al., 2022b] </span>
<span class="ltx_bibblock">
Jooste, Wandri, Andy Way, Rejwanul Haque, and Riccardo Superbo.

</span>
<span class="ltx_bibblock">2022b.

</span>
<span class="ltx_bibblock">Knowledge distillation for sustainable neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)</span>, pages 221–230, Orlando, USA, September. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Junczys-Dowmunt et al., 2018] </span>
<span class="ltx_bibblock">
Junczys-Dowmunt, Marcin, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Marian: Fast neural machine translation in C++.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">Proceedings of ACL 2018, System Demonstrations</span>, pages 116–121, Melbourne, Australia, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Klein et al., 2017] </span>
<span class="ltx_bibblock">
Klein, Guillaume, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">OpenNMT: Open-source toolkit for neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">Proceedings of ACL 2017, System Demonstrations</span>, pages 67–72, Vancouver, Canada, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Lacoste et al., 2019] </span>
<span class="ltx_bibblock">
Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Quantifying the carbon emissions of machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">arXiv preprint arXiv:1910.09700</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2021a] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, and Andy Way.

</span>
<span class="ltx_bibblock">2021a.

</span>
<span class="ltx_bibblock">Transformers for low-resource languages: Is féidir linn!

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">Proceedings of Machine Translation Summit XVIII: Research Track</span>, pages 48–60.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2021b] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, and Andy Way.

</span>
<span class="ltx_bibblock">2021b.

</span>
<span class="ltx_bibblock">Machine translation in the covid domain: an English-Irish case study for LoResMT 2021.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</span>, pages 144–150, Virtual, August. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2022a] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, Órla Ní Loinsigh, and Andy Way.

</span>
<span class="ltx_bibblock">2022a.

</span>
<span class="ltx_bibblock">gaHealth: An English–Irish bilingual corpus of health data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">Proceedings of the Thirteenth Language Resources and Evaluation Conference</span>, pages 6753–6758, Marseille, France, June. European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2022b] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, and Andy Way.

</span>
<span class="ltx_bibblock">2022b.

</span>
<span class="ltx_bibblock">Human evaluation of English–Irish Transformer-Based NMT.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">Information</span>, 13(7):309.

</span>
<span class="ltx_bibblock">19pp.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2023a] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, and Andy Way.

</span>
<span class="ltx_bibblock">2023a.

</span>
<span class="ltx_bibblock">adaptNMT: an open-source, language-agnostic development environment for neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Language Resources and Evaluation</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2023b] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, and Andy Way.

</span>
<span class="ltx_bibblock">2023b.

</span>
<span class="ltx_bibblock">adaptLLM: Fine-tuning large language models for improved machine translation. an open-source approach with NLLB and GPT-J integration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">Journal of Artificial Intelligence Research [manuscript submitted]</span>, El Segundo, CA, United States. AI Access Foundation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Lankford et al., 2023c] </span>
<span class="ltx_bibblock">
Lankford, Séamus, Haithem Afli, and Andy Way.

</span>
<span class="ltx_bibblock">2023c.

</span>
<span class="ltx_bibblock">Fine-tuning LLMs using low-resource language pairs: A comparative study with loresmt2021 shared task baselines.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">Proceedings of Machine Translation Summit XIX: Research Track [manuscript submitted]</span>, Macau SAR, China, September 4-8. Asia-Pacific Association for Machine Translation (AAMT).

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Melamed et al., 2003] </span>
<span class="ltx_bibblock">
Melamed, I. Dan, Ryan Green, and Joseph P. Turian.

</span>
<span class="ltx_bibblock">2003.

</span>
<span class="ltx_bibblock">Precision and recall of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">Companion Volume of the Proceedings of HLT-NAACL 2003 - Short Papers</span>, pages 61–63, Edmonton, Canada.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Ott et al., 2019] </span>
<span class="ltx_bibblock">
Ott, Myle, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</span>, pages 48–53, Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Papineni et al., 2002] </span>
<span class="ltx_bibblock">
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</span>, pages 311–318, Philadelphia, PA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Popović, 2015] </span>
<span class="ltx_bibblock">
Popović, Maja.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">chrF: character n-gram F-score for automatic MT evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</span>, pages 392–395, Lisbon, Portugal, September. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[Post, 2018] </span>
<span class="ltx_bibblock">
Post, Matt.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 186–191, Brussels, Belgium, October. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[Snover et al., 2006] </span>
<span class="ltx_bibblock">
Snover, Matthew, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock">A study of translation edit rate with targeted human annotation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</span>, pages 223–231, Cambridge, Massachusetts, USA, August 8-12. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al., 2017] </span>
<span class="ltx_bibblock">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx29.1.1">Proceedings of the 31st International Conference on Neural Information Processing Systems</span>, NIPS’17, page 6000–6010, Red Hook, NY, USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[Way, 2018] </span>
<span class="ltx_bibblock">
Way, Andy.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Quality expectations of machine translation.

</span>
<span class="ltx_bibblock">In Moorkens, Joss, Sheila Castilho, Federico Gaspari, and Stephen Doherty, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">Translation Quality Assessment: From Principles to Practice</span>, pages 159–178. Springer, Cham, Switzerland.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[Way, 2019] </span>
<span class="ltx_bibblock">
Way, Andy.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Machine translation: Where are we at today?

</span>
<span class="ltx_bibblock">In Angelone, Eric, Gary Massey, and Maureen Ehrensberger-Dow, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">The Bloomsbury Companion to Language Industry Studies</span>, page 311—332. Bloomsbury, London.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Mar  6 09:48:52 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
