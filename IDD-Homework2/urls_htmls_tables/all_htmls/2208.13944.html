<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.13944] Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data</title><meta property="og:description" content="Accurately annotated image datasets are essential components for studying animal behaviors from their poses. Compared to the number of species we know and may exist, the existing labeled pose datasets cover only a smalâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.13944">

<!--Generated on Wed Mar 13 20:26:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\addauthor</span>
<p id="p1.2" class="ltx_p">Le Jiang1
<span id="p1.2.1" class="ltx_ERROR undefined">\addauthor</span>Shuangjun Liu1
<span id="p1.2.2" class="ltx_ERROR undefined">\addauthor</span>Xiangyu Bai1
<span id="p1.2.3" class="ltx_ERROR undefined">\addauthor</span>Sarah Ostadabbashttps://web.northeastern.edu/ostadabbas1
<span id="p1.2.4" class="ltx_ERROR undefined">\addinstitution</span>
Augmented Cognition Lab,
<br class="ltx_break">Electrical and Computer Engineering Department,
Northeastern University,
<br class="ltx_break">Boston, MA, USA

Prior-Aware Synthetic Data to the Rescue




</p>
</div>
<h1 class="ltx_title ltx_title_document">Prior-Aware Synthetic Data to the Rescue:
<br class="ltx_break">Animal Pose Estimation with Very Limited Real Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Accurately annotated image datasets are essential components for studying animal behaviors from their poses. Compared to the number of species we know and may exist, the existing labeled pose datasets cover only a small portion of them, while building comprehensive large-scale datasets is prohibitively expensive. Here, we present a very data efficient strategy targeted for pose estimation in quadrupeds that requires only a small amount of real images from the target animal. It is confirmed that fine-tuning a backbone network with pretrained weights on generic image datasets such as ImageNet can mitigate the high demand for target animal pose data and shorten the training time by learning the the prior knowledge of object segmentation and keypoint estimation in advance. However, when faced with serious data scarcity (i.e., <math id="id1.1.m1.1" class="ltx_Math" alttext="&lt;10^{2}" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">&lt;</mo><msup id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml"><mn id="id1.1.m1.1.1.3.2" xref="id1.1.m1.1.1.3.2.cmml">10</mn><mn id="id1.1.m1.1.1.3.3" xref="id1.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><lt id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">absent</csymbol><apply id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3"><csymbol cd="ambiguous" id="id1.1.m1.1.1.3.1.cmml" xref="id1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="id1.1.m1.1.1.3.2.cmml" xref="id1.1.m1.1.1.3.2">10</cn><cn type="integer" id="id1.1.m1.1.1.3.3.cmml" xref="id1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">&lt;10^{2}</annotation></semantics></math> real images), the model performance stays unsatisfactory, particularly for limbs with considerable flexibility and several comparable parts. We therefore introduce a prior-aware synthetic animal data generation pipeline called PASyn to augment the animal pose data essential for robust pose estimation. PASyn generates a probabilistically-valid synthetic pose dataset, SynAP, through training a variational generative model on several animated 3D animal models. In addition, a style transfer strategy is utilized to blend the synthetic animal image into the real backgrounds. We evaluate the improvement made by our approach with three popular backbone networks and test their pose estimation accuracy on publicly available animal pose images as well as collected from real animals in a zoo<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The SynAP dataset and PASyn code is available at <a target="_blank" href="https://github.com/ostadabbas/Prior-aware-Synthetic-Data-Generation-PASyn-" title="" class="ltx_ref ltx_href">https://github.com/ostadabbas/Prior-aware-Synthetic-Data-Generation-PASyn-</a>.</span></span></span>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The research on animal pose estimation has grown in recent years, covering 2D/3D pose estimation, animal model recovery, and multi-animal pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Mu etÂ al.(2020)Mu, Qiu, Hager, and Yuille</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Li and Lee(2021)</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Mathis etÂ al.(2021)Mathis, Biasi, Schneider, Yuksekgonul, Rogers,
Bethge, and Mathis</a>, <a href="#bib.bibx28" title="" class="ltx_ref">Zuffi etÂ al.(2019)Zuffi, Kanazawa, Berger-Wolf, and
Black</a>]</cite>. The makeup of the available training sets for the animal pose estimation models divides the study into two branches. The first proposes training the model with significant amounts of labeled real data of a single species.
The other one is based on a small amount of real data of a target animal and more data from other adjacent domains to make up for the data scarcity of the target animal. Current data scarcity solution is centered around learning animalsâ€™ common prior knowledge from large amounts of real data of similar species or even humans. Previous research has shown that quadrupeds, including humans, have similar appearances and skeletons, and that knowledge can be shared <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Cao etÂ al.(2019)Cao, Tang, Fang, Shen, Lu, and Tai</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Sanakoyeu etÂ al.(2020)Sanakoyeu, Khalidov, McCarthy, Vedaldi, and
Neverova</a>, <a href="#bib.bibx16" title="" class="ltx_ref">Neverova etÂ al.(2020)Neverova, Novotny, Khalidov, Szafraniec, Labatut,
and Vedaldi</a>]</cite>.
This method is easier to implement after the largest labeled dataset for general animal pose estimation, AP10K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>]</cite>, was made publicly available in 2021. However, there are restrictions imposed by pose label divergence and the high cost of customization in the existing datasets. In contrast to human, there are huge varieties between animals with different length of bones, number of joints, and extra body parts. Large animal datasets with uniform labeling may leave out the special needs of researchers looking for creatures with unique physical traits.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2208.13944/assets/figures/overview6.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The outcome samples show the effect of model trained with or without synthetic animal pose (SynAP) dataset. The left side is the pose estimation results based on the DeepLabCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Mathis etÂ al.(2021)Mathis, Biasi, Schneider, Yuksekgonul, Rogers,
Bethge, and Mathis</a>]</cite> pre-trained on ImageNet and fine-tuned by 99 real animal images and the right side shows the results of the same model when trained on the same amount of real images in addition to our SynAP dataset. Wrong predictions are marked by red cross.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic data is another potential choice of adjacent domain for the target animal when aiming for both "inexpensive" and "personalization" aspects. Once the synthetic animal model is built, the label can be defined on the target model and annotating pose data becomes considerably faster and less expensive. In addition to synthetic data, previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Mathis etÂ al.(2018)Mathis, Mamidanna, Cury, Abe, Murthy, Mathis, and
Bethge</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>]</cite> have proven that the backbone networks pre-trained on large general datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Deng etÂ al.(2009)Deng, Dong, Socher, Li, Li, and
Fei-Fei</a>]</cite> will gain the prior of object segmentation and keypoint prediction. Thus, training even with few frames from the target leads to a high precision pose prediction. However, when there are far fewer images of the target available (e.g. less than 100), the size of the training set is insufficient for a robust model fine-tuning. The accuracy of the model would decrease significantly when facing more actions from free-ranging behaviors, more self or environmental occlusions, and changes in the environment, textures, and shapes. Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that how EfficientNet-B6 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Mathis etÂ al.(2021)Mathis, Biasi, Schneider, Yuksekgonul, Rogers,
Bethge, and Mathis</a>]</cite> pre-trained on ImageNet and fine-tuned by 99 real animal images is error prone when tested on the images in the wild.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To account for severe data scarcity and guarantee a high degree of label personalization, we present a cost-effective and generic prior-aware synthetic data generation pipeline, called PASyn, for animals pose estimate tasks that suffer from severe data scarcity. In short, this paper contributions are: (1) designing a novel variational autoencoder (VAE)-based synthetic animal data generation pipeline to generate probabilistically-valid pose data and verifying its performance on several animal pose test sets; (2) blending the synthetic animal images into real backgrounds through style transfer to mitigate the inconsistency between synthetic and real domains; and (3) building a synthetic animal pose (SynAP) dataset through PASyn, containing 3,000 zebra images and extending it with the 3,600 images of six common quadrupeds including, cows, dogs, sheep, deer, giraffes, and horses to make SynAP+ dataset; and (4) releasing a new pose-labeled dataset of mountain zebras in zoo.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">It has been almost ten years since the early animal pose estimation work introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Vicente and Agapito(2013)</a>]</cite>. Yet, the performance of these models is still far inferior to the human pose estimation in terms of accuracy, cross-domain adaptation, and model robustness. This is mainly due to a lack of real-world data, which is the challenge for almost all animal pose estimation work.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Animal Pose Estimation with Label Scarcity</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The most common statement in animal pose estimation articles is the lack of labeled dataset for training, which has been mentioned in many works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Wu etÂ al.(2020)Wu, Buchanan, Whiteway, Schartner, Meijer, Noel,
Rodriguez, Everett, Norovich, Schaffer, etÂ al.</a>, <a href="#bib.bibx2" title="" class="ltx_ref">Cao etÂ al.(2019)Cao, Tang, Fang, Shen, Lu, and Tai</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Li and Lee(2021)</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Mu etÂ al.(2020)Mu, Qiu, Hager, and Yuille</a>, <a href="#bib.bibx13" title="" class="ltx_ref">Mathis etÂ al.(2018)Mathis, Mamidanna, Cury, Abe, Murthy, Mathis, and
Bethge</a>, <a href="#bib.bibx8" title="" class="ltx_ref">Graving etÂ al.(2019)Graving, Chae, Naik, Li, Koger, Costelloe, and
Couzin</a>]</cite>. Numerable variety of species and subspecies, and considerable differences in physical characteristics and behavior patterns between them cause it difficult to form a labeled dataset with adequate samples. The cross-domain adaptation challenge exacerbates the situation. For each new animal, it is necessary to collect data from scratch and label them, since it is insufficient to learn the prior knowledge exclusively from the data of other animals. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Cao etÂ al.(2019)Cao, Tang, Fang, Shen, Lu, and Tai</a>]</cite> proposed a cross-domain adaptive method and a large multi-species dataset, Animal-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Jinkun etÂ al.(2019)Jinkun, Hongyang, Hao-Shu, Xiaoyong, Cewu, and
Yu-Wing</a>]</cite> to learn the shared feature space between human and animals, in order to transfer the prior knowledge between them. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>]</cite> built their own large multi-species dataset, called AP10k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>]</cite> in order to train a robust and cross-domain model. However, both models are still unable to achieve the same level of
pose prediction as animals in-domain when facing unseen species.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Animal Pose Estimation with Synthetic Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Synthetic data is a promising substitute for real data in previous works. Coarse prior can be learnt and then it would be used to build pseudo-labels for enormous amounts of unlabeled real animal data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Mu etÂ al.(2020)Mu, Qiu, Hager, and Yuille</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Li and Lee(2021)</a>]</cite>. The fly in the ointment is that works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Mu etÂ al.(2020)Mu, Qiu, Hager, and Yuille</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Li and Lee(2021)</a>]</cite> still use significant amounts of real data (such as TigDog dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Pero etÂ al.(2016)Pero, Susanna, Rahul, and Vittorio</a>]</cite>) in training, which may not be possible to access for the unseen species. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Zuffi etÂ al.(2017)Zuffi, Kanazawa, Jacobs, and Black</a>]</cite>, which focuses on animal model recovery, also gives an extraordinary hint on this problem. It purposed a general 3D animal model (called SMAL) by learning the animal toysâ€™ scan, and use the SMAL model and a few real pictures to generate a large amount of realistic synthetic data by adjusting the modelâ€™s texture, shape, pose, background, camera, and other parameters. They also trained an end-to-end network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Zuffi etÂ al.(2019)Zuffi, Kanazawa, Berger-Wolf, and
Black</a>]</cite> using only synthetic Grevyâ€™s zebra data, which came from direct reconstruction from animal 2D images. However, their results are much worse than the current state-of-the-art in terms of 2D animal pose estimation.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Domain Gap between Real and Synthetic</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Although synthetic data has advantages in terms of cost, it still faces the problem of fatal domain gap with the real data, which is even challenging to alleviate by current domain adaptation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Ganin etÂ al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
Laviolette, Marchand, and Lempitsky</a>]</cite>. Pasting synthetic animal directly to the real background will result in the strong incongruity, mainly from the difference in projection, brightness, contrast, saturation, etc., between the two images. This incongruity can lead to excessive domain differences between synthetic and real data. Besides, to increase the texture diversity of synthetic animals to enhance the appearance robustness of the model, a common approach is to assign the synthetic data random textures from ImageNet. However, this further increases the discrepancy between synthetic and real domains and thus weakens the improvement of synthetic data in joint training of pose estimation tasks. The approach to increase model robustness and reduce domain variance by style transfer has been applied in medicine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Wagner etÂ al.(2021)Wagner, Khalili, Sharma, Boxberg, Marr, Back, and
Peng</a>]</cite>, monocular depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Atapour-Abarghouei and Breckon(2018)</a>]</cite>, person re-identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Chong etÂ al.(2021)Chong, Peng, Zhang, and Pan</a>]</cite>. We, therefore, adopt style transfer to alleviate the domain gap between the synthetic animal and the real background while increasing the texture diversity of the synthetic animal.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>PASyn Pipeline: Prior-Aware Synthetic Data Generation</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2208.13944/assets/figures/pasyn.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview architecture of our prior-aware synthetic data generation (PASyn) pipeline, composed of three parts: pose augmentation, domain transfer and dataset generation. The PASyn pipeline leads to generation of our probabilistically-valid synthetic animal pose (SynAP) dataset.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our proposed prior-aware synthetic data generation (PASyn) pipeline enables robust animal pose estimation under severe label scarcity and divergence. The architecture of PASyn is shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 PASyn Pipeline: Prior-Aware Synthetic Data Generation â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where we employ a graphic engine (i.e., Blender) to render the 3D animal mesh model into synthetic images. PASyn pipeline is composed of 3 key components: (1) pose data augmentation, (2) synthetic data stylization, and (3) synthetic animal pose image dataset generation. The details are presented in the following subsections. This paper presents PASyn with a working example of zebra as its target animal for pose estimation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Capturing Animal Pose Prior</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As evident based on the sample results shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, when the real target data provided to a pose estimation model is scarce, to improve the model performance, the prior knowledge of the target animal pose can be learned from a carefully curated synthetic dataset. To learn the animal pose priors, we use a variational autoencoder (VAE) framework. VAE has already been proven to be useful for human pose priors learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Pavlakos etÂ al.(2019)Pavlakos, Choutas, Ghorbani, Bolkart, Osman,
Tzionas, and Black</a>]</cite>, when it is trained on large-scale 3D human pose datasets. However, appropriate 3D datasets cannot be easily obtained for animal pose studies. Therefore, we feed the animation of multiple simulated animals made by human animators (inexpensively purchased from the CGTrader website), to the VAE, for it to learn the probabilistic distribution of the feasible poses of the targeted animals. Then, the trained VAE is used as a generative model to create our 3D synthetic animal pose (SynAP) dataset. As seen in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1 Capturing Animal Pose Prior â€£ 3 PASyn Pipeline: Prior-Aware Synthetic Data Generation â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a), the legs of quadrupeds like horses and dogs are similar in structure. This structure is obtained by simplifying the leg skeleton. Due to its applicability across quadrupeds, this structure is useful for training animal pose priors. Angles between neighboring bones (12 of them) in the structure shown in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.1 Capturing Animal Pose Prior â€£ 3 PASyn Pipeline: Prior-Aware Synthetic Data Generation â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(c) are chosen as data for training animal leg pose priors to minimise the influence of model sizes or bone lengths on training.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2208.13944/assets/figures/armature.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>General quadruped armature. (a) shows the real skeletons of horse (bottom) and dog (top) respectively. (b) shows the artificially designed animal skeleton based on the real one. The red box marks the skeleton of limbs which we are interested in. (c) shows the skeleton of limbs. In (d), <math id="S3.F3.3.m1.1" class="ltx_Math" alttext="v1" display="inline"><semantics id="S3.F3.3.m1.1b"><mrow id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml"><mi id="S3.F3.3.m1.1.1.2" xref="S3.F3.3.m1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.F3.3.m1.1.1.1" xref="S3.F3.3.m1.1.1.1.cmml">â€‹</mo><mn id="S3.F3.3.m1.1.1.3" xref="S3.F3.3.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><apply id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1"><times id="S3.F3.3.m1.1.1.1.cmml" xref="S3.F3.3.m1.1.1.1"></times><ci id="S3.F3.3.m1.1.1.2.cmml" xref="S3.F3.3.m1.1.1.2">ğ‘£</ci><cn type="integer" id="S3.F3.3.m1.1.1.3.cmml" xref="S3.F3.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">v1</annotation></semantics></math> and <math id="S3.F3.4.m2.1" class="ltx_Math" alttext="v2" display="inline"><semantics id="S3.F3.4.m2.1b"><mrow id="S3.F3.4.m2.1.1" xref="S3.F3.4.m2.1.1.cmml"><mi id="S3.F3.4.m2.1.1.2" xref="S3.F3.4.m2.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.F3.4.m2.1.1.1" xref="S3.F3.4.m2.1.1.1.cmml">â€‹</mo><mn id="S3.F3.4.m2.1.1.3" xref="S3.F3.4.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.4.m2.1c"><apply id="S3.F3.4.m2.1.1.cmml" xref="S3.F3.4.m2.1.1"><times id="S3.F3.4.m2.1.1.1.cmml" xref="S3.F3.4.m2.1.1.1"></times><ci id="S3.F3.4.m2.1.1.2.cmml" xref="S3.F3.4.m2.1.1.2">ğ‘£</ci><cn type="integer" id="S3.F3.4.m2.1.1.3.cmml" xref="S3.F3.4.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.m2.1d">v2</annotation></semantics></math> represent the spatial orientation of two adjacent bones. The angle between the vectors is the angle between the two bones.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.17" class="ltx_p">Our network follows the basic VAE process, including input, encoder, random sampling, decoder, and output as demonstrated in Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.1 Capturing Animal Pose Prior â€£ 3 PASyn Pipeline: Prior-Aware Synthetic Data Generation â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="A\in\mathbb{R}^{36}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">A</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">â„</mi><mn id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">36</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><in id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></in><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ´</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">â„</ci><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3">36</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">A\in\mathbb{R}^{36}</annotation></semantics></math> is a <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="1\times 36" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">36</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">1\times 36</annotation></semantics></math> vector containing the angles between 12 pairs of adjacent bones, and each space angle is decomposed into three directions, and <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mover accent="true" id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">A</mi><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><ci id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1">^</ci><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\hat{A}</annotation></semantics></math> is a vector of the same shape representing the generated angles as the model output. The role of the encoder which is composed of dense layers with rectified linear unit (ReLU) activations is to calculate the mean <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mu</annotation></semantics></math> and variance <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\sigma</annotation></semantics></math> of the network input. We use the reparameterization trick to randomly sample <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">z</annotation></semantics></math> in latent space <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="Z\in\mathbb{R}^{16}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">Z</mi><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">â„</mi><mn id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml">16</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><in id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"></in><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğ‘</ci><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2">â„</ci><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">Z\in\mathbb{R}^{16}</annotation></semantics></math>, where <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="z=\mu+\epsilon\times\sigma" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mrow id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">z</mi><mo id="S3.SS1.p2.8.m8.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.cmml">=</mo><mrow id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.3.2.cmml">Î¼</mi><mo id="S3.SS1.p2.8.m8.1.1.3.1" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">+</mo><mrow id="S3.SS1.p2.8.m8.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.3.2" xref="S3.SS1.p2.8.m8.1.1.3.3.2.cmml">Ïµ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.8.m8.1.1.3.3.1" xref="S3.SS1.p2.8.m8.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p2.8.m8.1.1.3.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.3.cmml">Ïƒ</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><eq id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1"></eq><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">ğ‘§</ci><apply id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3"><plus id="S3.SS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.1"></plus><ci id="S3.SS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.2">ğœ‡</ci><apply id="S3.SS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3"><times id="S3.SS1.p2.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.1"></times><ci id="S3.SS1.p2.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.2">italic-Ïµ</ci><ci id="S3.SS1.p2.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.3">ğœ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">z=\mu+\epsilon\times\sigma</annotation></semantics></math> and <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">e</annotation></semantics></math> satisfies the distribution in the shape of <math id="S3.SS1.p2.10.m10.2" class="ltx_Math" alttext="\mathcal{N}(0,I)" display="inline"><semantics id="S3.SS1.p2.10.m10.2a"><mrow id="S3.SS1.p2.10.m10.2.3" xref="S3.SS1.p2.10.m10.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.10.m10.2.3.2" xref="S3.SS1.p2.10.m10.2.3.2.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.10.m10.2.3.1" xref="S3.SS1.p2.10.m10.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p2.10.m10.2.3.3.2" xref="S3.SS1.p2.10.m10.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.10.m10.2.3.3.2.1" xref="S3.SS1.p2.10.m10.2.3.3.1.cmml">(</mo><mn id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">0</mn><mo id="S3.SS1.p2.10.m10.2.3.3.2.2" xref="S3.SS1.p2.10.m10.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.10.m10.2.2" xref="S3.SS1.p2.10.m10.2.2.cmml">I</mi><mo stretchy="false" id="S3.SS1.p2.10.m10.2.3.3.2.3" xref="S3.SS1.p2.10.m10.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.2b"><apply id="S3.SS1.p2.10.m10.2.3.cmml" xref="S3.SS1.p2.10.m10.2.3"><times id="S3.SS1.p2.10.m10.2.3.1.cmml" xref="S3.SS1.p2.10.m10.2.3.1"></times><ci id="S3.SS1.p2.10.m10.2.3.2.cmml" xref="S3.SS1.p2.10.m10.2.3.2">ğ’©</ci><interval closure="open" id="S3.SS1.p2.10.m10.2.3.3.1.cmml" xref="S3.SS1.p2.10.m10.2.3.3.2"><cn type="integer" id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">0</cn><ci id="S3.SS1.p2.10.m10.2.2.cmml" xref="S3.SS1.p2.10.m10.2.2">ğ¼</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.2c">\mathcal{N}(0,I)</annotation></semantics></math>. Finally, the decoder which has a mirror structure of encoder is used to reconstruct the set of space angle, and make the reconstruction results <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><mover accent="true" id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><mi id="S3.SS1.p2.11.m11.1.1.2" xref="S3.SS1.p2.11.m11.1.1.2.cmml">A</mi><mo id="S3.SS1.p2.11.m11.1.1.1" xref="S3.SS1.p2.11.m11.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><ci id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1">^</ci><ci id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">\hat{A}</annotation></semantics></math> as close to <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><mi id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><ci id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">A</annotation></semantics></math> as possible. Considering that one infeasible angle can ruin an entire pose, during the pre-test, we first assign random sampling from a normal distribution to the variable <math id="S3.SS1.p2.13.m13.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p2.13.m13.1a"><mi id="S3.SS1.p2.13.m13.1.1" xref="S3.SS1.p2.13.m13.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.1b"><ci id="S3.SS1.p2.13.m13.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.1c">z</annotation></semantics></math>, and then we create the histograms of the 12 joint angles using enough <math id="S3.SS1.p2.14.m14.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS1.p2.14.m14.1a"><mover accent="true" id="S3.SS1.p2.14.m14.1.1" xref="S3.SS1.p2.14.m14.1.1.cmml"><mi id="S3.SS1.p2.14.m14.1.1.2" xref="S3.SS1.p2.14.m14.1.1.2.cmml">A</mi><mo id="S3.SS1.p2.14.m14.1.1.1" xref="S3.SS1.p2.14.m14.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.1b"><apply id="S3.SS1.p2.14.m14.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1"><ci id="S3.SS1.p2.14.m14.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1">^</ci><ci id="S3.SS1.p2.14.m14.1.1.2.cmml" xref="S3.SS1.p2.14.m14.1.1.2">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.1c">\hat{A}</annotation></semantics></math> (<math id="S3.SS1.p2.15.m15.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S3.SS1.p2.15.m15.1a"><mo id="S3.SS1.p2.15.m15.1.1" xref="S3.SS1.p2.15.m15.1.1.cmml">â‰ˆ</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m15.1b"><approx id="S3.SS1.p2.15.m15.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m15.1c">\approx</annotation></semantics></math>10,000 poses). Based on the statistical results, we can establish the sampling range for each angle. For each range, 5% are excluded of total angles, which are far from the mean value, and all of the ranges are recorded. After the pre-test, when we generate the new poses, a â€˜pose filterâ€™ is designed to remove the pose <math id="S3.SS1.p2.16.m16.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS1.p2.16.m16.1a"><mover accent="true" id="S3.SS1.p2.16.m16.1.1" xref="S3.SS1.p2.16.m16.1.1.cmml"><mi id="S3.SS1.p2.16.m16.1.1.2" xref="S3.SS1.p2.16.m16.1.1.2.cmml">A</mi><mo id="S3.SS1.p2.16.m16.1.1.1" xref="S3.SS1.p2.16.m16.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m16.1b"><apply id="S3.SS1.p2.16.m16.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1"><ci id="S3.SS1.p2.16.m16.1.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1.1">^</ci><ci id="S3.SS1.p2.16.m16.1.1.2.cmml" xref="S3.SS1.p2.16.m16.1.1.2">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m16.1c">\hat{A}</annotation></semantics></math> as long as there is an angle outside these specified ranges and finally produce the refined poses <math id="S3.SS1.p2.17.m17.1" class="ltx_Math" alttext="\hat{A^{\prime}}" display="inline"><semantics id="S3.SS1.p2.17.m17.1a"><mover accent="true" id="S3.SS1.p2.17.m17.1.1" xref="S3.SS1.p2.17.m17.1.1.cmml"><msup id="S3.SS1.p2.17.m17.1.1.2" xref="S3.SS1.p2.17.m17.1.1.2.cmml"><mi id="S3.SS1.p2.17.m17.1.1.2.2" xref="S3.SS1.p2.17.m17.1.1.2.2.cmml">A</mi><mo id="S3.SS1.p2.17.m17.1.1.2.3" xref="S3.SS1.p2.17.m17.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS1.p2.17.m17.1.1.1" xref="S3.SS1.p2.17.m17.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.17.m17.1b"><apply id="S3.SS1.p2.17.m17.1.1.cmml" xref="S3.SS1.p2.17.m17.1.1"><ci id="S3.SS1.p2.17.m17.1.1.1.cmml" xref="S3.SS1.p2.17.m17.1.1.1">^</ci><apply id="S3.SS1.p2.17.m17.1.1.2.cmml" xref="S3.SS1.p2.17.m17.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m17.1.1.2.1.cmml" xref="S3.SS1.p2.17.m17.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.17.m17.1.1.2.2.cmml" xref="S3.SS1.p2.17.m17.1.1.2.2">ğ´</ci><ci id="S3.SS1.p2.17.m17.1.1.2.3.cmml" xref="S3.SS1.p2.17.m17.1.1.2.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.17.m17.1c">\hat{A^{\prime}}</annotation></semantics></math>. We can also obtain poses with higher diversity by increasing the variance of the sampling distribution after determining the pose filter. The training loss of the VAE is:</p>
<table id="S1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{total}}=w_{1}\mathcal{L}_{KL}+w_{2}\mathcal{L}_{\text{rec}}," display="inline"><semantics id="S3.Ex1.m2.1a"><mrow id="S3.Ex1.m2.1.1.1" xref="S3.Ex1.m2.1.1.1.1.cmml"><mrow id="S3.Ex1.m2.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.cmml"><msub id="S3.Ex1.m2.1.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.1.1.2.2" xref="S3.Ex1.m2.1.1.1.1.2.2.cmml">â„’</mi><mtext id="S3.Ex1.m2.1.1.1.1.2.3" xref="S3.Ex1.m2.1.1.1.1.2.3a.cmml">total</mtext></msub><mo id="S3.Ex1.m2.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m2.1.1.1.1.3" xref="S3.Ex1.m2.1.1.1.1.3.cmml"><mrow id="S3.Ex1.m2.1.1.1.1.3.2" xref="S3.Ex1.m2.1.1.1.1.3.2.cmml"><msub id="S3.Ex1.m2.1.1.1.1.3.2.2" xref="S3.Ex1.m2.1.1.1.1.3.2.2.cmml"><mi id="S3.Ex1.m2.1.1.1.1.3.2.2.2" xref="S3.Ex1.m2.1.1.1.1.3.2.2.2.cmml">w</mi><mn id="S3.Ex1.m2.1.1.1.1.3.2.2.3" xref="S3.Ex1.m2.1.1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.3.2.1" xref="S3.Ex1.m2.1.1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.Ex1.m2.1.1.1.1.3.2.3" xref="S3.Ex1.m2.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.1.1.3.2.3.2" xref="S3.Ex1.m2.1.1.1.1.3.2.3.2.cmml">â„’</mi><mrow id="S3.Ex1.m2.1.1.1.1.3.2.3.3" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.cmml"><mi id="S3.Ex1.m2.1.1.1.1.3.2.3.3.2" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.3.2.3.3.1" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.1.cmml">â€‹</mo><mi id="S3.Ex1.m2.1.1.1.1.3.2.3.3.3" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.3.cmml">L</mi></mrow></msub></mrow><mo id="S3.Ex1.m2.1.1.1.1.3.1" xref="S3.Ex1.m2.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.Ex1.m2.1.1.1.1.3.3" xref="S3.Ex1.m2.1.1.1.1.3.3.cmml"><msub id="S3.Ex1.m2.1.1.1.1.3.3.2" xref="S3.Ex1.m2.1.1.1.1.3.3.2.cmml"><mi id="S3.Ex1.m2.1.1.1.1.3.3.2.2" xref="S3.Ex1.m2.1.1.1.1.3.3.2.2.cmml">w</mi><mn id="S3.Ex1.m2.1.1.1.1.3.3.2.3" xref="S3.Ex1.m2.1.1.1.1.3.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.3.3.1" xref="S3.Ex1.m2.1.1.1.1.3.3.1.cmml">â€‹</mo><msub id="S3.Ex1.m2.1.1.1.1.3.3.3" xref="S3.Ex1.m2.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.1.1.3.3.3.2" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.cmml">â„’</mi><mtext id="S3.Ex1.m2.1.1.1.1.3.3.3.3" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3a.cmml">rec</mtext></msub></mrow></mrow></mrow><mo id="S3.Ex1.m2.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.1b"><apply id="S3.Ex1.m2.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1"><eq id="S3.Ex1.m2.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1"></eq><apply id="S3.Ex1.m2.1.1.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2">â„’</ci><ci id="S3.Ex1.m2.1.1.1.1.2.3a.cmml" xref="S3.Ex1.m2.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.Ex1.m2.1.1.1.1.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.2.3">total</mtext></ci></apply><apply id="S3.Ex1.m2.1.1.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3"><plus id="S3.Ex1.m2.1.1.1.1.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.1"></plus><apply id="S3.Ex1.m2.1.1.1.1.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2"><times id="S3.Ex1.m2.1.1.1.1.3.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.1"></times><apply id="S3.Ex1.m2.1.1.1.1.3.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.3.2.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.3.2.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.2.2">ğ‘¤</ci><cn type="integer" id="S3.Ex1.m2.1.1.1.1.3.2.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.2.3">1</cn></apply><apply id="S3.Ex1.m2.1.1.1.1.3.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.3.2.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.3.2.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3.2">â„’</ci><apply id="S3.Ex1.m2.1.1.1.1.3.2.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3"><times id="S3.Ex1.m2.1.1.1.1.3.2.3.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.1"></times><ci id="S3.Ex1.m2.1.1.1.1.3.2.3.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.2">ğ¾</ci><ci id="S3.Ex1.m2.1.1.1.1.3.2.3.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.2.3.3.3">ğ¿</ci></apply></apply></apply><apply id="S3.Ex1.m2.1.1.1.1.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3"><times id="S3.Ex1.m2.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.1"></times><apply id="S3.Ex1.m2.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.2.2">ğ‘¤</ci><cn type="integer" id="S3.Ex1.m2.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.2.3">2</cn></apply><apply id="S3.Ex1.m2.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2">â„’</ci><ci id="S3.Ex1.m2.1.1.1.1.3.3.3.3a.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.Ex1.m2.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3">rec</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.1c">\displaystyle\mathcal{L}_{\text{total}}=w_{1}\mathcal{L}_{KL}+w_{2}\mathcal{L}_{\text{rec}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\mathcal{L}_{KL}=KL(q(z|A)||" display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1b"><msub id="S3.E1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.2">â„’</mi><mrow id="S3.E1.m1.1.1.3"><mi id="S3.E1.m1.1.1.3.2">K</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1">â€‹</mo><mi id="S3.E1.m1.1.1.3.3">L</mi></mrow></msub><mo id="S3.E1.m1.1.2">=</mo><mi id="S3.E1.m1.1.3">K</mi><mi id="S3.E1.m1.1.4">L</mi><mrow id="S3.E1.m1.1.5"><mo stretchy="false" id="S3.E1.m1.1.5.1">(</mo><mi id="S3.E1.m1.1.5.2">q</mi><mrow id="S3.E1.m1.1.5.3"><mo stretchy="false" id="S3.E1.m1.1.5.3.1">(</mo><mi id="S3.E1.m1.1.5.3.2">z</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.5.3.3">|</mo><mi id="S3.E1.m1.1.5.3.4">A</mi><mo stretchy="false" id="S3.E1.m1.1.5.3.5">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.5.4">|</mo><mo fence="false" stretchy="false" id="S3.E1.m1.1.5.5">|</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle\mathcal{L}_{KL}=KL(q(z|A)||</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.2" class="ltx_math_unparsed" alttext="\displaystyle\mathcal{N}(0,I)),\,\,\,\text{and}\,\,\,\mathcal{L}_{\text{rec}}=\|A-\hat{A}\|_{2}^{2}," display="inline"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2b"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m2.2.3">ğ’©</mi><mrow id="S3.E1.m2.2.4"><mo stretchy="false" id="S3.E1.m2.2.4.1">(</mo><mn id="S3.E1.m2.1.1">0</mn><mo id="S3.E1.m2.2.4.2">,</mo><mi id="S3.E1.m2.2.2">I</mi><mo stretchy="false" id="S3.E1.m2.2.4.3">)</mo></mrow><mo stretchy="false" id="S3.E1.m2.2.5">)</mo><mo rspace="0.667em" id="S3.E1.m2.2.6">,</mo><mtext id="S3.E1.m2.2.7">and</mtext><mi class="ltx_font_mathcaligraphic" id="S3.E1.m2.2.8">â„’</mi><msub id="S3.E1.m2.2.9"><mi id="S3.E1.m2.2.9a"></mi><mtext id="S3.E1.m2.2.9.1">rec</mtext></msub><mo rspace="0em" id="S3.E1.m2.2.10">=</mo><mo lspace="0em" rspace="0.167em" id="S3.E1.m2.2.11">âˆ¥</mo><mi id="S3.E1.m2.2.12">A</mi><mo id="S3.E1.m2.2.13">âˆ’</mo><mover accent="true" id="S3.E1.m2.2.14"><mi id="S3.E1.m2.2.14.2">A</mi><mo id="S3.E1.m2.2.14.1">^</mo></mover><mo lspace="0em" rspace="0.167em" id="S3.E1.m2.2.15">âˆ¥</mo><msub id="S3.E1.m2.2.16"><mi id="S3.E1.m2.2.16a"></mi><mn id="S3.E1.m2.2.16.1">2</mn></msub><msup id="S3.E1.m2.2.17"><mi id="S3.E1.m2.2.17a"></mi><mn id="S3.E1.m2.2.17.1">2</mn></msup><mo id="S3.E1.m2.2.18">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle\mathcal{N}(0,I)),\,\,\,\text{and}\,\,\,\mathcal{L}_{\text{rec}}=\|A-\hat{A}\|_{2}^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.25" class="ltx_p">where <math id="S3.SS1.p2.18.m1.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S3.SS1.p2.18.m1.1a"><msub id="S3.SS1.p2.18.m1.1.1" xref="S3.SS1.p2.18.m1.1.1.cmml"><mi id="S3.SS1.p2.18.m1.1.1.2" xref="S3.SS1.p2.18.m1.1.1.2.cmml">w</mi><mi id="S3.SS1.p2.18.m1.1.1.3" xref="S3.SS1.p2.18.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.18.m1.1b"><apply id="S3.SS1.p2.18.m1.1.1.cmml" xref="S3.SS1.p2.18.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m1.1.1.1.cmml" xref="S3.SS1.p2.18.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.18.m1.1.1.2.cmml" xref="S3.SS1.p2.18.m1.1.1.2">ğ‘¤</ci><ci id="S3.SS1.p2.18.m1.1.1.3.cmml" xref="S3.SS1.p2.18.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.18.m1.1c">w_{i}</annotation></semantics></math> is the weight of each loss term. The Kullback-Leibler term, <math id="S3.SS1.p2.19.m2.1" class="ltx_Math" alttext="\mathcal{L}_{KL}" display="inline"><semantics id="S3.SS1.p2.19.m2.1a"><msub id="S3.SS1.p2.19.m2.1.1" xref="S3.SS1.p2.19.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.19.m2.1.1.2" xref="S3.SS1.p2.19.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS1.p2.19.m2.1.1.3" xref="S3.SS1.p2.19.m2.1.1.3.cmml"><mi id="S3.SS1.p2.19.m2.1.1.3.2" xref="S3.SS1.p2.19.m2.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.19.m2.1.1.3.1" xref="S3.SS1.p2.19.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p2.19.m2.1.1.3.3" xref="S3.SS1.p2.19.m2.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.19.m2.1b"><apply id="S3.SS1.p2.19.m2.1.1.cmml" xref="S3.SS1.p2.19.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.19.m2.1.1.1.cmml" xref="S3.SS1.p2.19.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.19.m2.1.1.2.cmml" xref="S3.SS1.p2.19.m2.1.1.2">â„’</ci><apply id="S3.SS1.p2.19.m2.1.1.3.cmml" xref="S3.SS1.p2.19.m2.1.1.3"><times id="S3.SS1.p2.19.m2.1.1.3.1.cmml" xref="S3.SS1.p2.19.m2.1.1.3.1"></times><ci id="S3.SS1.p2.19.m2.1.1.3.2.cmml" xref="S3.SS1.p2.19.m2.1.1.3.2">ğ¾</ci><ci id="S3.SS1.p2.19.m2.1.1.3.3.cmml" xref="S3.SS1.p2.19.m2.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.19.m2.1c">\mathcal{L}_{KL}</annotation></semantics></math>, represent the divergence between the encoderâ€™s distribution <math id="S3.SS1.p2.20.m3.1" class="ltx_Math" alttext="q(z|A)" display="inline"><semantics id="S3.SS1.p2.20.m3.1a"><mrow id="S3.SS1.p2.20.m3.1.1" xref="S3.SS1.p2.20.m3.1.1.cmml"><mi id="S3.SS1.p2.20.m3.1.1.3" xref="S3.SS1.p2.20.m3.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.20.m3.1.1.2" xref="S3.SS1.p2.20.m3.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p2.20.m3.1.1.1.1" xref="S3.SS1.p2.20.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.20.m3.1.1.1.1.2" xref="S3.SS1.p2.20.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.20.m3.1.1.1.1.1" xref="S3.SS1.p2.20.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.20.m3.1.1.1.1.1.2" xref="S3.SS1.p2.20.m3.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.SS1.p2.20.m3.1.1.1.1.1.1" xref="S3.SS1.p2.20.m3.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p2.20.m3.1.1.1.1.1.3" xref="S3.SS1.p2.20.m3.1.1.1.1.1.3.cmml">A</mi></mrow><mo stretchy="false" id="S3.SS1.p2.20.m3.1.1.1.1.3" xref="S3.SS1.p2.20.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.20.m3.1b"><apply id="S3.SS1.p2.20.m3.1.1.cmml" xref="S3.SS1.p2.20.m3.1.1"><times id="S3.SS1.p2.20.m3.1.1.2.cmml" xref="S3.SS1.p2.20.m3.1.1.2"></times><ci id="S3.SS1.p2.20.m3.1.1.3.cmml" xref="S3.SS1.p2.20.m3.1.1.3">ğ‘</ci><apply id="S3.SS1.p2.20.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.20.m3.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.20.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.20.m3.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.20.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.20.m3.1.1.1.1.1.2">ğ‘§</ci><ci id="S3.SS1.p2.20.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.20.m3.1.1.1.1.1.3">ğ´</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.20.m3.1c">q(z|A)</annotation></semantics></math> and <math id="S3.SS1.p2.21.m4.2" class="ltx_math_unparsed" alttext="\mathcal{N}(0,I))" display="inline"><semantics id="S3.SS1.p2.21.m4.2a"><mrow id="S3.SS1.p2.21.m4.2b"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.21.m4.2.3">ğ’©</mi><mrow id="S3.SS1.p2.21.m4.2.4"><mo stretchy="false" id="S3.SS1.p2.21.m4.2.4.1">(</mo><mn id="S3.SS1.p2.21.m4.1.1">0</mn><mo id="S3.SS1.p2.21.m4.2.4.2">,</mo><mi id="S3.SS1.p2.21.m4.2.2">I</mi><mo stretchy="false" id="S3.SS1.p2.21.m4.2.4.3">)</mo></mrow><mo stretchy="false" id="S3.SS1.p2.21.m4.2.5">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.21.m4.2c">\mathcal{N}(0,I))</annotation></semantics></math>. <math id="S3.SS1.p2.22.m5.1" class="ltx_Math" alttext="\mathcal{L}_{\text{rec}}" display="inline"><semantics id="S3.SS1.p2.22.m5.1a"><msub id="S3.SS1.p2.22.m5.1.1" xref="S3.SS1.p2.22.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.22.m5.1.1.2" xref="S3.SS1.p2.22.m5.1.1.2.cmml">â„’</mi><mtext id="S3.SS1.p2.22.m5.1.1.3" xref="S3.SS1.p2.22.m5.1.1.3a.cmml">rec</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.22.m5.1b"><apply id="S3.SS1.p2.22.m5.1.1.cmml" xref="S3.SS1.p2.22.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.22.m5.1.1.1.cmml" xref="S3.SS1.p2.22.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.22.m5.1.1.2.cmml" xref="S3.SS1.p2.22.m5.1.1.2">â„’</ci><ci id="S3.SS1.p2.22.m5.1.1.3a.cmml" xref="S3.SS1.p2.22.m5.1.1.3"><mtext mathsize="70%" id="S3.SS1.p2.22.m5.1.1.3.cmml" xref="S3.SS1.p2.22.m5.1.1.3">rec</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.22.m5.1c">\mathcal{L}_{\text{rec}}</annotation></semantics></math> is the reconstruction term. <math id="S3.SS1.p2.23.m6.1" class="ltx_Math" alttext="\mathcal{L}_{KL}" display="inline"><semantics id="S3.SS1.p2.23.m6.1a"><msub id="S3.SS1.p2.23.m6.1.1" xref="S3.SS1.p2.23.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.23.m6.1.1.2" xref="S3.SS1.p2.23.m6.1.1.2.cmml">â„’</mi><mrow id="S3.SS1.p2.23.m6.1.1.3" xref="S3.SS1.p2.23.m6.1.1.3.cmml"><mi id="S3.SS1.p2.23.m6.1.1.3.2" xref="S3.SS1.p2.23.m6.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.23.m6.1.1.3.1" xref="S3.SS1.p2.23.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p2.23.m6.1.1.3.3" xref="S3.SS1.p2.23.m6.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.23.m6.1b"><apply id="S3.SS1.p2.23.m6.1.1.cmml" xref="S3.SS1.p2.23.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.23.m6.1.1.1.cmml" xref="S3.SS1.p2.23.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.23.m6.1.1.2.cmml" xref="S3.SS1.p2.23.m6.1.1.2">â„’</ci><apply id="S3.SS1.p2.23.m6.1.1.3.cmml" xref="S3.SS1.p2.23.m6.1.1.3"><times id="S3.SS1.p2.23.m6.1.1.3.1.cmml" xref="S3.SS1.p2.23.m6.1.1.3.1"></times><ci id="S3.SS1.p2.23.m6.1.1.3.2.cmml" xref="S3.SS1.p2.23.m6.1.1.3.2">ğ¾</ci><ci id="S3.SS1.p2.23.m6.1.1.3.3.cmml" xref="S3.SS1.p2.23.m6.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.23.m6.1c">\mathcal{L}_{KL}</annotation></semantics></math> encourages a normal distribution noise while <math id="S3.SS1.p2.24.m7.1" class="ltx_Math" alttext="\mathcal{L}_{\text{rec}}" display="inline"><semantics id="S3.SS1.p2.24.m7.1a"><msub id="S3.SS1.p2.24.m7.1.1" xref="S3.SS1.p2.24.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.24.m7.1.1.2" xref="S3.SS1.p2.24.m7.1.1.2.cmml">â„’</mi><mtext id="S3.SS1.p2.24.m7.1.1.3" xref="S3.SS1.p2.24.m7.1.1.3a.cmml">rec</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.24.m7.1b"><apply id="S3.SS1.p2.24.m7.1.1.cmml" xref="S3.SS1.p2.24.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.24.m7.1.1.1.cmml" xref="S3.SS1.p2.24.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.24.m7.1.1.2.cmml" xref="S3.SS1.p2.24.m7.1.1.2">â„’</ci><ci id="S3.SS1.p2.24.m7.1.1.3a.cmml" xref="S3.SS1.p2.24.m7.1.1.3"><mtext mathsize="70%" id="S3.SS1.p2.24.m7.1.1.3.cmml" xref="S3.SS1.p2.24.m7.1.1.3">rec</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.24.m7.1c">\mathcal{L}_{\text{rec}}</annotation></semantics></math>, in contrast, encourages to reconstruct the <math id="S3.SS1.p2.25.m8.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p2.25.m8.1a"><mi id="S3.SS1.p2.25.m8.1.1" xref="S3.SS1.p2.25.m8.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.25.m8.1b"><ci id="S3.SS1.p2.25.m8.1.1.cmml" xref="S3.SS1.p2.25.m8.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.25.m8.1c">A</annotation></semantics></math> without any divergence.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2208.13944/assets/figures/VAE4.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An overview architecture of our VAE-based animal pose generative model, composed of two main parts: training (blue-dot box) and test (green-dot box). Before we run â€œtestâ€ to generate new poses, pre-test (brown-dot box) should be performed independently to specify the sampling value ranges for each angles in the â€˜pose filterâ€™.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stylization: Blending into the Background</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.6" class="ltx_p">As shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 PASyn Pipeline: Prior-Aware Synthetic Data Generation â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we use the large number of new synthetic poses obtained through VAE to rig the 3D zebra models and render synthetic animal images by changing parameters such as lighting and camera angles. Then, we assign a real (and context-related) background from forest scenes to each image. To alleviate the domain gap between the background image <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="I_{sty}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">I</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.1.m1.1.1.3.4" xref="S3.SS2.p1.1.m1.1.1.3.4.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ¼</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ğ‘ </ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">ğ‘¡</ci><ci id="S3.SS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.4">ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">I_{sty}</annotation></semantics></math> (style) and synthetic animal image <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="I_{cont}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">I</mi><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1a" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.1.1.3.4" xref="S3.SS2.p1.2.m2.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1b" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.1.1.3.5" xref="S3.SS2.p1.2.m2.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ¼</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><times id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">ğ‘œ</ci><ci id="S3.SS2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p1.2.m2.1.1.3.5.cmml" xref="S3.SS2.p1.2.m2.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">I_{cont}</annotation></semantics></math> (content), while increasing the texture diversity of the synthetic animal we employ a style transfer technique. Unlike common convolution-only style transfer methods, we adopt an innovative image style transfer method, called StyTr<sup id="S3.SS2.p1.6.1" class="ltx_sup">2</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Deng etÂ al.(2022)Deng, Tang, Dong, Ma, Pan, Wang, and
Xu</a>]</cite> based on multi-layer transformers. The <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="I_{sty}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">I</mi><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.3.1" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.3.1a" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.4.m4.1.1.3.4" xref="S3.SS2.p1.4.m4.1.1.3.4.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">ğ¼</ci><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><times id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">ğ‘ </ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">ğ‘¡</ci><ci id="S3.SS2.p1.4.m4.1.1.3.4.cmml" xref="S3.SS2.p1.4.m4.1.1.3.4">ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">I_{sty}</annotation></semantics></math> and <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="I_{cont}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">I</mi><mrow id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.1.3.1" xref="S3.SS2.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.1.3.1a" xref="S3.SS2.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.5.m5.1.1.3.4" xref="S3.SS2.p1.5.m5.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.1.3.1b" xref="S3.SS2.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.5.m5.1.1.3.5" xref="S3.SS2.p1.5.m5.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">ğ¼</ci><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><times id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.1"></times><ci id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3">ğ‘œ</ci><ci id="S3.SS2.p1.5.m5.1.1.3.4.cmml" xref="S3.SS2.p1.5.m5.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p1.5.m5.1.1.3.5.cmml" xref="S3.SS2.p1.5.m5.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">I_{cont}</annotation></semantics></math> images are segmented into patches to generate feature embedding and eventually formed encoded content sequence by adding positional embedding for each patch. This process is calculated through the content-aware positional encoding based on the semantics of the image content, thereby eliminating the negative effect of scaling on the spatial relationship between patches. Two transformers are applied to encode the content sequence and style sequence, respectively. Then, a transformer decoder is utilized to decode the encoded content sequence according to the encoded style sequence in a regressive fashion. Finally a convolution-based decoder is used to decode the output sequence, consisting of a linear combination of encoded content and style sequence as well as positional embedding, to obtain the stylized content image <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="I^{sty}_{cont}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msubsup id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2.2" xref="S3.SS2.p1.6.m6.1.1.2.2.cmml">I</mi><mrow id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m6.1.1.3.1" xref="S3.SS2.p1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m6.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m6.1.1.3.1a" xref="S3.SS2.p1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m6.1.1.3.4" xref="S3.SS2.p1.6.m6.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m6.1.1.3.1b" xref="S3.SS2.p1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m6.1.1.3.5" xref="S3.SS2.p1.6.m6.1.1.3.5.cmml">t</mi></mrow><mrow id="S3.SS2.p1.6.m6.1.1.2.3" xref="S3.SS2.p1.6.m6.1.1.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2.3.2" xref="S3.SS2.p1.6.m6.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m6.1.1.2.3.1" xref="S3.SS2.p1.6.m6.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m6.1.1.2.3.3" xref="S3.SS2.p1.6.m6.1.1.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m6.1.1.2.3.1a" xref="S3.SS2.p1.6.m6.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m6.1.1.2.3.4" xref="S3.SS2.p1.6.m6.1.1.2.3.4.cmml">y</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2.2">ğ¼</ci><apply id="S3.SS2.p1.6.m6.1.1.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3"><times id="S3.SS2.p1.6.m6.1.1.2.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.1"></times><ci id="S3.SS2.p1.6.m6.1.1.2.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.2">ğ‘ </ci><ci id="S3.SS2.p1.6.m6.1.1.2.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.3">ğ‘¡</ci><ci id="S3.SS2.p1.6.m6.1.1.2.3.4.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.4">ğ‘¦</ci></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><times id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.1"></times><ci id="S3.SS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3">ğ‘œ</ci><ci id="S3.SS2.p1.6.m6.1.1.3.4.cmml" xref="S3.SS2.p1.6.m6.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p1.6.m6.1.1.3.5.cmml" xref="S3.SS2.p1.6.m6.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">I^{sty}_{cont}</annotation></semantics></math>. Due to strong representation ability of transformers, this method can better capture accurate content representations and avoid loss of details than classical methods.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">However, StyTr<sup id="S3.SS2.p2.4.1" class="ltx_sup">2</sup> cannot freely adjust the intensity of style transfer, unlike the adaptive instance normalization (AdaIN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Huang and Belongie(2017)</a>]</cite>. Besides StyTr<sup id="S3.SS2.p2.4.2" class="ltx_sup">2</sup>, we also perform a simple pixel summation with normalization of the stylized synthetic animal data with the original data to control the intensity of style transfer with the fusion rate <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\alpha</annotation></semantics></math>. The final stylized synthetic animal image is formulated as <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="I^{final}_{cont}=(1-\alpha)I_{cont}+\alpha I^{sty}_{cont}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><msubsup id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.2.2" xref="S3.SS2.p2.4.m4.1.1.3.2.2.cmml">I</mi><mrow id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.3.2" xref="S3.SS2.p2.4.m4.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.3.1" xref="S3.SS2.p2.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.3.1a" xref="S3.SS2.p2.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3.4" xref="S3.SS2.p2.4.m4.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.3.1b" xref="S3.SS2.p2.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3.5" xref="S3.SS2.p2.4.m4.1.1.3.3.5.cmml">t</mi></mrow><mrow id="S3.SS2.p2.4.m4.1.1.3.2.3" xref="S3.SS2.p2.4.m4.1.1.3.2.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.2.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.2.3.1" xref="S3.SS2.p2.4.m4.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.2.3.3" xref="S3.SS2.p2.4.m4.1.1.3.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.2.3.1a" xref="S3.SS2.p2.4.m4.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.2.3.4" xref="S3.SS2.p2.4.m4.1.1.3.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.2.3.1b" xref="S3.SS2.p2.4.m4.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.2.3.5" xref="S3.SS2.p2.4.m4.1.1.3.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.3.2.3.1c" xref="S3.SS2.p2.4.m4.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.3.2.3.6" xref="S3.SS2.p2.4.m4.1.1.3.2.3.6.cmml">l</mi></mrow></msubsup><mo id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml"><mrow id="S3.SS2.p2.4.m4.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.cmml"><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.3.cmml">Î±</mi></mrow><mo stretchy="false" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.2.cmml">â€‹</mo><msub id="S3.SS2.p2.4.m4.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.1.1.3.2.cmml">I</mi><mrow id="S3.SS2.p2.4.m4.1.1.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.3.3.2" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.1.3.3.1" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.1.3.3.3" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.1.3.3.1a" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.1.3.3.4" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.1.3.3.1b" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.1.3.3.5" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.5.cmml">t</mi></mrow></msub></mrow><mo id="S3.SS2.p2.4.m4.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.2.cmml">+</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.1.3.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.3.1" xref="S3.SS2.p2.4.m4.1.1.1.3.1.cmml">â€‹</mo><msubsup id="S3.SS2.p2.4.m4.1.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.1.3.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.2.2" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.2.cmml">I</mi><mrow id="S3.SS2.p2.4.m4.1.1.1.3.3.3" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.3.2" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.3.3.3.1" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.3.3" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.3.3.3.1a" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.3.4" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.3.3.3.1b" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.3.5" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.5.cmml">t</mi></mrow><mrow id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.2" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.1" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.3" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.1a" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.4" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.4.cmml">y</mi></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><eq id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"></eq><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.2">ğ¼</ci><apply id="S3.SS2.p2.4.m4.1.1.3.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3"><times id="S3.SS2.p2.4.m4.1.1.3.2.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.3.2.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3.2">ğ‘“</ci><ci id="S3.SS2.p2.4.m4.1.1.3.2.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3.3">ğ‘–</ci><ci id="S3.SS2.p2.4.m4.1.1.3.2.3.4.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3.4">ğ‘›</ci><ci id="S3.SS2.p2.4.m4.1.1.3.2.3.5.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3.5">ğ‘</ci><ci id="S3.SS2.p2.4.m4.1.1.3.2.3.6.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2.3.6">ğ‘™</ci></apply></apply><apply id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3"><times id="S3.SS2.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.2">ğ‘</ci><ci id="S3.SS2.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.3">ğ‘œ</ci><ci id="S3.SS2.p2.4.m4.1.1.3.3.4.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.4">ğ‘›</ci><ci id="S3.SS2.p2.4.m4.1.1.3.3.5.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.5">ğ‘¡</ci></apply></apply><apply id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"><plus id="S3.SS2.p2.4.m4.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.2"></plus><apply id="S3.SS2.p2.4.m4.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.2"></times><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1"><minus id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.2">1</cn><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.3">ğ›¼</ci></apply><apply id="S3.SS2.p2.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.2">ğ¼</ci><apply id="S3.SS2.p2.4.m4.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3"><times id="S3.SS2.p2.4.m4.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.2">ğ‘</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.3">ğ‘œ</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.3.3.4.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.4">ğ‘›</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.3.3.5.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.3.3.5">ğ‘¡</ci></apply></apply></apply><apply id="S3.SS2.p2.4.m4.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3"><times id="S3.SS2.p2.4.m4.1.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.2">ğ›¼</ci><apply id="S3.SS2.p2.4.m4.1.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.1.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.3.3.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.2">ğ¼</ci><apply id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3"><times id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.2">ğ‘ </ci><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.3">ğ‘¡</ci><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.4.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.2.3.4">ğ‘¦</ci></apply></apply><apply id="S3.SS2.p2.4.m4.1.1.1.3.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3"><times id="S3.SS2.p2.4.m4.1.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.2">ğ‘</ci><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.3">ğ‘œ</ci><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.3.4.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.4">ğ‘›</ci><ci id="S3.SS2.p2.4.m4.1.1.1.3.3.3.5.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3.3.3.5">ğ‘¡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">I^{final}_{cont}=(1-\alpha)I_{cont}+\alpha I^{sty}_{cont}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Creating Synthetic Animal Pose (SynAP) Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Synthetic Animal Pose (SynAP) dataset contains 3,000 synthetic animal images generated by our prior-aware synthetic animal (PASyn) pipeline. Since zebra was selected as the main species for quantitative evaluation in this work, we collected 5 zebra toys and textured zebra 3D models and synthesized the images from these models with Blender. 3D synthetic model generation pipeline (3D-SMG) introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Vyas etÂ al.(2021)Vyas, Jiang, Liu, and Ostadabbas</a>]</cite> was used to reconstruct two models from zebra toys, while the remaining 3 are adapted from 3D models. In addition to SynAP, which only contains zebra data, SynAP+ extends the SynAP with 3,600 images of 6 other animals, including horses, cows, sheep, dogs, giraffes and deer. We purchased one textured 3D model for each animal from a 3D model website, CGTrader and rendered 600 images for each of the aforementioned animals. The VAE model generates the animal pose in each image and 300 grass, savanna, and forest real scenes are collected from Internet to stylize the synthetic animal.The annotations will be automatically created with each image through calculating the coordinates of each zebra joint in the pose using bone vector transformations.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first describe implementation details of PASyn pipeline and different backbone models which we use to prove its effectiveness in animal pose estimation. In order to achieve a high prediction accuracy on an â€œunseenâ€ animal in the existing labeled dataset with only a small amount of real data available, we select <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">zebra</span> as the main animal to verify the general effect of our PASyn pipeline on different models and quantify the result with the metric, PCK@0.05. we also perform an ablation study on each part of PASyn pipeline and finally show our modelâ€™s generalization capacity on different species including deer, cows, dogs, sheep, horses, and giraffes.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.5" class="ltx_p"><span id="S4.SS1.p1.5.1" class="ltx_text ltx_font_bold">PASyn Pipeline:</span> We keep the setting in Vposer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Pavlakos etÂ al.(2019)Pavlakos, Choutas, Ghorbani, Bolkart, Osman,
Tzionas, and Black</a>]</cite> for training the VAE model. The learning rate of Adam optimizer is 0.001 and we set <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="w_{1}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">w</mi><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ğ‘¤</ci><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">w_{1}</annotation></semantics></math> and <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="w_{2}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">w</mi><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ‘¤</ci><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">w_{2}</annotation></semantics></math> which are the weights of <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{KL}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><msub id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">â„’</mi><mrow id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.3.2" xref="S4.SS1.p1.3.m3.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.3.1" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p1.3.m3.1.1.3.3" xref="S4.SS1.p1.3.m3.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">â„’</ci><apply id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3"><times id="S4.SS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.3.1"></times><ci id="S4.SS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.3.2">ğ¾</ci><ci id="S4.SS1.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\mathcal{L}_{KL}</annotation></semantics></math> and <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{rec}}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><msub id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">â„’</mi><mtext id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3a.cmml">rec</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">â„’</ci><ci id="S4.SS1.p1.4.m4.1.1.3a.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">rec</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\mathcal{L}_{\text{rec}}</annotation></semantics></math> in EquationÂ (<a href="#S3.Ex1" title="3.1 Capturing Animal Pose Prior â€£ 3 PASyn Pipeline: Prior-Aware Synthetic Data Generation â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), as 0.005 and 0.01, respectively. The model is trained on 600 poses of animated realistic 3D models for 200 epochs with 128 poses in a batch. The models include horse, dog, sheep, cow, and deer. Each of these animals has over 20 common actions designed by 3D animators, such as running, walking, and jumping. To increase the diversity of the poses, we choose to generate random samples from a Gaussian distribution <math id="S4.SS1.p1.5.m5.2" class="ltx_Math" alttext="\mathcal{N}(0,2I)" display="inline"><semantics id="S4.SS1.p1.5.m5.2a"><mrow id="S4.SS1.p1.5.m5.2.2" xref="S4.SS1.p1.5.m5.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.5.m5.2.2.3" xref="S4.SS1.p1.5.m5.2.2.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.2.2.2" xref="S4.SS1.p1.5.m5.2.2.2.cmml">â€‹</mo><mrow id="S4.SS1.p1.5.m5.2.2.1.1" xref="S4.SS1.p1.5.m5.2.2.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.5.m5.2.2.1.1.2" xref="S4.SS1.p1.5.m5.2.2.1.2.cmml">(</mo><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">0</mn><mo id="S4.SS1.p1.5.m5.2.2.1.1.3" xref="S4.SS1.p1.5.m5.2.2.1.2.cmml">,</mo><mrow id="S4.SS1.p1.5.m5.2.2.1.1.1" xref="S4.SS1.p1.5.m5.2.2.1.1.1.cmml"><mn id="S4.SS1.p1.5.m5.2.2.1.1.1.2" xref="S4.SS1.p1.5.m5.2.2.1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.2.2.1.1.1.1" xref="S4.SS1.p1.5.m5.2.2.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.5.m5.2.2.1.1.1.3" xref="S4.SS1.p1.5.m5.2.2.1.1.1.3.cmml">I</mi></mrow><mo stretchy="false" id="S4.SS1.p1.5.m5.2.2.1.1.4" xref="S4.SS1.p1.5.m5.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.2b"><apply id="S4.SS1.p1.5.m5.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2"><times id="S4.SS1.p1.5.m5.2.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2.2"></times><ci id="S4.SS1.p1.5.m5.2.2.3.cmml" xref="S4.SS1.p1.5.m5.2.2.3">ğ’©</ci><interval closure="open" id="S4.SS1.p1.5.m5.2.2.1.2.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">0</cn><apply id="S4.SS1.p1.5.m5.2.2.1.1.1.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.1"><times id="S4.SS1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.1.2">2</cn><ci id="S4.SS1.p1.5.m5.2.2.1.1.1.3.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.1.3">ğ¼</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.2c">\mathcal{N}(0,2I)</annotation></semantics></math>. The sampling value range of the pose filter will be described in the supplementary. For our domain transfer part, we use the pre-trained transformer decoder provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Deng etÂ al.(2022)Deng, Tang, Dong, Ma, Pan, Wang, and
Xu</a>]</cite> and set the fusion rate as 0.5.
<br class="ltx_break"><span id="S4.SS1.p1.5.2" class="ltx_text ltx_font_bold">Backbone Pose Models:</span> We employ several state-of-the-art pose estimation structures with varying complexity as our backbone networks, including the ResNet-50 and HRNet-W32 of AP10K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>]</cite> and EfficientNet-B6 of DeepLabCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Mathis etÂ al.(2018)Mathis, Mamidanna, Cury, Abe, Murthy, Mathis, and
Bethge</a>]</cite> to reflect the general effect of our PASyn pipeline. All of the backbones are pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Deng etÂ al.(2009)Deng, Dong, Socher, Li, Li, and
Fei-Fei</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Zebra-300 Dataset</span> is the primary test set we use. It contains 40 images from the AP10K test set, 160 unlabeled images randomly selected from AP10K, and 100 images from the Grevyâ€™s zebra dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Zuffi etÂ al.(2019)Zuffi, Kanazawa, Berger-Wolf, and
Black</a>]</cite>. We labeled the images of the latter two according to the AP10K dataset. The images in Zebra-300 dataset are mostly taken in the wild, so the environment occlusion makes the pose estimation task challenging. 
<br class="ltx_break"><span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">Zoo Zebra Dataset</span> is made up of pictures and videos we took at a zoo. The dataset contains 100 zebra images, including 2 different mountain zebra individuals. We performed manual animal pose labeling on the cropped and resized images following the label setting of the AP10K dataset. Each label has 17 key points: nose, eyes, neck, shoulders, elbows, front paws, hips, knees, back paws and root of tail. The occlusion of barbed wire makes this dataset more challenging. We will release this dataset along with the paper.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The effect of SynAP with limited real data of pose estimation results of three common backbones, HRNet-w32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Sun etÂ al.(2019)Sun, Xiao, Liu, and Wang</a>]</cite>, EfficientNet-B6 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Tan and Le(2019)</a>]</cite>, and ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">He etÂ al.(2016)He, Zhang, Ren, and Sun</a>]</cite> tested on Zebra-300 (300 real images). The training set contains only 99 real zebra images (from AP10K) and augmented with SynAP (3000 synthetic zebra images) or SynAP+ (3000 zebra and 2000 other animal synthetic images). Best results for each backbone are shown in bold.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:109.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-175.6pt,44.3pt) scale(0.552557306024624,0.552557306024624) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" rowspan="2"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Training Set</span></th>
<td id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center" colspan="11"><span id="S4.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">PCK@0.05 Pose Estimation Accuracy on Zebra-300 Set</span></td>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Eye</td>
<td id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Nose</td>
<td id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Neck</td>
<td id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Shoulders</td>
<td id="S4.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Elbows</td>
<td id="S4.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">F-Paws</td>
<td id="S4.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Hips</td>
<td id="S4.T1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">Knees</td>
<td id="S4.T1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">B-Paws</td>
<td id="S4.T1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">RoT</td>
<td id="S4.T1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">Average</td>
</tr>
<tr id="S4.T1.1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T1.1.1.3.3.1.1" class="ltx_text">MMPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Contributors(2020)</a>]</cite></span></th>
<th id="S4.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_t" rowspan="3"><span id="S4.T1.1.1.3.3.2.1" class="ltx_text">HRNet-w32</span></th>
<th id="S4.T1.1.1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">R(99)</th>
<td id="S4.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">97.3</td>
<td id="S4.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">95.8</td>
<td id="S4.T1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.3.6.1" class="ltx_text ltx_font_bold">83.2</span></td>
<td id="S4.T1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">78.8</td>
<td id="S4.T1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">77.1</td>
<td id="S4.T1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">62.6</td>
<td id="S4.T1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">86.0</td>
<td id="S4.T1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">74.9</td>
<td id="S4.T1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">59.8</td>
<td id="S4.T1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">82.4</td>
<td id="S4.T1.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_t">78.7</td>
</tr>
<tr id="S4.T1.1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">R(99)+ S(3K)</th>
<td id="S4.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.2.1" class="ltx_text ltx_font_bold">97.8</span></td>
<td id="S4.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.3.1" class="ltx_text ltx_font_bold">98.3</span></td>
<td id="S4.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">81.1</td>
<td id="S4.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.5.1" class="ltx_text ltx_font_bold">94.0</span></td>
<td id="S4.T1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.6.1" class="ltx_text ltx_font_bold">93.5</span></td>
<td id="S4.T1.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.7.1" class="ltx_text ltx_font_bold">92.0</span></td>
<td id="S4.T1.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">93.7</td>
<td id="S4.T1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">93.5</td>
<td id="S4.T1.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">89.0</td>
<td id="S4.T1.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.11.1" class="ltx_text ltx_font_bold">87.6</span></td>
<td id="S4.T1.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.4.12.1" class="ltx_text ltx_font_bold">92.4</span></td>
</tr>
<tr id="S4.T1.1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">R(99)+ S(5K)</th>
<td id="S4.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">97.5</td>
<td id="S4.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">96.9</td>
<td id="S4.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">81.8</td>
<td id="S4.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">89.6</td>
<td id="S4.T1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">91.3</td>
<td id="S4.T1.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">90.7</td>
<td id="S4.T1.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.5.5.8.1" class="ltx_text ltx_font_bold">94.1</span></td>
<td id="S4.T1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.5.5.9.1" class="ltx_text ltx_font_bold">94.1</span></td>
<td id="S4.T1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.5.5.10.1" class="ltx_text ltx_font_bold">90.4</span></td>
<td id="S4.T1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t">86.0</td>
<td id="S4.T1.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_t">91.6</td>
</tr>
<tr id="S4.T1.1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="3"><span id="S4.T1.1.1.6.6.1.1" class="ltx_text">DeepLabCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Mathis etÂ al.(2018)Mathis, Mamidanna, Cury, Abe, Murthy, Mathis, and
Bethge</a>]</cite></span></th>
<th id="S4.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_tt" rowspan="3"><span id="S4.T1.1.1.6.6.2.1" class="ltx_text">EfficientNet-B6</span></th>
<th id="S4.T1.1.1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">R(99)</th>
<td id="S4.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_tt">93.7</td>
<td id="S4.T1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_tt">96.2</td>
<td id="S4.T1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.6.6.6.1" class="ltx_text ltx_font_bold">82.5</span></td>
<td id="S4.T1.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.6.6.7.1" class="ltx_text ltx_font_bold">91.4</span></td>
<td id="S4.T1.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_tt">80.8</td>
<td id="S4.T1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_tt">67.4</td>
<td id="S4.T1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_tt">88.1</td>
<td id="S4.T1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_tt">84.5</td>
<td id="S4.T1.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_tt">71.8</td>
<td id="S4.T1.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_tt">83.2</td>
<td id="S4.T1.1.1.6.6.14" class="ltx_td ltx_align_center ltx_border_tt">83.6</td>
</tr>
<tr id="S4.T1.1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">R(99)+ S(3K)</th>
<td id="S4.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.7.7.2.1" class="ltx_text ltx_font_bold">95.1</span></td>
<td id="S4.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.7.7.3.1" class="ltx_text ltx_font_bold">97.9</span></td>
<td id="S4.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">81.5</td>
<td id="S4.T1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">90.1</td>
<td id="S4.T1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">83.3</td>
<td id="S4.T1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">75.5</td>
<td id="S4.T1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.7.7.8.1" class="ltx_text ltx_font_bold">93.2</span></td>
<td id="S4.T1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_t">89.3</td>
<td id="S4.T1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_t">83.9</td>
<td id="S4.T1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.7.7.11.1" class="ltx_text ltx_font_bold">86.8</span></td>
<td id="S4.T1.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_t">87.6</td>
</tr>
<tr id="S4.T1.1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">R(99)+ S(5K)</th>
<td id="S4.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">94.1</td>
<td id="S4.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">92.6</td>
<td id="S4.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">80.8</td>
<td id="S4.T1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">90.8</td>
<td id="S4.T1.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.8.6.1" class="ltx_text ltx_font_bold">87.0</span></td>
<td id="S4.T1.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.8.7.1" class="ltx_text ltx_font_bold">85.7</span></td>
<td id="S4.T1.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t">90.5</td>
<td id="S4.T1.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.8.9.1" class="ltx_text ltx_font_bold">93.3</span></td>
<td id="S4.T1.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.8.10.1" class="ltx_text ltx_font_bold">88.3</span></td>
<td id="S4.T1.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.8.11.1" class="ltx_text ltx_font_bold">86.8</span></td>
<td id="S4.T1.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.8.12.1" class="ltx_text ltx_font_bold">89.2</span></td>
</tr>
<tr id="S4.T1.1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_tt" rowspan="3"><span id="S4.T1.1.1.9.9.1.1" class="ltx_text">MMPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Contributors(2020)</a>]</cite></span></th>
<th id="S4.T1.1.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_tt" rowspan="3"><span id="S4.T1.1.1.9.9.2.1" class="ltx_text">ResNet-50</span></th>
<th id="S4.T1.1.1.9.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">R(99)</th>
<td id="S4.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_tt">96.2</td>
<td id="S4.T1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_tt">96.9</td>
<td id="S4.T1.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.9.9.6.1" class="ltx_text ltx_font_bold">80.8</span></td>
<td id="S4.T1.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_tt">59.0</td>
<td id="S4.T1.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_tt">71.3</td>
<td id="S4.T1.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_tt">71.2</td>
<td id="S4.T1.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_tt">88.5</td>
<td id="S4.T1.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_tt">78.2</td>
<td id="S4.T1.1.1.9.9.12" class="ltx_td ltx_align_center ltx_border_tt">59.3</td>
<td id="S4.T1.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_tt">85.2</td>
<td id="S4.T1.1.1.9.9.14" class="ltx_td ltx_align_center ltx_border_tt">76.9</td>
</tr>
<tr id="S4.T1.1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">R(99)+ S(3K)</th>
<td id="S4.T1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">95.6</td>
<td id="S4.T1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">95.8</td>
<td id="S4.T1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">69.9</td>
<td id="S4.T1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.10.10.5.1" class="ltx_text ltx_font_bold">87.3</span></td>
<td id="S4.T1.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.10.10.6.1" class="ltx_text ltx_font_bold">84.6</span></td>
<td id="S4.T1.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_t">84.3</td>
<td id="S4.T1.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_t">90.8</td>
<td id="S4.T1.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_t">91.2</td>
<td id="S4.T1.1.1.10.10.10" class="ltx_td ltx_align_center ltx_border_t">84.4</td>
<td id="S4.T1.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_t">77.2</td>
<td id="S4.T1.1.1.10.10.12" class="ltx_td ltx_align_center ltx_border_t">86.7</td>
</tr>
<tr id="S4.T1.1.1.11.11" class="ltx_tr">
<th id="S4.T1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">R(99)+ S(5K)</th>
<td id="S4.T1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.2.1" class="ltx_text ltx_font_bold">97.0</span></td>
<td id="S4.T1.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.3.1" class="ltx_text ltx_font_bold">97.9</span></td>
<td id="S4.T1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">74.5</td>
<td id="S4.T1.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">85.3</td>
<td id="S4.T1.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">84.2</td>
<td id="S4.T1.1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.7.1" class="ltx_text ltx_font_bold">84.5</span></td>
<td id="S4.T1.1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.8.1" class="ltx_text ltx_font_bold">94.6</span></td>
<td id="S4.T1.1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.9.1" class="ltx_text ltx_font_bold">91.4</span></td>
<td id="S4.T1.1.1.11.11.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.10.1" class="ltx_text ltx_font_bold">88.0</span></td>
<td id="S4.T1.1.1.11.11.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.11.1" class="ltx_text ltx_font_bold">85.6</span></td>
<td id="S4.T1.1.1.11.11.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.11.11.12.1" class="ltx_text ltx_font_bold">88.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pose Estimation Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The number of real (R) and synthetic (S) data shown in training set in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.2 Evaluation Datasets â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> means the total number of images we used for model training. SynAP and SynAP+ are divided into training and validation sets with a ratio of 7 to 1 as the AP10K setting. For real data, considering the data scarcity, we divide it with a ratio of 4 to 1. From TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.2 Evaluation Datasets â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can clearly know that only using 99 images for training is insufficient for any backbone network. The highest prediction accuracy is 83.2%, reached by EfficientNet-B6. After adding SynAP to the training set, the prediction results dramatically increase to around 90%. And after adding SynAP+ to the training set, the accuracy is further improved. Among them, HRNet-w32 achieves the highest accuracy of 92.4% when trained with SynAP.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Different animals have domain similarity and the possibility of mutual transfer learning, which is proven in many works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Cao etÂ al.(2019)Cao, Tang, Fang, Shen, Lu, and Tai</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao</a>]</cite>. Thus, we cannot ignore the existing labeled data, which can improve the prediction of unseen animal. In TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.2 Evaluation Datasets â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Pose Estimation Results â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can see that training the model with 99 zebra images and 8,000 images of the other animals can increase the average PCK from 78.7% to 91.4%. However, TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.2 Evaluation Datasets â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that even with only 99 real we are still able to surpass that result and reach SOTA by adding SynAP and SynAP+. Moreover, TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Pose Estimation Results â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> points that the models trained with 8,000 real and SynAP or SynAP+ is further improved to 93.8% and 94.2%. When there is no target animal (i.e., zebra) in the training set, while the state-of-the-art model suffers from this situation (accuracy drops to 78.3%), the model can recover its performance if given the SynAP dataset, as TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Pose Estimation Results â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows. This confirms that our method can achieve high-precision prediction even for the unseen animals.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The effect of SynAP with large set of real data in pose estimation results of HRNet-w32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Sun etÂ al.(2019)Sun, Xiao, Liu, and Wang</a>]</cite> tested on Zebra-300 (300 real images). The training set contains 8000 images from real animals (from AP10K) and augmented with SynAP (3000 synthetic zebra images) or SynAP+ (3000 zebra and 2000 other animal synthetic images). We tested the results when the 8000 images contain 99 real zebra inside them and when no zebra images is used. Please note that the HRNet-w32 model trained on AP10K is the state-of-the-art. Best results are shown in bold.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:108.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-70.9pt,17.7pt) scale(0.753647481033518,0.753647481033518) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training Set</span></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" rowspan="2"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Real Zebra</span></th>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center" colspan="11"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">PCK@0.05 Pose Estimation Accuracy on Zebra-300 Set</span></td>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<td id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Eye</td>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Nose</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Neck</td>
<td id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Shoulders</td>
<td id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Elbows</td>
<td id="S4.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">F-Paws</td>
<td id="S4.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Hips</td>
<td id="S4.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">Knees</td>
<td id="S4.T2.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">B-Paws</td>
<td id="S4.T2.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">RoT</td>
<td id="S4.T2.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">Average</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<th id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">R(8K) (SOTA)</th>
<th id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.1.1.3.3.2.1" class="ltx_text">âœ“</span></th>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.3.3.3.1" class="ltx_text ltx_font_bold">97.5</span></td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">97.2</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">79.4</td>
<td id="S4.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">87.8</td>
<td id="S4.T2.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">90.3</td>
<td id="S4.T2.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">93.8</td>
<td id="S4.T2.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">95.3</td>
<td id="S4.T2.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">94.1</td>
<td id="S4.T2.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">89.5</td>
<td id="S4.T2.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">86.4</td>
<td id="S4.T2.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">91.4</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<th id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R(8K) + S(3K)</th>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">97.3</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.4.4.3.1" class="ltx_text ltx_font_bold">98.3</span></td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">79.0</td>
<td id="S4.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">93.1</td>
<td id="S4.T2.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">94.9</td>
<td id="S4.T2.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">96.0</td>
<td id="S4.T2.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">95.3</td>
<td id="S4.T2.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.4.4.9.1" class="ltx_text ltx_font_bold">96.7</span></td>
<td id="S4.T2.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">93.3</td>
<td id="S4.T2.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.4.4.11.1" class="ltx_text ltx_font_bold">89.6</span></td>
<td id="S4.T2.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_t">93.8</td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<th id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R(8K) + S(5K)</th>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">97.3</td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">97.6</td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.4.1" class="ltx_text ltx_font_bold">81.1</span></td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.5.1" class="ltx_text ltx_font_bold">93.7</span></td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.6.1" class="ltx_text ltx_font_bold">95.7</span></td>
<td id="S4.T2.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.7.1" class="ltx_text ltx_font_bold">96.0</span></td>
<td id="S4.T2.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.8.1" class="ltx_text ltx_font_bold">96.6</span></td>
<td id="S4.T2.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">96.0</td>
<td id="S4.T2.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.10.1" class="ltx_text ltx_font_bold">94.3</span></td>
<td id="S4.T2.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t">87.6</td>
<td id="S4.T2.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.5.5.12.1" class="ltx_text ltx_font_bold">94.2</span></td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<th id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">R(8K)</th>
<th id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt" rowspan="3"><span id="S4.T2.1.1.6.6.2.1" class="ltx_text">âœ—</span></th>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">79.7</td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">87.7</td>
<td id="S4.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">37.4</td>
<td id="S4.T2.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">77.6</td>
<td id="S4.T2.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">80.0</td>
<td id="S4.T2.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">87.6</td>
<td id="S4.T2.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">82.0</td>
<td id="S4.T2.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">86.4</td>
<td id="S4.T2.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">81.3</td>
<td id="S4.T2.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">67.2</td>
<td id="S4.T2.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">78.3</td>
</tr>
<tr id="S4.T2.1.1.7.7" class="ltx_tr">
<th id="S4.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R(8K) + S(3K)</th>
<td id="S4.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">94.8</td>
<td id="S4.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">96.2</td>
<td id="S4.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.7.7.4.1" class="ltx_text ltx_font_bold">67.1</span></td>
<td id="S4.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">90.8</td>
<td id="S4.T2.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">87.9</td>
<td id="S4.T2.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">90.2</td>
<td id="S4.T2.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t">87.6</td>
<td id="S4.T2.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_t">91.6</td>
<td id="S4.T2.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_t">89.7</td>
<td id="S4.T2.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_t">77.6</td>
<td id="S4.T2.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_t">88.2</td>
</tr>
<tr id="S4.T2.1.1.8.8" class="ltx_tr">
<th id="S4.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">R(8K) + S(5K)</th>
<td id="S4.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.2.1" class="ltx_text ltx_font_bold">97.5</span></td>
<td id="S4.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.3.1" class="ltx_text ltx_font_bold">96.2</span></td>
<td id="S4.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">66.1</td>
<td id="S4.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.5.1" class="ltx_text ltx_font_bold">91.6</span></td>
<td id="S4.T2.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.6.1" class="ltx_text ltx_font_bold">89.5</span></td>
<td id="S4.T2.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.7.1" class="ltx_text ltx_font_bold">93.8</span></td>
<td id="S4.T2.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.8.1" class="ltx_text ltx_font_bold">93.9</span></td>
<td id="S4.T2.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.9.1" class="ltx_text ltx_font_bold">93.5</span></td>
<td id="S4.T2.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.10.1" class="ltx_text ltx_font_bold">91.1</span></td>
<td id="S4.T2.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.11.1" class="ltx_text ltx_font_bold">77.6</span></td>
<td id="S4.T2.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.8.8.12.1" class="ltx_text ltx_font_bold">90.2</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study of two parts of our PASyn pipeline, VAE , style transfer and <math id="S4.T3.3.m1.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S4.T3.3.m1.1b"><msup id="S4.T3.3.m1.1.1" xref="S4.T3.3.m1.1.1.cmml"><mi id="S4.T3.3.m1.1.1.2" xref="S4.T3.3.m1.1.1.2.cmml">Ïƒ</mi><mn id="S4.T3.3.m1.1.1.3" xref="S4.T3.3.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.3.m1.1c"><apply id="S4.T3.3.m1.1.1.cmml" xref="S4.T3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.m1.1.1.1.cmml" xref="S4.T3.3.m1.1.1">superscript</csymbol><ci id="S4.T3.3.m1.1.1.2.cmml" xref="S4.T3.3.m1.1.1.2">ğœ</ci><cn type="integer" id="S4.T3.3.m1.1.1.3.cmml" xref="S4.T3.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.m1.1d">\sigma^{2}</annotation></semantics></math> random sampling distribution on two different zebra dataset with resolution of 300<math id="S4.T3.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.4.m2.1b"><mo id="S4.T3.4.m2.1.1" xref="S4.T3.4.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.m2.1c"><times id="S4.T3.4.m2.1.1.cmml" xref="S4.T3.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.m2.1d">\times</annotation></semantics></math>300. We use HRNet-w32 as our backbone here. The training set contains 99 real zebra (from AP10K) and augmented with SynAP (3000 synthetic zebra images). Best results are shown in bold.</figcaption>
<div id="S4.T3.5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:297.0pt;height:117.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-63.6pt,25.1pt) scale(0.7,0.7) ;">
<table id="S4.T3.5.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.5.1.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T3.5.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.2.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.5.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Index</span></td>
</tr>
</table>
</td>
<td id="S4.T3.5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T3.5.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.3.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.5.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Training Set</span></td>
</tr>
</table>
</td>
<td id="S4.T3.5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T3.5.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.4.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.5.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">VAE</span></td>
</tr>
</table>
</td>
<td id="S4.T3.5.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T3.5.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.5.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.5.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Style Transfer</span></td>
</tr>
</table>
</td>
<td id="S4.T3.5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T3.5.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.1.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S4.T3.5.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S4.T3.5.1.1.1.1.1.1.1.m1.1a"><msup id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.2.cmml">Ïƒ</mi><mn id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.3" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.5.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.2">ğœ</ci><cn type="integer" id="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.5.1.1.1.1.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.1.1.1.1.1.1.1.m1.1c">\sigma^{2}</annotation></semantics></math></td>
</tr>
</table>
</td>
<td id="S4.T3.5.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T3.5.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.6.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.5.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Zoo Zebra Set</span></td>
</tr>
</table>
</td>
<td id="S4.T3.5.1.1.1.7" class="ltx_td ltx_align_center">
<table id="S4.T3.5.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.1.1.1.7.1.1" class="ltx_tr">
<td id="S4.T3.5.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.5.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">Zebra-300 Set</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.5.1.1.2.1" class="ltx_tr">
<td id="S4.T3.5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">a</td>
<td id="S4.T3.5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R(99)</td>
<td id="S4.T3.5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T3.5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T3.5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2I</td>
<td id="S4.T3.5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.0</td>
<td id="S4.T3.5.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">78.7</td>
</tr>
<tr id="S4.T3.5.1.1.3.2" class="ltx_tr">
<td id="S4.T3.5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">b</td>
<td id="S4.T3.5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">S(3K)</td>
<td id="S4.T3.5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">âœ—</td>
<td id="S4.T3.5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">âœ—</td>
<td id="S4.T3.5.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2I</td>
<td id="S4.T3.5.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">38.7</td>
<td id="S4.T3.5.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_tt">30.0</td>
</tr>
<tr id="S4.T3.5.1.1.4.3" class="ltx_tr">
<td id="S4.T3.5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">c</td>
<td id="S4.T3.5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S(3K)</td>
<td id="S4.T3.5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T3.5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2I</td>
<td id="S4.T3.5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.2</td>
<td id="S4.T3.5.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">36.7</td>
</tr>
<tr id="S4.T3.5.1.1.5.4" class="ltx_tr">
<td id="S4.T3.5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">d</td>
<td id="S4.T3.5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S(3K)</td>
<td id="S4.T3.5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2I</td>
<td id="S4.T3.5.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.9</td>
<td id="S4.T3.5.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">46.6</td>
</tr>
<tr id="S4.T3.5.1.1.6.5" class="ltx_tr">
<td id="S4.T3.5.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">e</td>
<td id="S4.T3.5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">R(99)+S(3K)</td>
<td id="S4.T3.5.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">âœ—</td>
<td id="S4.T3.5.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">âœ—</td>
<td id="S4.T3.5.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2I</td>
<td id="S4.T3.5.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">89.8</td>
<td id="S4.T3.5.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_tt">88.0</td>
</tr>
<tr id="S4.T3.5.1.1.7.6" class="ltx_tr">
<td id="S4.T3.5.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">f</td>
<td id="S4.T3.5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R(99)+S(3K)</td>
<td id="S4.T3.5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T3.5.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2I</td>
<td id="S4.T3.5.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.4</td>
<td id="S4.T3.5.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t">89.8</td>
</tr>
<tr id="S4.T3.5.1.1.8.7" class="ltx_tr">
<td id="S4.T3.5.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">g</td>
<td id="S4.T3.5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R(99)+S(3K)</td>
<td id="S4.T3.5.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">I</td>
<td id="S4.T3.5.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.5</td>
<td id="S4.T3.5.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">91.1</td>
</tr>
<tr id="S4.T3.5.1.1.9.8" class="ltx_tr">
<td id="S4.T3.5.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">h</td>
<td id="S4.T3.5.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">R(99)+S(3K)</td>
<td id="S4.T3.5.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">âœ“</td>
<td id="S4.T3.5.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2I</td>
<td id="S4.T3.5.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.5.1.1.9.8.6.1" class="ltx_text ltx_font_bold">91.5</span></td>
<td id="S4.T3.5.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T3.5.1.1.9.8.7.1" class="ltx_text ltx_font_bold">92.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.5" class="ltx_p">As seen from TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.2 Evaluation Datasets â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, even if only trained on a small amount of real data, the model can accurately predict the keypoints with apparent features and less flexibility, such as nose and eyes. Various alternative candidate positions and large degrees of freedom of keypoints on the limbs would considerably affect the prediction results. The main improvement brought by SynAP and SynAP+ is the significant increase of the prediction of keypoints in animal limbs. To verify that the VAE, StyTr<sup id="S4.SS4.p1.5.1" class="ltx_sup">2</sup> models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Deng etÂ al.(2022)Deng, Tang, Dong, Ma, Pan, Wang, and
Xu</a>]</cite>, and the higher variance of VAE sampling distribution in our PASyn pipeline can mitigate the mismatching prediction of the limbs and reduce the domain discrepancy between the real (R) and synthetic (S) domains, we test them on the Zoo Zebra and Zebra-300 datasets. TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Pose Estimation Results â€£ 4 Experimental Analysis â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the outcomes of seven experiments (a-g) with various training conditions. When the model trained without VAE, we will manually set a reasonable value range for the angles between adjacent bones on the limbs, and conduct randomly sampling, similar to the method used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Zuffi etÂ al.(2019)Zuffi, Kanazawa, Berger-Wolf, and
Black</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Mu etÂ al.(2020)Mu, Qiu, Hager, and Yuille</a>]</cite>. We will keep the original texture of the model when StyTr<sup id="S4.SS4.p1.5.2" class="ltx_sup">2</sup> is not used.
The various training and test sets clearly reflect that the model trained with both of VAE and StyTr<sup id="S4.SS4.p1.5.3" class="ltx_sup">2</sup> in our PASyn pipeline can achieve obviously higher accuracy than the model trained without them. Also, the effect of variance of the random sampling distribution, <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><msup id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">Ïƒ</mi><mn id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">superscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">ğœ</ci><cn type="integer" id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">\sigma^{2}</annotation></semantics></math> on the pose estimation performance is shown in experiments (g) and (h). The higher diversity of the poses can improve the pose estimation, hence our choice of <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="\sigma^{2}=2I" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><mrow id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><msup id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml"><mi id="S4.SS4.p1.5.m5.1.1.2.2" xref="S4.SS4.p1.5.m5.1.1.2.2.cmml">Ïƒ</mi><mn id="S4.SS4.p1.5.m5.1.1.2.3" xref="S4.SS4.p1.5.m5.1.1.2.3.cmml">2</mn></msup><mo id="S4.SS4.p1.5.m5.1.1.1" xref="S4.SS4.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml"><mn id="S4.SS4.p1.5.m5.1.1.3.2" xref="S4.SS4.p1.5.m5.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS4.p1.5.m5.1.1.3.1" xref="S4.SS4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS4.p1.5.m5.1.1.3.3" xref="S4.SS4.p1.5.m5.1.1.3.3.cmml">I</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><eq id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1.1"></eq><apply id="S4.SS4.p1.5.m5.1.1.2.cmml" xref="S4.SS4.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p1.5.m5.1.1.2.1.cmml" xref="S4.SS4.p1.5.m5.1.1.2">superscript</csymbol><ci id="S4.SS4.p1.5.m5.1.1.2.2.cmml" xref="S4.SS4.p1.5.m5.1.1.2.2">ğœ</ci><cn type="integer" id="S4.SS4.p1.5.m5.1.1.2.3.cmml" xref="S4.SS4.p1.5.m5.1.1.2.3">2</cn></apply><apply id="S4.SS4.p1.5.m5.1.1.3.cmml" xref="S4.SS4.p1.5.m5.1.1.3"><times id="S4.SS4.p1.5.m5.1.1.3.1.cmml" xref="S4.SS4.p1.5.m5.1.1.3.1"></times><cn type="integer" id="S4.SS4.p1.5.m5.1.1.3.2.cmml" xref="S4.SS4.p1.5.m5.1.1.3.2">2</cn><ci id="S4.SS4.p1.5.m5.1.1.3.3.cmml" xref="S4.SS4.p1.5.m5.1.1.3.3">ğ¼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">\sigma^{2}=2I</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented a cost-effective and general prior-aware synthetic data generation pipeline (PASyn) for animal pose estimation, for target animal studies that suffer from a severe data scarcity. A probabilistic variational generative model as well as a domain adaptation technique are introduced to increase the validity of the generated poses and to reduce the domain discrepancy between the synthetic and real images. Our synthetic animal pose dataset SynAP and its extended version SynAP+, and the positive effect of them on pose estimation task of animals with a small amount of real data is verified on different backbones and achieves state-of-the-art performance.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Atapour-Abarghouei and Breckon(2018)]</span>
<span class="ltx_bibblock">
Amir Atapour-Abarghouei and TobyÂ P. Breckon.

</span>
<span class="ltx_bibblock">Real-time monocular depth estimation using synthetic data with domain
adaptation via image style transfer.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Cao etÂ al.(2019)Cao, Tang, Fang, Shen, Lu, and Tai]</span>
<span class="ltx_bibblock">
Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing
Tai.

</span>
<span class="ltx_bibblock">Cross-domain adaptation for animal pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 9498â€“9507, 2019.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chong etÂ al.(2021)Chong, Peng, Zhang, and Pan]</span>
<span class="ltx_bibblock">
Yanwen Chong, Chengwei Peng, Jingjing Zhang, and Shaoming Pan.

</span>
<span class="ltx_bibblock">Style transfer for unsupervised domain-adaptive person
re-identification.

</span>
<span class="ltx_bibblock"><em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 422:314â€“321, 2021.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Contributors(2020)]</span>
<span class="ltx_bibblock">
MMPose Contributors.

</span>
<span class="ltx_bibblock">Openmmlab pose estimation toolbox and benchmark.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmpose" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/open-mmlab/mmpose</a>, 2020.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Deng etÂ al.(2009)Deng, Dong, Socher, Li, Li, and
Fei-Fei]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiÂ Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</em>, pages 248â€“255. Ieee, 2009.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Deng etÂ al.(2022)Deng, Tang, Dong, Ma, Pan, Wang, and
Xu]</span>
<span class="ltx_bibblock">
Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and
Changsheng Xu.

</span>
<span class="ltx_bibblock">Stytr2: Image style transfer with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 11326â€“11336, 2022.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ganin etÂ al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
Laviolette, Marchand, and Lempitsky]</span>
<span class="ltx_bibblock">
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky.

</span>
<span class="ltx_bibblock">Domain-adversarial training of neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">The journal of machine learning research</em>, 17(1):2096â€“2030, 2016.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Graving etÂ al.(2019)Graving, Chae, Naik, Li, Koger, Costelloe, and
Couzin]</span>
<span class="ltx_bibblock">
JacobÂ M Graving, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, BlairÂ R
Costelloe, and IainÂ D Couzin.

</span>
<span class="ltx_bibblock">Deepposekit, a software toolkit for fast and robust animal pose
estimation using deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Elife</em>, 8:e47994, 2019.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[He etÂ al.(2016)He, Zhang, Ren, and Sun]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, June 2016.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Huang and Belongie(2017)]</span>
<span class="ltx_bibblock">
Xun Huang and Serge Belongie.

</span>
<span class="ltx_bibblock">Arbitrary style transfer in real-time with adaptive instance
normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 1501â€“1510, 2017.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Jinkun etÂ al.(2019)Jinkun, Hongyang, Hao-Shu, Xiaoyong, Cewu, and
Yu-Wing]</span>
<span class="ltx_bibblock">
Cao Jinkun, Tang Hongyang, Fang Hao-Shu, Shen Xiaoyong, LuÂ Cewu, and Tai
Yu-Wing.

</span>
<span class="ltx_bibblock">Animal pose dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sites.google.com/view/animal-pose/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sites.google.com/view/animal-pose/</a>, 2019.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Li and Lee(2021)]</span>
<span class="ltx_bibblock">
Chen Li and GimÂ Hee Lee.

</span>
<span class="ltx_bibblock">From synthetic to real: Unsupervised domain adaptation for animal
pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 1482â€“1491, 2021.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mathis etÂ al.(2018)Mathis, Mamidanna, Cury, Abe, Murthy, Mathis, and
Bethge]</span>
<span class="ltx_bibblock">
Alexander Mathis, Pranav Mamidanna, KevinÂ M Cury, Taiga Abe, VenkateshÂ N
Murthy, MackenzieÂ Weygandt Mathis, and Matthias Bethge.

</span>
<span class="ltx_bibblock">Deeplabcut: markerless pose estimation of user-defined body parts
with deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Nature neuroscience</em>, 21(9):1281â€“1289,
2018.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mathis etÂ al.(2021)Mathis, Biasi, Schneider, Yuksekgonul, Rogers,
Bethge, and Mathis]</span>
<span class="ltx_bibblock">
Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert Yuksekgonul, Byron
Rogers, Matthias Bethge, and MackenzieÂ W. Mathis.

</span>
<span class="ltx_bibblock">Pretraining boosts out-of-domain robustness for pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV)</em>, pages 1859â€“1868, January 2021.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mu etÂ al.(2020)Mu, Qiu, Hager, and Yuille]</span>
<span class="ltx_bibblock">
Jiteng Mu, Weichao Qiu, GregoryÂ D Hager, and AlanÂ L Yuille.

</span>
<span class="ltx_bibblock">Learning from synthetic animals.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 12386â€“12395, 2020.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Neverova etÂ al.(2020)Neverova, Novotny, Khalidov, Szafraniec, Labatut,
and Vedaldi]</span>
<span class="ltx_bibblock">
Natalia Neverova, David Novotny, Vasil Khalidov, Marc Szafraniec, Patrick
Labatut, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Continuous surface embeddings, 2020.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Pavlakos etÂ al.(2019)Pavlakos, Choutas, Ghorbani, Bolkart, Osman,
Tzionas, and Black]</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A.Â A.
Osman, Dimitrios Tzionas, and MichaelÂ J. Black.

</span>
<span class="ltx_bibblock">Expressive body capture: 3d hands, face, and body from a single
image.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Pero etÂ al.(2016)Pero, Susanna, Rahul, and Vittorio]</span>
<span class="ltx_bibblock">
LÂ Del Pero, Ricco Susanna, Sukthankar Rahul, and Ferrari Vittorio.

</span>
<span class="ltx_bibblock">Tigdog dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://calvin-vision.net/datasets/tigdog/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://calvin-vision.net/datasets/tigdog/</a>, 2016.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sanakoyeu etÂ al.(2020)Sanakoyeu, Khalidov, McCarthy, Vedaldi, and
Neverova]</span>
<span class="ltx_bibblock">
Artsiom Sanakoyeu, Vasil Khalidov, MaureenÂ S. McCarthy, Andrea Vedaldi, and
Natalia Neverova.

</span>
<span class="ltx_bibblock">Transferring dense pose to proximal animal classes.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sun etÂ al.(2019)Sun, Xiao, Liu, and Wang]</span>
<span class="ltx_bibblock">
KeÂ Sun, Bin Xiao, Dong Liu, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for human pose
estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 5693â€“5703, 2019.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tan and Le(2019)]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
6105â€“6114. PMLR, 2019.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Vicente and Agapito(2013)]</span>
<span class="ltx_bibblock">
Sara Vicente and Lourdes Agapito.

</span>
<span class="ltx_bibblock">Balloon shapes: Reconstructing and deforming objects with volume from
images.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">2013 International Conference on 3D Vision-3DV 2013</em>, pages
223â€“230. IEEE, 2013.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Vyas etÂ al.(2021)Vyas, Jiang, Liu, and Ostadabbas]</span>
<span class="ltx_bibblock">
Kathan Vyas, LeÂ Jiang, Shuangjun Liu, and Sarah Ostadabbas.

</span>
<span class="ltx_bibblock">An efficient 3d synthetic model generation pipeline for human pose
data augmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 1542â€“1552, 2021.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wagner etÂ al.(2021)Wagner, Khalili, Sharma, Boxberg, Marr, Back, and
Peng]</span>
<span class="ltx_bibblock">
SophiaÂ J Wagner, Nadieh Khalili, Raghav Sharma, Melanie Boxberg, Carsten Marr,
WalterÂ de Back, and Tingying Peng.

</span>
<span class="ltx_bibblock">Structure-preserving multi-domain stain color augmentation using
style-transfer with disentangled representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical Image Computing and
Computer-Assisted Intervention</em>, pages 257â€“266. Springer, 2021.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wu etÂ al.(2020)Wu, Buchanan, Whiteway, Schartner, Meijer, Noel,
Rodriguez, Everett, Norovich, Schaffer, etÂ al.]</span>
<span class="ltx_bibblock">
Anqi Wu, EstefanyÂ Kelly Buchanan, Matthew Whiteway, Michael Schartner, Guido
Meijer, Jean-Paul Noel, Erica Rodriguez, Claire Everett, Amy Norovich, Evan
Schaffer, etÂ al.

</span>
<span class="ltx_bibblock">Deep graph pose: a semi-supervised deep graphical model for improved
animal pose tracking.

</span>
<span class="ltx_bibblock"><em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:6040â€“6052, 2020.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yu etÂ al.(2021)Yu, Xu, Zhang, Zhao, Guan, and Tao]</span>
<span class="ltx_bibblock">
Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Ap-10k: A benchmark for animal pose estimation in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.12617</em>, 2021.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zuffi etÂ al.(2017)Zuffi, Kanazawa, Jacobs, and Black]</span>
<span class="ltx_bibblock">
Silvia Zuffi, Angjoo Kanazawa, DavidÂ W Jacobs, and MichaelÂ J Black.

</span>
<span class="ltx_bibblock">3d menagerie: Modeling the 3d shape and pose of animals.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6365â€“6373, 2017.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zuffi etÂ al.(2019)Zuffi, Kanazawa, Berger-Wolf, and
Black]</span>
<span class="ltx_bibblock">
Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and MichaelÂ J Black.

</span>
<span class="ltx_bibblock">Three-d safari: Learning to estimate zebra pose, shape, and texture
from images" in the wild".

</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 5359â€“5368, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A </span>Supplementary Materials</h2>

<div id="S1a.p1" class="ltx_para">
<p id="S1a.p1.1" class="ltx_p">In this section we offer figures and analyses supplementing our discussion from the main paper.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Pose Filter</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Based on the Fig.Â <a href="#S1.F1a" title="Figure S1 â€£ A.1 Pose Filter â€£ A Supplementary Materials â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S1</span></a>, which is made by decoding of 10,000 random samples from a normal Gaussian distribution, we can set a special sampling range for each joint. The angle value within this range can be regarded as a valid angle, and the pose that satisfies the twelve ranges can be seen as an appropriate pose. These ranges are shoulder_right [40, 100], elbow_right [-125, 0], front-paw_right [-25, 100], shoulder_left [40, 100], elbow_left [-125, 0], front-paw_left [-25, 100], hip_right [-120, -60], knee_right [0, 80], back-paw_right [-125, 0], hip_left [-120, -60], knee_left [0, 80], back-paw_left [-125, 0]. In order to increase the variety of poses and generate more poses with angles near the boundary, we choose to generate random samples from a Gaussian distribution <math id="S1.SS1.p1.1.m1.2" class="ltx_Math" alttext="\mathcal{N}(0,2I)" display="inline"><semantics id="S1.SS1.p1.1.m1.2a"><mrow id="S1.SS1.p1.1.m1.2.2" xref="S1.SS1.p1.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S1.SS1.p1.1.m1.2.2.3" xref="S1.SS1.p1.1.m1.2.2.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S1.SS1.p1.1.m1.2.2.2" xref="S1.SS1.p1.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S1.SS1.p1.1.m1.2.2.1.1" xref="S1.SS1.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S1.SS1.p1.1.m1.2.2.1.1.2" xref="S1.SS1.p1.1.m1.2.2.1.2.cmml">(</mo><mn id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml">0</mn><mo id="S1.SS1.p1.1.m1.2.2.1.1.3" xref="S1.SS1.p1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S1.SS1.p1.1.m1.2.2.1.1.1" xref="S1.SS1.p1.1.m1.2.2.1.1.1.cmml"><mn id="S1.SS1.p1.1.m1.2.2.1.1.1.2" xref="S1.SS1.p1.1.m1.2.2.1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S1.SS1.p1.1.m1.2.2.1.1.1.1" xref="S1.SS1.p1.1.m1.2.2.1.1.1.1.cmml">â€‹</mo><mi id="S1.SS1.p1.1.m1.2.2.1.1.1.3" xref="S1.SS1.p1.1.m1.2.2.1.1.1.3.cmml">I</mi></mrow><mo stretchy="false" id="S1.SS1.p1.1.m1.2.2.1.1.4" xref="S1.SS1.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.2b"><apply id="S1.SS1.p1.1.m1.2.2.cmml" xref="S1.SS1.p1.1.m1.2.2"><times id="S1.SS1.p1.1.m1.2.2.2.cmml" xref="S1.SS1.p1.1.m1.2.2.2"></times><ci id="S1.SS1.p1.1.m1.2.2.3.cmml" xref="S1.SS1.p1.1.m1.2.2.3">ğ’©</ci><interval closure="open" id="S1.SS1.p1.1.m1.2.2.1.2.cmml" xref="S1.SS1.p1.1.m1.2.2.1.1"><cn type="integer" id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">0</cn><apply id="S1.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S1.SS1.p1.1.m1.2.2.1.1.1"><times id="S1.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S1.SS1.p1.1.m1.2.2.1.1.1.1"></times><cn type="integer" id="S1.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S1.SS1.p1.1.m1.2.2.1.1.1.2">2</cn><ci id="S1.SS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S1.SS1.p1.1.m1.2.2.1.1.1.3">ğ¼</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.2c">\mathcal{N}(0,2I)</annotation></semantics></math> after setting the pose filter. The dropout rate of the pose is 68.0%.</p>
</div>
<figure id="S1.F1a" class="ltx_figure"><img src="/html/2208.13944/assets/figures/posefilter.jpg" id="S1.F1a.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="348" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure S1: </span>The histograms of angle values of 12 joints, which are shoulder_right, elbow_right, front-paw_right, shoulder_left, elbow_left , front-paw_left, hip_right, knee_right, back-paw_right, hip_left, knee_left , back-paw_left. </figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>The Effect of Out-of-Domain Species on In-Domain Pose Estimation</h3>

<figure id="S1.F2" class="ltx_figure"><img src="/html/2208.13944/assets/figures/result.jpg" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure S2: </span>The line graphs showing the results of the three backbones, HRNet-w32 (in blue) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Sun etÂ al.(2019)Sun, Xiao, Liu, and Wang</a>]</cite>, EfficientNet-B6 (in green) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Tan and Le(2019)</a>]</cite>, and ResNet-50 (in orange) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">He etÂ al.(2016)He, Zhang, Ren, and Sun</a>]</cite>, tested on Zebra-300 (300 real images) under different training sets. The training set contains only 99 real zebra images (from AP10K) and augmented with SynAP (3000 synthetic zebra images) or SynAP+ (3000 zebra and other animal synthetic images). We tried 4 different sizes of other animal synthetic images in SynAP+. There are 500, 1K, 2K, 3.6K.</figcaption>
</figure>
<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">We explored the effect of the size of other synthetic animal data on target animalâ€™s pose predictions during training. The result is shown in Fig.Â <a href="#S1.F2" title="Figure S2 â€£ A.2 The Effect of Out-of-Domain Species on In-Domain Pose Estimation â€£ A Supplementary Materials â€£ Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S2</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.13943" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.13944" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.13944">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.13944" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.13945" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 20:26:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
