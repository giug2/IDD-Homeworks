<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.17287] Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study</title><meta property="og:description" content="Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have give…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.17287">

<!--Generated on Fri Apr  5 14:13:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gustav A. Baumgart<sup id="9.9.1" class="ltx_sup"><span id="9.9.1.1" class="ltx_text ltx_font_italic">†</span></sup>   Jaemin Shin<sup id="10.10.2" class="ltx_sup"><span id="10.10.2.1" class="ltx_text ltx_font_italic">‡</span></sup>   Ali Payani<sup id="11.11.3" class="ltx_sup">∗</sup>   Myungjin Lee<sup id="12.12.4" class="ltx_sup">∗</sup>   Ramana Rao Kompella<sup id="13.13.5" class="ltx_sup">∗</sup>
<br class="ltx_break"><sup id="14.14.6" class="ltx_sup"><span id="14.14.6.1" class="ltx_text ltx_font_italic">†</span></sup>University of Minnesota  <sup id="15.15.7" class="ltx_sup"><span id="15.15.7.1" class="ltx_text ltx_font_italic">‡</span></sup>KAIST  <sup id="16.16.8" class="ltx_sup">∗</sup>Cisco Research
<br class="ltx_break"><span id="17.17.9" class="ltx_text ltx_font_typewriter">baumg260@umn.edu
<br class="ltx_break">jaemin.shin@kaist.ac.kr
<br class="ltx_break">{apayani, myungjle, rkompell}@cisco.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="18.1" class="ltx_p">Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD). (2) Recent algorithms present smaller standard deviation in accuracy across clients than FedAvg, indicating that the advanced algorithms’ performances are stable. (3) However, algorithms such as FedDyn and SCAFFOLD are more prone to catastrophic failures without the support of additional techniques such as gradient clipping. We hope that our empirical study can help the community to build best practices in evaluating FL algorithms.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine learning (ML) is fueling modern applications, and high-performance ML models are becoming key to business success. However, ML training often requires a large volume of high quality datasets and centralized learning can violate data privacy. Federated learning (FL) emerged as a practical approach that can train machine learning models from decentralized data while preserving privacy.
Its potential drew tremendous attention from the research community, which resulted in the invention of numerous FL algorithms and mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Many prior works focus on performance metrics such as accuracy, theoretical convergence rate, fairness, and communication efficiency.
For instance, some FL algorithm studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> focus on accuracy while others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> evaluate group fairness. To discuss communication efficiency and speedup of algorithms, most studies measure the number of rounds to reach a certain accuracy as their key metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, such a performance evaluation of previous approaches provide little insight into the performance of FL algorithms under realistic scenarios, without considering actual wall-clock time or computational
overheads. For example, previous studies reporting the number of rounds to convergence neglect computation and communication overheads per round. While there were few studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> that measured the wall-clock time and resource usage of FL algorithms (time-to-accuracy and resource-to-accuracy), they were conducted at simulation-based environments without actual model weights communication in FL. Thus, previous analysis significantly limits understanding of the FL approaches in a real-world setting and makes it difficult for the FL practitioners to choose an appropriate approach under different constraints.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we argue for the need for a holistic evaluation of FL approaches, which is imperative to assist FL practitioners better. While there may be many aspects that the practitioners want to understand for a given approach, we primarily consider four aspects: (i) computation costs, (ii) communication overheads, (iii) performance stability across clients, and (iv) training instability.
First, computation costs can influence time-to-accuracy depending on the algorithm’s complexity.
Second, in addition to model weights, some algorithms ask for the exchange of extra information between clients and a server. This incurs different degrees of communication overheads and could bottleneck FL, thereby affecting time-to-accuracy. Third, the global model produced by an FL job should maintain a similar performance across different clients. We call this property performance stability across clients.
There is a lack of research on the performance stability.
The instability can potentially harm an application’s credibility as some users may experience poorer performance compared to others. Finally, instability can occur during training due to various reasons in real FL deployments, which we call training instability. As a means to explore the training instability of the algorithms, we choose gradient clipping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
The technique and its variants
have been adopted in FL settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The main idea behind the technique is to prevent overflow for the model weights by controlling large client updates (from a batch in local training or from a complete round).
However, it is unknown how the absence of gradient clipping would affect these algorithms.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To conduct a comprehensive evaluation study on FL approaches, we consider various factors: algorithm, accelerator<span id="S1.p5.1.1" class="ltx_text" style="color:#0000FF;"> ,</span> and model architecture. First, we focus on several canonical algorithms (FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, server-side optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> (e.g., FedAdam and FedYogi), SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and FedDyn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>). Second, we rely on different hardware settings (e.g., CPU and NVIDIA GPUs such as A100, V100 and T4)
to check how the algorithms behave under different resource constraints. Third, we employ different model architectures such as CNN, ResNet, and LSTM. We use CNN and ResNet for training with the CIFAR-10 dataset and LSTM for Shakespeare dataset. We leverage
Flame<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/cisco-open/flame</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>,
an open-source federated learning framework, to run experiments by combining those factors. Its well-defined programming interface allows us to implement various algorithms consistently. In turn, the evaluation of the algorithms under a <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">single</span> framework renders our results easily reproducible.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our comprehensive performance evaluation study makes the following key contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Performance metrics based on round (such as accuracy-to-round, the accuracy achieved after a certain number of rounds)
should be interpreted carefully.
Such metrics suggest that the smaller number of rounds is indicative of high performance. This ignores the amount of computations (hence, the actual walk-clock time) needed for training.
Our experiments show that FedDyn achieves higher accuracy for 100 rounds of training than other algorithms but takes approximately 1.58<math id="S1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.I1.i1.p1.1.m1.1a"><mo id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><times id="S1.I1.i1.p1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">\times</annotation></semantics></math> as long compared to FedAvg (see Section <a href="#S4.SS1" title="4.1 General Performance ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">FL algorithms exhibit different degrees of computation overheads depending on hardware and model architecture (Section <a href="#S4.SS2" title="4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). In general, the state-of-the-art algorithms such as SCAFFOLD, FedProx, and FedDyn tend to have significant runtime increases over FedAvg (Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) when CNN and ResNet are employed. In particular, when resources are constrained, these algorithms incur much higher runtime (e.g., FedDyn has a 252.90% runtime increase over FedAvg as shown in Figure <a href="#S4.F2.sf4" title="In Figure 2 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2d</span></a>).
In the case of LSTM, however, Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the less the resources are, the less runtime gap is between algorithms. With an A100 GPU, FedDyn’s runtime is about 40% higher than that of FedAvg (Figure <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a>). In contrast, under a CPU, the gap is less than 5% across the algorithms; even FedDyn is 10% faster than that of FedAvg (Figure <a href="#S4.F3.sf4" title="In Figure 3 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3d</span></a>).
This indicates FL algorithms runtime is influenced by model architectures.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Algorithms exhibit different levels of performance stability
across clients (Section <a href="#S4.SS4" title="4.4 Performance Stability Across Clients ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Our experiment results allow us to make several observations. First, FedDyn achieves the best performance stability among all the algorithms. In other words, it delivers more consistent local test accuracy results across clients.
Second, SCAFFOLD is more vulnerable to class imbalances seen in non-IID data. It tends to show higher standard deviation in accuracy than FedAvg. Third, server-side optimization algorithms such as FedAdam and FedYogi can be alternatives to client-side optimization algorithms as they are lightweight and often obtain better performance stability than SCAFFOLD and FedProx.
The violin plots for local test accuracies in Figure <a href="#S4.F4.sf1" title="In Figure 4 ‣ 4.3 Communication Overheads ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4a</span></a> and Figure <a href="#S4.F4.sf2" title="In Figure 4 ‣ 4.3 Communication Overheads ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4b</span></a> support these observations.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Client-side optimization algorithms are more vulnerable to catastrophic failures.
As a means for testing, we conduct experiments by disabling gradient clipping.
The experiment results in Table <a href="#S4.T2" title="Table 2 ‣ 4.4 Performance Stability Across Clients ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show that as the dataset distribution becomes heterogeneous, SCAFFOLD and FedDyn experience more frequent failures while other algorithms experience no failure.
More specifically, we observe that only FedDyn and SCAFFOLD face failure rates at or above 60%. Notably, a smaller learning rate helps these failure rates drop to 0%. Therefore, it is essential to use gradient clipping for algorithms such as FedDyn and SCAFFOLD.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">To conduct our performance evaluation study, we implement many of the algorithms used in this paper within Flame <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We make these algorithms available in the Flame’s open-source repository, which can help reproduce the evaluation results in this work reliably.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Federated Learning Approaches.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Upon the introduction of Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, numerous approaches have been subsequently proposed to optimize FL at various aspects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Among these proposals, we select six widely adopted approaches for extensive performance evaluation as follows: (1) <span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, the most commonly used FL approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, has been proposed to optimize FL communication efficiency by training multiple local epochs on clients instead of one. (2) Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed <span id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">FedProx</span>, which adds a client-side regularization for robust training on clients with heterogeneous non-IID data distributions. (3) <span id="S2.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">FedAdam</span> and (4) <span id="S2.SS0.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">FedYogi</span> were proposed by Reddi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. These algorithms provide adaptivity at server-side optimization to improve FL convergence. (5) <span id="S2.SS0.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_italic">SCAFFOLD</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> was also proposed to achieve improved convergence rates by reducing the variance among clients with heterogeneous data, using control variates to correct local model updates. (6) <span id="S2.SS0.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_italic">FedDyn</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> focuses on aligning local and global objectives in FL through a client-side regularizer, mitigating the impact of data heterogeneity and improving the convergence rate. In this paper, we report the extensive experimental comparison of these approaches on model performance and system-level metrics.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Practical Evaluation of Federated Learning Approaches.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Several prior studies have approached to evaluate the performance of the FL approaches as follows: Reddi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> compared the model performance and hyperparameter sensitivity across the aforementioned approaches, except for FedDyn, on four datasets. Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> measured the model performance and number of rounds for convergence of the approaches on six non-iid datasets. Charles et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> investigated the effects of the number of clients sampled per round in FL, examining aspects such as model failures, convergence, and fairness. However, these results predominantly focus on model performance through simulation-based experiments without actual model weights communication or computation time measurement. This approach overlooks crucial practical considerations, such as the evaluation of system-level metrics like training and communication overhead. There exist studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> that approached to simulate train and communication latency of clients by leveraging data from 136k user mobile devices. However, these studies rest on the assumption that client time distributions remain constant, which may not hold in dynamic real-world environments where train and computation latency can fluctuate significantly. Moreover, their experiments only partially incorporated the FL approaches that we consider in this work (FedAvg, FedProx).</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Recognizing these gaps, our work aims to present a comprehensive measurement study of the six widely adopted FL approaches. We focus not just on traditional metrics on model performance but also on system-level factors, employing a testbed that facilitates actual model training and weight communication in FL. This approach is designed to provide a more realistic and holistic assessment of FL approaches, addressing practical aspects that previous studies have overlooked.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first discuss the algorithms we use in our evaluation. We choose six different algorithms: FedAvg, FedYogi, FedAdam, FedProx, SCAFFOLD and FedDyn.
We discuss the loss functions of these algorithms in Section <a href="#S3.SS1" title="3.1 FL Algorithms ‣ 3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We exclude algorithms that are particularly tailored for certain metrics (e.g., fairness); we leave the evaluation on those algorithms as future work.
Next, in Section <a href="#S3.SS2" title="3.2 Flame ‣ 3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we briefly describe a federated learning framework called Flame we rely on to facilitate realistic experiments. Flame isolates individual components (client and server) into separate processes and lets them run in parallel, which can demonstrate concurrent execution of clients’ training that takes place in a real-world setting.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>FL Algorithms</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p"><span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_bold">FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</span> FedAvg is the first aggregation algorithm that employs
local stochastic gradient descent (SGD) in a federated learning setting.
This approach is susceptible to non-IID datasets, suffering from an inferior convergence rate
in a non-IID setting to its convergence rate in an IID setting.
In FedAvg’s loss function, there is no extra correction (regularization) term. We let <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="L(\theta)" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.2" xref="S3.SS1.p1.1.m1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.2.2" xref="S3.SS1.p1.1.m1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.2.1" xref="S3.SS1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.p1.1.m1.1.2.3.2" xref="S3.SS1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.1.2.3.2.1" xref="S3.SS1.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.1.2.3.2.2" xref="S3.SS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.2"><times id="S3.SS1.p1.1.m1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.2.1"></times><ci id="S3.SS1.p1.1.m1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.2.2">𝐿</ci><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">L(\theta)</annotation></semantics></math> denote FedAvg’s loss function, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\theta</annotation></semantics></math> represents the model weights.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">FedYogi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span> and <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_bold">FedAdam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</span> They attempt to address the issue of heterogeneous data.
They additionally introduce server-side aggregation formulae that exploit second-degree approximation during the aggregation step at the server.
At the client side, the training process is the same as FedAvg, meaning they both have the same loss function <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="L(\theta)" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.2" xref="S3.SS1.p2.1.m1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.2.2" xref="S3.SS1.p2.1.m1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.2.1" xref="S3.SS1.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.p2.1.m1.1.2.3.2" xref="S3.SS1.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.1.2.3.2.1" xref="S3.SS1.p2.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.1.2.3.2.2" xref="S3.SS1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.2"><times id="S3.SS1.p2.1.m1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.2.1"></times><ci id="S3.SS1.p2.1.m1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.2.2">𝐿</ci><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">L(\theta)</annotation></semantics></math>.
They neither require any additional communication nor does a server-side state need to be maintained throughout the training process at the client side (besides the global model).</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</span> This algorithm adds a regularization term to the loss function at the client side, which would essentially keep the updates closer to global model at that round. The right-hand side term in Equation <a href="#S3.E1" title="In 3.1 FL Algorithms ‣ 3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> discourages large client-side updates by minimizing over the squared norm of the distance between the local model weights and the current global model weights.
Here, <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\theta^{t}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msup id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">θ</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝜃</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\theta^{t}</annotation></semantics></math> represents the current global model’s weights.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="L^{\prime}(\theta)=L(\theta)+\frac{\mu}{2}||\theta-\theta^{t}||^{2}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><msup id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml"><mi id="S3.E1.m1.3.3.3.2.2" xref="S3.E1.m1.3.3.3.2.2.cmml">L</mi><mo id="S3.E1.m1.3.3.3.2.3" xref="S3.E1.m1.3.3.3.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.3.2" xref="S3.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.1" xref="S3.E1.m1.3.3.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mrow id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml"><mi id="S3.E1.m1.3.3.1.3.2" xref="S3.E1.m1.3.3.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.3.1" xref="S3.E1.m1.3.3.1.3.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.3.3.2" xref="S3.E1.m1.3.3.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.3.3.2.1" xref="S3.E1.m1.3.3.1.3.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.3.3.1.3.3.2.2" xref="S3.E1.m1.3.3.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">+</mo><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mfrac id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml">μ</mi><mn id="S3.E1.m1.3.3.1.1.3.3" xref="S3.E1.m1.3.3.1.1.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">​</mo><msup id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml">θ</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.E1.m1.3.3.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml">θ</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml">t</mi></msup></mrow><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"></eq><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1"></times><apply id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.2">superscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2">𝐿</ci><ci id="S3.E1.m1.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.2.3">′</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝜃</ci></apply><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><plus id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></plus><apply id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3"><times id="S3.E1.m1.3.3.1.3.1.cmml" xref="S3.E1.m1.3.3.1.3.1"></times><ci id="S3.E1.m1.3.3.1.3.2.cmml" xref="S3.E1.m1.3.3.1.3.2">𝐿</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝜃</ci></apply><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><times id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><divide id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3"></divide><ci id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2">𝜇</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.3.3">2</cn></apply><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1"></minus><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2">𝜃</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.2">𝜃</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><cn type="integer" id="S3.E1.m1.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">L^{\prime}(\theta)=L(\theta)+\frac{\mu}{2}||\theta-\theta^{t}||^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.4" class="ltx_p"><span id="S3.SS1.p4.4.1" class="ltx_text ltx_font_bold">SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span> This algorithm converges faster than FedAvg under heterogeneous settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Although the modifications to client-side training are not as significant as FedProx, this method requires two times as much communication volume per round because communication includes the control variates, which have the same size as the model weights.
The server keeps track of a general gradient direction for all clients <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">c</annotation></semantics></math>, and the client keeps track of its own direction <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝑐</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">c_{i}</annotation></semantics></math>.
That way, the term <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="c-c_{i}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">c</mi><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">−</mo><msub id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">c</mi><mi id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><minus id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></minus><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝑐</ci><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">𝑐</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">c-c_{i}</annotation></semantics></math> is used to correct local training step in order to account for other client’s distributions.
Instead of changing it by adding it to the gradient step, we factor this in by modifying the loss function in Equation <a href="#S3.E2" title="In 3.1 FL Algorithms ‣ 3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which delivers an equivalent result during training.
The operator <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mo id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><csymbol cd="latexml" id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\odot</annotation></semantics></math> represents a dot product.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="L^{\prime}(\theta)=L(\theta)+(c-c_{i})\odot\theta" display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><msup id="S3.E2.m1.3.3.3.2" xref="S3.E2.m1.3.3.3.2.cmml"><mi id="S3.E2.m1.3.3.3.2.2" xref="S3.E2.m1.3.3.3.2.2.cmml">L</mi><mo id="S3.E2.m1.3.3.3.2.3" xref="S3.E2.m1.3.3.3.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.3.3.2" xref="S3.E2.m1.3.3.3.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.3.3.2.1" xref="S3.E2.m1.3.3.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.E2.m1.3.3.3.3.2.2" xref="S3.E2.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mrow id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.3.3.1.3.cmml"><mi id="S3.E2.m1.3.3.1.3.2" xref="S3.E2.m1.3.3.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.3.1" xref="S3.E2.m1.3.3.1.3.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.3.3.2" xref="S3.E2.m1.3.3.1.3.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.3.3.2.1" xref="S3.E2.m1.3.3.1.3.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">θ</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.3.3.2.2" xref="S3.E2.m1.3.3.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.2.cmml">+</mo><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.cmml">c</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.cmml">c</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">⊙</mo><mi id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml">θ</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><times id="S3.E2.m1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3.1"></times><apply id="S3.E2.m1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.2.1.cmml" xref="S3.E2.m1.3.3.3.2">superscript</csymbol><ci id="S3.E2.m1.3.3.3.2.2.cmml" xref="S3.E2.m1.3.3.3.2.2">𝐿</ci><ci id="S3.E2.m1.3.3.3.2.3.cmml" xref="S3.E2.m1.3.3.3.2.3">′</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝜃</ci></apply><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><plus id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.2"></plus><apply id="S3.E2.m1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.3"><times id="S3.E2.m1.3.3.1.3.1.cmml" xref="S3.E2.m1.3.3.1.3.1"></times><ci id="S3.E2.m1.3.3.1.3.2.cmml" xref="S3.E2.m1.3.3.1.3.2">𝐿</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝜃</ci></apply><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2">direct-product</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"></minus><ci id="S3.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2">𝑐</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2">𝑐</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">L^{\prime}(\theta)=L(\theta)+(c-c_{i})\odot\theta</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.4" class="ltx_p"><span id="S3.SS1.p5.4.1" class="ltx_text ltx_font_bold">FedDyn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</span> This algorithm is closely related to SCAFFOLD. However, they are different in that FedDyn does not incur the two-fold increase in SCAFFOLD’s communication volume. Further, while SCAFFOLD requires additional hyperparameter tuning, the exact minimization technique in FedDyn demands less hyperarameter tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> .
However, the FedDyn client-side regularization term is more computationally expensive than SCAFFOLD’s.
While SCAFFOLD modifies the gradient at each step by a fixed amount (which is equivalent to adding a term linear in <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">w</annotation></semantics></math> to the loss function), FedDyn adds both linear and quadratic terms.
As shown in Equation <a href="#S3.E3" title="In 3.1 FL Algorithms ‣ 3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, FedDyn contains two terms; one is the term in FedProx and the other is a linear term similar to what is in SCAFFOLD.
<math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="\nabla L(\theta^{t-1}_{k})" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mrow id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mrow id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml"><mo rspace="0.167em" id="S3.SS1.p5.2.m2.1.1.3.1" xref="S3.SS1.p5.2.m2.1.1.3.1.cmml">∇</mo><mi id="S3.SS1.p5.2.m2.1.1.3.2" xref="S3.SS1.p5.2.m2.1.1.3.2.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.p5.2.m2.1.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p5.2.m2.1.1.1.1.2" xref="S3.SS1.p5.2.m2.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS1.p5.2.m2.1.1.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p5.2.m2.1.1.1.1.1.2.2" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.2.cmml">θ</mi><mi id="S3.SS1.p5.2.m2.1.1.1.1.1.3" xref="S3.SS1.p5.2.m2.1.1.1.1.1.3.cmml">k</mi><mrow id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.cmml"><mi id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.2" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.2.cmml">t</mi><mo id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.1" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.3" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo stretchy="false" id="S3.SS1.p5.2.m2.1.1.1.1.3" xref="S3.SS1.p5.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><times id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2"></times><apply id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3"><ci id="S3.SS1.p5.2.m2.1.1.3.1.cmml" xref="S3.SS1.p5.2.m2.1.1.3.1">∇</ci><ci id="S3.SS1.p5.2.m2.1.1.3.2.cmml" xref="S3.SS1.p5.2.m2.1.1.3.2">𝐿</ci></apply><apply id="S3.SS1.p5.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p5.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p5.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.2">𝜃</ci><apply id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3"><minus id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.1"></minus><ci id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.2.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p5.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\nabla L(\theta^{t-1}_{k})</annotation></semantics></math> is the local gradient, which is updated last for each client <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">k</annotation></semantics></math> at the end of training from the last round (<math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="t-1" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><mrow id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">t</mi><mo id="S3.SS1.p5.4.m4.1.1.1" xref="S3.SS1.p5.4.m4.1.1.1.cmml">−</mo><mn id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><minus id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1.1"></minus><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">t-1</annotation></semantics></math>).</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="L^{\prime}(\theta)=L(\theta)+\nabla L(\theta^{t-1}_{k})\odot\theta+\frac{\alpha}{2}||\theta-\theta^{t}||^{2}" display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.4.4.4" xref="S3.E3.m1.4.4.4.cmml"><msup id="S3.E3.m1.4.4.4.2" xref="S3.E3.m1.4.4.4.2.cmml"><mi id="S3.E3.m1.4.4.4.2.2" xref="S3.E3.m1.4.4.4.2.2.cmml">L</mi><mo id="S3.E3.m1.4.4.4.2.3" xref="S3.E3.m1.4.4.4.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.4.1" xref="S3.E3.m1.4.4.4.1.cmml">​</mo><mrow id="S3.E3.m1.4.4.4.3.2" xref="S3.E3.m1.4.4.4.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.4.3.2.1" xref="S3.E3.m1.4.4.4.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.E3.m1.4.4.4.3.2.2" xref="S3.E3.m1.4.4.4.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml">=</mo><mrow id="S3.E3.m1.4.4.2" xref="S3.E3.m1.4.4.2.cmml"><mrow id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.4.cmml"><mi id="S3.E3.m1.4.4.2.4.2" xref="S3.E3.m1.4.4.2.4.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.4.1" xref="S3.E3.m1.4.4.2.4.1.cmml">​</mo><mrow id="S3.E3.m1.4.4.2.4.3.2" xref="S3.E3.m1.4.4.2.4.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.4.3.2.1" xref="S3.E3.m1.4.4.2.4.cmml">(</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">θ</mi><mo stretchy="false" id="S3.E3.m1.4.4.2.4.3.2.2" xref="S3.E3.m1.4.4.2.4.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.2.3" xref="S3.E3.m1.4.4.2.3.cmml">+</mo><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml"><mo rspace="0.167em" id="S3.E3.m1.3.3.1.1.1.3.1" xref="S3.E3.m1.3.3.1.1.1.3.1.cmml">∇</mo><mi id="S3.E3.m1.3.3.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.3.2.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E3.m1.3.3.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.cmml">θ</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml">k</mi><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.2.cmml">t</mi><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo rspace="0.055em" stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml">⊙</mo><mi id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml">θ</mi></mrow><mo id="S3.E3.m1.4.4.2.3a" xref="S3.E3.m1.4.4.2.3.cmml">+</mo><mrow id="S3.E3.m1.4.4.2.2" xref="S3.E3.m1.4.4.2.2.cmml"><mfrac id="S3.E3.m1.4.4.2.2.3" xref="S3.E3.m1.4.4.2.2.3.cmml"><mi id="S3.E3.m1.4.4.2.2.3.2" xref="S3.E3.m1.4.4.2.2.3.2.cmml">α</mi><mn id="S3.E3.m1.4.4.2.2.3.3" xref="S3.E3.m1.4.4.2.2.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.2" xref="S3.E3.m1.4.4.2.2.2.cmml">​</mo><msup id="S3.E3.m1.4.4.2.2.1" xref="S3.E3.m1.4.4.2.2.1.cmml"><mrow id="S3.E3.m1.4.4.2.2.1.1.1" xref="S3.E3.m1.4.4.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.2.1.1.1.2" xref="S3.E3.m1.4.4.2.2.1.1.2.1.cmml">‖</mo><mrow id="S3.E3.m1.4.4.2.2.1.1.1.1" xref="S3.E3.m1.4.4.2.2.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.2.2.1.1.1.1.2" xref="S3.E3.m1.4.4.2.2.1.1.1.1.2.cmml">θ</mi><mo id="S3.E3.m1.4.4.2.2.1.1.1.1.1" xref="S3.E3.m1.4.4.2.2.1.1.1.1.1.cmml">−</mo><msup id="S3.E3.m1.4.4.2.2.1.1.1.1.3" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3.cmml"><mi id="S3.E3.m1.4.4.2.2.1.1.1.1.3.2" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3.2.cmml">θ</mi><mi id="S3.E3.m1.4.4.2.2.1.1.1.1.3.3" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3.3.cmml">t</mi></msup></mrow><mo stretchy="false" id="S3.E3.m1.4.4.2.2.1.1.1.3" xref="S3.E3.m1.4.4.2.2.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E3.m1.4.4.2.2.1.3" xref="S3.E3.m1.4.4.2.2.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><eq id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"></eq><apply id="S3.E3.m1.4.4.4.cmml" xref="S3.E3.m1.4.4.4"><times id="S3.E3.m1.4.4.4.1.cmml" xref="S3.E3.m1.4.4.4.1"></times><apply id="S3.E3.m1.4.4.4.2.cmml" xref="S3.E3.m1.4.4.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.4.2.1.cmml" xref="S3.E3.m1.4.4.4.2">superscript</csymbol><ci id="S3.E3.m1.4.4.4.2.2.cmml" xref="S3.E3.m1.4.4.4.2.2">𝐿</ci><ci id="S3.E3.m1.4.4.4.2.3.cmml" xref="S3.E3.m1.4.4.4.2.3">′</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝜃</ci></apply><apply id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2"><plus id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.3"></plus><apply id="S3.E3.m1.4.4.2.4.cmml" xref="S3.E3.m1.4.4.2.4"><times id="S3.E3.m1.4.4.2.4.1.cmml" xref="S3.E3.m1.4.4.2.4.1"></times><ci id="S3.E3.m1.4.4.2.4.2.cmml" xref="S3.E3.m1.4.4.2.4.2">𝐿</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝜃</ci></apply><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2">direct-product</csymbol><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2"></times><apply id="S3.E3.m1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3"><ci id="S3.E3.m1.3.3.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.3.1">∇</ci><ci id="S3.E3.m1.3.3.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.3.2">𝐿</ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2">𝜃</ci><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3"><minus id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3">𝑘</ci></apply></apply><ci id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3">𝜃</ci></apply><apply id="S3.E3.m1.4.4.2.2.cmml" xref="S3.E3.m1.4.4.2.2"><times id="S3.E3.m1.4.4.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2"></times><apply id="S3.E3.m1.4.4.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.3"><divide id="S3.E3.m1.4.4.2.2.3.1.cmml" xref="S3.E3.m1.4.4.2.2.3"></divide><ci id="S3.E3.m1.4.4.2.2.3.2.cmml" xref="S3.E3.m1.4.4.2.2.3.2">𝛼</ci><cn type="integer" id="S3.E3.m1.4.4.2.2.3.3.cmml" xref="S3.E3.m1.4.4.2.2.3.3">2</cn></apply><apply id="S3.E3.m1.4.4.2.2.1.cmml" xref="S3.E3.m1.4.4.2.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.1.2.cmml" xref="S3.E3.m1.4.4.2.2.1">superscript</csymbol><apply id="S3.E3.m1.4.4.2.2.1.1.2.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.4.4.2.2.1.1.2.1.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.4.4.2.2.1.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1"><minus id="S3.E3.m1.4.4.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1.1"></minus><ci id="S3.E3.m1.4.4.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1.2">𝜃</ci><apply id="S3.E3.m1.4.4.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.1.1.1.1.3.1.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3">superscript</csymbol><ci id="S3.E3.m1.4.4.2.2.1.1.1.1.3.2.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3.2">𝜃</ci><ci id="S3.E3.m1.4.4.2.2.1.1.1.1.3.3.cmml" xref="S3.E3.m1.4.4.2.2.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><cn type="integer" id="S3.E3.m1.4.4.2.2.1.3.cmml" xref="S3.E3.m1.4.4.2.2.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">L^{\prime}(\theta)=L(\theta)+\nabla L(\theta^{t-1}_{k})\odot\theta+\frac{\alpha}{2}||\theta-\theta^{t}||^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Flame</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We conduct performance evaluation study on FL algorithms and mechanisms in realistic settings to better understand various aspects of the algorithms such as computational overheads, communication costs, performance stability, time to convergence, etc. To achieve this goal, we need a federated learning framework that facilitates easy developments of algorithms and mechanisms and allows realistic experiments in a real-world testbed.
We choose Flame <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as it satisfies our needs. First, its modular design makes it highly extensible and easy to use.
It also lets each component in FL (such as server and client) run as separate process with real communication protocols, thereby enabling realistic experiments. In the remainder of this section, we briefly describe Flame.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Flame’s extensibility comes from its abstraction on federated learning components. Flame’s abstraction consists of two elements: role and channel. In the abstraction, an FL component is defined as role and communication between components is expressed as channel. For instance, a conventional FL setting has server and client as roles and the connection between the two represents a channel, which means that communication between the two is allowed. In order to enable new mechanisms such as hierarchical FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, defining a new role such as intermediate server and implementing its logic is sufficient while keeping other roles and their implementation intact.
Different FL algorithms may need different protocols to exchange extra information in addition to model weights. Deviations from the basic FL algorithm, FedAvg, can be easily accomplished by overriding existing roles and defining new message types.
We do implement several of the federated learning algorithms (FedYogi, FedAdam, SCAFFOLD, FedProx, and FedDyn) into Flame in a cohesive fashion.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conduct a performance evaluation in terms of various metrics such as accuracy, algorithm and communication overheads, performance stability across clients, and training instability. We make use of Flame in realistic settings to accurately reflect on the influences of real systems in running federated learning algorithms.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x1.png" id="S4.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Test accuracy vs. round</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x2.png" id="S4.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="207" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Test accuracy vs. time</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Test accuracy for CIFAR-10 dataset with <math id="S4.F1.2.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.3)" display="inline"><semantics id="S4.F1.2.m1.1b"><mrow id="S4.F1.2.m1.1.2" xref="S4.F1.2.m1.1.2.cmml"><mi id="S4.F1.2.m1.1.2.2" xref="S4.F1.2.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.F1.2.m1.1.2.1" xref="S4.F1.2.m1.1.2.1.cmml">​</mo><mrow id="S4.F1.2.m1.1.2.3.2" xref="S4.F1.2.m1.1.2.cmml"><mo stretchy="false" id="S4.F1.2.m1.1.2.3.2.1" xref="S4.F1.2.m1.1.2.cmml">(</mo><mn id="S4.F1.2.m1.1.1" xref="S4.F1.2.m1.1.1.cmml">0.3</mn><mo stretchy="false" id="S4.F1.2.m1.1.2.3.2.2" xref="S4.F1.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F1.2.m1.1c"><apply id="S4.F1.2.m1.1.2.cmml" xref="S4.F1.2.m1.1.2"><times id="S4.F1.2.m1.1.2.1.cmml" xref="S4.F1.2.m1.1.2.1"></times><ci id="S4.F1.2.m1.1.2.2.cmml" xref="S4.F1.2.m1.1.2.2">Dir</ci><cn type="float" id="S4.F1.2.m1.1.1.cmml" xref="S4.F1.2.m1.1.1">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.2.m1.1d">\mathrm{Dir}(0.3)</annotation></semantics></math> and 100 clients. We compare the accuracy across algorithms by choosing round or time on the x-axis.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>General Performance</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">To get a better idea of how the algorithms perform in terms of accuracy, we run them over 100 rounds while keeping track of the test accuracy of the global model at every round.
In particular, in order to understand how the algorithms work when resources are constrained, we run the experiment by using 100 clients on a machine with 32 CPUs and 256 GB of RAM. We make data distribution heterogeneous and non-IID by setting the parameter (<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\alpha</annotation></semantics></math>) of Dirichlet distribution (denoted as <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{Dir}(\alpha)" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.2" xref="S4.SS1.p1.2.m2.1.2.cmml"><mi id="S4.SS1.p1.2.m2.1.2.2" xref="S4.SS1.p1.2.m2.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.2.1" xref="S4.SS1.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S4.SS1.p1.2.m2.1.2.3.2" xref="S4.SS1.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.2.m2.1.2.3.2.1" xref="S4.SS1.p1.2.m2.1.2.cmml">(</mo><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">α</mi><mo stretchy="false" id="S4.SS1.p1.2.m2.1.2.3.2.2" xref="S4.SS1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.2.cmml" xref="S4.SS1.p1.2.m2.1.2"><times id="S4.SS1.p1.2.m2.1.2.1.cmml" xref="S4.SS1.p1.2.m2.1.2.1"></times><ci id="S4.SS1.p1.2.m2.1.2.2.cmml" xref="S4.SS1.p1.2.m2.1.2.2">Dir</ci><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathrm{Dir}(\alpha)</annotation></semantics></math>) to 0.3 (i.e., <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\alpha=0.3" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">α</mi><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><eq id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></eq><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">𝛼</ci><cn type="float" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\alpha=0.3</annotation></semantics></math>).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Takeaway 1:</span> <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">Different algorithms can exhibit significant difference in runtime to finish the same number of rounds.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">As shown in Figure <a href="#S4.F1.sf2" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1b</span></a>, SCAFFOLD, FedProx, and FedDyn take 34.0%, 37.4% and 57.9% longer to complete the training than FedAvg, respectively.
These results clearly indicate that the algorithms have different computational complexity and potentially pose important implications on time-to-accuracy under different hardware settings. We explore this aspect in Section <a href="#S4.SS2" title="4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Takeaway 2:</span> <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_italic">Accuracy-to-round can be a misleading metric in evaluating the performance of an FL algorithm.</span></p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">In the federated learning community, accuracy-to-round (accuracy obtained after a certain number of rounds) is often adopted as a key performance metric
because higher accuracy-to-round can generally mean smaller number of communication rounds and hence lower communication overheads.
However, this metric ignores computation overhead in each round. This metric therefore fails to capture the fundamental trade-off between communication and computation overheads.
By comparing Figure <a href="#S4.F1.sf1" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1a</span></a> and Figure <a href="#S4.F1.sf2" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1b</span></a>, we observe the limitation of the metric. While FedDyn clearly stands out in terms of accuracy-to-round (Figure <a href="#S4.F1.sf1" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1a</span></a>), the time-to-accuracy of FedYogi and FedAdam is comparable to FedDyn’s (Figure <a href="#S4.F1.sf2" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1b</span></a>) until their training completes (around the 14.38 hour mark). FedYogi and FedAdam are server-side optimization algorithms, and they do not require modification in the loss function, so they take less time to complete a round at client than FedDyn.
This essentially implies that they can learn for more
rounds and further improve the accuracy before FedDyn finishes training.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Execution Runtime</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As runtimes can be different across algorithms (discussed in Section <a href="#S4.SS1" title="4.1 General Performance ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), it becomes important to understand how the runtime is affected under different hardware.
We look into this further by focusing on the client-side training runtime, since it drives the total runtime of a round.
Therefore, we run experiments by using one client and one server under four different kinds of compute resources (CPU and NVIDIA’s T4, V100, A100).
As mentioned in Section <a href="#S3" title="3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, some algorithms (e.g., FedProx, SCAFFOLD and FedDyn) introduce more terms in their loss function, compared to the basic FedAvg algorithm.
For GPU experiments, only the client process is allowed to use one GPU.
For the CPU experiment, the client process is configured to only use one CPU.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x3.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>A100</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S4.F2.sf2.g1" class="ltx_graphics ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>V100</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x5.png" id="S4.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>T4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x6.png" id="S4.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>CPU</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The relative runtime overhead of algorithms compared to FedAvg’s runtime with CNN (798K parameters), ResNet18 (11.7M), and ResNet34 (21.8M). </figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x7.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>A100</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x8.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>V100</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x9.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>T4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.17287/assets/x10.png" id="S4.F3.sf4.g1" class="ltx_graphics ltx_img_square" width="111" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>CPU</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The relative runtime overhead of algorithms compared to FedAvg’s runtime with LSTM-2 (134K parameters), LSTM-10 (780K), and LSTM-20 (1.59M).
</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Takeaway 3:</span> <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">FedDyn is the most computation-heavy algorithm among the algorithms under consideration.</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Figures <a href="#S4.F2" title="Figure 2 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.F3" title="Figure 3 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show the computation overhead of the algorithms running on the CIFAR-10 and Shakespeare datasets,
respectively. In the figures, the overhead is represented in a relative form (i.e., a relative overhead compared to FedAvg). The absolute runtime increases as the hardware becomes less powerful. As shown in the figures, FedDyn is the most expensive algorithm across most of the hardware types and model architectures. For instance, when run on a single CPU using ResNet34, the increases in runtime are 206.26%, 215.41%, and 252.90% for SCAFFOLD, FedProx, and FedDyn, respectively. This demonstrates that the high overhead of FedDyn stems from the more complex regularizer than other algorithms for the loss function in the algorithm (as described in Equation <a href="#S3.E3" title="In 3.1 FL Algorithms ‣ 3 Preliminaries ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Takeaway 4:</span> <span id="S4.SS2.p4.1.2" class="ltx_text ltx_font_italic">Algorithm overheads are influenced by both hardware and neural network architecture.</span></p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">We investigate how the hardware and neural architecture influence the runtime of the algorithms with three convolutional neural networks (CNN, ResNet18 and ResNet34) and three recurrent neural networks (LSTM-2, LSTM-10, and LSTM-20).
As shown in Figure <a href="#S4.F2.sf1" title="In Figure 2 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>, when the algorithms are tested on more powerful resources (e.g., A100), the computation overhead is lower: the maximum runtime difference with respect to FedAvg is about 50%. As the computing power decreases (from Figure <a href="#S4.F2.sf2" title="In Figure 2 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a> to Figure <a href="#S4.F2.sf4" title="In Figure 2 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2d</span></a>), the runtime gap becomes larger: the maximum runtime difference is about 252.9% in case of FedDyn with ResNet34. This suggests that the basic FedAvg is more suitable than other algorithms for less powerful devices.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">However, Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the opposite trend. As the computing power decreases, the runtime difference among the algorithms also diminshes. We observe the largest runtime overhead under the A100 setting (Figure <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4.2 Execution Runtime ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a>) while the maximum runtime overhead is merely 4% under the CPU setting. Even with LSTM-20, FedDyn’s runtime is 10.91% faster than FedAvg; we also run the same experiment across three different machines and obtain around 4.1% execution runtime speedup compared to FedAvg, thereby validating this observation.
Under less powerful devices, the small difference in overhead may be attributed to the fact that the parameters in an LSTM are reused frequently in the neural network, which therefore makes the complexities of the FL algorithms negligible for the runtime. On the other hand, since parameters are limited to being used in one layer for other neural networks such as CNN and ResNet, the limited computing power aggravates the runtime of the expensive algorithms such as FedDyn. Therefore, if a recurrent neural network is employed in FL training, the algorithms with the highest complexity (e.g., FedDyn) can still be a viable option for less powerful devices.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Communication Overheads</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Communication overhead is an important factor when choosing an algorithm.
We capture the communication overhead across the algorithms by running an experiment with one client and one server.
We use a CNN architecture (798K parameters) for the CIFAR-10 dataset and the LSTM-2 architecture (134K parameters) for the Shakespeare dataset.
We only report the results of FedAvg and SCAFFOLD as other algorithms have the same communication overhead as FedAvg in terms of the volume of data exchanged between the server and client.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Takeaway 5:</span> <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">The communication overhead of SCAFFOLD is two times as large per round as that of all other algorithms considered.</span></p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Communication Overheads ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the exact amounts of data exchanged by the algorithms we consider.
The 100% increase in communication between FedAvg and SCAFFOLD is due to the control variates used during client-side training in SCAFFOLD.
These control variates need to be shared over the communication network. Since the variates are added to the gradient at every client-side training step, they have the same size as the model weights. Therefore SCAFFOLD’s communication overhead becomes the highest among the algorithms.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.1.1" class="ltx_text">Architecture</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number of</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Algorithm</th>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Parameters</th>
<th id="S4.T1.1.2.2.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">FedAvg (baseline)</th>
<th id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">SCAFFOLD</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.3.1" class="ltx_tr">
<td id="S4.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S4.T1.1.3.1.1.1" class="ltx_text">LSTM-2</span></td>
<td id="S4.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T1.1.3.1.2.1" class="ltx_text">134K</span></td>
<td id="S4.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">Sent</td>
<td id="S4.T1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">25.89 MB</td>
<td id="S4.T1.1.3.1.5" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t">(0.00%)</td>
<td id="S4.T1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">51.78 MB</td>
<td id="S4.T1.1.3.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t">(99.97%)</td>
</tr>
<tr id="S4.T1.1.4.2" class="ltx_tr">
<td id="S4.T1.1.4.2.1" class="ltx_td ltx_align_center">Received</td>
<td id="S4.T1.1.4.2.2" class="ltx_td ltx_align_right">25.89 MB</td>
<td id="S4.T1.1.4.2.3" class="ltx_td ltx_nopad_l ltx_align_right">(0.00%)</td>
<td id="S4.T1.1.4.2.4" class="ltx_td ltx_align_right">51.78 MB</td>
<td id="S4.T1.1.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right">(99.98%)</td>
</tr>
<tr id="S4.T1.1.5.3" class="ltx_tr">
<td id="S4.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T1.1.5.3.1.1" class="ltx_text">CNN</span></td>
<td id="S4.T1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T1.1.5.3.2.1" class="ltx_text">798K</span></td>
<td id="S4.T1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">Sent</td>
<td id="S4.T1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_t">153.87 MB</td>
<td id="S4.T1.1.5.3.5" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t">(0.00%)</td>
<td id="S4.T1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_t">307.74 MB</td>
<td id="S4.T1.1.5.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t">(100.00%)</td>
</tr>
<tr id="S4.T1.1.6.4" class="ltx_tr">
<td id="S4.T1.1.6.4.1" class="ltx_td ltx_align_center ltx_border_bb">Received</td>
<td id="S4.T1.1.6.4.2" class="ltx_td ltx_align_right ltx_border_bb">153.87 MB</td>
<td id="S4.T1.1.6.4.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb">(0.00%)</td>
<td id="S4.T1.1.6.4.4" class="ltx_td ltx_align_right ltx_border_bb">307.74 MB</td>
<td id="S4.T1.1.6.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb">(100.00%)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average megabytes communicated between the server and one client over 100 rounds for different algorithms datasets. We use the Shakespeare dataset for the LSTM-2 architecture, and the CIFAR-10 dataset for the CNN architecture.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.17287/assets/x11.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="207" height="116" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.17287/assets/x12.png" id="S4.F4.sf1.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="207" height="116" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CIFAR-10; left: non-IID, right: IID</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.17287/assets/x13.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="207" height="116" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.17287/assets/x14.png" id="S4.F4.sf2.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="207" height="116" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Shakespeare; left: non-IID, right: IID</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Violin plots of local test accuracies for CIFAR-10 and Shakespeare for 5 different trials (a total of 500 values for each violin plot). These plots represent the distribution of local test accuracies across different runs.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Performance Stability Across Clients</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">A typical process to evaluate a global model is to apply it with an isolated test dataset. This offers a simple view on the performance of the model. However, when the model is deployed in reality, it may face a diverse set of data, thus making its performance fluctuate across deployment settings (e.g., devices and users).
The less performance variation means that the model provides a consistent performance across the deployment settings, which is a desirable characteristic of an ML model. To quantify this characteristic, we define performance stability as the standard deviation of the accuracy values across clients. A lower standard deviation would mean the global model is fairer across local test sets. Given the variety of FL algorithms, we investigate the performance stability of the algorithms of interest.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We generate IID and non-IID data distributions for 100 clients with the CIFAR-10 and Shakespeare datasets. For the CIFAR-10 dataset, we use <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.3)" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.2" xref="S4.SS4.p2.1.m1.1.2.cmml"><mi id="S4.SS4.p2.1.m1.1.2.2" xref="S4.SS4.p2.1.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.1.m1.1.2.1" xref="S4.SS4.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S4.SS4.p2.1.m1.1.2.3.2" xref="S4.SS4.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S4.SS4.p2.1.m1.1.2.3.2.1" xref="S4.SS4.p2.1.m1.1.2.cmml">(</mo><mn id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">0.3</mn><mo stretchy="false" id="S4.SS4.p2.1.m1.1.2.3.2.2" xref="S4.SS4.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.2"><times id="S4.SS4.p2.1.m1.1.2.1.cmml" xref="S4.SS4.p2.1.m1.1.2.1"></times><ci id="S4.SS4.p2.1.m1.1.2.2.cmml" xref="S4.SS4.p2.1.m1.1.2.2">Dir</ci><cn type="float" id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\mathrm{Dir}(0.3)</annotation></semantics></math> for the non-IID distribution, and generate balanced class distributions for IID. We use the Shakespeare dataset’s natural non-IID distribution (a device is a character), and generate the IID version by pooling samples from all characters, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. We keep the global test set separately and only use it to confirm that training led to similar results as with the complete dataset.
Then, for each client we perform a random 80-20 local train-test split.
We run the training process by only using the train part of the train-test split to 100 rounds, and then test the final global model on each of the local test sets.
We measure the performance stability by averaging the standard deviations across 5 trials.
In Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Communication Overheads ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we include violin plots of all 500 measured local test accuracies.
The figure not only provides an idea of the distribution of local test accuracies, but also depicts the performance stability across the algorithms. The lower accuracy standard deviation means better performance stability.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Takeaway 6:</span> <span id="S4.SS4.p3.1.2" class="ltx_text ltx_font_italic">FedDyn achieves the best performance stability among all the algorithms.</span></p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">FedDyn obtains 5-14.7% lower standard deviation of accuracy than FedAvg’s. Other algorithms excluding FedAvg perform 1-9% worse than FedDyn. While FedDyn, FedProx, and SCAFFOLD all have client-side regularization terms, FedDyn is the only algorithm that performs better than FedAvg across all cases.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold">Takeaway 7:</span> <span id="S4.SS4.p5.1.2" class="ltx_text ltx_font_italic">SCAFFOLD is more prone to class imbalances in terms of performance stability.</span></p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p">Our experiment results in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Communication Overheads ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show that SCAFFOLD’s standard deviation is 2.5-4.3% higher than FedAvg’s across all non-IID distributions from the CIFAR-10 and Shakespeare datasets. SCAFFOLD only beats FedAvg in case of the IID distribution, which suggests that it does not handle class imbalances effectively across clients.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.1" class="ltx_p"><span id="S4.SS4.p7.1.1" class="ltx_text ltx_font_bold">Takeaway 8:</span> <span id="S4.SS4.p7.1.2" class="ltx_text ltx_font_italic">Server-side optimization algorithms can be a good alternative to client-side optimization algorithms.</span></p>
</div>
<div id="S4.SS4.p8" class="ltx_para">
<p id="S4.SS4.p8.1" class="ltx_p">Server-side optimization algorithms such as FedAdam and FedYogi obtain better performance stability than FedAvg across all the distributions and datasets. Because SCAFFOLD and FedProx often fail to achieve better stability than FedAvg, this implies that the server-side algorithms are more stable and cost-efficient than SCAFFOLD and FedProx.</p>
</div>
<div id="S4.SS4.p9" class="ltx_para">
<p id="S4.SS4.p9.1" class="ltx_p"><span id="S4.SS4.p9.1.1" class="ltx_text ltx_font_bold">Takeaway 9:</span> <span id="S4.SS4.p9.1.2" class="ltx_text ltx_font_italic">Performance stability across clients improves as the distributions become more IID.</span></p>
</div>
<div id="S4.SS4.p10" class="ltx_para">
<p id="S4.SS4.p10.1" class="ltx_p">Performance stability turns out to be more consistent across clients as the test set distributions are more similar to each other in IID settings. For instance, for CIFAR-10, SCAFFOLD’s accuracy standard deviation of the IID distribution decreases by almost 41.8%, compared to that of the non-IID distribution. Other algorithms also see the similar reduction in the accuracy standard deviation (40.3% for FedDyn and 38.6% for FedAvg).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.7.7" class="ltx_tr">
<th id="S4.T2.7.7.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Algorithms</th>
<th id="S4.T2.7.7.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">IID</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(10)" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1.2" xref="S4.T2.1.1.1.m1.1.2.cmml"><mi id="S4.T2.1.1.1.m1.1.2.2" xref="S4.T2.1.1.1.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.2.1" xref="S4.T2.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.1.1.1.m1.1.2.3.2" xref="S4.T2.1.1.1.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.1.1.1.m1.1.2.3.2.1" xref="S4.T2.1.1.1.m1.1.2.cmml">(</mo><mn id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">10</mn><mo stretchy="false" id="S4.T2.1.1.1.m1.1.2.3.2.2" xref="S4.T2.1.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.2"><times id="S4.T2.1.1.1.m1.1.2.1.cmml" xref="S4.T2.1.1.1.m1.1.2.1"></times><ci id="S4.T2.1.1.1.m1.1.2.2.cmml" xref="S4.T2.1.1.1.m1.1.2.2">Dir</ci><cn type="integer" id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\mathrm{Dir}(10)</annotation></semantics></math></th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(5)" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.m1.1.2" xref="S4.T2.2.2.2.m1.1.2.cmml"><mi id="S4.T2.2.2.2.m1.1.2.2" xref="S4.T2.2.2.2.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.m1.1.2.1" xref="S4.T2.2.2.2.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.2.2.2.m1.1.2.3.2" xref="S4.T2.2.2.2.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.2.2.2.m1.1.2.3.2.1" xref="S4.T2.2.2.2.m1.1.2.cmml">(</mo><mn id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">5</mn><mo stretchy="false" id="S4.T2.2.2.2.m1.1.2.3.2.2" xref="S4.T2.2.2.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.m1.1.2.cmml" xref="S4.T2.2.2.2.m1.1.2"><times id="S4.T2.2.2.2.m1.1.2.1.cmml" xref="S4.T2.2.2.2.m1.1.2.1"></times><ci id="S4.T2.2.2.2.m1.1.2.2.cmml" xref="S4.T2.2.2.2.m1.1.2.2">Dir</ci><cn type="integer" id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\mathrm{Dir}(5)</annotation></semantics></math></th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(1)" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.m1.1.2" xref="S4.T2.3.3.3.m1.1.2.cmml"><mi id="S4.T2.3.3.3.m1.1.2.2" xref="S4.T2.3.3.3.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.m1.1.2.1" xref="S4.T2.3.3.3.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.3.3.3.m1.1.2.3.2" xref="S4.T2.3.3.3.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.3.3.3.m1.1.2.3.2.1" xref="S4.T2.3.3.3.m1.1.2.cmml">(</mo><mn id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">1</mn><mo stretchy="false" id="S4.T2.3.3.3.m1.1.2.3.2.2" xref="S4.T2.3.3.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.m1.1.2.cmml" xref="S4.T2.3.3.3.m1.1.2"><times id="S4.T2.3.3.3.m1.1.2.1.cmml" xref="S4.T2.3.3.3.m1.1.2.1"></times><ci id="S4.T2.3.3.3.m1.1.2.2.cmml" xref="S4.T2.3.3.3.m1.1.2.2">Dir</ci><cn type="integer" id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\mathrm{Dir}(1)</annotation></semantics></math></th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.6)" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mrow id="S4.T2.4.4.4.m1.1.2" xref="S4.T2.4.4.4.m1.1.2.cmml"><mi id="S4.T2.4.4.4.m1.1.2.2" xref="S4.T2.4.4.4.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.4.4.4.m1.1.2.1" xref="S4.T2.4.4.4.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.4.4.4.m1.1.2.3.2" xref="S4.T2.4.4.4.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.4.4.4.m1.1.2.3.2.1" xref="S4.T2.4.4.4.m1.1.2.cmml">(</mo><mn id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">0.6</mn><mo stretchy="false" id="S4.T2.4.4.4.m1.1.2.3.2.2" xref="S4.T2.4.4.4.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><apply id="S4.T2.4.4.4.m1.1.2.cmml" xref="S4.T2.4.4.4.m1.1.2"><times id="S4.T2.4.4.4.m1.1.2.1.cmml" xref="S4.T2.4.4.4.m1.1.2.1"></times><ci id="S4.T2.4.4.4.m1.1.2.2.cmml" xref="S4.T2.4.4.4.m1.1.2.2">Dir</ci><cn type="float" id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\mathrm{Dir}(0.6)</annotation></semantics></math></th>
<th id="S4.T2.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.5.5.5.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.5)" display="inline"><semantics id="S4.T2.5.5.5.m1.1a"><mrow id="S4.T2.5.5.5.m1.1.2" xref="S4.T2.5.5.5.m1.1.2.cmml"><mi id="S4.T2.5.5.5.m1.1.2.2" xref="S4.T2.5.5.5.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.5.5.5.m1.1.2.1" xref="S4.T2.5.5.5.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.5.5.5.m1.1.2.3.2" xref="S4.T2.5.5.5.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.5.5.5.m1.1.2.3.2.1" xref="S4.T2.5.5.5.m1.1.2.cmml">(</mo><mn id="S4.T2.5.5.5.m1.1.1" xref="S4.T2.5.5.5.m1.1.1.cmml">0.5</mn><mo stretchy="false" id="S4.T2.5.5.5.m1.1.2.3.2.2" xref="S4.T2.5.5.5.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.m1.1b"><apply id="S4.T2.5.5.5.m1.1.2.cmml" xref="S4.T2.5.5.5.m1.1.2"><times id="S4.T2.5.5.5.m1.1.2.1.cmml" xref="S4.T2.5.5.5.m1.1.2.1"></times><ci id="S4.T2.5.5.5.m1.1.2.2.cmml" xref="S4.T2.5.5.5.m1.1.2.2">Dir</ci><cn type="float" id="S4.T2.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.m1.1.1">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">\mathrm{Dir}(0.5)</annotation></semantics></math></th>
<th id="S4.T2.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.6.6.6.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.4)" display="inline"><semantics id="S4.T2.6.6.6.m1.1a"><mrow id="S4.T2.6.6.6.m1.1.2" xref="S4.T2.6.6.6.m1.1.2.cmml"><mi id="S4.T2.6.6.6.m1.1.2.2" xref="S4.T2.6.6.6.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.6.6.6.m1.1.2.1" xref="S4.T2.6.6.6.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.6.6.6.m1.1.2.3.2" xref="S4.T2.6.6.6.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.6.6.6.m1.1.2.3.2.1" xref="S4.T2.6.6.6.m1.1.2.cmml">(</mo><mn id="S4.T2.6.6.6.m1.1.1" xref="S4.T2.6.6.6.m1.1.1.cmml">0.4</mn><mo stretchy="false" id="S4.T2.6.6.6.m1.1.2.3.2.2" xref="S4.T2.6.6.6.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.m1.1b"><apply id="S4.T2.6.6.6.m1.1.2.cmml" xref="S4.T2.6.6.6.m1.1.2"><times id="S4.T2.6.6.6.m1.1.2.1.cmml" xref="S4.T2.6.6.6.m1.1.2.1"></times><ci id="S4.T2.6.6.6.m1.1.2.2.cmml" xref="S4.T2.6.6.6.m1.1.2.2">Dir</ci><cn type="float" id="S4.T2.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.m1.1.1">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.m1.1c">\mathrm{Dir}(0.4)</annotation></semantics></math></th>
<th id="S4.T2.7.7.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T2.7.7.7.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.3)" display="inline"><semantics id="S4.T2.7.7.7.m1.1a"><mrow id="S4.T2.7.7.7.m1.1.2" xref="S4.T2.7.7.7.m1.1.2.cmml"><mi id="S4.T2.7.7.7.m1.1.2.2" xref="S4.T2.7.7.7.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.7.7.7.m1.1.2.1" xref="S4.T2.7.7.7.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.7.7.7.m1.1.2.3.2" xref="S4.T2.7.7.7.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.7.7.7.m1.1.2.3.2.1" xref="S4.T2.7.7.7.m1.1.2.cmml">(</mo><mn id="S4.T2.7.7.7.m1.1.1" xref="S4.T2.7.7.7.m1.1.1.cmml">0.3</mn><mo stretchy="false" id="S4.T2.7.7.7.m1.1.2.3.2.2" xref="S4.T2.7.7.7.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.m1.1b"><apply id="S4.T2.7.7.7.m1.1.2.cmml" xref="S4.T2.7.7.7.m1.1.2"><times id="S4.T2.7.7.7.m1.1.2.1.cmml" xref="S4.T2.7.7.7.m1.1.2.1"></times><ci id="S4.T2.7.7.7.m1.1.2.2.cmml" xref="S4.T2.7.7.7.m1.1.2.2">Dir</ci><cn type="float" id="S4.T2.7.7.7.m1.1.1.cmml" xref="S4.T2.7.7.7.m1.1.1">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.m1.1c">\mathrm{Dir}(0.3)</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.7.8.1" class="ltx_tr">
<th id="S4.T2.7.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SCAFFOLD</th>
<th id="S4.T2.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">10%</th>
<td id="S4.T2.7.8.1.3" class="ltx_td ltx_align_center ltx_border_t">60%</td>
<td id="S4.T2.7.8.1.4" class="ltx_td ltx_align_center ltx_border_t">80%</td>
<td id="S4.T2.7.8.1.5" class="ltx_td ltx_align_center ltx_border_t">100%</td>
<td id="S4.T2.7.8.1.6" class="ltx_td ltx_align_center ltx_border_t">100%</td>
<td id="S4.T2.7.8.1.7" class="ltx_td ltx_align_center ltx_border_t">100%</td>
<td id="S4.T2.7.8.1.8" class="ltx_td ltx_align_center ltx_border_t">100%</td>
<td id="S4.T2.7.8.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">100%</td>
</tr>
<tr id="S4.T2.7.9.2" class="ltx_tr">
<th id="S4.T2.7.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">FedDyn</th>
<th id="S4.T2.7.9.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">0%</th>
<td id="S4.T2.7.9.2.3" class="ltx_td ltx_align_center ltx_border_bb">0%</td>
<td id="S4.T2.7.9.2.4" class="ltx_td ltx_align_center ltx_border_bb">0%</td>
<td id="S4.T2.7.9.2.5" class="ltx_td ltx_align_center ltx_border_bb">50%</td>
<td id="S4.T2.7.9.2.6" class="ltx_td ltx_align_center ltx_border_bb">80%</td>
<td id="S4.T2.7.9.2.7" class="ltx_td ltx_align_center ltx_border_bb">100%</td>
<td id="S4.T2.7.9.2.8" class="ltx_td ltx_align_center ltx_border_bb">80%</td>
<td id="S4.T2.7.9.2.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">40%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Catastrophic failure rates in the absence of gradient clipping. For each distribution generated from the CIFAR-10 dataset, the catastrophic failure rate is recorded out of 10 runs. All other algorithms face no failure across data distributions while FedAdam experiences 20% failure rate only in the case of the <math id="S4.T2.9.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.4)" display="inline"><semantics id="S4.T2.9.m1.1b"><mrow id="S4.T2.9.m1.1.2" xref="S4.T2.9.m1.1.2.cmml"><mi id="S4.T2.9.m1.1.2.2" xref="S4.T2.9.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.T2.9.m1.1.2.1" xref="S4.T2.9.m1.1.2.1.cmml">​</mo><mrow id="S4.T2.9.m1.1.2.3.2" xref="S4.T2.9.m1.1.2.cmml"><mo stretchy="false" id="S4.T2.9.m1.1.2.3.2.1" xref="S4.T2.9.m1.1.2.cmml">(</mo><mn id="S4.T2.9.m1.1.1" xref="S4.T2.9.m1.1.1.cmml">0.4</mn><mo stretchy="false" id="S4.T2.9.m1.1.2.3.2.2" xref="S4.T2.9.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.m1.1c"><apply id="S4.T2.9.m1.1.2.cmml" xref="S4.T2.9.m1.1.2"><times id="S4.T2.9.m1.1.2.1.cmml" xref="S4.T2.9.m1.1.2.1"></times><ci id="S4.T2.9.m1.1.2.2.cmml" xref="S4.T2.9.m1.1.2.2">Dir</ci><cn type="float" id="S4.T2.9.m1.1.1.cmml" xref="S4.T2.9.m1.1.1">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.m1.1d">\mathrm{Dir}(0.4)</annotation></semantics></math> distribution.
</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Impact of Gradient Clipping</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">A catastrophic training failure occurs when the accuracy decreases by more than half in a single round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The catastrophic failure hinges on a variety of factors including learning rate, distribution, and neural network architecture.
Gradient clipping is used to make federated learning algorithms more stable and avoid catastrophic failures.
However, the technique is not for all types of models. It is in general useful for the cases where models’ weights can explode during the training. Models such as RNN, LSTM and transformers can present such a risk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
As it introduces an additional hyperparameter (the maximum norm of a gradient step),
a failure to setting a proper value for this parameter may lead to little learning and inefficient resource usage.
Therefore, in this experiment, our goal is to understand the stability of algorithms when gradient clipping is disabled from the client-side training process. We also vary data heterogeneity by employing Dirichlet distributions
to test the effect of data heterogeneity among clients for catastrophic failure rate.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Takeaway 10:</span> <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">Complex client-side optimizers experience catastrophic failure with high probability.</span></p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.4" class="ltx_p">Most algorithms we test (FedAvg, FedYogi, and FedProx) experience no failure. FedAdam has no failure except for one case with the <math id="S4.SS5.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.4)" display="inline"><semantics id="S4.SS5.p3.1.m1.1a"><mrow id="S4.SS5.p3.1.m1.1.2" xref="S4.SS5.p3.1.m1.1.2.cmml"><mi id="S4.SS5.p3.1.m1.1.2.2" xref="S4.SS5.p3.1.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.1.m1.1.2.1" xref="S4.SS5.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S4.SS5.p3.1.m1.1.2.3.2" xref="S4.SS5.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S4.SS5.p3.1.m1.1.2.3.2.1" xref="S4.SS5.p3.1.m1.1.2.cmml">(</mo><mn id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml">0.4</mn><mo stretchy="false" id="S4.SS5.p3.1.m1.1.2.3.2.2" xref="S4.SS5.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><apply id="S4.SS5.p3.1.m1.1.2.cmml" xref="S4.SS5.p3.1.m1.1.2"><times id="S4.SS5.p3.1.m1.1.2.1.cmml" xref="S4.SS5.p3.1.m1.1.2.1"></times><ci id="S4.SS5.p3.1.m1.1.2.2.cmml" xref="S4.SS5.p3.1.m1.1.2.2">Dir</ci><cn type="float" id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">\mathrm{Dir}(0.4)</annotation></semantics></math> distribution; it has a failure rate of 20%. SCAFFOLD and FedDyn are the only algorithms that exhibit higher failure rates. Table <a href="#S4.T2" title="Table 2 ‣ 4.4 Performance Stability Across Clients ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that both algorithms experience 40% or higher failure rates when <math id="S4.SS5.p3.2.m2.1" class="ltx_Math" alttext="\alpha\leq 1" display="inline"><semantics id="S4.SS5.p3.2.m2.1a"><mrow id="S4.SS5.p3.2.m2.1.1" xref="S4.SS5.p3.2.m2.1.1.cmml"><mi id="S4.SS5.p3.2.m2.1.1.2" xref="S4.SS5.p3.2.m2.1.1.2.cmml">α</mi><mo id="S4.SS5.p3.2.m2.1.1.1" xref="S4.SS5.p3.2.m2.1.1.1.cmml">≤</mo><mn id="S4.SS5.p3.2.m2.1.1.3" xref="S4.SS5.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.2.m2.1b"><apply id="S4.SS5.p3.2.m2.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1"><leq id="S4.SS5.p3.2.m2.1.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1.1"></leq><ci id="S4.SS5.p3.2.m2.1.1.2.cmml" xref="S4.SS5.p3.2.m2.1.1.2">𝛼</ci><cn type="integer" id="S4.SS5.p3.2.m2.1.1.3.cmml" xref="S4.SS5.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.2.m2.1c">\alpha\leq 1</annotation></semantics></math> for the Dirichlet distribution.
These two FL methods are the only ones that preserve a state aside from the model, which increases training complexity. FedDyn maintains the term <math id="S4.SS5.p3.3.m3.1" class="ltx_Math" alttext="\nabla L(\theta_{k}^{t-1})" display="inline"><semantics id="S4.SS5.p3.3.m3.1a"><mrow id="S4.SS5.p3.3.m3.1.1" xref="S4.SS5.p3.3.m3.1.1.cmml"><mrow id="S4.SS5.p3.3.m3.1.1.3" xref="S4.SS5.p3.3.m3.1.1.3.cmml"><mo rspace="0.167em" id="S4.SS5.p3.3.m3.1.1.3.1" xref="S4.SS5.p3.3.m3.1.1.3.1.cmml">∇</mo><mi id="S4.SS5.p3.3.m3.1.1.3.2" xref="S4.SS5.p3.3.m3.1.1.3.2.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S4.SS5.p3.3.m3.1.1.2" xref="S4.SS5.p3.3.m3.1.1.2.cmml">​</mo><mrow id="S4.SS5.p3.3.m3.1.1.1.1" xref="S4.SS5.p3.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS5.p3.3.m3.1.1.1.1.2" xref="S4.SS5.p3.3.m3.1.1.1.1.1.cmml">(</mo><msubsup id="S4.SS5.p3.3.m3.1.1.1.1.1" xref="S4.SS5.p3.3.m3.1.1.1.1.1.cmml"><mi id="S4.SS5.p3.3.m3.1.1.1.1.1.2.2" xref="S4.SS5.p3.3.m3.1.1.1.1.1.2.2.cmml">θ</mi><mi id="S4.SS5.p3.3.m3.1.1.1.1.1.2.3" xref="S4.SS5.p3.3.m3.1.1.1.1.1.2.3.cmml">k</mi><mrow id="S4.SS5.p3.3.m3.1.1.1.1.1.3" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.cmml"><mi id="S4.SS5.p3.3.m3.1.1.1.1.1.3.2" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.2.cmml">t</mi><mo id="S4.SS5.p3.3.m3.1.1.1.1.1.3.1" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.1.cmml">−</mo><mn id="S4.SS5.p3.3.m3.1.1.1.1.1.3.3" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.3.cmml">1</mn></mrow></msubsup><mo stretchy="false" id="S4.SS5.p3.3.m3.1.1.1.1.3" xref="S4.SS5.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.3.m3.1b"><apply id="S4.SS5.p3.3.m3.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1"><times id="S4.SS5.p3.3.m3.1.1.2.cmml" xref="S4.SS5.p3.3.m3.1.1.2"></times><apply id="S4.SS5.p3.3.m3.1.1.3.cmml" xref="S4.SS5.p3.3.m3.1.1.3"><ci id="S4.SS5.p3.3.m3.1.1.3.1.cmml" xref="S4.SS5.p3.3.m3.1.1.3.1">∇</ci><ci id="S4.SS5.p3.3.m3.1.1.3.2.cmml" xref="S4.SS5.p3.3.m3.1.1.3.2">𝐿</ci></apply><apply id="S4.SS5.p3.3.m3.1.1.1.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1">superscript</csymbol><apply id="S4.SS5.p3.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.3.m3.1.1.1.1.1.2.1.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS5.p3.3.m3.1.1.1.1.1.2.2.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1.1.2.2">𝜃</ci><ci id="S4.SS5.p3.3.m3.1.1.1.1.1.2.3.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S4.SS5.p3.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3"><minus id="S4.SS5.p3.3.m3.1.1.1.1.1.3.1.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.1"></minus><ci id="S4.SS5.p3.3.m3.1.1.1.1.1.3.2.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S4.SS5.p3.3.m3.1.1.1.1.1.3.3.cmml" xref="S4.SS5.p3.3.m3.1.1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.3.m3.1c">\nabla L(\theta_{k}^{t-1})</annotation></semantics></math> locally, and also maintains a history term <math id="S4.SS5.p3.4.m4.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S4.SS5.p3.4.m4.1a"><msub id="S4.SS5.p3.4.m4.1.1" xref="S4.SS5.p3.4.m4.1.1.cmml"><mi id="S4.SS5.p3.4.m4.1.1.2" xref="S4.SS5.p3.4.m4.1.1.2.cmml">h</mi><mi id="S4.SS5.p3.4.m4.1.1.3" xref="S4.SS5.p3.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.4.m4.1b"><apply id="S4.SS5.p3.4.m4.1.1.cmml" xref="S4.SS5.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.4.m4.1.1.1.cmml" xref="S4.SS5.p3.4.m4.1.1">subscript</csymbol><ci id="S4.SS5.p3.4.m4.1.1.2.cmml" xref="S4.SS5.p3.4.m4.1.1.2">ℎ</ci><ci id="S4.SS5.p3.4.m4.1.1.3.cmml" xref="S4.SS5.p3.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.4.m4.1c">h_{t}</annotation></semantics></math> on the server side which is used in aggregation.
SCAFFOLD has a control variate which is used in the local loss function.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p"><span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">Takeaway 11:</span> <span id="S4.SS5.p4.1.2" class="ltx_text ltx_font_italic">Failure rates improve for IID distributions.</span></p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p">From Table <a href="#S4.T2" title="Table 2 ‣ 4.4 Performance Stability Across Clients ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe a failure rate of 100% for SCAFFOLD in the distributions with 0.6–0.3 Dirichlet values. As the distributions, became more IID the failure rate monotonically declines.
FedDyn’s failure rate is highest (100%) at <math id="S4.SS5.p5.1.m1.1" class="ltx_Math" alttext="\mathrm{Dir}(0.5)" display="inline"><semantics id="S4.SS5.p5.1.m1.1a"><mrow id="S4.SS5.p5.1.m1.1.2" xref="S4.SS5.p5.1.m1.1.2.cmml"><mi id="S4.SS5.p5.1.m1.1.2.2" xref="S4.SS5.p5.1.m1.1.2.2.cmml">Dir</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p5.1.m1.1.2.1" xref="S4.SS5.p5.1.m1.1.2.1.cmml">​</mo><mrow id="S4.SS5.p5.1.m1.1.2.3.2" xref="S4.SS5.p5.1.m1.1.2.cmml"><mo stretchy="false" id="S4.SS5.p5.1.m1.1.2.3.2.1" xref="S4.SS5.p5.1.m1.1.2.cmml">(</mo><mn id="S4.SS5.p5.1.m1.1.1" xref="S4.SS5.p5.1.m1.1.1.cmml">0.5</mn><mo stretchy="false" id="S4.SS5.p5.1.m1.1.2.3.2.2" xref="S4.SS5.p5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p5.1.m1.1b"><apply id="S4.SS5.p5.1.m1.1.2.cmml" xref="S4.SS5.p5.1.m1.1.2"><times id="S4.SS5.p5.1.m1.1.2.1.cmml" xref="S4.SS5.p5.1.m1.1.2.1"></times><ci id="S4.SS5.p5.1.m1.1.2.2.cmml" xref="S4.SS5.p5.1.m1.1.2.2">Dir</ci><cn type="float" id="S4.SS5.p5.1.m1.1.1.cmml" xref="S4.SS5.p5.1.m1.1.1">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p5.1.m1.1c">\mathrm{Dir}(0.5)</annotation></semantics></math>, and declines on either side.
In both algorithms, the lowest failure rates (10% for SCAFFOLD and 0% for FedDyn) are obtained when the dataset distribution is IID. This meets our expectation since the IID case would lead the data to be similar across clients, which, in turn, would likely lead to smaller training updates. Therefore, the likelihood of overflow is smaller than that of the non-IID case.</p>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<p id="S4.SS5.p6.1" class="ltx_p"><span id="S4.SS5.p6.1.1" class="ltx_text ltx_font_bold">Takeaway 12:</span> <span id="S4.SS5.p6.1.2" class="ltx_text ltx_font_italic">Failure rates improve with a smaller learning rate.</span></p>
</div>
<div id="S4.SS5.p7" class="ltx_para">
<p id="S4.SS5.p7.3" class="ltx_p">Previously, there have been examples showing some correlation between gradient norm and learning rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, suggesting that a large learning rate could accelerate the growth in magnitude of the model weights.
Therefore, we also tested SCAFFOLD and FedDyn with a smaller learning rate.
In addition to our original learning rate <math id="S4.SS5.p7.1.m1.1" class="ltx_Math" alttext="\eta=0.1" display="inline"><semantics id="S4.SS5.p7.1.m1.1a"><mrow id="S4.SS5.p7.1.m1.1.1" xref="S4.SS5.p7.1.m1.1.1.cmml"><mi id="S4.SS5.p7.1.m1.1.1.2" xref="S4.SS5.p7.1.m1.1.1.2.cmml">η</mi><mo id="S4.SS5.p7.1.m1.1.1.1" xref="S4.SS5.p7.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS5.p7.1.m1.1.1.3" xref="S4.SS5.p7.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p7.1.m1.1b"><apply id="S4.SS5.p7.1.m1.1.1.cmml" xref="S4.SS5.p7.1.m1.1.1"><eq id="S4.SS5.p7.1.m1.1.1.1.cmml" xref="S4.SS5.p7.1.m1.1.1.1"></eq><ci id="S4.SS5.p7.1.m1.1.1.2.cmml" xref="S4.SS5.p7.1.m1.1.1.2">𝜂</ci><cn type="float" id="S4.SS5.p7.1.m1.1.1.3.cmml" xref="S4.SS5.p7.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p7.1.m1.1c">\eta=0.1</annotation></semantics></math>,
we employ <math id="S4.SS5.p7.2.m2.1" class="ltx_Math" alttext="\eta=10^{-\frac{3}{2}}(\sim 0.032)" display="inline"><semantics id="S4.SS5.p7.2.m2.1a"><mrow id="S4.SS5.p7.2.m2.1.1" xref="S4.SS5.p7.2.m2.1.1.cmml"><mi id="S4.SS5.p7.2.m2.1.1.3" xref="S4.SS5.p7.2.m2.1.1.3.cmml">η</mi><mo id="S4.SS5.p7.2.m2.1.1.2" xref="S4.SS5.p7.2.m2.1.1.2.cmml">=</mo><mrow id="S4.SS5.p7.2.m2.1.1.1" xref="S4.SS5.p7.2.m2.1.1.1.cmml"><msup id="S4.SS5.p7.2.m2.1.1.1.3" xref="S4.SS5.p7.2.m2.1.1.1.3.cmml"><mn id="S4.SS5.p7.2.m2.1.1.1.3.2" xref="S4.SS5.p7.2.m2.1.1.1.3.2.cmml">10</mn><mrow id="S4.SS5.p7.2.m2.1.1.1.3.3" xref="S4.SS5.p7.2.m2.1.1.1.3.3.cmml"><mo id="S4.SS5.p7.2.m2.1.1.1.3.3a" xref="S4.SS5.p7.2.m2.1.1.1.3.3.cmml">−</mo><mfrac id="S4.SS5.p7.2.m2.1.1.1.3.3.2" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2.cmml"><mn id="S4.SS5.p7.2.m2.1.1.1.3.3.2.2" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2.2.cmml">3</mn><mn id="S4.SS5.p7.2.m2.1.1.1.3.3.2.3" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2.3.cmml">2</mn></mfrac></mrow></msup><mspace width="0.3888888888888889em" id="S4.SS5.p7.2.m2.1.1.1a" xref="S4.SS5.p7.2.m2.1.1.1.cmml"></mspace><mrow id="S4.SS5.p7.2.m2.1.1.1.1.1" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS5.p7.2.m2.1.1.1.1.1.2" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS5.p7.2.m2.1.1.1.1.1.1" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.cmml"><mi id="S4.SS5.p7.2.m2.1.1.1.1.1.1.2" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.2.cmml"></mi><mo id="S4.SS5.p7.2.m2.1.1.1.1.1.1.1" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.1.cmml">∼</mo><mn id="S4.SS5.p7.2.m2.1.1.1.1.1.1.3" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.3.cmml">0.032</mn></mrow><mo stretchy="false" id="S4.SS5.p7.2.m2.1.1.1.1.1.3" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p7.2.m2.1b"><apply id="S4.SS5.p7.2.m2.1.1.cmml" xref="S4.SS5.p7.2.m2.1.1"><eq id="S4.SS5.p7.2.m2.1.1.2.cmml" xref="S4.SS5.p7.2.m2.1.1.2"></eq><ci id="S4.SS5.p7.2.m2.1.1.3.cmml" xref="S4.SS5.p7.2.m2.1.1.3">𝜂</ci><apply id="S4.SS5.p7.2.m2.1.1.1.cmml" xref="S4.SS5.p7.2.m2.1.1.1"><csymbol cd="latexml" id="S4.SS5.p7.2.m2.1.1.1.2.cmml" xref="S4.SS5.p7.2.m2.1.1.1">annotated</csymbol><apply id="S4.SS5.p7.2.m2.1.1.1.3.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS5.p7.2.m2.1.1.1.3.1.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS5.p7.2.m2.1.1.1.3.2.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.2">10</cn><apply id="S4.SS5.p7.2.m2.1.1.1.3.3.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.3"><minus id="S4.SS5.p7.2.m2.1.1.1.3.3.1.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.3"></minus><apply id="S4.SS5.p7.2.m2.1.1.1.3.3.2.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2"><divide id="S4.SS5.p7.2.m2.1.1.1.3.3.2.1.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2"></divide><cn type="integer" id="S4.SS5.p7.2.m2.1.1.1.3.3.2.2.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2.2">3</cn><cn type="integer" id="S4.SS5.p7.2.m2.1.1.1.3.3.2.3.cmml" xref="S4.SS5.p7.2.m2.1.1.1.3.3.2.3">2</cn></apply></apply></apply><apply id="S4.SS5.p7.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS5.p7.2.m2.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS5.p7.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS5.p7.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.2">absent</csymbol><cn type="float" id="S4.SS5.p7.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS5.p7.2.m2.1.1.1.1.1.1.3">0.032</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p7.2.m2.1c">\eta=10^{-\frac{3}{2}}(\sim 0.032)</annotation></semantics></math> since the value is used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
With this smaller learning rate, the failure completely disappears for SCAFFOLD and FedDyn across all distributions. This indicates that the failure rate, in the absence of gradient clipping, is highly sensitive to the learning rate, since the two learning rates we tested only differ by a factor of <math id="S4.SS5.p7.3.m3.1" class="ltx_Math" alttext="10^{-\frac{1}{2}}" display="inline"><semantics id="S4.SS5.p7.3.m3.1a"><msup id="S4.SS5.p7.3.m3.1.1" xref="S4.SS5.p7.3.m3.1.1.cmml"><mn id="S4.SS5.p7.3.m3.1.1.2" xref="S4.SS5.p7.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS5.p7.3.m3.1.1.3" xref="S4.SS5.p7.3.m3.1.1.3.cmml"><mo id="S4.SS5.p7.3.m3.1.1.3a" xref="S4.SS5.p7.3.m3.1.1.3.cmml">−</mo><mfrac id="S4.SS5.p7.3.m3.1.1.3.2" xref="S4.SS5.p7.3.m3.1.1.3.2.cmml"><mn id="S4.SS5.p7.3.m3.1.1.3.2.2" xref="S4.SS5.p7.3.m3.1.1.3.2.2.cmml">1</mn><mn id="S4.SS5.p7.3.m3.1.1.3.2.3" xref="S4.SS5.p7.3.m3.1.1.3.2.3.cmml">2</mn></mfrac></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS5.p7.3.m3.1b"><apply id="S4.SS5.p7.3.m3.1.1.cmml" xref="S4.SS5.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p7.3.m3.1.1.1.cmml" xref="S4.SS5.p7.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS5.p7.3.m3.1.1.2.cmml" xref="S4.SS5.p7.3.m3.1.1.2">10</cn><apply id="S4.SS5.p7.3.m3.1.1.3.cmml" xref="S4.SS5.p7.3.m3.1.1.3"><minus id="S4.SS5.p7.3.m3.1.1.3.1.cmml" xref="S4.SS5.p7.3.m3.1.1.3"></minus><apply id="S4.SS5.p7.3.m3.1.1.3.2.cmml" xref="S4.SS5.p7.3.m3.1.1.3.2"><divide id="S4.SS5.p7.3.m3.1.1.3.2.1.cmml" xref="S4.SS5.p7.3.m3.1.1.3.2"></divide><cn type="integer" id="S4.SS5.p7.3.m3.1.1.3.2.2.cmml" xref="S4.SS5.p7.3.m3.1.1.3.2.2">1</cn><cn type="integer" id="S4.SS5.p7.3.m3.1.1.3.2.3.cmml" xref="S4.SS5.p7.3.m3.1.1.3.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p7.3.m3.1c">10^{-\frac{1}{2}}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Choosing algorithm.</span>
This paper focuses on generic synchronous FL algorithms and excludes algorithms which try to optimize metrics such as fairness. Here we discuss what to consider beyond accruacy in choosing an FL algorithm.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The algorithms considered in this paper are frequently evaluated with respect to round.
There is a cost associated with modifying the client-side loss functions, which can result in the round being an unfair measure of work (see Figure <a href="#S4.F1.sf1" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1a</span></a> and Figure <a href="#S4.F1.sf2" title="In Figure 1 ‣ 4 Evaluation ‣ Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1b</span></a>).
In the presence of an additional loss function component, the graph used for back propagation grows in size linearly related to the number of trainable parameters in the neural network.
Therefore, we observe the training runtime increase from SCAFFOLD, FedProx and FedDyn.
In some cases, this means algorithms with simpler server-side optimization may be cost-efficient.
This can be taken into account when the cost of training in one round needs to be minimized.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Additionally, consistent performance across clients can be important for an application.
We observe differences across optimizers for this metric.
FedProx is among the worst performing for this test.
In general, FedDyn, FedAdam, and FedYogi usually perform better.
We also notice that SCAFFOLD performs worse on all non-IID distributions.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Catastrophic failure is another factor to consider in choosing an algorithm. In particular, we learn that gradient clipping can affect optimizers severely.
That is, algorithms that preserve additional state apart from the global model weights are more likely to experience catastrophic failures while training without gradient clipping. We demonstrate algorithm’s vulnerability with gradient clipping. As far as gradient clipping is concerned, it is useful to keep it enabled. However, there may be other vulnerabilities due to unknown factors (e.g., dataset heterogeneity degree, straggling client, and so on).
Thus, one should carefully assess the tradeoff between failure possibility and performance gain.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Different types of FL approaches.</span> There are numerous FL approaches beyond the algorithms we considered in this paper. Oort <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> employs client overselection and client’s utility to expedite the training process. FedBalancer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> systematically picks ‘useful’ data samples to achieve a faster convergence rate. FedBuff <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> trains the model asynchronously by aggregating local models at any point in time during the training without per-round synchronization barrier. REFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> selects clients to improve resource-efficiency. One future direction is to study how the algorithms would work with these approaches.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we empirically study six well-known federated learning algorithms in terms of time-to-accuracy, computation and communication overhead, performance stability across clients, and training instability. Our experimental evaluation reveals that no single federated learning algorithm outperforms across all the evaluation metrics under consideration. One of the most recent algorithms, FedDyn in general achieves the highest accuracy given a fixed number of rounds. However, it tends to require higher amount of computation resources than other algorithms, thus taking longer than other algorithms to finish the fixed number of rounds. In addition, FedDyn is likely to face some instability issue more frequently than other algorithms during training; we test this hypothesis by disabling gradient clipping. While FedDyn achieves the smallest accuracy standard deviation across clients (hence, the highest performance stability), other client-side optimization algorithms such as FedProx and SCAFFOLD obtain lower performance stability than server-side algorithms such as FedYogi and FedAdam. In general, those server-side optimization algorithms can be good alternatives as they tend to perform reasonably well with little extra computation overhead and run without failures in the absence of gradient clipping. We hope that our results can assist FL algorithm selection task for FL practitioners and encourage the community to build best practices for FL algorithm evaluation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. S. H. Abad, E. Ozfatura, D. GUndUz, and O. Ercetin.

</span>
<span class="ltx_bibblock">Hierarchical federated learning across heterogeneous cellular
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 8866–8870, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and
Heiko Ludwig.

</span>
<span class="ltx_bibblock">Mitigating bias in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.02447</span>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ahmed M. Abdelmoniem, Atal Narayan Sahu, Marco Canini, and Suhaib A. Fahmy.

</span>
<span class="ltx_bibblock">Refl: Resource-efficient federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the Eighteenth European Conference on Computer
Systems</span>, EuroSys ’23, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N
Whatmough, and Venkatesh Saligrama.

</span>
<span class="ltx_bibblock">Federated learning based on dynamic regularization.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.04263</span>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Zachary Charles, Zachary Garrett, Zhouyuan Huo, Sergei Shmulyian, and Virginia
Smith.

</span>
<span class="ltx_bibblock">On large-cohort training for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
volume 34. Curran Associates, Inc., 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Lingyang Chu, Lanjun Wang, Yanjie Dong, Jian Pei, Zirui Zhou, and Yong Zhang.

</span>
<span class="ltx_bibblock">Fedfair: Training fair models in cross-silo federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.05662</span>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Harshit Daga, Jaemin Shin, Dhruv Garg, Ada Gavrilovska, Myungjin Lee, and
Ramana Rao Kompella.

</span>
<span class="ltx_bibblock">Flame: Simplifying topology extension in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2023 ACM Symposium on Cloud Computing</span>,
SoCC ’23, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fairfed: Enabling group fairness in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2110.00857</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Allen-Jasmin Farcas, Myungjin Lee, Ramana Rao Kompella, Hugo Latapie, Gustavo
de Veciana, and Radu Marculescu.

</span>
<span class="ltx_bibblock">Mohawk: Mobility and heterogeneity-aware dynamic community selection
for hierarchical federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">International Conference on Internet-of-Things Design and
Implementation (IoTDI)</span>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu.

</span>
<span class="ltx_bibblock">Feddc: Federated learning with non-iid data via local drift
decoupling and correction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 10112–10121, June 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
I. Goodfellow, Y. Bengio, and A. Courville.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Deep Learning</span>.

</span>
<span class="ltx_bibblock">Adaptive computation and machine learning. MIT Press, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ji Chu Jiang, Burak Kantarci, Sema Oktug, and Tolga Soyata.

</span>
<span class="ltx_bibblock">Federated learning in smart city sensing: Challenges and
opportunities.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 20(21):6230, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">SCAFFOLD: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In Hal Daumé III and Aarti Singh, editors, <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the
37th International Conference on Machine Learning</span>, volume 119 of <span id="bib.bib13.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 5132–5143. PMLR, 13–18 Jul
2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Daniel Ramage, and Peter
Richtárik.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine learning for on-device
intelligence.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.02527</span>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Fan Lai, Yinwei Dai, Xiangfeng Zhu, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">Fedscale: Benchmarking model and system performance of federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2105.11367, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">Oort: Efficient federated learning via guided participant selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">15th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 21)</span>, pages 19–35. USENIX Association, July 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and
Bingsheng He.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: Vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Knowl. and Data Eng.</span>, 35(4):3347–3366, apr
2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock">In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, volume 2, pages 429–450, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera
y Arcas.

</span>
<span class="ltx_bibblock">Federated learning of deep networks using model averaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1602.05629, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani
Malek, and Dzmitry Huba.

</span>
<span class="ltx_bibblock">Federated learning with buffered asynchronous aggregation.

</span>
<span class="ltx_bibblock">In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera,
editors, <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of The 25th International Conference on Artificial
Intelligence and Statistics</span>, volume 151 of <span id="bib.bib21.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine
Learning Research</span>, pages 3581–3607. PMLR, 28–30 Mar 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečný, Sanjiv Kumar, and H. Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2003.00295, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jaemin Shin, Yuanchun Li, Yunxin Liu, and Sung-Ju Lee.

</span>
<span class="ltx_bibblock">Fedbalancer: Data and pace control for efficient federated learning
on heterogeneous clients.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the 20th Annual International Conference on
Mobile Systems, Applications and Services</span>, MobiSys ’22, page 436–449, New
York, NY, USA, 2022. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ganghua Wang, Ali Payani, Myungjin Lee, and Ramana Kompella.

</span>
<span class="ltx_bibblock">Mitigating group bias in federated learning: Beyond local fairness,
2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor.

</span>
<span class="ltx_bibblock">Tackling the objective inconsistency problem in heterogeneous
federated optimization.

</span>
<span class="ltx_bibblock">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, volume 33,
pages 7611–7623. Curran Associates, Inc., 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu,
and Xuanzhe Liu.

</span>
<span class="ltx_bibblock">Characterizing impacts of heterogeneity in federated learning upon
large-scale smartphone data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the Web Conference 2021</span>, WWW ’21, page
935–946, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.

</span>
<span class="ltx_bibblock">Why gradient clipping accelerates training: A theoretical
justification for adaptivity.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.17286" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.17287" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.17287">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.17287" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.17288" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 14:13:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
