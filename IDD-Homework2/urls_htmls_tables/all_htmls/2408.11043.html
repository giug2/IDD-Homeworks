<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research</title>
<!--Generated on Tue Aug 20 17:48:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Retrieval Augmented Generation (RAG),  AI in Talent Management,  Qualitative Research" lang="en" name="keywords"/>
<base href="/html/2408.11043v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S1" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S2" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Quantitative and Qualitative Paradigms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S3" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S4" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Thematic Analysis Using LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S4.SS1" title="In 4. Thematic Analysis Using LLMs ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Thematic analysis enhanced through Retrieval Augmented Generation (RAG)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S4.SS1.SSS1" title="In 4.1. Thematic analysis enhanced through Retrieval Augmented Generation (RAG) ‣ 4. Thematic Analysis Using LLMs ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Focused Analysis:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S4.SS1.SSS2" title="In 4.1. Thematic analysis enhanced through Retrieval Augmented Generation (RAG) ‣ 4. Thematic Analysis Using LLMs ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Context Dilution/Managing Information Overload:</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S5" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Findings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S6" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Learnings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S6.SS1" title="In 6. Learnings ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Approaching LLMs as Novice Research Assistants can help prepare better prompts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S6.SS2" title="In 6. Learnings ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Used prudently, LLMs can help increase time effectiveness and resource efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S6.SS3" title="In 6. Learnings ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>LLM augmented approaches offer significant increase in ease and enhanced context compared to traditional NLP approaches.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S7" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Recommendations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S7.SS1" title="In 7. Recommendations ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Establishing credibility of findings by incorporating mechanism for member checks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S7.SS2" title="In 7. Recommendations ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Practicing increased researcher reflexivity.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S7.SS3" title="In 7. Recommendations ‣ Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Increasing transparency of decisions made throughout the research study.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#S8" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Closing Thoughts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#A1" title="In Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Results from analyzing the same dataset using an LDA Approach.</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sreyoshi Bhaduri
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Arlington</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Virginia</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Satya Kapoor
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Vancouver</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">British Columbia</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">Canada</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alex Gil
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Arlington</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">Virginia</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anshul Mittal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Arlington</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">Virginia</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rutu Mulkar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Seattle</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">Washington</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id21.id1">Qualitative data collection and analysis approaches, such as those employing interviews and focus groups, provide rich insights into customer attitudes, sentiment, and behavior. However, manually analyzing qualitative data requires extensive time and effort to identify relevant topics and thematic insights. This study proposes a novel approach to address this challenge by leveraging Retrieval Augmented Generation (RAG) based Large Language Models (LLMs) for analyzing interview transcripts. The novelty of this work lies in strategizing the research inquiry as one that is augmented by an LLM that serves as a novice research assistant. This research explores the mental model of LLMs to serve as novice qualitative research assistants for researchers in the talent management space. A RAG-based LLM approach is extended to enable topic modeling of semi-structured interview data, showcasing the versatility of these models beyond their traditional use in information retrieval and search. Our findings demonstrate that the LLM-augmented RAG approach can successfully extract topics of interest, with significant coverage compared to manually generated topics from the same dataset. This establishes the viability of employing LLMs as novice qualitative research assistants. Additionally, the study recommends that researchers leveraging such models lean heavily on quality criteria used in traditional qualitative research to ensure rigor and trustworthiness of their approach. Finally, the paper presents key recommendations for industry practitioners seeking to reconcile the use of LLMs with established qualitative research paradigms, providing a roadmap for the effective integration of these powerful, albeit novice, AI tools in the analysis of qualitative datasets within talent management research.</p>
</div>
<div class="ltx_keywords">Retrieval Augmented Generation (RAG), AI in Talent Management, Qualitative Research
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Talent management researchers frequently work backwards from their customers, the employees at the organization. Understanding employee sentiment and behavior often involves conducting deep-dive interviews, explanatory in nature – e.g., demystifying the why behind customer choices, attitudes or behaviors (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Leino and Räihä, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib23" title="">2007</a>)</cite>). Talent management research, at its core, seeks to use science to equip every employee with resources to help them best navigate their careers <cite class="ltx_cite ltx_citemacro_citep">(Zhao, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib41" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Consequently, qualitative research methodology plays a critical role in talent management. Many of the key considerations around employee engagement, motivation, and workforce culture involve subjective, context-dependent factors that are best explored through in-depth interviews, focus groups, and other qualitative data collection approaches. Talent management professionals often rely on rich qualitative datasets to gain deep insights into employee experiences, organizational dynamics, and the nuances of human capital. However, these qualitative paradigms can clash with the more positivist, quantitative worldview that underlies many of the analytic tools used to evaluate talent management data. Talent management researchers may find that standard statistical techniques and data visualization approaches struggle to fully capture the complexities inherent in qualitative datasets, leading to potential misinterpretations or oversimplifications of the human elements involved in managing an organization’s workforce. Navigating this tension between qualitative and quantitative approaches is an ongoing challenge for talent management professionals.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Large language models (LLMs) like BERT, GPT-3 and PaLM have demonstrated strong aptitude for summarization (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib38" title="">2023</a>)</cite>), classification (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Pelaez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib32" title="">2024</a>)</cite>), and information extraction (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Dunn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib14" title="">2022</a>)</cite>) for text-based data. Consequently, LLMs are also increasingly being leveraged within talent management contexts for tasks such as interview analysis. However, language models are themselves designed primarily from a quantitative, data-driven paradigm. These models are trained on vast troves of text data using statistical machine learning techniques optimized for numerical patterns and correlations. While powerful at extracting insights from large-scale datasets, LLMs can often struggle to fully capture the nuanced, contextual nature of language <cite class="ltx_cite ltx_citemacro_citep">(Bender et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib2" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Dwivedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib15" title="">2023</a>)</cite> that is critical for qualitative information sourced from interviews, focus groups, and other qualitative research methods common in talent management.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Talent management professionals must therefore continuously navigate a tension between the quantitative orientation of their analytical tools and the qualitative richness of the human dynamics they seek to understand. Bridging this gap requires innovative approaches that combine the opportunity for scale and speed offered by LLM-powered analysis augmented by borrowing evaluative nuances of traditional qualitative techniques. Talent leaders, thus, must carefully select and configure their AI-powered tools to ensure the voices and experiences of employees are authentically represented, rather than reduced to oversimplified metrics. Mastering this balance is an ongoing challenge, but one that is critical for talent management to yield truly holistic and impactful insights.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper presents results from leveraging LLMs as a novice qualitative researcher to augment qualitative research workstreams, specifically for data generated through semi-structured interviews.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The purpose of this paper is two-fold – 1) provide an overview of a successful implementation of a Retrieval Augmented Generation-based model for analyzing semi-structured interviews, and more importantly, 2) enumerate pragmatic take-aways and learnings drawing from traditional qualitative research to help fellow industry practitioners in reconciling the methodological paradigms. We posit the second purpose to be valuable to the larger discussion within talent management research communities on how and where to integrate AI capabilities across different talent management workstreams.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Quantitative and Qualitative Paradigms</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Quantitative and qualitative research represent two fundamental paradigms or philosophical frameworks that guide research strategies, methods, analysis, and use of results <cite class="ltx_cite ltx_citemacro_citep">(Yilmaz, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib39" title="">2013</a>)</cite>. While both methodological approaches seek to rigorously study research problems, they are based on distinct assumptions and procedures adapted to investigating particular types of questions and drawing different conclusions. Quantitative research is based on the assumptions of positivism, the philosophical tradition premised on the application of natural science methods to the study of social reality and beyond <cite class="ltx_cite ltx_citemacro_citep">(Bryman, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib7" title="">2016</a>)</cite>. Quantitative researchers believe that objective facts and truths about human behavior and society can be measured and quantified numerically. Quantitative methods such as surveys, structured observations, and experiments aim to test hypotheses derived from theories by examining relationships between precisely measured variables statistically analyzed using large sample sizes <cite class="ltx_cite ltx_citemacro_citep">(Creswell and Creswell, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib11" title="">2017</a>)</cite>. These methods seek to minimize subjectivity and generalize findings to a population. In contrast, qualitative research aligns with interpretivist and constructivist philosophical traditions by embracing subjectivity and focused meaning-making by and with research participants <cite class="ltx_cite ltx_citemacro_citep">(Denzin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib12" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Qualitative researchers often use an inductive approach aimed at discovering and understanding processes, experiences, and worldviews by collecting non-numerical data through methods like in-depth interviews, ethnographic fieldwork, and document analysis. Findings derive from themes that emerge openly from the data rather than testing predetermined hypotheses. Samples tend to be small and purposely selected to illuminate a phenomenon in depth and detail. The aim is particularization rather than generalization, with a priority on ecological validity and multiple realities situated in time, place, culture, and context.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">While debates once positioned these paradigms in opposition, contemporary mixed methods research leverages the complementary strengths of quantitative and qualitative approaches <cite class="ltx_cite ltx_citemacro_citep">(Halcomb and Hickman, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib19" title="">2015</a>)</cite>. Mixed methods investigations integrate quantitative and qualitative data collection and analysis within a single program of inquiry by combining these approaches in creative ways to deepen understanding <cite class="ltx_cite ltx_citemacro_citep">(Creamer, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib9" title="">2017</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Creamer, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib10" title="">2018</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Greene, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib18" title="">2008</a>)</cite>. This reconciliation of methodological perspectives offers opportunities to generate more robust, contextualized insights to address complex research problems. The use of large language models (LLMs) as novice qualitative research assistants, as explored in this paper, can be considered an exercise in mixed methods research design.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Prior to LLMs, in previous work, Natural Language Processing based modeling of qualitative data from social science contexts, have also been used as "novice insight" augmented by the more expert contextualization provided by human researchers (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Bhaduri, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib4" title="">2018</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Bhaduri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib5" title="">2021</a>)</cite>). Popular traditional topic modeling techniques (e.g. Latent Dirichlet Allocation), however, suffer from several limitations (e.g. specifying number of clusters) when compared to existing deep learning-based methods. They also often fail to capture the contextual nuances and ambiguities inherent in natural language, as they rely heavily on predefined rules and patterns <cite class="ltx_cite ltx_citemacro_citep">(Devlin, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib13" title="">2018</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib33" title="">2019</a>)</cite>. This can make it challenging to handle the complexities and variations present in real-world text data, and may require domain-specific knowledge or fine-tuning to achieve acceptable performance <cite class="ltx_cite ltx_citemacro_citep">(Lee and Hsiang, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib22" title="">2019</a>)</cite>. Recent advancements in LLMs, such as BERT and GPT, have largely overcome these limitations by leveraging deep neural networks to learn rich, contextual representations from large amounts of text data <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib36" title="">2017</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Devlin, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib13" title="">2018</a>)</cite>. These powerful models can capture subtle semantic and pragmatic features of language, and demonstrate strong generalization capabilities through transfer learning <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib6" title="">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib33" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Further, in traditional qualitative research, thematic analysis is the process of gathering themes across topics from qualitative data, such as interview data, through iteratively analyzing the dataset for topics of interest <cite class="ltx_cite ltx_citemacro_citep">(Creamer, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib9" title="">2017</a>)</cite>. Inductive coding and deductive coding are two approaches to analyzing data from semi-structured interviews. Inductive coding involves starting with raw data and gradually developing codes and categories based on patterns and topics that emerge from the data as the researcher manually interacts with it <cite class="ltx_cite ltx_citemacro_citep">(Patton, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib30" title="">2014</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Strauss and Corbin, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib35" title="">1998</a>)</cite>. This approach is bottom-up, where the data drives the development of codes and theories <cite class="ltx_cite ltx_citemacro_citep">(Glaser, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib17" title="">1965</a>)</cite>. Deductive coding, on the other hand, involves starting with preconceived codes or theories and applying them to the data <cite class="ltx_cite ltx_citemacro_citep">(Pearse, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib31" title="">2019</a>)</cite>. This approach is top-down, where existing theories or frameworks guide the coding process <cite class="ltx_cite ltx_citemacro_citep">(Maxwell, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib27" title="">2018</a>)</cite>. Researchers in industry typically work backwards from research question of interest. Most of the research questions in industry driving qualitative data collection are also explanatory (i.e., tend to explain the quantitative findings such as low customer satisfaction, low product adoption numbers), rather than exploratory (i.e., ethnography of a community of interest or a phenomenon) and as a result deductive approaches are often more popular than inductive coding.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Ultimately, by augmenting traditional deep-dive qualitative analysis with the time and resource efficient pattern recognition and text processing capabilities of LLMs, researchers can integrate quantitative and qualitative techniques to enhance the speed, depth, and rigor of their investigations. This mental model of a novice-LLM approach holds promise for bridging the divide between positivist and interpretive paradigms, ultimately working towards a more comprehensive understanding of the phenomenon under study.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.F1.1" style="width:433.6pt;height:314.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-303.2pt,219.7pt) scale(0.416956214294893,0.416956214294893) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="1043" id="S2.F1.1.g1" src="extracted/5803374/2.png" width="1439"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">Comparison across prompting approaches</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We used an open-source dataset <cite class="ltx_cite ltx_citemacro_citep">(Paskevicius, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib29" title="">2018</a>)</cite> to demonstrate how an LLM prompted as a novice researcher can enhance traditional qualitative deductive thematic coding. This dataset was originally collected to explore educators’ experiences implementing open educational practices <cite class="ltx_cite ltx_citemacro_citep">(Paskevicius, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib29" title="">2018</a>)</cite>. The dataset contains eight transcripts each from hour-long interviews conducted with educators to understand how they are using openly accessible sources of knowledge and open-source tools. The original research involved a deep-dive qualitative analysis through using a phenomenological approach to extract topics manually from the dataset. We chose this open-source dataset for two reasons – 1) structural match to proprietary dataset, and 2) rich description and manually identified topics by an expert to serve as a gold standard to measure the efficacy of our LLM based approach. Semi-structured interviews provide critical insights through participant perspectives, making them foundational in various industry settings.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The semi-structured approach used to create this dataset is a close match to proprietary talent management data from our organization, where employees are interviewed on a particular phenomenon to get deeper understanding of their related sentiment, attitudes, and behaviors. Manually extracted topics serve as gold standard for benchmarking findings from our LLM-based approach. The paper <cite class="ltx_cite ltx_citemacro_citep">(Paskevicius, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib29" title="">2018</a>)</cite> describing the dataset explains the manual process establishing how each transcript was read twice: first, for a comprehensive analysis, and subsequently, to initiate a thematic exploration. Additional reviewing continued as codes and topics emerged and intersected among the interviews. A manual qualitative coding approach was applied at each iteration to reveal themes, following constant comparison methodology <cite class="ltx_cite ltx_citemacro_citep">(Glaser, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib17" title="">1965</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We posit that our approach, as demonstrated on this sample semi-structured interview dataset, can easily extend to multiple industry settings in talent management research where researchers conduct interviews and focus groups.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F2.1" style="width:433.6pt;height:137.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-647.5pt,205.7pt) scale(0.250836118334875,0.250836118334875) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="760" id="S3.F2.1.g1" src="extracted/5803374/1.png" width="2392"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Sample of the interview transcript</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Thematic Analysis Using LLMs</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In traditional, manual qualitative research, deductive thematic analysis process begins with the researcher first formulating the research questions. Then, upon collection of the data, such as interview transcripts, the researcher iterates manually through the transcripts to identify and extract themes or topics of interest. This labor-intensive process involves carefully reading through the data, taking notes, and organizing the topics iteratively into broader coherent themes that address the research questions. The researcher may go through multiple rounds of coding and analysis to refine the themes and ensure they comprehensively capture the key insights from the data. Our approach finds that LLMs can quickly uncover topics of interest from the dataset which can then be iterated upon to garner broader themes
of interest across topics. Thus, for our novice-LLM led approach, we leveraged the power of Large Language Models (LLMs) as a novice research assistant in the thematic analysis process. Specifically, we used the open-source framework called Langchain to create dynamic prompt templates, such as few-shot prompts and chain of thoughts, that guided the LLM in performing topic modeling and generating insights from the interview transcripts. We then opted to use Anthropic’s Claude2 model to execute these prompts and extract the relevant themes.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To initiate the analysis, we first selected a main research question and corresponding sub-questions from our dataset <cite class="ltx_cite ltx_citemacro_citep">(Paskevicius, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib29" title="">2018</a>)</cite>. We then fed these research questions, along with the interview transcripts, into the LLM-powered Langchain framework. The model was able to quickly identify and summarize the key topics, and iteratively, themes emerging from the data. This approach provided a quick yet relatively comprehensive analysis that would have taken a human researcher significant time and effort to reproduce manually.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Thematic analysis enhanced through Retrieval Augmented Generation (RAG)</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In our LLM based approaches, we experiment with four methods - zero-shot prompting, few-shot prompting, chain-of-thought reasoning, and Retrieval Augmented Generation based Question Answering. In zero-shot prompting we provide a single prompt to the model. In few-shot prompting, we provide a set of topics and anecdotes to the model as examples. In the chain of thought (COT) approach, we provide a set of instructions for the model to follow. Finally, for Retrieval Augmented Generation (RAG) we provide context and questions to the model, from which it extracts information.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Zero-shot prompts are simple instructions or tasks given to an LLM that have not been specifically trained on that task. It serves as a baseline because it demonstrates the model’s fundamental ability to understand and respond to prompts based solely on its pre-training <cite class="ltx_cite ltx_citemacro_citep">(Kong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib20" title="">2023</a>)</cite>. In few-shot prompting, a small set of examples illustrating the desired outcome are manually selected and provided to the LLM. These examples allow the model to understand the tasks at hand and generate similar results <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib6" title="">2020</a>)</cite>. Chain-of-thought prompting provides a set of intermediate steps to guide the LLM to mimic human-like reasoning. This significantly improves the capability of the LLM to understand complex reasoning and generate better topics <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib37" title="">[n. d.]</a>)</cite>. Retrieval-augmented generation (RAG) combines the capabilities of an LLM with a retrieval system to source and integrate additional information into its responses <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib24" title="">2020</a>)</cite>. This effort provides contextually richer and ultimately more accurate outputs. We do this by providing all the interview transcripts to the LLM as a custom knowledge base. Two considerations helped the RAG approach outperform the other approaches:</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Focused Analysis:</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">In our approach, LLM searches the knowledge base to find and retrieve parts of documents that are most relevant to the question in the query. This narrows the focus to the most relevant information and ensures attention to critical topics and nuances.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Context Dilution/Managing Information Overload:</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Using all transcripts as input in a single instance creates information overload scenarios, ultimately leading to dilution of important topics or nuances. If the dataset is too large or complex, LLM might lose track of what’s most relevant to specific query, leading hallucinations. Hallucinations or inaccuracies within this context refers to instances where the model generates information which is not grounded in input data. In our approach, the use of RAG mitigates some of the hallucination by anchoring LLM responses relevant information, and providing a form of contextual validation for the output.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.2">
<tr class="ltx_tr" id="S4.T1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1">Distillbert-base-uncased</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.2">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.4">F1-Score</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1">Chain of Thought</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2">67%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.3">62%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.4">64%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3">
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.1">Few Shot</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2">72%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3">67%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.4">70%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4">
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.1">Zero Shot</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.2">68%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.3">66%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4">67%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5">
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.1.1">RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.2.1">79%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.3.1">80%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.4.1">79%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.6.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.6.1.1">Bert-base-uncased</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.6.2">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.6.3">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.6.4">F1-Score</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.1">Chain of Thought</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.2">56%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.3">48%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.4">52%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8">
<td class="ltx_td ltx_align_center" id="S4.T1.2.8.1">Few Shot</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.8.2">64%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.8.3">56%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.8.4">60%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9">
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.1">Zero Shot</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.2">59%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.3">55%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.4">57%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.10">
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.1.1">RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.2.1">70%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.3.1">70%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.10.4.1">70%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.11.1.1">Roberta-large</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.2">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.3">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.4">F1-Score</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.12">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.12.1">Chain of Thought</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.12.2">89%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.12.3">85%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.12.4">87%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.13">
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.1">Few Shot</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.2">90%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.3">87%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.4">88%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.14">
<td class="ltx_td ltx_align_center" id="S4.T1.2.14.1">Zero Shot</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.14.2">89%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.14.3">86%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.14.4">88%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.15">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.15.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.15.1.1">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.15.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.15.2.1">92%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.15.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.15.3.1">91%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.15.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.15.4.1">91%</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Comparison of Results across the LLM Enhanced Thematic Analysis Strategies Employed</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Findings</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In the paper describing the dataset leveraged for this work, the authors collected and conducted a manual analysis <cite class="ltx_cite ltx_citemacro_citep">(Paskevicius, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib29" title="">2018</a>)</cite>. Their research led to identification of significant, recurring topics within the interviews. Our evaluation strategy uses these manually generated topics from the paper’s work as gold standard to compare against topics generated by the LLMs-based approach. We use Precision (Equation 1), Recall (Equation 2), and F1-score (Equation 3) to benchmark topics generated by our LLM-augmented qualitative research approach against the topics generated by the human researcher.</p>
</div>
<div class="ltx_para" id="S5.p2">
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{R}_{BERT}=\frac{1}{|x|}\sum_{x_{i}\in x}\max_{x_{j}\in\hat{x}}x_{i}^{T}%
\hat{x}_{j}" class="ltx_Math" display="block" id="S5.E1.m1.1"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.2" xref="S5.E1.m1.1.2.cmml"><msub id="S5.E1.m1.1.2.2" xref="S5.E1.m1.1.2.2.cmml"><mtext id="S5.E1.m1.1.2.2.2" xref="S5.E1.m1.1.2.2.2a.cmml">R</mtext><mrow id="S5.E1.m1.1.2.2.3" xref="S5.E1.m1.1.2.2.3.cmml"><mi id="S5.E1.m1.1.2.2.3.2" xref="S5.E1.m1.1.2.2.3.2.cmml">B</mi><mo id="S5.E1.m1.1.2.2.3.1" xref="S5.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.2.2.3.3" xref="S5.E1.m1.1.2.2.3.3.cmml">E</mi><mo id="S5.E1.m1.1.2.2.3.1a" xref="S5.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.2.2.3.4" xref="S5.E1.m1.1.2.2.3.4.cmml">R</mi><mo id="S5.E1.m1.1.2.2.3.1b" xref="S5.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.2.2.3.5" xref="S5.E1.m1.1.2.2.3.5.cmml">T</mi></mrow></msub><mo id="S5.E1.m1.1.2.1" xref="S5.E1.m1.1.2.1.cmml">=</mo><mrow id="S5.E1.m1.1.2.3" xref="S5.E1.m1.1.2.3.cmml"><mfrac id="S5.E1.m1.1.1" xref="S5.E1.m1.1.1.cmml"><mn id="S5.E1.m1.1.1.3" xref="S5.E1.m1.1.1.3.cmml">1</mn><mrow id="S5.E1.m1.1.1.1.3" xref="S5.E1.m1.1.1.1.2.cmml"><mo id="S5.E1.m1.1.1.1.3.1" stretchy="false" xref="S5.E1.m1.1.1.1.2.1.cmml">|</mo><mi id="S5.E1.m1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml">x</mi><mo id="S5.E1.m1.1.1.1.3.2" stretchy="false" xref="S5.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S5.E1.m1.1.2.3.1" xref="S5.E1.m1.1.2.3.1.cmml">⁢</mo><mrow id="S5.E1.m1.1.2.3.2" xref="S5.E1.m1.1.2.3.2.cmml"><munder id="S5.E1.m1.1.2.3.2.1" xref="S5.E1.m1.1.2.3.2.1.cmml"><mo id="S5.E1.m1.1.2.3.2.1.2" movablelimits="false" xref="S5.E1.m1.1.2.3.2.1.2.cmml">∑</mo><mrow id="S5.E1.m1.1.2.3.2.1.3" xref="S5.E1.m1.1.2.3.2.1.3.cmml"><msub id="S5.E1.m1.1.2.3.2.1.3.2" xref="S5.E1.m1.1.2.3.2.1.3.2.cmml"><mi id="S5.E1.m1.1.2.3.2.1.3.2.2" xref="S5.E1.m1.1.2.3.2.1.3.2.2.cmml">x</mi><mi id="S5.E1.m1.1.2.3.2.1.3.2.3" xref="S5.E1.m1.1.2.3.2.1.3.2.3.cmml">i</mi></msub><mo id="S5.E1.m1.1.2.3.2.1.3.1" xref="S5.E1.m1.1.2.3.2.1.3.1.cmml">∈</mo><mi id="S5.E1.m1.1.2.3.2.1.3.3" xref="S5.E1.m1.1.2.3.2.1.3.3.cmml">x</mi></mrow></munder><mrow id="S5.E1.m1.1.2.3.2.2" xref="S5.E1.m1.1.2.3.2.2.cmml"><munder id="S5.E1.m1.1.2.3.2.2.1" xref="S5.E1.m1.1.2.3.2.2.1.cmml"><mi id="S5.E1.m1.1.2.3.2.2.1.2" xref="S5.E1.m1.1.2.3.2.2.1.2.cmml">max</mi><mrow id="S5.E1.m1.1.2.3.2.2.1.3" xref="S5.E1.m1.1.2.3.2.2.1.3.cmml"><msub id="S5.E1.m1.1.2.3.2.2.1.3.2" xref="S5.E1.m1.1.2.3.2.2.1.3.2.cmml"><mi id="S5.E1.m1.1.2.3.2.2.1.3.2.2" xref="S5.E1.m1.1.2.3.2.2.1.3.2.2.cmml">x</mi><mi id="S5.E1.m1.1.2.3.2.2.1.3.2.3" xref="S5.E1.m1.1.2.3.2.2.1.3.2.3.cmml">j</mi></msub><mo id="S5.E1.m1.1.2.3.2.2.1.3.1" xref="S5.E1.m1.1.2.3.2.2.1.3.1.cmml">∈</mo><mover accent="true" id="S5.E1.m1.1.2.3.2.2.1.3.3" xref="S5.E1.m1.1.2.3.2.2.1.3.3.cmml"><mi id="S5.E1.m1.1.2.3.2.2.1.3.3.2" xref="S5.E1.m1.1.2.3.2.2.1.3.3.2.cmml">x</mi><mo id="S5.E1.m1.1.2.3.2.2.1.3.3.1" xref="S5.E1.m1.1.2.3.2.2.1.3.3.1.cmml">^</mo></mover></mrow></munder><mo id="S5.E1.m1.1.2.3.2.2a" lspace="0.167em" xref="S5.E1.m1.1.2.3.2.2.cmml">⁡</mo><mrow id="S5.E1.m1.1.2.3.2.2.2" xref="S5.E1.m1.1.2.3.2.2.2.cmml"><msubsup id="S5.E1.m1.1.2.3.2.2.2.2" xref="S5.E1.m1.1.2.3.2.2.2.2.cmml"><mi id="S5.E1.m1.1.2.3.2.2.2.2.2.2" xref="S5.E1.m1.1.2.3.2.2.2.2.2.2.cmml">x</mi><mi id="S5.E1.m1.1.2.3.2.2.2.2.2.3" xref="S5.E1.m1.1.2.3.2.2.2.2.2.3.cmml">i</mi><mi id="S5.E1.m1.1.2.3.2.2.2.2.3" xref="S5.E1.m1.1.2.3.2.2.2.2.3.cmml">T</mi></msubsup><mo id="S5.E1.m1.1.2.3.2.2.2.1" xref="S5.E1.m1.1.2.3.2.2.2.1.cmml">⁢</mo><msub id="S5.E1.m1.1.2.3.2.2.2.3" xref="S5.E1.m1.1.2.3.2.2.2.3.cmml"><mover accent="true" id="S5.E1.m1.1.2.3.2.2.2.3.2" xref="S5.E1.m1.1.2.3.2.2.2.3.2.cmml"><mi id="S5.E1.m1.1.2.3.2.2.2.3.2.2" xref="S5.E1.m1.1.2.3.2.2.2.3.2.2.cmml">x</mi><mo id="S5.E1.m1.1.2.3.2.2.2.3.2.1" xref="S5.E1.m1.1.2.3.2.2.2.3.2.1.cmml">^</mo></mover><mi id="S5.E1.m1.1.2.3.2.2.2.3.3" xref="S5.E1.m1.1.2.3.2.2.2.3.3.cmml">j</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.2.cmml" xref="S5.E1.m1.1.2"><eq id="S5.E1.m1.1.2.1.cmml" xref="S5.E1.m1.1.2.1"></eq><apply id="S5.E1.m1.1.2.2.cmml" xref="S5.E1.m1.1.2.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.2.1.cmml" xref="S5.E1.m1.1.2.2">subscript</csymbol><ci id="S5.E1.m1.1.2.2.2a.cmml" xref="S5.E1.m1.1.2.2.2"><mtext id="S5.E1.m1.1.2.2.2.cmml" xref="S5.E1.m1.1.2.2.2">R</mtext></ci><apply id="S5.E1.m1.1.2.2.3.cmml" xref="S5.E1.m1.1.2.2.3"><times id="S5.E1.m1.1.2.2.3.1.cmml" xref="S5.E1.m1.1.2.2.3.1"></times><ci id="S5.E1.m1.1.2.2.3.2.cmml" xref="S5.E1.m1.1.2.2.3.2">𝐵</ci><ci id="S5.E1.m1.1.2.2.3.3.cmml" xref="S5.E1.m1.1.2.2.3.3">𝐸</ci><ci id="S5.E1.m1.1.2.2.3.4.cmml" xref="S5.E1.m1.1.2.2.3.4">𝑅</ci><ci id="S5.E1.m1.1.2.2.3.5.cmml" xref="S5.E1.m1.1.2.2.3.5">𝑇</ci></apply></apply><apply id="S5.E1.m1.1.2.3.cmml" xref="S5.E1.m1.1.2.3"><times id="S5.E1.m1.1.2.3.1.cmml" xref="S5.E1.m1.1.2.3.1"></times><apply id="S5.E1.m1.1.1.cmml" xref="S5.E1.m1.1.1"><divide id="S5.E1.m1.1.1.2.cmml" xref="S5.E1.m1.1.1"></divide><cn id="S5.E1.m1.1.1.3.cmml" type="integer" xref="S5.E1.m1.1.1.3">1</cn><apply id="S5.E1.m1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.3"><abs id="S5.E1.m1.1.1.1.2.1.cmml" xref="S5.E1.m1.1.1.1.3.1"></abs><ci id="S5.E1.m1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1">𝑥</ci></apply></apply><apply id="S5.E1.m1.1.2.3.2.cmml" xref="S5.E1.m1.1.2.3.2"><apply id="S5.E1.m1.1.2.3.2.1.cmml" xref="S5.E1.m1.1.2.3.2.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.1.1.cmml" xref="S5.E1.m1.1.2.3.2.1">subscript</csymbol><sum id="S5.E1.m1.1.2.3.2.1.2.cmml" xref="S5.E1.m1.1.2.3.2.1.2"></sum><apply id="S5.E1.m1.1.2.3.2.1.3.cmml" xref="S5.E1.m1.1.2.3.2.1.3"><in id="S5.E1.m1.1.2.3.2.1.3.1.cmml" xref="S5.E1.m1.1.2.3.2.1.3.1"></in><apply id="S5.E1.m1.1.2.3.2.1.3.2.cmml" xref="S5.E1.m1.1.2.3.2.1.3.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.1.3.2.1.cmml" xref="S5.E1.m1.1.2.3.2.1.3.2">subscript</csymbol><ci id="S5.E1.m1.1.2.3.2.1.3.2.2.cmml" xref="S5.E1.m1.1.2.3.2.1.3.2.2">𝑥</ci><ci id="S5.E1.m1.1.2.3.2.1.3.2.3.cmml" xref="S5.E1.m1.1.2.3.2.1.3.2.3">𝑖</ci></apply><ci id="S5.E1.m1.1.2.3.2.1.3.3.cmml" xref="S5.E1.m1.1.2.3.2.1.3.3">𝑥</ci></apply></apply><apply id="S5.E1.m1.1.2.3.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2"><apply id="S5.E1.m1.1.2.3.2.2.1.cmml" xref="S5.E1.m1.1.2.3.2.2.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.2.1.1.cmml" xref="S5.E1.m1.1.2.3.2.2.1">subscript</csymbol><max id="S5.E1.m1.1.2.3.2.2.1.2.cmml" xref="S5.E1.m1.1.2.3.2.2.1.2"></max><apply id="S5.E1.m1.1.2.3.2.2.1.3.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3"><in id="S5.E1.m1.1.2.3.2.2.1.3.1.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.1"></in><apply id="S5.E1.m1.1.2.3.2.2.1.3.2.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.2.1.3.2.1.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.2">subscript</csymbol><ci id="S5.E1.m1.1.2.3.2.2.1.3.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.2.2">𝑥</ci><ci id="S5.E1.m1.1.2.3.2.2.1.3.2.3.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.2.3">𝑗</ci></apply><apply id="S5.E1.m1.1.2.3.2.2.1.3.3.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.3"><ci id="S5.E1.m1.1.2.3.2.2.1.3.3.1.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.3.1">^</ci><ci id="S5.E1.m1.1.2.3.2.2.1.3.3.2.cmml" xref="S5.E1.m1.1.2.3.2.2.1.3.3.2">𝑥</ci></apply></apply></apply><apply id="S5.E1.m1.1.2.3.2.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2.2"><times id="S5.E1.m1.1.2.3.2.2.2.1.cmml" xref="S5.E1.m1.1.2.3.2.2.2.1"></times><apply id="S5.E1.m1.1.2.3.2.2.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.2.2.2.1.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2">superscript</csymbol><apply id="S5.E1.m1.1.2.3.2.2.2.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.2.2.2.2.1.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2">subscript</csymbol><ci id="S5.E1.m1.1.2.3.2.2.2.2.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2.2.2">𝑥</ci><ci id="S5.E1.m1.1.2.3.2.2.2.2.2.3.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2.2.3">𝑖</ci></apply><ci id="S5.E1.m1.1.2.3.2.2.2.2.3.cmml" xref="S5.E1.m1.1.2.3.2.2.2.2.3">𝑇</ci></apply><apply id="S5.E1.m1.1.2.3.2.2.2.3.cmml" xref="S5.E1.m1.1.2.3.2.2.2.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.3.2.2.2.3.1.cmml" xref="S5.E1.m1.1.2.3.2.2.2.3">subscript</csymbol><apply id="S5.E1.m1.1.2.3.2.2.2.3.2.cmml" xref="S5.E1.m1.1.2.3.2.2.2.3.2"><ci id="S5.E1.m1.1.2.3.2.2.2.3.2.1.cmml" xref="S5.E1.m1.1.2.3.2.2.2.3.2.1">^</ci><ci id="S5.E1.m1.1.2.3.2.2.2.3.2.2.cmml" xref="S5.E1.m1.1.2.3.2.2.2.3.2.2">𝑥</ci></apply><ci id="S5.E1.m1.1.2.3.2.2.2.3.3.cmml" xref="S5.E1.m1.1.2.3.2.2.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">\text{R}_{BERT}=\frac{1}{|x|}\sum_{x_{i}\in x}\max_{x_{j}\in\hat{x}}x_{i}^{T}%
\hat{x}_{j}</annotation><annotation encoding="application/x-llamapun" id="S5.E1.m1.1d">R start_POSTSUBSCRIPT italic_B italic_E italic_R italic_T end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | italic_x | end_ARG ∑ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_x end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ over^ start_ARG italic_x end_ARG end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.p3">
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{P}_{BERT}=\frac{1}{|\hat{x}|}\sum_{\hat{x}_{i}\in\hat{x}}\max_{x_{j}\in x%
}\hat{x}_{i}^{T}x_{j}" class="ltx_Math" display="block" id="S5.E2.m1.1"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.2" xref="S5.E2.m1.1.2.cmml"><msub id="S5.E2.m1.1.2.2" xref="S5.E2.m1.1.2.2.cmml"><mtext id="S5.E2.m1.1.2.2.2" xref="S5.E2.m1.1.2.2.2a.cmml">P</mtext><mrow id="S5.E2.m1.1.2.2.3" xref="S5.E2.m1.1.2.2.3.cmml"><mi id="S5.E2.m1.1.2.2.3.2" xref="S5.E2.m1.1.2.2.3.2.cmml">B</mi><mo id="S5.E2.m1.1.2.2.3.1" xref="S5.E2.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.E2.m1.1.2.2.3.3" xref="S5.E2.m1.1.2.2.3.3.cmml">E</mi><mo id="S5.E2.m1.1.2.2.3.1a" xref="S5.E2.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.E2.m1.1.2.2.3.4" xref="S5.E2.m1.1.2.2.3.4.cmml">R</mi><mo id="S5.E2.m1.1.2.2.3.1b" xref="S5.E2.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.E2.m1.1.2.2.3.5" xref="S5.E2.m1.1.2.2.3.5.cmml">T</mi></mrow></msub><mo id="S5.E2.m1.1.2.1" xref="S5.E2.m1.1.2.1.cmml">=</mo><mrow id="S5.E2.m1.1.2.3" xref="S5.E2.m1.1.2.3.cmml"><mfrac id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><mn id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3.cmml">1</mn><mrow id="S5.E2.m1.1.1.1.3" xref="S5.E2.m1.1.1.1.2.cmml"><mo id="S5.E2.m1.1.1.1.3.1" stretchy="false" xref="S5.E2.m1.1.1.1.2.1.cmml">|</mo><mover accent="true" id="S5.E2.m1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.2.cmml">x</mi><mo id="S5.E2.m1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.cmml">^</mo></mover><mo id="S5.E2.m1.1.1.1.3.2" stretchy="false" xref="S5.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S5.E2.m1.1.2.3.1" xref="S5.E2.m1.1.2.3.1.cmml">⁢</mo><mrow id="S5.E2.m1.1.2.3.2" xref="S5.E2.m1.1.2.3.2.cmml"><munder id="S5.E2.m1.1.2.3.2.1" xref="S5.E2.m1.1.2.3.2.1.cmml"><mo id="S5.E2.m1.1.2.3.2.1.2" movablelimits="false" xref="S5.E2.m1.1.2.3.2.1.2.cmml">∑</mo><mrow id="S5.E2.m1.1.2.3.2.1.3" xref="S5.E2.m1.1.2.3.2.1.3.cmml"><msub id="S5.E2.m1.1.2.3.2.1.3.2" xref="S5.E2.m1.1.2.3.2.1.3.2.cmml"><mover accent="true" id="S5.E2.m1.1.2.3.2.1.3.2.2" xref="S5.E2.m1.1.2.3.2.1.3.2.2.cmml"><mi id="S5.E2.m1.1.2.3.2.1.3.2.2.2" xref="S5.E2.m1.1.2.3.2.1.3.2.2.2.cmml">x</mi><mo id="S5.E2.m1.1.2.3.2.1.3.2.2.1" xref="S5.E2.m1.1.2.3.2.1.3.2.2.1.cmml">^</mo></mover><mi id="S5.E2.m1.1.2.3.2.1.3.2.3" xref="S5.E2.m1.1.2.3.2.1.3.2.3.cmml">i</mi></msub><mo id="S5.E2.m1.1.2.3.2.1.3.1" xref="S5.E2.m1.1.2.3.2.1.3.1.cmml">∈</mo><mover accent="true" id="S5.E2.m1.1.2.3.2.1.3.3" xref="S5.E2.m1.1.2.3.2.1.3.3.cmml"><mi id="S5.E2.m1.1.2.3.2.1.3.3.2" xref="S5.E2.m1.1.2.3.2.1.3.3.2.cmml">x</mi><mo id="S5.E2.m1.1.2.3.2.1.3.3.1" xref="S5.E2.m1.1.2.3.2.1.3.3.1.cmml">^</mo></mover></mrow></munder><mrow id="S5.E2.m1.1.2.3.2.2" xref="S5.E2.m1.1.2.3.2.2.cmml"><munder id="S5.E2.m1.1.2.3.2.2.1" xref="S5.E2.m1.1.2.3.2.2.1.cmml"><mi id="S5.E2.m1.1.2.3.2.2.1.2" xref="S5.E2.m1.1.2.3.2.2.1.2.cmml">max</mi><mrow id="S5.E2.m1.1.2.3.2.2.1.3" xref="S5.E2.m1.1.2.3.2.2.1.3.cmml"><msub id="S5.E2.m1.1.2.3.2.2.1.3.2" xref="S5.E2.m1.1.2.3.2.2.1.3.2.cmml"><mi id="S5.E2.m1.1.2.3.2.2.1.3.2.2" xref="S5.E2.m1.1.2.3.2.2.1.3.2.2.cmml">x</mi><mi id="S5.E2.m1.1.2.3.2.2.1.3.2.3" xref="S5.E2.m1.1.2.3.2.2.1.3.2.3.cmml">j</mi></msub><mo id="S5.E2.m1.1.2.3.2.2.1.3.1" xref="S5.E2.m1.1.2.3.2.2.1.3.1.cmml">∈</mo><mi id="S5.E2.m1.1.2.3.2.2.1.3.3" xref="S5.E2.m1.1.2.3.2.2.1.3.3.cmml">x</mi></mrow></munder><mo id="S5.E2.m1.1.2.3.2.2a" lspace="0.167em" xref="S5.E2.m1.1.2.3.2.2.cmml">⁡</mo><mrow id="S5.E2.m1.1.2.3.2.2.2" xref="S5.E2.m1.1.2.3.2.2.2.cmml"><msubsup id="S5.E2.m1.1.2.3.2.2.2.2" xref="S5.E2.m1.1.2.3.2.2.2.2.cmml"><mover accent="true" id="S5.E2.m1.1.2.3.2.2.2.2.2.2" xref="S5.E2.m1.1.2.3.2.2.2.2.2.2.cmml"><mi id="S5.E2.m1.1.2.3.2.2.2.2.2.2.2" xref="S5.E2.m1.1.2.3.2.2.2.2.2.2.2.cmml">x</mi><mo id="S5.E2.m1.1.2.3.2.2.2.2.2.2.1" xref="S5.E2.m1.1.2.3.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S5.E2.m1.1.2.3.2.2.2.2.2.3" xref="S5.E2.m1.1.2.3.2.2.2.2.2.3.cmml">i</mi><mi id="S5.E2.m1.1.2.3.2.2.2.2.3" xref="S5.E2.m1.1.2.3.2.2.2.2.3.cmml">T</mi></msubsup><mo id="S5.E2.m1.1.2.3.2.2.2.1" xref="S5.E2.m1.1.2.3.2.2.2.1.cmml">⁢</mo><msub id="S5.E2.m1.1.2.3.2.2.2.3" xref="S5.E2.m1.1.2.3.2.2.2.3.cmml"><mi id="S5.E2.m1.1.2.3.2.2.2.3.2" xref="S5.E2.m1.1.2.3.2.2.2.3.2.cmml">x</mi><mi id="S5.E2.m1.1.2.3.2.2.2.3.3" xref="S5.E2.m1.1.2.3.2.2.2.3.3.cmml">j</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.2.cmml" xref="S5.E2.m1.1.2"><eq id="S5.E2.m1.1.2.1.cmml" xref="S5.E2.m1.1.2.1"></eq><apply id="S5.E2.m1.1.2.2.cmml" xref="S5.E2.m1.1.2.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.2.1.cmml" xref="S5.E2.m1.1.2.2">subscript</csymbol><ci id="S5.E2.m1.1.2.2.2a.cmml" xref="S5.E2.m1.1.2.2.2"><mtext id="S5.E2.m1.1.2.2.2.cmml" xref="S5.E2.m1.1.2.2.2">P</mtext></ci><apply id="S5.E2.m1.1.2.2.3.cmml" xref="S5.E2.m1.1.2.2.3"><times id="S5.E2.m1.1.2.2.3.1.cmml" xref="S5.E2.m1.1.2.2.3.1"></times><ci id="S5.E2.m1.1.2.2.3.2.cmml" xref="S5.E2.m1.1.2.2.3.2">𝐵</ci><ci id="S5.E2.m1.1.2.2.3.3.cmml" xref="S5.E2.m1.1.2.2.3.3">𝐸</ci><ci id="S5.E2.m1.1.2.2.3.4.cmml" xref="S5.E2.m1.1.2.2.3.4">𝑅</ci><ci id="S5.E2.m1.1.2.2.3.5.cmml" xref="S5.E2.m1.1.2.2.3.5">𝑇</ci></apply></apply><apply id="S5.E2.m1.1.2.3.cmml" xref="S5.E2.m1.1.2.3"><times id="S5.E2.m1.1.2.3.1.cmml" xref="S5.E2.m1.1.2.3.1"></times><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><divide id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1"></divide><cn id="S5.E2.m1.1.1.3.cmml" type="integer" xref="S5.E2.m1.1.1.3">1</cn><apply id="S5.E2.m1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.3"><abs id="S5.E2.m1.1.1.1.2.1.cmml" xref="S5.E2.m1.1.1.1.3.1"></abs><apply id="S5.E2.m1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1"><ci id="S5.E2.m1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1">^</ci><ci id="S5.E2.m1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.2">𝑥</ci></apply></apply></apply><apply id="S5.E2.m1.1.2.3.2.cmml" xref="S5.E2.m1.1.2.3.2"><apply id="S5.E2.m1.1.2.3.2.1.cmml" xref="S5.E2.m1.1.2.3.2.1"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.1.1.cmml" xref="S5.E2.m1.1.2.3.2.1">subscript</csymbol><sum id="S5.E2.m1.1.2.3.2.1.2.cmml" xref="S5.E2.m1.1.2.3.2.1.2"></sum><apply id="S5.E2.m1.1.2.3.2.1.3.cmml" xref="S5.E2.m1.1.2.3.2.1.3"><in id="S5.E2.m1.1.2.3.2.1.3.1.cmml" xref="S5.E2.m1.1.2.3.2.1.3.1"></in><apply id="S5.E2.m1.1.2.3.2.1.3.2.cmml" xref="S5.E2.m1.1.2.3.2.1.3.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.1.3.2.1.cmml" xref="S5.E2.m1.1.2.3.2.1.3.2">subscript</csymbol><apply id="S5.E2.m1.1.2.3.2.1.3.2.2.cmml" xref="S5.E2.m1.1.2.3.2.1.3.2.2"><ci id="S5.E2.m1.1.2.3.2.1.3.2.2.1.cmml" xref="S5.E2.m1.1.2.3.2.1.3.2.2.1">^</ci><ci id="S5.E2.m1.1.2.3.2.1.3.2.2.2.cmml" xref="S5.E2.m1.1.2.3.2.1.3.2.2.2">𝑥</ci></apply><ci id="S5.E2.m1.1.2.3.2.1.3.2.3.cmml" xref="S5.E2.m1.1.2.3.2.1.3.2.3">𝑖</ci></apply><apply id="S5.E2.m1.1.2.3.2.1.3.3.cmml" xref="S5.E2.m1.1.2.3.2.1.3.3"><ci id="S5.E2.m1.1.2.3.2.1.3.3.1.cmml" xref="S5.E2.m1.1.2.3.2.1.3.3.1">^</ci><ci id="S5.E2.m1.1.2.3.2.1.3.3.2.cmml" xref="S5.E2.m1.1.2.3.2.1.3.3.2">𝑥</ci></apply></apply></apply><apply id="S5.E2.m1.1.2.3.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2"><apply id="S5.E2.m1.1.2.3.2.2.1.cmml" xref="S5.E2.m1.1.2.3.2.2.1"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.2.1.1.cmml" xref="S5.E2.m1.1.2.3.2.2.1">subscript</csymbol><max id="S5.E2.m1.1.2.3.2.2.1.2.cmml" xref="S5.E2.m1.1.2.3.2.2.1.2"></max><apply id="S5.E2.m1.1.2.3.2.2.1.3.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3"><in id="S5.E2.m1.1.2.3.2.2.1.3.1.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3.1"></in><apply id="S5.E2.m1.1.2.3.2.2.1.3.2.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.2.1.3.2.1.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3.2">subscript</csymbol><ci id="S5.E2.m1.1.2.3.2.2.1.3.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3.2.2">𝑥</ci><ci id="S5.E2.m1.1.2.3.2.2.1.3.2.3.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3.2.3">𝑗</ci></apply><ci id="S5.E2.m1.1.2.3.2.2.1.3.3.cmml" xref="S5.E2.m1.1.2.3.2.2.1.3.3">𝑥</ci></apply></apply><apply id="S5.E2.m1.1.2.3.2.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2.2"><times id="S5.E2.m1.1.2.3.2.2.2.1.cmml" xref="S5.E2.m1.1.2.3.2.2.2.1"></times><apply id="S5.E2.m1.1.2.3.2.2.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.2.2.2.1.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2">superscript</csymbol><apply id="S5.E2.m1.1.2.3.2.2.2.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.2.2.2.2.1.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2">subscript</csymbol><apply id="S5.E2.m1.1.2.3.2.2.2.2.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2.2.2"><ci id="S5.E2.m1.1.2.3.2.2.2.2.2.2.1.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2.2.2.1">^</ci><ci id="S5.E2.m1.1.2.3.2.2.2.2.2.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2.2.2.2">𝑥</ci></apply><ci id="S5.E2.m1.1.2.3.2.2.2.2.2.3.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2.2.3">𝑖</ci></apply><ci id="S5.E2.m1.1.2.3.2.2.2.2.3.cmml" xref="S5.E2.m1.1.2.3.2.2.2.2.3">𝑇</ci></apply><apply id="S5.E2.m1.1.2.3.2.2.2.3.cmml" xref="S5.E2.m1.1.2.3.2.2.2.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.2.2.3.1.cmml" xref="S5.E2.m1.1.2.3.2.2.2.3">subscript</csymbol><ci id="S5.E2.m1.1.2.3.2.2.2.3.2.cmml" xref="S5.E2.m1.1.2.3.2.2.2.3.2">𝑥</ci><ci id="S5.E2.m1.1.2.3.2.2.2.3.3.cmml" xref="S5.E2.m1.1.2.3.2.2.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">\text{P}_{BERT}=\frac{1}{|\hat{x}|}\sum_{\hat{x}_{i}\in\hat{x}}\max_{x_{j}\in x%
}\hat{x}_{i}^{T}x_{j}</annotation><annotation encoding="application/x-llamapun" id="S5.E2.m1.1d">P start_POSTSUBSCRIPT italic_B italic_E italic_R italic_T end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | over^ start_ARG italic_x end_ARG | end_ARG ∑ start_POSTSUBSCRIPT over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ over^ start_ARG italic_x end_ARG end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ italic_x end_POSTSUBSCRIPT over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.p4">
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{F}_{BERT}=2\cdot\frac{P_{\text{BERT}}\cdot R_{\text{BERT}}}{P_{\text{%
BERT}}+R_{\text{BERT}}}" class="ltx_Math" display="block" id="S5.E3.m1.1"><semantics id="S5.E3.m1.1a"><mrow id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml"><msub id="S5.E3.m1.1.1.2" xref="S5.E3.m1.1.1.2.cmml"><mtext id="S5.E3.m1.1.1.2.2" xref="S5.E3.m1.1.1.2.2a.cmml">F</mtext><mrow id="S5.E3.m1.1.1.2.3" xref="S5.E3.m1.1.1.2.3.cmml"><mi id="S5.E3.m1.1.1.2.3.2" xref="S5.E3.m1.1.1.2.3.2.cmml">B</mi><mo id="S5.E3.m1.1.1.2.3.1" xref="S5.E3.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.E3.m1.1.1.2.3.3" xref="S5.E3.m1.1.1.2.3.3.cmml">E</mi><mo id="S5.E3.m1.1.1.2.3.1a" xref="S5.E3.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.E3.m1.1.1.2.3.4" xref="S5.E3.m1.1.1.2.3.4.cmml">R</mi><mo id="S5.E3.m1.1.1.2.3.1b" xref="S5.E3.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.E3.m1.1.1.2.3.5" xref="S5.E3.m1.1.1.2.3.5.cmml">T</mi></mrow></msub><mo id="S5.E3.m1.1.1.1" xref="S5.E3.m1.1.1.1.cmml">=</mo><mrow id="S5.E3.m1.1.1.3" xref="S5.E3.m1.1.1.3.cmml"><mn id="S5.E3.m1.1.1.3.2" xref="S5.E3.m1.1.1.3.2.cmml">2</mn><mo id="S5.E3.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S5.E3.m1.1.1.3.1.cmml">⋅</mo><mfrac id="S5.E3.m1.1.1.3.3" xref="S5.E3.m1.1.1.3.3.cmml"><mrow id="S5.E3.m1.1.1.3.3.2" xref="S5.E3.m1.1.1.3.3.2.cmml"><msub id="S5.E3.m1.1.1.3.3.2.2" xref="S5.E3.m1.1.1.3.3.2.2.cmml"><mi id="S5.E3.m1.1.1.3.3.2.2.2" xref="S5.E3.m1.1.1.3.3.2.2.2.cmml">P</mi><mtext id="S5.E3.m1.1.1.3.3.2.2.3" xref="S5.E3.m1.1.1.3.3.2.2.3a.cmml">BERT</mtext></msub><mo id="S5.E3.m1.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S5.E3.m1.1.1.3.3.2.1.cmml">⋅</mo><msub id="S5.E3.m1.1.1.3.3.2.3" xref="S5.E3.m1.1.1.3.3.2.3.cmml"><mi id="S5.E3.m1.1.1.3.3.2.3.2" xref="S5.E3.m1.1.1.3.3.2.3.2.cmml">R</mi><mtext id="S5.E3.m1.1.1.3.3.2.3.3" xref="S5.E3.m1.1.1.3.3.2.3.3a.cmml">BERT</mtext></msub></mrow><mrow id="S5.E3.m1.1.1.3.3.3" xref="S5.E3.m1.1.1.3.3.3.cmml"><msub id="S5.E3.m1.1.1.3.3.3.2" xref="S5.E3.m1.1.1.3.3.3.2.cmml"><mi id="S5.E3.m1.1.1.3.3.3.2.2" xref="S5.E3.m1.1.1.3.3.3.2.2.cmml">P</mi><mtext id="S5.E3.m1.1.1.3.3.3.2.3" xref="S5.E3.m1.1.1.3.3.3.2.3a.cmml">BERT</mtext></msub><mo id="S5.E3.m1.1.1.3.3.3.1" xref="S5.E3.m1.1.1.3.3.3.1.cmml">+</mo><msub id="S5.E3.m1.1.1.3.3.3.3" xref="S5.E3.m1.1.1.3.3.3.3.cmml"><mi id="S5.E3.m1.1.1.3.3.3.3.2" xref="S5.E3.m1.1.1.3.3.3.3.2.cmml">R</mi><mtext id="S5.E3.m1.1.1.3.3.3.3.3" xref="S5.E3.m1.1.1.3.3.3.3.3a.cmml">BERT</mtext></msub></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.1b"><apply id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"><eq id="S5.E3.m1.1.1.1.cmml" xref="S5.E3.m1.1.1.1"></eq><apply id="S5.E3.m1.1.1.2.cmml" xref="S5.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.2">subscript</csymbol><ci id="S5.E3.m1.1.1.2.2a.cmml" xref="S5.E3.m1.1.1.2.2"><mtext id="S5.E3.m1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.2.2">F</mtext></ci><apply id="S5.E3.m1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.2.3"><times id="S5.E3.m1.1.1.2.3.1.cmml" xref="S5.E3.m1.1.1.2.3.1"></times><ci id="S5.E3.m1.1.1.2.3.2.cmml" xref="S5.E3.m1.1.1.2.3.2">𝐵</ci><ci id="S5.E3.m1.1.1.2.3.3.cmml" xref="S5.E3.m1.1.1.2.3.3">𝐸</ci><ci id="S5.E3.m1.1.1.2.3.4.cmml" xref="S5.E3.m1.1.1.2.3.4">𝑅</ci><ci id="S5.E3.m1.1.1.2.3.5.cmml" xref="S5.E3.m1.1.1.2.3.5">𝑇</ci></apply></apply><apply id="S5.E3.m1.1.1.3.cmml" xref="S5.E3.m1.1.1.3"><ci id="S5.E3.m1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.3.1">⋅</ci><cn id="S5.E3.m1.1.1.3.2.cmml" type="integer" xref="S5.E3.m1.1.1.3.2">2</cn><apply id="S5.E3.m1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.3.3"><divide id="S5.E3.m1.1.1.3.3.1.cmml" xref="S5.E3.m1.1.1.3.3"></divide><apply id="S5.E3.m1.1.1.3.3.2.cmml" xref="S5.E3.m1.1.1.3.3.2"><ci id="S5.E3.m1.1.1.3.3.2.1.cmml" xref="S5.E3.m1.1.1.3.3.2.1">⋅</ci><apply id="S5.E3.m1.1.1.3.3.2.2.cmml" xref="S5.E3.m1.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.3.3.2.2.1.cmml" xref="S5.E3.m1.1.1.3.3.2.2">subscript</csymbol><ci id="S5.E3.m1.1.1.3.3.2.2.2.cmml" xref="S5.E3.m1.1.1.3.3.2.2.2">𝑃</ci><ci id="S5.E3.m1.1.1.3.3.2.2.3a.cmml" xref="S5.E3.m1.1.1.3.3.2.2.3"><mtext id="S5.E3.m1.1.1.3.3.2.2.3.cmml" mathsize="70%" xref="S5.E3.m1.1.1.3.3.2.2.3">BERT</mtext></ci></apply><apply id="S5.E3.m1.1.1.3.3.2.3.cmml" xref="S5.E3.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.3.3.2.3.1.cmml" xref="S5.E3.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S5.E3.m1.1.1.3.3.2.3.2.cmml" xref="S5.E3.m1.1.1.3.3.2.3.2">𝑅</ci><ci id="S5.E3.m1.1.1.3.3.2.3.3a.cmml" xref="S5.E3.m1.1.1.3.3.2.3.3"><mtext id="S5.E3.m1.1.1.3.3.2.3.3.cmml" mathsize="70%" xref="S5.E3.m1.1.1.3.3.2.3.3">BERT</mtext></ci></apply></apply><apply id="S5.E3.m1.1.1.3.3.3.cmml" xref="S5.E3.m1.1.1.3.3.3"><plus id="S5.E3.m1.1.1.3.3.3.1.cmml" xref="S5.E3.m1.1.1.3.3.3.1"></plus><apply id="S5.E3.m1.1.1.3.3.3.2.cmml" xref="S5.E3.m1.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.3.3.3.2.1.cmml" xref="S5.E3.m1.1.1.3.3.3.2">subscript</csymbol><ci id="S5.E3.m1.1.1.3.3.3.2.2.cmml" xref="S5.E3.m1.1.1.3.3.3.2.2">𝑃</ci><ci id="S5.E3.m1.1.1.3.3.3.2.3a.cmml" xref="S5.E3.m1.1.1.3.3.3.2.3"><mtext id="S5.E3.m1.1.1.3.3.3.2.3.cmml" mathsize="70%" xref="S5.E3.m1.1.1.3.3.3.2.3">BERT</mtext></ci></apply><apply id="S5.E3.m1.1.1.3.3.3.3.cmml" xref="S5.E3.m1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.3.3.3.3.1.cmml" xref="S5.E3.m1.1.1.3.3.3.3">subscript</csymbol><ci id="S5.E3.m1.1.1.3.3.3.3.2.cmml" xref="S5.E3.m1.1.1.3.3.3.3.2">𝑅</ci><ci id="S5.E3.m1.1.1.3.3.3.3.3a.cmml" xref="S5.E3.m1.1.1.3.3.3.3.3"><mtext id="S5.E3.m1.1.1.3.3.3.3.3.cmml" mathsize="70%" xref="S5.E3.m1.1.1.3.3.3.3.3">BERT</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.1c">\text{F}_{BERT}=2\cdot\frac{P_{\text{BERT}}\cdot R_{\text{BERT}}}{P_{\text{%
BERT}}+R_{\text{BERT}}}</annotation><annotation encoding="application/x-llamapun" id="S5.E3.m1.1d">F start_POSTSUBSCRIPT italic_B italic_E italic_R italic_T end_POSTSUBSCRIPT = 2 ⋅ divide start_ARG italic_P start_POSTSUBSCRIPT BERT end_POSTSUBSCRIPT ⋅ italic_R start_POSTSUBSCRIPT BERT end_POSTSUBSCRIPT end_ARG start_ARG italic_P start_POSTSUBSCRIPT BERT end_POSTSUBSCRIPT + italic_R start_POSTSUBSCRIPT BERT end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.p5">
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Cosine Similarity}=\frac{x_{i}^{T}\hat{x}_{j}}{\|x_{i}\|\|\hat{x}_{j}\|}" class="ltx_Math" display="block" id="S5.E4.m1.2"><semantics id="S5.E4.m1.2a"><mrow id="S5.E4.m1.2.3" xref="S5.E4.m1.2.3.cmml"><mtext id="S5.E4.m1.2.3.2" xref="S5.E4.m1.2.3.2a.cmml">Cosine Similarity</mtext><mo id="S5.E4.m1.2.3.1" xref="S5.E4.m1.2.3.1.cmml">=</mo><mfrac id="S5.E4.m1.2.2" xref="S5.E4.m1.2.2.cmml"><mrow id="S5.E4.m1.2.2.4" xref="S5.E4.m1.2.2.4.cmml"><msubsup id="S5.E4.m1.2.2.4.2" xref="S5.E4.m1.2.2.4.2.cmml"><mi id="S5.E4.m1.2.2.4.2.2.2" xref="S5.E4.m1.2.2.4.2.2.2.cmml">x</mi><mi id="S5.E4.m1.2.2.4.2.2.3" xref="S5.E4.m1.2.2.4.2.2.3.cmml">i</mi><mi id="S5.E4.m1.2.2.4.2.3" xref="S5.E4.m1.2.2.4.2.3.cmml">T</mi></msubsup><mo id="S5.E4.m1.2.2.4.1" xref="S5.E4.m1.2.2.4.1.cmml">⁢</mo><msub id="S5.E4.m1.2.2.4.3" xref="S5.E4.m1.2.2.4.3.cmml"><mover accent="true" id="S5.E4.m1.2.2.4.3.2" xref="S5.E4.m1.2.2.4.3.2.cmml"><mi id="S5.E4.m1.2.2.4.3.2.2" xref="S5.E4.m1.2.2.4.3.2.2.cmml">x</mi><mo id="S5.E4.m1.2.2.4.3.2.1" xref="S5.E4.m1.2.2.4.3.2.1.cmml">^</mo></mover><mi id="S5.E4.m1.2.2.4.3.3" xref="S5.E4.m1.2.2.4.3.3.cmml">j</mi></msub></mrow><mrow id="S5.E4.m1.2.2.2" xref="S5.E4.m1.2.2.2.cmml"><mrow id="S5.E4.m1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.2.cmml"><mo id="S5.E4.m1.1.1.1.1.1.2" stretchy="false" xref="S5.E4.m1.1.1.1.1.2.1.cmml">‖</mo><msub id="S5.E4.m1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.2" xref="S5.E4.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S5.E4.m1.1.1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.E4.m1.1.1.1.1.1.3" stretchy="false" xref="S5.E4.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mo id="S5.E4.m1.2.2.2.3" xref="S5.E4.m1.2.2.2.3.cmml">⁢</mo><mrow id="S5.E4.m1.2.2.2.2.1" xref="S5.E4.m1.2.2.2.2.2.cmml"><mo id="S5.E4.m1.2.2.2.2.1.2" stretchy="false" xref="S5.E4.m1.2.2.2.2.2.1.cmml">‖</mo><msub id="S5.E4.m1.2.2.2.2.1.1" xref="S5.E4.m1.2.2.2.2.1.1.cmml"><mover accent="true" id="S5.E4.m1.2.2.2.2.1.1.2" xref="S5.E4.m1.2.2.2.2.1.1.2.cmml"><mi id="S5.E4.m1.2.2.2.2.1.1.2.2" xref="S5.E4.m1.2.2.2.2.1.1.2.2.cmml">x</mi><mo id="S5.E4.m1.2.2.2.2.1.1.2.1" xref="S5.E4.m1.2.2.2.2.1.1.2.1.cmml">^</mo></mover><mi id="S5.E4.m1.2.2.2.2.1.1.3" xref="S5.E4.m1.2.2.2.2.1.1.3.cmml">j</mi></msub><mo id="S5.E4.m1.2.2.2.2.1.3" stretchy="false" xref="S5.E4.m1.2.2.2.2.2.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.2b"><apply id="S5.E4.m1.2.3.cmml" xref="S5.E4.m1.2.3"><eq id="S5.E4.m1.2.3.1.cmml" xref="S5.E4.m1.2.3.1"></eq><ci id="S5.E4.m1.2.3.2a.cmml" xref="S5.E4.m1.2.3.2"><mtext id="S5.E4.m1.2.3.2.cmml" xref="S5.E4.m1.2.3.2">Cosine Similarity</mtext></ci><apply id="S5.E4.m1.2.2.cmml" xref="S5.E4.m1.2.2"><divide id="S5.E4.m1.2.2.3.cmml" xref="S5.E4.m1.2.2"></divide><apply id="S5.E4.m1.2.2.4.cmml" xref="S5.E4.m1.2.2.4"><times id="S5.E4.m1.2.2.4.1.cmml" xref="S5.E4.m1.2.2.4.1"></times><apply id="S5.E4.m1.2.2.4.2.cmml" xref="S5.E4.m1.2.2.4.2"><csymbol cd="ambiguous" id="S5.E4.m1.2.2.4.2.1.cmml" xref="S5.E4.m1.2.2.4.2">superscript</csymbol><apply id="S5.E4.m1.2.2.4.2.2.cmml" xref="S5.E4.m1.2.2.4.2"><csymbol cd="ambiguous" id="S5.E4.m1.2.2.4.2.2.1.cmml" xref="S5.E4.m1.2.2.4.2">subscript</csymbol><ci id="S5.E4.m1.2.2.4.2.2.2.cmml" xref="S5.E4.m1.2.2.4.2.2.2">𝑥</ci><ci id="S5.E4.m1.2.2.4.2.2.3.cmml" xref="S5.E4.m1.2.2.4.2.2.3">𝑖</ci></apply><ci id="S5.E4.m1.2.2.4.2.3.cmml" xref="S5.E4.m1.2.2.4.2.3">𝑇</ci></apply><apply id="S5.E4.m1.2.2.4.3.cmml" xref="S5.E4.m1.2.2.4.3"><csymbol cd="ambiguous" id="S5.E4.m1.2.2.4.3.1.cmml" xref="S5.E4.m1.2.2.4.3">subscript</csymbol><apply id="S5.E4.m1.2.2.4.3.2.cmml" xref="S5.E4.m1.2.2.4.3.2"><ci id="S5.E4.m1.2.2.4.3.2.1.cmml" xref="S5.E4.m1.2.2.4.3.2.1">^</ci><ci id="S5.E4.m1.2.2.4.3.2.2.cmml" xref="S5.E4.m1.2.2.4.3.2.2">𝑥</ci></apply><ci id="S5.E4.m1.2.2.4.3.3.cmml" xref="S5.E4.m1.2.2.4.3.3">𝑗</ci></apply></apply><apply id="S5.E4.m1.2.2.2.cmml" xref="S5.E4.m1.2.2.2"><times id="S5.E4.m1.2.2.2.3.cmml" xref="S5.E4.m1.2.2.2.3"></times><apply id="S5.E4.m1.1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E4.m1.1.1.1.1.2.1.cmml" xref="S5.E4.m1.1.1.1.1.1.2">norm</csymbol><apply id="S5.E4.m1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E4.m1.1.1.1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S5.E4.m1.1.1.1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S5.E4.m1.2.2.2.2.2.cmml" xref="S5.E4.m1.2.2.2.2.1"><csymbol cd="latexml" id="S5.E4.m1.2.2.2.2.2.1.cmml" xref="S5.E4.m1.2.2.2.2.1.2">norm</csymbol><apply id="S5.E4.m1.2.2.2.2.1.1.cmml" xref="S5.E4.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S5.E4.m1.2.2.2.2.1.1.1.cmml" xref="S5.E4.m1.2.2.2.2.1.1">subscript</csymbol><apply id="S5.E4.m1.2.2.2.2.1.1.2.cmml" xref="S5.E4.m1.2.2.2.2.1.1.2"><ci id="S5.E4.m1.2.2.2.2.1.1.2.1.cmml" xref="S5.E4.m1.2.2.2.2.1.1.2.1">^</ci><ci id="S5.E4.m1.2.2.2.2.1.1.2.2.cmml" xref="S5.E4.m1.2.2.2.2.1.1.2.2">𝑥</ci></apply><ci id="S5.E4.m1.2.2.2.2.1.1.3.cmml" xref="S5.E4.m1.2.2.2.2.1.1.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.2c">\text{Cosine Similarity}=\frac{x_{i}^{T}\hat{x}_{j}}{\|x_{i}\|\|\hat{x}_{j}\|}</annotation><annotation encoding="application/x-llamapun" id="S5.E4.m1.2d">Cosine Similarity = divide start_ARG italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG ∥ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ ∥ over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">These metrics are the current evaluation standard for classification models, but they can be adapted for text generation tasks <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib40" title="">2019</a>)</cite>. Precision and Recall measure the proportion of correctly identified positive cases. In the context of our experiment, every word from predicted text gets matched to a word in the referenced text to compute recall. This process is inverted to then compute precision. The precision and recall values are then combined to compute an F1 score. These metrics use cosine similarity (Equation 4) in which each predicted word is paired with its closest corresponding word from the reference text with the aim of maximizing the similarity score.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">In Table 1, the performance of various LLM prompting techniques including Chain of Thought, Few Shot, Zero Shot and RAG, are compared across different embedding models (Distillbert-base-uncased, Bert-base-uncased, and Roberta-large). This comparison aims to evaluate the robustness and effectiveness of these prompting techniques. Our results indicate that while each prompting technique shows varying level of precision, recall and F1-score, RAG consistently outperform the others on all three metrics, achieving highest performance across all models.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.2">
<tr class="ltx_tr" id="S5.T2.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.1">Example: Keywords from LDA Topic One</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2">
<td class="ltx_td ltx_align_center" id="S5.T2.2.2.1">Students</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.3">
<td class="ltx_td ltx_align_center" id="S5.T2.2.3.1">Course</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.4">
<td class="ltx_td ltx_align_center" id="S5.T2.2.4.1">Develop</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.5">
<td class="ltx_td ltx_align_center" id="S5.T2.2.5.1">People</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.6">
<td class="ltx_td ltx_align_center" id="S5.T2.2.6.1">Institution</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.7">
<td class="ltx_td ltx_align_center" id="S5.T2.2.7.1">Project</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.8">
<td class="ltx_td ltx_align_center" id="S5.T2.2.8.1">Science</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.9">
<td class="ltx_td ltx_align_center" id="S5.T2.2.9.1">Discipline</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.10">
<td class="ltx_td ltx_align_center" id="S5.T2.2.10.1">Material</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.11">
<td class="ltx_td ltx_align_center" id="S5.T2.2.11.1">Start</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.12">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.12.1">Example: Output from LLM approach</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.13">
<td class="ltx_td ltx_align_center" id="S5.T2.2.13.1">Collaboration: Co-creating resources and connecting with others</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.14">
<td class="ltx_td ltx_align_center" id="S5.T2.2.14.1"><span class="ltx_text ltx_font_italic" id="S5.T2.2.14.1.1">Corresponding Anecdote: You can also in your teaching have students connect with people outside the</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.15">
<td class="ltx_td ltx_align_center" id="S5.T2.2.15.1"><span class="ltx_text ltx_font_italic" id="S5.T2.2.15.1.1">course in various ways. Like, maybe some people outside the course are commenting on blogs</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.16">
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.16.1"><span class="ltx_text ltx_font_italic" id="S5.T2.2.16.1.1">and student are getting in a conversation around that.</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.3.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S5.T2.4.2" style="font-size:90%;">Example Output from LLM approach compared to Keywords from LDA Topic One </span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Learnings</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Treating large language models (LLMs) as novice research assistants during thematic analysis offered valuable insights for our research. By framing the LLM as a novice collaborator with little knowledge or insight of the context, prompts can be crafted to better guide the model and leverage its capabilities. Used prudently, similar novice LLM-augmented approaches can significantly increase time and resource efficiency compared to traditional qualitative coding methods in talent management research. The following sections explore some of our key learnings that may benefit other researchers considering designing LLMs as novice researchers to optimize thematic analysis.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Approaching LLMs as Novice Research Assistants can help prepare better prompts</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">A novice is a person who, “has no experience with the situations in which they are expected to perform tasks” <cite class="ltx_cite ltx_citemacro_citep">(Benner, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib3" title="">1982</a>)</cite>. The novice is thus at a basic proficiency level for skill acquisition, with limited information and prior experience related to a task at hand <cite class="ltx_cite ltx_citemacro_citep">(Montfort et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib28" title="">2013</a>)</cite>. For large qualitative datasets analyzed using LLMs we propose that a novice-led approach to analysis is a good fit. In our approach the human behaves as an expert prompting the novice LLM to provide insights related to topics of interest. We found this framework as a helpful mental model to ground the primary researcher prompting the LLM as they iteratively uncover insights from the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Used prudently, LLMs can help increase time effectiveness and resource efficiency</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">LLMs have advanced the field of natural language processing with their ability to understand and generate responses that closely mimic human language <cite class="ltx_cite ltx_citemacro_citep">(Shanahan, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib34" title="">2024</a>)</cite>. The strengths of LLMs extend beyond metrics, these models are adept at processing vast amounts of text rapidly, demonstrating a level of topic modeling that can mimic human analysis. Manual topic modeling is human labor intensive and time inefficient <cite class="ltx_cite ltx_citemacro_citep">(Clarke and Braun, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib8" title="">2017</a>)</cite>. LLMs also enhance efficiency by streamlining the processing of large datasets, allowing for the extraction of topics from qualitative data more quickly. Improvisations of these model using techniques like few-shot and zero-shot learning capabilities further reduce the need for expensive data labeling and annotations. In a nutshell, LLMs boost speed, reduce human effort, scale to massive datasets, and lower labeling costs. However, human expertise is still essential for judgment, validation and end-to-end framework design.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>LLM augmented approaches offer significant increase in ease and enhanced context compared to traditional NLP approaches.</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Using a RAG approach towards an LLM-augmented qualitative research analyzing semi-structure interviews shows great promise compared to natural language processing methods like Latent Dirichlet allocation (LDA). Currently, there are no widely accepted methods for comparing the two approaches as there is no bridge to compare keywords to themes, except from a human-evaluator ease of interpretability standpoint. We performed topic modeling analysis on the same dataset with the broader aim of finding themes. Manually comparing both approaches, each researcher of this workstream independently found that any of the approaches using an LLM yielded much greater context and consequently, better interpretability than the traditional LDA approach. This is likely because, with LDA, the model outputs a list of words and probability for each topic. With these words, the researcher would then have to manually define the topic. While this approach increases researcher flexibility, it remains time and resource consuming. In contrast, with the LLM approach, the output is richer in context of what particular topics mean. For example, our LDA model yielded 5 topics (see: Appendix A Figure 3). The first 10 words for topic 1 can also be seen in Table 2. Putting these words together into a comprehensive theme can be challenging without more context. However, an LLM is able to generate context grounded in the participant’s voice for researchers to work with. An example of an extracted theme and its corresponding anecdote using an LLM can also be seen in Table 2, above.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Recommendations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Traditional qualitative research is evaluated based on several criteria that ensure quality and rigor of the research, both in terms of methods as well as findings. Prior research has established four criteria for increased rigor and trustworthiness of qualitative research studies around credibility, dependability, confirmability, and transferability <cite class="ltx_cite ltx_citemacro_citep">(Lincoln and Guba, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib25" title="">1988</a>)</cite>. We recommend three ways in which quality criteria from traditional qualitative research can be used by practitioners employing LLM augmented analysis of qualitative data.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Establishing credibility of findings by incorporating mechanism for member checks.</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Member checks, i.e., the strategy of soliciting insights from research participants on research findings, are often relied on as the gold standard for increasing trustworthiness of qualitative research approaches (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Patton, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib30" title="">2014</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Kornbluh, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib21" title="">2015</a>)</cite>). Qualitative researchers employing LLMs can work on deepening their understanding of the research context using appropriate data-collection methods and tools that work best for particular contexts, as well as conduct adequate member checking to ensure the accuracy of findings.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Practicing increased researcher reflexivity.</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Qualitative researchers are recommended that they acknowledge and address their own biases, thus recognizing the influence of their own experiences and opinions on the research process <cite class="ltx_cite ltx_citemacro_citep">(Finlay, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib16" title="">2002</a>)</cite>. Similar exercises on reflectivity can also be helpful for researchers augmenting qualitative data analysis through employing LLMs. Researcher reflexivity in such instances can extend to querying the LLM to ask for rationale on why certain topics were extracted, grounding topics in anecdotes from the transcripts, and recognizing the influence the human researcher’s prior knowledge and biases will have on the prompts used. Future work in extending LLMs for qualitative research should continue to draw on evaluation criteria grounded in traditional qualitative research paradigm.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Increasing transparency of decisions made throughout the research study.</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">Qualitative researchers are recommended to thoroughly document all decisions that guide their analysis process by providing thick descriptions, allowing for increased transparency. This practice enhances reliability and reproducibility of the research <cite class="ltx_cite ltx_citemacro_citep">(Lincoln and Guba, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib25" title="">1988</a>)</cite>. Qualitative researchers employing LLMs should also similarly strategize maximizing transparency through mechanisms such as documenting changes in workflow, sharing prompts, and detailing model preferences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Closing Thoughts</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The approach outlined in this paper offers a promising avenue for industry-based talent management practitioners seeking to increase the time and resource efficiency of qualitative interview data analysis. By leveraging large language models (LLMs) as novice qualitative research assistants, organizations can potentially accelerate the coding, categorization, and thematic synthesis of rich interview data - a critical bottleneck in many talent management research initiatives.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">However, as the field of LLM-assisted qualitative research matures, it will be essential to not only benchmark model performance against traditional quantitative evaluation metrics, but also consider quality criteria more prominent within the qualitative research paradigm. Factors such as credibility, transferability, dependability, and confirmability will need to be carefully evaluated as LLMs are integrated into qualitative workflows. Furthermore, the ethical use of AI assistants in sensitive domains like talent management will require close, multi-disciplinary attention to issues at the intersection of data privacy, algorithmic bias, and model transparency, for which researchers will have to be trained <cite class="ltx_cite ltx_citemacro_citep">(Mackenzie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11043v1#bib.bib26" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Future research should seek to establish guidelines and best practices for LLM-augmented qualitative analysis that uphold the rigor and trustworthiness expected within the qualitative research community. Only by doing so can talent management scholars and practitioners unlock the full potential of these powerful language models, while respecting the epistemological foundations of qualitative inquiry. As the field evolves, we believe that a judicious, ethically-grounded approach to LLM integration can yield substantial gains in research efficiency and organizational impact.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>. 610–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benner (1982)</span>
<span class="ltx_bibblock">
Patricia Benner. 1982.

</span>
<span class="ltx_bibblock">From novice to expert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">AJN The American Journal of Nursing</em> 82, 3 (1982), 402–407.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhaduri (2018)</span>
<span class="ltx_bibblock">
Sreyoshi Bhaduri. 2018.

</span>
<span class="ltx_bibblock">NLP in Engineering Education-Demonstrating the use of Natural Language Processing Techniques for Use in Engineering Education Classrooms and Research.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhaduri et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Sreyoshi Bhaduri, Michelle Soledad, Tamoghna Roy, Homero Murzi, and Tamara Knott. 2021.

</span>
<span class="ltx_bibblock">A Semester Like No Other: Use of Natural Language Processing for Novice-Led Analysis on End-of-Semester Responses on Students’ Experience of Changing Learning Environments Due to COVID-19. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">2021 ASEE Virtual Annual Conference Content Access</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown (2020)</span>
<span class="ltx_bibblock">
Tom B Brown. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint ArXiv:2005.14165</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bryman (2016)</span>
<span class="ltx_bibblock">
Alan Bryman. 2016.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Social research methods</em>.

</span>
<span class="ltx_bibblock">Oxford university press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clarke and Braun (2017)</span>
<span class="ltx_bibblock">
Victoria Clarke and Virginia Braun. 2017.

</span>
<span class="ltx_bibblock">Thematic analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">The journal of positive psychology</em> 12, 3 (2017), 297–298.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creamer (2017)</span>
<span class="ltx_bibblock">
Elizabeth G Creamer. 2017.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">An introduction to fully integrated mixed methods research</em>.

</span>
<span class="ltx_bibblock">sage publications.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creamer (2018)</span>
<span class="ltx_bibblock">
Elizabeth G Creamer. 2018.

</span>
<span class="ltx_bibblock">Striving for methodological integrity in mixed methods research: The difference between mixed methods and mixed-up methods.

</span>
<span class="ltx_bibblock">, 526–530 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creswell and Creswell (2017)</span>
<span class="ltx_bibblock">
John W Creswell and J David Creswell. 2017.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Research design: Qualitative, quantitative, and mixed methods approaches</em>.

</span>
<span class="ltx_bibblock">Sage publications.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denzin et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Norman K Denzin, Yvonna S Lincoln, Michael D Giardina, and Gaile S Cannella. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">The Sage handbook of qualitative research</em>.

</span>
<span class="ltx_bibblock">Sage publications.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Alexander Dunn, John Dagdelen, Nicholas Walker, Sanghoon Lee, Andrew S Rosen, Gerbrand Ceder, Kristin Persson, and Anubhav Jain. 2022.

</span>
<span class="ltx_bibblock">Structured information extraction from complex scientific text with fine-tuned large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">arXiv preprint arXiv:2212.05238</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwivedi et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yogesh K Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M Baabdullah, Alex Koohang, Vishnupriya Raghavan, Manju Ahuja, et al<span class="ltx_text" id="bib.bib15.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Opinion Paper:“So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.4.1">International Journal of Information Management</em> 71 (2023), 102642.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finlay (2002)</span>
<span class="ltx_bibblock">
Linda Finlay. 2002.

</span>
<span class="ltx_bibblock">Negotiating the swamp: the opportunity and challenge of reflexivity in research practice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Qualitative research</em> 2, 2 (2002), 209–230.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glaser (1965)</span>
<span class="ltx_bibblock">
Barney G Glaser. 1965.

</span>
<span class="ltx_bibblock">The constant comparative method of qualitative analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Social problems</em> 12, 4 (1965), 436–445.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greene (2008)</span>
<span class="ltx_bibblock">
Jennifer C Greene. 2008.

</span>
<span class="ltx_bibblock">Is mixed methods social inquiry a distinctive methodology?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Journal of mixed methods research</em> 2, 1 (2008), 7–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halcomb and Hickman (2015)</span>
<span class="ltx_bibblock">
Elizabeth J Halcomb and Louise Hickman. 2015.

</span>
<span class="ltx_bibblock">Mixed methods research.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, and Xin Zhou. 2023.

</span>
<span class="ltx_bibblock">Better zero-shot reasoning with role-play prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2308.07702</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornbluh (2015)</span>
<span class="ltx_bibblock">
Mariah Kornbluh. 2015.

</span>
<span class="ltx_bibblock">Combatting challenges to establishing trustworthiness in qualitative research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Qualitative research in psychology</em> 12, 4 (2015), 397–414.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Hsiang (2019)</span>
<span class="ltx_bibblock">
Jieh-Sheng Lee and Jieh Hsiang. 2019.

</span>
<span class="ltx_bibblock">Patentbert: Patent classification with fine-tuning a pre-trained bert model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:1906.02124</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leino and Räihä (2007)</span>
<span class="ltx_bibblock">
Juha Leino and Kari-Jouko Räihä. 2007.

</span>
<span class="ltx_bibblock">Case amazon: ratings and reviews as part of recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2007 ACM conference on Recommender systems</em>. 137–140.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib24.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lincoln and Guba (1988)</span>
<span class="ltx_bibblock">
Yvonna S Lincoln and Egon G Guba. 1988.

</span>
<span class="ltx_bibblock">Criteria for Assessing Naturalistic Inquiries as Reports.

</span>
<span class="ltx_bibblock">(1988).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mackenzie et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tammy Mackenzie, Leslie Salgado, Sreyoshi Bhaduri, Victoria Kuketz, Solenne Savoia, and Lilianny Virguez. 2024.

</span>
<span class="ltx_bibblock">Beyond the Algorithm: Empowering AI Practitioners through Liberal Education. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">2024 ASEE Annual Conference &amp; Exposition</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maxwell (2018)</span>
<span class="ltx_bibblock">
Joseph A Maxwell. 2018.

</span>
<span class="ltx_bibblock">Collecting qualitative data: A realist approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The SAGE handbook of qualitative data collection</em> (2018), 19–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Montfort et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Devlin B Montfort, Geoffrey L Herman, Shane A Brown, Holly M Matusovich, and Ruth A Streveler. 2013.

</span>
<span class="ltx_bibblock">Novice-led paired thematic analysis: A method for conceptual change in engineering. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">2013 ASEE Annual Conference &amp; Exposition</em>. 23–933.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paskevicius (2018)</span>
<span class="ltx_bibblock">
Michael Paskevicius. 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Exploring educators experiences implementing open educational practices</em>.

</span>
<span class="ltx_bibblock">Ph. D. Dissertation.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patton (2014)</span>
<span class="ltx_bibblock">
Michael Quinn Patton. 2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Qualitative research &amp; evaluation methods: Integrating theory and practice</em>.

</span>
<span class="ltx_bibblock">Sage publications.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pearse (2019)</span>
<span class="ltx_bibblock">
Noel Pearse. 2019.

</span>
<span class="ltx_bibblock">An illustration of deductive analysis in qualitative research. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">18th European conference on research methodology for business and management studies</em>. 264.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pelaez et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sergio Pelaez, Gaurav Verma, Barbara Ribeiro, and Philip Shapira. 2024.

</span>
<span class="ltx_bibblock">Large-scale text analysis using generative language models: A case study in discovering public value expressions in AI patents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Quantitative Science Studies</em> 5, 1 (2024), 153–169.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al<span class="ltx_text" id="bib.bib33.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.4.1">OpenAI blog</em> 1, 8 (2019), 9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shanahan (2024)</span>
<span class="ltx_bibblock">
Murray Shanahan. 2024.

</span>
<span class="ltx_bibblock">Talking about large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Commun. ACM</em> 67, 2 (2024), 68–79.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strauss and Corbin (1998)</span>
<span class="ltx_bibblock">
Anselm Strauss and Juliet Corbin. 1998.

</span>
<span class="ltx_bibblock">Basics of qualitative research techniques.

</span>
<span class="ltx_bibblock">(1998).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is All you Need. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Advances in Neural Information Processing Systems</em>, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
J Wei, X Wang, D Schuurmans, M Bosma, F Xia, and E Chi. [n. d.].

</span>
<span class="ltx_bibblock">&amp; Zhou, D.(2022).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Chain-of-thought prompting elicits reasoning in large language models</em> ([n. d.]), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Binxia Yang, Xudong Luo, Kaili Sun, and Michael Y Luo. 2023.

</span>
<span class="ltx_bibblock">Recent progress on text summarisation based on bert and gpt. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">International Conference on Knowledge Science, Engineering and Management</em>. Springer, 225–241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yilmaz (2013)</span>
<span class="ltx_bibblock">
Kaya Yilmaz. 2013.

</span>
<span class="ltx_bibblock">Comparison of quantitative and qualitative research traditions: Epistemological, theoretical, and methodological differences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">European journal of education</em> 48, 2 (2013), 311–325.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:1904.09675</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao (2023)</span>
<span class="ltx_bibblock">
Wanqun Zhao. 2023.

</span>
<span class="ltx_bibblock">Using Science to Support and Develop Employees in the Tech Workforce—An Opportunity for Multidisciplinary Pursuits in Engineering Education. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">2023 ASEE Annual Conference &amp; Exposition</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<figure class="ltx_figure" id="S8.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S8.F3.1" style="width:433.6pt;height:290.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-97.6pt,65.4pt) scale(0.689655172413793,0.689655172413793) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="583" id="S8.F3.1.g1" src="extracted/5803374/3.png" width="870"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F3.3.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S8.F3.4.2" style="font-size:90%;">Topic Modeling using LDA</span></figcaption>
</figure>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Results from analyzing the same dataset using an LDA Approach.</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Traditional topic modeling using approaches such as Latent Dirichlet Allocation (LDA) often present the most representative words for each generated topic. For instance, for Topic 1 words such as "students", "develop", "institution", "science", etc. were found important. Attempting to interpret the underlying thematic meaning of these word lists can be challenging without additional contextual information about how those words were used within the original corpus. In contrast, large language models (LLMs) have demonstrated the capability to synthesize the semantically related words and phrases into more coherent topical representations. This ability of LLMs to generate primitive yet formative contextual information threading together words and phrases of interest and thereby provide researchers with a more insightful starting point for further analysis and interpretation of the latent topics uncovered through the LDA process.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug 20 17:48:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
