<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.08757] SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding</title><meta property="og:description" content="Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.08757">

<!--Generated on Fri Jul  5 22:24:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> <span id="id1.1.1" class="ltx_text ltx_font_bold">Jiefeng Ma<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
 <span id="id2.2.2" class="ltx_text ltx_font_bold">Yan Wang<sup id="id2.2.2.1" class="ltx_sup"><span id="id2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
 <span id="id3.3.3" class="ltx_text ltx_font_bold">Chenyu Liu<sup id="id3.3.3.1" class="ltx_sup"><span id="id3.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>
 <span id="id4.4.4" class="ltx_text ltx_font_bold">Jun Du<sup id="id4.4.4.1" class="ltx_sup"><span id="id4.4.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
 <span id="id5.5.5" class="ltx_text ltx_font_bold">Yu Hu<sup id="id5.5.5.1" class="ltx_sup"><span id="id5.5.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>
<br class="ltx_break"></span> <span id="id6.6.6" class="ltx_text ltx_font_bold">Zhenrong Zhang<sup id="id6.6.6.1" class="ltx_sup"><span id="id6.6.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
 <span id="id7.7.7" class="ltx_text ltx_font_bold">Pengfei Hu<sup id="id7.7.7.1" class="ltx_sup"><span id="id7.7.7.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
 <span id="id8.8.8" class="ltx_text ltx_font_bold">Qing Wang<sup id="id8.8.8.1" class="ltx_sup"><span id="id8.8.8.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
 <span id="id10.10.10" class="ltx_text ltx_font_bold">Jianshu Zhang<sup id="id10.10.10.1" class="ltx_sup"><span id="id10.10.10.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>
<br class="ltx_break"><sup id="id10.10.10.2" class="ltx_sup"><span id="id10.10.10.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>University of Science and Technology of China, Hefei, China 
<br class="ltx_break"><sup id="id12.12.id1" class="ltx_sup"><span id="id12.12.id1.1" class="ltx_text ltx_font_italic">2</span></sup>iFLYTEK, Hefei, China 
<br class="ltx_break"><span id="id13.13.id2" class="ltx_text ltx_font_typewriter">{jfma, yanwangsa, zzr666, hudeyouxiang}@mail.ustc.edu.cn</span>, 
<br class="ltx_break"><span id="id14.14.id3" class="ltx_text ltx_font_typewriter">{jundu, yuhu2, qingwang2}@ustc.edu.cn, {cyliu7, jszhang6}@iflytek.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) <span id="id15.id1.1" class="ltx_text ltx_font_bold">word to text-line merging</span>, (2) <span id="id15.id1.2" class="ltx_text ltx_font_bold">text-line to entity merging</span>, (3) <span id="id15.id1.3" class="ltx_text ltx_font_bold">entity category classification</span>, (4) <span id="id15.id1.4" class="ltx_text ltx_font_bold">item table localization</span>, and (5) <span id="id15.id1.5" class="ltx_text ltx_font_bold">entity-based full-document hierarchical structure recovery</span>. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including <span id="id15.id1.6" class="ltx_text ltx_font_italic">English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese</span>, making it a powerful tool for cross-lingual form understanding. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original dataset and implementations of baseline methods are available at <a target="_blank" href="https://sprateam-ustc.github.io/SRFUND" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sprateam-ustc.github.io/SRFUND</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">In the United States, billions of individuals and businesses submit tax returns annually,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.irs.gov/pub/irs-pdf/p55b.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.irs.gov/pub/irs-pdf/p55b.pdf</a></span></span></span> and globally, hundreds of billions of parcels are distributed each year,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.pitneybowes.com/content/dam/pitneybowes/us/en/shipping-index/23-mktc-03596-2023_global_parcel_shipping_index_ebook-web.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.pitneybowes.com/content/dam/pitneybowes/us/en/shipping-index/23-mktc-03596-2023_global_parcel_shipping_index_ebook-web.pdf</a></span></span></span> most of which are accompanied by invoices and delivery notes. Although these documents vary in format, they are all considered forms, which serve as crucial information mediums widely used in global information and merchandise exchange. Compared to storage formats like camera-captured images or scanned documents, digitizing original forms into structured text aids in reducing storage space and facilitates information dissemination <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Consequently, there has been a growing practical demand in recent years for understanding information within forms, including both textual content and document structures across various layouts and languages.
With the rapid development of document processing technologies, significant progress has been made in the field of form understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, along with the establishment of a series of benchmark datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. However, none of these existing datasets have established the global and hierarchical structural dependencies considering all elements at different granularity, including words, text lines, and entities within the forms.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/figures/83553333_3334_word_boxes.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_portrait" width="138" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Word level</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/figures/83553333_3334_line_boxes.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_portrait" width="138" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Text-line level</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/figures/83553333_3334_entity_boxes.png" id="S1.F1.sf3.g1" class="ltx_graphics ltx_img_portrait" width="138" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Entity level</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/figures/83553333_3334_table_boxes.png" id="S1.F1.sf4.g1" class="ltx_graphics ltx_img_portrait" width="138" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Item table level</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/x1.png" id="S1.F1.sf5.g1" class="ltx_graphics ltx_img_landscape" width="442" height="71" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Overall form structure based on entities.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Multiple granularity of annotations and supported tasks on SRFUND.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To enhance the applicability of form understanding tasks in hierarchical structure recovery, we introduce the SRFUND, a multilingual form structure reconstruction dataset. The SRFUND dataset comprises 1,592 form images across eight languages, with each language contributing 199 images. As illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, each form image is manually annotated with the locations and text contents of every word, text-line, and entity.
After identifying each independent entity, we categorize these entities into four classes including <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">Header, Question, Answer</span>, and <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">Other</span>, which is consistent with the FUNSD dataset definitions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Moreover, all entities in the form are annotated with their hierarchical dependencies, allowing us to reconstruct the global form structure.
For the multi-item table regions frequently found in forms, we have specifically annotated the positions of these tables, including their table headers, and grouped each line item within these tables individually.
The refined annotations of SRFUND support the evaluation of form structure reconstruction tasks at different granularities. We conducted benchmark tests on several tasks using representative methods from three categories: vision-only, language-only, and multi-modal approaches. These tasks include (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery.
Detailed experimental settings and results are presented in Sec. <a href="#S4" title="4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Prior research has divided document structure tasks into two main categories: physical layout analysis and logical structure analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The former refers to the physical locations of various regions within a document image, while the latter aims to understand their functional roles and relationships. In this chapter, we will first review the work related to physical and logical structure analysis. Additionally, we will introduce common benchmarks widely used in form understanding tasks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Document physical layout analysis</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Earlier document layout analysis methods can be classified into two main categories.
Algorithms employing a bottom-up strategy start from the finest elements of the document and iteratively merge these elements based on rules or clustering algorithms to create larger and more unified regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Conversely, top-down strategies begin from the entire document image and use histogram analysis or whitespace refinement methods to segment it into increasingly smaller regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. With the advancement of deep learning technology, several approaches have been proposed to address the document layout analysis challenges in more complex scenarios. These approaches can generally be divided into two categories. Detection-based approaches follow the route of object detection in computer vision, treating different elements within a document as distinct detection targets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Models such as Cascade-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> are used to directly predict the positions of various elements in the document images. On the other hand, methods based on instance segmentation employ frameworks used for instance segmentation in natural scene images to segment areas within documents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. For example, FCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> or Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> is utilized to segment text-line regions or other types of areas from complex document images.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Document logical structure analysis</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Logical structure analysis of documents focuses on analyzing the types and relationships of document elements at a logical level, which is often built upon the results of physical layout analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">In document element classification tasks, early approaches are often based on deterministic grammar rules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Such approaches typically require detailed rule specifications for a particular document layout and struggle to generalize to different document scenarios. Methods based on deep learning have boosted the performance on this task. Vision-only approaches for multi-class detection or segmentation do not rely on extracting the text content and position of document elements but instead employ visual models to directly locate different categories of document elements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Additionally, approaches based on natural language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> or multi-modal language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> are used to determine the types of document elements when the text content and positions of document elements are provided.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">In document structure analysis tasks, early solutions employ formal grammar or logical trees to represent and arrange hierarchical relationships among elements in documents. This often requires manually designing rules tailored to the current layout. To automatically learn relationships among document elements from diverse layout data, some deep learning-based approaches have been utilized for relationship prediction tasks. DocStruct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and StrucText <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> utilize a single learnable asymmetric parameter matrix for predicting asymmetric relationships between any two document elements. GraphDCM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> introduces a <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">Merger</span> module containing a set of asymmetric parameter matrices, further aggregating fine-grained elements with the same category in the document into coarse-grained elements. In LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, all possible head-tail pairs are first collected, and a bi-affine classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is used to determine if a relationship exists between them. GeoLayoutLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposes a relationship classification head composed of bi-linear layers and lightweight transformers, further enhancing the performance of element relationship classification tasks.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Form understanding benchmarks</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">The development process of form understanding tasks is closely related to relevant benchmarks. SROIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> comprises 973 scanned English receipts, each annotated with line-level texts, corresponding bounding boxes, and a structured extraction target with four predefined field types. CORD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> is another receipt dataset collected from various sources, including shops and restaurants, containing 1,000 receipt images from Indonesia. Compared to SROIE, the CORD dataset includes annotations at the word and entity levels with richer extraction field types. It also provides classification attributes for key-value pairs and group information within the same item, enabling the recovery of local relationships between different entities. EPHOIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> consists of 1,494 examination paper headers collected from Chinese school exams, annotated with ten types of entities. It offers annotations for text-line-level positions and contents, as well as classification attributes for key-value pairs. However, all the aforementioned datasets are provided under specific form categories, lacking diversity in form types.
The FUNSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> dataset contains 199 noisy scanned English documents, along with annotations at the word and entity levels. It categorizes all entities within forms into four classes: <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">Header</span>, <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">Question</span>, <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_italic">Answer</span>, and <span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_italic">Other</span>, and provides local relationships between entities, supporting entity labeling and entity linking tasks. XFUND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> is an extension of FUNSD in multiple languages, collecting additional forms in seven languages and providing similar annotations as FUNSD. It is worth noting that XFUND suffers from some entity definition confusion, where different text lines that should belong to the same entity are split and labeled as distinct entities.
The SIBR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> is a publicly available dataset designed for visual information extraction, comprising 1,000 form images, including 600 Chinese invoices, 300 English bills of entry, and 100 bilingual receipts. It offers text-line-level position and content information, along with two types of links to represent document element relationships: one for linking different text lines within the same entity and another for indicating relationships between different entities. However, these datasets lack uniform granularity in comprehensive annotations for words, text lines, and entities. In addition, they focus on local key-value relationships and ignore the nested relationships between elements of different hierarchies in the document, resulting in incomplete representation of form information.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>SRFUND benchmark</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data collection and annotation</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The objective of SRFUND is to advance the development of form understanding and structured reconstruction tasks.
Among existing form datasets, FUNSD and XFUND are two prominent works that have made outstanding contributions to the establishment of typical form understanding tasks and the extension to multilingual data scenarios, respectively. Our dataset, SRFUND, is built upon these two representative datasets, encompassing all document images from both datasets. SRFUND comprises 1,592 form images across eight languages, with each language contributing 199 images.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Upon careful analysis of the existing annotations in FUNSD and XFUND, we found inconsistencies in the granularity of annotations between the two datasets. The FUNSD dataset encapsulates complete semantic information of entity elements, regardless of whether this information is distributed across single or multiple lines, while the XFUND dataset includes cases where consecutive semantic multi-line text is independently counted as separate entities without connectivity between these text lines. Fortunately, both FUNSD and XFUND datasets cover word-level textual contents and positions. Leveraging the word-level annotation information from the original datasets, we meticulously followed several procedures to ensure a rigorous construction process for the dataset:
(1) Adjust inaccurate word-level bounding boxes and supplement missing textual information.
(2) Aggregate consecutive words with continuous semantics into one text-line and annotate the corresponding rectangular bounding box. It’s noteworthy that separate values with bullet points, keys and values in key-value pairs are considered different text lines, even when they are visually connected to each other.
(3) For entities composed of consecutive semantic multi-line text, we annotate the polygonal bounding box of the entity.
(4) Based on the roles of entities in the current form, modify or assign correct categories to different entities.
When the IOU between the annotation box of an entity and the existing entity box from the original datasets (i.e. FUNSD or XFUND) is greater than 0.8, the original label of the entity is adopted as the initial label of the current entity.
(5) Determine the location of item tables and annotate the headers and each individual row item within the tables.
(6) For separate entities with linkage relationships, annotate the relationships between these entities (unidirectional or bidirectional).
</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">To ensure the accuracy of the annotations, all annotated results underwent at least three rounds of cross-checking, with any disputed annotations resolved by domain experts in the field of document processing. Given the multilingual coverage within the SRFUND dataset, a commercial translation engine was employed during the annotation process to translate form images into the native languages of annotators, providing semantic context for reference. The collection, annotation, and refinement processes of the dataset collectively consumed approximately 6,000 person-hours.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with existing form understanding datasets.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:2356.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(80.0pt,-434.7pt) scale(1.58472165604989,1.58472165604989) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_tt" colspan="5">Supported Tasks</th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_tt" colspan="3">Statistics</th>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.1.1.1" class="ltx_p">Word to Text-line</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.2.1.1" class="ltx_p">Text-line to Entity</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.3.1.1" class="ltx_p">Entity Labeling</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.4.1.1" class="ltx_p">Item Table</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.5.1.1" class="ltx_p">Structure Recovery</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.6.1.1" class="ltx_p">Language</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.7.1.1" class="ltx_p">Images</span>
</span>
</td>
<td id="S3.T1.1.1.2.2.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.8.1.1" class="ltx_p ltx_align_center">Avg. Form Tree Depth</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SROIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.5.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.6.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.7.1.1" class="ltx_p">EN</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.8.1.1" class="ltx_p">1,000</span>
</span>
</td>
<td id="S3.T1.1.1.3.3.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.3.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.9.1.1" class="ltx_p ltx_align_center">-</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CORD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.5.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.6.1.1" class="ltx_p">Local</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.7.1.1" class="ltx_p">IND</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.8.1.1" class="ltx_p">1,000</span>
</span>
</td>
<td id="S3.T1.1.1.4.4.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.4.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.9.1.1" class="ltx_p ltx_align_center">1.173</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EPHOIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</th>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.5.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.6.1.1" class="ltx_p">Local</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.7.1.1" class="ltx_p">ZH</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.8.1.1" class="ltx_p">1,494</span>
</span>
</td>
<td id="S3.T1.1.1.5.5.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.5.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.9.1.1" class="ltx_p ltx_align_center">1.115</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SIBR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.5.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.6.1.1" class="ltx_p">Local</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.7.1.1" class="ltx_p">ZH, EN</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.8.1.1" class="ltx_p">1,000</span>
</span>
</td>
<td id="S3.T1.1.1.6.6.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.6.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.9.1.1" class="ltx_p ltx_align_center">1.515</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<th id="S3.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">FUNSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.5.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.6.1.1" class="ltx_p">Local</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.7.1.1" class="ltx_p">EN</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.7.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.8.1.1" class="ltx_p">199</span>
</span>
</td>
<td id="S3.T1.1.1.7.7.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.7.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.9.1.1" class="ltx_p ltx_align_center">1.570</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.8.8" class="ltx_tr">
<th id="S3.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">XFUND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</th>
<td id="S3.T1.1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.5.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.6.1.1" class="ltx_p">Local</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.8.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.7.1.1" class="ltx_p">ZH, JA, ES, FR, IT, DE, PT</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.8.1.1" class="ltx_p">1,393</span>
</span>
</td>
<td id="S3.T1.1.1.8.8.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.8.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.9.1.1" class="ltx_p ltx_align_center">1.699</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.9.9" class="ltx_tr">
<th id="S3.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">SRFUND (Ours)</th>
<td id="S3.T1.1.1.9.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:42.7pt;">
<span id="S3.T1.1.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:37.0pt;">
<span id="S3.T1.1.1.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:28.5pt;">
<span id="S3.T1.1.1.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.5.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:51.2pt;">
<span id="S3.T1.1.1.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.6.1.1" class="ltx_p">Global</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:71.1pt;">
<span id="S3.T1.1.1.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.7.1.1" class="ltx_p">EN, ZH, JA, ES, FR, IT, DE, PT</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:34.1pt;">
<span id="S3.T1.1.1.9.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.8.1.1" class="ltx_p">1,592</span>
</span>
</td>
<td id="S3.T1.1.1.9.9.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:45.5pt;">
<span id="S3.T1.1.1.9.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.9.1.1" class="ltx_p ltx_align_center">3.049</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset analysis</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Supported tasks</span>:
As illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Data collection and annotation ‣ 3 SRFUND benchmark ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, SRFUND covers annotations at different levels, enabling the dataset to support a wider range of tasks than all existing form understanding datasets. It is noteworthy that previous datasets focused solely on the structural relationships between local document entities, whereas we meticulously annotated the logical relationships among all entities. This makes SRFUND the first dataset supporting the task of structure recovery for each entity at a global level. Furthermore, with the complement of item tables and their constituent items, SRFUND is also the first dataset supporting the localization of item tables within forms. The recovery of global structures and the localization of item tables require models to possess a strong understanding of entities within forms, posing significant challenges to form understanding tasks.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Statistics of different granularity categories of SRFUND. <span id="S3.T2.4.1" class="ltx_text ltx_font_italic">Total</span>, <span id="S3.T2.5.2" class="ltx_text ltx_font_italic">Same</span>, <span id="S3.T2.6.3" class="ltx_text ltx_font_italic">New</span> refer to the total amount of a class in SRFUND, the same amount as in the original dataset (FUNSD/XFUND), and the amount added or modified, respectively.</figcaption>
<div id="S3.T2.7" class="ltx_inline-block ltx_transformed_outer" style="width:303.5pt;height:67pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.2pt,2.5pt) scale(0.931159691482669,0.931159691482669) ;">
<table id="S3.T2.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.7.1.1.1" class="ltx_tr">
<th id="S3.T2.7.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S3.T2.7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Words</td>
<td id="S3.T2.7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Lines</td>
<td id="S3.T2.7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Entities</td>
<td id="S3.T2.7.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Tables</td>
<td id="S3.T2.7.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Table Items</td>
<td id="S3.T2.7.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Links</td>
</tr>
<tr id="S3.T2.7.1.2.2" class="ltx_tr">
<th id="S3.T2.7.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Total</th>
<td id="S3.T2.7.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">529,711</td>
<td id="S3.T2.7.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">112,662</td>
<td id="S3.T2.7.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">96,824</td>
<td id="S3.T2.7.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">591</td>
<td id="S3.T2.7.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">1,954</td>
<td id="S3.T2.7.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">122,594</td>
</tr>
<tr id="S3.T2.7.1.3.3" class="ltx_tr">
<th id="S3.T2.7.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Same</th>
<td id="S3.T2.7.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">529,372</td>
<td id="S3.T2.7.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S3.T2.7.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">81,462</td>
<td id="S3.T2.7.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S3.T2.7.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S3.T2.7.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">47,113</td>
</tr>
<tr id="S3.T2.7.1.4.4" class="ltx_tr">
<th id="S3.T2.7.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">New</th>
<td id="S3.T2.7.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">339</td>
<td id="S3.T2.7.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">112,662</td>
<td id="S3.T2.7.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">15,362</td>
<td id="S3.T2.7.1.4.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">591</td>
<td id="S3.T2.7.1.4.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1,954</td>
<td id="S3.T2.7.1.4.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">75,481</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Statistical metrics</span>:
(a) As shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Data collection and annotation ‣ 3 SRFUND benchmark ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the SRFUND dataset includes annotations in eight different languages, making it more diverse than existing datasets and addressing form understanding needs across various languages. In terms of document hierarchy, we constructed global entity relationships, resulting in an average tree depth of 3.049, which surpasses previous datasets significantly.
(b) As depicted in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Dataset analysis ‣ 3 SRFUND benchmark ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the SRFUND dataset introduces a substantial number of annotations across different hierarchical levels in addition to the original data annotations. We modified 339 bounding boxes or texts at the word level and provided annotations for 112,662 text lines. Our dataset contains a total of 96,824 entities, out of which 15,362 are newly added or modified. Additionally, we provided detailed annotations for item tables in documents, totaling 591 item tables and 1,954 item contents within them. Furthermore, we added 75,481 entity links, resulting in a total of 122,594 entity links.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.F2" class="ltx_figure"><img src="" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Models with varied modalities used for evaluating on the SRFUND benchmark. </figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/figures/detect_explain_1.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="298" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Correct detection examples</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.08757/assets/figures/detect_explain_2.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="298" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Incorrect detection examples</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of correct (the green boxes) and incorrect (the red boxes) bounding box predictions to capture the <span id="S4.F3.2.1" class="ltx_text ltx_font_italic">Header</span> entity (texts with yellow background). Bounding box must include exactly the word-level centers that lie within the ground truth annotation. Note: in Figure <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a>, only one of the predictions would be considered correct if all three boxes were predicted.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">To comprehensively assess the SRFUND dataset, we conducted benchmark tests using models across three different modalities: language models based on pure text input, detection models based on purely visual inputs, and document pre-trained language models that utilize multi-modal inputs.
We performed experimental analyses on five tasks, detailed as follows: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setting</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">As illustrated in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the vision-only models rely on the document image as the input, and tasks 1 to 4 can be regarded as the text-line detection task, the entity detection task, the multi-class entity detection task and the line item table area detection task respectively.
We selected three distinct types of visual detection models for comparison: YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, a single-stage detector based on the YOLO architecture; Cascade-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a multi-stage detector based on the RCNN framework; and DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, an improved version of the DETR model with faster convergence speed and detection accuracy. All three detection models utilize a ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> backbone pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In the training stage, we follow the original configuration adopted in the mmdetection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. During testing, we only preserve predicted boxes with a score threshold larger than 0.3 and adopt the non-maximum suppression algorithm for further filtering.
Since the task is framed as a coarse-to-fine merging task, we use the standard F1 score as the main evaluation metric. Unlike the common practice in object detection, where true positives are determined by thresholding the Intersection-over-Union, we use a different criterion tailored to evaluate the usefulness of detections for text read-out. Inspired by the CLEval metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> used in text detection, we measure whether the predicted area contains nothing but the related word-level box centers as visualized in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">The text-only models rely on the word texts as the input, and the multi-modal models rely on the texts, two-dimensional coordinates, and form images as the input.
Given the multilingual nature of the SRFUND dataset, we employed several multilingual language models and document pre-trained language models that support multilingual inputs. The language models include InfoXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and XLM-Roberta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, while the document pre-trained language models include LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, LiLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and GraphDoc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, while the latter two models incorporate InfoXLM-base as the language model.
As illustrated in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, for tasks 1, 2, and 4, we use a symmetric attention relation matrix as the learning target, where each aggregation target may contain multiple fine-grained element sets. Each aggregation target is considered an independent prediction sample, and a prediction is deemed correct only if all elements within the target set are completely aggregated together. For task 5, we utilize an asymmetric attention relation matrix to learn the relationships between different entities
, while a pair prediction is only considered correct if the directional relationship between two entities is accurately predicted.
We employ the F1 score as the final evaluation metric for all tasks, and <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">Merger</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> is adopted as the default relation classification head in the following sections.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">All models were run on servers equipped with eight 48 GB A40 graphics cards with a batch size of 8. To ensure that different models achieved their optimal performance, we followed the training strategies for vision-only models as the original version in mmdetection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, with specific settings for learning rate, optimizer, and the number of epoches detailed in the appendix. The text-only and multi-modal models uniformly utilized the Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> optimizer with <math id="S4.SS1.p3.1.m1.4" class="ltx_Math" alttext="\left(\beta_{1},\beta_{2}\right)=\left(0.9,0.999\right)" display="inline"><semantics id="S4.SS1.p3.1.m1.4a"><mrow id="S4.SS1.p3.1.m1.4.4" xref="S4.SS1.p3.1.m1.4.4.cmml"><mrow id="S4.SS1.p3.1.m1.4.4.2.2" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml"><mo id="S4.SS1.p3.1.m1.4.4.2.2.3" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml">(</mo><msub id="S4.SS1.p3.1.m1.3.3.1.1.1" xref="S4.SS1.p3.1.m1.3.3.1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.3.3.1.1.1.2" xref="S4.SS1.p3.1.m1.3.3.1.1.1.2.cmml">β</mi><mn id="S4.SS1.p3.1.m1.3.3.1.1.1.3" xref="S4.SS1.p3.1.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p3.1.m1.4.4.2.2.4" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml">,</mo><msub id="S4.SS1.p3.1.m1.4.4.2.2.2" xref="S4.SS1.p3.1.m1.4.4.2.2.2.cmml"><mi id="S4.SS1.p3.1.m1.4.4.2.2.2.2" xref="S4.SS1.p3.1.m1.4.4.2.2.2.2.cmml">β</mi><mn id="S4.SS1.p3.1.m1.4.4.2.2.2.3" xref="S4.SS1.p3.1.m1.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p3.1.m1.4.4.2.2.5" xref="S4.SS1.p3.1.m1.4.4.2.3.cmml">)</mo></mrow><mo id="S4.SS1.p3.1.m1.4.4.3" xref="S4.SS1.p3.1.m1.4.4.3.cmml">=</mo><mrow id="S4.SS1.p3.1.m1.4.4.4.2" xref="S4.SS1.p3.1.m1.4.4.4.1.cmml"><mo id="S4.SS1.p3.1.m1.4.4.4.2.1" xref="S4.SS1.p3.1.m1.4.4.4.1.cmml">(</mo><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">0.9</mn><mo id="S4.SS1.p3.1.m1.4.4.4.2.2" xref="S4.SS1.p3.1.m1.4.4.4.1.cmml">,</mo><mn id="S4.SS1.p3.1.m1.2.2" xref="S4.SS1.p3.1.m1.2.2.cmml">0.999</mn><mo id="S4.SS1.p3.1.m1.4.4.4.2.3" xref="S4.SS1.p3.1.m1.4.4.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.4b"><apply id="S4.SS1.p3.1.m1.4.4.cmml" xref="S4.SS1.p3.1.m1.4.4"><eq id="S4.SS1.p3.1.m1.4.4.3.cmml" xref="S4.SS1.p3.1.m1.4.4.3"></eq><interval closure="open" id="S4.SS1.p3.1.m1.4.4.2.3.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2"><apply id="S4.SS1.p3.1.m1.3.3.1.1.1.cmml" xref="S4.SS1.p3.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.3.3.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.3.3.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.3.3.1.1.1.2">𝛽</ci><cn type="integer" id="S4.SS1.p3.1.m1.3.3.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.3.3.1.1.1.3">1</cn></apply><apply id="S4.SS1.p3.1.m1.4.4.2.2.2.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.4.4.2.2.2.1.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.2">subscript</csymbol><ci id="S4.SS1.p3.1.m1.4.4.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.2.2">𝛽</ci><cn type="integer" id="S4.SS1.p3.1.m1.4.4.2.2.2.3.cmml" xref="S4.SS1.p3.1.m1.4.4.2.2.2.3">2</cn></apply></interval><interval closure="open" id="S4.SS1.p3.1.m1.4.4.4.1.cmml" xref="S4.SS1.p3.1.m1.4.4.4.2"><cn type="float" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">0.9</cn><cn type="float" id="S4.SS1.p3.1.m1.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2">0.999</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.4c">\left(\beta_{1},\beta_{2}\right)=\left(0.9,0.999\right)</annotation></semantics></math> and underwent 200 training epoches, with an initial learning rate set at 5e-5. The training began with a linear warm-up during the first 10% epochs, followed by a continuation under a linear decay strategy.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results and analysis</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Word to text-line merging</h4>

<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of the word to text-line merging task, using F1-score as the metric.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:109.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.8pt,26.4pt) scale(0.674092462828124,0.674092462828124) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Type</th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Chinese</th>
<th id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Japanese</th>
<th id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
<th id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">French</th>
<th id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Spanish</th>
<th id="S4.T3.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Italian</th>
<th id="S4.T3.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Portuguese</th>
<th id="S4.T3.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text">Vision-only</span></th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.8222</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.8053</td>
<td id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.6959</td>
<td id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.8587</td>
<td id="S4.T3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.7310</td>
<td id="S4.T3.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.8301</td>
<td id="S4.T3.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">0.7470</td>
<td id="S4.T3.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">0.7900</td>
<td id="S4.T3.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">0.7850</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cascade-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.8520</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.8842</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.7569</td>
<td id="S4.T3.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.8683</td>
<td id="S4.T3.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">0.8191</td>
<td id="S4.T3.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">0.8404</td>
<td id="S4.T3.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">0.7590</td>
<td id="S4.T3.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">0.7710</td>
<td id="S4.T3.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">0.8189</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<th id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.8437</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.8500</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.7394</td>
<td id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">0.8795</td>
<td id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.8082</td>
<td id="S4.T3.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">0.8468</td>
<td id="S4.T3.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.7926</td>
<td id="S4.T3.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">0.7954</td>
<td id="S4.T3.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">0.8194</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<th id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T3.1.1.5.4.1.1" class="ltx_text">Text-only</span></th>
<th id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">XLM-RoBerta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.6290</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.6272</td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.6093</td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">0.6982</td>
<td id="S4.T3.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">0.6921</td>
<td id="S4.T3.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t">0.6470</td>
<td id="S4.T3.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t">0.6285</td>
<td id="S4.T3.1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t">0.6780</td>
<td id="S4.T3.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_t">0.6509</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<th id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">InfoXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">0.6426</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.6482</td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.6298</td>
<td id="S4.T3.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">0.7011</td>
<td id="S4.T3.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">0.6974</td>
<td id="S4.T3.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t">0.6551</td>
<td id="S4.T3.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t">0.6253</td>
<td id="S4.T3.1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t">0.6921</td>
<td id="S4.T3.1.1.6.5.10" class="ltx_td ltx_align_center ltx_border_t">0.6611</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<th id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T3.1.1.7.6.1.1" class="ltx_text">Multi-modal</span></th>
<th id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.3.1" class="ltx_text ltx_font_bold">0.9081</span></td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t">0.9360</td>
<td id="S4.T3.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.5.1" class="ltx_text ltx_font_bold">0.9118</span></td>
<td id="S4.T3.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.6.1" class="ltx_text ltx_font_bold">0.9255</span></td>
<td id="S4.T3.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.7.1" class="ltx_text ltx_font_bold">0.9282</span></td>
<td id="S4.T3.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.8.1" class="ltx_text ltx_font_bold">0.9372</span></td>
<td id="S4.T3.1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.9.1" class="ltx_text ltx_font_bold">0.9157</span></td>
<td id="S4.T3.1.1.7.6.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.10.1" class="ltx_text ltx_font_bold">0.9387</span></td>
<td id="S4.T3.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.7.6.11.1" class="ltx_text ltx_font_bold">0.9260</span></td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<th id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LiLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">0.8887</td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.8.7.3.1" class="ltx_text ltx_font_bold">0.9387</span></td>
<td id="S4.T3.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0.8803</td>
<td id="S4.T3.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">0.9193</td>
<td id="S4.T3.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">0.9223</td>
<td id="S4.T3.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">0.9202</td>
<td id="S4.T3.1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_t">0.8962</td>
<td id="S4.T3.1.1.8.7.9" class="ltx_td ltx_align_center ltx_border_t">0.9054</td>
<td id="S4.T3.1.1.8.7.10" class="ltx_td ltx_align_center ltx_border_t">0.9094</td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<th id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">GraphDoc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T3.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8755</td>
<td id="S4.T3.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9100</td>
<td id="S4.T3.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8005</td>
<td id="S4.T3.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9167</td>
<td id="S4.T3.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8954</td>
<td id="S4.T3.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8993</td>
<td id="S4.T3.1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8471</td>
<td id="S4.T3.1.1.9.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8708</td>
<td id="S4.T3.1.1.9.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8758</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p"><span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Aggregating words into text lines presents significant challenges for single-modal approaches.</span> As demonstrated in Table <a href="#S4.T3" title="Table 3 ‣ 4.2.1 Word to text-line merging ‣ 4.2 Results and analysis ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, text-only models faced difficulties due to the absence of two-dimensional spatial coordinates and visual cues. This lack of information impedes their ability to accurately identify breakpoints in semantically continuous texts that span multiple lines. Conversely, vision-only models depend exclusively on the visual boundary features of text lines to assess word aggregation. This approach often fails to correctly segment words that are visually close yet semantically distinct, such as <span id="S4.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">Question</span> and <span id="S4.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">Answer</span> appearing on the same line. Additionally, the intricate form layouts of languages like Japanese and Italian pose further challenges in precise text-line segmentation. In contrast, document pre-trained language models that integrate multiple modalities significantly improve performance by leveraging a broader range of data, thereby overcoming the limitations of single-modal systems.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Text-line to entity merging</h4>

<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of the text-line to entity merging task, using F1-score as the metric.</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:109.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.8pt,26.4pt) scale(0.674092462828124,0.674092462828124) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Type</th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Chinese</th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Japanese</th>
<th id="S4.T4.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
<th id="S4.T4.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">French</th>
<th id="S4.T4.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Spanish</th>
<th id="S4.T4.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Italian</th>
<th id="S4.T4.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Portuguese</th>
<th id="S4.T4.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<th id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T4.1.1.2.1.1.1" class="ltx_text">Vision-only</span></th>
<th id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.7415</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.7243</td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.5891</td>
<td id="S4.T4.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.7309</td>
<td id="S4.T4.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.6504</td>
<td id="S4.T4.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.7449</td>
<td id="S4.T4.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">0.6238</td>
<td id="S4.T4.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">0.6594</td>
<td id="S4.T4.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">0.6830</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<th id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cascade-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.7918</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.8336</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.6873</td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.7997</td>
<td id="S4.T4.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">0.8060</td>
<td id="S4.T4.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">0.8138</td>
<td id="S4.T4.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">0.7153</td>
<td id="S4.T4.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">0.7560</td>
<td id="S4.T4.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">0.7754</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<th id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.7681</td>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.7794</td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.6332</td>
<td id="S4.T4.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">0.7893</td>
<td id="S4.T4.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.7344</td>
<td id="S4.T4.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">0.7663</td>
<td id="S4.T4.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.7075</td>
<td id="S4.T4.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">0.7346</td>
<td id="S4.T4.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">0.7391</td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<th id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T4.1.1.5.4.1.1" class="ltx_text">Text-only</span></th>
<th id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">XLM-RoBerta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.8767</td>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.9354</td>
<td id="S4.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.8974</td>
<td id="S4.T4.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">0.8850</td>
<td id="S4.T4.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">0.9014</td>
<td id="S4.T4.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t">0.9044</td>
<td id="S4.T4.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t">0.8836</td>
<td id="S4.T4.1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t">0.9226</td>
<td id="S4.T4.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_t">0.9029</td>
</tr>
<tr id="S4.T4.1.1.6.5" class="ltx_tr">
<th id="S4.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">InfoXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T4.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">0.8773</td>
<td id="S4.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.9411</td>
<td id="S4.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.8921</td>
<td id="S4.T4.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">0.8729</td>
<td id="S4.T4.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">0.9010</td>
<td id="S4.T4.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t">0.9026</td>
<td id="S4.T4.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t">0.8847</td>
<td id="S4.T4.1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t">0.9188</td>
<td id="S4.T4.1.1.6.5.10" class="ltx_td ltx_align_center ltx_border_t">0.9012</td>
</tr>
<tr id="S4.T4.1.1.7.6" class="ltx_tr">
<th id="S4.T4.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T4.1.1.7.6.1.1" class="ltx_text">Multi-modal</span></th>
<th id="S4.T4.1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T4.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t">0.9151</td>
<td id="S4.T4.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.4.1" class="ltx_text ltx_font_bold">0.9681</span></td>
<td id="S4.T4.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.5.1" class="ltx_text ltx_font_bold">0.9387</span></td>
<td id="S4.T4.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.6.1" class="ltx_text ltx_font_bold">0.9157</span></td>
<td id="S4.T4.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.7.1" class="ltx_text ltx_font_bold">0.9408</span></td>
<td id="S4.T4.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.8.1" class="ltx_text ltx_font_bold">0.9463</span></td>
<td id="S4.T4.1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.9.1" class="ltx_text ltx_font_bold">0.9280</span></td>
<td id="S4.T4.1.1.7.6.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.10.1" class="ltx_text ltx_font_bold">0.9594</span></td>
<td id="S4.T4.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.11.1" class="ltx_text ltx_font_bold">0.9412</span></td>
</tr>
<tr id="S4.T4.1.1.8.7" class="ltx_tr">
<th id="S4.T4.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LiLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T4.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">0.9047</td>
<td id="S4.T4.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">0.9542</td>
<td id="S4.T4.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0.9117</td>
<td id="S4.T4.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">0.9140</td>
<td id="S4.T4.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">0.9368</td>
<td id="S4.T4.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">0.9351</td>
<td id="S4.T4.1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_t">0.9134</td>
<td id="S4.T4.1.1.8.7.9" class="ltx_td ltx_align_center ltx_border_t">0.9430</td>
<td id="S4.T4.1.1.8.7.10" class="ltx_td ltx_align_center ltx_border_t">0.9283</td>
</tr>
<tr id="S4.T4.1.1.9.8" class="ltx_tr">
<th id="S4.T4.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">GraphDoc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T4.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.9.8.2.1" class="ltx_text ltx_font_bold">0.9229</span></td>
<td id="S4.T4.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9343</td>
<td id="S4.T4.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8770</td>
<td id="S4.T4.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9113</td>
<td id="S4.T4.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9260</td>
<td id="S4.T4.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9326</td>
<td id="S4.T4.1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9060</td>
<td id="S4.T4.1.1.9.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9314</td>
<td id="S4.T4.1.1.9.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9181</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p"><span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">The task of merging text lines into entities relies more on semantic information.</span> Compared to task 1, we can observe from Table <a href="#S4.T4" title="Table 4 ‣ 4.2.2 Text-line to entity merging ‣ 4.2 Results and analysis ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> that the performance of text-only models surpasses that of vision-only models. This indicates that pure vision models have a weaker capability in understanding multi-line entities, while linguistic information significantly aids in capturing the semantic continuity between different text lines.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">The pre-training process greatly impacts the effectiveness of document pre-trained language models.</span> The GraphDoc model, which leverages sentence-level semantic information and is only pre-trained on English documents, performs well in the task of merging text lines in English forms. In contrast, LayoutXLM is pre-trained using documents in all languages included in the SRFUND dataset, which allows it to demonstrate superior performance on forms in other languages.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Entity category classification</h4>

<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results of the entity category classification task, using F1-score as the metric.</figcaption>
<div id="S4.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:109.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.8pt,26.4pt) scale(0.674092462828124,0.674092462828124) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Type</th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Chinese</th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Japanese</th>
<th id="S4.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
<th id="S4.T5.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">French</th>
<th id="S4.T5.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Spanish</th>
<th id="S4.T5.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Italian</th>
<th id="S4.T5.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Portuguese</th>
<th id="S4.T5.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T5.1.1.2.1.1.1" class="ltx_text">Vision-only</span></th>
<th id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.5284</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.6040</td>
<td id="S4.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.4619</td>
<td id="S4.T5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.4976</td>
<td id="S4.T5.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.4743</td>
<td id="S4.T5.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.5385</td>
<td id="S4.T5.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">0.4466</td>
<td id="S4.T5.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">0.4244</td>
<td id="S4.T5.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">0.4969</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cascade-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.6739</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.7482</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.6124</td>
<td id="S4.T5.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.7123</td>
<td id="S4.T5.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">0.7749</td>
<td id="S4.T5.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">0.7318</td>
<td id="S4.T5.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">0.6662</td>
<td id="S4.T5.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">0.6707</td>
<td id="S4.T5.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">0.6988</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.6531</td>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.6631</td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.5286</td>
<td id="S4.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">0.6735</td>
<td id="S4.T5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.6863</td>
<td id="S4.T5.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">0.6574</td>
<td id="S4.T5.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.6067</td>
<td id="S4.T5.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">0.6152</td>
<td id="S4.T5.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">0.6355</td>
</tr>
<tr id="S4.T5.1.1.5.4" class="ltx_tr">
<th id="S4.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.5.4.1.1" class="ltx_text">Text-only</span></th>
<th id="S4.T5.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">XLM-RoBerta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.8558</td>
<td id="S4.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.9666</td>
<td id="S4.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.8847</td>
<td id="S4.T5.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">0.8912</td>
<td id="S4.T5.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">0.9067</td>
<td id="S4.T5.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t">0.9161</td>
<td id="S4.T5.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t">0.8955</td>
<td id="S4.T5.1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t">0.8884</td>
<td id="S4.T5.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_t">0.9028</td>
</tr>
<tr id="S4.T5.1.1.6.5" class="ltx_tr">
<th id="S4.T5.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">InfoXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">0.8589</td>
<td id="S4.T5.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.9570</td>
<td id="S4.T5.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.8782</td>
<td id="S4.T5.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">0.8953</td>
<td id="S4.T5.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">0.9107</td>
<td id="S4.T5.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t">0.9221</td>
<td id="S4.T5.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t">0.8995</td>
<td id="S4.T5.1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t">0.8840</td>
<td id="S4.T5.1.1.6.5.10" class="ltx_td ltx_align_center ltx_border_t">0.9025</td>
</tr>
<tr id="S4.T5.1.1.7.6" class="ltx_tr">
<th id="S4.T5.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T5.1.1.7.6.1.1" class="ltx_text">Multi-modal</span></th>
<th id="S4.T5.1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.3.1" class="ltx_text ltx_font_bold">0.9045</span></td>
<td id="S4.T5.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.4.1" class="ltx_text ltx_font_bold">0.9718</span></td>
<td id="S4.T5.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.5.1" class="ltx_text ltx_font_bold">0.8957</span></td>
<td id="S4.T5.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t">0.9216</td>
<td id="S4.T5.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.7.1" class="ltx_text ltx_font_bold">0.9299</span></td>
<td id="S4.T5.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.8.1" class="ltx_text ltx_font_bold">0.9320</span></td>
<td id="S4.T5.1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.9.1" class="ltx_text ltx_font_bold">0.9269</span></td>
<td id="S4.T5.1.1.7.6.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.10.1" class="ltx_text ltx_font_bold">0.9086</span></td>
<td id="S4.T5.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.7.6.11.1" class="ltx_text ltx_font_bold">0.9248</span></td>
</tr>
<tr id="S4.T5.1.1.8.7" class="ltx_tr">
<th id="S4.T5.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LiLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">0.8678</td>
<td id="S4.T5.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">0.9631</td>
<td id="S4.T5.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0.8876</td>
<td id="S4.T5.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">0.9006</td>
<td id="S4.T5.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">0.9217</td>
<td id="S4.T5.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">0.9270</td>
<td id="S4.T5.1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_t">0.9135</td>
<td id="S4.T5.1.1.8.7.9" class="ltx_td ltx_align_center ltx_border_t">0.8967</td>
<td id="S4.T5.1.1.8.7.10" class="ltx_td ltx_align_center ltx_border_t">0.9118</td>
</tr>
<tr id="S4.T5.1.1.9.8" class="ltx_tr">
<th id="S4.T5.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">GraphDoc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T5.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8930</td>
<td id="S4.T5.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9619</td>
<td id="S4.T5.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8620</td>
<td id="S4.T5.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T5.1.1.9.8.5.1" class="ltx_text ltx_font_bold">0.9261</span></td>
<td id="S4.T5.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9129</td>
<td id="S4.T5.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9250</td>
<td id="S4.T5.1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9169</td>
<td id="S4.T5.1.1.9.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.8897</td>
<td id="S4.T5.1.1.9.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.9113</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p"><span id="S4.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Visual modalities can also address the task of entity classification by learning layout information.</span> Apart from <span id="S4.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_italic">Header</span> entities, which typically exhibit characteristics such as boldface and larger font sizes, the variations in font styles among other entity categories are minimal. However, as shown in Table <a href="#S4.T5" title="Table 5 ‣ 4.2.3 Entity category classification ‣ 4.2 Results and analysis ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, detection models are also capable of effectively handling the detection tasks for various entity categories. This indicates that the visual modality can acquire layout information, such as <span id="S4.SS2.SSS3.p1.1.3" class="ltx_text ltx_font_italic">Question</span> typically being located to the left of <span id="S4.SS2.SSS3.p1.1.4" class="ltx_text ltx_font_italic">Answer</span>, and entities with multi-line texts usually being categorized as <span id="S4.SS2.SSS3.p1.1.5" class="ltx_text ltx_font_italic">Answer</span> or <span id="S4.SS2.SSS3.p1.1.6" class="ltx_text ltx_font_italic">Other</span>.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Item table localization</h4>

<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results of the item table localization task, using F1-score as the metric.</figcaption>
<div id="S4.T6.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:109.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.8pt,26.4pt) scale(0.674092462828124,0.674092462828124) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Type</th>
<th id="S4.T6.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="S4.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Chinese</th>
<th id="S4.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Japanese</th>
<th id="S4.T6.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
<th id="S4.T6.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">French</th>
<th id="S4.T6.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Spanish</th>
<th id="S4.T6.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Italian</th>
<th id="S4.T6.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Portuguese</th>
<th id="S4.T6.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.2.1" class="ltx_tr">
<th id="S4.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T6.1.1.2.1.1.1" class="ltx_text">Vision-only</span></th>
<th id="S4.T6.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.1100</td>
<td id="S4.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.1721</td>
<td id="S4.T6.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.0100</td>
<td id="S4.T6.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.0467</td>
<td id="S4.T6.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.1000</td>
<td id="S4.T6.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.0710</td>
<td id="S4.T6.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">0.0600</td>
<td id="S4.T6.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">0.0911</td>
<td id="S4.T6.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">0.0826</td>
</tr>
<tr id="S4.T6.1.1.3.2" class="ltx_tr">
<th id="S4.T6.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cascade-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T6.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.0839</td>
<td id="S4.T6.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.2081</td>
<td id="S4.T6.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.0433</td>
<td id="S4.T6.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.0800</td>
<td id="S4.T6.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">0.1327</td>
<td id="S4.T6.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.3.2.7.1" class="ltx_text ltx_font_bold">0.1427</span></td>
<td id="S4.T6.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">0.0817</td>
<td id="S4.T6.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.3.2.9.1" class="ltx_text ltx_font_bold">0.1486</span></td>
<td id="S4.T6.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">0.1151</td>
</tr>
<tr id="S4.T6.1.1.4.3" class="ltx_tr">
<th id="S4.T6.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T6.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.1399</td>
<td id="S4.T6.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.2670</td>
<td id="S4.T6.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">0.0667</td>
<td id="S4.T6.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.1333</td>
<td id="S4.T6.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">0.0903</td>
<td id="S4.T6.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.0767</td>
<td id="S4.T6.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">0.1100</td>
<td id="S4.T6.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">0.1105</td>
</tr>
<tr id="S4.T6.1.1.5.4" class="ltx_tr">
<th id="S4.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T6.1.1.5.4.1.1" class="ltx_text">Text-only</span></th>
<th id="S4.T6.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">XLM-RoBerta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T6.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.0526</td>
<td id="S4.T6.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.2090</td>
<td id="S4.T6.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.0800</td>
<td id="S4.T6.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.5.4.6.1" class="ltx_text ltx_font_bold">0.5714</span></td>
<td id="S4.T6.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">0.2222</td>
<td id="S4.T6.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t">0.1333</td>
<td id="S4.T6.1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t">0.0526</td>
<td id="S4.T6.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_t">0.1514</td>
</tr>
<tr id="S4.T6.1.1.6.5" class="ltx_tr">
<th id="S4.T6.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">InfoXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T6.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">0.0513</td>
<td id="S4.T6.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.1846</td>
<td id="S4.T6.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">0.4545</td>
<td id="S4.T6.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">0.2143</td>
<td id="S4.T6.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.6.5.10" class="ltx_td ltx_align_center ltx_border_t">0.1124</td>
</tr>
<tr id="S4.T6.1.1.7.6" class="ltx_tr">
<th id="S4.T6.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T6.1.1.7.6.1.1" class="ltx_text">Multi-modal</span></th>
<th id="S4.T6.1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T6.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.7.6.3.1" class="ltx_text ltx_font_bold">0.7273</span></td>
<td id="S4.T6.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.7.6.4.1" class="ltx_text ltx_font_bold">0.3333</span></td>
<td id="S4.T6.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.7.6.5.1" class="ltx_text ltx_font_bold">0.1053</span></td>
<td id="S4.T6.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t">0.4348</td>
<td id="S4.T6.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t">0.1053</td>
<td id="S4.T6.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_t">0.0588</td>
<td id="S4.T6.1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.7.6.9.1" class="ltx_text ltx_font_bold">0.3158</span></td>
<td id="S4.T6.1.1.7.6.10" class="ltx_td ltx_align_center ltx_border_t">0.1250</td>
<td id="S4.T6.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.7.6.11.1" class="ltx_text ltx_font_bold">0.3022</span></td>
</tr>
<tr id="S4.T6.1.1.8.7" class="ltx_tr">
<th id="S4.T6.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LiLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T6.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">0.2273</td>
<td id="S4.T6.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">0.1867</td>
<td id="S4.T6.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">0.5263</td>
<td id="S4.T6.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">0.0769</td>
<td id="S4.T6.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.8.7.9" class="ltx_td ltx_align_center ltx_border_t">0.0417</td>
<td id="S4.T6.1.1.8.7.10" class="ltx_td ltx_align_center ltx_border_t">0.1306</td>
</tr>
<tr id="S4.T6.1.1.9.8" class="ltx_tr">
<th id="S4.T6.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">GraphDoc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T6.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.0556</td>
<td id="S4.T6.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.3333</td>
<td id="S4.T6.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T6.1.1.9.8.6.1" class="ltx_text ltx_font_bold">0.3333</span></td>
<td id="S4.T6.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.0606</td>
<td id="S4.T6.1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.0000</td>
<td id="S4.T6.1.1.9.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.1224</td>
<td id="S4.T6.1.1.9.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.0945</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p"><span id="S4.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_bold">The item table localization task presents significant challenges.</span> Since this task requires that all entities within the item table be included, any missing entity predictions result in the outcome being deemed incomplete. This requirement makes it difficult for models of any modality to accurately locate item tables within documents, as shown in Table <a href="#S4.T6" title="Table 6 ‣ 4.2.4 Item table localization ‣ 4.2 Results and analysis ‣ 4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS2.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS4.p2.1" class="ltx_p"><span id="S4.SS2.SSS4.p2.1.1" class="ltx_text ltx_font_bold">There is significant variability in performance across forms in different languages.</span> Item table localization in forms of different languages requires optimization through models of various modalities. For some languages, the localization of item tables relies more heavily on the capabilities of vision models, such as in Spanish and Portuguese forms.</p>
</div>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Hierarchical structure recovery</h4>

<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results of the hierarchical structure recovery task, using F1-score as the metric.</figcaption>
<div id="S4.T7.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:74.2pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-101.9pt,17.3pt) scale(0.680320483948248,0.680320483948248) ;">
<table id="S4.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Type</th>
<th id="S4.T7.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="S4.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Chinese</th>
<th id="S4.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Japanese</th>
<th id="S4.T7.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
<th id="S4.T7.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">French</th>
<th id="S4.T7.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Spanish</th>
<th id="S4.T7.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Italian</th>
<th id="S4.T7.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Portuguese</th>
<th id="S4.T7.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.1.2.1" class="ltx_tr">
<th id="S4.T7.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T7.1.1.2.1.1.1" class="ltx_text">Text-only</span></th>
<th id="S4.T7.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">XLM-RoBerta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T7.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.5270</td>
<td id="S4.T7.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.6514</td>
<td id="S4.T7.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.5388</td>
<td id="S4.T7.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.6637</td>
<td id="S4.T7.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.6054</td>
<td id="S4.T7.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.6121</td>
<td id="S4.T7.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">0.5839</td>
<td id="S4.T7.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">0.5081</td>
<td id="S4.T7.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">0.5830</td>
</tr>
<tr id="S4.T7.1.1.3.2" class="ltx_tr">
<th id="S4.T7.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">InfoXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T7.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.5305</td>
<td id="S4.T7.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.6436</td>
<td id="S4.T7.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.5227</td>
<td id="S4.T7.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.6695</td>
<td id="S4.T7.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">0.6071</td>
<td id="S4.T7.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">0.5941</td>
<td id="S4.T7.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">0.5736</td>
<td id="S4.T7.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">0.4872</td>
<td id="S4.T7.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">0.5732</td>
</tr>
<tr id="S4.T7.1.1.4.3" class="ltx_tr">
<th id="S4.T7.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T7.1.1.4.3.1.1" class="ltx_text">Multi-modal</span></th>
<th id="S4.T7.1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LayoutXLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T7.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.7135</td>
<td id="S4.T7.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.7601</td>
<td id="S4.T7.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">0.6626</td>
<td id="S4.T7.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.7734</td>
<td id="S4.T7.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">0.7415</td>
<td id="S4.T7.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.7009</td>
<td id="S4.T7.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">0.6710</td>
<td id="S4.T7.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">0.6310</td>
<td id="S4.T7.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t">0.7013</td>
</tr>
<tr id="S4.T7.1.1.5.4" class="ltx_tr">
<th id="S4.T7.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LiLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T7.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">0.7050</td>
<td id="S4.T7.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.7578</td>
<td id="S4.T7.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.6538</td>
<td id="S4.T7.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.7499</td>
<td id="S4.T7.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">0.7153</td>
<td id="S4.T7.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">0.6940</td>
<td id="S4.T7.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t">0.6702</td>
<td id="S4.T7.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t">0.5747</td>
<td id="S4.T7.1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t">0.6821</td>
</tr>
<tr id="S4.T7.1.1.6.5" class="ltx_tr">
<th id="S4.T7.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">GraphDoc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T7.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.2.1" class="ltx_text ltx_font_bold">0.7938</span></td>
<td id="S4.T7.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.3.1" class="ltx_text ltx_font_bold">0.7881</span></td>
<td id="S4.T7.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.4.1" class="ltx_text ltx_font_bold">0.6714</span></td>
<td id="S4.T7.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.5.1" class="ltx_text ltx_font_bold">0.7976</span></td>
<td id="S4.T7.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.6.1" class="ltx_text ltx_font_bold">0.7754</span></td>
<td id="S4.T7.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.7.1" class="ltx_text ltx_font_bold">0.7416</span></td>
<td id="S4.T7.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.8.1" class="ltx_text ltx_font_bold">0.6969</span></td>
<td id="S4.T7.1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.9.1" class="ltx_text ltx_font_bold">0.6648</span></td>
<td id="S4.T7.1.1.6.5.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.1.1.6.5.10.1" class="ltx_text ltx_font_bold">0.7349</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p"><span id="S4.SS2.SSS5.p1.1.1" class="ltx_text ltx_font_bold">The granularity of semantic information must align with the task requirements.</span> Among the three document pre-trained language models, only the GraphDoc model maintains sentence-level or entity-level linguistic input during both the pre-training stage and the training/testing phases of the current task. This alignment enables the GraphDoc model to achieve the best performance in the task of entity-based document structure recovery.</p>
</div>
</section>
<section id="S4.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.6 </span>Overall analysis</h4>

<div id="S4.SS2.SSS6.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS6.p1.1" class="ltx_p">The results from the aforementioned five tasks demonstrate that uni-modal models exhibit relatively poor performance, while document pre-trained models that incorporate multiple input modalities perform significantly better. Concurrently, no single model has consistently outperformed others across all tasks and languages, indicating that in practical applications, we cannot simply rely on a single model or approach to handle all types of form structuring tasks. Instead, we need to select appropriate models and strategies based on the specific requirements of the task and the characteristics of the language involved. This finding underscores the importance of adopting a nuanced and tailored approach when tackling form structuring challenges, rather than employing a one-size-fits-all solution.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In summary, this paper presents two main contributions. Firstly, we propose a multilingual, multitask document hierarchical structuring benchmark named SRFUND, encompassing 1,592 forms from eight languages. To the best of our knowledge, this is the first benchmark in form understanding that integrates multi-level structure reconstruction, spanning from words to the global structure of forms. Secondly, we conducted baseline experiments on five tasks using various representative approaches from different modalities, demonstrating that the SRFUND benchmark introduces new challenges to the field of form understanding. We believe that the SRFUND benchmark holds significant potential for future academic research, contributing continuously to the in-depth study of global form structures.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and
R Manmatha.

</span>
<span class="ltx_bibblock">Docformer: End-to-end transformer for document understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 993–1003, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Youngmin Baek, Daehyun Nam, Sungrae Park, Junyeop Lee, Seung Shin, Jeonghun
Baek, Chae Young Lee, and Hwalsuk Lee.

</span>
<span class="ltx_bibblock">Cleval: Character-level evaluation for text detection and recognition
tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 564–565, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Zhaowei Cai and Nuno Vasconcelos.

</span>
<span class="ltx_bibblock">Cascade r-cnn: Delving into high quality object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 6154–6162, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 213–229,
2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li,
Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng,
Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,
Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and
Dahua Lin.

</span>
<span class="ltx_bibblock">MMDetection: Open mmlab detection toolbox and benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.07155</span>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song,
Xian-Ling Mao, He-Yan Huang, and Ming Zhou.

</span>
<span class="ltx_bibblock">Infoxlm: An information-theoretic framework for cross-lingual
language model pre-training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</span>, pages 3576–3588, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Alexis Conneau and Guillaume Lample.

</span>
<span class="ltx_bibblock">Cross-lingual language model pretraining.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 248–255, 2009.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D Derrien-Peden.

</span>
<span class="ltx_bibblock">Frame-based system for macrotypographical structure analysis in
scientific papers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of 1st International Conference on Document
Analysis and Recognition</span>, pages 311–319, 1991.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Timothy Dozat and Christopher D Manning.

</span>
<span class="ltx_bibblock">Deep biaffine attention for neural dependency parsing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.

</span>
<span class="ltx_bibblock">Yolox: Exceeding yolo series in 2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.08430</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jaekyu Ha, Robert M Haralick, and Ihsin T Phillips.

</span>
<span class="ltx_bibblock">Recursive xy cut using bounding boxes of connected components.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of 3rd International Conference on Document
Analysis and Recognition</span>, pages 952–955, 1995.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Haralick.

</span>
<span class="ltx_bibblock">Document image understanding: Geometric and logical layout.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 385–390, 1994.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 2961–2969, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu,
and CV Jawahar.

</span>
<span class="ltx_bibblock">Icdar2019 competition on scanned receipt ocr and information
extraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of 15th International Conference on Document
Analysis and Recognition</span>, pages 1516–1520, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran.

</span>
<span class="ltx_bibblock">Funsd: A dataset for form understanding in noisy scanned documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2019 International Conference on Document Analysis and
Recognition Workshops</span>, pages 1–6, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Saima Khan, Shazia Khan, and Mohsina Aftab.

</span>
<span class="ltx_bibblock">Digitization and its impact on economy.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International Journal of Digital Library Services</span>,
5(2):138–149, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Koichi Kise, Akinori Sato, and Motoi Iwata.

</span>
<span class="ltx_bibblock">Segmentation of page images using the area voronoi diagram.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, 70(3):370–382, 1998.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Mukkai Krishnamoorthy, George Nagy, Sharad Seth, and Mahesh Viswanathan.

</span>
<span class="ltx_bibblock">Syntactic segmentation and labeling of digitized pages from technical
journals.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
15(7):737–747, 1993.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo
Si.

</span>
<span class="ltx_bibblock">Structurallm: Structural pre-training for form understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing</span>, pages 6309–6318, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang.

</span>
<span class="ltx_bibblock">Show, attend and read: A simple and strong baseline for irregular
text recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, pages 8610–8617, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kai Li, Curtis Wigington, Chris Tensmeyer, Handong Zhao, Nikolaos Barmpalios,
Vlad I Morariu, Varun Manjunatha, Tong Sun, and Yun Fu.

</span>
<span class="ltx_bibblock">Cross-domain document object detection: Benchmark suite and method.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 12915–12924, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun
Yao, Junyu Han, Jingtuo Liu, and Errui Ding.

</span>
<span class="ltx_bibblock">Structext: Structured text understanding with multi-modal
transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</span>, pages 1912–1920, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and
Lei Zhang.

</span>
<span class="ltx_bibblock">Dab-detr: Dynamic anchor boxes are better queries for detr.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 3431–3440, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Chuwei Luo, Changxu Cheng, Qi Zheng, and Cong Yao.

</span>
<span class="ltx_bibblock">Geolayoutlm: Geometric pre-training for visual information
extraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7092–7101, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen Kan.

</span>
<span class="ltx_bibblock">Logical structure recovery in scholarly articles with rich document
features.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Multimedia Storage and Retrieval Innovations for Digital
Library Systems</span>, pages 270–292. 2012.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Anoop M Namboodiri and Anil K Jain.

</span>
<span class="ltx_bibblock">Document structure and layout analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Digital Document Processing: Major Directions and Recent
Advances</span>, pages 29–48. 2007.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lawrence O’Gorman.

</span>
<span class="ltx_bibblock">The document spectrum for page layout analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
15(11):1162–1173, 1993.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Dario Augusto Borges Oliveira and Matheus Palhares Viana.

</span>
<span class="ltx_bibblock">Fast cnn-based document layout analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span>, pages 1173–1180, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo,
and Hwalsuk Lee.

</span>
<span class="ltx_bibblock">Cord: a consolidated receipt dataset for post-ocr parsing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Workshop on Document Intelligence at NeurIPS 2019</span>, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita
Sultanpure.

</span>
<span class="ltx_bibblock">Cascadetabnet: An approach for end to end table detection and
structure recognition from image-based documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 572–573, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jiapeng Wang, Lianwen Jin, and Kai Ding.

</span>
<span class="ltx_bibblock">Lilt: A simple yet effective language-independent layout transformer
for structured document understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics</span>, pages 7747–7757, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang, Shuaitao
Zhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai.

</span>
<span class="ltx_bibblock">Towards robust visual information extraction in real world: new
dataset and novel solution.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 35, pages 2738–2745, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yan Wang, Jun Du, Jiefeng Ma, Pengfei Hu, Zhenrong Zhang, and Jianshu Zhang.

</span>
<span class="ltx_bibblock">Ustc-iflytek at docile: a multi-modal approach using domain-specific
graphdoc.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Working Notes of CLEF</span>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang.

</span>
<span class="ltx_bibblock">Docstruct: A multimodal method to extract hierarchy structure in
document for general form understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</span>, pages 898–908, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.

</span>
<span class="ltx_bibblock">Layoutlm: Pre-training of text and layout for document image
understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</span>, pages 1192–1200, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha
Zhang, and Furu Wei.

</span>
<span class="ltx_bibblock">Xfund: a benchmark dataset for multilingual visually rich form
understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022</span>, pages 3214–3224, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yue Xu, Fei Yin, Zhaoxiang Zhang, and Cheng-Lin Liu.

</span>
<span class="ltx_bibblock">Multi-task layout analysis for historical handwritten documents using
fully convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th International Joint Conference on
Artificial Intelligence</span>, pages 1057–1063, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, and
C Lee Giles.

</span>
<span class="ltx_bibblock">Learning to extract semantic structure from documents using
multimodal fully convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5315–5324, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zhibo Yang, Rujiao Long, Pengfei Wang, Sibo Song, Humen Zhong, Wenqing Cheng,
Xiang Bai, and Cong Yao.

</span>
<span class="ltx_bibblock">Modeling entities as semantic points for visual information
extraction in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 15358–15367, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zhenrong Zhang, Jiefeng Ma, Jun Du, Licheng Wang, and Jianshu Zhang.

</span>
<span class="ltx_bibblock">Multimodal pre-training based on graph attention network for document
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Multimedia</span>, 25:6743–6755, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yefeng Zheng, Huiping Li, and David Doermann.

</span>
<span class="ltx_bibblock">A model-based line detection algorithm in documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Proceedings of 7th International Conference on Document
Analysis and Recognition</span>, pages 44–48, 2003.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Checklist</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<ol id="Sx1.I1" class="ltx_enumerate">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">For all authors…</p>
<ol id="Sx1.I1.i1.I1" class="ltx_enumerate">
<li id="Sx1.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="Sx1.I1.i1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.I1.i1.p1.1" class="ltx_p">Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?
<span id="Sx1.I1.i1.I1.i1.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li id="Sx1.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="Sx1.I1.i1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i1.I1.i2.p1.1" class="ltx_p">Did you describe the limitations of your work?
<span id="Sx1.I1.i1.I1.i2.p1.1.1" class="ltx_text" style="color:#FF8000;">[No] </span></p>
</div>
</li>
<li id="Sx1.I1.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="Sx1.I1.i1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i1.I1.i3.p1.1" class="ltx_p">Did you discuss any potential negative societal impacts of your work?
<span id="Sx1.I1.i1.I1.i3.p1.1.1" class="ltx_text" style="color:#FF8000;">[No] </span></p>
</div>
</li>
<li id="Sx1.I1.i1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="Sx1.I1.i1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i1.I1.i4.p1.1" class="ltx_p">Have you read the ethics review guidelines and ensured that your paper conforms to them?
<span id="Sx1.I1.i1.I1.i4.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">If you are including theoretical results…</p>
<ol id="Sx1.I1.i2.I1" class="ltx_enumerate">
<li id="Sx1.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="Sx1.I1.i2.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i2.I1.i1.p1.1" class="ltx_p">Did you state the full set of assumptions of all theoretical results?
<span id="Sx1.I1.i2.I1.i1.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span>We don’t have theoretical results.</p>
</div>
</li>
<li id="Sx1.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="Sx1.I1.i2.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i2.I1.i2.p1.1" class="ltx_p">Did you include complete proofs of all theoretical results?
<span id="Sx1.I1.i2.I1.i2.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span>We don’t have theorems.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">If you ran experiments (e.g. for benchmarks)…</p>
<ol id="Sx1.I1.i3.I1" class="ltx_enumerate">
<li id="Sx1.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="Sx1.I1.i3.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i3.I1.i1.p1.1" class="ltx_p">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
<span id="Sx1.I1.i3.I1.i1.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span>As a URL</p>
</div>
</li>
<li id="Sx1.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="Sx1.I1.i3.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i3.I1.i2.p1.1" class="ltx_p">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
<span id="Sx1.I1.i3.I1.i2.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span>In Sec. <a href="#S4" title="4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></p>
</div>
</li>
<li id="Sx1.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="Sx1.I1.i3.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.I1.i3.p1.1" class="ltx_p">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
<span id="Sx1.I1.i3.I1.i3.p1.1.1" class="ltx_text" style="color:#FF8000;">[No] </span></p>
</div>
</li>
<li id="Sx1.I1.i3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="Sx1.I1.i3.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i3.I1.i4.p1.1" class="ltx_p">Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
<span id="Sx1.I1.i3.I1.i4.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span>In Sec. <a href="#S4" title="4 Experiments ‣ SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></p>
</div>
</li>
</ol>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">If you are using existing assets (e.g., code, data, models) or curating/releasing new assets…</p>
<ol id="Sx1.I1.i4.I1" class="ltx_enumerate">
<li id="Sx1.I1.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="Sx1.I1.i4.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i4.I1.i1.p1.1" class="ltx_p">If your work uses existing assets, did you cite the creators?
<span id="Sx1.I1.i4.I1.i1.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li id="Sx1.I1.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="Sx1.I1.i4.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i4.I1.i2.p1.1" class="ltx_p">Did you mention the license of the assets?
<span id="Sx1.I1.i4.I1.i2.p1.1.1" class="ltx_text" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li id="Sx1.I1.i4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="Sx1.I1.i4.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i4.I1.i3.p1.1" class="ltx_p">Did you include any new assets either in the supplemental material or as a URL?
<span id="Sx1.I1.i4.I1.i3.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li id="Sx1.I1.i4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="Sx1.I1.i4.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.I1.i4.p1.1" class="ltx_p">Did you discuss whether and how consent was obtained from people whose data you’re using/curating?
<span id="Sx1.I1.i4.I1.i4.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li id="Sx1.I1.i4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(e)</span> 
<div id="Sx1.I1.i4.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i4.I1.i5.p1.1" class="ltx_p">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
<span id="Sx1.I1.i4.I1.i5.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="Sx1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="Sx1.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i5.p1.1" class="ltx_p">If you used crowdsourcing or conducted research with human subjects…</p>
<ol id="Sx1.I1.i5.I1" class="ltx_enumerate">
<li id="Sx1.I1.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="Sx1.I1.i5.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i5.I1.i1.p1.1" class="ltx_p">Did you include the full text of instructions given to participants and screenshots, if applicable?
<span id="Sx1.I1.i5.I1.i1.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li id="Sx1.I1.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="Sx1.I1.i5.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i5.I1.i2.p1.1" class="ltx_p">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
<span id="Sx1.I1.i5.I1.i2.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span></p>
</div>
</li>
<li id="Sx1.I1.i5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="Sx1.I1.i5.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i5.I1.i3.p1.1" class="ltx_p">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
<span id="Sx1.I1.i5.I1.i3.p1.1.1" class="ltx_text" style="color:#808080;">[N/A] </span></p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.08756" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.08757" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.08757">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.08757" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.08758" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 22:24:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
