<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Large Language Models as Narrative-Driven Recommenders</title>
<!--Generated on Thu Oct 17 14:05:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender systems,  Large language models,  Narrative-driven recommendations,  Movie recommendations,  Prompting strategies" lang="en" name="keywords"/>
<base href="/html/2410.13604v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>LLMs as Recommender Systems</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.SS1" title="In 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.SS2" title="In 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Prompting Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.SS3" title="In 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Evaluation of LLM Responses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.SS1" title="In 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Format Adherence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.SS2" title="In 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Recommendation Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.SS3" title="In 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Addressing Potential Data Leakage</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S4" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S5" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Further Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S6" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1" title="In Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Large Language Models as Narrative-Driven Recommenders</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lukas Eberhard
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Graz University of Technology</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Graz</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Austria</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:lukas.eberhard@tugraz.at">lukas.eberhard@tugraz.at</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thorsten Ruprechter
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Graz University of Technology</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Graz</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Austria</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ruprechter@tugraz.at">ruprechter@tugraz.at</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denis Helic
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Graz University of Technology</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Graz</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Austria</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dhelic@tugraz.at">dhelic@tugraz.at</a>
</span></span></span>
</div>
<div class="ltx_dates">(2025)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">Narrative-driven recommenders aim to provide personalized suggestions for user requests expressed in free-form text such as “<span class="ltx_text ltx_font_italic" id="id10.id1.1">I want to watch a thriller with a mind-bending story, like Shutter Island.</span>”
Although large language models (LLMs) have been shown to excel in processing general natural language queries, their effectiveness for handling such recommendation requests remains relatively unexplored.
To close this gap, we compare the performance of 38 open- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in a movie recommendation setting.
For this, we utilize a gold-standard, crowdworker-annotated dataset of posts from reddit’s movie suggestion community and employ various prompting strategies, including zero-shot, identity, and few-shot prompting.
Our findings demonstrate the ability of LLMs to generate contextually relevant movie recommendations, significantly outperforming other state-of-the-art approaches, such as <span class="ltx_text ltx_font_typewriter" id="id10.id1.2">doc2vec</span>.
While we find that closed-source and large-parameterized models generally perform best, medium-sized open-source models remain competitive, being only slightly outperformed by their more computationally expensive counterparts.
Furthermore, we observe no significant differences across prompting strategies for most models, underscoring the effectiveness of simple approaches such as zero-shot prompting for narrative-driven recommendations.
Overall, this work offers valuable insights for recommender system researchers as well as practitioners aiming to integrate LLMs into real-world recommendation tools.</p>
</div>
<div class="ltx_keywords">Recommender systems, Large language models, Narrative-driven recommendations, Movie recommendations, Prompting strategies
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Under Review; 2025; Anonymous Location</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recently, tools powered by large language models (LLMs), such as OpenAI’s chatbot ChatGPT, gained attention due to their high performance in various natural language processing (NLP) tasks. For example, LLMs showed increased performance on tasks such as translation, question-answering, cloze tests <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib10" title="">2020</a>)</cite>,
linguistic analyses of generated content <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib22" title="">2023</a>)</cite>, or re-ranking tasks <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib53" title="">2023</a>)</cite>.
In some initial studies related to recommender systems, LLMs also demonstrated great potential for specific recommendation tasks, such as explanation <cite class="ltx_cite ltx_citemacro_citep">(Lubos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib35" title="">2024</a>)</cite>, ranking <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib12" title="">2023</a>)</cite>, as well as conversational <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib24" title="">2023</a>)</cite>, content-based <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib34" title="">2024</a>)</cite>, or next-item recommendations <cite class="ltx_cite ltx_citemacro_citep">(Wang and Lim, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib56" title="">2023</a>)</cite>.
However, suitability of LLMs for a particularly promising narrative-driven recommendation scenario <cite class="ltx_cite ltx_citemacro_citep">(Bogers and Koolen, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib9" title="">2017</a>)</cite> remains still relatively unexplored.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In narrative-driven recommendation scenarios users pose free-form requests such as “<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">I just want to see a movie where the good guy kicks some ass!</span>”
These queries are commonly submitted to online forums such as reddit, where communities respond with tailored suggestions (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, right).
While asynchronous community responses to such posts generally fulfill users’ requests, recent studies applying traditional machine learning methods to such recommendation scenarios indicate potential for improvement, as the problem of narrative-driven recommendations proves difficult for existing recommender approaches <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib18" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib17" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib16" title="">2024</a>)</cite>.
Given these challenges, in this paper, we set out to evaluate the potential of LLMs in such a recommendation scenario, leveraging their capabilities to better understand and process user-generated narratives.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">To this end, we probe LLMs with movie recommendation requests originally posed by real users to the r/MovieSuggestions <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reddit.com/r/MovieSuggestions/" title="">https://www.reddit.com/r/MovieSuggestions/</a></span></span></span> community on reddit (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, center).
Utilizing these user requests along with the accompanying comments that contain community recommendations <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib18" title="">2019</a>)</cite>, we investigate how well responses by open- and closed-source LLMs match the recommendations from the reddit community.
In total, we evaluate 38 state-of-the-art LLMs, categorized by size, ranging from tiny (<math alttext="{&lt;}4" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml"></mi><mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><lt id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">absent</csymbol><cn id="S1.p3.1.m1.1.1.3.cmml" type="integer" xref="S1.p3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">{&lt;}4</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">&lt; 4</annotation></semantics></math> billion parameters) to large (<math alttext="{\geq}50" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mrow id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml"><mi id="S1.p3.2.m2.1.1.2" xref="S1.p3.2.m2.1.1.2.cmml"></mi><mo id="S1.p3.2.m2.1.1.1" xref="S1.p3.2.m2.1.1.1.cmml">≥</mo><mn id="S1.p3.2.m2.1.1.3" xref="S1.p3.2.m2.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><apply id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1"><geq id="S1.p3.2.m2.1.1.1.cmml" xref="S1.p3.2.m2.1.1.1"></geq><csymbol cd="latexml" id="S1.p3.2.m2.1.1.2.cmml" xref="S1.p3.2.m2.1.1.2">absent</csymbol><cn id="S1.p3.2.m2.1.1.3.cmml" type="integer" xref="S1.p3.2.m2.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">{\geq}50</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">≥ 50</annotation></semantics></math> billion parameters), employing various prompting strategies. Specifically, we assess the performance of these models using zero-shot <cite class="ltx_cite ltx_citemacro_citep">(Larochelle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib31" title="">2008</a>)</cite>, identity <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib30" title="">2024</a>)</cite>, and few-shot <cite class="ltx_cite ltx_citemacro_citep">(Fei-Fei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib19" title="">2006</a>; Fink, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib20" title="">2004</a>)</cite> prompting (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, left).
Finally, we compare the performance of the evaluated LLMs with other recommendation approaches, such as <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.1">doc2vec</span> <cite class="ltx_cite ltx_citemacro_citep">(Le and Mikolov, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib32" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our results demonstrate that LLMs can effectively respond to natural language prompts, translating user requests into relevant recommendations.
We present four key findings from our comprehensive evaluation of movie recommendation tasks. First, we observe that using basic zero-shot prompting, LLMs are able to generate movie recommendations that rival or surpass those of traditional and state-of-the-art recommender approaches.
In particular, GPT-4o as the overall best-performing LLM exhibits a recommendation performance 70% higher than the baseline.
Second, expanding the prompting techniques through identity or few-shot prompting does not significantly improve recommendation performance, underscoring the strength of LLMs when using simple zero-shot prompting for this task.
Third, we find that medium-sized open-source models (e.g., Gemma 2 27B) perform competitively with similarly sized closed-source models (e.g., GPT-3.5 Turbo) and even larger open-source models (e.g., Mistral Large 2 123B).
Finally, our findings hold even when accounting for potential data leakage, specifically the possibility that data used for our evaluation may have also been incorporated during the pre-training phase of certain LLMs.
Altogether, this work offers practical insights for both recommender system researchers and practitioners looking to integrate LLMs into real-world recommendation applications.
Finally, we make our code and employed datasets publicly available.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://osf.io/uc6r2/?view_only=813aec647d7344a7ab782da2978fb7dc" title="">https://osf.io/uc6r2/?view_only=813aec647d7344a7ab782da2978fb7dc</a></span></span></span></p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="327" id="S1.F1.g1" src="x1.png" width="830"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.8.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text ltx_font_bold" id="S1.F1.9.2" style="font-size:90%;">Evaluation of Movie Recommendations Using LLMs and Reddit Submissions.<span class="ltx_text ltx_font_medium" id="S1.F1.9.2.1">
We assess LLM movie recommendation by combining different prompt templates (left) into zero-shot (<span class="ltx_text ltx_font_italic" id="S1.F1.9.2.1.1">Task</span>), identity (<span class="ltx_text ltx_font_italic" id="S1.F1.9.2.1.2">Persona</span> and <span class="ltx_text ltx_font_italic" id="S1.F1.9.2.1.3">Task</span>), and few-shot (<span class="ltx_text ltx_font_italic" id="S1.F1.9.2.1.4">Task</span> and <span class="ltx_text ltx_font_italic" id="S1.F1.9.2.1.5">Examples</span>) prompting.
We utilize these different prompts* to produce suggestions for real movie requests submitted by reddit users (right) by generating these recommendations with various LLMs (center).
The LLM-generated recommendations (bottom center, dashed box) are compared to actual responses from reddit’s community (right, dashed boxes) to evaluate the performance of LLMs as narrative-driven movie recommenders.
</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_align_left ltx_figure_panel ltx_align_center" id="S1.F1.10"><span class="ltx_text ltx_font_bold" id="S1.F1.10.1" style="font-size:80%;">*<span class="ltx_text ltx_font_medium" id="S1.F1.10.1.1">We instruct the LLMs to exclude movies released after the <span class="ltx_text" id="S1.F1.10.1.1.1" style="color:#FF8000;">year</span> of the reddit request, to enforce that responses match our use case.</span></span></p>
</div>
</div>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>LLMs as Recommender Systems</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Setup</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.6"><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.6.1">Dataset.</span>
We use a crowdworker-curated dataset<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.17605/osf.io/ma2bj" title="">https://doi.org/10.17605/osf.io/ma2bj</a></span></span></span> for our experiments <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib18" title="">2019</a>)</cite>. This dataset consists of annotated submissions and comments from reddit <cite class="ltx_cite ltx_citemacro_citep">(Baumgartner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib8" title="">2020</a>)</cite>, more specifically from the subreddit r/MovieSuggestions.
In particular, this dataset includes reddit submission IDs, titles, and original texts, as well as movie titles, actor names, keywords, and genres that were annotated by crowdworkers.
Each recommendation request in the dataset contains one or more positively mentioned movies (i.e., examples of movies that the user liked before) as well as additional information, such as negatively mentioned movies (i.e., movies that the user did not like before), positively or negatively mentioned keywords (describing further aspects of the movies), and genres.
While the full dataset contains <math alttext="1\,480" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">1 480</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><cn id="S2.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1">1480</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">1\,480</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">1 480</annotation></semantics></math> submissions (from August 2011 to July 2017) making up test and training data, for fair comparison with prior work <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib16" title="">2024</a>)</cite> we solely utilize the test set of <math alttext="296" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mn id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">296</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><cn id="S2.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1">296</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">296</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">296</annotation></semantics></math> submissions (from November 2016 to July 2017), with <math alttext="778" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mn id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">778</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><cn id="S2.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S2.SS1.p1.3.m3.1.1">778</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">778</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">778</annotation></semantics></math> unique mentioned movies and <math alttext="4\,329" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mn id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">4 329</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><cn id="S2.SS1.p1.4.m4.1.1.cmml" type="integer" xref="S2.SS1.p1.4.m4.1.1">4329</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">4\,329</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">4 329</annotation></semantics></math> comments including over <math alttext="11\,000" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><mn id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">11 000</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><cn id="S2.SS1.p1.5.m5.1.1.cmml" type="integer" xref="S2.SS1.p1.5.m5.1.1">11000</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">11\,000</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">11 000</annotation></semantics></math> individual recommendations by reddit users and <math alttext="3\,593" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><mn id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">3 593</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><cn id="S2.SS1.p1.6.m6.1.1.cmml" type="integer" xref="S2.SS1.p1.6.m6.1.1">3593</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">3\,593</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">3 593</annotation></semantics></math> different movies as suggestions.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.4.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text ltx_font_bold" id="S2.T1.5.2" style="font-size:90%;">Evaluated LLMs.<span class="ltx_text ltx_font_medium" id="S2.T1.5.2.1">
We report size category, family, name, number of parameters, and relevant references for all 38 investigated models.
We always utilize the newest model versions as of June to September 2024.
</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.2.1">
<tr class="ltx_tr" id="S2.T1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.1.1.1">Model</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.2.1.1">Category</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.3.1">
<tr class="ltx_tr" id="S2.T1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1.1.1.1">Model</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1.2.1.1">Family</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.4.1">Model Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.1.1">
<tr class="ltx_tr" id="S2.T1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.2.1.1">Params.</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1.1">(<math alttext="\mathbf{10^{9}}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.2.cmml">𝟏𝟎</mn><mn id="S2.T1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.3.cmml">𝟗</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn id="S2.T1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.2">10</cn><cn id="S2.T1.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.1.m1.1c">\mathbf{10^{9}}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.1.m1.1d">bold_10 start_POSTSUPERSCRIPT bold_9 end_POSTSUPERSCRIPT</annotation></semantics></math>)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.1">Ref.</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1.1" rowspan="11"><span class="ltx_text" id="S2.T1.1.2.1.1.1">Tiny</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1.2">Gemma</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1.3">Gemma 2B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4">2.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.5"><cite class="ltx_cite ltx_citemacro_citep">(Team et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib54" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.1">Gemma 2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.2">Gemma 2 2B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.3">2.61</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.4"><cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib21" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.1">Llama 3.2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2">Llama 3.2 1B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.3">1.24</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib38" title="">2024c</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.1">Llama 3.2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.2">Llama 3.2 3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.3">3.21</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib38" title="">2024c</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S2.T1.1.6.5.1">Phi-3</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.6.5.2">Phi-3 3.8B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.3">3.82</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5.4"><cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib2" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.6.1">Phi-3.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.6.2">Phi-3.5 3.8B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.3">3.82</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6.4"><cite class="ltx_cite ltx_citemacro_citep">(Ollama, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib40" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S2.T1.1.8.7.1">Qwen2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.8.7.2">Qwen2 0.5B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.7.3">0.49</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.7.4"><cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib59" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<td class="ltx_td ltx_align_left" id="S2.T1.1.9.8.1">Qwen2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.9.8.2">Qwen2 1.5B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.8.3">1.54</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.8.4"><cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib59" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.9">
<td class="ltx_td ltx_align_left" id="S2.T1.1.10.9.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.10.9.2">Qwen2.5 0.5B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.9.3">0.49</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.9.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.10">
<td class="ltx_td ltx_align_left" id="S2.T1.1.11.10.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.11.10.2">Qwen2.5 1.5B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.10.3">1.54</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.10.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.11">
<td class="ltx_td ltx_align_left" id="S2.T1.1.12.11.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.12.11.2">Qwen2.5 3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.11.3">3.09</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.11.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.13.12.1" rowspan="10"><span class="ltx_text" id="S2.T1.1.13.12.1.1">Small</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.13.12.2">Gemma</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.13.12.3">Gemma 7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.13.12.4">8.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.13.12.5"><cite class="ltx_cite ltx_citemacro_citep">(Team et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib54" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.14.13">
<td class="ltx_td ltx_align_left" id="S2.T1.1.14.13.1">Gemma 2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.14.13.2">Gemma 2 9B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.13.3">9.24</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.13.4"><cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib21" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.15.14">
<td class="ltx_td ltx_align_left" id="S2.T1.1.15.14.1">GPT</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.15.14.2">GPT-4o mini</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.14.3">*</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.14.4"><cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib41" title="">2024a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.16.15">
<td class="ltx_td ltx_align_left" id="S2.T1.1.16.15.1">Llama 3</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.16.15.2">Llama 3 8B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.15.3">8.03</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.15.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib37" title="">2024b</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.17.16">
<td class="ltx_td ltx_align_left" id="S2.T1.1.17.16.1">Llama 3.1</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.17.16.2">Llama 3.1 8B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.16.3">8.03</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.16.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib36" title="">2024a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.18.17">
<td class="ltx_td ltx_align_left" id="S2.T1.1.18.17.1">Mistral</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.18.17.2">Mistral 7B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.17.3">7.25</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.17.4"><cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib28" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.19.18">
<td class="ltx_td ltx_align_left" id="S2.T1.1.19.18.1">Qwen2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.19.18.2">Qwen2 7B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.18.3">7.62</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.18.4"><cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib59" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.20.19">
<td class="ltx_td ltx_align_left" id="S2.T1.1.20.19.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.20.19.2">Qwen2.5 7B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.19.3">7.62</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.19.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.21.20">
<td class="ltx_td ltx_align_left" id="S2.T1.1.21.20.1">Yi</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.21.20.2">Yi 6B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.20.3">6.06</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.20.4"><cite class="ltx_cite ltx_citemacro_citep">(AI et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib3" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.22.21">
<td class="ltx_td ltx_align_left" id="S2.T1.1.22.21.1">Yi</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.22.21.2">Yi 9B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.21.3">8.83</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.21.4"><cite class="ltx_cite ltx_citemacro_citep">(AI et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib3" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.23.22">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.23.22.1" rowspan="9"><span class="ltx_text" id="S2.T1.1.23.22.1.1">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.23.22.2">Gemma 2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.23.22.3">Gemma 2 27B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.23.22.4">27.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.23.22.5"><cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib21" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.24.23">
<td class="ltx_td ltx_align_left" id="S2.T1.1.24.23.1">GPT</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.24.23.2">GPT-3.5 Turbo</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.24.23.3">*</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.24.23.4"><cite class="ltx_cite ltx_citemacro_citep">(OpenAI et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib43" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.25.24">
<td class="ltx_td ltx_align_left" id="S2.T1.1.25.24.1">Mistral</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.25.24.2">Mistral NeMo 12B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.25.24.3">12.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.25.24.4"><cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib7" title="">2024d</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.26.25">
<td class="ltx_td ltx_align_left" id="S2.T1.1.26.25.1">Mistral</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.26.25.2">Mistral Small 22B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.26.25.3">22.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.26.25.4"><cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib4" title="">2024a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.27.26">
<td class="ltx_td ltx_align_left" id="S2.T1.1.27.26.1">Mixtral</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.27.26.2">Mixtral 8x7B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.27.26.3">46.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.27.26.4"><cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib29" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.28.27">
<td class="ltx_td ltx_align_left" id="S2.T1.1.28.27.1">Phi-3</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.28.27.2">Phi-3 14B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.28.27.3">14</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.28.27.4"><cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib2" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.29.28">
<td class="ltx_td ltx_align_left" id="S2.T1.1.29.28.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.29.28.2">Qwen2.5 14B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.29.28.3">14.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.29.28.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.30.29">
<td class="ltx_td ltx_align_left" id="S2.T1.1.30.29.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.30.29.2">Qwen2.5 32B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.30.29.3">32.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.30.29.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.31.30">
<td class="ltx_td ltx_align_left" id="S2.T1.1.31.30.1">Yi</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.31.30.2">Yi 34B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.31.30.3">34.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.31.30.4"><cite class="ltx_cite ltx_citemacro_citep">(AI et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib3" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.32.31">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.32.31.1" rowspan="8"><span class="ltx_text" id="S2.T1.1.32.31.1.1">Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.32.31.2">GPT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.32.31.3">GPT-4o</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.32.31.4">*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.32.31.5"><cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib42" title="">2024b</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.33.32">
<td class="ltx_td ltx_align_left" id="S2.T1.1.33.32.1">Llama 3</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.33.32.2">Llama 3 70B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.33.32.3">70.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.33.32.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib37" title="">2024b</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.34.33">
<td class="ltx_td ltx_align_left" id="S2.T1.1.34.33.1">Llama 3.1</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.34.33.2">Llama 3.1 70B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.34.33.3">70.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.34.33.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib36" title="">2024a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.35.34">
<td class="ltx_td ltx_align_left" id="S2.T1.1.35.34.1">Llama 3.1</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.35.34.2">Llama 3.1 405B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.35.34.3">406</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.35.34.4"><cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib36" title="">2024a</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.36.35">
<td class="ltx_td ltx_align_left" id="S2.T1.1.36.35.1">Mistral</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.36.35.2">Mistral Large 123B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.36.35.3">123</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.36.35.4"><cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib6" title="">2024c</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.37.36">
<td class="ltx_td ltx_align_left" id="S2.T1.1.37.36.1">Mixtral</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.37.36.2">Mixtral 8x22B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.37.36.3">141</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.37.36.4"><cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib5" title="">2024b</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.38.37">
<td class="ltx_td ltx_align_left" id="S2.T1.1.38.37.1">Qwen2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.38.37.2">Qwen2 72B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.38.37.3">72.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.38.37.4"><cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib59" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.39.38">
<td class="ltx_td ltx_align_left" id="S2.T1.1.39.38.1">Qwen2.5</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.39.38.2">Qwen2.5 72B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.39.38.3">72.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.39.38.4"><cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib55" title="">2024</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.40.39">
<td class="ltx_td ltx_align_left ltx_border_tt" colspan="5" id="S2.T1.1.40.39.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.40.39.1.1">
<span class="ltx_p" id="S2.T1.1.40.39.1.1.1"><span class="ltx_text" id="S2.T1.1.40.39.1.1.1.1" style="font-size:80%;">*We estimate the size of closed-source models to be within the bounds of the corresponding model category.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.10"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.10.1">LLMs.</span>
In our experiments, we use <math alttext="35" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mn id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><cn id="S2.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S2.SS1.p2.1.m1.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">35</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">35</annotation></semantics></math> open-source and three commercial OpenAI LLMs, spanning <math alttext="13" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><cn id="S2.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S2.SS1.p2.2.m2.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">13</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">13</annotation></semantics></math> distinct model families (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.T1" title="Table 1 ‣ 2.1. Setup ‣ 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>).
In particular, we use Ollama<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ollama.com" title="">https://ollama.com</a></span></span></span> with q4_0 quantization <cite class="ltx_cite ltx_citemacro_citep">(Jacob et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib27" title="">2018</a>)</cite> for all open-source LLMs and OpenAI’s paid API<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com" title="">https://platform.openai.com</a></span></span></span> for the closed-source GPT models.
We group the models into four categories based on the number of parameters: tiny (<math alttext="{&lt;}4" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml"></mi><mo id="S2.SS1.p2.3.m3.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.cmml">&lt;</mo><mn id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><lt id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1"></lt><csymbol cd="latexml" id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">absent</csymbol><cn id="S2.SS1.p2.3.m3.1.1.3.cmml" type="integer" xref="S2.SS1.p2.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">{&lt;}4</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">&lt; 4</annotation></semantics></math> billion), small (<math alttext="{\geq}4" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.1"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml"></mi><mo id="S2.SS1.p2.4.m4.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.cmml">≥</mo><mn id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><geq id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1"></geq><csymbol cd="latexml" id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">absent</csymbol><cn id="S2.SS1.p2.4.m4.1.1.3.cmml" type="integer" xref="S2.SS1.p2.4.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">{\geq}4</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">≥ 4</annotation></semantics></math> and <math alttext="{&lt;}10" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.1"><semantics id="S2.SS1.p2.5.m5.1a"><mrow id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml"><mi id="S2.SS1.p2.5.m5.1.1.2" xref="S2.SS1.p2.5.m5.1.1.2.cmml"></mi><mo id="S2.SS1.p2.5.m5.1.1.1" xref="S2.SS1.p2.5.m5.1.1.1.cmml">&lt;</mo><mn id="S2.SS1.p2.5.m5.1.1.3" xref="S2.SS1.p2.5.m5.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><apply id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1"><lt id="S2.SS1.p2.5.m5.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1.1"></lt><csymbol cd="latexml" id="S2.SS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.p2.5.m5.1.1.2">absent</csymbol><cn id="S2.SS1.p2.5.m5.1.1.3.cmml" type="integer" xref="S2.SS1.p2.5.m5.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">{&lt;}10</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.1d">&lt; 10</annotation></semantics></math> billion), medium (<math alttext="{\geq}10" class="ltx_Math" display="inline" id="S2.SS1.p2.6.m6.1"><semantics id="S2.SS1.p2.6.m6.1a"><mrow id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml"><mi id="S2.SS1.p2.6.m6.1.1.2" xref="S2.SS1.p2.6.m6.1.1.2.cmml"></mi><mo id="S2.SS1.p2.6.m6.1.1.1" xref="S2.SS1.p2.6.m6.1.1.1.cmml">≥</mo><mn id="S2.SS1.p2.6.m6.1.1.3" xref="S2.SS1.p2.6.m6.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><apply id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1"><geq id="S2.SS1.p2.6.m6.1.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1.1"></geq><csymbol cd="latexml" id="S2.SS1.p2.6.m6.1.1.2.cmml" xref="S2.SS1.p2.6.m6.1.1.2">absent</csymbol><cn id="S2.SS1.p2.6.m6.1.1.3.cmml" type="integer" xref="S2.SS1.p2.6.m6.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">{\geq}10</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m6.1d">≥ 10</annotation></semantics></math> and <math alttext="{&lt;}50" class="ltx_Math" display="inline" id="S2.SS1.p2.7.m7.1"><semantics id="S2.SS1.p2.7.m7.1a"><mrow id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><mi id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml"></mi><mo id="S2.SS1.p2.7.m7.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.cmml">&lt;</mo><mn id="S2.SS1.p2.7.m7.1.1.3" xref="S2.SS1.p2.7.m7.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><lt id="S2.SS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1.1"></lt><csymbol cd="latexml" id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2">absent</csymbol><cn id="S2.SS1.p2.7.m7.1.1.3.cmml" type="integer" xref="S2.SS1.p2.7.m7.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">{&lt;}50</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.7.m7.1d">&lt; 50</annotation></semantics></math> billion), and large (<math alttext="{\geq}50" class="ltx_Math" display="inline" id="S2.SS1.p2.8.m8.1"><semantics id="S2.SS1.p2.8.m8.1a"><mrow id="S2.SS1.p2.8.m8.1.1" xref="S2.SS1.p2.8.m8.1.1.cmml"><mi id="S2.SS1.p2.8.m8.1.1.2" xref="S2.SS1.p2.8.m8.1.1.2.cmml"></mi><mo id="S2.SS1.p2.8.m8.1.1.1" xref="S2.SS1.p2.8.m8.1.1.1.cmml">≥</mo><mn id="S2.SS1.p2.8.m8.1.1.3" xref="S2.SS1.p2.8.m8.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.1b"><apply id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1"><geq id="S2.SS1.p2.8.m8.1.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1.1"></geq><csymbol cd="latexml" id="S2.SS1.p2.8.m8.1.1.2.cmml" xref="S2.SS1.p2.8.m8.1.1.2">absent</csymbol><cn id="S2.SS1.p2.8.m8.1.1.3.cmml" type="integer" xref="S2.SS1.p2.8.m8.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.1c">{\geq}50</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.8.m8.1d">≥ 50</annotation></semantics></math> billion) LLMs.
In this paper, we focus on the evaluation of prompting strategies and do not optimize fine-grained configuration of model parameters, such as temperature <cite class="ltx_cite ltx_citemacro_citep">(Peeperkorn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib45" title="">2024</a>)</cite>.
However, we change the context window size to <math alttext="4\,096" class="ltx_Math" display="inline" id="S2.SS1.p2.9.m9.1"><semantics id="S2.SS1.p2.9.m9.1a"><mn id="S2.SS1.p2.9.m9.1.1" xref="S2.SS1.p2.9.m9.1.1.cmml">4 096</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m9.1b"><cn id="S2.SS1.p2.9.m9.1.1.cmml" type="integer" xref="S2.SS1.p2.9.m9.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m9.1c">4\,096</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.9.m9.1d">4 096</annotation></semantics></math> tokens (Ollama default is <math alttext="2\,048" class="ltx_Math" display="inline" id="S2.SS1.p2.10.m10.1"><semantics id="S2.SS1.p2.10.m10.1a"><mn id="S2.SS1.p2.10.m10.1.1" xref="S2.SS1.p2.10.m10.1.1.cmml">2 048</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.10.m10.1b"><cn id="S2.SS1.p2.10.m10.1.1.cmml" type="integer" xref="S2.SS1.p2.10.m10.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.10.m10.1c">2\,048</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.10.m10.1d">2 048</annotation></semantics></math>) to avoid truncation of longer LLM requests. We also set the maximum number of tokens for response generation to 500 to enable reasonable response times.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Prompting Experiments</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">We evaluate three popular prompting strategies including zero-shot, identity, and few-shot prompting in separate experiments.
In particular, for each reddit submission from the test dataset we generate a single LLM request that is composed of a system prompt and a user prompt (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, center).
While the user prompt maintains a consistent format across all experiments—comprising the title and body of a submission within specified tags—the system prompt varies depending on the experiment.
The system prompt includes a <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">Task</span> section and, depending on the experiment, optional <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">Persona</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">Examples</span> sections (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, left).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Zero-Shot Prompting.</span>
In this experiment, we assess out-of-the-box performance of LLMs as narrative-driven recommenders in the movie domain.
Hence, our zero-shot prompt consists of a <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">Task</span> section including instructions for the model to recommend movies based on a user’s specific request provided in the form of tags, and constraints that define the format specifications and limitations for the expected output.
To facilitate post-processing, we request the model to return a JSON object containing exactly ten movie recommendations to calculate our evaluation metrics @10 (e.g., F1@10)—for fair comparisons with previous results <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib16" title="">2024</a>)</cite>.
To simplify the distinct mapping of movies during evaluation, we request from LLMs to format each recommendation as a single string including the movie’s title and release year (e.g., “Titanic (1997)”).
Additionally, to compare recommendations with the gold-standard recommendations from the reddit community, we request that recommended movies are released before the date of the original reddit submission.
Finally, the zero-shot prompt serves as the foundation that we extend in the remaining two experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Identity Prompting.</span>
The <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">Persona</span> section of the prompt templates defines the LLM identity such as a reddit user, a movie critic, or a movie recommender.
Hence, in the identity prompting experiments, this section is prefixed to the system prompt used in the zero-shot experiment to direct LLM responses towards a particular persona.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">Few-Shot Prompting.</span>
In few-shot prompting, we provide multiple random input-output examples that showcase the requested response structure.
Thus, we extend the zero-shot system prompt with the <span class="ltx_text ltx_font_italic" id="S2.SS2.p4.1.2">Examples</span> section containing either one, five, or ten examples to steer the LLM’s generating process.
These examples contain exemplary user prompts as well as JSON responses.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="178" id="S2.F2.g1" src="x2.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S2.F2.fig1" style="width:108.4pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.fig1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F2.fig1.2.2" style="font-size:90%;">Valid JSON Format</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S2.F2.fig2" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.fig2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F2.fig2.2.2" style="font-size:90%;">10 Recommendations</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S2.F2.fig3" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.fig3.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F2.fig3.2.2" style="font-size:90%;">Unique Movies</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S2.F2.fig4" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.fig4.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S2.F2.fig4.2.2" style="font-size:90%;">Movies Released Before Request</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text ltx_font_bold" id="S2.F2.4.2" style="font-size:90%;">LLM Response.<span class="ltx_text ltx_font_medium" id="S2.F2.4.2.1">
We report the ratio of (a) LLM responses with valid JSON, (b) responses with exactly ten recommendations, (c) unique movies, and (d) movies released before request, with
vertical bars showing bootstrapped 95% confidence intervals (often too narrow to be visible).
We observe that medium and large LLMs mostly follow the constraints outlined in our prompt (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, left), while tiny and small LLMs show slightly lower reliability, though their overall numbers remain high.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Evaluation of LLM Responses</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Following our prompting experiments, we parse the corresponding LLM responses, match the contained titles to actual movies, and compute established evaluation metrics to assess LLM performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">Identifying Recommendations.</span>
After prompting the LLMs, we proceed by extracting recommendations from the JSON responses.
In particular, we apply a regex to parse correct recommendations in the format “<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p2.1.2">&lt;movie_title&gt; (&lt;release_year&gt;)</span>” (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">1</span></a>, center) and remove any duplicate strings as well as ignoring case.
Finally, for responses containing more than ten movies, we randomly sample ten to compute our evaluation metrics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">Title Matching.</span>
We compare LLM recommendations to the recommendations from the reddit community by exact matching of title and release year.
As the community recommendations in the dataset are identified by their IDs on IMDb, we first filter out recommendations that were already mentioned in the submission text. Then, we retrieve the titles of the remaining community recommendations from IMDb in all available languages (e.g., original title: “Un prophète (2009)”, international title: “A Prophet (2009)”).
Additionally, we apply soft matching using Ratcliff/Obershelp pattern-matching  <cite class="ltx_cite ltx_citemacro_citep">(Ratcliff and Metzener, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib48" title="">1988</a>)</cite> with a threshold of 0.9.
As we observe no significant difference in the results between exact and soft matching, we only report the exact matching results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">Evaluation Metrics.</span>
We compute four standard recommendation metrics—precision, recall, F1 score, and Normalized Discounted Cumulative Gain (NDCG) <cite class="ltx_cite ltx_citemacro_citep">(Yilmaz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib61" title="">2008</a>; Powers, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib46" title="">2008</a>)</cite>, with fixed cut-off value (i.e., Precision@10, Recall@10, F1@10, NDCG@10<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Note that Recall@10 and F1@10 have an upper limit of 0.43 and 0.57, resp., as the number of community suggestions varies per submission (mean=<math alttext="30.77" class="ltx_Math" display="inline" id="footnote6.m1.1"><semantics id="footnote6.m1.1b"><mn id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">30.77</mn><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><cn id="footnote6.m1.1.1.cmml" type="float" xref="footnote6.m1.1.1">30.77</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">30.77</annotation><annotation encoding="application/x-llamapun" id="footnote6.m1.1e">30.77</annotation></semantics></math>).</span></span></span>)—for each request and report their overall means over the whole test dataset (i.e., macro average).
Due to space limitation, we report results aggregated by model size category and prompting strategy for all metrics and only F1 results for individual models in the main text. We present more detailed results for all evaluation metrics in the Appendix.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.1">Response Variance.</span>
To estimate the variance of LLM responses, we repeat each request 30 times for all models in the zero-shot prompting experiment and compute one-way ANOVA.
The test results show no significant differences across repetitions in any of our evaluation metrics and models (Appendix, Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.T2" title="Table 2 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2</span></a>).
For this reason, we repeat all other experiments only three times and use the repetition with the median F1 score as the final result.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We present our experimental findings across <math alttext="38" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">38</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn id="S3.p1.1.m1.1.1.cmml" type="integer" xref="S3.p1.1.m1.1.1">38</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">38</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">38</annotation></semantics></math> evaluated LLMs.
First, we assess the structural correctness and validity of the LLM responses in relation to the constraints posed in our prompts.
We then evaluate the performance of LLM recommendation, especially considering different-sized LLMs, the effectiveness of different prompting strategies, and open- versus closed-source models.
Finally, we compare LLMs to other recommendation approaches and present a robustness experiment regarding potential data leakage. To statistically substantiate our results, we bootstrap the dataset in all experiments and report bootstrapped 95% confidence intervals.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Format Adherence</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We assess the compliance of the LLM-generated outputs with the formatting and structural instructions by checking correctness of the JSON format, the total number of returned recommendations, the frequency of unique movie entries within a recommendation list, and the movie release year.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">JSON Format.</span>
Our analysis of LLM responses reveals that the fraction of valid JSON responses exceeds 97% for all aggregations of model size category and prompting strategy (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.F2" title="Figure 2 ‣ 2.2. Prompting Experiments ‣ 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2</span></a>). This demonstrates high proficiency of LLMs in generating correctly formatted responses.
Large LLMs exhibit exceptional performance, achieving valid JSON output in over 99.9% of cases across all prompting strategies.
In more details (Appendix, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F8" title="Figure 8 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">8</span></a>), the GPT models as well as Gemma 2 9B, Llama 3 70B, Llama 3.1 8B and 70B, and Qwen 2.5 14B consistently produce 100% valid JSON responses.
In contrast, some of the smaller models are more prone to error, such as Llama 3.2 1B with 93.39%, Mistral 7B with 91.75%, Qwen 2 0.5B with 92.76%, Yi 6B with 94.45%, and Phi-3 14B with around 94% of correct JSON responses.
Moreover, for Phi-3 14B we observe a significant decline in performance for few-shot prompting, likely due to the model’s difficulty in processing longer requests.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Number of Recommendations.</span>
Compliance with the constraint of responding with exactly ten recommendations varies considerably across model size categories and prompting strategies (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.F2" title="Figure 2 ‣ 2.2. Prompting Experiments ‣ 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2</span></a>).
Large LLMs reach nearly perfect adherence to the prompt, with the fraction of responses containing precisely ten recommendations exceeding 98% on average over all models and prompting strategies.
Medium-sized models attain an average score of 96% over all prompting strategies with slightly greater variability than their larger counterparts.
In contrast, smaller, especially tiny-sized models demonstrate more inconsistent behavior. On average, less than 40% of valid JSON responses from tiny models contain exactly ten recommendations.
Furthermore, different prompting approaches (e.g., zero-shot versus few-shot) show only marginal and no significant differences in the average number of recommendations, except for small-sized LLMs with a significant higher average when using few-shot prompting.
This highlights the robustness of modern LLMs in meeting specific output requirements with simple prompting configurations, particularly for well-trained and larger architectures.
In detail, we observe that each large LLM consistently provide exactly ten recommendations in over 91% of the requests, regardless of the prompting strategy (Appendix, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F9" title="Figure 9 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">9</span></a>).
Qwen2 72B achieves the highest adherence to the number of recommendations constraint, with more than 99.9% of its responses meeting this criterion.
In contrast, the smaller models in the tiny category, such as Qwen2 and Qwen2.5 0.5B, frequently fail to include any movies in the zero-shot and identity prompting experiments, with over 92% of their responses lacking movie recommendations. However, the performance of these smaller models improve significantly when provided with few-shot prompts, reducing the proportion of responses without any movie recommendations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Unique Movies.</span>
On average across all model size categories and prompting strategies, LLMs return over 99% unique movies in their valid outputs (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.F2" title="Figure 2 ‣ 2.2. Prompting Experiments ‣ 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2</span></a>). However, tiny and small models exhibit a marginal but statistically significant increase in duplicate movie recommendations.
Gemma 2B and 7B, alongside Qwen 2.5 7B, show the highest rates of duplication, averaging 2–4% duplicate recommendations (Appendix, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F10" title="Figure 10 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Release Year.</span>
We show the average fractions of recommended movies that were released before or in the year specified in the system prompt, grouped by model size and prompting strategy (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.F2" title="Figure 2 ‣ 2.2. Prompting Experiments ‣ 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2</span></a>). Across all model categories from tiny to large, the average fractions of recommendations with correct years exceeds 95%, indicating that LLMs are generally effective at adhering to temporal constraints in narrative-driven movie recommendation tasks. Larger models, on average, demonstrate a higher adherence, achieving accuracy rates exceeding 99% regardless of the prompting strategy.
We provide more detailed breakdowns in the Appendix (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F11" title="Figure 11 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="220" id="S3.F3.g1" src="x3.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S3.F3.fig1" style="width:112.7pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.fig1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F3.fig1.2.2" style="font-size:90%;">F1 Score</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S3.F3.fig2" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.fig2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F3.fig2.2.2" style="font-size:90%;">Precision</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S3.F3.fig3" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.fig3.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F3.fig3.2.2" style="font-size:90%;">Recall</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S3.F3.fig4" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.fig4.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F3.fig4.2.2" style="font-size:90%;">NDCG</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text ltx_font_bold" id="S3.F3.5.2" style="font-size:90%;">Aggregated Performance Metrics.<span class="ltx_text ltx_font_medium" id="S3.F3.5.2.1"> We report aggregated recommendation performance of LLMs across model size categories and prompting strategies using the metrics (a) F1@10, (b) Precision@10, (c) Recall@10, and (d) NDCG@10, with
vertical bars representing bootstrapped 95% confidence intervals (often too narrow to be visible).
We indicate the <span class="ltx_text ltx_font_typewriter" id="S3.F3.5.2.1.1">doc2vec</span> baseline performance as horizontal reference line.
On average, large LLMs consistently demonstrate superior performance, surpassing the baseline across all metrics regardless of the prompting strategy.
Medium-sized models also outperform the baseline substantially, particularly in Precision@10 and NDCG@10.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Recommendation Performance</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our evaluation of the recommendation performance of various LLMs reveals several key insights regarding different model sizes and prompting strategies.
In particular, we highlight the general suitability of medium and large LLMs as narrative-driven recommenders, the effectiveness of zero-shot prompting, competitiveness of medium- with larger-parameterized LLMs, as well as high performance of open-source models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F3" title="Figure 3 ‣ 3.1. Format Adherence ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3</span></a> depicts aggregated performance results over model size categories and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F4" title="Figure 4 ‣ 3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a> shows F1 scores (i.e., F1@10) of all models categorized by model families across various prompting strategies. We list further detailed results for all models in the Appendix (Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F12" title="Figure 12 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F13" title="Figure 13 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F14" title="Figure 14 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">14</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F15" title="Figure 15 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">15</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Medium and Large LLMs Outperform Baselines.</span>
To evaluate the general suitability of LLMs as narrative-driven recommenders, we compare our results with the state-of-the-art recommendation baselines evaluated on the same dataset <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib16" title="">2024</a>)</cite>.
The baseline methods utilized a range of established recommendation algorithms, such as <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.2">doc2vec</span>, collaborative filtering, matrix factorization, or a TF–IDF-based approach, with the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.3">doc2vec</span> approach being the best-performing method with an F1 score of 0.1258 <math alttext="[0.1125,0.1388]" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.2"><semantics id="S3.SS2.p2.1.m1.2a"><mrow id="S3.SS2.p2.1.m1.2.3.2" xref="S3.SS2.p2.1.m1.2.3.1.cmml"><mo id="S3.SS2.p2.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.p2.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">0.1125</mn><mo id="S3.SS2.p2.1.m1.2.3.2.2" xref="S3.SS2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.2.2" xref="S3.SS2.p2.1.m1.2.2.cmml">0.1388</mn><mo id="S3.SS2.p2.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.2b"><interval closure="closed" id="S3.SS2.p2.1.m1.2.3.1.cmml" xref="S3.SS2.p2.1.m1.2.3.2"><cn id="S3.SS2.p2.1.m1.1.1.cmml" type="float" xref="S3.SS2.p2.1.m1.1.1">0.1125</cn><cn id="S3.SS2.p2.1.m1.2.2.cmml" type="float" xref="S3.SS2.p2.1.m1.2.2">0.1388</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.2c">[0.1125,0.1388]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.2d">[ 0.1125 , 0.1388 ]</annotation></semantics></math> (horizontal lines in Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F3" title="Figure 3 ‣ 3.1. Format Adherence ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F4" title="Figure 4 ‣ 3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Our experimental results show that the medium and large model categories significantly outperform the traditional methods across all evaluated configurations regardless of the prompting strategy.
Specifically, all of the large, the majority of the medium, as well as some of the small LLMs achieve higher scores across all metrics, indicating a substantial improvement when generating movie recommendations from narrative user queries.
For example, apart from most of the medium and large models even small-sized LLMs, such as Gemma 2 7B or GPT-4o mini, outperform the best state-of-the-art recommender method on this dataset, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">doc2vec</span>, across all applied prompting strategies (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F4" title="Figure 4 ‣ 3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a> and Appendix, Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F12" title="Figure 12 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F13" title="Figure 13 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F14" title="Figure 14 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">14</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F15" title="Figure 15 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">15</span></a>).
In particular, the best-performing LLM, large-sized GPT-4o, with identity prompting and the persona movie critic achieves an F1 score of 0.2157 <math alttext="[0.2009,0.2302]" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.2"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.3.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml"><mo id="S3.SS2.p3.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.p3.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">0.2009</mn><mo id="S3.SS2.p3.1.m1.2.3.2.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">0.2302</mn><mo id="S3.SS2.p3.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.p3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><interval closure="closed" id="S3.SS2.p3.1.m1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.3.2"><cn id="S3.SS2.p3.1.m1.1.1.cmml" type="float" xref="S3.SS2.p3.1.m1.1.1">0.2009</cn><cn id="S3.SS2.p3.1.m1.2.2.cmml" type="float" xref="S3.SS2.p3.1.m1.2.2">0.2302</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">[0.2009,0.2302]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.2d">[ 0.2009 , 0.2302 ]</annotation></semantics></math>, which is more than 70% higher than the performance of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2">doc2vec</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.2">Given the minor differences in experimental setups between our work and the studies from which we derive our baselines <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib16" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib18" title="">2019</a>)</cite>, we conduct an additional sensitivity experiment.
Notably, the recommender approaches serving as our baselines generated exactly ten unique movie recommendations from a predefined IMDb movie pool of around <math alttext="12\,000" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mn id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">12 000</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><cn id="S3.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1">12000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">12\,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">12 000</annotation></semantics></math> movies.
In this sensitivity experiment, we map our LLM recommendations to that same movie pool and filter out all others.
If the LLM produces fewer than ten recommendations, we repeat the request until we collect exactly ten movies.
For this experiment, we only use GPT-3.5 Turbo—a medium-sized, closed-source LLM that performs strongly in our benchmarks and shows comparable results to similar-sized open-source models (e.g., Gemma 2 27B) as well as other closed-source models of varying sizes (e.g., GPT-4o).
Using identity prompting with the reddit user persona, we obtain an F1 score of 0.2348 <math alttext="[0.2189,0.2508]" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.2"><semantics id="S3.SS2.p4.2.m2.2a"><mrow id="S3.SS2.p4.2.m2.2.3.2" xref="S3.SS2.p4.2.m2.2.3.1.cmml"><mo id="S3.SS2.p4.2.m2.2.3.2.1" stretchy="false" xref="S3.SS2.p4.2.m2.2.3.1.cmml">[</mo><mn id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">0.2189</mn><mo id="S3.SS2.p4.2.m2.2.3.2.2" xref="S3.SS2.p4.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS2.p4.2.m2.2.2" xref="S3.SS2.p4.2.m2.2.2.cmml">0.2508</mn><mo id="S3.SS2.p4.2.m2.2.3.2.3" stretchy="false" xref="S3.SS2.p4.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.2b"><interval closure="closed" id="S3.SS2.p4.2.m2.2.3.1.cmml" xref="S3.SS2.p4.2.m2.2.3.2"><cn id="S3.SS2.p4.2.m2.1.1.cmml" type="float" xref="S3.SS2.p4.2.m2.1.1">0.2189</cn><cn id="S3.SS2.p4.2.m2.2.2.cmml" type="float" xref="S3.SS2.p4.2.m2.2.2">0.2508</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.2c">[0.2189,0.2508]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.2d">[ 0.2189 , 0.2508 ]</annotation></semantics></math>, indicating a further improvement in LLM performance over the baselines.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.2.1">Effectiveness of Zero-Shot Prompting.</span>
Our results demonstrate that zero-shot prompting achieves a high level of performance across all model size categories (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F3" title="Figure 3 ‣ 3.1. Format Adherence ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3</span></a>), suggesting that additional prompting complexity with identity or few-shot prompting yields only minor gains in recommendation accuracy.
This pattern is consistent across all model sizes, indicating that zero-shot prompting is a robust strategy for a variety of models, from tiny to large.
In more details, we find that the performance gains from zero-shot to identity and few-shot prompting are more substantial for some of the smaller models, suggesting that these strategies are particularly useful for improving the capabilities of less parameterized models (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F4" title="Figure 4 ‣ 3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a> and Appendix, Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F12" title="Figure 12 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F13" title="Figure 13 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F14" title="Figure 14 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">14</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F15" title="Figure 15 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">15</span></a>).
For example, the F1 score using the small-sized Mistral 7B model with zero-shot prompting of 0.1053 <math alttext="[0.0929,0.1174]" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.2"><semantics id="S3.SS2.p5.1.m1.2a"><mrow id="S3.SS2.p5.1.m1.2.3.2" xref="S3.SS2.p5.1.m1.2.3.1.cmml"><mo id="S3.SS2.p5.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.p5.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">0.0929</mn><mo id="S3.SS2.p5.1.m1.2.3.2.2" xref="S3.SS2.p5.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p5.1.m1.2.2" xref="S3.SS2.p5.1.m1.2.2.cmml">0.1174</mn><mo id="S3.SS2.p5.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.p5.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.2b"><interval closure="closed" id="S3.SS2.p5.1.m1.2.3.1.cmml" xref="S3.SS2.p5.1.m1.2.3.2"><cn id="S3.SS2.p5.1.m1.1.1.cmml" type="float" xref="S3.SS2.p5.1.m1.1.1">0.0929</cn><cn id="S3.SS2.p5.1.m1.2.2.cmml" type="float" xref="S3.SS2.p5.1.m1.2.2">0.1174</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.2c">[0.0929,0.1174]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.2d">[ 0.0929 , 0.1174 ]</annotation></semantics></math> significantly increases when applying few-shot prompting up to 0.1342 <math alttext="[0.1219,0.1462]" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.2"><semantics id="S3.SS2.p5.2.m2.2a"><mrow id="S3.SS2.p5.2.m2.2.3.2" xref="S3.SS2.p5.2.m2.2.3.1.cmml"><mo id="S3.SS2.p5.2.m2.2.3.2.1" stretchy="false" xref="S3.SS2.p5.2.m2.2.3.1.cmml">[</mo><mn id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">0.1219</mn><mo id="S3.SS2.p5.2.m2.2.3.2.2" xref="S3.SS2.p5.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS2.p5.2.m2.2.2" xref="S3.SS2.p5.2.m2.2.2.cmml">0.1462</mn><mo id="S3.SS2.p5.2.m2.2.3.2.3" stretchy="false" xref="S3.SS2.p5.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.2b"><interval closure="closed" id="S3.SS2.p5.2.m2.2.3.1.cmml" xref="S3.SS2.p5.2.m2.2.3.2"><cn id="S3.SS2.p5.2.m2.1.1.cmml" type="float" xref="S3.SS2.p5.2.m2.1.1">0.1219</cn><cn id="S3.SS2.p5.2.m2.2.2.cmml" type="float" xref="S3.SS2.p5.2.m2.2.2">0.1462</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.2c">[0.1219,0.1462]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.2d">[ 0.1219 , 0.1462 ]</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="261" id="S3.F4.g1" src="x4.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.F4.fig1" style="width:112.7pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.fig1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F4.fig1.2.2" style="font-size:90%;">Zero-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.F4.fig2" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.fig2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F4.fig2.2.2" style="font-size:90%;">Identity Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.F4.fig3" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.fig3.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F4.fig3.2.2" style="font-size:90%;">Few-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.F4.fig4" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.fig4.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F4.fig4.2.2" style="font-size:90%;">Average of All Strategies</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.3.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text ltx_font_bold" id="S3.F4.4.2" style="font-size:90%;">Performance of Different Models and Prompting Strategies.<span class="ltx_text ltx_font_medium" id="S3.F4.4.2.1">
We report F1 scores for different models according to prompting strategy.
We observe that closed-source GPT models perform best within their respective model size*, but open-source models such as Mistral perform similarly well given parameter size and scenario.
Additionally, we see that medium-sized LLMs perform similar to large models, signaling a ceiling of (our) prompting techniques for certain parameter sizes.
</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<p class="ltx_p ltx_align_left ltx_figure_panel ltx_align_center" id="S3.F4.5"><span class="ltx_text ltx_font_bold" id="S3.F4.5.1" style="font-size:80%;">*<span class="ltx_text ltx_font_medium" id="S3.F4.5.1.1">We visualize closed-source models in the center of their respective model size category, given that their parameter size is undisclosed.</span></span></p>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.p6.2.1">Medium-Sized Models Compete With Larger LLMs.</span>
Despite the general expectation that larger models would significantly outperform smaller ones, our results demonstrate that medium-sized models with parameter counts between 10 and 50 billion can achieve almost equivalent accuracy as their larger counterparts.
The aggregated performance scores by model size category and prompting strategy highlight this finding (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F3" title="Figure 3 ‣ 3.1. Format Adherence ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3</span></a>).
Moreover, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F4" title="Figure 4 ‣ 3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a> also illustrates this performance across various models and model families.
Model families such as Gemma 2 are competitive with more computationally expensive LLMs such as the larger GPT or Mistral models, achieving similar F1 scores.
More detailed results for all models in Appendix (Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F12" title="Figure 12 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F13" title="Figure 13 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F14" title="Figure 14 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">14</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F15" title="Figure 15 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">15</span></a>) further highlight our findings.
For example, the best-performing medium-sized GPT-3.5 Turbo model with an F1 score of 0.1932 <math alttext="[0.1876,0.1986]" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.2"><semantics id="S3.SS2.p6.1.m1.2a"><mrow id="S3.SS2.p6.1.m1.2.3.2" xref="S3.SS2.p6.1.m1.2.3.1.cmml"><mo id="S3.SS2.p6.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.p6.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">0.1876</mn><mo id="S3.SS2.p6.1.m1.2.3.2.2" xref="S3.SS2.p6.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p6.1.m1.2.2" xref="S3.SS2.p6.1.m1.2.2.cmml">0.1986</mn><mo id="S3.SS2.p6.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.p6.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.2b"><interval closure="closed" id="S3.SS2.p6.1.m1.2.3.1.cmml" xref="S3.SS2.p6.1.m1.2.3.2"><cn id="S3.SS2.p6.1.m1.1.1.cmml" type="float" xref="S3.SS2.p6.1.m1.1.1">0.1876</cn><cn id="S3.SS2.p6.1.m1.2.2.cmml" type="float" xref="S3.SS2.p6.1.m1.2.2">0.1986</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.2c">[0.1876,0.1986]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.2d">[ 0.1876 , 0.1986 ]</annotation></semantics></math> on average over all prompting strategies, almost performs on par with the large-sized GPT-4o, which exhibits the highest score of all models with an F1 score of 0.2137 <math alttext="[0.2084,0.2191]" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.2"><semantics id="S3.SS2.p6.2.m2.2a"><mrow id="S3.SS2.p6.2.m2.2.3.2" xref="S3.SS2.p6.2.m2.2.3.1.cmml"><mo id="S3.SS2.p6.2.m2.2.3.2.1" stretchy="false" xref="S3.SS2.p6.2.m2.2.3.1.cmml">[</mo><mn id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">0.2084</mn><mo id="S3.SS2.p6.2.m2.2.3.2.2" xref="S3.SS2.p6.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS2.p6.2.m2.2.2" xref="S3.SS2.p6.2.m2.2.2.cmml">0.2191</mn><mo id="S3.SS2.p6.2.m2.2.3.2.3" stretchy="false" xref="S3.SS2.p6.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.2b"><interval closure="closed" id="S3.SS2.p6.2.m2.2.3.1.cmml" xref="S3.SS2.p6.2.m2.2.3.2"><cn id="S3.SS2.p6.2.m2.1.1.cmml" type="float" xref="S3.SS2.p6.2.m2.1.1">0.2084</cn><cn id="S3.SS2.p6.2.m2.2.2.cmml" type="float" xref="S3.SS2.p6.2.m2.2.2">0.2191</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.2c">[0.2084,0.2191]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.2d">[ 0.2084 , 0.2191 ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p7.1.1">Closed-Source Over Open-Source Models.</span>
We find that the closed-source GPT models show prominent performances compared to most of the similar sized open-source models within their respective model size categories (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.F4" title="Figure 4 ‣ 3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a>).
The results further exhibit that medium-sized open-source models often outperform smaller closed-source models, suggesting that even without proprietary data, open-source models can be competitive with appropriate parameter sizes.
In more details, the large closed-source model GPT-4o achieves the highest F1 scores of over 0.21 throughout all prompting strategies (Appendix, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F12" title="Figure 12 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">12</span></a>).
The best-performing open-source model, Mistral Large 2 123B, reaches an F1 score of around 0.20.
Medium-sized open-source models such as Gemma 2 27B perform on a similar accuracy level as the closed-source medium-sized GPT-3.5 Turbo model.
In the category of small models we find that the closed-source GPT-4o mini slightly but not significantly outperforms the best open-source counterpart, Gemma 2 9B.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Addressing Potential Data Leakage</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Given the setup of our experiment, our results may be susceptible to data leakage due to the possible overlap of submissions in our reddit dataset with the LLMs’ training data <cite class="ltx_cite ltx_citemacro_citep">(Roberts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib50" title="">2023</a>)</cite>.
For this, we follow the methodology of prior work <cite class="ltx_cite ltx_citemacro_citep">(Sainz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib51" title="">2023</a>)</cite> to compare model performance before and after the knowledge cutoff to detect any significant pre- and post-cutoff differences.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Setup.</span>
As previously, we conduct this robustness analysis with GPT-3.5 Turbo.
Given that GPT-3.5 Turbo’s knowledge cutoff (i.e., September 2021) is outside the timeframe of our original dataset, we first use GPT-4o as a higher-parameterized expert model to extract recommendations from reddit posts made after this cutoff.
Using this dataset, we then repeat our zero-shot experiment with GPT-3.5 Turbo and compare the results to the main zero-shot results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.2"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.2.1">Robustness Analysis Dataset.</span>
We consider all submissions from r/MovieSuggestions submitted from October 2021 to July 2024 that contain the word “request” in the submission title to ensure the post contains actual movie inquiries (<math alttext="92\,040" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">92 040</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><cn id="S3.SS3.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1">92040</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">92\,040</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">92 040</annotation></semantics></math> submissions).
We then keep all submissions with at least five comments with more upvotes than downvotes, focusing on high-quality and engaging user discussions (<math alttext="441" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mn id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">441</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><cn id="S3.SS3.p3.2.m2.1.1.cmml" type="integer" xref="S3.SS3.p3.2.m2.1.1">441</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">441</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">441</annotation></semantics></math> submissions remaining).
In contrast to the crowdworker-labeled dataset for our previous experiments, we now employ GPT-4o to extract movie mentions from the requesting user’s post as well as from commenters’ replies (Appendix, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F7" title="Figure 7 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">7</span></a> for the prompt).
Finally, similar as in our original dataset, we exclude submissions with no movies mentioned in the request and less than ten recommendations made by commenters.
This final robustness analysis dataset consists of 236 submissions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.4"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.4.1">Performance After Knowledge Cutoff.</span>
We rerun our zero-shot prompting experiment using GPT-3.5 Turbo on the robustness analysis dataset, achieving an F1@10 of <math alttext="0.1667" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><mn id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">0.1667</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><cn id="S3.SS3.p4.1.m1.1.1.cmml" type="float" xref="S3.SS3.p4.1.m1.1.1">0.1667</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">0.1667</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">0.1667</annotation></semantics></math> <math alttext="[0.1513,0.1814]" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.2"><semantics id="S3.SS3.p4.2.m2.2a"><mrow id="S3.SS3.p4.2.m2.2.3.2" xref="S3.SS3.p4.2.m2.2.3.1.cmml"><mo id="S3.SS3.p4.2.m2.2.3.2.1" stretchy="false" xref="S3.SS3.p4.2.m2.2.3.1.cmml">[</mo><mn id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">0.1513</mn><mo id="S3.SS3.p4.2.m2.2.3.2.2" xref="S3.SS3.p4.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS3.p4.2.m2.2.2" xref="S3.SS3.p4.2.m2.2.2.cmml">0.1814</mn><mo id="S3.SS3.p4.2.m2.2.3.2.3" stretchy="false" xref="S3.SS3.p4.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.2b"><interval closure="closed" id="S3.SS3.p4.2.m2.2.3.1.cmml" xref="S3.SS3.p4.2.m2.2.3.2"><cn id="S3.SS3.p4.2.m2.1.1.cmml" type="float" xref="S3.SS3.p4.2.m2.1.1">0.1513</cn><cn id="S3.SS3.p4.2.m2.2.2.cmml" type="float" xref="S3.SS3.p4.2.m2.2.2">0.1814</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.2c">[0.1513,0.1814]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.2d">[ 0.1513 , 0.1814 ]</annotation></semantics></math>,<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Note that Recall@10 and F1@10 now have reduced upper limits of 0.41 and 0.54, resp., due to an increase in community suggestions per submission (mean=<math alttext="37.09" class="ltx_Math" display="inline" id="footnote7.m1.1"><semantics id="footnote7.m1.1b"><mn id="footnote7.m1.1.1" xref="footnote7.m1.1.1.cmml">37.09</mn><annotation-xml encoding="MathML-Content" id="footnote7.m1.1c"><cn id="footnote7.m1.1.1.cmml" type="float" xref="footnote7.m1.1.1">37.09</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote7.m1.1d">37.09</annotation><annotation encoding="application/x-llamapun" id="footnote7.m1.1e">37.09</annotation></semantics></math>).</span></span></span> which does not indicate a significant decline compared to our original results (F1@10: <math alttext="0.1962" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><mn id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">0.1962</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><cn id="S3.SS3.p4.3.m3.1.1.cmml" type="float" xref="S3.SS3.p4.3.m3.1.1">0.1962</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">0.1962</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">0.1962</annotation></semantics></math> <math alttext="[0.1809,0.2112]" class="ltx_Math" display="inline" id="S3.SS3.p4.4.m4.2"><semantics id="S3.SS3.p4.4.m4.2a"><mrow id="S3.SS3.p4.4.m4.2.3.2" xref="S3.SS3.p4.4.m4.2.3.1.cmml"><mo id="S3.SS3.p4.4.m4.2.3.2.1" stretchy="false" xref="S3.SS3.p4.4.m4.2.3.1.cmml">[</mo><mn id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">0.1809</mn><mo id="S3.SS3.p4.4.m4.2.3.2.2" xref="S3.SS3.p4.4.m4.2.3.1.cmml">,</mo><mn id="S3.SS3.p4.4.m4.2.2" xref="S3.SS3.p4.4.m4.2.2.cmml">0.2112</mn><mo id="S3.SS3.p4.4.m4.2.3.2.3" stretchy="false" xref="S3.SS3.p4.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.2b"><interval closure="closed" id="S3.SS3.p4.4.m4.2.3.1.cmml" xref="S3.SS3.p4.4.m4.2.3.2"><cn id="S3.SS3.p4.4.m4.1.1.cmml" type="float" xref="S3.SS3.p4.4.m4.1.1">0.1809</cn><cn id="S3.SS3.p4.4.m4.2.2.cmml" type="float" xref="S3.SS3.p4.4.m4.2.2">0.2112</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.2c">[0.1809,0.2112]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.4.m4.2d">[ 0.1809 , 0.2112 ]</annotation></semantics></math>).
However, we observe a minor decline in performance, potentially due to movies released after the knowledge cutoff being relevant recommendations for certain requests in the robustness analysis dataset.
This supports the validity of our findings, even considering that parts of our evaluation dataset could have been included in the LLMs’ training data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our findings reveal that LLMs are highly suitable as narrative-driven recommenders.
LLMs demonstrate remarkable utility, especially compared to traditional recommender baselines, in generating contextually relevant recommendations from free-form narrative inputs, which can be particularly valuable in enhancing user experience in personalized movie suggestions.
In particular, we compare the performance of LLMs as narrative-driven movie recommenders to state-of-the-art recommender approaches <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib16" title="">2024</a>)</cite> and find substantial leaps in recommendation quality.
We attribute this to the superior ability of LLMs to interpret complex natural language expressions and nuances, which traditional methods, such as matrix factorization or <span class="ltx_text ltx_font_typewriter" id="S4.p1.1.1">doc2vec</span>, often fail to capture effectively.
Unlike these traditional systems, which rely on extensive feature engineering and domain-specific fine-tuning, LLMs can extract meaningful contextual information directly from narrative requests due to their extensive pre-training.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Moreover, similarly to recent work that demonstrated the effectiveness of zero-shot prompting in various NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Reynolds and McDonell, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib49" title="">2021</a>; Wang and Lim, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib56" title="">2023</a>)</cite>, we find that more extensive prompting strategies such as identity or few-shot prompting do not outperform zero-shot prompting.
These results suggest that even simple prompting strategies can effectively capture essential user preferences in narrative-driven recommendation tasks, reducing the need for extensive prompt engineering, model training, or fine-tuning.
This result aligns with a recent study that evaluated the performance of various prompts for GPT-3.5 Turbo on multiple recommendation scenarios based on Amazon customer reviews, such as rating prediction <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib33" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S4.F5.g1" src="x5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text ltx_font_bold" id="S4.F5.4.2" style="font-size:90%;">Inter-List Diversity for All Model Size Categories.<span class="ltx_text ltx_font_medium" id="S4.F5.4.2.1">
We show inter-list diversity using Jaccard distance with
vertical bars indicating bootstrapped 95% confidence intervals (often too narrow to be visible), observing a trend where large LLMs tend to produce less diverse responses across multiple experiment repetitions using the same prompt.
Notably, few-shot prompting consistently results in higher diversity across all model sizes.
</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Nevertheless, while we find that simple zero-shot prompting performs as well as other strategies such as few-shot prompting, more sophisticated techniques could potentially further enhance recommendation quality.
For example, <cite class="ltx_cite ltx_citemacro_citet">Wang and Lim (<a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib56" title="">2023</a>)</cite> explored the application of LLMs utilizing GPT-3 for zero-shot next-item recommendation in the context of movie suggestions.
The authors developed a three-step prompting strategy to execute subtasks that capture user preferences, select representative previously watched movies, and recommend a ranked list of ten movies.
Their experimental evaluation on the MovieLens 100K dataset indicates that this method can outperform traditional recommendation models that require extensive training.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Furthermore, our experiments suggest that medium-sized LLMs, such as Gemma 2 27B, offer a compelling trade-off between computational efficiency and recommendation quality, competing with larger and more resource-intensive models.
This finding relates to a study by <cite class="ltx_cite ltx_citemacro_citet">He et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib24" title="">2023</a>)</cite> in the field of conversational recommenders, where medium-sized models such as GPT-3.5 Turbo produced results comparable to larger counterparts.
Thus, medium-sized models emerge as viable alternatives for real-world applications, balancing performance and effectiveness.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Besides, while the closed-source GPT-4o model outperforms all other evaluated LLMs, the minimal performance gap between closed- and open-source models (e.g., Mistral 123B) highlights that open-source LLMs can provide high-quality movie recommendations without the significant computational or cost overhead of closed-source solutions.
<cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib34" title="">2024</a>)</cite> corroborates this finding, showing that both open- and closed-source LLMs exhibit strong performance in content-based recommendation tasks.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">In addition to recommendation performance, we also analyze the inter-list diversity using Jaccard distance <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib14" title="">2012</a>)</cite> of LLM recommendations (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S4.F5" title="Figure 5 ‣ 4. Discussion ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">5</span></a> and Appendix Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F16" title="Figure 16 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">16</span></a>) as diverse recommendations typically enhance user experience in recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Ziegler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib64" title="">2005</a>)</cite>.
Our investigation reveals that larger models exhibit lower diversity across prompting repetitions, indicating a higher tendency towards stable and repeated outputs.
In contrast, smaller models, particularly tiny LLMs, show higher diversity, producing more varied recommendations across iterations.
On top of that, few-shot prompting consistently increases recommendation diversity compared to zero-shot or identity prompting, likely due to the inclusion of different random examples across repetitions, as minor variations in prompts often lead to more output variability <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib62" title="">2024</a>)</cite>.
Importantly, this increase in diversity does not adversely impact recommendation accuracy across prompting strategies, as demonstrated by our reported results.
Therefore, when system operators aim to improve user experience by increasing diversity, few-shot prompting or introducing minor variation in prompts may be preferred over simple zero-shot prompting, as performance remains comparable while allowing for greater variation in the resulting LLM responses.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="178" id="S4.F6.g1" src="x6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.3.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text ltx_font_bold" id="S4.F6.4.2" style="font-size:90%;">Release Years.<span class="ltx_text ltx_font_medium" id="S4.F6.4.2.1"> Distribution of movies recommended by LLMs by their release year (visualized from 1940 to 2024) depicts strong preference for movies released after 2000.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">Finally, as recent works have highlighted the recency bias in LLMs <cite class="ltx_cite ltx_citemacro_citep">(Deldjoo, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib13" title="">2024</a>)</cite>, we investigate release years of the recommended movies (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S4.F6" title="Figure 6 ‣ 4. Discussion ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">6</span></a> and Appendix, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F17" title="Figure 17 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">17</span></a>).
Our investigation indicates a noticeable preference for more recent movies, particularly those released after 2000.
This suggests that LLMs tend to favor newer content, possibly influenced by inherent biases in the training data such as data availability or popularity of contemporary movies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p8">
<p class="ltx_p" id="S4.p8.1"><span class="ltx_text ltx_font_bold" id="S4.p8.1.1">Limitations and Future Work.</span>
Our study has certain limitations, which we consider as opportunities for future research.
First, our evaluation is restricted to a single domain and dataset from the r/MovieSuggestions subreddit.
While this allows for a focused analysis of a wide range of open- and closed-source LLMs across various sizes, our work can be expanded by a broader validation of our approaches in similar recommendation scenarios where users formulate narrative requests for other domains, such as books or music (e.g., on the r/ifyoulikeblank subreddit).
Second, we focus on three prompting strategies—zero-shot, identity, and few-shot prompting—while excluding more sophisticated approaches, such as chain-of-thought or tree-of-thought prompting, which have shown promise in tasks requiring strategic lookahead and may enhance recommender adaptability.
Additionally, given the marginal differences in the results for our employed strategies, we do not investigate combinations of multiple prompting strategies (e.g., identity and few-shot).
Moreover, the integration of multimodal inputs, such as images or audio, alongside textual user queries could further enrich the recommendation process, enhancing the system’s ability to respond to complex, multifaceted user preferences.
Third, we did not test different individual LLM configurations, including parameters such as sampling method, temperature, or quantization details, which could further modify the output’s variability, diversity, and performance.
Instead, we employed all LLMs using their default parameter settings, derived from official documentation or Ollama configuration files.
When applying our findings to build real-world tools, practitioners should consider further evaluating these default settings to optimize recommendation quality and computational efficiency based on their specific use cases.
Finally, we do not explore beyond recency bias to uncover other biases, such as skews in topic or popularity, that may be inherent in LLM responses.
Exploring these biases further could yield valuable insights for both recommendation algorithms and pre-training of LLMs.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Further Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The emergence of LLMs, such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib15" title="">2018</a>)</cite> or GPT <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib47" title="">2018</a>)</cite>, has led to new developments in NLP.
Due to the significant increase in model sizes and the massive amount of training data, LLMs have shown ability of understanding and executing a wide variety of tasks, such as reasoning <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib11" title="">2022</a>)</cite>, software engineering <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib25" title="">2024</a>)</cite> or language translation <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib10" title="">2020</a>)</cite>.
OpenAI’s ChatGPT as well as several competitive open-source LLMs, including Meta’s Llama, Mistral, and Microsoft’s Phi, have demonstrated exceptional performance, surpassing prior state-of-the-art NLP models across numerous tasks <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib58" title="">2024</a>; Ibrahim, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib26" title="">2024</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib63" title="">2024</a>; Haider et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib23" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">LLMs are typically instructed through prompts—instructions to translate texts, answer questions, or write essays.
Recent advancements in LLMs have sparked growing interest in enhancing their task performance through various prompting strategies <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib10" title="">2020</a>)</cite>, such as zero-shot, identity, or few-shot prompting.
Further promising prompting strategies, such as chain-of-thought <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib57" title="">2022</a>)</cite> or tree-of-thought <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib60" title="">2023</a>)</cite>, enhance the reasoning abilities of generative models, particularly in tasks such as question answering, by guiding them through human-like logical reasoning processes.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Additionally, recent work explored the suitability of LLMs for recommenders, revealing that models, such as Llama and GPT, improve user satisfaction with richer explanations <cite class="ltx_cite ltx_citemacro_citep">(Lubos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib35" title="">2024</a>)</cite>, excel in data-sparse <cite class="ltx_cite ltx_citemacro_citep">(Palma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib44" title="">2024</a>)</cite> and cold-start <cite class="ltx_cite ltx_citemacro_citep">(Sanner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib52" title="">2023</a>)</cite> scenarios, and can deliver highly relevant recommendations through conversational interactions <cite class="ltx_cite ltx_citemacro_citep">(Musto et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#bib.bib39" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">This Work.</span>
In this paper, we are—to the best of our knowledge—the first to address the suitability of LLMs as narrative-driven recommenders.
In particular, we utilize user-submitted text from reddit, consisting of narrative requests for movie recommendations, to evaluate the quality of LLM-generated recommendations.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we evaluate the suitability of LLMs in a narrative-driven movie recommendation setting, comparing their performance to traditional state-of-the-art recommender systems. Our comprehensive analysis of 38 LLMs, both open- and closed-source, reveals that LLMs can effectively generate personalized movie recommendations from user-provided narratives, significantly outperforming traditional approaches, such as <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.1">doc2vec</span>.
We find that while larger closed-source models generally demonstrate superior performance, medium-sized open-source models remain competitive, offering a viable trade-off between computational cost and recommendation quality.
Notably, we observe minimal differences in effectiveness between zero-shot, identity, and few-shot prompting, indicating that simple approaches are sufficient for generating high-quality recommendations.
These findings highlight the potential of LLMs to transform narrative-driven recommender systems, offering scalable solutions for integrating natural language capabilities into real-world applications.
Our results also underscore the versatility of LLMs in real-world recommender applications.
Minimizing prompt complexity is crucial, as it reduces operational overhead and latency, simplifying the integration of LLMs into existing recommendation frameworks.
This finding is particularly valuable for researchers and practitioners aiming to incorporate LLMs in practical settings without the additional computational costs of model fine-tuning or complex prompt engineering.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade
Jacobs, Ammar Ahmad Awan, Jyoti Aneja,
Ahmed Awadallah, et al<span class="ltx_text" id="bib.bib2.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">Phi-3 Technical Report: A Highly Capable Language
Model Locally on Your Phone.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.14219 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2404.14219" title="">https://doi.org/10.48550/arXiv.2404.14219</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
01. AI, :,
Alex Young, Bei Chen,
Chao Li, et al<span class="ltx_text" id="bib.bib3.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Yi: Open Foundation Models by 01.AI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.04652 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2403.04652" title="">https://doi.org/10.48550/arXiv.2403.04652</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024a)</span>
<span class="ltx_bibblock">
Mistral AI.
2024a.

</span>
<span class="ltx_bibblock">AI in abundance: Introducing a free API, improved
pricing across the board, a new enterprise-grade Mistral Small, and free
vision capabilities on le Chat.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/september-24-release/" title="">https://mistral.ai/news/september-24-release/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024b)</span>
<span class="ltx_bibblock">
Mistral AI.
2024b.

</span>
<span class="ltx_bibblock">Cheaper, Better, Faster, Stronger.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mixtral-8x22b/" title="">https://mistral.ai/news/mixtral-8x22b/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024c)</span>
<span class="ltx_bibblock">
Mistral AI.
2024c.

</span>
<span class="ltx_bibblock">Large Enough.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mistral-large-2407/" title="">https://mistral.ai/news/mistral-large-2407/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024d)</span>
<span class="ltx_bibblock">
Mistral AI.
2024d.

</span>
<span class="ltx_bibblock">Mistral NeMo.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mistral-nemo/" title="">https://mistral.ai/news/mistral-nemo/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baumgartner et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jason Baumgartner, Savvas
Zannettou, Brian Keegan, Megan Squire,
and Jeremy Blackburn. 2020.

</span>
<span class="ltx_bibblock">The pushshift reddit dataset. In
<em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the international AAAI conference on
web and social media</em>, Vol. 14. 830–839.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609/icwsm.v14i1.7347" title="">https://doi.org/10.1609/icwsm.v14i1.7347</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogers and Koolen (2017)</span>
<span class="ltx_bibblock">
Toine Bogers and Marijn
Koolen. 2017.

</span>
<span class="ltx_bibblock">Defining and Supporting Narrative-Driven
Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Eleventh ACM
Conference on Recommender Systems</em> (Como, Italy)
<em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">(RecSys ’17)</em>. Association for
Computing Machinery, New York, NY, USA,
238–242.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3109859.3109893" title="">https://doi.org/10.1145/3109859.3109893</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann,
Nick Ryder, Melanie Subbiah,
Jared D Kaplan, et al<span class="ltx_text" id="bib.bib10.3.1">.</span>
2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners. In
<em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">Advances in Neural Information Processing
Systems</em>, H. Larochelle,
M. Ranzato, R. Hadsell,
M.F. Balcan, and H. Lin (Eds.),
Vol. 33. Curran Associates, Inc.,
1877–1901.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery,
Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra,
et al<span class="ltx_text" id="bib.bib11.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2204.02311 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2204.02311" title="">https://doi.org/10.48550/arXiv.2204.02311</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunhao Dai, Ninglu Shao,
Haiyuan Zhao, Weijie Yu,
Zihua Si, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Uncovering ChatGPT’s Capabilities in Recommender
Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">Proceedings of the 17th ACM Conference
on Recommender Systems</em> (Singapore, Singapore)
<em class="ltx_emph ltx_font_italic" id="bib.bib12.5.2">(RecSys ’23)</em>. Association for
Computing Machinery, New York, NY, USA,
1126–1132.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3610646" title="">https://doi.org/10.1145/3604915.3610646</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deldjoo (2024)</span>
<span class="ltx_bibblock">
Yashar Deldjoo.
2024.

</span>
<span class="ltx_bibblock">Understanding Biases in ChatGPT-based Recommender
Systems: Provider Fairness, Temporal Stability, and Recency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">ACM Trans. Recomm. Syst.</em>
(Aug. 2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3690655" title="">https://doi.org/10.1145/3690655</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Fan Deng, Stefan
Siersdorfer, and Sergej Zerr.
2012.

</span>
<span class="ltx_bibblock">Efficient jaccard-based diversity analysis of large
document collections. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 21st
ACM International Conference on Information and Knowledge Management</em> (Maui,
Hawaii, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.2">(CIKM ’12)</em>.
Association for Computing Machinery,
New York, NY, USA, 1402–1411.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2396761.2398445" title="">https://doi.org/10.1145/2396761.2398445</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2018.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Computing Research Repository (CoRR)</em>
(2018).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.1810.04805" title="">https://doi.org/10.48550/arXiv.1810.04805</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eberhard et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lukas Eberhard, Kristina
Popova, Simon Walk, and Denis Helic.
2024.

</span>
<span class="ltx_bibblock">Computing recommendations from free-form text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Expert Systems with Applications</em>
236 (2024), 121268.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.eswa.2023.121268" title="">https://doi.org/10.1016/j.eswa.2023.121268</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eberhard et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lukas Eberhard, Simon
Walk, and Denis Helic. 2020.

</span>
<span class="ltx_bibblock">Tell Me What You Want: Embedding Narratives for
Movie Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 31st
ACM Conference on Hypertext and Social Media</em> (Virtual Event, USA)
<em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(HT ’20)</em>. Association for
Computing Machinery, New York, NY, USA,
301–306.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3372923.3404818" title="">https://doi.org/10.1145/3372923.3404818</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eberhard et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Lukas Eberhard, Simon
Walk, Lisa Posch, and Denis Helic.
2019.

</span>
<span class="ltx_bibblock">Evaluating Narrative-driven Movie Recommendations
on Reddit. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 24th
International Conference on Intelligent User Interfaces</em> (Marina del Ray,
California) <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.2">(IUI ’19)</em>.
Association for Computing Machinery,
New York, NY, USA, 1–11.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.acm.org/10.1145/3301275.3302287" title="">https://doi.acm.org/10.1145/3301275.3302287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei-Fei et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Li Fei-Fei, R. Fergus,
and P. Perona. 2006.

</span>
<span class="ltx_bibblock">One-shot learning of object categories.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em> 28, 4
(2006), 594–611.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2006.79" title="">https://doi.org/10.1109/TPAMI.2006.79</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fink (2004)</span>
<span class="ltx_bibblock">
Michael Fink.
2004.

</span>
<span class="ltx_bibblock">Object Classification from a Single Example
Utilizing Class Relevance Metrics. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings
of the 17th International Conference on Neural Information Processing
Systems</em> (Vancouver, British Columbia, Canada)
<em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">(NIPS’04)</em>. MIT Press,
Cambridge, MA, USA, 449–456.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2024)</span>
<span class="ltx_bibblock">
Google. 2024.

</span>
<span class="ltx_bibblock">Gemma 2 is now available to researchers and
developers.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/technology/developers/google-gemma-2/" title="">https://blog.google/technology/developers/google-gemma-2/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang,
Ziyuan Wang, Minqi Jiang,
Jinran Nie, et al<span class="ltx_text" id="bib.bib22.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">How Close is ChatGPT to Human Experts? Comparison
Corpus, Evaluation, and Detection.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2301.07597 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2301.07597" title="">https://doi.org/10.48550/arXiv.2301.07597</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haider et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Emman Haider, Daniel
Perez-Becker, Thomas Portet, Piyush
Madan, Amit Garg, et al<span class="ltx_text" id="bib.bib23.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">Phi-3 Safety Post-Training: Aligning Language Models
with a ”Break-Fix” Cycle.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.13833 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2407.13833" title="">https://doi.org/10.48550/arXiv.2407.13833</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhankui He, Zhouhang Xie,
Rahul Jha, Harald Steck,
Dawen Liang, et al<span class="ltx_text" id="bib.bib24.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Large Language Models as Zero-Shot Conversational
Recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management</em>
(Birmingham, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib24.5.2">(CIKM ’23)</em>.
Association for Computing Machinery,
New York, NY, USA, 720–730.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3583780.3614949" title="">https://doi.org/10.1145/3583780.3614949</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xinyi Hou, Yanjie Zhao,
Yue Liu, Zhou Yang,
Kailong Wang, et al<span class="ltx_text" id="bib.bib25.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">Large Language Models for Software Engineering: A
Systematic Literature Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.4.1">ACM Trans. Softw. Eng. Methodol.</em>
(Sept. 2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3695988" title="">https://doi.org/10.1145/3695988</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ibrahim (2024)</span>
<span class="ltx_bibblock">
Michael Ibrahim.
2024.

</span>
<span class="ltx_bibblock">Fine-Grained Languagebased Reliability Detection in
Spanish New with Fine-Tuned Llama-3 Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">In
Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2024),
co-located with the 40th Conference of the Spanish Society for Natural
Language Processing (SEPLN 2024), CEURWS. org</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacob et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Benoit Jacob, Skirmantas
Kligys, Bo Chen, Menglong Zhu,
Matthew Tang, et al<span class="ltx_text" id="bib.bib27.3.1">.</span>
2018.

</span>
<span class="ltx_bibblock">Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In
<em class="ltx_emph ltx_font_italic" id="bib.bib27.4.1">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>. 2704–2713.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre
Sablayrolles, Arthur Mensch, Chris
Bamford, Devendra Singh Chaplot, et al<span class="ltx_text" id="bib.bib28.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.06825 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2310.06825" title="">https://doi.org/10.48550/arXiv.2310.06825</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre
Sablayrolles, Antoine Roux, Arthur
Mensch, Blanche Savary, et al<span class="ltx_text" id="bib.bib29.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">Mixtral of Experts.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.04088 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2401.04088" title="">https://doi.org/10.48550/arXiv.2401.04088</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Junseok Kim, Nakyeong
Yang, and Kyomin Jung. 2024.

</span>
<span class="ltx_bibblock">Persona is a Double-edged Sword: Enhancing the
Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2408.08631</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Larochelle et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Hugo Larochelle, Dumitru
Erhan, and Yoshua Bengio.
2008.

</span>
<span class="ltx_bibblock">Zero-Data Learning of New Tasks. In
<em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 23rd National Conference on
Artificial Intelligence - Volume 2</em> (Chicago, Illinois)
<em class="ltx_emph ltx_font_italic" id="bib.bib31.4.2">(AAAI’08)</em>. AAAI Press,
646–651.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.5555/1620163.1620172" title="">https://dl.acm.org/doi/10.5555/1620163.1620172</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le and Mikolov (2014)</span>
<span class="ltx_bibblock">
Quoc Le and Tomas
Mikolov. 2014.

</span>
<span class="ltx_bibblock">Distributed Representations of Sentences and
Documents. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 31st
International Conference on Machine Learning</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">(Proceedings of Machine Learning Research,
Vol. 32)</em>. PMLR,
Bejing, China, 1188–1196.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v32/le14.html" title="">https://proceedings.mlr.press/v32/le14.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junling Liu, Chao Liu,
Renjie Lv, Kang Zhou, and
Yan Zhang. 2023.

</span>
<span class="ltx_bibblock">Is ChatGPT a Good Recommender? A Preliminary Study.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.10149 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2304.10149" title="">https://doi.org/10.48550/arXiv.2304.10149</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Qijiong Liu, Nuo Chen,
Tetsuya Sakai, and Xiao-Ming Wu.
2024.

</span>
<span class="ltx_bibblock">ONCE: Boosting Content-based Recommendation with
Both Open- and Closed-source Large Language Models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the 17th ACM International
Conference on Web Search and Data Mining</em> (Merida, Mexico)
<em class="ltx_emph ltx_font_italic" id="bib.bib34.4.2">(WSDM ’24)</em>. Association for
Computing Machinery, New York, NY, USA,
452–461.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3616855.3635845" title="">https://doi.org/10.1145/3616855.3635845</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lubos et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sebastian Lubos, Thi
Ngoc Trang Tran, Alexander Felfernig,
Seda Polat Erdeniz, and Viet-Man Le.
2024.

</span>
<span class="ltx_bibblock">LLM-generated Explanations for Recommender
Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Adjunct Proceedings of the 32nd ACM
Conference on User Modeling, Adaptation and Personalization</em> (Cagliari,
Italy) <em class="ltx_emph ltx_font_italic" id="bib.bib35.4.2">(UMAP Adjunct ’24)</em>.
Association for Computing Machinery,
New York, NY, USA, 276–285.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3631700.3665185" title="">https://doi.org/10.1145/3631700.3665185</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024a)</span>
<span class="ltx_bibblock">
Meta. 2024a.

</span>
<span class="ltx_bibblock">Introducing Llama 3.1: Our most capable models to
date.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3-1/" title="">https://ai.meta.com/blog/meta-llama-3-1/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024b)</span>
<span class="ltx_bibblock">
Meta. 2024b.

</span>
<span class="ltx_bibblock">Introducing Meta Llama 3: The most capable openly
available LLM to date.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3/" title="">https://ai.meta.com/blog/meta-llama-3/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024c)</span>
<span class="ltx_bibblock">
Meta. 2024c.

</span>
<span class="ltx_bibblock">Llama 3.2: Revolutionizing edge AI and vision with
open, customizable models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" title="">https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Musto et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Cataldo Musto, Alessandro
Francesco Maria Martina, Andrea Iovine,
Marco de Gemmis, and Giovanni
Semeraro. 2022.

</span>
<span class="ltx_bibblock">Tell Me What You Like: Introducing Natural Language
Preference Elicitation Strategies in a Virtual Assistant for the Movie
Domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Preprint submitted to Expert Systems with
Applications</em> Available at SSRN 4140047
(2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2139/ssrn.4140047" title="">https://doi.org/10.2139/ssrn.4140047</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ollama (2024)</span>
<span class="ltx_bibblock">
Ollama. 2024.

</span>
<span class="ltx_bibblock">Phi-3.5-mini-instruct.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.azure.com/explore/models/Phi-3.5-mini-instruct/version/2/registry/azureml" title="">https://ai.azure.com/explore/models/Phi-3.5-mini-instruct/version/2/registry/azureml</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024a)</span>
<span class="ltx_bibblock">
OpenAI. 2024a.

</span>
<span class="ltx_bibblock">GPT-4o mini: advancing cost-efficient intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" title="">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024b)</span>
<span class="ltx_bibblock">
OpenAI. 2024b.

</span>
<span class="ltx_bibblock">Hello GPT-4o.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam,
Steven Adler, Sandhini Agarwal,
Lama Ahmad, et al<span class="ltx_text" id="bib.bib43.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.08774" title="">https://doi.org/10.48550/arXiv.2303.08774</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Palma et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dario Di Palma,
Giovanni Maria Biancofiore, Vito Walter
Anelli, Fedelucio Narducci, Tommaso Di
Noia, et al<span class="ltx_text" id="bib.bib44.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Evaluating ChatGPT as a Recommender System: A
Rigorous Approach.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.03613 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2309.03613" title="">https://doi.org/10.48550/arXiv.2309.03613</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peeperkorn et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Max Peeperkorn, Tom
Kouwenhoven, Dan Brown, and Anna
Jordanous. 2024.

</span>
<span class="ltx_bibblock">Is temperature the creativity parameter of large
language models?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">arXiv preprint arXiv:2405.00492</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Powers (2008)</span>
<span class="ltx_bibblock">
David Powers.
2008.

</span>
<span class="ltx_bibblock">Evaluation: From Precision, Recall and F-Factor to
ROC, Informedness, Markedness &amp; Correlation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Mach. Learn. Technol.</em> 2
(01 2008).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik
Narasimhan, Tim Salimans, Ilya
Sutskever, et al<span class="ltx_text" id="bib.bib47.3.1">.</span> 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative
pre-training.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ratcliff and Metzener (1988)</span>
<span class="ltx_bibblock">
John W Ratcliff and
David E Metzener. 1988.

</span>
<span class="ltx_bibblock">Pattern-matching-the gestalt approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Dr Dobbs Journal</em> 13,
7 (1988), 46.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reynolds and McDonell (2021)</span>
<span class="ltx_bibblock">
Laria Reynolds and Kyle
McDonell. 2021.

</span>
<span class="ltx_bibblock">Prompt programming for large language models:
Beyond the few-shot paradigm. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Extended
abstracts of the 2021 CHI conference on human factors in computing systems</em>.
1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Manley Roberts, Himanshu
Thakur, Christine Herlihy, Colin White,
and Samuel Dooley. 2023.

</span>
<span class="ltx_bibblock">To the cutoff… and beyond? a longitudinal
perspective on LLM data contamination. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">The
Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sainz et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Oscar Sainz, Jon Campos,
Iker García-Ferrero, Julen Etxaniz,
Oier Lopez de Lacalle, et al<span class="ltx_text" id="bib.bib51.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">NLP Evaluation in trouble: On the Need to Measure
LLM Data Contamination for each Benchmark. In
<em class="ltx_emph ltx_font_italic" id="bib.bib51.4.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, Houda Bouamor,
Juan Pino, and Kalika Bali (Eds.).
Association for Computational Linguistics,
Singapore, 10776–10787.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-emnlp.722" title="">https://doi.org/10.18653/v1/2023.findings-emnlp.722</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanner et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Scott Sanner, Krisztian
Balog, Filip Radlinski, Ben Wedin, and
Lucas Dixon. 2023.

</span>
<span class="ltx_bibblock">Large Language Models are Competitive Near
Cold-start Recommenders for Language- and Item-based Preferences. In
<em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Proceedings of the 17th ACM Conference on
Recommender Systems</em> (Singapore, Singapore) <em class="ltx_emph ltx_font_italic" id="bib.bib52.4.2">(RecSys
’23)</em>. Association for Computing Machinery,
New York, NY, USA, 890–896.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3608845" title="">https://doi.org/10.1145/3604915.3608845</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan,
Xinyu Ma, Shuaiqiang Wang,
Pengjie Ren, et al<span class="ltx_text" id="bib.bib53.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Is ChatGPT Good at Search? Investigating Large
Language Models as Re-Ranking Agents.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.09542 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2304.09542" title="">https://doi.org/10.48550/arXiv.2304.09542</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Gemma Team, Thomas
Mesnard, Cassidy Hardin, Robert Dadashi,
Surya Bhupatiraju, et al<span class="ltx_text" id="bib.bib54.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">Gemma: Open Models Based on Gemini Research and
Technology.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.08295 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2403.08295" title="">https://doi.org/10.48550/arXiv.2403.08295</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2024)</span>
<span class="ltx_bibblock">
Qwen Team.
2024.

</span>
<span class="ltx_bibblock">Qwen2.5: A Party of Foundation Models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://qwenlm.github.io/blog/qwen2.5/" title="">https://qwenlm.github.io/blog/qwen2.5/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed on 2024-10-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Lim (2023)</span>
<span class="ltx_bibblock">
Lei Wang and Ee-Peng
Lim. 2023.

</span>
<span class="ltx_bibblock">Zero-Shot Next-Item Recommendation using Large
Pretrained Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.03153 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2304.03153" title="">https://doi.org/10.48550/arXiv.2304.03153</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang,
Dale Schuurmans, Maarten Bosma,
brian ichter, et al<span class="ltx_text" id="bib.bib57.3.1">.</span>
2022.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in
Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.4.1">Advances in Neural
Information Processing Systems</em>,
S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave,
K. Cho, et al<span class="ltx_text" id="bib.bib57.5.2">.</span> (Eds.), Vol. 35.
Curran Associates, Inc., 24824–24837.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shuyuan Xu, Wenyue Hua,
and Yongfeng Zhang. 2024.

</span>
<span class="ltx_bibblock">OpenP5: An Open-Source Platform for Developing,
Training, and Evaluating LLM-based Recommender Systems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2306.11134 [cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2306.11134" title="">https://doi.org/10.48550/arXiv.2306.11134</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
An Yang, Baosong Yang,
Binyuan Hui, Bo Zheng,
Bowen Yu, et al<span class="ltx_text" id="bib.bib59.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Qwen2 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.10671 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2407.10671" title="">https://doi.org/10.48550/arXiv.2407.10671</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu,
Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, et al<span class="ltx_text" id="bib.bib60.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Tree of Thoughts: Deliberate Problem Solving with
Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.4.1">Advances in Neural
Information Processing Systems</em>, A. Oh,
T. Naumann, A. Globerson,
K. Saenko, M. Hardt, et al<span class="ltx_text" id="bib.bib60.5.2">.</span>
(Eds.), Vol. 36. Curran Associates,
Inc., 11809–11822.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yilmaz et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Emine Yilmaz, Evangelos
Kanoulas, and Javed A. Aslam.
2008.

</span>
<span class="ltx_bibblock">A simple and efficient sampling method for
estimating AP and NDCG. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the 31st
Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval</em> (Singapore, Singapore) <em class="ltx_emph ltx_font_italic" id="bib.bib61.4.2">(SIGIR
’08)</em>. Association for Computing Machinery,
New York, NY, USA, 603–610.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1390334.1390437" title="">https://doi.org/10.1145/1390334.1390437</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lexin Zhou, Wout
Schellaert, Fernando Martínez-Plumed,
Yael Moros-Daval, Cèsar Ferri,
et al<span class="ltx_text" id="bib.bib62.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Larger and more instructable language models become
less reliable.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.4.1">Nature</em> (2024),
1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yutao Zhu, Peitian Zhang,
Chenghao Zhang, Yifei Chen,
Binyu Xie, et al<span class="ltx_text" id="bib.bib63.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">INTERS: Unlocking the Power of Large Language Models
in Search with Instruction Tuning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.06532 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2401.06532" title="">https://doi.org/10.48550/arXiv.2401.06532</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziegler et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Cai-Nicolas Ziegler,
Sean M. McNee, Joseph A. Konstan, and
Georg Lausen. 2005.

</span>
<span class="ltx_bibblock">Improving recommendation lists through topic
diversification. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the 14th
International Conference on World Wide Web</em> (Chiba, Japan)
<em class="ltx_emph ltx_font_italic" id="bib.bib64.4.2">(WWW ’05)</em>. Association for
Computing Machinery, New York, NY, USA,
22–32.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1060745.1060754" title="">https://doi.org/10.1145/1060745.1060754</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We show supplementary materials and figures for multiple subsections of the main body of our paper in this Appendix.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Evaluation of LLM Responses: Response Variance.</span>
For our analysis estimating the variance of LLM responses (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S2.SS3" title="2.3. Evaluation of LLM Responses ‣ 2. LLMs as Recommender Systems ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2.3</span></a>), we show detailed results of our one-way ANOVA in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.T2" title="Table 2 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.p3.1.1">Results: Format Adherence.</span>
While we show aggregated results for all metrics measuring formatting and structural correctness of LLM responses in the main part of the paper (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.SS1" title="3.1. Format Adherence ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we hereafter visualize detailed results for all models regarding valid JSON format (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F8" title="Figure 8 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">8</span></a>), number of recommendations (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F9" title="Figure 9 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">9</span></a>), unique movies (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F10" title="Figure 10 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">10</span></a>), and movies released before the request (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F11" title="Figure 11 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p" id="A1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.p4.1.1">Results: Addressing Potential Data Leakage.</span>
We show the prompt we utilize to label our robustness analysis dataset using GPT-4o in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F7" title="Figure 7 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p" id="A1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.p5.1.1">Results: Recommendation Performance.</span>
In our main result section, we report aggregate performance metrics as well as discuss F1@10 across models in regard to their model families and size category (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.SS2" title="3.2. Recommendation Performance ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
To supplement this, we now plot results for Precision@10, Recall@10, and NDCG@10—which do not differ substantially to F1@10—in similar style (Figs. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F13" title="Figure 13 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F14" title="Figure 14 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">14</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F15" title="Figure 15 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">15</span></a>) as well as further detailing results for F1@10 in a heatmap (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F12" title="Figure 12 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">12</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p6">
<p class="ltx_p" id="A1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.p6.1.1">Discussion: Diversity and Movie Years.</span>
We substantiate the aggregated plot of diversity for all model size categories (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S4" title="4. Discussion ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">4</span></a>, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S4.F5" title="Figure 5 ‣ 4. Discussion ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">5</span></a>) by visualizing inter-list diversity using Jaccard distance for individual models in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F16" title="Figure 16 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">16</span></a>.
Furthermore, we lay out more details about the release year distribution of the movies contained in LLM responses in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#A1.F17" title="Figure 17 ‣ Appendix A Appendix ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="457" id="A1.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F7.3.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F7.4.2" style="font-size:90%;">Prompt for Robustness Analysis Dataset Creation.<span class="ltx_text ltx_font_medium" id="A1.F7.4.2.1"> We visualize the system prompt for tagging movies in reddit submissions for our robustness experiment addressing data leakage (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13604v1#S3.SS3" title="3.3. Addressing Potential Data Leakage ‣ 3. Results ‣ Large Language Models as Narrative-Driven Recommenders"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
It contains the task and format constraints as well as a few-shot example.
When prompting GPT-4o as an expert labeler, we utilize this system prompt along user prompts containing reddit submission texts.
</span></span></figcaption>
</figure>
<figure class="ltx_table" id="A1.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T2.5.2.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text ltx_font_bold" id="A1.T2.2.1" style="font-size:90%;">Zero-Shot Performance Comparison Across 30 Repetitions Using One-Way ANOVA RM.<span class="ltx_text ltx_font_medium" id="A1.T2.2.1.1">
This table presents the F-values (F Value) and associated p-values (Pr ¿ F) for Precision@10, Recall@10, F1@10, and NDCG@10 metrics across 30 repetitions using zero-shot prompting for each LLM.
The analysis is based on 29 numerator degrees of freedom (DF) and 8 555 denominator DF.
The results exhibit no statistically significant differences in performance metrics between repetitions at the standard threshold of p ¡ 0.05. To account for multiple tests we use Bonferroni correction and divide the significance level of 0.05 with the total number of tests (152) to obtain the final significance level of p ¡ <math alttext="\mathbf{3.3\cdot 10^{-4}}" class="ltx_Math" display="inline" id="A1.T2.2.1.1.m1.1"><semantics id="A1.T2.2.1.1.m1.1b"><mrow id="A1.T2.2.1.1.m1.1.1" xref="A1.T2.2.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="A1.T2.2.1.1.m1.1.1.2" mathvariant="bold" xref="A1.T2.2.1.1.m1.1.1.2.cmml">3.3</mn><mo id="A1.T2.2.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.T2.2.1.1.m1.1.1.1.cmml">⋅</mo><msup id="A1.T2.2.1.1.m1.1.1.3" xref="A1.T2.2.1.1.m1.1.1.3.cmml"><mn id="A1.T2.2.1.1.m1.1.1.3.2" xref="A1.T2.2.1.1.m1.1.1.3.2.cmml">𝟏𝟎</mn><mrow id="A1.T2.2.1.1.m1.1.1.3.3" xref="A1.T2.2.1.1.m1.1.1.3.3.cmml"><mo id="A1.T2.2.1.1.m1.1.1.3.3b" xref="A1.T2.2.1.1.m1.1.1.3.3.cmml">−</mo><mn id="A1.T2.2.1.1.m1.1.1.3.3.2" xref="A1.T2.2.1.1.m1.1.1.3.3.2.cmml">𝟒</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.T2.2.1.1.m1.1c"><apply id="A1.T2.2.1.1.m1.1.1.cmml" xref="A1.T2.2.1.1.m1.1.1"><ci id="A1.T2.2.1.1.m1.1.1.1.cmml" xref="A1.T2.2.1.1.m1.1.1.1">⋅</ci><cn id="A1.T2.2.1.1.m1.1.1.2.cmml" type="float" xref="A1.T2.2.1.1.m1.1.1.2">3.3</cn><apply id="A1.T2.2.1.1.m1.1.1.3.cmml" xref="A1.T2.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.T2.2.1.1.m1.1.1.3.1.cmml" xref="A1.T2.2.1.1.m1.1.1.3">superscript</csymbol><cn id="A1.T2.2.1.1.m1.1.1.3.2.cmml" type="integer" xref="A1.T2.2.1.1.m1.1.1.3.2">10</cn><apply id="A1.T2.2.1.1.m1.1.1.3.3.cmml" xref="A1.T2.2.1.1.m1.1.1.3.3"><minus id="A1.T2.2.1.1.m1.1.1.3.3.1.cmml" xref="A1.T2.2.1.1.m1.1.1.3.3"></minus><cn id="A1.T2.2.1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A1.T2.2.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.2.1.1.m1.1d">\mathbf{3.3\cdot 10^{-4}}</annotation><annotation encoding="application/x-llamapun" id="A1.T2.2.1.1.m1.1e">bold_3.3 ⋅ bold_10 start_POSTSUPERSCRIPT - bold_4 end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T2.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T2.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T2.6.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T2.6.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T2.6.1.1.2" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.1.1.2.1">
<span class="ltx_p" id="A1.T2.6.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T2.6.1.1.2.1.1.1">Model Name</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T2.6.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T2.6.1.1.3.1">Precision@10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T2.6.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T2.6.1.1.4.1">Recall@10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T2.6.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T2.6.1.1.5.1">F1@10</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T2.6.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T2.6.1.1.6.1">NDCG@10</span></td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T2.6.2.2.1.1">Category</span></th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.2.2.2">F Value</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.2.2.3">Pr ¿ F</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.2.2.4">F Value</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.2.2.5">Pr ¿ F</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.2.2.6">F Value</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.2.2.7">Pr ¿ F</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.2.2.8">F Value</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.2.2.9">Pr ¿ F</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.3.3.1" rowspan="11"><span class="ltx_text" id="A1.T2.6.3.3.1.1">Tiny</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.3.3.2.1">
<span class="ltx_p" id="A1.T2.6.3.3.2.1.1">Gemma 2B</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.3.3.3">0.5892</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.3.3.4">0.9607</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.3.3.5">1.0027</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.3.3.6">0.4612</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.3.3.7">0.8696</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.3.3.8">0.6667</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.3.3.9">0.8290</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.3.3.10">0.7267</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.4.4">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.4.4.1.1">
<span class="ltx_p" id="A1.T2.6.4.4.1.1.1">Gemma 2 2B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.4.4.2">0.5518</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.4.4.3">0.9754</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.4.4.4">0.7571</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.4.4.5">0.8218</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.4.4.6">0.6942</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.4.4.7">0.8885</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.4.4.8">0.5749</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.4.4.9">0.9669</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.5.5">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.5.5.1.1">
<span class="ltx_p" id="A1.T2.6.5.5.1.1.1">Llama 3.2 1B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.5.5.2">1.5303</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.5.5.3">0.0341</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.5.5.4">1.7137</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.5.5.5">0.0099</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.5.5.6">1.6844</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.5.5.7">0.0122</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.5.5.8">1.5194</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.5.5.9">0.0366</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.6.6">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.6.6.1.1">
<span class="ltx_p" id="A1.T2.6.6.6.1.1.1">Llama 3.2 3B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.6.6.2">1.1144</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.6.6.3">0.3066</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.6.6.4">1.2545</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.6.6.5">0.1633</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.6.6.6">1.1839</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.6.6.7">0.2278</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.6.6.8">1.1578</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.6.6.9">0.2556</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.7.7">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.7.7.1.1">
<span class="ltx_p" id="A1.T2.6.7.7.1.1.1">Phi-3 3.8B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.7.7.2">1.0710</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.7.7.3">0.3630</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.7.7.4">0.6720</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.7.7.5">0.9079</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.7.7.6">0.7118</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.7.7.7">0.8716</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.7.7.8">0.9528</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.7.7.9">0.5377</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.8.8">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.8.8.1.1">
<span class="ltx_p" id="A1.T2.6.8.8.1.1.1">Phi-3.5 3.8B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.8.8.2">0.9381</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.8.8.3">0.5607</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.8.8.4">1.1038</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.8.8.5">0.3199</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.8.8.6">1.0670</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.8.8.7">0.3684</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.8.8.8">0.9265</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.8.8.9">0.5788</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.9.9">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.9.9.1.1">
<span class="ltx_p" id="A1.T2.6.9.9.1.1.1">Qwen2 0.5B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.9.9.2">0.8661</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.9.9.3">0.6720</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.9.9.4">0.8565</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.9.9.5">0.6865</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.9.9.6">0.8247</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.9.9.7">0.7328</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.9.9.8">0.8677</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.9.9.9">0.6696</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.10.10">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.10.10.1.1">
<span class="ltx_p" id="A1.T2.6.10.10.1.1.1">Qwen2 1.5B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.10.10.2">0.7861</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.10.10.3">0.7855</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.10.10.4">1.3928</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.10.10.5">0.0782</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.10.10.6">1.2753</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.10.10.7">0.1472</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.10.10.8">0.9708</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.10.10.9">0.5099</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.11.11">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.11.11.1.1">
<span class="ltx_p" id="A1.T2.6.11.11.1.1.1">Qwen2.5 0.5B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.11.11.2">1.3712</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.11.11.3">0.0884</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.11.11.4">0.9001</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.11.11.5">0.6199</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.11.11.6">0.9464</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.11.11.7">0.5477</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.11.11.8">1.3100</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.11.11.9">0.1231</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.12.12">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.12.12.1.1">
<span class="ltx_p" id="A1.T2.6.12.12.1.1.1">Qwen2.5 1.5B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.12.12.2">0.8943</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.12.12.3">0.6289</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.12.12.4">1.0432</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.12.12.5">0.4017</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.12.12.6">1.0125</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.12.12.7">0.4466</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.12.12.8">0.8725</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.12.12.9">0.6623</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.13.13">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.13.13.1.1">
<span class="ltx_p" id="A1.T2.6.13.13.1.1.1">Qwen2.5 3B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.13.13.2">0.7361</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.13.13.3">0.8460</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.13.13.4">0.7078</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.13.13.5">0.8756</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.13.13.6">0.7026</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.13.13.7">0.8806</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.13.13.8">0.7395</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.13.13.9">0.8422</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.14.14.1" rowspan="10"><span class="ltx_text" id="A1.T2.6.14.14.1.1">Small</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.14.14.2.1">
<span class="ltx_p" id="A1.T2.6.14.14.2.1.1">Gemma 7B</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.14.14.3">0.9148</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.14.14.4">0.5970</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.14.14.5">0.9425</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.14.14.6">0.5537</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.14.14.7">0.9248</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.14.14.8">0.5815</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.14.14.9">0.6911</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.14.14.10">0.8914</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.15.15">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.15.15.1.1">
<span class="ltx_p" id="A1.T2.6.15.15.1.1.1">Gemma 2 9B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.15.15.2">1.2181</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.15.15.3">0.1946</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.15.15.4">1.3006</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.15.15.5">0.1293</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.15.15.6">1.2950</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.15.15.7">0.1331</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.15.15.8">1.0597</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.15.15.9">0.3785</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.16.16">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.16.16.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.16.16.1.1">
<span class="ltx_p" id="A1.T2.6.16.16.1.1.1">GPT-4o mini</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.16.16.2">1.0238</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.16.16.3">0.4299</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.16.16.4">0.8861</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.16.16.5">0.6416</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.16.16.6">0.9437</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.16.16.7">0.5519</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.16.16.8">1.3070</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.16.16.9">0.1251</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.17.17">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.17.17.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.17.17.1.1">
<span class="ltx_p" id="A1.T2.6.17.17.1.1.1">Llama 3 8B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.17.17.2">1.0565</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.17.17.3">0.3829</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.17.17.4">0.8686</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.17.17.5">0.6683</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.17.17.6">0.8875</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.17.17.7">0.6393</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.17.17.8">1.0257</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.17.17.9">0.4271</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.18.18">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.18.18.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.18.18.1.1">
<span class="ltx_p" id="A1.T2.6.18.18.1.1.1">Llama 3.1 8B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.18.18.2">0.9477</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.18.18.3">0.5457</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.18.18.4">0.7396</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.18.18.5">0.8421</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.18.18.6">0.7734</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.18.18.7">0.8017</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.18.18.8">0.7961</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.18.18.9">0.7723</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.19.19">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.19.19.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.19.19.1.1">
<span class="ltx_p" id="A1.T2.6.19.19.1.1.1">Mistral 7B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.19.19.2">1.4327</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.19.19.3">0.0621</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.19.19.4">1.3772</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.19.19.5">0.0855</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.19.19.6">1.3807</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.19.19.7">0.0838</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.19.19.8">1.2420</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.19.19.9">0.1736</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.20.20">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.20.20.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.20.20.1.1">
<span class="ltx_p" id="A1.T2.6.20.20.1.1.1">Qwen2 7B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.20.20.2">0.9254</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.20.20.3">0.5805</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.20.20.4">0.9736</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.20.20.5">0.5056</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.20.20.6">0.9290</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.20.20.7">0.5748</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.20.20.8">0.7936</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.20.20.9">0.7756</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.21.21">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.21.21.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.21.21.1.1">
<span class="ltx_p" id="A1.T2.6.21.21.1.1.1">Qwen2.5 7B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.21.21.2">0.7321</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.21.21.3">0.8503</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.21.21.4">0.6830</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.21.21.5">0.8986</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.21.21.6">0.6897</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.21.21.7">0.8926</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.21.21.8">0.8768</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.21.21.9">0.6558</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.22.22">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.22.22.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.22.22.1.1">
<span class="ltx_p" id="A1.T2.6.22.22.1.1.1">Yi 6B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.22.22.2">0.6908</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.22.22.3">0.8916</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.22.22.4">0.7751</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.22.22.5">0.7996</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.22.22.6">0.7841</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.22.22.7">0.7881</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.22.22.8">0.7409</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.22.22.9">0.8406</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.23.23">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.23.23.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.23.23.1.1">
<span class="ltx_p" id="A1.T2.6.23.23.1.1.1">Yi 9B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.23.23.2">1.0973</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.23.23.3">0.3281</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.23.23.4">1.1183</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.23.23.5">0.3018</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.23.23.6">1.1393</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.23.23.7">0.2766</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.23.23.8">1.2357</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.23.23.9">0.1789</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.24.24.1" rowspan="9"><span class="ltx_text" id="A1.T2.6.24.24.1.1">Medium</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.24.24.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.24.24.2.1">
<span class="ltx_p" id="A1.T2.6.24.24.2.1.1">Gemma 2 27B</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.24.24.3">1.0867</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.24.24.4">0.3419</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.24.24.5">1.0807</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.24.24.6">0.3499</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.24.24.7">1.0892</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.24.24.8">0.3387</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.24.24.9">1.0701</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.24.24.10">0.3643</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.25.25">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.25.25.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.25.25.1.1">
<span class="ltx_p" id="A1.T2.6.25.25.1.1.1">GPT-3.5 Turbo</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.25.25.2">0.8981</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.25.25.3">0.6230</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.25.25.4">0.9619</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.25.25.5">0.5235</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.25.25.6">0.9313</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.25.25.7">0.5712</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.25.25.8">0.9267</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.25.25.9">0.5785</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.26.26">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.26.26.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.26.26.1.1">
<span class="ltx_p" id="A1.T2.6.26.26.1.1.1">Mistral NeMo 12B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.26.26.2">0.8833</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.26.26.3">0.6459</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.26.26.4">0.8893</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.26.26.5">0.6367</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.26.26.6">0.8701</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.26.26.7">0.6660</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.26.26.8">0.7129</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.26.26.9">0.8704</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.27.27">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.27.27.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.27.27.1.1">
<span class="ltx_p" id="A1.T2.6.27.27.1.1.1">Mistral Small 22B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.27.27.2">0.9447</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.27.27.3">0.5504</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.27.27.4">0.9341</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.27.27.5">0.5669</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.27.27.6">0.8949</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.27.27.7">0.6279</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.27.27.8">0.9096</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.27.27.9">0.6052</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.28.28">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.28.28.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.28.28.1.1">
<span class="ltx_p" id="A1.T2.6.28.28.1.1.1">Mixtral 8x7B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.28.28.2">0.6807</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.28.28.3">0.9005</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.28.28.4">0.6872</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.28.28.5">0.8949</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.28.28.6">0.6802</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.28.28.7">0.9010</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.28.28.8">0.8734</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.28.28.9">0.6609</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.29.29">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.29.29.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.29.29.1.1">
<span class="ltx_p" id="A1.T2.6.29.29.1.1.1">Phi-3 14B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.29.29.2">0.7426</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.29.29.3">0.8387</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.29.29.4">0.8350</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.29.29.5">0.7181</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.29.29.6">0.8038</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.29.29.7">0.7619</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.29.29.8">0.6936</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.29.29.9">0.8890</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.30.30">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.30.30.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.30.30.1.1">
<span class="ltx_p" id="A1.T2.6.30.30.1.1.1">Qwen2.5 14B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.30.30.2">1.6317</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.30.30.3">0.0175</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.30.30.4">1.3929</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.30.30.5">0.0782</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.30.30.6">1.4445</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.30.30.7">0.0579</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.30.30.8">1.7059</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.30.30.9">0.0105</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.31.31">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.31.31.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.31.31.1.1">
<span class="ltx_p" id="A1.T2.6.31.31.1.1.1">Qwen2.5 32B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.31.31.2">1.3921</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.31.31.3">0.0785</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.31.31.4">1.1496</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.31.31.5">0.2648</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.31.31.6">1.1724</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.31.31.7">0.2397</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.31.31.8">1.5363</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.31.31.9">0.0328</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.32.32">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.32.32.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.32.32.1.1">
<span class="ltx_p" id="A1.T2.6.32.32.1.1.1">Yi 34B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.32.32.2">0.4645</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.32.32.3">0.9937</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.32.32.4">0.5696</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.32.32.5">0.9690</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.32.32.6">0.5277</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.32.32.7">0.9824</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.32.32.8">0.7070</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.32.32.9">0.8764</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.33.33">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A1.T2.6.33.33.1" rowspan="8"><span class="ltx_text" id="A1.T2.6.33.33.1.1">Large</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T2.6.33.33.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.33.33.2.1">
<span class="ltx_p" id="A1.T2.6.33.33.2.1.1">GPT-4o</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.33.33.3">1.1677</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.33.33.4">0.2448</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.33.33.5">1.2949</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.33.33.6">0.1332</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.33.33.7">1.2904</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.6.33.33.8">0.1363</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.33.33.9">0.7112</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.6.33.33.10">0.8722</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.34.34">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.34.34.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.34.34.1.1">
<span class="ltx_p" id="A1.T2.6.34.34.1.1.1">Llama 3 70B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.34.34.2">0.7460</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.34.34.3">0.8348</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.34.34.4">0.8824</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.34.34.5">0.6472</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.34.34.6">0.8725</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.34.34.7">0.6623</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.34.34.8">0.5791</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.34.34.9">0.9651</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.35.35">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.35.35.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.35.35.1.1">
<span class="ltx_p" id="A1.T2.6.35.35.1.1.1">Llama 3.1 70B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.35.35.2">1.4396</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.35.35.3">0.0596</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.35.35.4">1.3234</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.35.35.5">0.1147</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.35.35.6">1.3243</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.35.35.7">0.1141</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.35.35.8">1.4188</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.35.35.9">0.0674</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.36.36">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.36.36.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.36.36.1.1">
<span class="ltx_p" id="A1.T2.6.36.36.1.1.1">Llama 3.1 405B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.36.36.2">1.0021</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.36.36.3">0.4620</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.36.36.4">0.9738</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.36.36.5">0.5051</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.36.36.6">1.0004</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.36.36.7">0.4647</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.36.36.8">0.9988</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.36.36.9">0.4671</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.37.37">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.37.37.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.37.37.1.1">
<span class="ltx_p" id="A1.T2.6.37.37.1.1.1">Mistral Large 123B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.37.37.2">1.1131</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.37.37.3">0.3082</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.37.37.4">1.0666</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.37.37.5">0.3690</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.37.37.6">1.0983</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.37.37.7">0.3269</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.37.37.8">1.4827</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.37.37.9">0.0459</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.38.38">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.38.38.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.38.38.1.1">
<span class="ltx_p" id="A1.T2.6.38.38.1.1.1">Mixtral 8x22B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.38.38.2">0.9408</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.38.38.3">0.5565</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.38.38.4">0.7787</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.38.38.5">0.7951</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.38.38.6">0.8451</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.38.38.7">0.7033</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.38.38.8">1.0889</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.38.38.9">0.3391</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.39.39">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r" id="A1.T2.6.39.39.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.39.39.1.1">
<span class="ltx_p" id="A1.T2.6.39.39.1.1.1">Qwen2 72B</span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T2.6.39.39.2">0.8674</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.39.39.3">0.6701</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.39.39.4">0.7723</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.39.39.5">0.8032</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.39.39.6">0.7891</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.6.39.39.7">0.7816</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.39.39.8">0.8315</td>
<td class="ltx_td ltx_align_center" id="A1.T2.6.39.39.9">0.7231</td>
</tr>
<tr class="ltx_tr" id="A1.T2.6.40.40">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T2.6.40.40.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T2.6.40.40.1.1">
<span class="ltx_p" id="A1.T2.6.40.40.1.1.1">Qwen2.5 72B</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.6.40.40.2">1.0417</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T2.6.40.40.3">0.4039</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.6.40.40.4">1.0040</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T2.6.40.40.5">0.4592</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.6.40.40.6">1.0458</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T2.6.40.40.7">0.3980</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.6.40.40.8">1.1607</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.6.40.40.9">0.2525</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="936" id="A1.F8.g1" src="x8.png" width="780"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F8.3.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F8.4.2" style="font-size:90%;">JSON Format.<span class="ltx_text ltx_font_medium" id="A1.F8.4.2.1"> This figure shows the proportion of valid JSON responses generated by LLMs [bootstrapped 95% confidence intervals]. The results are categorized by model size and prompting strategies. Despite observing high proportion of valid output across all models, increased model size impacts the accuracy of valid output. Larger models achieve higher percentages of valid responses, demonstrating a benefit with scale.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.F9.1" style="width:419.4pt;height:491.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:491.9pt;transform:translate(-36.22pt,-35.72pt) rotate(-90deg) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="804" id="A1.F9.1.g1" src="x9.png" width="938"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F9.4.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F9.5.2" style="font-size:90%;">Number of Recommendations.<span class="ltx_text ltx_font_medium" id="A1.F9.5.2.1"> This figure presents the proportion of LLM-generated responses containing zero, one to nine, ten, and more than ten recommendations [bootstrapped 95% confidence intervals]. The results are categorized by model size and prompting strategies. Larger models consistently provide more accurate numbers of recommendations, adhering closely to the prompt specifications, while smaller models display greater variability.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="936" id="A1.F10.g1" src="x10.png" width="780"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F10.3.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F10.4.2" style="font-size:90%;">Unique Movies.<span class="ltx_text ltx_font_medium" id="A1.F10.4.2.1"> This figure shows the average fraction of unique movie recommendations per request [bootstrapped 95% confidence intervals]. The results are categorized by model size and prompting strategies. Medium- and large-sized LLMs consistently returned over 99% unique movies within their valid JSON responses, while tiny and small models exhibited a slight but statistically significant increase in duplicate recommendations.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="936" id="A1.F11.g1" src="x11.png" width="780"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F11.3.1.1" style="font-size:90%;">Figure 11</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F11.4.2" style="font-size:90%;">Movies Matching the Requested Year.<span class="ltx_text ltx_font_medium" id="A1.F11.4.2.1"> This figure presents the fraction of movies that correspond to the requested release year [bootstrapped 95% confidence intervals]. The results are categorized by model size and prompting strategies. The results highlight the ability of larger LLMs to comply with the constraints stated in the prompt. Tiny model, specifically Qwen2.5 0.5B with zero-shot prompting, struggled to recommend movies in the requested timespan with around 30% of the recommended movies outside the given period.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="936" id="A1.F12.g1" src="x12.png" width="780"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F12.3.1.1" style="font-size:90%;">Figure 12</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F12.4.2" style="font-size:90%;">Detailed Recommendation Performance of Different LLMs and Prompting Strategies.<span class="ltx_text ltx_font_medium" id="A1.F12.4.2.1"> We report F1@10 [bootstrapped 95% confidence intervals] for evaluated LLMs. The results are categorized by model size and prompting strategies.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="262" id="A1.F13.g1" src="x13.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F13.fig1" style="width:112.7pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F13.fig1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A1.F13.fig1.2.2" style="font-size:90%;">Zero-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F13.fig2" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F13.fig2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A1.F13.fig2.2.2" style="font-size:90%;">Identity Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F13.fig3" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F13.fig3.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="A1.F13.fig3.2.2" style="font-size:90%;">Few-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F13.fig4" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F13.fig4.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="A1.F13.fig4.2.2" style="font-size:90%;">Average of All Strategies</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F13.3.1.1" style="font-size:90%;">Figure 13</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F13.4.2" style="font-size:90%;">Precision@10 Across All Models.<span class="ltx_text ltx_font_medium" id="A1.F13.4.2.1">
We show results for precision for all models and prompting strategies.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="259" id="A1.F14.g1" src="x14.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F14.fig1" style="width:112.7pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F14.fig1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A1.F14.fig1.2.2" style="font-size:90%;">Zero-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F14.fig2" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F14.fig2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A1.F14.fig2.2.2" style="font-size:90%;">Identity Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F14.fig3" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F14.fig3.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="A1.F14.fig3.2.2" style="font-size:90%;">Few-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F14.fig4" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F14.fig4.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="A1.F14.fig4.2.2" style="font-size:90%;">Average of All Strategies</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F14.3.1.1" style="font-size:90%;">Figure 14</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F14.4.2" style="font-size:90%;">Recall@10 Across All Models.<span class="ltx_text ltx_font_medium" id="A1.F14.4.2.1">
We show results for recall for all models and prompting strategies.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="262" id="A1.F15.g1" src="x15.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F15.fig1" style="width:112.7pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F15.fig1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A1.F15.fig1.2.2" style="font-size:90%;">Zero-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F15.fig2" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F15.fig2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A1.F15.fig2.2.2" style="font-size:90%;">Identity Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F15.fig3" style="width:104.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F15.fig3.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="A1.F15.fig3.2.2" style="font-size:90%;">Few-Shot Prompting</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.F15.fig4" style="width:106.2pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F15.fig4.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="A1.F15.fig4.2.2" style="font-size:90%;">Average of All Strategies</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F15.3.1.1" style="font-size:90%;">Figure 15</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F15.4.2" style="font-size:90%;">NDCG@10 Across All Models.<span class="ltx_text ltx_font_medium" id="A1.F15.4.2.1">
We show results for NDCG for all models and prompting strategies.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="936" id="A1.F16.g1" src="x16.png" width="780"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F16.3.1.1" style="font-size:90%;">Figure 16</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F16.4.2" style="font-size:90%;">Inter-list Diversity Across Repetitions.<span class="ltx_text ltx_font_medium" id="A1.F16.4.2.1"> The figure illustrates the inter-list Diversity@10 [bootstrapped 95% confidence intervals] using Jaccard distance of recommendations generated by different LLMs across 30 repetitions for zero-shot and three repetitions for other prompting strategies, showing the variability in the uniqueness of recommended items across runs.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="941" id="A1.F17.g1" src="x17.png" width="847"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F17.3.1.1" style="font-size:90%;">Figure 17</span>. </span><span class="ltx_text ltx_font_bold" id="A1.F17.4.2" style="font-size:90%;">Distribution of Movie Release Years.<span class="ltx_text ltx_font_medium" id="A1.F17.4.2.1"> This figure shows the distribution of movies recommended by LLMs based on their release years. The results are categorized by model size and prompting strategies. The x-axis represents the year of release, spanning from 1950 to 2025, while the y-axis indicates the fraction of movies released each year. The data, aggregated in 5-year-spans, exhibits a noticeable increase in movie releases over time, with a sharp rise in the number of releases after the year 2000.</span></span></figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 17 14:05:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
