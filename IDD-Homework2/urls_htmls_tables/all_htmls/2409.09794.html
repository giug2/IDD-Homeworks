<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09794] Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity</title><meta property="og:description" content="This paper presents the design and implementation of a Federated Learning (FL) testbed, focusing on its application in cybersecurity and evaluating its resilience against poisoning attacks. Federated Learning allows mu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09794">

<!--Generated on Sat Oct  5 22:38:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  Testbed,  Model Poisoning,  Data Poisoning,  Cybersecurity,  Data Privacy,  Distributed Learning,  Adversarial Attacks.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Anonymous Authors
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Hao Jian Huang1,
Bekzod Iskandarov1,
Mizanur Rahman2,
Hakan T. Otal2, and
M. Abdullah Canbaz2
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">
1Department of Computer Science
</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">
2Department of Information Science and Technology
</span>
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_italic">University at Albany, SUNY</span>
<br class="ltx_break">Albany, New York, United States 
<br class="ltx_break">hhuang22, biskandarov, mmrahman, hotal, mcanbaz [at] albany [dot] edu


</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">This paper presents the design and implementation of a Federated Learning (FL) testbed, focusing on its application in cybersecurity and evaluating its resilience against poisoning attacks. Federated Learning allows multiple clients to collaboratively train a global model while keeping their data decentralized, addressing critical needs for data privacy and security, particularly in sensitive fields like cybersecurity. Our testbed, built using the Flower framework, facilitates experimentation with various FL frameworks, assessing their performance, scalability, and ease of integration. Through a case study on federated intrusion detection systems, we demonstrate the testbed’s capabilities in detecting anomalies and securing critical infrastructure without exposing sensitive network data. Comprehensive poisoning tests, targeting both model and data integrity, evaluate the system’s robustness under adversarial conditions. Our results show that while federated learning enhances data privacy and distributed learning, it remains vulnerable to poisoning attacks, which must be mitigated to ensure its reliability in real-world applications.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, Testbed, Model Poisoning, Data Poisoning, Cybersecurity, Data Privacy, Distributed Learning, Adversarial Attacks.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In today’s data-driven world, the ability to share and analyze data collaboratively is crucial, yet data privacy remains a paramount concern, particularly in sectors like healthcare, finance, and cybersecurity, where sensitive information must be protected under stringent regulations such as GDPR and HIPAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Traditional machine learning, which relies on centralized data collection, heightens the risks of data breaches and regulatory violations, prompting the development of privacy-preserving techniques to address these challenges. The recent Presidential Executive Order on the <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence</span> underscores the growing emphasis on ensuring AI systems, including federated learning, are both secure and privacy-preserving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) has emerged as a groundbreaking solution to these issues, offering a decentralized machine learning framework that allows multiple clients to collaboratively train a model without the need to share raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Originally introduced by Google, FL enables individual devices or organizations to train local models on their own datasets while sharing only model updates (e.g., gradients or parameters) with a central server. This central server aggregates the updates to build a global model, ensuring that the original data never leaves the local devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. By maintaining data privacy and decentralization, FL not only mitigates the risks associated with centralized data collection but also complies with data protection regulations, making it an ideal approach for privacy-sensitive domains such as healthcare, finance, and cybersecurity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The decentralized nature of FL is particularly beneficial in fields where data sharing is restricted due to legal or ethical considerations. In healthcare, for instance, hospitals can collaboratively develop predictive models for disease diagnosis without sharing sensitive patient records <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Similarly, in cybersecurity, organizations can detect threats like malware or phishing attacks without exposing their proprietary network data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This privacy-preserving collaboration significantly enhances the ability to derive meaningful insights from distributed datasets while ensuring compliance with data protection laws. Furthermore, by limiting communication to model updates rather than raw data, FL reduces the risk of data leakage during transmission, further bolstering the security of the process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite these advantages, Federated Learning is not without its challenges. The decentralized nature of FL introduces new vulnerabilities, particularly in the form of poisoning attacks. In a poisoning attack, an adversary deliberately manipulates the training process by introducing malicious data or altering model updates, which can lead to incorrect predictions or even the failure of the global model to converge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. These attacks can take several forms, including data poisoning, where the attacker corrupts the training data, and model poisoning, where the attacker directly manipulates the model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Such attacks pose significant risks in critical domains like healthcare and cybersecurity, where model accuracy and integrity are vital <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Recognizing these challenges, researchers have explored various defense mechanisms to protect FL systems from poisoning attacks. One prominent approach is the use of Byzantine Robust Aggregation (BRA), which aims to filter out malicious updates during the model aggregation process, ensuring that only legitimate updates are incorporated into the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Another strategy involves integrating differential privacy techniques, which add noise to model updates to obscure the contribution of any single participant, thereby preventing adversaries from inferring sensitive information or manipulating the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In addition, blockchain-based approaches have been proposed to enhance the security of FL systems by creating a decentralized, immutable ledger that records all interactions between clients and the central server, ensuring transparency and trust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we introduce a novel approach by developing a Federated Learning testbed tailored to evaluate the nuanced effects of poisoning attacks in cybersecurity contexts. We conduct an in-depth measurement study to systematically analyze the impact of both data and model poisoning on federated learning systems. By simulating real-world adversarial scenarios, we investigate how malicious inputs compromise model performance, convergence, and accuracy at scale. This research offers a detailed exploration of the vulnerabilities inherent in federated learning, providing critical insights that can guide the design of more resilient FL architectures. Our findings contribute to a deeper understanding of how adversarial attacks can degrade system integrity, shaping future strategies for safeguarding FL in privacy-sensitive and security-critical environments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To address the challenges of poisoning attacks in FL and evaluate defense mechanisms, we developed a specialized FL testbed. The testbed is designed to replicate real-world scenarios where data and model poisoning can occur, while providing a controlled environment to assess the resilience of such systems. This section outlines the architecture of our testbed and explains how we simulate and monitor poisoning attacks. By leveraging this setup, we aim to understand the impacts of adversarial interventions and compare the performance of poisoned versus non-poisoned datasets, highlighting key vulnerabilities and potential mitigation strategies.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Overview of System Architecture</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The FL testbed, illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ II-A Overview of System Architecture ‣ II Methodology ‣ Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is designed to support comprehensive experimentation and evaluation of FL models in both poisoned and non-poisoned environments. The testbed follows a client-server architecture implemented using the Flower framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which provides a flexible platform for centralized federated learning workflows. The setup consists of three main components: client nodes, an aggregation server, and communication protocols.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">At the core of the testbed is the aggregation server, a Raspberry Pi 4, chosen for its lightweight footprint and ability to simulate an edge server environment. The server orchestrates the training process, initializing and distributing the global model to the client nodes, which are Nvidia Jetson Nano devices. Each client operates independently, processing local data and sending only model updates (e.g., gradients or weights) back to the server. This decentralized approach ensures that sensitive data remains on the client devices, reflecting real-world FL scenarios where devices vary in computational power, network latency, and data quality (non-IID). The server aggregates these updates, typically using Federated Averaging (FedAvg) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, to refine the global model, which is then redistributed to the clients for further training in iterative rounds.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The communication between the server and the clients is facilitated by bi-directional gRPC streams, enabling efficient, parallel data exchanges. The architecture is highly scalable, allowing adjustments to the number of clients based on specific experiments or use cases.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">In this setup, we explore two key use cases: the first establishes a baseline performance using clean datasets, while the second introduces data poisoning attacks at the client level. By manipulating client data in the poisoned scenario, we analyze how adversarial actions impact the global model’s accuracy, convergence, and robustness. This setup allows us to experiment with different defense mechanisms to mitigate the effects of these attacks, providing insights into the vulnerabilities of federated learning systems under adversarial conditions.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">The following sections delve into the details of our poisoning attack tests and how the testbed is adapted for experimentation with various machine learning models and defense strategies.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.09794/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="322" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Graphical representation of the system architecture for the testbed, illustrating the key components and interactions within the federated learning setup.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Software and Hardware Setup</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Our federated learning testbed is built around a Raspberry Pi 4 as the central server, running Pi OS, paired with five Nvidia Jetson Nano devices acting as client nodes. Each Jetson Nano is equipped with 4 GB of RAM and operates on Ubuntu OS, providing a lightweight yet capable environment for edge computing tasks.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For software development, we utilized Visual Studio Code as the integrated development environment (IDE) and Python as the primary programming language. The machine learning models were implemented using TensorFlow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and Scikit-learn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, enabling us to develop and train an intrusion detection model based on a multi-layer perceptron architecture.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">To support data manipulation and analysis, we integrated Numpy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Pandas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> for efficient data handling and preprocessing. Additionally, Matplotlib <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> was used for visualizing the results of the training processes, providing clear insights into the performance of both ML and FL approaches.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Comparison of FL Frameworks</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the development phase of the testbed, we explored several open-source FL frameworks to identify the most suitable one for our research. Our evaluation focused on the ability of each framework to handle diverse machine learning libraries, scale effectively, ensure data privacy, and function within resource-constrained environments like IoT devices. After extensive testing and analysis, four key frameworks emerged as potential candidates:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">TensorFlow Federated (TFF)</span>: Developed by Google, TFF integrates with TensorFlow and Keras, making it ideal for users already in the TensorFlow ecosystem. However, its limitation to TensorFlow reduces flexibility for those using other libraries like PyTorch or Scikit-learn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">PySyft</span>: PySyft excels in privacy-preserving deep learning with advanced cryptographic techniques, such as differential privacy. However, its complexity and focus on security make it less suitable for general-purpose FL in resource-constrained environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Federated AI Technology Enabler (FATE)</span>: Developed by WeBank, FATE targets large-scale, enterprise-level FL. Its resource intensity makes it less optimal for smaller, research-focused environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Flower</span>: Flower is highly flexible and scalable, supporting multiple libraries, including PyTorch, TensorFlow, and Scikit-learn. Its lightweight architecture makes it ideal for resource-constrained devices like IoT hardware, suiting both academic and real-world applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">After thorough consideration and testing, Flower was chosen as the optimal framework for our FL testbed due to several key advantages. Its flexibility allows seamless integration with multiple machine learning libraries, such as PyTorch and Scikit-learn, making it ideal for our experiments, which require testing various models across different ecosystems. Moreover, Flower’s lightweight design is well-suited for resource-constrained devices like Nvidia Jetson Nanos, ensuring efficient operation without overburdening hardware. Its scalability also facilitates easy expansion with more clients or complex models, without requiring significant reconfiguration. Additionally, Flower benefits from strong community support, frequent updates, and comprehensive documentation, making it reliable for long-term research. Considering the trade-offs, we concluded that Flower offers the best balance of flexibility, scalability, and ease of use, and it is utilized in all our experiments, particularly in assessing the robustness of FL against poisoning attacks.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Results</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present the results of our federated learning experiments, focusing on a cybersecurity use case involving intrusion detection within critical infrastructure systems. Specifically, our objective is to identify and mitigate malicious activities within Distributed Network Protocol 3 (DNP3) communications, which play a crucial role in industrial control systems (ICS), particularly in sectors like energy and water management. The ability to detect intrusions in DNP3 traffic is vital for preventing unauthorized access, system disruptions, and potential security breaches<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To address this challenge, we implemented a multi-layer perceptron (MLP) model with one hidden layer, trained on the DNP3 intrusion detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This dataset contains labeled network flow statistics and DNP3 flow characteristics related to various cyberattacks, including unauthorized commands and denial-of-service (DoS) attacks, all targeting the DNP3 protocol. The dataset was generated using CICFlowMeter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and offers a comprehensive representation of real-world attack vectors in critical infrastructure environments.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The original dataset consists of 86 features, with the “label” feature serving as the classification target. During preprocessing, non-numerical values were encoded into numerical features, and any <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">Inf</span> or <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">NaN</span> values were removed, resulting in a final dataset of 77 features and 7,326 rows. We performed a 70/30 train-test split, with the training data further partitioned among the client devices, simulating a FL environment with multiple clients contributing to the model’s training.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.09794/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="85" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The performance of federated learning testbed clients models with normal data</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The intrusion detection system we developed is designed to classify network traffic into 11 different categories, corresponding to specific types of DNP3 traffic and attacks. Our model processes 76 input features, feeding them through a hidden layer containing 50 neurons activated by the ReLU function. To mitigate overfitting, a dropout layer deactivates 20% of the neurons during training. The final output layer employs a Softmax activation function to classify the input into one of the 11 labels.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.09794/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The performance of each experiment with multiple clients (3,4,5) in evaluation accuracy, loss, and F1-Score</figcaption>
</figure>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Training for our experiments was carried out over 10 rounds, with each round consisting of 20 epochs. The Adam optimizer was employed due to its efficiency in handling sparse gradients and its adaptive learning rate properties, making it well-suited for FL environments with diverse client data distributions. To prevent overfitting, we implemented an early stopping mechanism, monitoring the evaluation loss with a patience of 10 epochs. This ensured that training would halt if there was no improvement in the loss, thereby maintaining model efficiency and avoiding unnecessary training cycles.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">In addition to standard, clean training, we introduced data poisoning as part of our experiments. Specifically, one of the clients (client 3) was designated as the poisoned node across experiments involving 3, 4, and 5 clients. The poisoning attack was executed by selectively altering the labels in client 3’s local dataset. To simulate a realistic attack scenario, 70% of the samples from 6 of the 11 available labels were randomly reassigned to incorrect labels within the dataset, excluding their original labels. This type of targeted label flipping is designed to subtly corrupt the local training data without raising immediate suspicion, simulating a scenario in which an adversary might manipulate a portion of the data on a compromised client node.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">The corrupted data was then used to train the local model on client 3, which introduced malicious gradients during the model update process. Because federated learning involves aggregating model updates from all clients, the poisoned updates from client 3 were integrated into the global model during the aggregation phase. This gradual introduction of poisoned gradients led to what is known as model poisoning, where the global model becomes compromised over time due to the integration of corrupted updates. As more rounds of training progressed, the cumulative effect of the poisoned data from client 3 distorted the global model’s decision boundary, ultimately reducing the model’s overall accuracy and compromising its ability to correctly classify data.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">The poisoning attack not only impacted client 3’s local model performance but also degraded the performance of the global model shared among all clients. This demonstrates the cascading effect of data poisoning in a federated learning system, where adversarial manipulation at the local level can propagate through the system, corrupting the global model. By experimenting with different configurations of clients (3, 4, and 5 clients), we were able to observe how the severity of the poisoning varied based on the number of benign clients and the scale of the poisoned data. These experiments provided valuable insights into the vulnerabilities of federated learning systems under adversarial conditions and how such attacks compromise the integrity and performance of the global model.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.09794/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The performance of the aggregated model of each experiment with various number of clients in evaluation accuracy, loss, and F1-Score</figcaption>
</figure>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p">The following sections provide a detailed analysis of the model’s performance in both clean and poisoned settings, highlighting the robustness of federated learning in the face of adversarial attacks and exploring the effectiveness of our defense strategies.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Clean Performance Results</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In Figure <a href="#S3.F2" title="Figure 2 ‣ III Experimental Results ‣ Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show the performance of a federated learning system using non-poisoned data across five clients, with evaluation accuracy, loss, and F1-score tracked over 20 training rounds. Evaluation accuracy steadily improves, converging around 0.70–0.73 for all clients, demonstrating consistent learning. The evaluation loss decreases smoothly from 1.2 to below 0.6, indicating effective convergence without disruptions. The F1-score follows a similar pattern, increasing to 0.65–0.70 across clients, reflecting balanced precision and recall. Overall, the system performs robustly under normal conditions, with minimal variation among clients and a successful convergence of the global model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Poisoned Model Performance Results</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ III Experimental Results ‣ Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the performance of each client across three experiments with different numbers of clients (3, 4, and 5) in federated learning settings. The evaluation metrics displayed include evaluation accuracy, evaluation loss, and F1-score over 20 rounds of training. Each experiment involves a unique configuration of clients, with client 3 being consistently poisoned across all trials. The figures provide valuable insights into how the introduction of poisoned data from client 3 affects the global model’s overall performance and how the system’s robustness changes as the number of clients increases.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In the case of 3 clients, we observe a significant drop in evaluation accuracy and F1-score for client 3 compared to clients 1 and 2. This degradation indicates that client 3’s poisoned data severely impacts its local model’s ability to perform accurately. Furthermore, the evaluation loss for client 3 remains higher throughout the rounds, signifying that the poisoned data is contributing to poor model convergence. In the 4-client and 5-client scenarios, the introduction of additional clients helps mitigate the effect of the poisoned data. As more clients are added, the global model appears to be more resilient to the poisoned contributions from client 3, as seen by the relatively stable evaluation accuracy and F1-scores for the non-poisoned clients. However, client 3’s performance continues to lag significantly, highlighting the local impact of the poisoning.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">As the number of clients increases to 4 and 5, the overall evaluation loss and performance trends suggest that federated learning becomes more robust to individual client poisoning. Clients 1, 2, 4, and 5 show improvements in both accuracy and F1-score, even with the presence of a poisoned client. This resilience can be attributed to the aggregation of updates from a larger number of benign clients, which helps to dilute the effect of client 3’s poisoned updates. The results suggest that while poisoned clients can have a severe impact on local model performance, the collective aggregation of updates in federated learning helps maintain global model accuracy, especially as the number of clients increases. However, it also emphasizes the importance of incorporating defense mechanisms to detect and mitigate poisoned contributions, particularly in smaller federated learning environments where the influence of a poisoned client is more pronounced.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In Figure <a href="#S3.F3" title="Figure 3 ‣ III Experimental Results ‣ Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that Client 3 consistently exhibits the worst performance across all three metrics -evaluation accuracy, loss, and F1-Score- in all three experiments conducted. This trend is expected, as approximately 55% of Client 3’s data was heavily poisoned. The graphs have highlighted the impact of poison data on client 3’s performance.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Comparison of Aggregated Models</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In Figure <a href="#S3.F4" title="Figure 4 ‣ III Experimental Results ‣ Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the performance of the federated learning system is compared under two conditions: a normal scenario (top row) and a poisoned scenario (bottom row). The results are presented across three metrics: evaluation accuracy, evaluation loss, and F1-score, for configurations of 3, 4, and 5 clients. The overall trends in the graphs show how the model performs during the 20 rounds of training under both normal and poisoned conditions.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In the normal scenario (top row), the evaluation accuracy shows a steady increase during the first 5 rounds, stabilizing around 0.7 for all client configurations (3, 4, and 5 clients). The evaluation loss also significantly decreases over time, with a sharp drop in the first 5 rounds before leveling out as the model converges. The F1-score follows a similar pattern, increasing rapidly in the first few rounds and stabilizing at around 0.7. The similar behavior of the different client configurations in terms of accuracy, loss, and F1-score indicates that in the absence of poisoning, the federated learning model performs consistently well, regardless of the number of clients involved.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">In contrast, the poisoned scenario (bottom row) exhibits a notable degradation in performance, especially in evaluation accuracy and F1-score. While the initial increase in accuracy during the first few rounds appears similar to the normal scenario, the final accuracy levels off at a slightly lower value compared to the normal condition. Evaluation loss also shows a similar decreasing trend, but it does not reach as low a value as in the normal condition, indicating that the model struggles to fully converge when poisoned data is introduced. Additionally, the F1-score, while following the same upward trajectory, stabilizes at a marginally lower value compared to the normal scenario. This suggests that while the model remains relatively robust, the presence of poisoned data still impacts its overall classification performance.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">These results demonstrate that while federated learning systems can maintain some level of resilience to poisoned data, the performance in terms of accuracy and F1-score is slightly reduced. The model does not fully converge as effectively as in the normal scenario, highlighting the need for further improvements in defense mechanisms to mitigate the impact of adversarial attacks like data poisoning.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">In Figure <a href="#S3.F4" title="Figure 4 ‣ III Experimental Results ‣ Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, despite the significant performance differences between the poisoned client and the other clients, the global model’s overall performance remains largely unaffected.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we developed and evaluated a federated learning testbed to assess the resilience of models in the face of adversarial data poisoning, specifically in a cybersecurity use case involving DNP3 intrusion detection. Through our experiments with both normal and poisoned data, we demonstrated that while federated learning systems show robust performance under normal conditions, poisoned data significantly impacts local model accuracy and convergence. However, as the number of clients increases, the global model remains more resilient due to the aggregation of updates from non-poisoned clients. The use of the Flower framework proved highly effective for its flexibility, scalability, and compatibility with resource-constrained devices like the Nvidia Jetson Nano. Our findings underscore the need for robust defense mechanisms to address federated learning’s susceptibility to adversarial attacks in critical applications like cybersecurity. For future work, it is possible to expand the testbed to evaluate defense strategies such as Byzantine Robust Aggregation and differential privacy, aiming to mitigate the impact of poisoning and enhance system robustness. Additionally, we plan to explore more complex attack strategies to further stress-test federated learning models and inform the development of secure and resilient FL systems for privacy-sensitive environments.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R. Ganjoo, M. Ganjoo, and M. Patil, “Mitigating Poisoning Attacks in Federated Learning,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Innovative Data Communication Technologies and Application</em>, J. S. Raj, K. Kamel, and P. Lafata, Eds.   Springer Nature, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Guduri, C. Chakraborty, U. Maheswari, and M. Margala, “Blockchain-Based Federated Learning Technique for Privacy Preservation and Security of Smart Electronic Health Records,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Consumer Electronics</em>, Feb. 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. R. Biden, “Executive order on the safe, secure, and trustworthy development and use of artificial intelligence,” 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, and Others, “Advances and Open Problems in Federated Learning,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>, Jun. 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Banabilah, M. Aloqaily, E. Alsayed, N. Malik, and Y. Jararweh, “Federated learning review: Fundamentals, enabling technologies, and future applications,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Information Processing &amp; Management</em>, Nov. 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. Zhang, Y. Xie, H. Bai, B. Yu, W. Li, and Y. Gao, “A survey on federated learning,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, Mar. 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang, “Federated Learning for Healthcare Informatics,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">JOURNAL OF HEALTHCARE INFORMATICS RESEARCH</em>, Mar. 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Nassar and M. Kamal, “Machine Learning and Big Data Analytics for Cybersecurity Threat Detection: A Holistic Review of Techniques and Case Studies,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of Artificial Intelligence and Machine Learning in Management</em>, Feb. 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Jin, H. Zhu, J. Xu, and Y. Chen, <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Federated Learning</em>.   Springer, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. E. Cinà, K. Grosse, A. Demontis, B. Biggio, F. Roli, and M. Pelillo, “Machine Learning Security Against Data Poisoning: Are We There Yet?” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Computer</em>, Mar. 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Hu, S. Gong, Q. Zhang, C. Seng, M. Xia, and S. Jiang, “An overview of implementing security and privacy in federated learning,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ARTIFICIAL INTELLIGENCE REVIEW</em>, Jul. 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
T. Chaalan, S. Pang, J. Kamruzzaman, I. Gondal, and X. Zhang, “The Path to Defence: A Roadmap to Characterising Data Poisoning Attacks on Victim Models,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, Jul. 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. N. Priya, A. S. M. Harish, S. S. Gowri, and P. D. Rathika, “Improving Security with Federated Learning,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2021 INTERNATIONAL CONFERENCE ON COMPUTATIONAL PERFORMANCE EVALUATION (COMPE-2021)</em>.   IEEE, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, A. Mathur, X. Qiu <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Flower: A friendly federated learning research framework,” <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.14390</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Sun, D. Li, and B. Wang, “Decentralized federated averaging,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Abadi, “Tensorflow: learning functions at scale,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st ACM SIGPLAN international conference on functional programming</em>, 2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Scikit-learn: Machine learning in python,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">the Journal of machine Learning research</em>, 2011.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. E. Oliphant <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">Guide to numpy</em>.   Trelgol Publishing USA, 2006.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
W. McKinney <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “pandas: a foundational python library for data analysis and statistics,” <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">Python for high performance and scientific computing</em>, 2011.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Tosi, <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Matplotlib for Python developers</em>.   Packt Publishing Ltd, 2009.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Kugan, M. Q. U. Islam, and R. Kashef, “Decentralized federated deep learning image recognition models,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2023 4th International Conference on Artificial Intelligence, Robotics and Control (AIRC)</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P.-S. Lin, M.-C. Kao, W.-Y. Liang, and S.-H. Hung, “Performance analysis and optimization for federated learning applications with pysyft-based secure aggregation,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2020 International Computer Symposium (ICS)</em>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. Aledhari, R. Razzak, R. M. Parizi, and F. Saeed, “Federated learning: A survey on enabling technologies, protocols, and applications,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
I. Ji, S. Jeon, and J. T. Seo, “Ae-lstm based anomaly detection system for communication over dnp 3.0,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Conference on Information Security Applications</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P. Radoglou-Grammatikis, V. Kelli, T. Lagkas, V. Argyriou, and P. Sarigiannidis, “Dnp3 intrusion detection dataset,” 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. H. Lashkari, G. D. Gil, M. S. I. Mamun, and A. A. Ghorbani, “Characterization of tor traffic using time based features,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Information Systems Security and Privacy</em>, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09793" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09794" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09794">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09794" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09795" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:38:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
