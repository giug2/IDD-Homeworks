<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.09138] MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering</title><meta property="og:description" content="Knowledge-based visual question answering requires the ability of associating external knowledge for open-ended cross-modal scene understanding. One limitation of existing solutions is that they capture relevant knowleâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.09138">

<!--Generated on Mon Mar 11 08:04:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id7.7.7" class="ltx_text ltx_font_bold">Yang Ding<sup id="id7.7.7.1" class="ltx_sup"><span id="id7.7.7.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>, Jing Yu<sup id="id7.7.7.2" class="ltx_sup"><span id="id7.7.7.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2âˆ—</span></sup>, Bang Liu<sup id="id7.7.7.3" class="ltx_sup"><span id="id7.7.7.3.1" class="ltx_text ltx_font_medium ltx_font_italic">3,4â€ </span></sup>, Yue Hu<sup id="id7.7.7.4" class="ltx_sup"><span id="id7.7.7.4.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>, Mingxin Cui<sup id="id7.7.7.5" class="ltx_sup"><span id="id7.7.7.5.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>, Qi Wu<sup id="id7.7.7.6" class="ltx_sup"><span id="id7.7.7.6.1" class="ltx_text ltx_font_medium ltx_font_italic">5</span></sup>
<br class="ltx_break"><sup id="id7.7.7.7" class="ltx_sup"><span id="id7.7.7.7.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
<br class="ltx_break"><sup id="id14.12.id1" class="ltx_sup"><span id="id14.12.id1.1" class="ltx_text ltx_font_italic">2</span></sup>School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
<br class="ltx_break"><sup id="id15.13.id2" class="ltx_sup"><span id="id15.13.id2.1" class="ltx_text ltx_font_italic">3</span></sup>UniversitÃ© de MontrÃ©al, Canadaâ€ƒâ€ƒ<sup id="id16.14.id3" class="ltx_sup"><span id="id16.14.id3.1" class="ltx_text ltx_font_italic">4</span></sup>Mila - Quebec AI Institute, Canada
<br class="ltx_break"><sup id="id17.15.id4" class="ltx_sup"><span id="id17.15.id4.1" class="ltx_text ltx_font_italic">5</span></sup>University of Adelaide, Australia
<br class="ltx_break"><span id="id18.16.id5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{dingyang, yujing02, huyue, cuimingxin}@iie.ac.cn, bang.liu@umontreal.ca
<br class="ltx_break">qi.wu01@adelaide.edu.au</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.2" class="ltx_p">Knowledge-based visual question answering requires the ability of associating external knowledge for open-ended cross-modal scene understanding. One limitation of existing solutions is that they capture relevant knowledge from text-only knowledge bases, which merely contain facts expressed by first-order predicates or language descriptions while lacking complex but indispensable multimodal knowledge for visual understanding. How to construct vision-relevant and explainable multimodal knowledge for the VQA scenario has been less studied. In this paper, we propose MuKEA to represent multimodal knowledge by an explicit triplet to correlate visual objects and fact answers with implicit relations. To bridge the heterogeneous gap, we propose three objective losses to learn the triplet representations from complementary views: embedding structure, topological relation and semantic space. By adopting a pre-training and fine-tuning learning strategy, both basic and domain-specific multimodal knowledge are progressively accumulated for answer prediction. We outperform the state-of-the-art by 3.35<math id="id12.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="id12.1.m1.1a"><mo id="id12.1.m1.1.1" xref="id12.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="id12.1.m1.1b"><csymbol cd="latexml" id="id12.1.m1.1.1.cmml" xref="id12.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id12.1.m1.1c">\%</annotation></semantics></math> and 6.08<math id="id13.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="id13.2.m2.1a"><mo id="id13.2.m2.1.1" xref="id13.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="id13.2.m2.1b"><csymbol cd="latexml" id="id13.2.m2.1.1.cmml" xref="id13.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id13.2.m2.1c">\%</annotation></semantics></math> respectively on two challenging knowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the complementary benefits of the multimodal knowledge with existing knowledge bases and the advantages of our end-to-end framework over the existing pipeline methods. The code is available at <a target="_blank" href="https://github.com/AndersonStra/MuKEA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AndersonStra/MuKEA</a>.</p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">We provide additional materials to supplement our main submissions. In Section <a href="#A1" title="Appendix A A Multimodal Knowledge Construction and Related Applications â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, we introduce explicit multimodal knowledge construction, knowledge graph characteristics, application scenarios in detail, and provide extracted multimodal knowledge embeddings as off-the-shelf knowledge features to serve knowledge-based downstream tasks. Based on the knowledge graph constructed above, in Section <a href="#A2" title="Appendix B B Analysis of Progress Knowledge Accumulation â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> and <a href="#A3" title="Appendix C C Zero-shot Analysis of Accumulated Multimodal Knowledge â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, we respectively introduce how MuKEA performs multimodal knowledge accumulation and complex reasoning. Then we compare the model size of MuKEA with pre-trained models and analyse the influence of multimodal knowledge base size on inference time respectively in Section <a href="#A4" title="Appendix D D Model Size Analysis â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> and <a href="#A5" title="Appendix E E Efficiency Analysis â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, which proves that the inference time is not affected much when varying the knowledge size. In Section <a href="#A6" title="Appendix F F Effect of Ensemble Hyper-parameters â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>, we study the effect of hyper-parameters in model ensemble corresponding to the knowledge complementary experiments. At last, in Section <a href="#A8" title="Appendix H H Implementation Details â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a>, we introduce the implementation details about training.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Corresponding author.</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>Canada CIFAR AI Chair.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.09138/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">An illustration of our motivation. Compared with rigid facts in the knowledge graph, multimodal knowledge for depicting complex and inexpressible facts is indispensable in both open-ended object understanding (a) and scene understanding (b). </span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering based on external Knowledge Bases (KB-VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> requires an AI agent to answer a question by incorporating knowledge about the world beyond what the question and the image contains. Despite the great success in VQA tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, KB-VQA is more challenging for models to achieve human-like ability of open-ended cross-modal scene understanding associating with external knowledge. Therefore, how to appropriately represent and leverage knowledge in such cross-modal scenario becomes a core problem of KB-VQA.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most of recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> focus on capturing relevant knowledge from structured knowledge graphs, such as ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and DBpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, or unstructured/semi-structured knowledge, like Wikipedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Though these knowledge bases provide high-quality knowledge by large-scale human annotations, the information is generally limited to the definite facts that can be explicitly expressed by natural language or simple triplets with first-order predicate. Therefore, such knowledge bases are quite difficult to represent high-order predicate and multimodal knowledge, which is essential for human to tackle complex problems. Considering the question in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), the agent needs visual knowledge of motorcycle appearance in each
brand to identify the given motorcycle, but the knowledge graph lacks of such instantiated information. Besides object understanding, implicit visual knowledge in mind mostly dominate over the rigid facts when humans are asked for simple scene discrimination like the question â€˜Can you guess the place?â€™ in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b). <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">How to represent and accumulate the complex multimodal knowledge in the VQA scenario while maintaining the advantages of traditional knowledge graph in explainable reasoning is an essential but less studied problem.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Current progress <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> in emerging multimodal knowledge graph aims to correlate visual content with textual facts to form the augmented knowledge graph. The typical solutions can be divided into two categories: parsing images and texts to structured representations and grounding event/entities across modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, or simply aligning the entities in existing knowledge graphs with related images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. However, such multimodal knowledge graphs in essence still represent knowledge via the first-order predicate, which fails to model the high-order complex relationships such as the relationship between â€˜clockâ€™ and â€˜Londonâ€™ in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose a novel <span id="S1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Mu<span id="S1.p4.1.1.1" class="ltx_text ltx_font_medium">ltimodal </span>K<span id="S1.p4.1.1.2" class="ltx_text ltx_font_medium">nowledge </span>E<span id="S1.p4.1.1.3" class="ltx_text ltx_font_medium">xtraction and </span>A<span id="S1.p4.1.1.4" class="ltx_text ltx_font_medium">ccumulation</span></span> framework (MuKEA) for KB-VQA task. Independent of existing knowledge bases, the core mechanism behind MuKEA is to accumulate multimodal knowledge with complex relationships from observation of VQA samples, and perform explainable reasoning based on the self-accumulated knowledge. To this end, we first propose a novel schema to represent multimodal knowledge unit by an explicit triplet, where the visual objects referred by the question are embedded in the head entity,
the embedding of the fact answer is kept in the tail entity, and the implicit relation between the head and the tail is expressed by the relation.
We propose three objective loss functions to learn the representations of the triplets from coarse to fine by contrasting positive and negative triplets, aligning ground-truth triplets, and refining entity representations. A pre-training and fine-tuning learning strategy is then proposed to progressively accumulate multimodal knowledge from both out-domain and in-domain VQA samples for explainable reasoning.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contributions of this work are as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">(1) We propose an end-to-end multimodal knowledge representation learning framework, which first models the inexpressible multimodal facts by explicit triplets and provides complementary knowledge with the existing knowledge graphs and unstructured knowledge bases.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">(2) We exploit a pre-training and fine-tuning strategy to accumulate both out-domain and in-domain knowledge to form a neural multimodal knowledge base. It supports automatic knowledge association and answer prediction, which gets rid of the cascading error in existing â€˜knowledge retrieve and readâ€™ pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.2" class="ltx_p">(3) Our model with strong generalization ability outperforms the state-of-the-art models by 3.35<math id="S1.p8.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><csymbol cd="latexml" id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\%</annotation></semantics></math> and 6.08<math id="S1.p8.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><csymbol cd="latexml" id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\%</annotation></semantics></math> respectively on two challenging KB-VQA datasets: OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and KRVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The good performance can be well explained by visualizing the relevant multimodal knowledge triplets explicitly.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Knowledge-based Visual Question Answering.</span>â€ƒMost of recent works are based on â€˜knowledge retrieve and readâ€™ pipeline which requires highly-relevant knowledge to support knowledge reasoning. Structured knowledge based methods like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is based on ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to introduce knowledge in the form of triplet with first-order predicate. Unstructured knowledge based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> retrieve knowledge from Wikipedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and encode relevant text in a memory network for further reasoning. However, knowledge described in nature language lacks visual information to
assist cross-modal understanding. For the above challenge, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> augments the knowledge graph YAGO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> with related images to serve as multimodal knowledge. However, such graph in essence still represents knowledge via the first-order predicate. To go one step further, we extract multimodal information to represent high-order complex relations and represent multimodal knowledge by explicit triplets for explainable reasoning.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">From the view of model framework, most of recent works are based on the â€˜retrieve and readâ€™ pipeline, which first retrieve the relevant facts from knowledge bases and then perform explicit reasoning on the knowledge graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, or fusing the implicit knowledge embedding with image and questions for answer classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. All of these methods rely on object labels to retrieve external knowledge, which inevitably introduces irrelevant knowledge and leads to cascading error. There are also end-to-end methods based on implicit knowledge like pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. However, such implicit knowledge mainly captures the co-occurrence of image-question-answer triplet instead of explainable and refined knowledge. In this paper, we propose an end-to-end multimodal knowledge extraction and accumulation framework with interpretable triplet knowledge.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Multimodal Knowledge Graph.</span>â€ƒThe emerging multimodal knowledge graph works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> aim to correlate visual content with textual facts to form the augmented knowledge graph. One typical solution parses images and texts to structured representations first and grounds event/entities across modalities. The key problem lies in intra-modal relation extraction and cross-modal entity linking. Specifically, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> learn knowledge from structured textual and visual data and maintain the triplet structure for entity alignment. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> utilizes RDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> knowledge graphs to represent multimodal information based on
graph alignment and lacks multimodal correlation as well. Another kind of solutions directly links the entities in existing knowledge graphs with relevant images. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> adds images to expand the entity representation in YAGO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. However, all of these methods in essence still represent knowledge via the first-order predicate described by nature language, which fails to model the high-order complex relationships.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2203.09138/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.4.2" class="ltx_text" style="font-size:90%;">An overview of our model. The model contains two modules: Multimodal Knowledge Triplet Extraction aims to extract multimodal knowledge triplets from samples and Knowledge Triplet Representation Learning aims to unifiedly learn the triplet representation.</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">Given an image <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">I</annotation></semantics></math> and a question <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">Q</annotation></semantics></math>, the KB-VQA task aims to predict an answer <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">A</annotation></semantics></math> supported by additional knowledge beyond the given visual and textual content. We accumulate triplet-formed multimodal knowledge to serve as the external knowledge and directly infer the answer in an end-to-end mode. Figure <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives detailed illustration of our model. We first introduce a novel schema of extracting multimodal knowledge triplets from unstructured image-question-answer samples based on the pre-trained vision-language model. Then we propose three objective losses to learn the triplet embeddings that accurately depict question-attended visual content (head embeddings), question-desired fact answer (tail embeddings), and the implicit relation between the two (relation embeddings). By training with both out-domain and in-domain data, our model accumulates a wide range of multimodal knowledge and associates the optimal fact for answer prediction.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multimodal Knowledge Triplet Extraction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">In the VQA scenario, we define the complex and inexpressible facts as multimodal knowledge in the form of triplet, <span id="S3.SS1.p1.6.1" class="ltx_text ltx_font_italic">i.e.</span> <math id="S3.SS1.p1.1.m1.3" class="ltx_Math" alttext="(h,r,t)" display="inline"><semantics id="S3.SS1.p1.1.m1.3a"><mrow id="S3.SS1.p1.1.m1.3.4.2" xref="S3.SS1.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.3.4.2.1" xref="S3.SS1.p1.1.m1.3.4.1.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">h</mi><mo id="S3.SS1.p1.1.m1.3.4.2.2" xref="S3.SS1.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">r</mi><mo id="S3.SS1.p1.1.m1.3.4.2.3" xref="S3.SS1.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml">t</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.3.4.2.4" xref="S3.SS1.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.3b"><vector id="S3.SS1.p1.1.m1.3.4.1.cmml" xref="S3.SS1.p1.1.m1.3.4.2"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">â„</ci><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">ğ‘Ÿ</ci><ci id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">ğ‘¡</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.3c">(h,r,t)</annotation></semantics></math>, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">h</annotation></semantics></math> contains visual content in the image focused by the question, <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">t</annotation></semantics></math> is a representation of the answer given the question-image pair, and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">r</annotation></semantics></math> depicts the implicit relationship between <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">h</annotation></semantics></math> and <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">t</annotation></semantics></math> containing multimodal information. The triplet construction process mainly consists of the following four parts:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.15" class="ltx_p"><span id="S3.SS1.p2.15.1" class="ltx_text ltx_font_bold">Image and Question Encoding.</span>â€ƒSince the pre-trained vision-language models are strong at modeling the intra-modal and cross-modal implicit correlations, we first utilize the pre-trained model LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> to encode the question and image for further multimodal knowledge triplet extraction. We apply Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to detect a set of objects
<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{O}=\{o_{i}\}_{i=1}^{K}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">ğ’ª</mi><mo id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml"><mrow id="S3.SS1.p2.1.m1.1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.2.cmml">o</mi><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p2.1.m1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.1.m1.1.1.1.1.3.1" xref="S3.SS1.p2.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p2.1.m1.1.1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.1.m1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.3.cmml">K</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><eq id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"></eq><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ğ’ª</ci><apply id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1">superscript</csymbol><apply id="S3.SS1.p2.1.m1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1">subscript</csymbol><set id="S3.SS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1"><apply id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.2">ğ‘œ</ci><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.3"><eq id="S3.SS1.p2.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.3.1"></eq><ci id="S3.SS1.p2.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p2.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.1.m1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.3">ğ¾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathcal{O}=\{o_{i}\}_{i=1}^{K}</annotation></semantics></math> (<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">K</annotation></semantics></math> = 36) in <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">I</annotation></semantics></math> and represent
each object <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">o</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ğ‘œ</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">o_{i}</annotation></semantics></math> by a visual feature vector <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\bm{f}_{i}\in\mathbb{R}^{d_{f}}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><msub id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2.2" xref="S3.SS1.p2.5.m5.1.1.2.2.cmml">ğ’‡</mi><mi id="S3.SS1.p2.5.m5.1.1.2.3" xref="S3.SS1.p2.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.2" xref="S3.SS1.p2.5.m5.1.1.3.2.cmml">â„</mi><msub id="S3.SS1.p2.5.m5.1.1.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.3.2" xref="S3.SS1.p2.5.m5.1.1.3.3.2.cmml">d</mi><mi id="S3.SS1.p2.5.m5.1.1.3.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.3.cmml">f</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><in id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1"></in><apply id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.2.1.cmml" xref="S3.SS1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2.2">ğ’‡</ci><ci id="S3.SS1.p2.5.m5.1.1.2.3.cmml" xref="S3.SS1.p2.5.m5.1.1.2.3">ğ‘–</ci></apply><apply id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.2">â„</ci><apply id="S3.SS1.p2.5.m5.1.1.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS1.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3">ğ‘“</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\bm{f}_{i}\in\mathbb{R}^{d_{f}}</annotation></semantics></math> (<math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="d_{f}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">d</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğ‘‘</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">d_{f}</annotation></semantics></math> = 2048) and a spatial feature vector <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\bm{b}_{i}\in\mathbb{R}^{d_{b}}(d_{b}=4)" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><msub id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">ğ’ƒ</mi><mi id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">âˆˆ</mo><mrow id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml"><msup id="S3.SS1.p2.7.m7.1.1.1.3" xref="S3.SS1.p2.7.m7.1.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.1.3.2.cmml">â„</mi><msub id="S3.SS1.p2.7.m7.1.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.1.3.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.1.3.3.2" xref="S3.SS1.p2.7.m7.1.1.1.3.3.2.cmml">d</mi><mi id="S3.SS1.p2.7.m7.1.1.1.3.3.3" xref="S3.SS1.p2.7.m7.1.1.1.3.3.3.cmml">b</mi></msub></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m7.1.1.1.2" xref="S3.SS1.p2.7.m7.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p2.7.m7.1.1.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.7.m7.1.1.1.1.1.2" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.7.m7.1.1.1.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.2" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.2.cmml">d</mi><mi id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.3" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.3.cmml">b</mi></msub><mo id="S3.SS1.p2.7.m7.1.1.1.1.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.3.cmml">4</mn></mrow><mo stretchy="false" id="S3.SS1.p2.7.m7.1.1.1.1.1.3" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><in id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2"></in><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2">ğ’ƒ</ci><ci id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">ğ‘–</ci></apply><apply id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"><times id="S3.SS1.p2.7.m7.1.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.1.2"></times><apply id="S3.SS1.p2.7.m7.1.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3.2">â„</ci><apply id="S3.SS1.p2.7.m7.1.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.3.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.1.3.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS1.p2.7.m7.1.1.1.3.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.1.3.3.3">ğ‘</ci></apply></apply><apply id="S3.SS1.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1"><eq id="S3.SS1.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.1"></eq><apply id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.2">ğ‘‘</ci><ci id="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.2.3">ğ‘</ci></apply><cn type="integer" id="S3.SS1.p2.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.1.1.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\bm{b}_{i}\in\mathbb{R}^{d_{b}}(d_{b}=4)</annotation></semantics></math> as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. We tokenize a question <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">Q</annotation></semantics></math> using WordPiece <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and obtain a sequence of <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">D</annotation></semantics></math> tokens.
We feed the visual features <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="\{\bm{f}_{i}\}_{i=1}^{K}" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><msubsup id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mrow id="S3.SS1.p2.10.m10.1.1.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.10.m10.1.1.1.1.1.2" xref="S3.SS1.p2.10.m10.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p2.10.m10.1.1.1.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.1.1.1.1.2" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.2.cmml">ğ’‡</mi><mi id="S3.SS1.p2.10.m10.1.1.1.1.1.1.3" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.10.m10.1.1.1.1.1.3" xref="S3.SS1.p2.10.m10.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p2.10.m10.1.1.1.3" xref="S3.SS1.p2.10.m10.1.1.1.3.cmml"><mi id="S3.SS1.p2.10.m10.1.1.1.3.2" xref="S3.SS1.p2.10.m10.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.10.m10.1.1.1.3.1" xref="S3.SS1.p2.10.m10.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p2.10.m10.1.1.1.3.3" xref="S3.SS1.p2.10.m10.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1">superscript</csymbol><apply id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1">subscript</csymbol><set id="S3.SS1.p2.10.m10.1.1.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1"><apply id="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.2">ğ’‡</ci><ci id="S3.SS1.p2.10.m10.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p2.10.m10.1.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.1.3"><eq id="S3.SS1.p2.10.m10.1.1.1.3.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1.3.1"></eq><ci id="S3.SS1.p2.10.m10.1.1.1.3.2.cmml" xref="S3.SS1.p2.10.m10.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p2.10.m10.1.1.1.3.3.cmml" xref="S3.SS1.p2.10.m10.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">\{\bm{f}_{i}\}_{i=1}^{K}</annotation></semantics></math> and <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="\{\bm{b}_{i}\}_{i=1}^{K}" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><msubsup id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><mrow id="S3.SS1.p2.11.m11.1.1.1.1.1" xref="S3.SS1.p2.11.m11.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.11.m11.1.1.1.1.1.2" xref="S3.SS1.p2.11.m11.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p2.11.m11.1.1.1.1.1.1" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.11.m11.1.1.1.1.1.1.2" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1.2.cmml">ğ’ƒ</mi><mi id="S3.SS1.p2.11.m11.1.1.1.1.1.1.3" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.11.m11.1.1.1.1.1.3" xref="S3.SS1.p2.11.m11.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p2.11.m11.1.1.1.3" xref="S3.SS1.p2.11.m11.1.1.1.3.cmml"><mi id="S3.SS1.p2.11.m11.1.1.1.3.2" xref="S3.SS1.p2.11.m11.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.11.m11.1.1.1.3.1" xref="S3.SS1.p2.11.m11.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p2.11.m11.1.1.1.3.3" xref="S3.SS1.p2.11.m11.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.11.m11.1.1.3" xref="S3.SS1.p2.11.m11.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1">superscript</csymbol><apply id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1">subscript</csymbol><set id="S3.SS1.p2.11.m11.1.1.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.1.1.1"><apply id="S3.SS1.p2.11.m11.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.11.m11.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1.2">ğ’ƒ</ci><ci id="S3.SS1.p2.11.m11.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p2.11.m11.1.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.1.3"><eq id="S3.SS1.p2.11.m11.1.1.1.3.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1.3.1"></eq><ci id="S3.SS1.p2.11.m11.1.1.1.3.2.cmml" xref="S3.SS1.p2.11.m11.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p2.11.m11.1.1.1.3.3.cmml" xref="S3.SS1.p2.11.m11.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.11.m11.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">\{\bm{b}_{i}\}_{i=1}^{K}</annotation></semantics></math>, and question tokens into the pre-trained LXMERT, obtaining the visual embeddings of <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="\mathcal{O}" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">ğ’ª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><ci id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">ğ’ª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">\mathcal{O}</annotation></semantics></math> denoted as <math id="S3.SS1.p2.13.m13.1" class="ltx_Math" alttext="\bm{V}\in\mathbb{R}^{K\times d_{v}}" display="inline"><semantics id="S3.SS1.p2.13.m13.1a"><mrow id="S3.SS1.p2.13.m13.1.1" xref="S3.SS1.p2.13.m13.1.1.cmml"><mi id="S3.SS1.p2.13.m13.1.1.2" xref="S3.SS1.p2.13.m13.1.1.2.cmml">ğ‘½</mi><mo id="S3.SS1.p2.13.m13.1.1.1" xref="S3.SS1.p2.13.m13.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.13.m13.1.1.3" xref="S3.SS1.p2.13.m13.1.1.3.cmml"><mi id="S3.SS1.p2.13.m13.1.1.3.2" xref="S3.SS1.p2.13.m13.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.p2.13.m13.1.1.3.3" xref="S3.SS1.p2.13.m13.1.1.3.3.cmml"><mi id="S3.SS1.p2.13.m13.1.1.3.3.2" xref="S3.SS1.p2.13.m13.1.1.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.13.m13.1.1.3.3.1" xref="S3.SS1.p2.13.m13.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.p2.13.m13.1.1.3.3.3" xref="S3.SS1.p2.13.m13.1.1.3.3.3.cmml"><mi id="S3.SS1.p2.13.m13.1.1.3.3.3.2" xref="S3.SS1.p2.13.m13.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS1.p2.13.m13.1.1.3.3.3.3" xref="S3.SS1.p2.13.m13.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.1b"><apply id="S3.SS1.p2.13.m13.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1"><in id="S3.SS1.p2.13.m13.1.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1.1"></in><ci id="S3.SS1.p2.13.m13.1.1.2.cmml" xref="S3.SS1.p2.13.m13.1.1.2">ğ‘½</ci><apply id="S3.SS1.p2.13.m13.1.1.3.cmml" xref="S3.SS1.p2.13.m13.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m13.1.1.3.1.cmml" xref="S3.SS1.p2.13.m13.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.13.m13.1.1.3.2.cmml" xref="S3.SS1.p2.13.m13.1.1.3.2">â„</ci><apply id="S3.SS1.p2.13.m13.1.1.3.3.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3"><times id="S3.SS1.p2.13.m13.1.1.3.3.1.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3.1"></times><ci id="S3.SS1.p2.13.m13.1.1.3.3.2.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3.2">ğ¾</ci><apply id="S3.SS1.p2.13.m13.1.1.3.3.3.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m13.1.1.3.3.3.1.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.13.m13.1.1.3.3.3.2.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3.3.2">ğ‘‘</ci><ci id="S3.SS1.p2.13.m13.1.1.3.3.3.3.cmml" xref="S3.SS1.p2.13.m13.1.1.3.3.3.3">ğ‘£</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.1c">\bm{V}\in\mathbb{R}^{K\times d_{v}}</annotation></semantics></math> (<math id="S3.SS1.p2.14.m14.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S3.SS1.p2.14.m14.1a"><msub id="S3.SS1.p2.14.m14.1.1" xref="S3.SS1.p2.14.m14.1.1.cmml"><mi id="S3.SS1.p2.14.m14.1.1.2" xref="S3.SS1.p2.14.m14.1.1.2.cmml">d</mi><mi id="S3.SS1.p2.14.m14.1.1.3" xref="S3.SS1.p2.14.m14.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.1b"><apply id="S3.SS1.p2.14.m14.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m14.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p2.14.m14.1.1.2.cmml" xref="S3.SS1.p2.14.m14.1.1.2">ğ‘‘</ci><ci id="S3.SS1.p2.14.m14.1.1.3.cmml" xref="S3.SS1.p2.14.m14.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.1c">d_{v}</annotation></semantics></math> = 768) and the token embeddings denoted as <math id="S3.SS1.p2.15.m15.1" class="ltx_Math" alttext="\bm{Q}\in\mathbb{R}^{D\times d_{v}}" display="inline"><semantics id="S3.SS1.p2.15.m15.1a"><mrow id="S3.SS1.p2.15.m15.1.1" xref="S3.SS1.p2.15.m15.1.1.cmml"><mi id="S3.SS1.p2.15.m15.1.1.2" xref="S3.SS1.p2.15.m15.1.1.2.cmml">ğ‘¸</mi><mo id="S3.SS1.p2.15.m15.1.1.1" xref="S3.SS1.p2.15.m15.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.15.m15.1.1.3" xref="S3.SS1.p2.15.m15.1.1.3.cmml"><mi id="S3.SS1.p2.15.m15.1.1.3.2" xref="S3.SS1.p2.15.m15.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.p2.15.m15.1.1.3.3" xref="S3.SS1.p2.15.m15.1.1.3.3.cmml"><mi id="S3.SS1.p2.15.m15.1.1.3.3.2" xref="S3.SS1.p2.15.m15.1.1.3.3.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.15.m15.1.1.3.3.1" xref="S3.SS1.p2.15.m15.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.p2.15.m15.1.1.3.3.3" xref="S3.SS1.p2.15.m15.1.1.3.3.3.cmml"><mi id="S3.SS1.p2.15.m15.1.1.3.3.3.2" xref="S3.SS1.p2.15.m15.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS1.p2.15.m15.1.1.3.3.3.3" xref="S3.SS1.p2.15.m15.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m15.1b"><apply id="S3.SS1.p2.15.m15.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1"><in id="S3.SS1.p2.15.m15.1.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1.1"></in><ci id="S3.SS1.p2.15.m15.1.1.2.cmml" xref="S3.SS1.p2.15.m15.1.1.2">ğ‘¸</ci><apply id="S3.SS1.p2.15.m15.1.1.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m15.1.1.3.1.cmml" xref="S3.SS1.p2.15.m15.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.15.m15.1.1.3.2.cmml" xref="S3.SS1.p2.15.m15.1.1.3.2">â„</ci><apply id="S3.SS1.p2.15.m15.1.1.3.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3"><times id="S3.SS1.p2.15.m15.1.1.3.3.1.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3.1"></times><ci id="S3.SS1.p2.15.m15.1.1.3.3.2.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3.2">ğ·</ci><apply id="S3.SS1.p2.15.m15.1.1.3.3.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m15.1.1.3.3.3.1.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.15.m15.1.1.3.3.3.2.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3.3.2">ğ‘‘</ci><ci id="S3.SS1.p2.15.m15.1.1.3.3.3.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3.3.3.3">ğ‘£</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m15.1c">\bm{Q}\in\mathbb{R}^{D\times d_{v}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Head Entity Extraction.</span>â€ƒWe define the head entity as the visual object and its context in the image that is most relevant to the question. To this end, we firstly evaluate the relevance of each object in the image to each token in the question by computing the question-guided object-question relevance affinity matrix <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bm{A}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ğ‘¨</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ‘¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\bm{A}</annotation></semantics></math> as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="{\bm{A}}=(\textbf{W}_{1}\bm{Q})^{T}(\textbf{W}_{2}\bm{V})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml">ğ‘¨</mi><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msup id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2a.cmml">W</mtext><mn id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">ğ‘¸</mi></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.2.2.2.2.1.1.2.2" xref="S3.E1.m1.2.2.2.2.1.1.2.2a.cmml">W</mtext><mn id="S3.E1.m1.2.2.2.2.1.1.2.3" xref="S3.E1.m1.2.2.2.2.1.1.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">ğ‘½</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></eq><ci id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4">ğ‘¨</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">W</mtext></ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">ğ‘¸</ci></apply><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ğ‘‡</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"></times><apply id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.2a.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2">W</mtext></ci><cn type="integer" id="S3.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.3">2</cn></apply><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">ğ‘½</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">{\bm{A}}=(\textbf{W}_{1}\bm{Q})^{T}(\textbf{W}_{2}\bm{V})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.3" class="ltx_p">where <math id="S3.SS1.p3.2.m1.1" class="ltx_Math" alttext="\textbf{W}_{1}" display="inline"><semantics id="S3.SS1.p3.2.m1.1a"><msub id="S3.SS1.p3.2.m1.1.1" xref="S3.SS1.p3.2.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p3.2.m1.1.1.2" xref="S3.SS1.p3.2.m1.1.1.2a.cmml">W</mtext><mn id="S3.SS1.p3.2.m1.1.1.3" xref="S3.SS1.p3.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m1.1b"><apply id="S3.SS1.p3.2.m1.1.1.cmml" xref="S3.SS1.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m1.1.1.1.cmml" xref="S3.SS1.p3.2.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m1.1.1.2a.cmml" xref="S3.SS1.p3.2.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p3.2.m1.1.1.2.cmml" xref="S3.SS1.p3.2.m1.1.1.2">W</mtext></ci><cn type="integer" id="S3.SS1.p3.2.m1.1.1.3.cmml" xref="S3.SS1.p3.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m1.1c">\textbf{W}_{1}</annotation></semantics></math> and <math id="S3.SS1.p3.3.m2.1" class="ltx_Math" alttext="\textbf{W}_{2}" display="inline"><semantics id="S3.SS1.p3.3.m2.1a"><msub id="S3.SS1.p3.3.m2.1.1" xref="S3.SS1.p3.3.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p3.3.m2.1.1.2" xref="S3.SS1.p3.3.m2.1.1.2a.cmml">W</mtext><mn id="S3.SS1.p3.3.m2.1.1.3" xref="S3.SS1.p3.3.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m2.1b"><apply id="S3.SS1.p3.3.m2.1.1.cmml" xref="S3.SS1.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m2.1.1.1.cmml" xref="S3.SS1.p3.3.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m2.1.1.2a.cmml" xref="S3.SS1.p3.3.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p3.3.m2.1.1.2.cmml" xref="S3.SS1.p3.3.m2.1.1.2">W</mtext></ci><cn type="integer" id="S3.SS1.p3.3.m2.1.1.3.cmml" xref="S3.SS1.p3.3.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m2.1c">\textbf{W}_{2}</annotation></semantics></math> are learned parameters.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.3" class="ltx_p">Under the guidance of the relevance affinity matrix, we then select one object in <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{O}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">ğ’ª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ğ’ª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathcal{O}</annotation></semantics></math> as the most relevant visual content to the question. Since the LXMERT models the implicit correlations among all the objects, it is noteworthy that the selected question-centric object already contains its context information, which provides indispensable clues for answering questions that involve multiple objects. Specifically we compute the row-wise max-pooling on <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\bm{A}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">ğ‘¨</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ğ‘¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\bm{A}</annotation></semantics></math> to evaluate the relevance of each object <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><msub id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">o</mi><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">ğ‘œ</ci><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">o_{i}</annotation></semantics></math> to the question as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="{\bm{a}}^{v-q}_{i}=\mathop{\max}_{j}{\bm{A}}_{i,j}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.3" xref="S3.E2.m1.2.3.cmml"><msubsup id="S3.E2.m1.2.3.2" xref="S3.E2.m1.2.3.2.cmml"><mi id="S3.E2.m1.2.3.2.2.2" xref="S3.E2.m1.2.3.2.2.2.cmml">ğ’‚</mi><mi id="S3.E2.m1.2.3.2.3" xref="S3.E2.m1.2.3.2.3.cmml">i</mi><mrow id="S3.E2.m1.2.3.2.2.3" xref="S3.E2.m1.2.3.2.2.3.cmml"><mi id="S3.E2.m1.2.3.2.2.3.2" xref="S3.E2.m1.2.3.2.2.3.2.cmml">v</mi><mo id="S3.E2.m1.2.3.2.2.3.1" xref="S3.E2.m1.2.3.2.2.3.1.cmml">âˆ’</mo><mi id="S3.E2.m1.2.3.2.2.3.3" xref="S3.E2.m1.2.3.2.2.3.3.cmml">q</mi></mrow></msubsup><mo rspace="0.1389em" id="S3.E2.m1.2.3.1" xref="S3.E2.m1.2.3.1.cmml">=</mo><mrow id="S3.E2.m1.2.3.3" xref="S3.E2.m1.2.3.3.cmml"><munder id="S3.E2.m1.2.3.3.1" xref="S3.E2.m1.2.3.3.1.cmml"><mo lspace="0.1389em" movablelimits="false" rspace="0.167em" id="S3.E2.m1.2.3.3.1.2" xref="S3.E2.m1.2.3.3.1.2.cmml">max</mo><mi id="S3.E2.m1.2.3.3.1.3" xref="S3.E2.m1.2.3.3.1.3.cmml">j</mi></munder><msub id="S3.E2.m1.2.3.3.2" xref="S3.E2.m1.2.3.3.2.cmml"><mi id="S3.E2.m1.2.3.3.2.2" xref="S3.E2.m1.2.3.3.2.2.cmml">ğ‘¨</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">i</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">j</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.3.cmml" xref="S3.E2.m1.2.3"><eq id="S3.E2.m1.2.3.1.cmml" xref="S3.E2.m1.2.3.1"></eq><apply id="S3.E2.m1.2.3.2.cmml" xref="S3.E2.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.2.1.cmml" xref="S3.E2.m1.2.3.2">subscript</csymbol><apply id="S3.E2.m1.2.3.2.2.cmml" xref="S3.E2.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.2.2.1.cmml" xref="S3.E2.m1.2.3.2">superscript</csymbol><ci id="S3.E2.m1.2.3.2.2.2.cmml" xref="S3.E2.m1.2.3.2.2.2">ğ’‚</ci><apply id="S3.E2.m1.2.3.2.2.3.cmml" xref="S3.E2.m1.2.3.2.2.3"><minus id="S3.E2.m1.2.3.2.2.3.1.cmml" xref="S3.E2.m1.2.3.2.2.3.1"></minus><ci id="S3.E2.m1.2.3.2.2.3.2.cmml" xref="S3.E2.m1.2.3.2.2.3.2">ğ‘£</ci><ci id="S3.E2.m1.2.3.2.2.3.3.cmml" xref="S3.E2.m1.2.3.2.2.3.3">ğ‘</ci></apply></apply><ci id="S3.E2.m1.2.3.2.3.cmml" xref="S3.E2.m1.2.3.2.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.3.3.cmml" xref="S3.E2.m1.2.3.3"><apply id="S3.E2.m1.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.1.1.cmml" xref="S3.E2.m1.2.3.3.1">subscript</csymbol><max id="S3.E2.m1.2.3.3.1.2.cmml" xref="S3.E2.m1.2.3.3.1.2"></max><ci id="S3.E2.m1.2.3.3.1.3.cmml" xref="S3.E2.m1.2.3.3.1.3">ğ‘—</ci></apply><apply id="S3.E2.m1.2.3.3.2.cmml" xref="S3.E2.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.2.1.cmml" xref="S3.E2.m1.2.3.3.2">subscript</csymbol><ci id="S3.E2.m1.2.3.3.2.2.cmml" xref="S3.E2.m1.2.3.3.2.2">ğ‘¨</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ‘–</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ğ‘—</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">{\bm{a}}^{v-q}_{i}=\mathop{\max}_{j}{\bm{A}}_{i,j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.5" class="ltx_p">Then hard attention instead soft attention is applied to select the most relevant object as the head entity based on <math id="S3.SS1.p4.4.m1.1" class="ltx_Math" alttext="\{{\bm{a}}^{v-q}_{i}\}_{i=1}^{K}" display="inline"><semantics id="S3.SS1.p4.4.m1.1a"><msubsup id="S3.SS1.p4.4.m1.1.1" xref="S3.SS1.p4.4.m1.1.1.cmml"><mrow id="S3.SS1.p4.4.m1.1.1.1.1.1" xref="S3.SS1.p4.4.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.4.m1.1.1.1.1.1.2" xref="S3.SS1.p4.4.m1.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS1.p4.4.m1.1.1.1.1.1.1" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.2" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.2.cmml">ğ’‚</mi><mi id="S3.SS1.p4.4.m1.1.1.1.1.1.1.3" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.3.cmml">i</mi><mrow id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.2" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.2.cmml">v</mi><mo id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.1" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.1.cmml">âˆ’</mo><mi id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.3" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.3.cmml">q</mi></mrow></msubsup><mo stretchy="false" id="S3.SS1.p4.4.m1.1.1.1.1.1.3" xref="S3.SS1.p4.4.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p4.4.m1.1.1.1.3" xref="S3.SS1.p4.4.m1.1.1.1.3.cmml"><mi id="S3.SS1.p4.4.m1.1.1.1.3.2" xref="S3.SS1.p4.4.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p4.4.m1.1.1.1.3.1" xref="S3.SS1.p4.4.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p4.4.m1.1.1.1.3.3" xref="S3.SS1.p4.4.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p4.4.m1.1.1.3" xref="S3.SS1.p4.4.m1.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m1.1b"><apply id="S3.SS1.p4.4.m1.1.1.cmml" xref="S3.SS1.p4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m1.1.1.2.cmml" xref="S3.SS1.p4.4.m1.1.1">superscript</csymbol><apply id="S3.SS1.p4.4.m1.1.1.1.cmml" xref="S3.SS1.p4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m1.1.1.1.2.cmml" xref="S3.SS1.p4.4.m1.1.1">subscript</csymbol><set id="S3.SS1.p4.4.m1.1.1.1.1.2.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1"><apply id="S3.SS1.p4.4.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.2">ğ’‚</ci><apply id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3"><minus id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.2">ğ‘£</ci><ci id="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.2.3.3">ğ‘</ci></apply></apply><ci id="S3.SS1.p4.4.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.4.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p4.4.m1.1.1.1.3.cmml" xref="S3.SS1.p4.4.m1.1.1.1.3"><eq id="S3.SS1.p4.4.m1.1.1.1.3.1.cmml" xref="S3.SS1.p4.4.m1.1.1.1.3.1"></eq><ci id="S3.SS1.p4.4.m1.1.1.1.3.2.cmml" xref="S3.SS1.p4.4.m1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p4.4.m1.1.1.1.3.3.cmml" xref="S3.SS1.p4.4.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p4.4.m1.1.1.3.cmml" xref="S3.SS1.p4.4.m1.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m1.1c">\{{\bm{a}}^{v-q}_{i}\}_{i=1}^{K}</annotation></semantics></math>. Compared with soft attention, hard attention provides more stable and explainable visual content for multimodal knowledge representation, which is also easier for combining with exiting knowledge graph by entity linking. Here we conduct Gumbel-Softmax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to obtain the approximate one-hot categorical distribution. The attention weight for object <math id="S3.SS1.p4.5.m2.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS1.p4.5.m2.1a"><msub id="S3.SS1.p4.5.m2.1.1" xref="S3.SS1.p4.5.m2.1.1.cmml"><mi id="S3.SS1.p4.5.m2.1.1.2" xref="S3.SS1.p4.5.m2.1.1.2.cmml">o</mi><mi id="S3.SS1.p4.5.m2.1.1.3" xref="S3.SS1.p4.5.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m2.1b"><apply id="S3.SS1.p4.5.m2.1.1.cmml" xref="S3.SS1.p4.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m2.1.1.1.cmml" xref="S3.SS1.p4.5.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m2.1.1.2.cmml" xref="S3.SS1.p4.5.m2.1.1.2">ğ‘œ</ci><ci id="S3.SS1.p4.5.m2.1.1.3.cmml" xref="S3.SS1.p4.5.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m2.1c">o_{i}</annotation></semantics></math> is computed as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.6" class="ltx_Math" alttext="\alpha_{i}=\frac{\exp((\log({\bm{a}}^{v-q}_{i})+g_{i})/\tau)}{\sum_{j=1}^{K}\exp((\log({\bm{a}}^{v-q}_{j})+g_{j})/\tau)}" display="block"><semantics id="S3.E3.m1.6a"><mrow id="S3.E3.m1.6.7" xref="S3.E3.m1.6.7.cmml"><msub id="S3.E3.m1.6.7.2" xref="S3.E3.m1.6.7.2.cmml"><mi id="S3.E3.m1.6.7.2.2" xref="S3.E3.m1.6.7.2.2.cmml">Î±</mi><mi id="S3.E3.m1.6.7.2.3" xref="S3.E3.m1.6.7.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.6.7.1" xref="S3.E3.m1.6.7.1.cmml">=</mo><mfrac id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml"><mrow id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.4.cmml"><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">exp</mi><mo id="S3.E3.m1.3.3.3.3a" xref="S3.E3.m1.3.3.3.4.cmml">â¡</mo><mrow id="S3.E3.m1.3.3.3.3.1" xref="S3.E3.m1.3.3.3.4.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.3.3.1.2" xref="S3.E3.m1.3.3.3.4.cmml">(</mo><mrow id="S3.E3.m1.3.3.3.3.1.1" xref="S3.E3.m1.3.3.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.3.3.1.1.1.1" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">log</mi><mo id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1a" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.2.cmml">â¡</mo><mrow id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.2.cmml">(</mo><msubsup id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ’‚</mi><mi id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi><mrow id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.2.cmml">v</mi><mo id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.1.cmml">âˆ’</mo><mi id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.3.cmml">q</mi></mrow></msubsup><mo stretchy="false" id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.3.3.1.1.1.1.1.2" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.2.cmml">+</mo><msub id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.2.cmml">g</mi><mi id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.3.3.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.3.3.3.3.1.1.2" xref="S3.E3.m1.3.3.3.3.1.1.2.cmml">/</mo><mi id="S3.E3.m1.3.3.3.3.1.1.3" xref="S3.E3.m1.3.3.3.3.1.1.3.cmml">Ï„</mi></mrow><mo stretchy="false" id="S3.E3.m1.3.3.3.3.1.3" xref="S3.E3.m1.3.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S3.E3.m1.6.6.6" xref="S3.E3.m1.6.6.6.cmml"><msubsup id="S3.E3.m1.6.6.6.4" xref="S3.E3.m1.6.6.6.4.cmml"><mo id="S3.E3.m1.6.6.6.4.2.2" xref="S3.E3.m1.6.6.6.4.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.6.6.6.4.2.3" xref="S3.E3.m1.6.6.6.4.2.3.cmml"><mi id="S3.E3.m1.6.6.6.4.2.3.2" xref="S3.E3.m1.6.6.6.4.2.3.2.cmml">j</mi><mo id="S3.E3.m1.6.6.6.4.2.3.1" xref="S3.E3.m1.6.6.6.4.2.3.1.cmml">=</mo><mn id="S3.E3.m1.6.6.6.4.2.3.3" xref="S3.E3.m1.6.6.6.4.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.6.6.6.4.3" xref="S3.E3.m1.6.6.6.4.3.cmml">K</mi></msubsup><mrow id="S3.E3.m1.6.6.6.3.1" xref="S3.E3.m1.6.6.6.3.2.cmml"><mi id="S3.E3.m1.5.5.5.2" xref="S3.E3.m1.5.5.5.2.cmml">exp</mi><mo id="S3.E3.m1.6.6.6.3.1a" xref="S3.E3.m1.6.6.6.3.2.cmml">â¡</mo><mrow id="S3.E3.m1.6.6.6.3.1.1" xref="S3.E3.m1.6.6.6.3.2.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.6.3.1.1.2" xref="S3.E3.m1.6.6.6.3.2.cmml">(</mo><mrow id="S3.E3.m1.6.6.6.3.1.1.1" xref="S3.E3.m1.6.6.6.3.1.1.1.cmml"><mrow id="S3.E3.m1.6.6.6.3.1.1.1.1.1" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.2" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.4.4.4.1" xref="S3.E3.m1.4.4.4.1.cmml">log</mi><mo id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1a" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.2.cmml">â¡</mo><mrow id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.2.cmml">(</mo><msubsup id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ’‚</mi><mi id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi><mrow id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">v</mi><mo id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">âˆ’</mo><mi id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">q</mi></mrow></msubsup><mo stretchy="false" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.2" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.2.cmml">+</mo><msub id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.2" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.2.cmml">g</mi><mi id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.3" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.6.6.6.3.1.1.1.2" xref="S3.E3.m1.6.6.6.3.1.1.1.2.cmml">/</mo><mi id="S3.E3.m1.6.6.6.3.1.1.1.3" xref="S3.E3.m1.6.6.6.3.1.1.1.3.cmml">Ï„</mi></mrow><mo stretchy="false" id="S3.E3.m1.6.6.6.3.1.1.3" xref="S3.E3.m1.6.6.6.3.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.6b"><apply id="S3.E3.m1.6.7.cmml" xref="S3.E3.m1.6.7"><eq id="S3.E3.m1.6.7.1.cmml" xref="S3.E3.m1.6.7.1"></eq><apply id="S3.E3.m1.6.7.2.cmml" xref="S3.E3.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.7.2.1.cmml" xref="S3.E3.m1.6.7.2">subscript</csymbol><ci id="S3.E3.m1.6.7.2.2.cmml" xref="S3.E3.m1.6.7.2.2">ğ›¼</ci><ci id="S3.E3.m1.6.7.2.3.cmml" xref="S3.E3.m1.6.7.2.3">ğ‘–</ci></apply><apply id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6"><divide id="S3.E3.m1.6.6.7.cmml" xref="S3.E3.m1.6.6"></divide><apply id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3"><exp id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2"></exp><apply id="S3.E3.m1.3.3.3.3.1.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1"><divide id="S3.E3.m1.3.3.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.2"></divide><apply id="S3.E3.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1"><plus id="S3.E3.m1.3.3.3.3.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.2"></plus><apply id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1"><log id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"></log><apply id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.2">ğ’‚</ci><apply id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.2">ğ‘£</ci><ci id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘</ci></apply></apply><ci id="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.2">ğ‘”</ci><ci id="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.3.3.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S3.E3.m1.3.3.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.3.3.1.1.3">ğœ</ci></apply></apply><apply id="S3.E3.m1.6.6.6.cmml" xref="S3.E3.m1.6.6.6"><apply id="S3.E3.m1.6.6.6.4.cmml" xref="S3.E3.m1.6.6.6.4"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.4.1.cmml" xref="S3.E3.m1.6.6.6.4">superscript</csymbol><apply id="S3.E3.m1.6.6.6.4.2.cmml" xref="S3.E3.m1.6.6.6.4"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.4.2.1.cmml" xref="S3.E3.m1.6.6.6.4">subscript</csymbol><sum id="S3.E3.m1.6.6.6.4.2.2.cmml" xref="S3.E3.m1.6.6.6.4.2.2"></sum><apply id="S3.E3.m1.6.6.6.4.2.3.cmml" xref="S3.E3.m1.6.6.6.4.2.3"><eq id="S3.E3.m1.6.6.6.4.2.3.1.cmml" xref="S3.E3.m1.6.6.6.4.2.3.1"></eq><ci id="S3.E3.m1.6.6.6.4.2.3.2.cmml" xref="S3.E3.m1.6.6.6.4.2.3.2">ğ‘—</ci><cn type="integer" id="S3.E3.m1.6.6.6.4.2.3.3.cmml" xref="S3.E3.m1.6.6.6.4.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.6.6.6.4.3.cmml" xref="S3.E3.m1.6.6.6.4.3">ğ¾</ci></apply><apply id="S3.E3.m1.6.6.6.3.2.cmml" xref="S3.E3.m1.6.6.6.3.1"><exp id="S3.E3.m1.5.5.5.2.cmml" xref="S3.E3.m1.5.5.5.2"></exp><apply id="S3.E3.m1.6.6.6.3.1.1.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1"><divide id="S3.E3.m1.6.6.6.3.1.1.1.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.2"></divide><apply id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1"><plus id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.2"></plus><apply id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1"><log id="S3.E3.m1.4.4.4.1.cmml" xref="S3.E3.m1.4.4.4.1"></log><apply id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.2">ğ’‚</ci><apply id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.2">ğ‘£</ci><ci id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘</ci></apply></apply><ci id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.1.1.1.1.3">ğ‘—</ci></apply></apply><apply id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.2">ğ‘”</ci><ci id="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply><ci id="S3.E3.m1.6.6.6.3.1.1.1.3.cmml" xref="S3.E3.m1.6.6.6.3.1.1.1.3">ğœ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.6c">\alpha_{i}=\frac{\exp((\log({\bm{a}}^{v-q}_{i})+g_{i})/\tau)}{\sum_{j=1}^{K}\exp((\log({\bm{a}}^{v-q}_{j})+g_{j})/\tau)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.8" class="ltx_p">where <math id="S3.SS1.p4.6.m1.1" class="ltx_Math" alttext="\{g_{i}\}_{i=1}^{K}" display="inline"><semantics id="S3.SS1.p4.6.m1.1a"><msubsup id="S3.SS1.p4.6.m1.1.1" xref="S3.SS1.p4.6.m1.1.1.cmml"><mrow id="S3.SS1.p4.6.m1.1.1.1.1.1" xref="S3.SS1.p4.6.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.6.m1.1.1.1.1.1.2" xref="S3.SS1.p4.6.m1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p4.6.m1.1.1.1.1.1.1" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p4.6.m1.1.1.1.1.1.1.2" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1.2.cmml">g</mi><mi id="S3.SS1.p4.6.m1.1.1.1.1.1.1.3" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p4.6.m1.1.1.1.1.1.3" xref="S3.SS1.p4.6.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p4.6.m1.1.1.1.3" xref="S3.SS1.p4.6.m1.1.1.1.3.cmml"><mi id="S3.SS1.p4.6.m1.1.1.1.3.2" xref="S3.SS1.p4.6.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p4.6.m1.1.1.1.3.1" xref="S3.SS1.p4.6.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p4.6.m1.1.1.1.3.3" xref="S3.SS1.p4.6.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p4.6.m1.1.1.3" xref="S3.SS1.p4.6.m1.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m1.1b"><apply id="S3.SS1.p4.6.m1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m1.1.1.2.cmml" xref="S3.SS1.p4.6.m1.1.1">superscript</csymbol><apply id="S3.SS1.p4.6.m1.1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m1.1.1.1.2.cmml" xref="S3.SS1.p4.6.m1.1.1">subscript</csymbol><set id="S3.SS1.p4.6.m1.1.1.1.1.2.cmml" xref="S3.SS1.p4.6.m1.1.1.1.1.1"><apply id="S3.SS1.p4.6.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.6.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1.2">ğ‘”</ci><ci id="S3.SS1.p4.6.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.6.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p4.6.m1.1.1.1.3.cmml" xref="S3.SS1.p4.6.m1.1.1.1.3"><eq id="S3.SS1.p4.6.m1.1.1.1.3.1.cmml" xref="S3.SS1.p4.6.m1.1.1.1.3.1"></eq><ci id="S3.SS1.p4.6.m1.1.1.1.3.2.cmml" xref="S3.SS1.p4.6.m1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p4.6.m1.1.1.1.3.3.cmml" xref="S3.SS1.p4.6.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p4.6.m1.1.1.3.cmml" xref="S3.SS1.p4.6.m1.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m1.1c">\{g_{i}\}_{i=1}^{K}</annotation></semantics></math> are <span id="S3.SS1.p4.8.1" class="ltx_text ltx_font_italic">i.i.d.</span> samples drawn from Gumbel(0,1)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The Gumbel (0,1) distribution can be sampled using inverse transform sampling by drawing <math id="footnote1.m1.1" class="ltx_Math" alttext="u\sim" display="inline"><semantics id="footnote1.m1.1b"><mrow id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml"><mi id="footnote1.m1.1.1.2" xref="footnote1.m1.1.1.2.cmml">u</mi><mo id="footnote1.m1.1.1.1" xref="footnote1.m1.1.1.1.cmml">âˆ¼</mo><mi id="footnote1.m1.1.1.3" xref="footnote1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"><csymbol cd="latexml" id="footnote1.m1.1.1.1.cmml" xref="footnote1.m1.1.1.1">similar-to</csymbol><ci id="footnote1.m1.1.1.2.cmml" xref="footnote1.m1.1.1.2">ğ‘¢</ci><csymbol cd="latexml" id="footnote1.m1.1.1.3.cmml" xref="footnote1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">u\sim</annotation></semantics></math> Uniform(0,1) and computing <math id="footnote1.m2.3" class="ltx_math_unparsed" alttext="g=-\log(-\log(u)))" display="inline"><semantics id="footnote1.m2.3b"><mrow id="footnote1.m2.3c"><mi id="footnote1.m2.3.4">g</mi><mo rspace="0em" id="footnote1.m2.3.5">=</mo><mo lspace="0em" id="footnote1.m2.3.6">âˆ’</mo><mi id="footnote1.m2.3.3">log</mi><mrow id="footnote1.m2.3.7"><mo stretchy="false" id="footnote1.m2.3.7.1">(</mo><mo lspace="0em" id="footnote1.m2.3.7.2">âˆ’</mo><mi id="footnote1.m2.1.1">log</mi><mrow id="footnote1.m2.3.7.3"><mo stretchy="false" id="footnote1.m2.3.7.3.1">(</mo><mi id="footnote1.m2.2.2">u</mi><mo stretchy="false" id="footnote1.m2.3.7.3.2">)</mo></mrow><mo stretchy="false" id="footnote1.m2.3.7.4">)</mo></mrow><mo stretchy="false" id="footnote1.m2.3.8">)</mo></mrow><annotation encoding="application/x-tex" id="footnote1.m2.3d">g=-\log(-\log(u)))</annotation></semantics></math></span></span></span>, and <math id="S3.SS1.p4.7.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS1.p4.7.m2.1a"><mi id="S3.SS1.p4.7.m2.1.1" xref="S3.SS1.p4.7.m2.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m2.1b"><ci id="S3.SS1.p4.7.m2.1.1.cmml" xref="S3.SS1.p4.7.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m2.1c">\tau</annotation></semantics></math> is the softmax temperature. Finally, we gather the question-centric object information and obtain the head entity representation <math id="S3.SS1.p4.8.m3.1" class="ltx_Math" alttext="\bm{h}" display="inline"><semantics id="S3.SS1.p4.8.m3.1a"><mi id="S3.SS1.p4.8.m3.1.1" xref="S3.SS1.p4.8.m3.1.1.cmml">ğ’‰</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m3.1b"><ci id="S3.SS1.p4.8.m3.1.1.cmml" xref="S3.SS1.p4.8.m3.1.1">ğ’‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m3.1c">\bm{h}</annotation></semantics></math> as:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="{\bm{h}}={\rm FFN}(\sum_{i=1}^{K}\alpha_{i}\bm{v}_{i})" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">ğ’‰</mi><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">FFN</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><munderover id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mo lspace="0em" movablelimits="false" id="S3.E4.m1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml">K</mi></munderover><mrow id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml">Î±</mi><mi id="S3.E4.m1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.2.3.2.cmml">ğ’—</mi><mi id="S3.E4.m1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">ğ’‰</ci><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">FFN</ci><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><sum id="S3.E4.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.2"></sum><apply id="S3.E4.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3"><eq id="S3.E4.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.1"></eq><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">ğ¾</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1"></times><apply id="S3.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2">ğ›¼</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.2">ğ’—</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">{\bm{h}}={\rm FFN}(\sum_{i=1}^{K}\alpha_{i}\bm{v}_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.9" class="ltx_p">where <math id="S3.SS1.p4.9.m1.1" class="ltx_Math" alttext="\bm{v}_{i}\in\bm{V}" display="inline"><semantics id="S3.SS1.p4.9.m1.1a"><mrow id="S3.SS1.p4.9.m1.1.1" xref="S3.SS1.p4.9.m1.1.1.cmml"><msub id="S3.SS1.p4.9.m1.1.1.2" xref="S3.SS1.p4.9.m1.1.1.2.cmml"><mi id="S3.SS1.p4.9.m1.1.1.2.2" xref="S3.SS1.p4.9.m1.1.1.2.2.cmml">ğ’—</mi><mi id="S3.SS1.p4.9.m1.1.1.2.3" xref="S3.SS1.p4.9.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p4.9.m1.1.1.1" xref="S3.SS1.p4.9.m1.1.1.1.cmml">âˆˆ</mo><mi id="S3.SS1.p4.9.m1.1.1.3" xref="S3.SS1.p4.9.m1.1.1.3.cmml">ğ‘½</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m1.1b"><apply id="S3.SS1.p4.9.m1.1.1.cmml" xref="S3.SS1.p4.9.m1.1.1"><in id="S3.SS1.p4.9.m1.1.1.1.cmml" xref="S3.SS1.p4.9.m1.1.1.1"></in><apply id="S3.SS1.p4.9.m1.1.1.2.cmml" xref="S3.SS1.p4.9.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.9.m1.1.1.2.1.cmml" xref="S3.SS1.p4.9.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.9.m1.1.1.2.2.cmml" xref="S3.SS1.p4.9.m1.1.1.2.2">ğ’—</ci><ci id="S3.SS1.p4.9.m1.1.1.2.3.cmml" xref="S3.SS1.p4.9.m1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.SS1.p4.9.m1.1.1.3.cmml" xref="S3.SS1.p4.9.m1.1.1.3">ğ‘½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m1.1c">\bm{v}_{i}\in\bm{V}</annotation></semantics></math> and FFN denotes a feed-forward network that contains two fully connected layers.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Relation Extraction.</span>â€ƒDifferent from the relation in traditional knowledge graph that depicts the first-order predicate independent of specific visual scenario, we define the relation in multimodal knowledge as the complex implicit relation between the observed instantiated object and the corresponding fact answer. Since LXMERT captures the implicit correlations between the image and the question via the self-attention mechanism in the hierarchical transformers, we extract the cross-modal representation from the [CLS] token, and feed it into a FFN layer to obtain the relation embedding, which is denoted as <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="{\bm{r}}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">ğ’“</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">ğ’“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">{\bm{r}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Tail Entity Extraction.</span>â€ƒWe define the tail entity as the answer in an image-question-answer sample, which reveals a specific aspect of facts regarding to the visual object referred by the question. In the training stage, we set ground truth answer as the tail entity to learn its representation <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="\bm{t}" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mi id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">ğ’•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">ğ’•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">\bm{t}</annotation></semantics></math> from scratch (details in Section <a href="#S3.SS2" title="3.2 Knowledge Triplet Representation Learning â€£ 3 Methodology â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). In the inference stage, we define the KB-VQA task as a multimodal knowledge graph completion problem and globally assess the knowledge in our neural multimodal knowledge base to predict the optimal tail entity as the answer (details in Section <a href="#S3.SS3" title="3.3 Knowledge Accumulation and Prediction â€£ 3 Methodology â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Knowledge Triplet Representation Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Since each component within a triplet contains modality-different and semantic-specific information, we propose three loss functions to unifiedly learn the triplet representation in order to bridge the heterogeneous gap as well as semantic gap. The three losses constrain the triplet representation from complementary views: the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Triplet TransE Loss</span> preserves the embedding structure by contrasting positive and negative triplets. The <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">Triplet Consistency Loss</span> further forces the three embeddings within a triplet to keep the strict topological relation, and the <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">Semantic Consistency Loss</span> maps the embeddings into a common semantic space for fair comparison among multimodal content.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.9" class="ltx_p"><span id="S3.SS2.p2.9.1" class="ltx_text ltx_font_bold">Triplet TransE Loss.</span>â€ƒInspired by the knowledge embedding method TransE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> in the traditional knowledge graph field, we apply TransE-like objective loss as a structure-preserving constraint in our multimodal scenario. Given an image-question pair, let <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{A}^{+}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ğ’œ</mi><mo id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ’œ</ci><plus id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{A}^{+}</annotation></semantics></math> and <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{A}^{-}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">ğ’œ</mi><mo id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">âˆ’</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ’œ</ci><minus id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathcal{A}^{-}</annotation></semantics></math> denote its sets of correct (positive) and incorrect (negative) answers, respectively. Let <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\bm{h}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">ğ’‰</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ’‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\bm{h}</annotation></semantics></math> and <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\bm{r}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">ğ’“</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ğ’“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\bm{r}</annotation></semantics></math> denote the corresponding extracted head and tail entity representations. We want the distance between <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\bm{h}+\bm{r}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">ğ’‰</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">+</mo><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">ğ’“</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><plus id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></plus><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">ğ’‰</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">ğ’“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\bm{h}+\bm{r}</annotation></semantics></math> and each positive tail <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\bm{t}^{+}\in\mathcal{A}^{+}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><msup id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">ğ’•</mi><mo id="S3.SS2.p2.6.m6.1.1.2.3" xref="S3.SS2.p2.6.m6.1.1.2.3.cmml">+</mo></msup><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.6.m6.1.1.3.2" xref="S3.SS2.p2.6.m6.1.1.3.2.cmml">ğ’œ</mi><mo id="S3.SS2.p2.6.m6.1.1.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><in id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></in><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">ğ’•</ci><plus id="S3.SS2.p2.6.m6.1.1.2.3.cmml" xref="S3.SS2.p2.6.m6.1.1.2.3"></plus></apply><apply id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.2">ğ’œ</ci><plus id="S3.SS2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\bm{t}^{+}\in\mathcal{A}^{+}</annotation></semantics></math> to be smaller than the distance between <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\bm{h}+\bm{r}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">ğ’‰</mi><mo id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml">+</mo><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">ğ’“</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><plus id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"></plus><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">ğ’‰</ci><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">ğ’“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\bm{h}+\bm{r}</annotation></semantics></math> and each negative tail <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="\bm{t}^{-}\in\mathcal{A}^{-}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mrow id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><msup id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2.2" xref="S3.SS2.p2.8.m8.1.1.2.2.cmml">ğ’•</mi><mo id="S3.SS2.p2.8.m8.1.1.2.3" xref="S3.SS2.p2.8.m8.1.1.2.3.cmml">âˆ’</mo></msup><mo id="S3.SS2.p2.8.m8.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.8.m8.1.1.3.2" xref="S3.SS2.p2.8.m8.1.1.3.2.cmml">ğ’œ</mi><mo id="S3.SS2.p2.8.m8.1.1.3.3" xref="S3.SS2.p2.8.m8.1.1.3.3.cmml">âˆ’</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><in id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1"></in><apply id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.2.1.cmml" xref="S3.SS2.p2.8.m8.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.2.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2.2">ğ’•</ci><minus id="S3.SS2.p2.8.m8.1.1.2.3.cmml" xref="S3.SS2.p2.8.m8.1.1.2.3"></minus></apply><apply id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.3.1.cmml" xref="S3.SS2.p2.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.3.2.cmml" xref="S3.SS2.p2.8.m8.1.1.3.2">ğ’œ</ci><minus id="S3.SS2.p2.8.m8.1.1.3.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3.3"></minus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">\bm{t}^{-}\in\mathcal{A}^{-}</annotation></semantics></math> by a certain margin <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mi id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><ci id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">\gamma</annotation></semantics></math>:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm TransE}=\sum_{\bm{t}^{+}\in\mathcal{A}^{+}}\sum_{\bm{t}^{-}\in\mathcal{A}^{-}}[\gamma+{\rm d}(\bm{h}+\bm{r},\bm{t}^{+})-{\rm d}(\bm{h}+\bm{r},\bm{t}^{-})]_{+}" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">â„’</mi><mi id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml">TransE</mi></msub><mo rspace="0.111em" id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><munder id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E5.m1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E5.m1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.2.3.cmml"><msup id="S3.E5.m1.1.1.1.2.3.2" xref="S3.E5.m1.1.1.1.2.3.2.cmml"><mi id="S3.E5.m1.1.1.1.2.3.2.2" xref="S3.E5.m1.1.1.1.2.3.2.2.cmml">ğ’•</mi><mo id="S3.E5.m1.1.1.1.2.3.2.3" xref="S3.E5.m1.1.1.1.2.3.2.3.cmml">+</mo></msup><mo id="S3.E5.m1.1.1.1.2.3.1" xref="S3.E5.m1.1.1.1.2.3.1.cmml">âˆˆ</mo><msup id="S3.E5.m1.1.1.1.2.3.3" xref="S3.E5.m1.1.1.1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.2.3.3.2" xref="S3.E5.m1.1.1.1.2.3.3.2.cmml">ğ’œ</mi><mo id="S3.E5.m1.1.1.1.2.3.3.3" xref="S3.E5.m1.1.1.1.2.3.3.3.cmml">+</mo></msup></mrow></munder><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><munder id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E5.m1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E5.m1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.2.3.cmml"><msup id="S3.E5.m1.1.1.1.1.2.3.2" xref="S3.E5.m1.1.1.1.1.2.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.2.3.2.2" xref="S3.E5.m1.1.1.1.1.2.3.2.2.cmml">ğ’•</mi><mo id="S3.E5.m1.1.1.1.1.2.3.2.3" xref="S3.E5.m1.1.1.1.1.2.3.2.3.cmml">âˆ’</mo></msup><mo id="S3.E5.m1.1.1.1.1.2.3.1" xref="S3.E5.m1.1.1.1.1.2.3.1.cmml">âˆˆ</mo><msup id="S3.E5.m1.1.1.1.1.2.3.3" xref="S3.E5.m1.1.1.1.1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.2.3.3.2" xref="S3.E5.m1.1.1.1.1.2.3.3.2.cmml">ğ’œ</mi><mo id="S3.E5.m1.1.1.1.1.2.3.3.3" xref="S3.E5.m1.1.1.1.1.2.3.3.3.cmml">âˆ’</mo></msup></mrow></munder><msub id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.2.4" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.4.cmml">Î³</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi mathvariant="normal" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.4" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">â€‹</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ’‰</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ’“</mi></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.4" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">,</mo><msup id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">ğ’•</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml">+</mo></msup><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.5" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.5" xref="S3.E5.m1.1.1.1.1.1.1.1.1.5.cmml">âˆ’</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.4" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.cmml"><mi mathvariant="normal" id="S3.E5.m1.1.1.1.1.1.1.1.1.4.4" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.3.cmml">â€‹</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.3.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.2.cmml">ğ’‰</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.1.cmml">+</mo><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.3.cmml">ğ’“</mi></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.4" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">,</mo><msup id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.2.cmml">ğ’•</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.3.cmml">âˆ’</mo></msup><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.5" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.3.cmml">+</mo></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"></eq><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">â„’</ci><ci id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3">TransE</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><apply id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.2.2"></sum><apply id="S3.E5.m1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.2.3"><in id="S3.E5.m1.1.1.1.2.3.1.cmml" xref="S3.E5.m1.1.1.1.2.3.1"></in><apply id="S3.E5.m1.1.1.1.2.3.2.cmml" xref="S3.E5.m1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.2.3.2.1.cmml" xref="S3.E5.m1.1.1.1.2.3.2">superscript</csymbol><ci id="S3.E5.m1.1.1.1.2.3.2.2.cmml" xref="S3.E5.m1.1.1.1.2.3.2.2">ğ’•</ci><plus id="S3.E5.m1.1.1.1.2.3.2.3.cmml" xref="S3.E5.m1.1.1.1.2.3.2.3"></plus></apply><apply id="S3.E5.m1.1.1.1.2.3.3.cmml" xref="S3.E5.m1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.2.3.3.1.cmml" xref="S3.E5.m1.1.1.1.2.3.3">superscript</csymbol><ci id="S3.E5.m1.1.1.1.2.3.3.2.cmml" xref="S3.E5.m1.1.1.1.2.3.3.2">ğ’œ</ci><plus id="S3.E5.m1.1.1.1.2.3.3.3.cmml" xref="S3.E5.m1.1.1.1.2.3.3.3"></plus></apply></apply></apply><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1"><apply id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2"></sum><apply id="S3.E5.m1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3"><in id="S3.E5.m1.1.1.1.1.2.3.1.cmml" xref="S3.E5.m1.1.1.1.1.2.3.1"></in><apply id="S3.E5.m1.1.1.1.1.2.3.2.cmml" xref="S3.E5.m1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2.3.2">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.2.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.3.2.2">ğ’•</ci><minus id="S3.E5.m1.1.1.1.1.2.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3.2.3"></minus></apply><apply id="S3.E5.m1.1.1.1.1.2.3.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.3.3.1.cmml" xref="S3.E5.m1.1.1.1.1.2.3.3">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.2.3.3.2.cmml" xref="S3.E5.m1.1.1.1.1.2.3.3.2">ğ’œ</ci><minus id="S3.E5.m1.1.1.1.1.2.3.3.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3.3.3"></minus></apply></apply></apply><apply id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1"><minus id="S3.E5.m1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.5"></minus><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2"><plus id="S3.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.3"></plus><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.4">ğ›¾</ci><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2"><times id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.3"></times><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.4.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.4">d</ci><interval closure="open" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2"><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ’‰</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ’“</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2">ğ’•</ci><plus id="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3"></plus></apply></interval></apply></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4"><times id="S3.E5.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.3"></times><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.4.4.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.4">d</ci><interval closure="open" id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2"><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1"><plus id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.1"></plus><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.2">ğ’‰</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.1.1.3">ğ’“</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.2">ğ’•</ci><minus id="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.4.2.2.2.3"></minus></apply></interval></apply></apply></apply><plus id="S3.E5.m1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.3"></plus></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\mathcal{L}_{\rm TransE}=\sum_{\bm{t}^{+}\in\mathcal{A}^{+}}\sum_{\bm{t}^{-}\in\mathcal{A}^{-}}[\gamma+{\rm d}(\bm{h}+\bm{r},\bm{t}^{+})-{\rm d}(\bm{h}+\bm{r},\bm{t}^{-})]_{+}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.11" class="ltx_p">where <math id="S3.SS2.p2.10.m1.3" class="ltx_Math" alttext="[\cdot]_{+}\triangleq{\rm max}(0,\cdot)" display="inline"><semantics id="S3.SS2.p2.10.m1.3a"><mrow id="S3.SS2.p2.10.m1.3.4" xref="S3.SS2.p2.10.m1.3.4.cmml"><msub id="S3.SS2.p2.10.m1.3.4.2" xref="S3.SS2.p2.10.m1.3.4.2.cmml"><mrow id="S3.SS2.p2.10.m1.3.4.2.2.2" xref="S3.SS2.p2.10.m1.3.4.2.2.1.cmml"><mo stretchy="false" id="S3.SS2.p2.10.m1.3.4.2.2.2.1" xref="S3.SS2.p2.10.m1.3.4.2.2.1.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p2.10.m1.1.1" xref="S3.SS2.p2.10.m1.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS2.p2.10.m1.3.4.2.2.2.2" xref="S3.SS2.p2.10.m1.3.4.2.2.1.1.cmml">]</mo></mrow><mo id="S3.SS2.p2.10.m1.3.4.2.3" xref="S3.SS2.p2.10.m1.3.4.2.3.cmml">+</mo></msub><mo id="S3.SS2.p2.10.m1.3.4.1" xref="S3.SS2.p2.10.m1.3.4.1.cmml">â‰œ</mo><mrow id="S3.SS2.p2.10.m1.3.4.3" xref="S3.SS2.p2.10.m1.3.4.3.cmml"><mi id="S3.SS2.p2.10.m1.3.4.3.2" xref="S3.SS2.p2.10.m1.3.4.3.2.cmml">max</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.10.m1.3.4.3.1" xref="S3.SS2.p2.10.m1.3.4.3.1.cmml">â€‹</mo><mrow id="S3.SS2.p2.10.m1.3.4.3.3.2" xref="S3.SS2.p2.10.m1.3.4.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.10.m1.3.4.3.3.2.1" xref="S3.SS2.p2.10.m1.3.4.3.3.1.cmml">(</mo><mn id="S3.SS2.p2.10.m1.2.2" xref="S3.SS2.p2.10.m1.2.2.cmml">0</mn><mo rspace="0em" id="S3.SS2.p2.10.m1.3.4.3.3.2.2" xref="S3.SS2.p2.10.m1.3.4.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p2.10.m1.3.3" xref="S3.SS2.p2.10.m1.3.3.cmml">â‹…</mo><mo stretchy="false" id="S3.SS2.p2.10.m1.3.4.3.3.2.3" xref="S3.SS2.p2.10.m1.3.4.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m1.3b"><apply id="S3.SS2.p2.10.m1.3.4.cmml" xref="S3.SS2.p2.10.m1.3.4"><ci id="S3.SS2.p2.10.m1.3.4.1.cmml" xref="S3.SS2.p2.10.m1.3.4.1">â‰œ</ci><apply id="S3.SS2.p2.10.m1.3.4.2.cmml" xref="S3.SS2.p2.10.m1.3.4.2"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m1.3.4.2.1.cmml" xref="S3.SS2.p2.10.m1.3.4.2">subscript</csymbol><apply id="S3.SS2.p2.10.m1.3.4.2.2.1.cmml" xref="S3.SS2.p2.10.m1.3.4.2.2.2"><csymbol cd="latexml" id="S3.SS2.p2.10.m1.3.4.2.2.1.1.cmml" xref="S3.SS2.p2.10.m1.3.4.2.2.2.1">delimited-[]</csymbol><ci id="S3.SS2.p2.10.m1.1.1.cmml" xref="S3.SS2.p2.10.m1.1.1">â‹…</ci></apply><plus id="S3.SS2.p2.10.m1.3.4.2.3.cmml" xref="S3.SS2.p2.10.m1.3.4.2.3"></plus></apply><apply id="S3.SS2.p2.10.m1.3.4.3.cmml" xref="S3.SS2.p2.10.m1.3.4.3"><times id="S3.SS2.p2.10.m1.3.4.3.1.cmml" xref="S3.SS2.p2.10.m1.3.4.3.1"></times><ci id="S3.SS2.p2.10.m1.3.4.3.2.cmml" xref="S3.SS2.p2.10.m1.3.4.3.2">max</ci><interval closure="open" id="S3.SS2.p2.10.m1.3.4.3.3.1.cmml" xref="S3.SS2.p2.10.m1.3.4.3.3.2"><cn type="integer" id="S3.SS2.p2.10.m1.2.2.cmml" xref="S3.SS2.p2.10.m1.2.2">0</cn><ci id="S3.SS2.p2.10.m1.3.3.cmml" xref="S3.SS2.p2.10.m1.3.3">â‹…</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m1.3c">[\cdot]_{+}\triangleq{\rm max}(0,\cdot)</annotation></semantics></math> and <math id="S3.SS2.p2.11.m2.2" class="ltx_Math" alttext="{\rm d}(\cdot,\cdot)" display="inline"><semantics id="S3.SS2.p2.11.m2.2a"><mrow id="S3.SS2.p2.11.m2.2.3" xref="S3.SS2.p2.11.m2.2.3.cmml"><mi mathvariant="normal" id="S3.SS2.p2.11.m2.2.3.2" xref="S3.SS2.p2.11.m2.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.11.m2.2.3.1" xref="S3.SS2.p2.11.m2.2.3.1.cmml">â€‹</mo><mrow id="S3.SS2.p2.11.m2.2.3.3.2" xref="S3.SS2.p2.11.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.11.m2.2.3.3.2.1" xref="S3.SS2.p2.11.m2.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p2.11.m2.1.1" xref="S3.SS2.p2.11.m2.1.1.cmml">â‹…</mo><mo rspace="0em" id="S3.SS2.p2.11.m2.2.3.3.2.2" xref="S3.SS2.p2.11.m2.2.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p2.11.m2.2.2" xref="S3.SS2.p2.11.m2.2.2.cmml">â‹…</mo><mo stretchy="false" id="S3.SS2.p2.11.m2.2.3.3.2.3" xref="S3.SS2.p2.11.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m2.2b"><apply id="S3.SS2.p2.11.m2.2.3.cmml" xref="S3.SS2.p2.11.m2.2.3"><times id="S3.SS2.p2.11.m2.2.3.1.cmml" xref="S3.SS2.p2.11.m2.2.3.1"></times><ci id="S3.SS2.p2.11.m2.2.3.2.cmml" xref="S3.SS2.p2.11.m2.2.3.2">d</ci><interval closure="open" id="S3.SS2.p2.11.m2.2.3.3.1.cmml" xref="S3.SS2.p2.11.m2.2.3.3.2"><ci id="S3.SS2.p2.11.m2.1.1.cmml" xref="S3.SS2.p2.11.m2.1.1">â‹…</ci><ci id="S3.SS2.p2.11.m2.2.2.cmml" xref="S3.SS2.p2.11.m2.2.2">â‹…</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m2.2c">{\rm d}(\cdot,\cdot)</annotation></semantics></math> denotes the cosine distance following the settings in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Triplet Consistency Loss.</span>â€ƒThe issue of the above TransE loss is that once the distance between the positive pairs is smaller than the negative pairs by margin <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\gamma</annotation></semantics></math> during training, the model will stop learning from the triplet. To further push the embeddings to satisfy the strict topological relation, we apply Mean Squared Error (MSE) criterion to constrain the representations on top of each positive triplet as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\mathcal{L}_{\rm Tri}={\rm MSE}(\bm{h}+\bm{r},\bm{t}^{+})" display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml"><msub id="S3.E6.m1.2.2.4" xref="S3.E6.m1.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.2.2.4.2" xref="S3.E6.m1.2.2.4.2.cmml">â„’</mi><mi id="S3.E6.m1.2.2.4.3" xref="S3.E6.m1.2.2.4.3.cmml">Tri</mi></msub><mo id="S3.E6.m1.2.2.3" xref="S3.E6.m1.2.2.3.cmml">=</mo><mrow id="S3.E6.m1.2.2.2" xref="S3.E6.m1.2.2.2.cmml"><mi id="S3.E6.m1.2.2.2.4" xref="S3.E6.m1.2.2.2.4.cmml">MSE</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.3" xref="S3.E6.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E6.m1.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E6.m1.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.3.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml">ğ’‰</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">ğ’“</mi></mrow><mo id="S3.E6.m1.2.2.2.2.2.4" xref="S3.E6.m1.2.2.2.2.3.cmml">,</mo><msup id="S3.E6.m1.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.2.cmml">ğ’•</mi><mo id="S3.E6.m1.2.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.2.2.3.cmml">+</mo></msup><mo stretchy="false" id="S3.E6.m1.2.2.2.2.2.5" xref="S3.E6.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2"><eq id="S3.E6.m1.2.2.3.cmml" xref="S3.E6.m1.2.2.3"></eq><apply id="S3.E6.m1.2.2.4.cmml" xref="S3.E6.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.4.1.cmml" xref="S3.E6.m1.2.2.4">subscript</csymbol><ci id="S3.E6.m1.2.2.4.2.cmml" xref="S3.E6.m1.2.2.4.2">â„’</ci><ci id="S3.E6.m1.2.2.4.3.cmml" xref="S3.E6.m1.2.2.4.3">Tri</ci></apply><apply id="S3.E6.m1.2.2.2.cmml" xref="S3.E6.m1.2.2.2"><times id="S3.E6.m1.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.3"></times><ci id="S3.E6.m1.2.2.2.4.cmml" xref="S3.E6.m1.2.2.2.4">MSE</ci><interval closure="open" id="S3.E6.m1.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2"><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><plus id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"></plus><ci id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2">ğ’‰</ci><ci id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">ğ’“</ci></apply><apply id="S3.E6.m1.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E6.m1.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2">ğ’•</ci><plus id="S3.E6.m1.2.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3"></plus></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\mathcal{L}_{\rm Tri}={\rm MSE}(\bm{h}+\bm{r},\bm{t}^{+})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â <span id="S3.T1.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â <span id="S3.T1.1.2.1.2.1" class="ltx_text ltx_font_bold">Knowledge Resources</span></th>
<th id="S3.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â <span id="S3.T1.1.2.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr">
<th id="S3.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â ArticleNet (AN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></th>
<td id="S3.T1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Wikipedia</td>
<td id="S3.T1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 5.28</td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<th id="S3.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Q-only <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></th>
<td id="S3.T1.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€”</td>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 14.93</td>
</tr>
<tr id="S3.T1.1.5.3" class="ltx_tr">
<th id="S3.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></th>
<td id="S3.T1.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€”</td>
<td id="S3.T1.1.5.3.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 25.17</td>
</tr>
<tr id="S3.T1.1.6.4" class="ltx_tr">
<th id="S3.T1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€ƒ+AN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></th>
<td id="S3.T1.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Wikipedia</td>
<td id="S3.T1.1.6.4.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 25.61</td>
</tr>
<tr id="S3.T1.1.7.5" class="ltx_tr">
<th id="S3.T1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€ƒ+ KG-AUG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></th>
<td id="S3.T1.1.7.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Wikipedia + ConceptNet</td>
<td id="S3.T1.1.7.5.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 26.71</td>
</tr>
<tr id="S3.T1.1.8.6" class="ltx_tr">
<th id="S3.T1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></th>
<td id="S3.T1.1.8.6.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€”</td>
<td id="S3.T1.1.8.6.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 26.41</td>
</tr>
<tr id="S3.T1.1.9.7" class="ltx_tr">
<th id="S3.T1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€ƒ+ AN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></th>
<td id="S3.T1.1.9.7.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Wikipedia</td>
<td id="S3.T1.1.9.7.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 27.84</td>
</tr>
<tr id="S3.T1.1.10.8" class="ltx_tr">
<th id="S3.T1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Mucko <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite></th>
<td id="S3.T1.1.10.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â ConceptNet</td>
<td id="S3.T1.1.10.8.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 29.20</td>
</tr>
<tr id="S3.T1.1.11.9" class="ltx_tr">
<th id="S3.T1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â GRUC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite></th>
<td id="S3.T1.1.11.9.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â ConceptNet</td>
<td id="S3.T1.1.11.9.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 29.87</td>
</tr>
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â KM<sup id="S3.T1.1.1.1.1" class="ltx_sup">4</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite></th>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â multimodal knowledge from OK-VQA</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 31.32</td>
</tr>
<tr id="S3.T1.1.12.10" class="ltx_tr">
<th id="S3.T1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></th>
<td id="S3.T1.1.12.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€”</td>
<td id="S3.T1.1.12.10.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 31.35</td>
</tr>
<tr id="S3.T1.1.13.11" class="ltx_tr">
<th id="S3.T1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></th>
<td id="S3.T1.1.13.11.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â â€”</td>
<td id="S3.T1.1.13.11.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 32.04</td>
</tr>
<tr id="S3.T1.1.14.12" class="ltx_tr">
<th id="S3.T1.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â KRISP(w/o mm pre.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></th>
<td id="S3.T1.1.14.12.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â DBpedia + ConceptNet + VisualGenome + haspartKB</td>
<td id="S3.T1.1.14.12.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 32.31</td>
</tr>
<tr id="S3.T1.1.15.13" class="ltx_tr">
<th id="S3.T1.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â KRISP(w/ mm pre.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></th>
<td id="S3.T1.1.15.13.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â DBpedia + ConceptNet + VisualGenome + haspartKB</td>
<td id="S3.T1.1.15.13.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 38.90</td>
</tr>
<tr id="S3.T1.1.16.14" class="ltx_tr">
<th id="S3.T1.1.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â ConceptBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></th>
<td id="S3.T1.1.16.14.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â ConceptNet</td>
<td id="S3.T1.1.16.14.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 33.66</td>
</tr>
<tr id="S3.T1.1.17.15" class="ltx_tr">
<th id="S3.T1.1.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â Knowledge is Power <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></th>
<td id="S3.T1.1.17.15.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â YAGO3</td>
<td id="S3.T1.1.17.15.3" class="ltx_td ltx_align_left" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â 39.24</td>
</tr>
<tr id="S3.T1.1.18.16" class="ltx_tr">
<th id="S3.T1.1.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â MuKEA</th>
<td id="S3.T1.1.18.16.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â multimodal knowledge from VQA 2.0 and OK-VQA</td>
<td id="S3.T1.1.18.16.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-left:17.1pt;padding-right:17.1pt;">Â Â Â Â Â Â <span id="S3.T1.1.18.16.3.1" class="ltx_text ltx_font_bold">42.59</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;">State-of-the-art comparison on OK-VQA dataset. The middle column lists the
external knowledge sources, if any, used in each VQA system. The rows in the middle part list the
method based on pre-trained model.</span></figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Semantic Consistency Loss.</span>â€ƒWe randomly initialize a look-up table of tail entities and learn their representations together with the head and the relation. Each tail entity in the look-up table <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bm{T}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">ğ‘»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ‘»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\bm{T}</annotation></semantics></math> corresponds to an unique answer in the training VQA samples. To introduce the semantics of answer in tail representation while narrowing the heterogeneous gap between text-formed tail entity and multimodal-formed head entity and relation,
we classify the triplet over the tail vocabulary and force the model to select the ground-truth tail (answer) by the negative log likelihood loss:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.3" class="ltx_Math" alttext="P(\bm{t}^{+})={\rm softmax}((\bm{T})^{T}(\bm{h}+\bm{r}))" display="block"><semantics id="S3.E7.m1.3a"><mrow id="S3.E7.m1.3.3" xref="S3.E7.m1.3.3.cmml"><mrow id="S3.E7.m1.2.2.1" xref="S3.E7.m1.2.2.1.cmml"><mi id="S3.E7.m1.2.2.1.3" xref="S3.E7.m1.2.2.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.2" xref="S3.E7.m1.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.2.2.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.cmml">(</mo><msup id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml"><mi id="S3.E7.m1.2.2.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml">ğ’•</mi><mo id="S3.E7.m1.2.2.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.3.cmml">+</mo></msup><mo stretchy="false" id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.3.3.3" xref="S3.E7.m1.3.3.3.cmml">=</mo><mrow id="S3.E7.m1.3.3.2" xref="S3.E7.m1.3.3.2.cmml"><mi id="S3.E7.m1.3.3.2.3" xref="S3.E7.m1.3.3.2.3.cmml">softmax</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.3.3.2.2" xref="S3.E7.m1.3.3.2.2.cmml">â€‹</mo><mrow id="S3.E7.m1.3.3.2.1.1" xref="S3.E7.m1.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.3.3.2.1.1.2" xref="S3.E7.m1.3.3.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.3.3.2.1.1.1" xref="S3.E7.m1.3.3.2.1.1.1.cmml"><msup id="S3.E7.m1.3.3.2.1.1.1.3" xref="S3.E7.m1.3.3.2.1.1.1.3.cmml"><mrow id="S3.E7.m1.3.3.2.1.1.1.3.2.2" xref="S3.E7.m1.3.3.2.1.1.1.3.cmml"><mo stretchy="false" id="S3.E7.m1.3.3.2.1.1.1.3.2.2.1" xref="S3.E7.m1.3.3.2.1.1.1.3.cmml">(</mo><mi id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">ğ‘»</mi><mo stretchy="false" id="S3.E7.m1.3.3.2.1.1.1.3.2.2.2" xref="S3.E7.m1.3.3.2.1.1.1.3.cmml">)</mo></mrow><mi id="S3.E7.m1.3.3.2.1.1.1.3.3" xref="S3.E7.m1.3.3.2.1.1.1.3.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E7.m1.3.3.2.1.1.1.2" xref="S3.E7.m1.3.3.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E7.m1.3.3.2.1.1.1.1.1" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.3.3.2.1.1.1.1.1.2" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.3.3.2.1.1.1.1.1.1" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.3.3.2.1.1.1.1.1.1.2" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.2.cmml">ğ’‰</mi><mo id="S3.E7.m1.3.3.2.1.1.1.1.1.1.1" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.3.3.2.1.1.1.1.1.1.3" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.3.cmml">ğ’“</mi></mrow><mo stretchy="false" id="S3.E7.m1.3.3.2.1.1.1.1.1.3" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E7.m1.3.3.2.1.1.3" xref="S3.E7.m1.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.3b"><apply id="S3.E7.m1.3.3.cmml" xref="S3.E7.m1.3.3"><eq id="S3.E7.m1.3.3.3.cmml" xref="S3.E7.m1.3.3.3"></eq><apply id="S3.E7.m1.2.2.1.cmml" xref="S3.E7.m1.2.2.1"><times id="S3.E7.m1.2.2.1.2.cmml" xref="S3.E7.m1.2.2.1.2"></times><ci id="S3.E7.m1.2.2.1.3.cmml" xref="S3.E7.m1.2.2.1.3">ğ‘ƒ</ci><apply id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1">superscript</csymbol><ci id="S3.E7.m1.2.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2">ğ’•</ci><plus id="S3.E7.m1.2.2.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3"></plus></apply></apply><apply id="S3.E7.m1.3.3.2.cmml" xref="S3.E7.m1.3.3.2"><times id="S3.E7.m1.3.3.2.2.cmml" xref="S3.E7.m1.3.3.2.2"></times><ci id="S3.E7.m1.3.3.2.3.cmml" xref="S3.E7.m1.3.3.2.3">softmax</ci><apply id="S3.E7.m1.3.3.2.1.1.1.cmml" xref="S3.E7.m1.3.3.2.1.1"><times id="S3.E7.m1.3.3.2.1.1.1.2.cmml" xref="S3.E7.m1.3.3.2.1.1.1.2"></times><apply id="S3.E7.m1.3.3.2.1.1.1.3.cmml" xref="S3.E7.m1.3.3.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.2.1.1.1.3.1.cmml" xref="S3.E7.m1.3.3.2.1.1.1.3">superscript</csymbol><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">ğ‘»</ci><ci id="S3.E7.m1.3.3.2.1.1.1.3.3.cmml" xref="S3.E7.m1.3.3.2.1.1.1.3.3">ğ‘‡</ci></apply><apply id="S3.E7.m1.3.3.2.1.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.2.1.1.1.1.1"><plus id="S3.E7.m1.3.3.2.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.1"></plus><ci id="S3.E7.m1.3.3.2.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.2">ğ’‰</ci><ci id="S3.E7.m1.3.3.2.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.3.3.2.1.1.1.1.1.1.3">ğ’“</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.3c">P(\bm{t}^{+})={\rm softmax}((\bm{T})^{T}(\bm{h}+\bm{r}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Sem}=-{\rm log}(P(\bm{t}^{+}))" display="block"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><msub id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.1.1.3.2" xref="S3.E8.m1.1.1.3.2.cmml">â„’</mi><mi id="S3.E8.m1.1.1.3.3" xref="S3.E8.m1.1.1.3.3.cmml">Sem</mi></msub><mo id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">=</mo><mrow id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml"><mo id="S3.E8.m1.1.1.1a" xref="S3.E8.m1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E8.m1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.3.cmml">log</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E8.m1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ’•</mi><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">+</mo></msup><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"></eq><apply id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.3.2">â„’</ci><ci id="S3.E8.m1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.3.3">Sem</ci></apply><apply id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><minus id="S3.E8.m1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1"></minus><apply id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1"><times id="S3.E8.m1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.2"></times><ci id="S3.E8.m1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.3">log</ci><apply id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1"><times id="S3.E8.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.2"></times><ci id="S3.E8.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.3">ğ‘ƒ</ci><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.2">ğ’•</ci><plus id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.3"></plus></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\mathcal{L}_{\rm Sem}=-{\rm log}(P(\bm{t}^{+}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.3" class="ltx_p">where <math id="S3.SS2.p4.2.m1.1" class="ltx_Math" alttext="P(\bm{t}^{+})" display="inline"><semantics id="S3.SS2.p4.2.m1.1a"><mrow id="S3.SS2.p4.2.m1.1.1" xref="S3.SS2.p4.2.m1.1.1.cmml"><mi id="S3.SS2.p4.2.m1.1.1.3" xref="S3.SS2.p4.2.m1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m1.1.1.2" xref="S3.SS2.p4.2.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS2.p4.2.m1.1.1.1.1" xref="S3.SS2.p4.2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p4.2.m1.1.1.1.1.2" xref="S3.SS2.p4.2.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS2.p4.2.m1.1.1.1.1.1" xref="S3.SS2.p4.2.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.2.m1.1.1.1.1.1.2" xref="S3.SS2.p4.2.m1.1.1.1.1.1.2.cmml">ğ’•</mi><mo id="S3.SS2.p4.2.m1.1.1.1.1.1.3" xref="S3.SS2.p4.2.m1.1.1.1.1.1.3.cmml">+</mo></msup><mo stretchy="false" id="S3.SS2.p4.2.m1.1.1.1.1.3" xref="S3.SS2.p4.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m1.1b"><apply id="S3.SS2.p4.2.m1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1"><times id="S3.SS2.p4.2.m1.1.1.2.cmml" xref="S3.SS2.p4.2.m1.1.1.2"></times><ci id="S3.SS2.p4.2.m1.1.1.3.cmml" xref="S3.SS2.p4.2.m1.1.1.3">ğ‘ƒ</ci><apply id="S3.SS2.p4.2.m1.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p4.2.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1.1.2">ğ’•</ci><plus id="S3.SS2.p4.2.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.2.m1.1.1.1.1.1.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m1.1c">P(\bm{t}^{+})</annotation></semantics></math> is the predicted probability of ground-truth tail <math id="S3.SS2.p4.3.m2.1" class="ltx_Math" alttext="\bm{t}^{+}" display="inline"><semantics id="S3.SS2.p4.3.m2.1a"><msup id="S3.SS2.p4.3.m2.1.1" xref="S3.SS2.p4.3.m2.1.1.cmml"><mi id="S3.SS2.p4.3.m2.1.1.2" xref="S3.SS2.p4.3.m2.1.1.2.cmml">ğ’•</mi><mo id="S3.SS2.p4.3.m2.1.1.3" xref="S3.SS2.p4.3.m2.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m2.1b"><apply id="S3.SS2.p4.3.m2.1.1.cmml" xref="S3.SS2.p4.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m2.1.1.1.cmml" xref="S3.SS2.p4.3.m2.1.1">superscript</csymbol><ci id="S3.SS2.p4.3.m2.1.1.2.cmml" xref="S3.SS2.p4.3.m2.1.1.2">ğ’•</ci><plus id="S3.SS2.p4.3.m2.1.1.3.cmml" xref="S3.SS2.p4.3.m2.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m2.1c">\bm{t}^{+}</annotation></semantics></math>. In conclusion, our final loss is defined as:</p>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1.1" class="ltx_Math" alttext="\mathcal{L}=\mathcal{L}_{\rm TransE}+\mathcal{L}_{\rm Tri}+\mathcal{L}_{\rm Sem}" display="block"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.1.1.2" xref="S3.E9.m1.1.1.2.cmml">â„’</mi><mo id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.cmml">=</mo><mrow id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3.cmml"><msub id="S3.E9.m1.1.1.3.2" xref="S3.E9.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.1.1.3.2.2" xref="S3.E9.m1.1.1.3.2.2.cmml">â„’</mi><mi id="S3.E9.m1.1.1.3.2.3" xref="S3.E9.m1.1.1.3.2.3.cmml">TransE</mi></msub><mo id="S3.E9.m1.1.1.3.1" xref="S3.E9.m1.1.1.3.1.cmml">+</mo><msub id="S3.E9.m1.1.1.3.3" xref="S3.E9.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.1.1.3.3.2" xref="S3.E9.m1.1.1.3.3.2.cmml">â„’</mi><mi id="S3.E9.m1.1.1.3.3.3" xref="S3.E9.m1.1.1.3.3.3.cmml">Tri</mi></msub><mo id="S3.E9.m1.1.1.3.1a" xref="S3.E9.m1.1.1.3.1.cmml">+</mo><msub id="S3.E9.m1.1.1.3.4" xref="S3.E9.m1.1.1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.1.1.3.4.2" xref="S3.E9.m1.1.1.3.4.2.cmml">â„’</mi><mi id="S3.E9.m1.1.1.3.4.3" xref="S3.E9.m1.1.1.3.4.3.cmml">Sem</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1"><eq id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"></eq><ci id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1.2">â„’</ci><apply id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3"><plus id="S3.E9.m1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.3.1"></plus><apply id="S3.E9.m1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.3.2.1.cmml" xref="S3.E9.m1.1.1.3.2">subscript</csymbol><ci id="S3.E9.m1.1.1.3.2.2.cmml" xref="S3.E9.m1.1.1.3.2.2">â„’</ci><ci id="S3.E9.m1.1.1.3.2.3.cmml" xref="S3.E9.m1.1.1.3.2.3">TransE</ci></apply><apply id="S3.E9.m1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.3.3.1.cmml" xref="S3.E9.m1.1.1.3.3">subscript</csymbol><ci id="S3.E9.m1.1.1.3.3.2.cmml" xref="S3.E9.m1.1.1.3.3.2">â„’</ci><ci id="S3.E9.m1.1.1.3.3.3.cmml" xref="S3.E9.m1.1.1.3.3.3">Tri</ci></apply><apply id="S3.E9.m1.1.1.3.4.cmml" xref="S3.E9.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.3.4.1.cmml" xref="S3.E9.m1.1.1.3.4">subscript</csymbol><ci id="S3.E9.m1.1.1.3.4.2.cmml" xref="S3.E9.m1.1.1.3.4.2">â„’</ci><ci id="S3.E9.m1.1.1.3.4.3.cmml" xref="S3.E9.m1.1.1.3.4.3">Sem</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\mathcal{L}=\mathcal{L}_{\rm TransE}+\mathcal{L}_{\rm Tri}+\mathcal{L}_{\rm Sem}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Knowledge Accumulation and Prediction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We adopt a two-stage training strategy to accumulate multimodal knowledge progressively: (1) pre-training on the VQA 2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to accumulate basic visual-dominant knowledge and then (2) fine-tuning on the training data of downstream KB-VQA task to accumulate more complex domain-specific multimodal knowledge.
All questions in VQA 2.0 are divided into three categories: <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">Yes/No</span>, <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">Number</span>, and <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">Other</span>.
Since answers in the first two categories can not serve as the fact knowledge,
we only keep <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">Other</span> type questions for pre-training.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.5" class="ltx_p">In the inference stage, we regard the answer prediction as a multimodal knowledge graph completion problem. Given an image and a question, we feed them into the network and obtain the embeddings of the head entity <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\bm{h}_{inf}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">ğ’‰</mi><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1a" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.1.m1.1.1.3.4" xref="S3.SS3.p2.1.m1.1.1.3.4.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ’‰</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><times id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">ğ‘›</ci><ci id="S3.SS3.p2.1.m1.1.1.3.4.cmml" xref="S3.SS3.p2.1.m1.1.1.3.4">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\bm{h}_{inf}</annotation></semantics></math> and the relation <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\bm{r}_{inf}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">ğ’“</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.3.1a" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.2.m2.1.1.3.4" xref="S3.SS3.p2.2.m2.1.1.3.4.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ğ’“</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><times id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">ğ‘–</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ğ‘›</ci><ci id="S3.SS3.p2.2.m2.1.1.3.4.cmml" xref="S3.SS3.p2.2.m2.1.1.3.4">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\bm{r}_{inf}</annotation></semantics></math>. We compute the distance between <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\bm{h}_{inf}+\bm{r}_{inf}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><msub id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.cmml">ğ’‰</mi><mrow id="S3.SS3.p2.3.m3.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.2.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.3.2" xref="S3.SS3.p2.3.m3.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.2.3.1" xref="S3.SS3.p2.3.m3.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.3.m3.1.1.2.3.3" xref="S3.SS3.p2.3.m3.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.2.3.1a" xref="S3.SS3.p2.3.m3.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.3.m3.1.1.2.3.4" xref="S3.SS3.p2.3.m3.1.1.2.3.4.cmml">f</mi></mrow></msub><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">+</mo><msub id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.cmml">ğ’“</mi><mrow id="S3.SS3.p2.3.m3.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.1" xref="S3.SS3.p2.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.1a" xref="S3.SS3.p2.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.4" xref="S3.SS3.p2.3.m3.1.1.3.3.4.cmml">f</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><plus id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></plus><apply id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2">ğ’‰</ci><apply id="S3.SS3.p2.3.m3.1.1.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3"><times id="S3.SS3.p2.3.m3.1.1.2.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.2.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.2">ğ‘–</ci><ci id="S3.SS3.p2.3.m3.1.1.2.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.3">ğ‘›</ci><ci id="S3.SS3.p2.3.m3.1.1.2.3.4.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.4">ğ‘“</ci></apply></apply><apply id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2">ğ’“</ci><apply id="S3.SS3.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3"><times id="S3.SS3.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2">ğ‘–</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3">ğ‘›</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.4.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.4">ğ‘“</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\bm{h}_{inf}+\bm{r}_{inf}</annotation></semantics></math> and each tail entity <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\bm{t}_{i}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">ğ’•</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">ğ’•</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\bm{t}_{i}</annotation></semantics></math> in the look-up table <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\bm{T}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">ğ‘»</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">ğ‘»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\bm{T}</annotation></semantics></math> and select the tail entity with the minimum distance as:</p>
<table id="S3.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E10.m1.2" class="ltx_Math" alttext="\bm{t}_{inf}=\mathop{\arg\min}_{\bm{t}_{i}\in\bm{T}}{\rm d}(\bm{h}_{inf}+\bm{r}_{inf},\bm{t}_{i})" display="block"><semantics id="S3.E10.m1.2a"><mrow id="S3.E10.m1.2.2" xref="S3.E10.m1.2.2.cmml"><msub id="S3.E10.m1.2.2.4" xref="S3.E10.m1.2.2.4.cmml"><mi id="S3.E10.m1.2.2.4.2" xref="S3.E10.m1.2.2.4.2.cmml">ğ’•</mi><mrow id="S3.E10.m1.2.2.4.3" xref="S3.E10.m1.2.2.4.3.cmml"><mi id="S3.E10.m1.2.2.4.3.2" xref="S3.E10.m1.2.2.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.2.2.4.3.1" xref="S3.E10.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.E10.m1.2.2.4.3.3" xref="S3.E10.m1.2.2.4.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.2.2.4.3.1a" xref="S3.E10.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.E10.m1.2.2.4.3.4" xref="S3.E10.m1.2.2.4.3.4.cmml">f</mi></mrow></msub><mo id="S3.E10.m1.2.2.3" xref="S3.E10.m1.2.2.3.cmml">=</mo><mrow id="S3.E10.m1.2.2.2" xref="S3.E10.m1.2.2.2.cmml"><munder id="S3.E10.m1.2.2.2.3" xref="S3.E10.m1.2.2.2.3.cmml"><mrow id="S3.E10.m1.2.2.2.3.2" xref="S3.E10.m1.2.2.2.3.2.cmml"><mi id="S3.E10.m1.2.2.2.3.2.1" xref="S3.E10.m1.2.2.2.3.2.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E10.m1.2.2.2.3.2a" xref="S3.E10.m1.2.2.2.3.2.cmml">â¡</mo><mi id="S3.E10.m1.2.2.2.3.2.2" xref="S3.E10.m1.2.2.2.3.2.2.cmml">min</mi></mrow><mrow id="S3.E10.m1.2.2.2.3.3" xref="S3.E10.m1.2.2.2.3.3.cmml"><msub id="S3.E10.m1.2.2.2.3.3.2" xref="S3.E10.m1.2.2.2.3.3.2.cmml"><mi id="S3.E10.m1.2.2.2.3.3.2.2" xref="S3.E10.m1.2.2.2.3.3.2.2.cmml">ğ’•</mi><mi id="S3.E10.m1.2.2.2.3.3.2.3" xref="S3.E10.m1.2.2.2.3.3.2.3.cmml">i</mi></msub><mo id="S3.E10.m1.2.2.2.3.3.1" xref="S3.E10.m1.2.2.2.3.3.1.cmml">âˆˆ</mo><mi id="S3.E10.m1.2.2.2.3.3.3" xref="S3.E10.m1.2.2.2.3.3.3.cmml">ğ‘»</mi></mrow></munder><mrow id="S3.E10.m1.2.2.2.2" xref="S3.E10.m1.2.2.2.2.cmml"><mi mathvariant="normal" id="S3.E10.m1.2.2.2.2.4" xref="S3.E10.m1.2.2.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.2.2.2.2.3" xref="S3.E10.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E10.m1.2.2.2.2.2.2" xref="S3.E10.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E10.m1.2.2.2.2.2.2.3" xref="S3.E10.m1.2.2.2.2.2.3.cmml">(</mo><mrow id="S3.E10.m1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E10.m1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.1.2.2" xref="S3.E10.m1.1.1.1.1.1.1.1.2.2.cmml">ğ’‰</mi><mrow id="S3.E10.m1.1.1.1.1.1.1.1.2.3" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.1.2.3.2" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.1.1.1.1.1.2.3.1" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E10.m1.1.1.1.1.1.1.1.2.3.3" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.1.1.1.1.1.2.3.1a" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E10.m1.1.1.1.1.1.1.1.2.3.4" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.4.cmml">f</mi></mrow></msub><mo id="S3.E10.m1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E10.m1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.1.3.2" xref="S3.E10.m1.1.1.1.1.1.1.1.3.2.cmml">ğ’“</mi><mrow id="S3.E10.m1.1.1.1.1.1.1.1.3.3" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.1.3.3.2" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.1.1.1.1.1.3.3.1" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E10.m1.1.1.1.1.1.1.1.3.3.3" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.1.1.1.1.1.3.3.1a" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E10.m1.1.1.1.1.1.1.1.3.3.4" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.4.cmml">f</mi></mrow></msub></mrow><mo id="S3.E10.m1.2.2.2.2.2.2.4" xref="S3.E10.m1.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E10.m1.2.2.2.2.2.2.2" xref="S3.E10.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E10.m1.2.2.2.2.2.2.2.2" xref="S3.E10.m1.2.2.2.2.2.2.2.2.cmml">ğ’•</mi><mi id="S3.E10.m1.2.2.2.2.2.2.2.3" xref="S3.E10.m1.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E10.m1.2.2.2.2.2.2.5" xref="S3.E10.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.2b"><apply id="S3.E10.m1.2.2.cmml" xref="S3.E10.m1.2.2"><eq id="S3.E10.m1.2.2.3.cmml" xref="S3.E10.m1.2.2.3"></eq><apply id="S3.E10.m1.2.2.4.cmml" xref="S3.E10.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E10.m1.2.2.4.1.cmml" xref="S3.E10.m1.2.2.4">subscript</csymbol><ci id="S3.E10.m1.2.2.4.2.cmml" xref="S3.E10.m1.2.2.4.2">ğ’•</ci><apply id="S3.E10.m1.2.2.4.3.cmml" xref="S3.E10.m1.2.2.4.3"><times id="S3.E10.m1.2.2.4.3.1.cmml" xref="S3.E10.m1.2.2.4.3.1"></times><ci id="S3.E10.m1.2.2.4.3.2.cmml" xref="S3.E10.m1.2.2.4.3.2">ğ‘–</ci><ci id="S3.E10.m1.2.2.4.3.3.cmml" xref="S3.E10.m1.2.2.4.3.3">ğ‘›</ci><ci id="S3.E10.m1.2.2.4.3.4.cmml" xref="S3.E10.m1.2.2.4.3.4">ğ‘“</ci></apply></apply><apply id="S3.E10.m1.2.2.2.cmml" xref="S3.E10.m1.2.2.2"><apply id="S3.E10.m1.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E10.m1.2.2.2.3.1.cmml" xref="S3.E10.m1.2.2.2.3">subscript</csymbol><apply id="S3.E10.m1.2.2.2.3.2.cmml" xref="S3.E10.m1.2.2.2.3.2"><arg id="S3.E10.m1.2.2.2.3.2.1.cmml" xref="S3.E10.m1.2.2.2.3.2.1"></arg><min id="S3.E10.m1.2.2.2.3.2.2.cmml" xref="S3.E10.m1.2.2.2.3.2.2"></min></apply><apply id="S3.E10.m1.2.2.2.3.3.cmml" xref="S3.E10.m1.2.2.2.3.3"><in id="S3.E10.m1.2.2.2.3.3.1.cmml" xref="S3.E10.m1.2.2.2.3.3.1"></in><apply id="S3.E10.m1.2.2.2.3.3.2.cmml" xref="S3.E10.m1.2.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.E10.m1.2.2.2.3.3.2.1.cmml" xref="S3.E10.m1.2.2.2.3.3.2">subscript</csymbol><ci id="S3.E10.m1.2.2.2.3.3.2.2.cmml" xref="S3.E10.m1.2.2.2.3.3.2.2">ğ’•</ci><ci id="S3.E10.m1.2.2.2.3.3.2.3.cmml" xref="S3.E10.m1.2.2.2.3.3.2.3">ğ‘–</ci></apply><ci id="S3.E10.m1.2.2.2.3.3.3.cmml" xref="S3.E10.m1.2.2.2.3.3.3">ğ‘»</ci></apply></apply><apply id="S3.E10.m1.2.2.2.2.cmml" xref="S3.E10.m1.2.2.2.2"><times id="S3.E10.m1.2.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.2.3"></times><ci id="S3.E10.m1.2.2.2.2.4.cmml" xref="S3.E10.m1.2.2.2.2.4">d</ci><interval closure="open" id="S3.E10.m1.2.2.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.2.2.2"><apply id="S3.E10.m1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1"><plus id="S3.E10.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E10.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2.2">ğ’‰</ci><apply id="S3.E10.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3"><times id="S3.E10.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E10.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.2">ğ‘–</ci><ci id="S3.E10.m1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.3">ğ‘›</ci><ci id="S3.E10.m1.1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2.3.4">ğ‘“</ci></apply></apply><apply id="S3.E10.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3.2">ğ’“</ci><apply id="S3.E10.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3"><times id="S3.E10.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E10.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.2">ğ‘–</ci><ci id="S3.E10.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.3">ğ‘›</ci><ci id="S3.E10.m1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3.3.4">ğ‘“</ci></apply></apply></apply><apply id="S3.E10.m1.2.2.2.2.2.2.2.cmml" xref="S3.E10.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E10.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E10.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E10.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E10.m1.2.2.2.2.2.2.2.2">ğ’•</ci><ci id="S3.E10.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.2c">\bm{t}_{inf}=\mathop{\arg\min}_{\bm{t}_{i}\in\bm{T}}{\rm d}(\bm{h}_{inf}+\bm{r}_{inf},\bm{t}_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.6" class="ltx_p">The answer corresponding to the optimal tail entity <math id="S3.SS3.p2.6.m1.1" class="ltx_Math" alttext="\bm{t}_{inf}" display="inline"><semantics id="S3.SS3.p2.6.m1.1a"><msub id="S3.SS3.p2.6.m1.1.1" xref="S3.SS3.p2.6.m1.1.1.cmml"><mi id="S3.SS3.p2.6.m1.1.1.2" xref="S3.SS3.p2.6.m1.1.1.2.cmml">ğ’•</mi><mrow id="S3.SS3.p2.6.m1.1.1.3" xref="S3.SS3.p2.6.m1.1.1.3.cmml"><mi id="S3.SS3.p2.6.m1.1.1.3.2" xref="S3.SS3.p2.6.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m1.1.1.3.1" xref="S3.SS3.p2.6.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.6.m1.1.1.3.3" xref="S3.SS3.p2.6.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m1.1.1.3.1a" xref="S3.SS3.p2.6.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p2.6.m1.1.1.3.4" xref="S3.SS3.p2.6.m1.1.1.3.4.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m1.1b"><apply id="S3.SS3.p2.6.m1.1.1.cmml" xref="S3.SS3.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m1.1.1.1.cmml" xref="S3.SS3.p2.6.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m1.1.1.2.cmml" xref="S3.SS3.p2.6.m1.1.1.2">ğ’•</ci><apply id="S3.SS3.p2.6.m1.1.1.3.cmml" xref="S3.SS3.p2.6.m1.1.1.3"><times id="S3.SS3.p2.6.m1.1.1.3.1.cmml" xref="S3.SS3.p2.6.m1.1.1.3.1"></times><ci id="S3.SS3.p2.6.m1.1.1.3.2.cmml" xref="S3.SS3.p2.6.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS3.p2.6.m1.1.1.3.3.cmml" xref="S3.SS3.p2.6.m1.1.1.3.3">ğ‘›</ci><ci id="S3.SS3.p2.6.m1.1.1.3.4.cmml" xref="S3.SS3.p2.6.m1.1.1.3.4">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m1.1c">\bm{t}_{inf}</annotation></semantics></math> is selected as the predicted answer.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" rowspan="3"><span id="S4.T2.2.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" colspan="7">KB-not-related</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" colspan="5">KB-related</th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" rowspan="3"><span id="S4.T2.2.1.1.4.1" class="ltx_text">Overall</span></th>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" colspan="3">one-step</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" colspan="4">two-step</th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">one-step</th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;" colspan="4">two-step</th>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<th id="S4.T2.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0</th>
<th id="S4.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">1</th>
<th id="S4.T2.2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">2</th>
<th id="S4.T2.2.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">3</th>
<th id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">4</th>
<th id="S4.T2.2.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">5</th>
<th id="S4.T2.2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">6</th>
<th id="S4.T2.2.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">2</th>
<th id="S4.T2.2.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">3</th>
<th id="S4.T2.2.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">4</th>
<th id="S4.T2.2.3.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">5</th>
<th id="S4.T2.2.3.3.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.4.1" class="ltx_tr">
<td id="S4.T2.2.4.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">Q-type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S4.T2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">36.19</td>
<td id="S4.T2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">2.78</td>
<td id="S4.T2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">8.21</td>
<td id="S4.T2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">33.18</td>
<td id="S4.T2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">35.97</td>
<td id="S4.T2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">3.66</td>
<td id="S4.T2.2.4.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">8.06</td>
<td id="S4.T2.2.4.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.09</td>
<td id="S4.T2.2.4.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.00</td>
<td id="S4.T2.2.4.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.18</td>
<td id="S4.T2.2.4.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.06</td>
<td id="S4.T2.2.4.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">0.33</td>
<td id="S4.T2.2.4.1.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">8.12</td>
</tr>
<tr id="S4.T2.2.5.2" class="ltx_tr">
<td id="S4.T2.2.5.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S4.T2.2.5.2.2" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">45.98</td>
<td id="S4.T2.2.5.2.3" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">2.79</td>
<td id="S4.T2.2.5.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">2.75</td>
<td id="S4.T2.2.5.2.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">43.26</td>
<td id="S4.T2.2.5.2.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">40.67</td>
<td id="S4.T2.2.5.2.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">2.62</td>
<td id="S4.T2.2.5.2.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">1.72</td>
<td id="S4.T2.2.5.2.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">0.43</td>
<td id="S4.T2.2.5.2.10" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">0.00</td>
<td id="S4.T2.2.5.2.11" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">0.52</td>
<td id="S4.T2.2.5.2.12" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">1.65</td>
<td id="S4.T2.2.5.2.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">0.74</td>
<td id="S4.T2.2.5.2.14" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">8.81</td>
</tr>
<tr id="S4.T2.2.6.3" class="ltx_tr">
<td id="S4.T2.2.6.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">FiLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S4.T2.2.6.3.2" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">52.42</td>
<td id="S4.T2.2.6.3.3" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">21.35</td>
<td id="S4.T2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">18.50</td>
<td id="S4.T2.2.6.3.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">45.23</td>
<td id="S4.T2.2.6.3.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">42.36</td>
<td id="S4.T2.2.6.3.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">21.32</td>
<td id="S4.T2.2.6.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">15.44</td>
<td id="S4.T2.2.6.3.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">6.27</td>
<td id="S4.T2.2.6.3.10" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">5.48</td>
<td id="S4.T2.2.6.3.11" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">4.37</td>
<td id="S4.T2.2.6.3.12" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">4.41</td>
<td id="S4.T2.2.6.3.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">7.19</td>
<td id="S4.T2.2.6.3.14" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">16.89</td>
</tr>
<tr id="S4.T2.2.7.4" class="ltx_tr">
<td id="S4.T2.2.7.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">MFH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S4.T2.2.7.4.2" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">43.74</td>
<td id="S4.T2.2.7.4.3" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">28.28</td>
<td id="S4.T2.2.7.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">27.49</td>
<td id="S4.T2.2.7.4.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">38.71</td>
<td id="S4.T2.2.7.4.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">36.48</td>
<td id="S4.T2.2.7.4.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">20.77</td>
<td id="S4.T2.2.7.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">21.01</td>
<td id="S4.T2.2.7.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">12.97</td>
<td id="S4.T2.2.7.4.10" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">5.10</td>
<td id="S4.T2.2.7.4.11" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">6.05</td>
<td id="S4.T2.2.7.4.12" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">5.02</td>
<td id="S4.T2.2.7.4.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">14.38</td>
<td id="S4.T2.2.7.4.14" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">19.55</td>
</tr>
<tr id="S4.T2.2.8.5" class="ltx_tr">
<td id="S4.T2.2.8.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S4.T2.2.8.5.2" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">56.42</td>
<td id="S4.T2.2.8.5.3" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">29.89</td>
<td id="S4.T2.2.8.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">28.63</td>
<td id="S4.T2.2.8.5.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">49.69</td>
<td id="S4.T2.2.8.5.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">43.87</td>
<td id="S4.T2.2.8.5.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">24.71</td>
<td id="S4.T2.2.8.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">21.28</td>
<td id="S4.T2.2.8.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">11.07</td>
<td id="S4.T2.2.8.5.10" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">8.16</td>
<td id="S4.T2.2.8.5.11" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">7.09</td>
<td id="S4.T2.2.8.5.12" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">5.37</td>
<td id="S4.T2.2.8.5.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">13.97</td>
<td id="S4.T2.2.8.5.14" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">21.85</td>
</tr>
<tr id="S4.T2.2.9.6" class="ltx_tr">
<td id="S4.T2.2.9.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S4.T2.2.9.6.2" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">49.60</td>
<td id="S4.T2.2.9.6.3" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">27.67</td>
<td id="S4.T2.2.9.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">25.76</td>
<td id="S4.T2.2.9.6.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">39.69</td>
<td id="S4.T2.2.9.6.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">37.92</td>
<td id="S4.T2.2.9.6.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">21.22</td>
<td id="S4.T2.2.9.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">18.63</td>
<td id="S4.T2.2.9.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">12.28</td>
<td id="S4.T2.2.9.6.10" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">9.35</td>
<td id="S4.T2.2.9.6.11" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">9.22</td>
<td id="S4.T2.2.9.6.12" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">5.23</td>
<td id="S4.T2.2.9.6.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">13.34</td>
<td id="S4.T2.2.9.6.14" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">20.52</td>
</tr>
<tr id="S4.T2.2.10.7" class="ltx_tr">
<td id="S4.T2.2.10.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">+ knowledge retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S4.T2.2.10.7.2" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">51.32</td>
<td id="S4.T2.2.10.7.3" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">27.14</td>
<td id="S4.T2.2.10.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">25.69</td>
<td id="S4.T2.2.10.7.5" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">41.23</td>
<td id="S4.T2.2.10.7.6" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">38.86</td>
<td id="S4.T2.2.10.7.7" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">23.25</td>
<td id="S4.T2.2.10.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">21.15</td>
<td id="S4.T2.2.10.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">13.59</td>
<td id="S4.T2.2.10.7.10" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.10.7.10.1" class="ltx_text ltx_font_bold">9.84</span></td>
<td id="S4.T2.2.10.7.11" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">9.24</td>
<td id="S4.T2.2.10.7.12" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">5.51</td>
<td id="S4.T2.2.10.7.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.3pt;padding-right:2.3pt;">13.89</td>
<td id="S4.T2.2.10.7.14" class="ltx_td ltx_align_center" style="padding-left:2.3pt;padding-right:2.3pt;">21.30</td>
</tr>
<tr id="S4.T2.2.11.8" class="ltx_tr">
<td id="S4.T2.2.11.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">MuKEA</td>
<td id="S4.T2.2.11.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.2.1" class="ltx_text ltx_font_bold">59.12</span></td>
<td id="S4.T2.2.11.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.3.1" class="ltx_text ltx_font_bold">44.88</span></td>
<td id="S4.T2.2.11.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.4.1" class="ltx_text ltx_font_bold">37.36</span></td>
<td id="S4.T2.2.11.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.5.1" class="ltx_text ltx_font_bold">52.47</span></td>
<td id="S4.T2.2.11.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.6.1" class="ltx_text ltx_font_bold">48.08</span></td>
<td id="S4.T2.2.11.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.7.1" class="ltx_text ltx_font_bold">35.63</span></td>
<td id="S4.T2.2.11.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.8.1" class="ltx_text ltx_font_bold">31.61</span></td>
<td id="S4.T2.2.11.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.9.1" class="ltx_text ltx_font_bold">17.62</span></td>
<td id="S4.T2.2.11.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;">6.14</td>
<td id="S4.T2.2.11.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.11.1" class="ltx_text ltx_font_bold">9.85</span></td>
<td id="S4.T2.2.11.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.12.1" class="ltx_text ltx_font_bold">6.22</span></td>
<td id="S4.T2.2.11.8.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.13.1" class="ltx_text ltx_font_bold">18.28</span></td>
<td id="S4.T2.2.11.8.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.3pt;padding-right:2.3pt;"><span id="S4.T2.2.11.8.14.1" class="ltx_text ltx_font_bold">27.38</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">State-of-the-art comparison on KRVQA dataset. The numbers in the third row mean different types of questions.</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Datasets and Evaluation Metrics.</span>â€ƒWe conduct extensive experiments on two datasets: Outside Knowledge VQA (OK-VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and Knowledge-Routed VQA (KRVQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
OK-VQA contains more than 14,000 questions that cover a variety of 10 knowledge categories. Itâ€™s diverse and challenging since all questions are human-annotated without fixed question templates or knowledge bases, which require exploring a wide range of open-ended knowledge resource. We evaluate the performance by the standard VQA evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. KRVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is to date the largest knowledge-based VQA dataset. It evaluates the multi-step reasoning ability of the models based on external knowledge. We use top-1 accuracy as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for fair comparison.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.3" class="ltx_p"><span id="S4.p2.3.1" class="ltx_text ltx_font_bold">Implementation Details.</span>â€ƒFor all experiments, we train our model with PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
The softmax temperature <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\tau</annotation></semantics></math> in Eq. <a href="#S3.E3" title="Equation 3 â€£ 3.1 Multimodal Knowledge Triplet Extraction â€£ 3 Methodology â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is set to 1.0. We used all the annotated answers in the train set to construct knowledge triplets. For the triplet ranking loss, we treat all samples in a batch with different answers from the positive samples as negative samples, the margin was set to 1.0. Our model is trained by AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> optimizer with 200 epochs, where the batch size is 256 and the learning rate is set to 1 Ã— 10<sup id="S4.p2.3.2" class="ltx_sup"><span id="S4.p2.3.2.1" class="ltx_text ltx_font_italic">-5</span></sup> and 1 Ã— 10<sup id="S4.p2.3.3" class="ltx_sup"><span id="S4.p2.3.3.1" class="ltx_text ltx_font_italic">-4</span></sup> in the pre-training and fine-tuning stage, respectively.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison with State-of-the-Art Methods</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Comparison on OK-VQA:</span>â€ƒTable <a href="#S3.T1" title="Table 1 â€£ 3.2 Knowledge Triplet Representation Learning â€£ 3 Methodology â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the comparison results with state-of-the-art models, including knowledge graph based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, unstructured-knowledge based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, multi-source knowledge based hybrid approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, implicit knowledge based pre-training approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and the multimodal knowledge based approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. Meanwhile, we also compare with traditional VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.3" class="ltx_p">Our model MuKEA consistently outperforms all the existing approaches and is superior to the state-of-the-art model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> remarkably by 3.35<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\%</annotation></semantics></math>. Compared with most models following the â€˜knowledge retrieve and readâ€™ pipeline and referring to fixed knowledge bases, our end-to-end model effectively avoids cascading error while benefits from human-focused diverse multimodal knowledge. Moreover, our model greatly outperforms the pre-trained models by 10<math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\%</annotation></semantics></math> since our model captures the question-centric and information-abstracted multimodal knowledge instead of simple vision and language co-occurrence â€˜knowledgeâ€™ in the pre-training framework. Though KM<sup id="S4.SS1.p2.3.1" class="ltx_sup">4</sup> leverages multimodal knowledge by correlating images with entities in the existing knowledge graph, it still lacks of knowledge with high-order complex relationships and is inferior to MuKEA by 11.27%.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Comparison on KRVQA:</span>â€ƒIn Table <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare MuKEA with traditional VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and the knowledge-based model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. â€˜KB-not-relatedâ€™ questions only require basic visual knowledge while â€˜KB-relatedâ€™ questions need
factual knowledge from knowledge bases.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Our model consistently outperforms existing models and achieves a remarkable boost of 6.08% on the overall metric over the best model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Itâ€™s worth to note that MuKEA obtains 7.81% improvement on average over <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> on the â€˜KB-not-relatedâ€™ questions, which indicates that even the vision-only questions require multimodal commonsense to bridge the low-level visual content and high-level semantics. MuKEA is inferior to some models on two-step reasoning type 3 questions since that the answers of these questions are mostly relations while the accumulated and predicted tail entities of MuKEA are fact entities in most cases.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.7.8.1" class="ltx_tr">
<th id="S4.T3.7.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.8.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T3.7.8.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.8.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.7.9.1" class="ltx_tr">
<td id="S4.T3.7.9.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">1.â€ƒMuKEA (full model)</td>
<td id="S4.T3.7.9.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.9.1.2.1" class="ltx_text ltx_font_bold">42.59</span></td>
</tr>
<tr id="S4.T3.7.10.2" class="ltx_tr">
<td id="S4.T3.7.10.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.10.2.1.1" class="ltx_text ltx_font_bold">Ablation of Loss Function</span></td>
<td id="S4.T3.7.10.2.2" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">2.â€ƒw/o <math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Tri}" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">â„’</mi><mi id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml">Tri</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">â„’</ci><ci id="S4.T3.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3">Tri</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\mathcal{L}_{\rm Tri}</annotation></semantics></math>
</td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">41.35</td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<td id="S4.T3.2.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">3.â€ƒw/o <math id="S4.T3.2.2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Sem}" display="inline"><semantics id="S4.T3.2.2.1.m1.1a"><msub id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T3.2.2.1.m1.1.1.2" xref="S4.T3.2.2.1.m1.1.1.2.cmml">â„’</mi><mi id="S4.T3.2.2.1.m1.1.1.3" xref="S4.T3.2.2.1.m1.1.1.3.cmml">Sem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><apply id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T3.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.1.m1.1.1.2">â„’</ci><ci id="S4.T3.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.1.m1.1.1.3">Sem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">\mathcal{L}_{\rm Sem}</annotation></semantics></math>
</td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">42.06</td>
</tr>
<tr id="S4.T3.4.4" class="ltx_tr">
<td id="S4.T3.4.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">4.â€ƒw/o <math id="S4.T3.3.3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Tri}" display="inline"><semantics id="S4.T3.3.3.1.m1.1a"><msub id="S4.T3.3.3.1.m1.1.1" xref="S4.T3.3.3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.1.m1.1.1.2.cmml">â„’</mi><mi id="S4.T3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.1.m1.1.1.3.cmml">Tri</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.m1.1b"><apply id="S4.T3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T3.3.3.1.m1.1.1.2.cmml" xref="S4.T3.3.3.1.m1.1.1.2">â„’</ci><ci id="S4.T3.3.3.1.m1.1.1.3.cmml" xref="S4.T3.3.3.1.m1.1.1.3">Tri</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.m1.1c">\mathcal{L}_{\rm Tri}</annotation></semantics></math> &amp; <math id="S4.T3.4.4.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Sem}" display="inline"><semantics id="S4.T3.4.4.2.m2.1a"><msub id="S4.T3.4.4.2.m2.1.1" xref="S4.T3.4.4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T3.4.4.2.m2.1.1.2" xref="S4.T3.4.4.2.m2.1.1.2.cmml">â„’</mi><mi id="S4.T3.4.4.2.m2.1.1.3" xref="S4.T3.4.4.2.m2.1.1.3.cmml">Sem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.2.m2.1b"><apply id="S4.T3.4.4.2.m2.1.1.cmml" xref="S4.T3.4.4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.2.m2.1.1.1.cmml" xref="S4.T3.4.4.2.m2.1.1">subscript</csymbol><ci id="S4.T3.4.4.2.m2.1.1.2.cmml" xref="S4.T3.4.4.2.m2.1.1.2">â„’</ci><ci id="S4.T3.4.4.2.m2.1.1.3.cmml" xref="S4.T3.4.4.2.m2.1.1.3">Sem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.2.m2.1c">\mathcal{L}_{\rm Sem}</annotation></semantics></math>
</td>
<td id="S4.T3.4.4.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">40.84</td>
</tr>
<tr id="S4.T3.5.5" class="ltx_tr">
<td id="S4.T3.5.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">5.â€ƒw/o <math id="S4.T3.5.5.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm TransE}" display="inline"><semantics id="S4.T3.5.5.1.m1.1a"><msub id="S4.T3.5.5.1.m1.1.1" xref="S4.T3.5.5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T3.5.5.1.m1.1.1.2" xref="S4.T3.5.5.1.m1.1.1.2.cmml">â„’</mi><mi id="S4.T3.5.5.1.m1.1.1.3" xref="S4.T3.5.5.1.m1.1.1.3.cmml">TransE</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.1.m1.1b"><apply id="S4.T3.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1">subscript</csymbol><ci id="S4.T3.5.5.1.m1.1.1.2.cmml" xref="S4.T3.5.5.1.m1.1.1.2">â„’</ci><ci id="S4.T3.5.5.1.m1.1.1.3.cmml" xref="S4.T3.5.5.1.m1.1.1.3">TransE</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.1.m1.1c">\mathcal{L}_{\rm TransE}</annotation></semantics></math>
</td>
<td id="S4.T3.5.5.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">24.50</td>
</tr>
<tr id="S4.T3.7.11.3" class="ltx_tr">
<td id="S4.T3.7.11.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.11.3.1.1" class="ltx_text ltx_font_bold">Ablation of Triplet Representation</span></td>
<td id="S4.T3.7.11.3.2" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
</tr>
<tr id="S4.T3.7.12.4" class="ltx_tr">
<td id="S4.T3.7.12.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">6.â€ƒhead entity w/ soft-attention</td>
<td id="S4.T3.7.12.4.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">40.67</td>
</tr>
<tr id="S4.T3.7.13.5" class="ltx_tr">
<td id="S4.T3.7.13.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">7.â€ƒrelation w/ self-attention</td>
<td id="S4.T3.7.13.5.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">40.79</td>
</tr>
<tr id="S4.T3.7.14.6" class="ltx_tr">
<td id="S4.T3.7.14.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">8.â€ƒtail entity w/ GloVe</td>
<td id="S4.T3.7.14.6.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">41.42</td>
</tr>
<tr id="S4.T3.7.15.7" class="ltx_tr">
<td id="S4.T3.7.15.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.15.7.1.1" class="ltx_text ltx_font_bold">Ablation of Triplet Structure</span></td>
<td id="S4.T3.7.15.7.2" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
</tr>
<tr id="S4.T3.6.6" class="ltx_tr">
<td id="S4.T3.6.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">9.â€ƒw/o <math id="S4.T3.6.6.1.m1.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S4.T3.6.6.1.m1.1a"><mi id="S4.T3.6.6.1.m1.1.1" xref="S4.T3.6.6.1.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.1.m1.1b"><ci id="S4.T3.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.1.m1.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.1.m1.1c">h</annotation></semantics></math>
</td>
<td id="S4.T3.6.6.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">39.83</td>
</tr>
<tr id="S4.T3.7.7" class="ltx_tr">
<td id="S4.T3.7.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">10.â€… w/o <math id="S4.T3.7.7.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.T3.7.7.1.m1.1a"><mi id="S4.T3.7.7.1.m1.1.1" xref="S4.T3.7.7.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.1.m1.1b"><ci id="S4.T3.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.1.m1.1c">r</annotation></semantics></math>
</td>
<td id="S4.T3.7.7.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">39.40</td>
</tr>
<tr id="S4.T3.7.16.8" class="ltx_tr">
<td id="S4.T3.7.16.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.16.8.1.1" class="ltx_text ltx_font_bold">Ablation of Knowledge Source</span></td>
<td id="S4.T3.7.16.8.2" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
</tr>
<tr id="S4.T3.7.17.9" class="ltx_tr">
<td id="S4.T3.7.17.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">11.â€… w/o VQA 2.0 knowledge</td>
<td id="S4.T3.7.17.9.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">36.35</td>
</tr>
<tr id="S4.T3.7.18.10" class="ltx_tr">
<td id="S4.T3.7.18.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">12.â€… w/o OK-VQA knowledge</td>
<td id="S4.T3.7.18.10.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">27.20</td>
</tr>
<tr id="S4.T3.7.19.11" class="ltx_tr">
<td id="S4.T3.7.19.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T3.7.19.11.1.1" class="ltx_text ltx_font_bold">Ablation of Pre-training Knowledge</span></td>
<td id="S4.T3.7.19.11.2" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
</tr>
<tr id="S4.T3.7.20.12" class="ltx_tr">
<td id="S4.T3.7.20.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">13.â€… w/o LXMERT pre-training</td>
<td id="S4.T3.7.20.12.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:8.5pt;padding-right:8.5pt;">33.52</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.9.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.10.2" class="ltx_text" style="font-size:90%;">Ablation of key components in MuKEA on OK-VQA.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.14" class="ltx_p">In Table <a href="#S4.T3" title="Table 3 â€£ 4.1 Comparison with State-of-the-Art Methods â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we evaluate the contribution of knowledge learning losses, knowledge extraction schema, and knowledge accumulation strategy in MuKEA on the OK-VQA dataset. (1) In models â€˜2-5â€™, we evaluate the <span id="S4.SS2.p1.14.1" class="ltx_text ltx_font_bold">effect of each loss function</span> on the performance. The accuracy of removing <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Tri}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">Tri</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">â„’</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">Tri</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathcal{L}_{\rm Tri}</annotation></semantics></math> and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Sem}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">Sem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">â„’</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">Sem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\mathcal{L}_{\rm Sem}</annotation></semantics></math> respectively decreases by 1.24<math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mo id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><csymbol cd="latexml" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\%</annotation></semantics></math> and 0.53<math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mo id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><csymbol cd="latexml" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\%</annotation></semantics></math> while removing <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{\rm TransE}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><msub id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">TransE</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">â„’</ci><ci id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">TransE</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">\mathcal{L}_{\rm TransE}</annotation></semantics></math> results in a significant decrease in model â€˜5â€™. Because <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{L}_{\rm TransE}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><msub id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml">TransE</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">â„’</ci><ci id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3">TransE</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">\mathcal{L}_{\rm TransE}</annotation></semantics></math> preserves the embedding structure of the whole triplets in our multimodal knowledge base, which has greater impact than <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Tri}" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><msub id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.7.m7.1.1.3" xref="S4.SS2.p1.7.m7.1.1.3.cmml">Tri</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2">â„’</ci><ci id="S4.SS2.p1.7.m7.1.1.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3">Tri</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">\mathcal{L}_{\rm Tri}</annotation></semantics></math> and <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Sem}" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><msub id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.8.m8.1.1.2" xref="S4.SS2.p1.8.m8.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.8.m8.1.1.3" xref="S4.SS2.p1.8.m8.1.1.3.cmml">Sem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><apply id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.8.m8.1.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.p1.8.m8.1.1.2.cmml" xref="S4.SS2.p1.8.m8.1.1.2">â„’</ci><ci id="S4.SS2.p1.8.m8.1.1.3.cmml" xref="S4.SS2.p1.8.m8.1.1.3">Sem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">\mathcal{L}_{\rm Sem}</annotation></semantics></math>. Model â€˜4â€™ results in a further decrease compared with â€˜2â€™ and â€˜3â€™, which indicates the complementary benefits of <math id="S4.SS2.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Tri}" display="inline"><semantics id="S4.SS2.p1.9.m9.1a"><msub id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.9.m9.1.1.3" xref="S4.SS2.p1.9.m9.1.1.3.cmml">Tri</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m9.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2">â„’</ci><ci id="S4.SS2.p1.9.m9.1.1.3.cmml" xref="S4.SS2.p1.9.m9.1.1.3">Tri</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">\mathcal{L}_{\rm Tri}</annotation></semantics></math> and <math id="S4.SS2.p1.10.m10.1" class="ltx_Math" alttext="\mathcal{L}_{\rm Sem}" display="inline"><semantics id="S4.SS2.p1.10.m10.1a"><msub id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.10.m10.1.1.2" xref="S4.SS2.p1.10.m10.1.1.2.cmml">â„’</mi><mi id="S4.SS2.p1.10.m10.1.1.3" xref="S4.SS2.p1.10.m10.1.1.3.cmml">Sem</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><apply id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.10.m10.1.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.p1.10.m10.1.1.2.cmml" xref="S4.SS2.p1.10.m10.1.1.2">â„’</ci><ci id="S4.SS2.p1.10.m10.1.1.3.cmml" xref="S4.SS2.p1.10.m10.1.1.3">Sem</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">\mathcal{L}_{\rm Sem}</annotation></semantics></math>. (2) In models â€˜6-8â€™, we assess the <span id="S4.SS2.p1.14.2" class="ltx_text ltx_font_bold">influence of triplet extraction methods</span>. For head entity extraction, we replace Gumbel-Softmax with soft attention in â€˜6â€™ and the performance drops by 1.92<math id="S4.SS2.p1.11.m11.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.11.m11.1a"><mo id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><csymbol cd="latexml" id="S4.SS2.p1.11.m11.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">\%</annotation></semantics></math>. Itâ€™s because that the head entity derived from LXMERT already contains object-centric contextual semantics for complex questions while directly fusing object features together introduces unexpected noise. Similarly, we apply self-attention over all the output tokens of LXMEART to represent the relation in â€˜7â€™ and the accuracy decreases by 1.80<math id="S4.SS2.p1.12.m12.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.12.m12.1a"><mo id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><csymbol cd="latexml" id="S4.SS2.p1.12.m12.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">\%</annotation></semantics></math> compared with using [CLS] token, which benefits from the the pre-training classification task to contain highly-correlated multimodal information. Furthermore, we utilize GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to represent the tail entity in â€˜8â€™, resulting in a 1.17<math id="S4.SS2.p1.13.m13.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.13.m13.1a"><mo id="S4.SS2.p1.13.m13.1.1" xref="S4.SS2.p1.13.m13.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.13.m13.1b"><csymbol cd="latexml" id="S4.SS2.p1.13.m13.1.1.cmml" xref="S4.SS2.p1.13.m13.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.13.m13.1c">\%</annotation></semantics></math> accuracy drop because of the heterogeneous gap between fixed word embeddings and multimodal triplet representations. (3) In models â€˜9-10â€™, we prove the <span id="S4.SS2.p1.14.3" class="ltx_text ltx_font_bold">importance of triplet structure</span>. We remove the head entity and the tail entity respectively. The performance drops 2.76% and 3.19% accordingly, which proves the effectiveness of our triplet-based knowledge organization structure. (4) In models â€˜11-12â€™, we prove the <span id="S4.SS2.p1.14.4" class="ltx_text ltx_font_bold">importance of pre-training and fine-tuning strategy for knowledge accumulation</span>. Itâ€™s obvious that without either of the two process, the performance decreases remarkably. Though the basic knowledge in VQA 2.0 is less influential than the domain-specific knowledge in OK-VQA, the two works together in a curriculum learning mode achieves the best performance. (5) In model â€˜13â€™, we further test the <span id="S4.SS2.p1.14.5" class="ltx_text ltx_font_bold">influence of prior knowledge accumulated in the pre-trained LXMERT</span>. The accuracy drops 9.07<math id="S4.SS2.p1.14.m14.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.14.m14.1a"><mo id="S4.SS2.p1.14.m14.1.1" xref="S4.SS2.p1.14.m14.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.14.m14.1b"><csymbol cd="latexml" id="S4.SS2.p1.14.m14.1.1.cmml" xref="S4.SS2.p1.14.m14.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.14.m14.1c">\%</annotation></semantics></math> without pre-training since both the head entity and the relation representations rely on the contextual information from the pre-trained knowledge.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.3.4.1" class="ltx_tr">
<th id="S4.T4.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S4.T4.3.4.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T4.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="3"><span id="S4.T4.3.4.1.2.1" class="ltx_text ltx_font_bold">Failure subset</span></th>
</tr>
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">MUTAN + AN<sup id="S4.T4.1.1.1.1.1" class="ltx_sup"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_medium">âˆ—</span></sup></span></td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold">Mucko<sup id="S4.T4.2.2.2.1.1" class="ltx_sup"><span id="S4.T4.2.2.2.1.1.1" class="ltx_text ltx_font_medium">âˆ—</span></sup></span></td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.3.3.3.1" class="ltx_text ltx_font_bold">KRISP<sup id="S4.T4.3.3.3.1.1" class="ltx_sup"><span id="S4.T4.3.3.3.1.1.1" class="ltx_text ltx_font_medium">âˆ—</span></sup></span></td>
</tr>
<tr id="S4.T4.3.5.2" class="ltx_tr">
<th id="S4.T4.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA</th>
<td id="S4.T4.3.5.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">40.09</td>
<td id="S4.T4.3.5.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">40.06</td>
<td id="S4.T4.3.5.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">40.46</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T4.6" class="ltx_p ltx_figure_panel ltx_align_center">(a)
<br class="ltx_break">

<span id="S4.T4.6.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T4.6.3.4.1" class="ltx_tr">
<span id="S4.T4.6.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.6.3.4.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
<span id="S4.T4.6.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.6.3.4.1.2.1" class="ltx_text ltx_font_bold">Failure subset</span></span></span>
<span id="S4.T4.6.3.5.2" class="ltx_tr">
<span id="S4.T4.6.3.5.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T4.6.3.5.2.1.1" class="ltx_text ltx_font_bold">MuKEA</span></span></span>
<span id="S4.T4.4.1.1" class="ltx_tr">
<span id="S4.T4.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">MUTAN + AN<sup id="S4.T4.4.1.1.1.1" class="ltx_sup">âˆ—</sup></span>
<span id="S4.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">26.45</span></span>
<span id="S4.T4.5.2.2" class="ltx_tr">
<span id="S4.T4.5.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Mucko<sup id="S4.T4.5.2.2.1.1" class="ltx_sup">âˆ—</sup></span>
<span id="S4.T4.5.2.2.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">27.68</span></span>
<span id="S4.T4.6.3.3" class="ltx_tr">
<span id="S4.T4.6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">KRISP<sup id="S4.T4.6.3.3.1.1" class="ltx_sup">âˆ—</sup></span>
<span id="S4.T4.6.3.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">27.68</span></span>
</span>
</span>

<br class="ltx_break">(b)
<br class="ltx_break"></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.8.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.9.2" class="ltx_text" style="font-size:90%;">MuKEA accuracy on the failure subset of KB-based models (a) and vice versa (b). * indicates the model is re-implemented.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Knowledge Complementary Analysis</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To prove the complementary benefits of our multimodal knowledge with existing knowledge bases, we conduct two experiments on the OK-VQA dataset: (1) performance of MuKEA and existing models on mutual failure cases, and (2) performance of ensemble models of MuKEA and existing models. Here we test on three typical KB-based models: MUTAN + AN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> on unstructured Wikipedia, Mucko <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> on structured ConceptNet, and KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> on multiple knowledge bases. We re-implemented these models for fair comparison on the same subset.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 â€£ 4.2 Ablation Study â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performance of MuKEA on the failure OK-VQA test subset of the above three models and vice versa. MuKEA consistently achieves over 40% accuracy on all the failure cases of the KB-based models (Table <a href="#S4.T4" title="Table 4 â€£ 4.2 Ablation Study â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a)). Meanwhile, KB-based models obtain over 26% accuracy on questions difficult for MuKEA (Table <a href="#S4.T4" title="Table 4 â€£ 4.2 Ablation Study â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(b)). It proves that multimodal knowledge and existing KB knowledge respectively covers different types of open-ended questions.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.5" class="ltx_p">We further assemble MuKEA with three models respectively: if the difference of the top 2 minimum distances predicted by Eq. <a href="#S3.E10" title="Equation 10 â€£ 3.3 Knowledge Accumulation and Prediction â€£ 3 Methodology â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> is larger than a threshold <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mi id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">m</annotation></semantics></math> (<math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="m=0.07" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">m</mi><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">0.07</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><eq id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1"></eq><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">ğ‘š</ci><cn type="float" id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">0.07</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">m=0.07</annotation></semantics></math>), we select the predicted answer of MuKEA, otherwise, selecting the other. In Table <a href="#S4.T5" title="Table 5 â€£ 4.3 Knowledge Complementary Analysis â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the baseline models are respectively improved by 9.96<math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mo id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><csymbol cd="latexml" id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">\%</annotation></semantics></math>, 8.80<math id="S4.SS3.p3.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mo id="S4.SS3.p3.4.m4.1.1" xref="S4.SS3.p3.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m4.1b"><csymbol cd="latexml" id="S4.SS3.p3.4.m4.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">\%</annotation></semantics></math> and 5.73<math id="S4.SS3.p3.5.m5.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p3.5.m5.1a"><mo id="S4.SS3.p3.5.m5.1.1" xref="S4.SS3.p3.5.m5.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.m5.1b"><csymbol cd="latexml" id="S4.SS3.p3.5.m5.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m5.1c">\%</annotation></semantics></math> after model ensemble. We also present the oracle setting that takes the accurate prediction from either of the models as the answer. The oracle performance obtains significant improvement, which further proves the complementary benefits of multimodal knowledge and existing knowledge bases.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2203.09138/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="202" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.5.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.1" class="ltx_text" style="font-size:90%;">Visualization of the predicted answers and supporting knowledge of KRISP (green) and MuKEA (pink). For MuKEA, the red box in the image shows the head entity (<math id="S4.F3.3.1.m1.1" class="ltx_Math" alttext="\alpha_{i}" display="inline"><semantics id="S4.F3.3.1.m1.1b"><msub id="S4.F3.3.1.m1.1.1" xref="S4.F3.3.1.m1.1.1.cmml"><mi id="S4.F3.3.1.m1.1.1.2" xref="S4.F3.3.1.m1.1.1.2.cmml">Î±</mi><mi id="S4.F3.3.1.m1.1.1.3" xref="S4.F3.3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.3.1.m1.1c"><apply id="S4.F3.3.1.m1.1.1.cmml" xref="S4.F3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.3.1.m1.1.1.1.cmml" xref="S4.F3.3.1.m1.1.1">subscript</csymbol><ci id="S4.F3.3.1.m1.1.1.2.cmml" xref="S4.F3.3.1.m1.1.1.2">ğ›¼</ci><ci id="S4.F3.3.1.m1.1.1.3.cmml" xref="S4.F3.3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.3.1.m1.1d">\alpha_{i}</annotation></semantics></math> in Eq. <a href="#S3.E3" title="Equation 3 â€£ 3.1 Multimodal Knowledge Triplet Extraction â€£ 3 Methodology â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The bottom VQA training sample, which has the nearest relation embedding with the test sample, shows the scenario that accumulates relational knowledge supporting the current inference. The answer is shown in the tail.</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.9.10.1" class="ltx_tr">
<th id="S4.T5.9.10.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.9.10.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T5.9.10.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.9.10.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.9.11.1" class="ltx_tr">
<th id="S4.T5.9.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA</th>
<td id="S4.T5.9.11.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">42.59</td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">MUTAN + AN<sup id="S4.T5.1.1.1.1" class="ltx_sup">âˆ—</sup>
</th>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">25.43</td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<th id="S4.T5.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA + (MUTAN + AN<sup id="S4.T5.2.2.1.1" class="ltx_sup">âˆ—</sup>)</th>
<td id="S4.T5.2.2.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">35.39</td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<th id="S4.T5.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA + (MUTAN + AN<sup id="S4.T5.3.3.1.1" class="ltx_sup">âˆ—</sup>) oracle</th>
<td id="S4.T5.3.3.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">43.64</td>
</tr>
<tr id="S4.T5.4.4" class="ltx_tr">
<th id="S4.T5.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Mucko<sup id="S4.T5.4.4.1.1" class="ltx_sup">âˆ—</sup>
</th>
<td id="S4.T5.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">27.17</td>
</tr>
<tr id="S4.T5.5.5" class="ltx_tr">
<th id="S4.T5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA + Mucko<sup id="S4.T5.5.5.1.1" class="ltx_sup">âˆ—</sup>
</th>
<td id="S4.T5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">35.97</td>
</tr>
<tr id="S4.T5.6.6" class="ltx_tr">
<th id="S4.T5.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA + Mucko<sup id="S4.T5.6.6.1.1" class="ltx_sup">âˆ—</sup> oracle</th>
<td id="S4.T5.6.6.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">44.84</td>
</tr>
<tr id="S4.T5.7.7" class="ltx_tr">
<th id="S4.T5.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">KRISP<sup id="S4.T5.7.7.1.1" class="ltx_sup">âˆ—</sup>
</th>
<td id="S4.T5.7.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">32.02</td>
</tr>
<tr id="S4.T5.8.8" class="ltx_tr">
<th id="S4.T5.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA + KRISP<sup id="S4.T5.8.8.1.1" class="ltx_sup">âˆ—</sup>
</th>
<td id="S4.T5.8.8.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">37.75</td>
</tr>
<tr id="S4.T5.9.9" class="ltx_tr">
<th id="S4.T5.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA + KRISP<sup id="S4.T5.9.9.1.1" class="ltx_sup">âˆ—</sup> oracle</th>
<td id="S4.T5.9.9.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">47.15</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.11.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.12.2" class="ltx_text" style="font-size:90%;">Performance of model ensemble on OK-VQA.</span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<th id="S4.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.1.2.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
<th id="S4.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T6.1.2.1.3.1" class="ltx_text ltx_font_bold">mAccuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">KRISP<sup id="S4.T6.1.1.1.1" class="ltx_sup">âˆ—</sup>
</th>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">32.31</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">26.91</td>
</tr>
<tr id="S4.T6.1.3.1" class="ltx_tr">
<th id="S4.T6.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">MuKEA</th>
<td id="S4.T6.1.3.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">42.59</td>
<td id="S4.T6.1.3.1.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">35.42</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.4.2" class="ltx_text" style="font-size:90%;">Long-tail analysis on OK-VQA dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Long-Tail Analysis</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To prove the modelâ€™s generalization ability on the rare answers while not overfitting on the â€˜headâ€™ ones, we propose a new unbiased metric mean Accuracy (<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">mAccuracy</span>) to fairly evaluate the performance on the long-tail distributed answers. Inspired by the unbiased metric in scene graph generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, mAccuracy calculates the accuracy for each unique answer separately and average for all the answers. We compare MuKEA with KRISP, which demonstrates its great generalization ability by referring to multiple knowledge sources. In Table <a href="#S4.T6" title="Table 6 â€£ 4.3 Knowledge Complementary Analysis â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our model greatly outperforms KRISP by 8.51% on mAccuracy, which proves the strong generalization ability of the multimodal knowledge on the long-tail knowledge without sacrificing the accuracy of frequent referred knowledge.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Qualitative Analysis</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">From case study in Figure <a href="#S4.F3" title="Figure 3 â€£ 4.3 Knowledge Complementary Analysis â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we conclude that our model is interpretable by visualizing the predicted multimodal knowledge triplets: (1) <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">MuKEA captures instantiated knowledge beneficial for object understanding.</span> The examples in the first row indicate that MuKEA captures the complex knowledge between the object appearance and the object-centric facts. The accumulated knowledge is in the form of an entire triplet (left) or just the inexpressible relation (right). (2) <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_bold">MuKEA contains multi-object involved complex knowledge beneficial for scene understanding.</span> In the second row, MuKEA is capable to correlate the visual content of groups of buildings with the abstract city style â€˜gothicâ€™. (3) <span id="S4.SS5.p1.1.3" class="ltx_text ltx_font_bold">MuKEA avoids the cascading error by directly reasoning on knowledge embeddings.</span> Existing models generally first detect object labels to retrieve relevant knowledge, which introduces unexpected noise with false labels. MuKEA has the advantage of adopting the semantic-rich embeddings to represent the knowledge and reason about the answer in an end-to-end mode.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Limitation Analysis</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">MuKEA fails mostly in the following cases (Figure <a href="#S4.F4" title="Figure 4 â€£ 4.6 Limitation Analysis â€£ 4 Experiments â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>): (1) MuKEA lacks adequate multimodal knowledge, such as the knowledge to distinguish <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_italic">nylon</span> and <span id="S4.SS6.p1.1.2" class="ltx_text ltx_font_italic">canvas</span>,
due to the limited VQA scenarios in the training stage. (2) MuKEA fails in extracting some triplets.
Since the head entities and the relations are extracted in an unsupervised mode, vision-similar content causes attention deviation, such as the vest is incorrectly attended as the <span id="S4.SS6.p1.1.3" class="ltx_text ltx_font_italic">insignia</span>. The above problems need further research in accumulating more comprehensive knowledge and evaluating the triplet extraction quality. We also test the MuKEA on VQA 2.0 with inferior results to some works since that questions in VQA 2.0 mainly rely on visual appearance clues instead of external knowledge.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2203.09138/assets/x4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="190" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Representative failure cases of MuKEA on OK-VQA.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose a novel framework for knowledge-based visual question answering, which focuses on multimodal knowledge extraction and accumulation instead of using external knowledge bases. We propose a novel schema to represent multimodal knowledge by an explicit triplet and three loss functions to learn the representations from coarse to fine. We adopt a pre-training and fine-tuning strategy to accumulate multimodal knowledge progressively. Our model outperforms state-of-the-art on KB-VQA datasets and advances recent research from the multimodal knowledge view. We prove the complementary to existing knowledge graph. How to effectively combine MuKEA with knowledge bases will be the future work.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Wikipedia: The free encyclopedia.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.wikipedia.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.wikipedia.org/</a><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 6077â€“6086, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
CÂ Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 2425â€“2433, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
SÃ¶ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard
Cyganiak, and Zachary Ives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Dbpedia: A nucleus for a web of open data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The semantic web</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 722â€“735. Springer, 2007.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yalong Bai, Jianlong Fu, Tiejun Zhao, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Deep attention neural tensor network for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">,
pages 20â€“35, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Hedi Ben-Younes, RÃ©mi Cadene, Matthieu Cord, and Nicolas Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Mutan: Multimodal tucker fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 2612â€“2620, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Hedi Ben-Younes, Remi Cadene, Nicolas Thome, and Matthieu Cord.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Block: Bilinear superdiagonal fusion for visual question answering
and visual relationship detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 8102â€“8109, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Translating embeddings for modeling multi-relational data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 26, 2013.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Murel: Multimodal relational reasoning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1989â€“1998, 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Qingxing Cao, Bailin Li, Xiaodan Liang, Keze Wang, and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Knowledge-routed visual question reasoning: Challenges for deep
representation embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Tianshui Chen, Weihao Yu, Riquan Chen, and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Knowledge-embedded routing network for scene graph generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 6163â€“6171, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
FranÃ§ois Garderes, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Conceptbert: Concept-aware representation for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 489â€“498, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 6904â€“6913, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Xinzhe Han, Shuhui Wang, Chi Su, Weigang Zhang, Qingming Huang, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Interpretable visual reasoning via probabilistic formulation under
natural supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 553â€“570.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Yudong Han, Yangyang Guo, Jianhua Yin, Meng Liu, Yupeng Hu, and Liqiang Nie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Focal and composed vision-semantic modeling for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM International Conference on
Multimedia</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 4528â€“4536, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Eric Jang, Shixiang Gu, and Ben Poole.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Categorical reparameterization with gumbel-softmax.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference On Learning Representations</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
AmarÂ Viswanathan Kannan, Dmitriy Fradkin, Ioannis Akrotirianakis, Tugba
Kulahcioglu, Arquimedes Canedo, Aditi Roy, Shih-Yuan Yu, Malawade Arnav, and
MohammadÂ Abdullah AlÂ Faruque.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Multimodal knowledge graph for deep learning papers and code.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM International Conference on
Information &amp; Knowledge Management</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 3417â€“3420, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Bilinear attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 32rd International Conference on Neural
Information Processing Systems</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Hadamard product for low-rank bilinear pooling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference On Learning Representations</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A Shamma,
MichaelÂ S Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32â€“73, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Guohao Li, Xin Wang, and Wenwu Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Boosting visual question answering with context-aware knowledge
aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM International Conference on
Multimedia</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 1227â€“1235, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian
Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Gaia: A fine-grained multimedia knowledge extraction system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics: System Demonstrations</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 77â€“86, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Hugo Liu and Push Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Conceptnetâ€”a practical commonsense reasoning tool-kit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">BT technology journal</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 22(4):211â€“226, 2004.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Fixing weight decay regularization in adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference On Learning Representations</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Vilbert: pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 33rd International Conference on Neural
Information Processing Systems</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 13â€“23, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao, Shenqi Lai, and Jianyang
Gu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">A strong baseline and batch normalization neck for deep person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 22(10):2597â€“2609, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Frank Manola, Eric Miller, Brian McBride, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Rdf primer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">W3C recommendation</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 10(1-107):6, 2004.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Krisp: Integrating implicit and symbolic knowledge for open-domain
knowledge-based vqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 14111â€“14121, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Ok-vqa: A visual question answering benchmark requiring external
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 3195â€“3204, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Medhini Narasimhan, Svetlana Lazebnik, and Alexander Schwing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Out of the box: Reasoning with graph convolution nets for factual
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing
Systems</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 31, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Duy-Kien Nguyen and Takayuki Okatani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Improved fusion of visual and language representations by dense
symmetric co-attention for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 6087â€“6096, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Fudong Nian, Bing-Kun Bao, Teng Li, and Changsheng Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Multi-modal knowledge representation learning via webly-supervised
relationships mining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 25th ACM international conference on
Multimedia</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 411â€“419, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Learning conditioned graph structures for interpretable visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference and Workshop on Neural Information Processing
Systems</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">,
32:8026â€“8037, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Liang Peng, Yang Yang, Zheng Wang, Zi Huang, and HengÂ Tao Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Mra-net: Improving vqa via multi-modal relation attention network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Liang Peng, Yang Yang, Zheng Wang, Xiao Wu, and Zi Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Cra-net: Composed relation attention network for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 27th ACM International Conference on
Multimedia</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 1202â€“1210, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and ChristopherÂ D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 1532â€“1543, 2014.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Ethan Perez, Florian Strub, Harm DeÂ Vries, Vincent Dumoulin, and Aaron
Courville.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Film: Visual reasoning with a general conditioning layer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Pouya Pezeshkpour, Liyan Chen, and Sameer Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Embedding multimodal relational data for knowledge base completion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 3208â€“3218, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Thomas Rebele, Fabian Suchanek, Johannes Hoffart, Joanna Biega, Erdal Kuzey,
and Gerhard Weikum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Yago: A multilingual knowledge base from wikipedia, wordnet, and
geonames.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Semantic Web Conference</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 177â€“185.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 28:91â€“99,
2015.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Vl-bert: Pre-training of generic visual-linguistic representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan
Wang, and Kai Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Multi-modal knowledge graphs for recommender systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM International Conference on
Information &amp; Knowledge Management</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 1405â€“1414, 2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Lxmert: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 5100â€“5111, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Learning to compose dynamic tree structures for visual contexts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 6619â€“6628, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Meng Wang, Haofen Wang, Guilin Qi, and Qiushuo Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Richpedia: a large-scale, comprehensive multi-modal knowledge graph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Big Data Research</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 22:100159, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van DenÂ Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Fvqa: Fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">,
40(10):2413â€“2427, 2017.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, AnthonyÂ R Dick, and Anton vanÂ den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Explicit knowledge-based reasoning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Joint Conference on
Artificial Intelligence</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, TevenÂ Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and AlexanderÂ M. Rush.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Transformers: State-of-the-art natural language processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 38â€“45, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van DenÂ Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Ask me anything: Free-form visual question answering based on
knowledge from external sources.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 4622â€“4630, 2016.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Yonghui Wu, Mike Schuster, Zhifeng Chen, QuocÂ V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Googleâ€™s neural machine translation system: Bridging the gap between
human and machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1609.08144</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jiawang Xie, Zhenhao Dong, Qinghua Wen, Hongyin Zhu, Hailong Jin, Lei Hou, and
Juanzi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Construction of multimodal chinese tourism knowledge graph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference of Pioneering Computer Scientists,
Engineers and Educators</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 16â€“29. Springer, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Zhuoqian Yang, Zengchang Qin, Jing Yu, and Tao Wan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Prior visual relationship reasoning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE International Conference on Image Processing</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">,
pages 1411â€“1415, 2020.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Jing Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu, and Jianlong Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Cross-modal knowledge reasoning for knowledge-based visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, 108:107563, 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Deep modular co-attention networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 6281â€“6290, 2019.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Beyond bilinear: Generalized multimodal factorized high-order pooling
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">,
29(12):5947â€“5959, 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Yan Zhang, Jonathon Hare, and Adam PrÃ¼gel-Bennett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Learning to count objects in natural images for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Km4: Visual reasoning via knowledge embedding memory model with
mutual modulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Information Fusion</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 67:14â€“28, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Knowledge is power: Hierarchical-knowledge embedded meta-learning for
visual reasoning in artistic domains.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery &amp; Data Mining</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages 2360â€“2368, 2021.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and Qi Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Mucko: Multi-layer cross-modal knowledge reasoning for fact-based
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Joint Conference on
Artificial Intelligence</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, pages 1097â€“1103, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:120%;">MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual
Question Answering 
<br class="ltx_break"> Supplementary Material 
<br class="ltx_break">
</h2>

<figure id="Ax1.F1" class="ltx_figure"><img src="/html/2203.09138/assets/x5.png" id="Ax1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="292" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ax1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="Ax1.F1.4.2" class="ltx_text" style="font-size:90%;">Visualization of the accumulated knowledge graph. Gray lines and blue lines means knowledge accumulated in the VQA 2.0 and OK-VQA respectively. We show extra zoom-in examples for demonstration.</span></figcaption>
</figure>
<figure id="Ax1.F2" class="ltx_figure"><img src="/html/2203.09138/assets/x6.png" id="Ax1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ax1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="Ax1.F2.4.2" class="ltx_text" style="font-size:90%;">Knowledge accumulation in the pre-training stage and fine-tuning stages.</span></figcaption>
</figure>
<figure id="Ax1.F3" class="ltx_figure"><img src="/html/2203.09138/assets/x7.png" id="Ax1.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ax1.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="Ax1.F3.4.2" class="ltx_text" style="font-size:90%;">Testing samples based on manually constructed questions in zero-shot setting.</span></figcaption>
</figure>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ A A </span>Multimodal Knowledge Construction and Related Applications</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.8" class="ltx_p">In Figure <a href="#Ax1.F1" title="Figure 1 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering Supplementary Material â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we show a 1-hop sub-graph of our accumulated knowledge triplets centered in top-3 frequent answers in the OK-VQA train set. To construct multimodal knowledge graph, firstly we store the extracted knowledge triplet <math id="A1.p1.1.m1.3" class="ltx_Math" alttext="(h,r,t)" display="inline"><semantics id="A1.p1.1.m1.3a"><mrow id="A1.p1.1.m1.3.4.2" xref="A1.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="A1.p1.1.m1.3.4.2.1" xref="A1.p1.1.m1.3.4.1.cmml">(</mo><mi id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">h</mi><mo id="A1.p1.1.m1.3.4.2.2" xref="A1.p1.1.m1.3.4.1.cmml">,</mo><mi id="A1.p1.1.m1.2.2" xref="A1.p1.1.m1.2.2.cmml">r</mi><mo id="A1.p1.1.m1.3.4.2.3" xref="A1.p1.1.m1.3.4.1.cmml">,</mo><mi id="A1.p1.1.m1.3.3" xref="A1.p1.1.m1.3.3.cmml">t</mi><mo stretchy="false" id="A1.p1.1.m1.3.4.2.4" xref="A1.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.3b"><vector id="A1.p1.1.m1.3.4.1.cmml" xref="A1.p1.1.m1.3.4.2"><ci id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">â„</ci><ci id="A1.p1.1.m1.2.2.cmml" xref="A1.p1.1.m1.2.2">ğ‘Ÿ</ci><ci id="A1.p1.1.m1.3.3.cmml" xref="A1.p1.1.m1.3.3">ğ‘¡</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.3c">(h,r,t)</annotation></semantics></math> from training data, where <math id="A1.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="A1.p1.2.m2.1a"><mi id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><ci id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">h</annotation></semantics></math> is the visual region in the image focused by the question, <math id="A1.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="A1.p1.3.m3.1a"><mi id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><ci id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">t</annotation></semantics></math> is the ground-truth answer, and <math id="A1.p1.4.m4.1" class="ltx_Math" alttext="r" display="inline"><semantics id="A1.p1.4.m4.1a"><mi id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><ci id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">r</annotation></semantics></math> is the embedding of implicit relation between <math id="A1.p1.5.m5.1" class="ltx_Math" alttext="h" display="inline"><semantics id="A1.p1.5.m5.1a"><mi id="A1.p1.5.m5.1.1" xref="A1.p1.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="A1.p1.5.m5.1b"><ci id="A1.p1.5.m5.1.1.cmml" xref="A1.p1.5.m5.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.5.m5.1c">h</annotation></semantics></math> and <math id="A1.p1.6.m6.1" class="ltx_Math" alttext="t" display="inline"><semantics id="A1.p1.6.m6.1a"><mi id="A1.p1.6.m6.1.1" xref="A1.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A1.p1.6.m6.1b"><ci id="A1.p1.6.m6.1.1.cmml" xref="A1.p1.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.6.m6.1c">t</annotation></semantics></math>. We only display the image corresponding to the <math id="A1.p1.7.m7.1" class="ltx_Math" alttext="h" display="inline"><semantics id="A1.p1.7.m7.1a"><mi id="A1.p1.7.m7.1.1" xref="A1.p1.7.m7.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="A1.p1.7.m7.1b"><ci id="A1.p1.7.m7.1.1.cmml" xref="A1.p1.7.m7.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.7.m7.1c">h</annotation></semantics></math> and <math id="A1.p1.8.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="A1.p1.8.m8.1a"><mi id="A1.p1.8.m8.1.1" xref="A1.p1.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A1.p1.8.m8.1b"><ci id="A1.p1.8.m8.1.1.cmml" xref="A1.p1.8.m8.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.8.m8.1c">t</annotation></semantics></math> to explicitly show the structure of the accumulated multimodal knowledge graph. Then we merge all the tail entities with the same answer and merge all the head entities with the same image, while preserving object regions in images as shown in the example. After the pre-training and fine-tuning stages, we accumulate 218,135 multimodal knowledge triplets for knowledge graph construction.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">We summarize the characteristics of our proposed multimodal knowledge graph as follows:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">MuKEA extracts different instantiated knowledge for the same image based on different objects in the image. As shown in the left zoom-in example in Figure <a href="#Ax1.F1" title="Figure 1 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering Supplementary Material â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the same image connects both â€˜cell phoneâ€™ and â€˜travelâ€™ since different objects in the same image related to different knowledge. On the contrary, existing multimodal knowledge graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> is a complement of general knowledge graphs with entity-referred images.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">The same concept is associated with different visual knowledge in different scenes. As shown in the right zoom-in example in Figure <a href="#Ax1.F1" title="Figure 1 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering Supplementary Material â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, â€˜airportâ€™ can correspond to different scenarios, such as airport hall or suitcases in airport.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Compared to existing knowledge graphs with pre-defined types of relations, relation in our proposed multimodal knowledge graph is extensible and supports retrieval as well.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">By correlating relevant knowledge, the knowledge graph is capable of supporting complex reasoning. In Section <a href="#A3" title="Appendix C C Zero-shot Analysis of Accumulated Multimodal Knowledge â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> we provide a detailed analysis.</p>
</div>
</li>
</ul>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">Furthermore, We propose the following potential application scenarios for using our multimodal knowledge graph:</p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">Model-based knowledge search. MuKEA is capable of retrieving relevant knowledge for multimodal input.</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">Knowledge-based vision-language tasks, such as image caption, referring expression comprehension, vision-language navigation etc.</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p">Explainable deep learning, especially in the legal, medical fields.</p>
</div>
</li>
</ul>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p"><span id="A1.p4.1.1" class="ltx_text ltx_font_bold">The checkpoint of MuKEA, extracted multimodal knowledge graph, and off-the-shelf knowledge embeddings are available at <a target="_blank" href="https://github.com/AndersonStra/MuKEA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AndersonStra/MuKEA</a></span></p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ B B </span>Analysis of Progress Knowledge Accumulation</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">From case study in Figure <a href="#Ax1.F2" title="Figure 2 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering Supplementary Material â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we illustrate how the basic visual knowledge in VQA 2.0 helps to learn more complex knowledge in OK-VQA: (1) In the first row, benefiting from the question about the appearance of motorcycle, MuKEA is capable to correlate the visual content of motorcycles with the answer in a multi-object scenario. (2) In the second row, benefiting from the prior knowledge of visual content with plastic materials, MuKEA has the advantage of focusing on the key region and obtaining more generalized representation for objects made of plastic.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ C C </span>Zero-shot Analysis of Accumulated Multimodal Knowledge</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">In Figure <a href="#Ax1.F3" title="Figure 3 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering Supplementary Material â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show that our model is capable to combine different accumulated knowledge to answer complex questions in the zero-shot setting. (1) In the test sample of the first row, we correlate â€˜giraffeâ€™ with â€˜evolutionâ€™ through the manually constructed question. (2) In the test sample of the second row, we construct the question to correlate â€˜trackâ€™ and â€˜transportationâ€™. MuKEA performs correct prediction on both questions, which indicates that the accumulated multimodal knowledge can be applied to complex reasoning tasks in similar way as existing knowledge graphs.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ D D </span>Model Size Analysis</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">In Table <a href="#A4.T1" title="Table 1 â€£ Appendix D D Model Size Analysis â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the model size of MuKEA with pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. For MuKEA, we set the knowledge base size to the accumulated knowledge from VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The model size increases as the multimodal knowledge base size increases. Compared to ViLBERT, our model size only increases by 8.36% with the performance boost by 11.24%. The model size of KRISP is larger than ours by 86.78%, but its performance is inferior to ours by 3.69%. It indicates that our improvement is not from more parameters, but from the model structure.</p>
</div>
<figure id="A4.T1" class="ltx_table">
<table id="A4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T1.2.1.1" class="ltx_tr">
<th id="A4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="A4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Parameter</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T1.2.2.1" class="ltx_tr">
<th id="A4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</th>
<td id="A4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">138.4M</td>
</tr>
<tr id="A4.T1.2.3.2" class="ltx_tr">
<th id="A4.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="A4.T1.2.3.2.2" class="ltx_td ltx_align_center">218.9M</td>
</tr>
<tr id="A4.T1.2.4.3" class="ltx_tr">
<th id="A4.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="A4.T1.2.4.3.2" class="ltx_td ltx_align_center">183.5M</td>
</tr>
<tr id="A4.T1.2.5.4" class="ltx_tr">
<th id="A4.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</th>
<td id="A4.T1.2.5.4.2" class="ltx_td ltx_align_center">443.04M</td>
</tr>
<tr id="A4.T1.2.6.5" class="ltx_tr">
<th id="A4.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">MuKEA</th>
<td id="A4.T1.2.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">237.2M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="A4.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of model size.</span></figcaption>
</figure>
<figure id="A4.T2" class="ltx_table">
<table id="A4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T2.2.1.1" class="ltx_tr">
<th id="A4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="A4.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Knowledge Scale</span></th>
<th id="A4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A4.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">Inference Time(s)</span></th>
<th id="A4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A4.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">Ranking Time(s)</span></th>
<th id="A4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A4.T2.2.1.1.4.1" class="ltx_text ltx_font_bold">Ranking/Inference</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T2.2.2.1" class="ltx_tr">
<th id="A4.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">1000</th>
<td id="A4.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">65.1431</td>
<td id="A4.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.0026</td>
<td id="A4.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.0040%</td>
</tr>
<tr id="A4.T2.2.3.2" class="ltx_tr">
<th id="A4.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">10000</th>
<td id="A4.T2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_t">65.1459</td>
<td id="A4.T2.2.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.0054</td>
<td id="A4.T2.2.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.0083%</td>
</tr>
<tr id="A4.T2.2.4.3" class="ltx_tr">
<th id="A4.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">100000</th>
<td id="A4.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">67.3542</td>
<td id="A4.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.0064</td>
<td id="A4.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.0095%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="A4.T2.4.2" class="ltx_text" style="font-size:90%;">Inference time and ranking time comparison based on different scale of knowledge base on OK-VQA.</span></figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ E E </span>Efficiency Analysis</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">To verify that MuKEA strikes a good balance between efficiency and effectiveness , we compare the inference time and ranking time separately based on different scale of multimodal knowledge base. We test on OK-VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, which contains 5,046 samples for testing. Knowledge scale means the number of accumulated multimodal knowledge triplets. Inference time means the time spent on predicting over the entire test set. Ranking time means the time it takes to calculate the similarity with all tail entities in the knowledge base and rank the candidate tail entity.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">Table <a href="#A4.T2" title="Table 2 â€£ Appendix D D Model Size Analysis â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that although the ranking time is positively correlated with the size of knowledge base. It is relatively faster in the total inference time (less than 0.01%, as shown in the column of â€˜Ranking/Inferenceâ€™). Since the GPU uses threads to process matrix multiplication in parallel, ranking time is not linearly related to the size of knowledge base. Our model still has good efficiency based on large-scale multimodal knowledge base.</p>
</div>
<figure id="A5.F4" class="ltx_figure"><img src="/html/2203.09138/assets/x8.png" id="A5.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="312" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F4.5.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="A5.F4.3.1" class="ltx_text" style="font-size:90%;">Quantitative study of the confidence threshold <math id="A5.F4.3.1.m1.1" class="ltx_Math" alttext="\tau_{t}" display="inline"><semantics id="A5.F4.3.1.m1.1b"><msub id="A5.F4.3.1.m1.1.1" xref="A5.F4.3.1.m1.1.1.cmml"><mi id="A5.F4.3.1.m1.1.1.2" xref="A5.F4.3.1.m1.1.1.2.cmml">Ï„</mi><mi id="A5.F4.3.1.m1.1.1.3" xref="A5.F4.3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="A5.F4.3.1.m1.1c"><apply id="A5.F4.3.1.m1.1.1.cmml" xref="A5.F4.3.1.m1.1.1"><csymbol cd="ambiguous" id="A5.F4.3.1.m1.1.1.1.cmml" xref="A5.F4.3.1.m1.1.1">subscript</csymbol><ci id="A5.F4.3.1.m1.1.1.2.cmml" xref="A5.F4.3.1.m1.1.1.2">ğœ</ci><ci id="A5.F4.3.1.m1.1.1.3.cmml" xref="A5.F4.3.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.F4.3.1.m1.1d">\tau_{t}</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ F F </span>Effect of Ensemble Hyper-parameters</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.2" class="ltx_p">To verify the robustness of our ensemble model, we report the results of different threshold <math id="A6.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A6.p1.1.m1.1a"><mi id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><ci id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">m</annotation></semantics></math> for model ensemble in Figure <a href="#A5.F4" title="Figure 4 â€£ Appendix E E Efficiency Analysis â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Although we propose a simple method based on confidence to perform model ensemble, we can find that the performance of ensemble model remains stable as the hyper-parameters change in a reasonable range. The threshold <math id="A6.p1.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A6.p1.2.m2.1a"><mi id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><ci id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">m</annotation></semantics></math> is set in the range of 0.03 and 0.09, and the performance of the ensemble models varies in the range of 34.24% to 35.39%, 35.49% to 35.97%, 36.88% to 37.79% respectively. How to effectively combine MuKEA with knowledge bases will be the future work.</p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ G G </span>Analysis on VQA 2.0</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">To prove the generalization ability of our method, we compare our model with state-of-the-art models on the VQA 2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which requires models to understand the visible content instead of incorporating external knowledge. All questions in VQA 2.0 are divided into three categories: <span id="A7.p1.1.1" class="ltx_text ltx_font_italic">Yes/No</span>, <span id="A7.p1.1.2" class="ltx_text ltx_font_italic">Number</span>, and <span id="A7.p1.1.3" class="ltx_text ltx_font_italic">Other</span>. Since our model is pre-trained on <span id="A7.p1.1.4" class="ltx_text ltx_font_italic">Other</span> type questions for accumulating basic multimodal knowledge, we only keep <span id="A7.p1.1.5" class="ltx_text ltx_font_italic">Other</span> type questions for comparison.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p">Table <a href="#A7.T3" title="Table 3 â€£ Appendix G G Analysis on VQA 2.0 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that our model achieves comparable results compared with state-of-the-art models. This is mainly due to the following reasons: (1) VQA 2.0 mainly relies on visual appearance clues instead of external knowledge. As shown in Figure <a href="#A7.F5" title="Figure 5 â€£ Appendix G G Analysis on VQA 2.0 â€£ MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the example on the left requires the model to sense colors in multiple regions, while the example on the right requires the model to accurately detect object in the target region. (2) Existing models takes the head answers as the candidate answers, we accumulate multimodal knowledge on the whole dataset to ensure the diversity of answers, which is 10 times larger than the candidate answer set.</p>
</div>
<figure id="A7.T3" class="ltx_table">
<table id="A7.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A7.T3.4.1.1" class="ltx_tr">
<th id="A7.T3.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="A7.T3.4.1.1.1.1" class="ltx_text">Method</span></th>
<th id="A7.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">test-dev</th>
<th id="A7.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">test-std</th>
</tr>
<tr id="A7.T3.4.2.2" class="ltx_tr">
<th id="A7.T3.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Other</th>
<th id="A7.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Other</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A7.T3.4.3.1" class="ltx_tr">
<th id="A7.T3.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MLB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="A7.T3.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">56.34</td>
<td id="A7.T3.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A7.T3.4.4.2" class="ltx_tr">
<th id="A7.T3.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="A7.T3.4.4.2.2" class="ltx_td ltx_align_center">56.50</td>
<td id="A7.T3.4.4.2.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A7.T3.4.5.3" class="ltx_tr">
<th id="A7.T3.4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<td id="A7.T3.4.5.3.2" class="ltx_td ltx_align_center">57.44</td>
<td id="A7.T3.4.5.3.3" class="ltx_td ltx_align_center">56.83</td>
</tr>
<tr id="A7.T3.4.6.4" class="ltx_tr">
<th id="A7.T3.4.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DA-NTN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="A7.T3.4.6.4.2" class="ltx_td ltx_align_center">57.92</td>
<td id="A7.T3.4.6.4.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A7.T3.4.7.5" class="ltx_tr">
<th id="A7.T3.4.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</th>
<td id="A7.T3.4.7.5.2" class="ltx_td ltx_align_center">58.97</td>
<td id="A7.T3.4.7.5.3" class="ltx_td"></td>
</tr>
<tr id="A7.T3.4.8.6" class="ltx_tr">
<th id="A7.T3.4.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BLOCK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="A7.T3.4.8.6.2" class="ltx_td ltx_align_center">58.51</td>
<td id="A7.T3.4.8.6.3" class="ltx_td ltx_align_center">58.79</td>
</tr>
<tr id="A7.T3.4.9.7" class="ltx_tr">
<th id="A7.T3.4.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="A7.T3.4.9.7.2" class="ltx_td ltx_align_center">56.05</td>
<td id="A7.T3.4.9.7.3" class="ltx_td ltx_align_center">56.26</td>
</tr>
<tr id="A7.T3.4.10.8" class="ltx_tr">
<th id="A7.T3.4.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CGN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</th>
<td id="A7.T3.4.10.8.2" class="ltx_td ltx_align_center">-</td>
<td id="A7.T3.4.10.8.3" class="ltx_td ltx_align_center">56.22</td>
</tr>
<tr id="A7.T3.4.11.9" class="ltx_tr">
<th id="A7.T3.4.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CRA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="A7.T3.4.11.9.2" class="ltx_td ltx_align_center">59.08</td>
<td id="A7.T3.4.11.9.3" class="ltx_td ltx_align_center">59.42</td>
</tr>
<tr id="A7.T3.4.12.10" class="ltx_tr">
<th id="A7.T3.4.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MRA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<td id="A7.T3.4.12.10.2" class="ltx_td ltx_align_center">59.46</td>
<td id="A7.T3.4.12.10.3" class="ltx_td ltx_align_center">59.86</td>
</tr>
<tr id="A7.T3.4.13.11" class="ltx_tr">
<th id="A7.T3.4.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SceneGCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="A7.T3.4.13.11.2" class="ltx_td ltx_align_center">57.77</td>
<td id="A7.T3.4.13.11.3" class="ltx_td ltx_align_center">57.89</td>
</tr>
<tr id="A7.T3.4.14.12" class="ltx_tr">
<th id="A7.T3.4.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TRN+UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<td id="A7.T3.4.14.12.2" class="ltx_td ltx_align_center">57.44</td>
<td id="A7.T3.4.14.12.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A7.T3.4.15.13" class="ltx_tr">
<th id="A7.T3.4.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MuRel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="A7.T3.4.15.13.2" class="ltx_td ltx_align_center">57.85</td>
<td id="A7.T3.4.15.13.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A7.T3.4.16.14" class="ltx_tr">
<th id="A7.T3.4.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VCTREE+HL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="A7.T3.4.16.14.2" class="ltx_td ltx_align_center">59.11</td>
<td id="A7.T3.4.16.14.3" class="ltx_td ltx_align_center">59.34</td>
</tr>
<tr id="A7.T3.4.17.15" class="ltx_tr">
<th id="A7.T3.4.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LENA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<td id="A7.T3.4.17.15.2" class="ltx_td ltx_align_center">59.52</td>
<td id="A7.T3.4.17.15.3" class="ltx_td ltx_align_center">59.87</td>
</tr>
<tr id="A7.T3.4.18.16" class="ltx_tr">
<th id="A7.T3.4.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Ours</th>
<td id="A7.T3.4.18.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">57.45</td>
<td id="A7.T3.4.18.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">57.84</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A7.T3.5.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="A7.T3.2.1" class="ltx_text" style="font-size:90%;">Comparison on <math id="A7.T3.2.1.m1.1" class="ltx_Math" alttext="Other" display="inline"><semantics id="A7.T3.2.1.m1.1b"><mrow id="A7.T3.2.1.m1.1.1" xref="A7.T3.2.1.m1.1.1.cmml"><mi id="A7.T3.2.1.m1.1.1.2" xref="A7.T3.2.1.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="A7.T3.2.1.m1.1.1.1" xref="A7.T3.2.1.m1.1.1.1.cmml">â€‹</mo><mi id="A7.T3.2.1.m1.1.1.3" xref="A7.T3.2.1.m1.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="A7.T3.2.1.m1.1.1.1b" xref="A7.T3.2.1.m1.1.1.1.cmml">â€‹</mo><mi id="A7.T3.2.1.m1.1.1.4" xref="A7.T3.2.1.m1.1.1.4.cmml">h</mi><mo lspace="0em" rspace="0em" id="A7.T3.2.1.m1.1.1.1c" xref="A7.T3.2.1.m1.1.1.1.cmml">â€‹</mo><mi id="A7.T3.2.1.m1.1.1.5" xref="A7.T3.2.1.m1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="A7.T3.2.1.m1.1.1.1d" xref="A7.T3.2.1.m1.1.1.1.cmml">â€‹</mo><mi id="A7.T3.2.1.m1.1.1.6" xref="A7.T3.2.1.m1.1.1.6.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A7.T3.2.1.m1.1c"><apply id="A7.T3.2.1.m1.1.1.cmml" xref="A7.T3.2.1.m1.1.1"><times id="A7.T3.2.1.m1.1.1.1.cmml" xref="A7.T3.2.1.m1.1.1.1"></times><ci id="A7.T3.2.1.m1.1.1.2.cmml" xref="A7.T3.2.1.m1.1.1.2">ğ‘‚</ci><ci id="A7.T3.2.1.m1.1.1.3.cmml" xref="A7.T3.2.1.m1.1.1.3">ğ‘¡</ci><ci id="A7.T3.2.1.m1.1.1.4.cmml" xref="A7.T3.2.1.m1.1.1.4">â„</ci><ci id="A7.T3.2.1.m1.1.1.5.cmml" xref="A7.T3.2.1.m1.1.1.5">ğ‘’</ci><ci id="A7.T3.2.1.m1.1.1.6.cmml" xref="A7.T3.2.1.m1.1.1.6">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T3.2.1.m1.1d">Other</annotation></semantics></math> split of VQA 2.0 dataset.</span></figcaption>
</figure>
<figure id="A7.F5" class="ltx_figure"><img src="/html/2203.09138/assets/x9.png" id="A7.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="203" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A7.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A7.F5.4.2" class="ltx_text" style="font-size:90%;">samples in the VQA 2.0 dataset.</span></figcaption>
</figure>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">AppendixÂ H H </span>Implementation Details</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">For all experiments, we implement our model on top of LXMERT-based-uncased <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> with 2 NVIDIA V100 GPUs. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> to use Faster R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> pre-trained by the bottom-up model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> on the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The dimension of inner feed-forward network layer before head entity and relation is set to 1024. The dimensions of entity and relation in multimodal knowledge triplet are set to 300. The parameters in the look-up table are initialized by uniform distribution.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.09137" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.09138" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.09138">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.09138" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.09139" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 08:04:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
