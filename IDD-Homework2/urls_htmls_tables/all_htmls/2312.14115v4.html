<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.14115] LingoQA: Video Question Answering for Autonomous Driving</title><meta property="og:description" content="Autonomous driving has long faced a challenge with public acceptance due to the lack of explainability in the decision-making process. Video question-answering (QA) in natural language provides the opportunity for brid…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LingoQA: Video Question Answering for Autonomous Driving">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LingoQA: Video Question Answering for Autonomous Driving">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.14115">

<!--Generated on Tue Feb 27 12:29:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_ERROR undefined">\ul</span>






 



</p>
</div>
<h1 class="ltx_title ltx_title_document">LingoQA: Video Question Answering for Autonomous Driving</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ana-Maria Marcu<sup id="id1.1.id1" class="ltx_sup">*</sup>, Long Chen<sup id="id2.2.id2" class="ltx_sup">*</sup>, Jan Hünermann<sup id="id3.3.id3" class="ltx_sup">*</sup>, 
<br class="ltx_break">Alice Karnsund<sup id="id4.4.id4" class="ltx_sup">*</sup>, Benoit Hanotte<sup id="id5.5.id5" class="ltx_sup">*</sup>, Prajwal Chidananda, Saurabh Nair,
<br class="ltx_break">Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Oleg Sinavski<sup id="id6.6.id6" class="ltx_sup">*</sup>
<br class="ltx_break">Wayve
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">research@wayve.ai</span> 
<br class="ltx_break"><sup id="id8.8.id8" class="ltx_sup"><span id="id8.8.id8.1" class="ltx_text" style="font-size:90%;">*</span></sup><span id="id9.9.id9" class="ltx_text" style="font-size:90%;">Equal contributions
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Autonomous driving has long faced a challenge with public acceptance due to the lack of explainability in the decision-making process. Video question-answering (QA) in natural language provides the opportunity for bridging this gap. Nonetheless, evaluating the performance of Video QA models has proved particularly tough due to the absence of comprehensive benchmarks. To fill this gap, we introduce LingoQA<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_ref_self">https://github.com/wayveai/LingoQA</span></span></span></span>, a benchmark specifically for autonomous driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman correlation coefficient with human evaluations. We introduce a Video QA dataset of central London consisting of 419k samples that we release with the paper. We establish a baseline vision-language model and run extensive ablation studies to understand its performance. 
<br class="ltx_break"></p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">{strip}</span><img src="/html/2312.14115/assets/img/headline_image_new_2.png" id="p2.g1" class="ltx_graphics ltx_img_landscape" width="548" height="263" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.4.2" class="ltx_text" style="font-size:90%;">LingoQA is a comprehensive <span id="S0.F1.4.2.1" class="ltx_text ltx_font_italic">benchmark</span> for Video Question Answering in autonomous driving. Our baseline vision-language model on this benchmark, can answer questions related to driving reasoning, object recognition, action justification, and scene description.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Communication plays a pivotal role in naturally fostering trust among individuals. However, establishing trust between users and agents remains a significant challenge within the field of artificial intelligence. Recent studies have indicated that articulating explicit reasoning steps can significantly enhance user confidence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, in addition to improving the capabilities of Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. The need for explainability remains critical, particularly in safety-critical domains where technology adoption hinges upon this factor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Consider the domain of end-to-end autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, where the driving policy is often executed through deep neural networks processing camera inputs to generate control commands. Despite their efficiency, these systems have grappled with the persistent challenge of providing transparent and interpretable decisions. Integrating Vision-Language Models (VLMs) into the field of autonomous driving holds the promise of enhancing user trust in these systems. Recent strides in VLMs have solidified transformers as multimodal learners, showcasing remarkable performance in tasks such as Visual Question Answering (VQA) and underscoring their proficiency in acquiring robust representations for complex tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our focus is on vision-only end-to-end autonomous driving, aiming to bridge the gap between data-driven decision-making and user trust. We introduce LingoQA, a benchmark designed for autonomous driving video QA, utilizing a novel dataset comprising more than 419k QA pairs. Distinguished by its free-form approach to questions and answers, this dataset broadens the scope of autonomous driving video QA, encompassing reasoning and action justifications. Additionally, we publish a comprehensive evaluation suite consisting of 1,000 examples. At the core of our benchmark lies a novel evaluation metric based on a learned text classifier called <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Lingo-Judge</span>, inspired by GPT-Judge used in TruthfulQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. We perform rigorous studies correlating automatic metrics to human preferences and find that Lingo-Judge achieves a 0.950 Spearman and 0.993 Pearson correlation coefficient, surpassing existing automated labelling techniques like METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> on our benchmark, while being fast enough for frequent runs during training and development. The evaluation code and the weights for the classifier will be released with the paper to support robust benchmarking of autonomous driving explainability.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Equipped with this evaluation toolkit, we conducted a comprehensive empirical study on key components and their ablations in VLMs for autonomous driving. Our findings in Section <a href="#S5" title="5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> indicate that the most effective approach involves partially fine-tuning the attention layers of our vision-language model equipped with Vicuna-1.5-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, on both Action and Scenery datasets. This process involves using 5 video frames over 4 seconds and a late video fusion technique. Our collective work, spanning the LingoQA benchmark, the visual instruction-tuning dataset, and the innovative evaluation metric, aims to propel the domain of explainable autonomous driving, laying a robust foundation for subsequent research and development endeavors. To summarise the main contributions of this paper:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">LingoQA Benchmark</span>: We introduce a novel benchmark for autonomous driving video QA using a learned text classifier for evaluation. It outperforms existing metrics, including GPT-4, with a Spearman coefficient of 0.950 indicating a strong correlation with human evaluation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LingoQA Dataset</span>: Our 419.9k QA pair dataset stands out with its free-form questions and answers, covering not just perception but also driving reasoning from the drivers directly, broadening the scope of autonomous driving video QA.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">LingoQA Baseline</span>: Through testing of various video-language components on LingoQA, we find that the most effective approach involves partially fine-tuning the attention layers of our vision-language model equipped with Vicuna-1.5-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and a late video fusion technique. We establish a new baseline for this field with an identified model combination. Example outputs from the model are shown in Figure <a href="#S0.F1" title="Figure 1 ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Language in Autonomous Driving</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Modern autonomous vehicle software relies heavily on artificial intelligence models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. This, together with the increased number of such vehicles on the road, poses a fundamental challenge in terms of interpretability in the decision-making process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Understanding why a decision is made is crucial for understanding areas of uncertainty, building trust, enabling effective human-AI collaboration, and ensuring safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. In a survey conducted by Partners for Automated Vehicle Education (PAVE) in 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, 60% of participants stated that they would trust AVs more if they better understood the underlying process of the system. To establish trust with the general public, the inner workings of systems must be explained in a human-interpretable way, such as through language and visual explanations.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The field of autonomous driving has been embracing the opportunity to make end-to-end driving models more explainable using language, understanding that methods based purely on attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> are not sufficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The early explorations of GPT3.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and GPT4-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> on autonomous driving scenarios show that LLMs/VLMs demonstrate superior performance in scene understanding and causal reasoning compared to existing autonomous systems. Works such as ADAPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and LLM-Driver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> propose multi-task learning frameworks for jointly predicting language and control outputs. Inspired by progress in large language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, vision-language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and multi-modal transformers for robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Closely related to our proposed baseline is DriveGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, proposing a multi-modal vision-language-action model that tokenizes videos, as well as text and control actions.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Evaluation Metrics</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Progress has been relatively slow for developing vision-language models for autonomous driving, with only a few works aiming to quantitatively improve upon prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. A key challenge consists of automated, reproducible evaluation metrics that are highly correlated with human ratings, particularly due to the inherent complexities in assessing natural language. ADAPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> reports human feedback in addition to standard natural language generation metrics, while DriveGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> reports ChatGPT ratings. Automated methods such as BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> show weak alignment with human feedback <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> is also based on <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">n-gram</span> level similarity, as opposed to capturing the correctness of an answer based on its meaning. Newer evaluation metrics using ChatGPT have shown improvement in the area of sentence understanding, while still having limitations, such as providing high scores to elaborate, eloquent, but incorrect sentences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Evaluation based on human feedback is subjective and difficult to reproduce. In this work, we address this challenge by introducing a novel video QA benchmark for autonomous driving that checks for factual correctness and is highly correlated to human correctness ratings on our proposed evaluation dataset.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Datasets for Autonomous Driving</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Recent advances in generative AI have been underpinned by training with increasingly large and diverse internet-scale datasets. Nonetheless, when evaluated zero-shot on autonomous driving datasets, many pre-trained <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">models</span> such as Flamingo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> fall short of expectations. Consequently, there remains a need for high-quality training datasets for autonomous driving, as well as for reliable benchmarks. Autonomous driving datasets have been focused on commentary, as opposed to question-answer pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. However, we note that action, justification is a relatively easy task for foundation models, that are capable of efficiently compressing large amounts of data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The advantage of VQA is that related, but slightly different questions can be asked to truly probe the validity of the model representations. Further datasets focus on ranking the importance of elements in the scene and explaining the reasoning behind the choice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. The datasets available for VQA have been previously constructed around existing object detection datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Datasets such as NuScenesQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, while useful for testing perception, contain simple language outputs of on average one word per question that do not tackle the more challenging reasoning problem.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Our proposed dataset LingoQA addresses the existing gap in autonomous driving as it contains a diverse set of questions related to <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">driving behaviours</span> and <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">scenery</span> in addition to perception questions related to object presence and positioning. This dataset has the strength of being diverse with respect to the language used while being grounded in human reasoning. Examples of the complex questions and answers existent in the dataset are provided in Appendix <a href="#A1" title="Appendix A LingoQA Dataset Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. Moreover, a significant advantage of LingoQA is that the intentions and reasoning behind driving decisions are directly labeled by the drivers themselves. This provides an accurate understanding of driving dynamics.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>LingoQA Benchmark</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce LingoQA, a benchmark to evaluate video question-answering models for autonomous driving.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We collected a small, low-density dataset from in-house human labelers, creating both the questions and the answers associated with the short videos. We labeled a small portion of held-out data on 500 human-generated questions using 20+ different evaluators to obtain our test set. Since answers are subjective and noisy, we labeled them twice, making sure the same evaluator does not receive the same question twice. After that, we manually reviewed the answers for semantic disagreements and mistakes. We relabeled such samples two more times and fixed the disagreements, preferring the semantics of the majority of responders but preserving maximal variety in the responses. Finally, we condensed this into 1k high-quality answers to 500 questions, with two correct but diverse answers per question. The dataset evaluates a range of competencies, including action and justification, attention, description, localisation, identification, counting and anticipation, as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Datasets ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Metric</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Evaluating open-ended textual dialogues is a challenging task. Quite often the correct answers are ambiguous, subjective, or even not attainable. The most common language-based metrics for evaluating question-answering models in autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> are BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, despite their known limitations, such as relying heavily on the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">n-gram</span> frequency as opposed to the underlying meaning of the answer. To address these limitations, we set ourselves the challenge to develop an <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">automated</span>, <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">non-visual</span> evaluation method for free-form language answers from vision-language models which checks correctness independent of phrasing against a ground truth answer and which is <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">highly correlated</span> with human ratings.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.6.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:131.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(8.4pt,-2.6pt) scale(1.04054766931027,1.04054766931027) ;">
<table id="S3.T1.6.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.6.6.6.7" class="ltx_tr">
<td id="S3.T1.6.6.6.7.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.6.6.6.7.2" class="ltx_td ltx_align_center ltx_border_tt">Pearson</td>
<td id="S3.T1.6.6.6.7.3" class="ltx_td ltx_align_center ltx_border_tt">Spearman</td>
<td id="S3.T1.6.6.6.7.4" class="ltx_td ltx_align_center ltx_border_tt">Val Acc. [%]</td>
<td id="S3.T1.6.6.6.7.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Time [sec]</td>
</tr>
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_markedasmath">Lingo-Judge</span></td>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">0.993</span></td>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">0.950</span></td>
<td id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">95.0</span></td>
<td id="S3.T1.1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">10.5</td>
</tr>
<tr id="S3.T1.2.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.2.1" class="ltx_td ltx_align_left"><span id="S3.T1.2.2.2.2.1.1" class="ltx_text ltx_markedasmath">GPT-4 with CoT</span></td>
<td id="S3.T1.2.2.2.2.2" class="ltx_td ltx_align_center">0.990</td>
<td id="S3.T1.2.2.2.2.3" class="ltx_td ltx_align_center">0.932</td>
<td id="S3.T1.2.2.2.2.4" class="ltx_td ltx_align_center">91.2</td>
<td id="S3.T1.2.2.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center">3016.0</td>
</tr>
<tr id="S3.T1.3.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.3.1" class="ltx_td ltx_align_left">
<span id="S3.T1.3.3.3.3.1.1" class="ltx_text ltx_markedasmath">GPT-4</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S3.T1.3.3.3.3.2" class="ltx_td ltx_align_center">0.988</td>
<td id="S3.T1.3.3.3.3.3" class="ltx_td ltx_align_center">0.941</td>
<td id="S3.T1.3.3.3.3.4" class="ltx_td ltx_align_center">90.6</td>
<td id="S3.T1.3.3.3.3.5" class="ltx_td ltx_nopad_r ltx_align_center">812.4</td>
</tr>
<tr id="S3.T1.4.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.4.1" class="ltx_td ltx_align_left">
<span id="S3.T1.4.4.4.4.1.1" class="ltx_text ltx_markedasmath">BLEU</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.4.4.4.4.2" class="ltx_td ltx_align_center">0.881</td>
<td id="S3.T1.4.4.4.4.3" class="ltx_td ltx_align_center">0.835</td>
<td id="S3.T1.4.4.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.4.4.4.4.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.4.4.4.4.5.1" class="ltx_text ltx_font_bold">0.1</span></td>
</tr>
<tr id="S3.T1.5.5.5.5" class="ltx_tr">
<td id="S3.T1.5.5.5.5.1" class="ltx_td ltx_align_left">
<span id="S3.T1.5.5.5.5.1.1" class="ltx_text ltx_markedasmath">METEOR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S3.T1.5.5.5.5.2" class="ltx_td ltx_align_center">0.891</td>
<td id="S3.T1.5.5.5.5.3" class="ltx_td ltx_align_center">0.876</td>
<td id="S3.T1.5.5.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.5.5.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center">8.0</td>
</tr>
<tr id="S3.T1.6.6.6.6" class="ltx_tr">
<td id="S3.T1.6.6.6.6.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.6.6.6.6.1.1" class="ltx_text ltx_markedasmath">CIDEr</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb">0.878</td>
<td id="S3.T1.6.6.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">0.853</td>
<td id="S3.T1.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S3.T1.6.6.6.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0.2</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.9.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.10.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Lingo-Judge Performance.<span id="S3.T1.10.2.1" class="ltx_text ltx_font_medium"> Correlation with human ratings, validation accuracy, and time taken to run of our proposed LingoQA evaluation metric compared to previous language-based metrics. All metrics use textual ground truth and have no access to vision information. Further examples are presented in Appendix <a href="#A2" title="Appendix B Lingo-Judge Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></span></figcaption>
</figure>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">GPT-4 based evaluation</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Inspired by the G-Eval metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we used GPT-4 to evaluate answers on a larger scale. Given a question and answer pair from the test set and a model’s answer, we ask GPT-4 to evaluate whether the model’s answer corresponds to a human’s answer. Notice that it doesn’t make use of any visual input.
We experimented with prompts and methods achieving good quality of judgements. We achieved the highest accuracy by employing chain-of-thought prompting where we ask GPT-4 to first come up with an evaluation strategy before grading a model’s answer. However, as shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Evaluation Metric ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this leads to increased inference time. Further details are provided in Appendix <a href="#A3" title="Appendix C GPT-4 Grading ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.
Unfortunately, we found GPT-4 based evaluation impractical to use as a main development and training metric due to the time required to evaluate answers on our relatively small evaluation dataset (from 13min up to 50min for a single evaluation due to the API rate limit).</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Lingo-Judge</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Given these limitations and inspired by TruthfulQA GPT-Judge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, we pursued an alternative approach using a learned text classifier, dubbed Lingo-Judge, which estimates the correctness of model answers. We measure the correctness of model predictions as an accuracy using a small transformer-based text classifier that takes in a question, the human’s, and the model’s answer and outputs a probability that the model’s answer is correct. Please note, Lingo-Judge does not receive video input and must rely only on the supporting human’s answers. For every question, we run Lingo-Judge on all combinations of <span id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">(ground-truth answer, predicted answer)</span> and take the maximum correctness estimate, as shown in Equation <a href="#S3.E1" title="Equation 1 ‣ Lingo-Judge ‣ 3.2 Evaluation Metric ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">S</annotation></semantics></math> is the score per sample. We found this recipe yield the best predictive power provided enough diversity of human answers in our evaluation dataset.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.5" class="ltx_Math" alttext="S=\max_{j\in\{0,1\}}F_{Judge}(pred,ground\_truth[j])" display="block"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mi id="S3.E1.m1.5.5.4" xref="S3.E1.m1.5.5.4.cmml">S</mi><mo id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml"><mrow id="S3.E1.m1.5.5.2.4" xref="S3.E1.m1.5.5.2.4.cmml"><munder id="S3.E1.m1.5.5.2.4.1" xref="S3.E1.m1.5.5.2.4.1.cmml"><mi id="S3.E1.m1.5.5.2.4.1.2" xref="S3.E1.m1.5.5.2.4.1.2.cmml">max</mi><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">j</mi><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">∈</mo><mrow id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.5.2.1" xref="S3.E1.m1.2.2.2.5.1.cmml">{</mo><mn id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">0</mn><mo id="S3.E1.m1.2.2.2.5.2.2" xref="S3.E1.m1.2.2.2.5.1.cmml">,</mo><mn id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">1</mn><mo stretchy="false" id="S3.E1.m1.2.2.2.5.2.3" xref="S3.E1.m1.2.2.2.5.1.cmml">}</mo></mrow></mrow></munder><mo lspace="0.167em" id="S3.E1.m1.5.5.2.4a" xref="S3.E1.m1.5.5.2.4.cmml">⁡</mo><msub id="S3.E1.m1.5.5.2.4.2" xref="S3.E1.m1.5.5.2.4.2.cmml"><mi id="S3.E1.m1.5.5.2.4.2.2" xref="S3.E1.m1.5.5.2.4.2.2.cmml">F</mi><mrow id="S3.E1.m1.5.5.2.4.2.3" xref="S3.E1.m1.5.5.2.4.2.3.cmml"><mi id="S3.E1.m1.5.5.2.4.2.3.2" xref="S3.E1.m1.5.5.2.4.2.3.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.4.2.3.1" xref="S3.E1.m1.5.5.2.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.4.2.3.3" xref="S3.E1.m1.5.5.2.4.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.4.2.3.1a" xref="S3.E1.m1.5.5.2.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.4.2.3.4" xref="S3.E1.m1.5.5.2.4.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.4.2.3.1b" xref="S3.E1.m1.5.5.2.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.4.2.3.5" xref="S3.E1.m1.5.5.2.4.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.4.2.3.1c" xref="S3.E1.m1.5.5.2.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.4.2.3.6" xref="S3.E1.m1.5.5.2.4.2.3.6.cmml">e</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.3" xref="S3.E1.m1.5.5.2.3.cmml">​</mo><mrow id="S3.E1.m1.5.5.2.2.2" xref="S3.E1.m1.5.5.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.3" xref="S3.E1.m1.5.5.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">​</mo><mi id="S3.E1.m1.4.4.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1a" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">​</mo><mi id="S3.E1.m1.4.4.1.1.1.1.4" xref="S3.E1.m1.4.4.1.1.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1b" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">​</mo><mi id="S3.E1.m1.4.4.1.1.1.1.5" xref="S3.E1.m1.4.4.1.1.1.1.5.cmml">d</mi></mrow><mo id="S3.E1.m1.5.5.2.2.2.4" xref="S3.E1.m1.5.5.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.5.5.2.2.2.2" xref="S3.E1.m1.5.5.2.2.2.2.cmml"><mi id="S3.E1.m1.5.5.2.2.2.2.2" xref="S3.E1.m1.5.5.2.2.2.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.3" xref="S3.E1.m1.5.5.2.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1a" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.4" xref="S3.E1.m1.5.5.2.2.2.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1b" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.5" xref="S3.E1.m1.5.5.2.2.2.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1c" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.6" xref="S3.E1.m1.5.5.2.2.2.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1d" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.7" xref="S3.E1.m1.5.5.2.2.2.2.7.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1e" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.E1.m1.5.5.2.2.2.2.8" xref="S3.E1.m1.5.5.2.2.2.2.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1f" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.9" xref="S3.E1.m1.5.5.2.2.2.2.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1g" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.10" xref="S3.E1.m1.5.5.2.2.2.2.10.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1h" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.11" xref="S3.E1.m1.5.5.2.2.2.2.11.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1i" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.12" xref="S3.E1.m1.5.5.2.2.2.2.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1j" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.2.2.2.13" xref="S3.E1.m1.5.5.2.2.2.2.13.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.2.2.1k" xref="S3.E1.m1.5.5.2.2.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.5.5.2.2.2.2.14.2" xref="S3.E1.m1.5.5.2.2.2.2.14.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.2.14.2.1" xref="S3.E1.m1.5.5.2.2.2.2.14.1.1.cmml">[</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">j</mi><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.2.14.2.2" xref="S3.E1.m1.5.5.2.2.2.2.14.1.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.5.5.2.2.2.5" xref="S3.E1.m1.5.5.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3"></eq><ci id="S3.E1.m1.5.5.4.cmml" xref="S3.E1.m1.5.5.4">𝑆</ci><apply id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"><times id="S3.E1.m1.5.5.2.3.cmml" xref="S3.E1.m1.5.5.2.3"></times><apply id="S3.E1.m1.5.5.2.4.cmml" xref="S3.E1.m1.5.5.2.4"><apply id="S3.E1.m1.5.5.2.4.1.cmml" xref="S3.E1.m1.5.5.2.4.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.4.1.1.cmml" xref="S3.E1.m1.5.5.2.4.1">subscript</csymbol><max id="S3.E1.m1.5.5.2.4.1.2.cmml" xref="S3.E1.m1.5.5.2.4.1.2"></max><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><in id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></in><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">𝑗</ci><set id="S3.E1.m1.2.2.2.5.1.cmml" xref="S3.E1.m1.2.2.2.5.2"><cn type="integer" id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">0</cn><cn type="integer" id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">1</cn></set></apply></apply><apply id="S3.E1.m1.5.5.2.4.2.cmml" xref="S3.E1.m1.5.5.2.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.4.2.1.cmml" xref="S3.E1.m1.5.5.2.4.2">subscript</csymbol><ci id="S3.E1.m1.5.5.2.4.2.2.cmml" xref="S3.E1.m1.5.5.2.4.2.2">𝐹</ci><apply id="S3.E1.m1.5.5.2.4.2.3.cmml" xref="S3.E1.m1.5.5.2.4.2.3"><times id="S3.E1.m1.5.5.2.4.2.3.1.cmml" xref="S3.E1.m1.5.5.2.4.2.3.1"></times><ci id="S3.E1.m1.5.5.2.4.2.3.2.cmml" xref="S3.E1.m1.5.5.2.4.2.3.2">𝐽</ci><ci id="S3.E1.m1.5.5.2.4.2.3.3.cmml" xref="S3.E1.m1.5.5.2.4.2.3.3">𝑢</ci><ci id="S3.E1.m1.5.5.2.4.2.3.4.cmml" xref="S3.E1.m1.5.5.2.4.2.3.4">𝑑</ci><ci id="S3.E1.m1.5.5.2.4.2.3.5.cmml" xref="S3.E1.m1.5.5.2.4.2.3.5">𝑔</ci><ci id="S3.E1.m1.5.5.2.4.2.3.6.cmml" xref="S3.E1.m1.5.5.2.4.2.3.6">𝑒</ci></apply></apply></apply><interval closure="open" id="S3.E1.m1.5.5.2.2.3.cmml" xref="S3.E1.m1.5.5.2.2.2"><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"></times><ci id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2">𝑝</ci><ci id="S3.E1.m1.4.4.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3">𝑟</ci><ci id="S3.E1.m1.4.4.1.1.1.1.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.4">𝑒</ci><ci id="S3.E1.m1.4.4.1.1.1.1.5.cmml" xref="S3.E1.m1.4.4.1.1.1.1.5">𝑑</ci></apply><apply id="S3.E1.m1.5.5.2.2.2.2.cmml" xref="S3.E1.m1.5.5.2.2.2.2"><times id="S3.E1.m1.5.5.2.2.2.2.1.cmml" xref="S3.E1.m1.5.5.2.2.2.2.1"></times><ci id="S3.E1.m1.5.5.2.2.2.2.2.cmml" xref="S3.E1.m1.5.5.2.2.2.2.2">𝑔</ci><ci id="S3.E1.m1.5.5.2.2.2.2.3.cmml" xref="S3.E1.m1.5.5.2.2.2.2.3">𝑟</ci><ci id="S3.E1.m1.5.5.2.2.2.2.4.cmml" xref="S3.E1.m1.5.5.2.2.2.2.4">𝑜</ci><ci id="S3.E1.m1.5.5.2.2.2.2.5.cmml" xref="S3.E1.m1.5.5.2.2.2.2.5">𝑢</ci><ci id="S3.E1.m1.5.5.2.2.2.2.6.cmml" xref="S3.E1.m1.5.5.2.2.2.2.6">𝑛</ci><ci id="S3.E1.m1.5.5.2.2.2.2.7.cmml" xref="S3.E1.m1.5.5.2.2.2.2.7">𝑑</ci><ci id="S3.E1.m1.5.5.2.2.2.2.8.cmml" xref="S3.E1.m1.5.5.2.2.2.2.8">_</ci><ci id="S3.E1.m1.5.5.2.2.2.2.9.cmml" xref="S3.E1.m1.5.5.2.2.2.2.9">𝑡</ci><ci id="S3.E1.m1.5.5.2.2.2.2.10.cmml" xref="S3.E1.m1.5.5.2.2.2.2.10">𝑟</ci><ci id="S3.E1.m1.5.5.2.2.2.2.11.cmml" xref="S3.E1.m1.5.5.2.2.2.2.11">𝑢</ci><ci id="S3.E1.m1.5.5.2.2.2.2.12.cmml" xref="S3.E1.m1.5.5.2.2.2.2.12">𝑡</ci><ci id="S3.E1.m1.5.5.2.2.2.2.13.cmml" xref="S3.E1.m1.5.5.2.2.2.2.13">ℎ</ci><apply id="S3.E1.m1.5.5.2.2.2.2.14.1.cmml" xref="S3.E1.m1.5.5.2.2.2.2.14.2"><csymbol cd="latexml" id="S3.E1.m1.5.5.2.2.2.2.14.1.1.cmml" xref="S3.E1.m1.5.5.2.2.2.2.14.2.1">delimited-[]</csymbol><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑗</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">S=\max_{j\in\{0,1\}}F_{Judge}(pred,ground\_truth[j])</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.1" class="ltx_p">The architecture of the classifier is a DeBERTa-V3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> language model, fine-tuned with LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The classification score is predicted using a linear head on top of the class token output. We fine-tuned the model on a diverse dataset of model predictions from early experiments, where questions and ground truth answers come from our evaluation dataset and the correctness target is labeled by human annotators. On top of this initial dataset, we iteratively improved the classifier using active learning by correcting the wrong predictions of discarded models and adding corrections to the training dataset. On a held-out test set, we find that the binary classification accuracy of the classifier is 95%.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p4" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p4.1" class="ltx_p">In comparison to metrics such as CIDEr, which provide a <span id="S3.SS2.SSS0.Px2.p4.1.1" class="ltx_text ltx_font_italic">system-level</span> performance metric, the classifier provides a probability of correctness for each of the model predictions, meaning that it provides metrics at the <span id="S3.SS2.SSS0.Px2.p4.1.2" class="ltx_text ltx_font_italic">sample</span> level. Examples are provided in the Appendix <a href="#A2" title="Appendix B Lingo-Judge Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. This means that 100% classifier accuracy is easy to interpret. The classifier allows us to compute metrics during training, running over our full evaluation dataset in 10 seconds using an A100 GPU.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Correlation to human ratings</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">We studied empirical <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">correlation</span> of various metrics with human judgments. Several human annotators assigned a scalar score [0, 1] to the inference outputs of 17 different models which can be interpreted as the probability that the response correctly addresses the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Notably, this process takes several days, highlighting the need for an automated evaluation metric that provides faster development feedback. The final human score of each model is the average of all inference output scores. Further details regarding the methodology for the correlation analysis are in the Appendix <a href="#A4" title="Appendix D Lingo-Judge Correlation Study ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p2.1" class="ltx_p">The <span id="S3.SS2.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_italic">Spearman rank correlation</span> coefficient of our automated metric, Lingo-Judge, with human scores is 0.95, and the <span id="S3.SS2.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_italic">Pearson correlation</span> coefficient is 0.993. These values are notably higher compared to other natural language evaluation metrics and GPT-4, as detailed in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Evaluation Metric ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our analysis demonstrates that Lingo-Judge accurately mirrors human judgments, outperforming existing metrics such as BLEU, METEOR, and CIDEr, as well as GPT-4 with and without chain-of-thoughts prompting. This indicates that Lingo-Judge can effectively serve as a proxy for human labelling, which is particularly significant given the stagnant nature of metrics in autonomous driving since the introduction of the CIDEr metric in 2015. Notably, despite their limitations, prominent models like ADAPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and DriveGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> still use BLEU, METEOR, and CIDEr metrics and report ChatGPT ratings without analyzing their correlation to human preferences. Our work fills this gap by providing a reliable benchmark that better reflects human preferences.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:120.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.4pt,-24.3pt) scale(1.67492447500284,1.67492447500284) ;">
<table id="S3.T2.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.3.4" class="ltx_tr">
<td id="S3.T2.3.3.4.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.3.3.4.2.1" class="ltx_text ltx_font_bold">Scenarios</span></td>
<td id="S3.T2.3.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.3.3.4.3.1" class="ltx_text ltx_font_bold">QA pairs</span></td>
<td id="S3.T2.3.3.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.3.3.4.4.1" class="ltx_text ltx_font_bold">QA per scenario</span></td>
</tr>
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Action</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.5k</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">267.8k</td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\approx 10.9" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.1.m1.1.1.2.cmml"></mi><mo id="S3.T2.1.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.1.cmml">≈</mo><mn id="S3.T2.1.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.1.m1.1.1.3.cmml">10.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1"><approx id="S3.T2.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S3.T2.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.1.m1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.1.m1.1.1.3">10.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\approx 10.9</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Scenery</td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">3.5k</td>
<td id="S3.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">152.5k</td>
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\approx 43.6" display="inline"><semantics id="S3.T2.2.2.2.1.m1.1a"><mrow id="S3.T2.2.2.2.1.m1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.cmml"><mi id="S3.T2.2.2.2.1.m1.1.1.2" xref="S3.T2.2.2.2.1.m1.1.1.2.cmml"></mi><mo id="S3.T2.2.2.2.1.m1.1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.1.cmml">≈</mo><mn id="S3.T2.2.2.2.1.m1.1.1.3" xref="S3.T2.2.2.2.1.m1.1.1.3.cmml">43.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><apply id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1"><approx id="S3.T2.2.2.2.1.m1.1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S3.T2.2.2.2.1.m1.1.1.2.cmml" xref="S3.T2.2.2.2.1.m1.1.1.2">absent</csymbol><cn type="float" id="S3.T2.2.2.2.1.m1.1.1.3.cmml" xref="S3.T2.2.2.2.1.m1.1.1.3">43.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\approx 43.6</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.3.3.3" class="ltx_tr">
<td id="S3.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Eval. Dataset</td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">100</td>
<td id="S3.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">1000</td>
<td id="S3.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math id="S3.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.T2.3.3.3.1.m1.1a"><mn id="S3.T2.3.3.3.1.m1.1.1" xref="S3.T2.3.3.3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.1.m1.1b"><cn type="integer" id="S3.T2.3.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.1.m1.1c">10</annotation></semantics></math></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.8.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.9.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset Split.<span id="S3.T2.9.2.1" class="ltx_text ltx_font_medium"> It consists of three different datasets of varying annotation densities. The <span id="S3.T2.9.2.1.1" class="ltx_text ltx_font_italic">Action</span> dataset focuses on questions related to driving behaviours, the <span id="S3.T2.9.2.1.2" class="ltx_text ltx_font_italic">Scenery</span> dataset focuses on perception capabilities, while the evaluation dataset is designed to probe a range of competencies.</span></span></figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.2.2.1" class="ltx_text ltx_font_bold">Scenarios</span></td>
<td id="S3.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.2.3.1" class="ltx_text ltx_font_bold">Annotations</span></td>
<td id="S3.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.2.4.1" class="ltx_text ltx_font_bold">QA</span></td>
<td id="S3.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.2.5.1" class="ltx_text ltx_font_bold">Captioning</span></td>
<td id="S3.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.2.6.1" class="ltx_text ltx_font_bold">Video length [sec]</span></td>
</tr>
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Rank2Tell <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">118</td>
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="&gt;118" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2.cmml"></mi><mo id="S3.T3.1.1.1.m1.1.1.1" xref="S3.T3.1.1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S3.T3.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.m1.1.1.3.cmml">118</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><gt id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.T3.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.T3.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.m1.1.1.3">118</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">&gt;118</annotation></semantics></math></td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S3.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">BDD-OIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_center ltx_border_r">22.9k</td>
<td id="S3.T3.1.3.3" class="ltx_td ltx_align_center ltx_border_r">35k</td>
<td id="S3.T3.1.3.4" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S3.T3.1.3.5" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S3.T3.1.3.6" class="ltx_td ltx_align_center ltx_border_r">5</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">BDD-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_r">6.9k</td>
<td id="S3.T3.1.4.3" class="ltx_td ltx_align_center ltx_border_r">26k</td>
<td id="S3.T3.1.4.4" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S3.T3.1.4.5" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S3.T3.1.4.6" class="ltx_td ltx_align_center ltx_border_r">40</td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">NuScenesQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_r">34k</td>
<td id="S3.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_r">460k</td>
<td id="S3.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S3.T3.1.5.5" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S3.T3.1.5.6" class="ltx_td ltx_align_center ltx_border_r">20</td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">DriveLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_center ltx_border_r">30k</td>
<td id="S3.T3.1.6.3" class="ltx_td ltx_align_center ltx_border_r">360k</td>
<td id="S3.T3.1.6.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S3.T3.1.6.5" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S3.T3.1.6.6" class="ltx_td ltx_align_center ltx_border_r">20</td>
</tr>
<tr id="S3.T3.1.7" class="ltx_tr">
<td id="S3.T3.1.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T3.1.7.1.1" class="ltx_text ltx_font_bold">LingoQA</span></td>
<td id="S3.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">28k</td>
<td id="S3.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">419.9k</td>
<td id="S3.T3.1.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✓</td>
<td id="S3.T3.1.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✓</td>
<td id="S3.T3.1.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset Features.<span id="S3.T3.5.2.1" class="ltx_text ltx_font_medium"> The dataset that we introduce alongside our benchmark consists of questions related to object presence, as well as action, justification, attention, localisation, counting, anticipation, and counterfactuals. In total, it has a similar size to other driving-related datasets such as NuScenesQA, while having a much higher diversity and not being limited to questions related to object positioning.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Datasets</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2312.14115/assets/img/datasets_stats_one_line.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset Statistics.<span id="S3.F2.6.2.1" class="ltx_text ltx_font_medium"> Dataset split by the number of question-answer pairs for the competencies covered and for the objects referred. One question-answer pair might cover more than one competency or object, hence the total is higher than the size of the datasets. The <span id="S3.F2.6.2.1.1" class="ltx_text ltx_font_italic">Action</span> and <span id="S3.F2.6.2.1.2" class="ltx_text ltx_font_italic">Scenery</span> datasets have complementary strengths, with one focused more on action-justification competencies and one more on description and localisation.</span></span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We created a collection of datasets for explainable driving. The total dataset size is 419.9k question-answer pairs, where a single data sample consists of a 4-second video clip at 1Hz. The total size of the dataset is about 10x larger than BDD-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, as shown in Table <a href="#S3.T3" title="Table 3 ‣ Correlation to human ratings ‣ 3.2 Evaluation Metric ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Compared to prior datasets such as NuScenesQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, our dataset contains reasoning pairs in addition to object presence, description, and localisation. The answers are also more free-form and more complex, with an average answer length of 17.2 words versus 1.0 words in NuScenesQA. Examples of question answers pairs from LingoQA are shown in Appendix <a href="#A1" title="Appendix A LingoQA Dataset Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Our labeled autonomous driving training dataset consists of two complementary parts: the <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">action</span> dataset and the <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">scenery</span> dataset.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Action dataset.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">This dataset was made from a recorded driving corpus of interesting events where the car’s behavior changes, such as decelerations, accelerations, lane changes, narrow gaps, and turns. Such events were succinctly labeled by driving operators with very short high-level descriptions of the situations and behavioral policies (e.g. “following lane, pedestrian on a zebra crossing, should stop”). Additionally, we added metadata for such events from various perception systems, such as traffic light presence, vehicles and pedestrian visual detectors, weather descriptors, as well as other metadata (speed, steering wheel position, and road type from the map data). Using this data, we developed prompt templates for (1) describing the current action and its justification and (2) a set of example questions and hints about what the answer should mention. Next, we used those prompts with GPT-3.5 to rephrase, answer, and extend the example questions using the provided action description and answer hints. We rebalanced events by bucketing by actions and behavioral policies and sampled up to 500 events from each bucket without replacement, leading to 24,577 video snippets with 167,774 question/answer pairs.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scenery dataset</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">The scenery dataset was built to complement the action dataset by focusing on perception-related question in addition to driving behaviours. The dataset was made by densely and thoroughly labelling three 30-minute driving sessions with the ELAN video annotation software<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. For the entire duration of the driving sessions, we provided short captions in about 15 different categories:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Driver’s actions, and their justifications</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Driver’s attention</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Observations about relevant vehicles, pedestrians, and other road actors with their visual descriptions</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Observations about relevant static road elements such as traffic lights, traffic islands, lane and intersection structures</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Miscellaneous observations about the environment, such as weather, tube stations, and buildings.</p>
</div>
</li>
</ul>
<p id="S3.SS3.SSS0.Px2.p1.2" class="ltx_p">Then, for every keyframe every second (1fps), we collect all annotations around this frame and build a textual description containing the driver’s actions and their justifications, the objects requiring the driver’s attention, and the observations. As opposed to the Action dataset, where recommended questions were provided to GPT-3.5 for rephasing, for the Scenery dataset, we asked GPT-4 to generate questions and answers using a set of generic prompts, but also using a prompt with the chain of thought specifically targeting perception questions. This forced GPT-4 to generate many diverse questions and answers. This led to a high-quality diverse dataset with about 43 QA-pairs per video.</p>
</div>
<div id="S3.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p2.1" class="ltx_p">Our training dataset covers 9 different competencies: action (what the vehicle is doing), justification (why the action is taken), attention (what should be paid attention to in the current situation), identification (identifying an object given its description), localisation, description, counting, anticipation and reasoning given counterfactuals. The questions also cover a diverse set of objects, such as pedestrians, vehicles, cyclists, buildings, road infrastructure, signs, markings.
In Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Datasets ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present the number of question and answer pairs for each of the 9 competencies above, as well as for the referred objects, for our two datasets, namely Action and Scenery. The complementary strengths of the datasets are apparent, with one focused on driving behaviours and one on perception tasks.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Model Methodology</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose LingoQA Baseline, a vision language model for autonomous driving based on Vicuna v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> with 7B parameters that takes explainability beyond simple image captioning and can answer reasoning questions grounded in video outputs. We train a model that consumes a short video segment and produces answers to autonomous driving-related questions.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Architecture</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The LingoQA Baseline model architecture is based on recent VLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> but enhances them by incorporating a video encoding strategy to process multiple frames from a video snippet, as is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Architecture ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2312.14115/assets/img/lingo_arch_overview.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">LingoQA Baseline model architecture.<span id="S4.F3.4.2.1" class="ltx_text ltx_font_medium"> We first encode individual frames using CLIP and Q-Former. The Q-Former outputs tokens and we feed the tokens from all frames along with chat history and questions into the LLM, which then predicts an answer.</span></span></figcaption>
</figure>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Vision encoder.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.2" class="ltx_p">We use CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, a Vision Transformer (ViT) pre-trained contrastively on image-language pairs, to encode images into features. The inputs to the vision encoder are RGB images from the front camera. We squash the input images to a size of <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">224\times 224</annotation></semantics></math> as opposed to cropping them in order to keep the full image context. Subsequently, we pass the features through a transformer network, the Querying Transformer (Q-Former), that akin to BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> acts as a bridge between the vision and language feature spaces. The embeddings are then projected into the large language model (LLM) space using a linear projection layer. We repeat this process for <math id="S4.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="T=5" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">T</mi><mo id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1"><eq id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝑇</ci><cn type="integer" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.1c">T=5</annotation></semantics></math> frames of the input video and concatenate the tokens from each image.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Large language model.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We leverage pretrained LLMs to give LingoQA Baseline the ability to answer general questions related to both driving scenes, as well as general knowledge. We use Vicuna v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> with 7B parameters built on top of Llama-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. The language model is auto-regressive and hence can be conditioned on textual inputs, as well as image tokens. The training objective is to predict the next language token in a sequence. We mask all tokens from the training loss that belong to the text prompt, including question and chat history.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Recipe</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our training uses a two-step approach to better utilise video features and improve learning in answering questions based on video data. Through this two-step training, we aim for a better understanding and use of video data in addressing challenges in autonomous driving. During both stages, we train only the attention layers of both CLIP and the large language model, as well as the parameters of the Q-Former and language projection layer, while keeping all other parameters frozen. Further details regarding training parameters are presented in Appendix <a href="#A5" title="Appendix E Training Parameters ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure id="S4.SS2.56" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.SS2.56.56" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S4.SS2.4.4.4" class="ltx_tr">
<td id="S4.SS2.4.4.4.5" class="ltx_td ltx_border_tt"></td>
<td id="S4.SS2.4.4.4.6" class="ltx_td ltx_align_left ltx_border_tt">Ablation</td>
<td id="S4.SS2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Lingo-Judge [%] <math id="S4.SS2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.SS2.1.1.1.1.m1.1.1" xref="S4.SS2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.1.1.1.1.m1.1b"><ci id="S4.SS2.1.1.1.1.m1.1.1.cmml" xref="S4.SS2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.SS2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">BLEU <math id="S4.SS2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.SS2.2.2.2.2.m1.1.1" xref="S4.SS2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.2.2.2.2.m1.1b"><ci id="S4.SS2.2.2.2.2.m1.1.1.cmml" xref="S4.SS2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.SS2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">METEOR <math id="S4.SS2.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS2.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.SS2.3.3.3.3.m1.1.1" xref="S4.SS2.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.3.3.3.3.m1.1b"><ci id="S4.SS2.3.3.3.3.m1.1.1.cmml" xref="S4.SS2.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.SS2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">CIDEr <math id="S4.SS2.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS2.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.SS2.4.4.4.4.m1.1.1" xref="S4.SS2.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.4.4.4.4.m1.1b"><ci id="S4.SS2.4.4.4.4.m1.1.1.cmml" xref="S4.SS2.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.SS2.8.8.8" class="ltx_tr">
<td id="S4.SS2.8.8.8.5" class="ltx_td ltx_border_t"></td>
<td id="S4.SS2.8.8.8.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.SS2.8.8.8.6.1" class="ltx_text ltx_font_bold">LingoQA Baseline</span></td>
<td id="S4.SS2.5.5.5.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.SS2.5.5.5.1.1" class="ltx_text ltx_markedasmath ltx_font_bold">60.80</span></td>
<td id="S4.SS2.6.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.SS2.6.6.6.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">15.00</span></td>
<td id="S4.SS2.7.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.7.7.7.3.m1.1" class="ltx_Math" alttext="18.56" display="inline"><semantics id="S4.SS2.7.7.7.3.m1.1a"><mn id="S4.SS2.7.7.7.3.m1.1.1" xref="S4.SS2.7.7.7.3.m1.1.1.cmml">18.56</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.7.7.7.3.m1.1b"><cn type="float" id="S4.SS2.7.7.7.3.m1.1.1.cmml" xref="S4.SS2.7.7.7.3.m1.1.1">18.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.7.7.7.3.m1.1c">18.56</annotation></semantics></math></td>
<td id="S4.SS2.8.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.SS2.8.8.8.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">65.61</span></td>
</tr>
<tr id="S4.SS2.12.12.12" class="ltx_tr">
<td id="S4.SS2.12.12.12.5" class="ltx_td ltx_align_left ltx_border_t" rowspan="2">
<span id="S4.SS2.12.12.12.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Training recipe</span>
Instead of pre-train and fine-tune</td>
<td id="S4.SS2.12.12.12.6" class="ltx_td ltx_align_left ltx_border_t">No fine-tuning</td>
<td id="S4.SS2.9.9.9.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.9.9.9.1.m1.1" class="ltx_Math" alttext="33.60" display="inline"><semantics id="S4.SS2.9.9.9.1.m1.1a"><mn id="S4.SS2.9.9.9.1.m1.1.1" xref="S4.SS2.9.9.9.1.m1.1.1.cmml">33.60</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.9.9.9.1.m1.1b"><cn type="float" id="S4.SS2.9.9.9.1.m1.1.1.cmml" xref="S4.SS2.9.9.9.1.m1.1.1">33.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.9.9.9.1.m1.1c">33.60</annotation></semantics></math></td>
<td id="S4.SS2.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.10.10.10.2.m1.1" class="ltx_Math" alttext="8.33" display="inline"><semantics id="S4.SS2.10.10.10.2.m1.1a"><mn id="S4.SS2.10.10.10.2.m1.1.1" xref="S4.SS2.10.10.10.2.m1.1.1.cmml">8.33</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.10.10.10.2.m1.1b"><cn type="float" id="S4.SS2.10.10.10.2.m1.1.1.cmml" xref="S4.SS2.10.10.10.2.m1.1.1">8.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.10.10.10.2.m1.1c">8.33</annotation></semantics></math></td>
<td id="S4.SS2.11.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.11.11.11.3.m1.1" class="ltx_Math" alttext="14.33" display="inline"><semantics id="S4.SS2.11.11.11.3.m1.1a"><mn id="S4.SS2.11.11.11.3.m1.1.1" xref="S4.SS2.11.11.11.3.m1.1.1.cmml">14.33</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.11.11.11.3.m1.1b"><cn type="float" id="S4.SS2.11.11.11.3.m1.1.1.cmml" xref="S4.SS2.11.11.11.3.m1.1.1">14.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.11.11.11.3.m1.1c">14.33</annotation></semantics></math></td>
<td id="S4.SS2.12.12.12.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.12.12.12.4.m1.1" class="ltx_Math" alttext="39.16" display="inline"><semantics id="S4.SS2.12.12.12.4.m1.1a"><mn id="S4.SS2.12.12.12.4.m1.1.1" xref="S4.SS2.12.12.12.4.m1.1.1.cmml">39.16</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.12.12.12.4.m1.1b"><cn type="float" id="S4.SS2.12.12.12.4.m1.1.1.cmml" xref="S4.SS2.12.12.12.4.m1.1.1">39.16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.12.12.12.4.m1.1c">39.16</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.16.16.16" class="ltx_tr">
<td id="S4.SS2.16.16.16.5" class="ltx_td ltx_align_left">No pre-training</td>
<td id="S4.SS2.13.13.13.1" class="ltx_td ltx_align_center"><math id="S4.SS2.13.13.13.1.m1.1" class="ltx_Math" alttext="56.60" display="inline"><semantics id="S4.SS2.13.13.13.1.m1.1a"><mn id="S4.SS2.13.13.13.1.m1.1.1" xref="S4.SS2.13.13.13.1.m1.1.1.cmml">56.60</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.13.13.13.1.m1.1b"><cn type="float" id="S4.SS2.13.13.13.1.m1.1.1.cmml" xref="S4.SS2.13.13.13.1.m1.1.1">56.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.13.13.13.1.m1.1c">56.60</annotation></semantics></math></td>
<td id="S4.SS2.14.14.14.2" class="ltx_td ltx_align_center"><math id="S4.SS2.14.14.14.2.m1.1" class="ltx_Math" alttext="13.53" display="inline"><semantics id="S4.SS2.14.14.14.2.m1.1a"><mn id="S4.SS2.14.14.14.2.m1.1.1" xref="S4.SS2.14.14.14.2.m1.1.1.cmml">13.53</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.14.14.14.2.m1.1b"><cn type="float" id="S4.SS2.14.14.14.2.m1.1.1.cmml" xref="S4.SS2.14.14.14.2.m1.1.1">13.53</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.14.14.14.2.m1.1c">13.53</annotation></semantics></math></td>
<td id="S4.SS2.15.15.15.3" class="ltx_td ltx_align_center"><math id="S4.SS2.15.15.15.3.m1.1" class="ltx_Math" alttext="17.91" display="inline"><semantics id="S4.SS2.15.15.15.3.m1.1a"><mn id="S4.SS2.15.15.15.3.m1.1.1" xref="S4.SS2.15.15.15.3.m1.1.1.cmml">17.91</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.15.15.15.3.m1.1b"><cn type="float" id="S4.SS2.15.15.15.3.m1.1.1.cmml" xref="S4.SS2.15.15.15.3.m1.1.1">17.91</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.15.15.15.3.m1.1c">17.91</annotation></semantics></math></td>
<td id="S4.SS2.16.16.16.4" class="ltx_td ltx_align_center"><math id="S4.SS2.16.16.16.4.m1.1" class="ltx_Math" alttext="57.98" display="inline"><semantics id="S4.SS2.16.16.16.4.m1.1a"><mn id="S4.SS2.16.16.16.4.m1.1.1" xref="S4.SS2.16.16.16.4.m1.1.1.cmml">57.98</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.16.16.16.4.m1.1b"><cn type="float" id="S4.SS2.16.16.16.4.m1.1.1.cmml" xref="S4.SS2.16.16.16.4.m1.1.1">57.98</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.16.16.16.4.m1.1c">57.98</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.20.20.20" class="ltx_tr">
<td id="S4.SS2.20.20.20.5" class="ltx_td ltx_align_left ltx_border_t" rowspan="2">
<span id="S4.SS2.20.20.20.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Fine-tuning dataset</span>
Instead of action and scenery</td>
<td id="S4.SS2.20.20.20.6" class="ltx_td ltx_align_left ltx_border_t">Action only</td>
<td id="S4.SS2.17.17.17.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.17.17.17.1.m1.1" class="ltx_Math" alttext="53.80" display="inline"><semantics id="S4.SS2.17.17.17.1.m1.1a"><mn id="S4.SS2.17.17.17.1.m1.1.1" xref="S4.SS2.17.17.17.1.m1.1.1.cmml">53.80</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.17.17.17.1.m1.1b"><cn type="float" id="S4.SS2.17.17.17.1.m1.1.1.cmml" xref="S4.SS2.17.17.17.1.m1.1.1">53.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.17.17.17.1.m1.1c">53.80</annotation></semantics></math></td>
<td id="S4.SS2.18.18.18.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.18.18.18.2.m1.1" class="ltx_Math" alttext="11.65" display="inline"><semantics id="S4.SS2.18.18.18.2.m1.1a"><mn id="S4.SS2.18.18.18.2.m1.1.1" xref="S4.SS2.18.18.18.2.m1.1.1.cmml">11.65</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.18.18.18.2.m1.1b"><cn type="float" id="S4.SS2.18.18.18.2.m1.1.1.cmml" xref="S4.SS2.18.18.18.2.m1.1.1">11.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.18.18.18.2.m1.1c">11.65</annotation></semantics></math></td>
<td id="S4.SS2.19.19.19.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.19.19.19.3.m1.1" class="ltx_Math" alttext="17.68" display="inline"><semantics id="S4.SS2.19.19.19.3.m1.1a"><mn id="S4.SS2.19.19.19.3.m1.1.1" xref="S4.SS2.19.19.19.3.m1.1.1.cmml">17.68</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.19.19.19.3.m1.1b"><cn type="float" id="S4.SS2.19.19.19.3.m1.1.1.cmml" xref="S4.SS2.19.19.19.3.m1.1.1">17.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.19.19.19.3.m1.1c">17.68</annotation></semantics></math></td>
<td id="S4.SS2.20.20.20.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.20.20.20.4.m1.1" class="ltx_Math" alttext="46.50" display="inline"><semantics id="S4.SS2.20.20.20.4.m1.1a"><mn id="S4.SS2.20.20.20.4.m1.1.1" xref="S4.SS2.20.20.20.4.m1.1.1.cmml">46.50</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.20.20.20.4.m1.1b"><cn type="float" id="S4.SS2.20.20.20.4.m1.1.1.cmml" xref="S4.SS2.20.20.20.4.m1.1.1">46.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.20.20.20.4.m1.1c">46.50</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.24.24.24" class="ltx_tr">
<td id="S4.SS2.24.24.24.5" class="ltx_td ltx_align_left">Scenery only</td>
<td id="S4.SS2.21.21.21.1" class="ltx_td ltx_align_center"><math id="S4.SS2.21.21.21.1.m1.1" class="ltx_Math" alttext="55.40" display="inline"><semantics id="S4.SS2.21.21.21.1.m1.1a"><mn id="S4.SS2.21.21.21.1.m1.1.1" xref="S4.SS2.21.21.21.1.m1.1.1.cmml">55.40</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.21.21.21.1.m1.1b"><cn type="float" id="S4.SS2.21.21.21.1.m1.1.1.cmml" xref="S4.SS2.21.21.21.1.m1.1.1">55.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.21.21.21.1.m1.1c">55.40</annotation></semantics></math></td>
<td id="S4.SS2.22.22.22.2" class="ltx_td ltx_align_center"><math id="S4.SS2.22.22.22.2.m1.1" class="ltx_Math" alttext="13.00" display="inline"><semantics id="S4.SS2.22.22.22.2.m1.1a"><mn id="S4.SS2.22.22.22.2.m1.1.1" xref="S4.SS2.22.22.22.2.m1.1.1.cmml">13.00</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.22.22.22.2.m1.1b"><cn type="float" id="S4.SS2.22.22.22.2.m1.1.1.cmml" xref="S4.SS2.22.22.22.2.m1.1.1">13.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.22.22.22.2.m1.1c">13.00</annotation></semantics></math></td>
<td id="S4.SS2.23.23.23.3" class="ltx_td ltx_align_center"><math id="S4.SS2.23.23.23.3.m1.1" class="ltx_Math" alttext="18.38" display="inline"><semantics id="S4.SS2.23.23.23.3.m1.1a"><mn id="S4.SS2.23.23.23.3.m1.1.1" xref="S4.SS2.23.23.23.3.m1.1.1.cmml">18.38</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.23.23.23.3.m1.1b"><cn type="float" id="S4.SS2.23.23.23.3.m1.1.1.cmml" xref="S4.SS2.23.23.23.3.m1.1.1">18.38</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.23.23.23.3.m1.1c">18.38</annotation></semantics></math></td>
<td id="S4.SS2.24.24.24.4" class="ltx_td ltx_align_center"><math id="S4.SS2.24.24.24.4.m1.1" class="ltx_Math" alttext="55.88" display="inline"><semantics id="S4.SS2.24.24.24.4.m1.1a"><mn id="S4.SS2.24.24.24.4.m1.1.1" xref="S4.SS2.24.24.24.4.m1.1.1.cmml">55.88</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.24.24.24.4.m1.1b"><cn type="float" id="S4.SS2.24.24.24.4.m1.1.1.cmml" xref="S4.SS2.24.24.24.4.m1.1.1">55.88</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.24.24.24.4.m1.1c">55.88</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.28.28.28" class="ltx_tr">
<td id="S4.SS2.28.28.28.5" class="ltx_td ltx_align_left ltx_border_t" rowspan="3">
<span id="S4.SS2.28.28.28.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Frame count</span>
Instead of 5 frames</td>
<td id="S4.SS2.28.28.28.6" class="ltx_td ltx_align_left ltx_border_t">Single frame</td>
<td id="S4.SS2.25.25.25.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.25.25.25.1.m1.1" class="ltx_Math" alttext="57.00" display="inline"><semantics id="S4.SS2.25.25.25.1.m1.1a"><mn id="S4.SS2.25.25.25.1.m1.1.1" xref="S4.SS2.25.25.25.1.m1.1.1.cmml">57.00</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.25.25.25.1.m1.1b"><cn type="float" id="S4.SS2.25.25.25.1.m1.1.1.cmml" xref="S4.SS2.25.25.25.1.m1.1.1">57.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.25.25.25.1.m1.1c">57.00</annotation></semantics></math></td>
<td id="S4.SS2.26.26.26.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.26.26.26.2.m1.1" class="ltx_Math" alttext="14.21" display="inline"><semantics id="S4.SS2.26.26.26.2.m1.1a"><mn id="S4.SS2.26.26.26.2.m1.1.1" xref="S4.SS2.26.26.26.2.m1.1.1.cmml">14.21</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.26.26.26.2.m1.1b"><cn type="float" id="S4.SS2.26.26.26.2.m1.1.1.cmml" xref="S4.SS2.26.26.26.2.m1.1.1">14.21</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.26.26.26.2.m1.1c">14.21</annotation></semantics></math></td>
<td id="S4.SS2.27.27.27.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.27.27.27.3.m1.1" class="ltx_Math" alttext="18.40" display="inline"><semantics id="S4.SS2.27.27.27.3.m1.1a"><mn id="S4.SS2.27.27.27.3.m1.1.1" xref="S4.SS2.27.27.27.3.m1.1.1.cmml">18.40</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.27.27.27.3.m1.1b"><cn type="float" id="S4.SS2.27.27.27.3.m1.1.1.cmml" xref="S4.SS2.27.27.27.3.m1.1.1">18.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.27.27.27.3.m1.1c">18.40</annotation></semantics></math></td>
<td id="S4.SS2.28.28.28.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.28.28.28.4.m1.1" class="ltx_Math" alttext="59.46" display="inline"><semantics id="S4.SS2.28.28.28.4.m1.1a"><mn id="S4.SS2.28.28.28.4.m1.1.1" xref="S4.SS2.28.28.28.4.m1.1.1.cmml">59.46</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.28.28.28.4.m1.1b"><cn type="float" id="S4.SS2.28.28.28.4.m1.1.1.cmml" xref="S4.SS2.28.28.28.4.m1.1.1">59.46</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.28.28.28.4.m1.1c">59.46</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.32.32.32" class="ltx_tr">
<td id="S4.SS2.32.32.32.5" class="ltx_td ltx_align_left">3 frames</td>
<td id="S4.SS2.29.29.29.1" class="ltx_td ltx_align_center"><math id="S4.SS2.29.29.29.1.m1.1" class="ltx_Math" alttext="59.80" display="inline"><semantics id="S4.SS2.29.29.29.1.m1.1a"><mn id="S4.SS2.29.29.29.1.m1.1.1" xref="S4.SS2.29.29.29.1.m1.1.1.cmml">59.80</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.29.29.29.1.m1.1b"><cn type="float" id="S4.SS2.29.29.29.1.m1.1.1.cmml" xref="S4.SS2.29.29.29.1.m1.1.1">59.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.29.29.29.1.m1.1c">59.80</annotation></semantics></math></td>
<td id="S4.SS2.30.30.30.2" class="ltx_td ltx_align_center"><math id="S4.SS2.30.30.30.2.m1.1" class="ltx_Math" alttext="14.61" display="inline"><semantics id="S4.SS2.30.30.30.2.m1.1a"><mn id="S4.SS2.30.30.30.2.m1.1.1" xref="S4.SS2.30.30.30.2.m1.1.1.cmml">14.61</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.30.30.30.2.m1.1b"><cn type="float" id="S4.SS2.30.30.30.2.m1.1.1.cmml" xref="S4.SS2.30.30.30.2.m1.1.1">14.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.30.30.30.2.m1.1c">14.61</annotation></semantics></math></td>
<td id="S4.SS2.31.31.31.3" class="ltx_td ltx_align_center"><math id="S4.SS2.31.31.31.3.m1.1" class="ltx_Math" alttext="18.44" display="inline"><semantics id="S4.SS2.31.31.31.3.m1.1a"><mn id="S4.SS2.31.31.31.3.m1.1.1" xref="S4.SS2.31.31.31.3.m1.1.1.cmml">18.44</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.31.31.31.3.m1.1b"><cn type="float" id="S4.SS2.31.31.31.3.m1.1.1.cmml" xref="S4.SS2.31.31.31.3.m1.1.1">18.44</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.31.31.31.3.m1.1c">18.44</annotation></semantics></math></td>
<td id="S4.SS2.32.32.32.4" class="ltx_td ltx_align_center"><math id="S4.SS2.32.32.32.4.m1.1" class="ltx_Math" alttext="62.61" display="inline"><semantics id="S4.SS2.32.32.32.4.m1.1a"><mn id="S4.SS2.32.32.32.4.m1.1.1" xref="S4.SS2.32.32.32.4.m1.1.1.cmml">62.61</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.32.32.32.4.m1.1b"><cn type="float" id="S4.SS2.32.32.32.4.m1.1.1.cmml" xref="S4.SS2.32.32.32.4.m1.1.1">62.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.32.32.32.4.m1.1c">62.61</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.36.36.36" class="ltx_tr">
<td id="S4.SS2.36.36.36.5" class="ltx_td ltx_align_left">7 frames</td>
<td id="S4.SS2.33.33.33.1" class="ltx_td ltx_align_center"><math id="S4.SS2.33.33.33.1.m1.1" class="ltx_Math" alttext="60.60" display="inline"><semantics id="S4.SS2.33.33.33.1.m1.1a"><mn id="S4.SS2.33.33.33.1.m1.1.1" xref="S4.SS2.33.33.33.1.m1.1.1.cmml">60.60</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.33.33.33.1.m1.1b"><cn type="float" id="S4.SS2.33.33.33.1.m1.1.1.cmml" xref="S4.SS2.33.33.33.1.m1.1.1">60.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.33.33.33.1.m1.1c">60.60</annotation></semantics></math></td>
<td id="S4.SS2.34.34.34.2" class="ltx_td ltx_align_center"><math id="S4.SS2.34.34.34.2.m1.1" class="ltx_Math" alttext="14.46" display="inline"><semantics id="S4.SS2.34.34.34.2.m1.1a"><mn id="S4.SS2.34.34.34.2.m1.1.1" xref="S4.SS2.34.34.34.2.m1.1.1.cmml">14.46</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.34.34.34.2.m1.1b"><cn type="float" id="S4.SS2.34.34.34.2.m1.1.1.cmml" xref="S4.SS2.34.34.34.2.m1.1.1">14.46</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.34.34.34.2.m1.1c">14.46</annotation></semantics></math></td>
<td id="S4.SS2.35.35.35.3" class="ltx_td ltx_align_center"><span id="S4.SS2.35.35.35.3.1" class="ltx_text ltx_markedasmath ltx_font_bold">18.61</span></td>
<td id="S4.SS2.36.36.36.4" class="ltx_td ltx_align_center"><math id="S4.SS2.36.36.36.4.m1.1" class="ltx_Math" alttext="61.82" display="inline"><semantics id="S4.SS2.36.36.36.4.m1.1a"><mn id="S4.SS2.36.36.36.4.m1.1.1" xref="S4.SS2.36.36.36.4.m1.1.1.cmml">61.82</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.36.36.36.4.m1.1b"><cn type="float" id="S4.SS2.36.36.36.4.m1.1.1.cmml" xref="S4.SS2.36.36.36.4.m1.1.1">61.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.36.36.36.4.m1.1c">61.82</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.40.40.40" class="ltx_tr">
<td id="S4.SS2.40.40.40.5" class="ltx_td ltx_align_left ltx_border_t" rowspan="2">
<span id="S4.SS2.40.40.40.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Video fusion</span>
Instead of late-fusion</td>
<td id="S4.SS2.40.40.40.6" class="ltx_td ltx_align_left ltx_border_t">Early-fusion</td>
<td id="S4.SS2.37.37.37.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.37.37.37.1.m1.1" class="ltx_Math" alttext="48.40" display="inline"><semantics id="S4.SS2.37.37.37.1.m1.1a"><mn id="S4.SS2.37.37.37.1.m1.1.1" xref="S4.SS2.37.37.37.1.m1.1.1.cmml">48.40</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.37.37.37.1.m1.1b"><cn type="float" id="S4.SS2.37.37.37.1.m1.1.1.cmml" xref="S4.SS2.37.37.37.1.m1.1.1">48.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.37.37.37.1.m1.1c">48.40</annotation></semantics></math></td>
<td id="S4.SS2.38.38.38.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.38.38.38.2.m1.1" class="ltx_Math" alttext="13.98" display="inline"><semantics id="S4.SS2.38.38.38.2.m1.1a"><mn id="S4.SS2.38.38.38.2.m1.1.1" xref="S4.SS2.38.38.38.2.m1.1.1.cmml">13.98</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.38.38.38.2.m1.1b"><cn type="float" id="S4.SS2.38.38.38.2.m1.1.1.cmml" xref="S4.SS2.38.38.38.2.m1.1.1">13.98</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.38.38.38.2.m1.1c">13.98</annotation></semantics></math></td>
<td id="S4.SS2.39.39.39.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.39.39.39.3.m1.1" class="ltx_Math" alttext="17.61" display="inline"><semantics id="S4.SS2.39.39.39.3.m1.1a"><mn id="S4.SS2.39.39.39.3.m1.1.1" xref="S4.SS2.39.39.39.3.m1.1.1.cmml">17.61</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.39.39.39.3.m1.1b"><cn type="float" id="S4.SS2.39.39.39.3.m1.1.1.cmml" xref="S4.SS2.39.39.39.3.m1.1.1">17.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.39.39.39.3.m1.1c">17.61</annotation></semantics></math></td>
<td id="S4.SS2.40.40.40.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.40.40.40.4.m1.1" class="ltx_Math" alttext="61.42" display="inline"><semantics id="S4.SS2.40.40.40.4.m1.1a"><mn id="S4.SS2.40.40.40.4.m1.1.1" xref="S4.SS2.40.40.40.4.m1.1.1.cmml">61.42</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.40.40.40.4.m1.1b"><cn type="float" id="S4.SS2.40.40.40.4.m1.1.1.cmml" xref="S4.SS2.40.40.40.4.m1.1.1">61.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.40.40.40.4.m1.1c">61.42</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.44.44.44" class="ltx_tr">
<td id="S4.SS2.44.44.44.5" class="ltx_td ltx_align_left">Mid-fusion</td>
<td id="S4.SS2.41.41.41.1" class="ltx_td ltx_align_center"><math id="S4.SS2.41.41.41.1.m1.1" class="ltx_Math" alttext="59.20" display="inline"><semantics id="S4.SS2.41.41.41.1.m1.1a"><mn id="S4.SS2.41.41.41.1.m1.1.1" xref="S4.SS2.41.41.41.1.m1.1.1.cmml">59.20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.41.41.41.1.m1.1b"><cn type="float" id="S4.SS2.41.41.41.1.m1.1.1.cmml" xref="S4.SS2.41.41.41.1.m1.1.1">59.20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.41.41.41.1.m1.1c">59.20</annotation></semantics></math></td>
<td id="S4.SS2.42.42.42.2" class="ltx_td ltx_align_center"><math id="S4.SS2.42.42.42.2.m1.1" class="ltx_Math" alttext="14.44" display="inline"><semantics id="S4.SS2.42.42.42.2.m1.1a"><mn id="S4.SS2.42.42.42.2.m1.1.1" xref="S4.SS2.42.42.42.2.m1.1.1.cmml">14.44</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.42.42.42.2.m1.1b"><cn type="float" id="S4.SS2.42.42.42.2.m1.1.1.cmml" xref="S4.SS2.42.42.42.2.m1.1.1">14.44</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.42.42.42.2.m1.1c">14.44</annotation></semantics></math></td>
<td id="S4.SS2.43.43.43.3" class="ltx_td ltx_align_center"><math id="S4.SS2.43.43.43.3.m1.1" class="ltx_Math" alttext="18.47" display="inline"><semantics id="S4.SS2.43.43.43.3.m1.1a"><mn id="S4.SS2.43.43.43.3.m1.1.1" xref="S4.SS2.43.43.43.3.m1.1.1.cmml">18.47</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.43.43.43.3.m1.1b"><cn type="float" id="S4.SS2.43.43.43.3.m1.1.1.cmml" xref="S4.SS2.43.43.43.3.m1.1.1">18.47</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.43.43.43.3.m1.1c">18.47</annotation></semantics></math></td>
<td id="S4.SS2.44.44.44.4" class="ltx_td ltx_align_center"><math id="S4.SS2.44.44.44.4.m1.1" class="ltx_Math" alttext="63.05" display="inline"><semantics id="S4.SS2.44.44.44.4.m1.1a"><mn id="S4.SS2.44.44.44.4.m1.1.1" xref="S4.SS2.44.44.44.4.m1.1.1.cmml">63.05</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.44.44.44.4.m1.1b"><cn type="float" id="S4.SS2.44.44.44.4.m1.1.1.cmml" xref="S4.SS2.44.44.44.4.m1.1.1">63.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.44.44.44.4.m1.1c">63.05</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.48.48.48" class="ltx_tr">
<td id="S4.SS2.48.48.48.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="3">
<span id="S4.SS2.48.48.48.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Language model</span>
Instead of Vicuna-1.5-7B<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.SS2.48.48.48.6" class="ltx_td ltx_align_left ltx_border_t">OPT-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.SS2.45.45.45.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.45.45.45.1.m1.1" class="ltx_Math" alttext="50.00" display="inline"><semantics id="S4.SS2.45.45.45.1.m1.1a"><mn id="S4.SS2.45.45.45.1.m1.1.1" xref="S4.SS2.45.45.45.1.m1.1.1.cmml">50.00</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.45.45.45.1.m1.1b"><cn type="float" id="S4.SS2.45.45.45.1.m1.1.1.cmml" xref="S4.SS2.45.45.45.1.m1.1.1">50.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.45.45.45.1.m1.1c">50.00</annotation></semantics></math></td>
<td id="S4.SS2.46.46.46.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.46.46.46.2.m1.1" class="ltx_Math" alttext="14.98" display="inline"><semantics id="S4.SS2.46.46.46.2.m1.1a"><mn id="S4.SS2.46.46.46.2.m1.1.1" xref="S4.SS2.46.46.46.2.m1.1.1.cmml">14.98</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.46.46.46.2.m1.1b"><cn type="float" id="S4.SS2.46.46.46.2.m1.1.1.cmml" xref="S4.SS2.46.46.46.2.m1.1.1">14.98</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.46.46.46.2.m1.1c">14.98</annotation></semantics></math></td>
<td id="S4.SS2.47.47.47.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.47.47.47.3.m1.1" class="ltx_Math" alttext="15.99" display="inline"><semantics id="S4.SS2.47.47.47.3.m1.1a"><mn id="S4.SS2.47.47.47.3.m1.1.1" xref="S4.SS2.47.47.47.3.m1.1.1.cmml">15.99</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.47.47.47.3.m1.1b"><cn type="float" id="S4.SS2.47.47.47.3.m1.1.1.cmml" xref="S4.SS2.47.47.47.3.m1.1.1">15.99</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.47.47.47.3.m1.1c">15.99</annotation></semantics></math></td>
<td id="S4.SS2.48.48.48.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.SS2.48.48.48.4.m1.1" class="ltx_Math" alttext="60.08" display="inline"><semantics id="S4.SS2.48.48.48.4.m1.1a"><mn id="S4.SS2.48.48.48.4.m1.1.1" xref="S4.SS2.48.48.48.4.m1.1.1.cmml">60.08</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.48.48.48.4.m1.1b"><cn type="float" id="S4.SS2.48.48.48.4.m1.1.1.cmml" xref="S4.SS2.48.48.48.4.m1.1.1">60.08</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.48.48.48.4.m1.1c">60.08</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.52.52.52" class="ltx_tr">
<td id="S4.SS2.52.52.52.5" class="ltx_td ltx_align_left">Llama-2-7B-Chat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S4.SS2.49.49.49.1" class="ltx_td ltx_align_center"><math id="S4.SS2.49.49.49.1.m1.1" class="ltx_Math" alttext="59.20" display="inline"><semantics id="S4.SS2.49.49.49.1.m1.1a"><mn id="S4.SS2.49.49.49.1.m1.1.1" xref="S4.SS2.49.49.49.1.m1.1.1.cmml">59.20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.49.49.49.1.m1.1b"><cn type="float" id="S4.SS2.49.49.49.1.m1.1.1.cmml" xref="S4.SS2.49.49.49.1.m1.1.1">59.20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.49.49.49.1.m1.1c">59.20</annotation></semantics></math></td>
<td id="S4.SS2.50.50.50.2" class="ltx_td ltx_align_center"><math id="S4.SS2.50.50.50.2.m1.1" class="ltx_Math" alttext="13.52" display="inline"><semantics id="S4.SS2.50.50.50.2.m1.1a"><mn id="S4.SS2.50.50.50.2.m1.1.1" xref="S4.SS2.50.50.50.2.m1.1.1.cmml">13.52</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.50.50.50.2.m1.1b"><cn type="float" id="S4.SS2.50.50.50.2.m1.1.1.cmml" xref="S4.SS2.50.50.50.2.m1.1.1">13.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.50.50.50.2.m1.1c">13.52</annotation></semantics></math></td>
<td id="S4.SS2.51.51.51.3" class="ltx_td ltx_align_center"><math id="S4.SS2.51.51.51.3.m1.1" class="ltx_Math" alttext="18.43" display="inline"><semantics id="S4.SS2.51.51.51.3.m1.1a"><mn id="S4.SS2.51.51.51.3.m1.1.1" xref="S4.SS2.51.51.51.3.m1.1.1.cmml">18.43</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.51.51.51.3.m1.1b"><cn type="float" id="S4.SS2.51.51.51.3.m1.1.1.cmml" xref="S4.SS2.51.51.51.3.m1.1.1">18.43</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.51.51.51.3.m1.1c">18.43</annotation></semantics></math></td>
<td id="S4.SS2.52.52.52.4" class="ltx_td ltx_align_center"><math id="S4.SS2.52.52.52.4.m1.1" class="ltx_Math" alttext="59.87" display="inline"><semantics id="S4.SS2.52.52.52.4.m1.1a"><mn id="S4.SS2.52.52.52.4.m1.1.1" xref="S4.SS2.52.52.52.4.m1.1.1.cmml">59.87</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.52.52.52.4.m1.1b"><cn type="float" id="S4.SS2.52.52.52.4.m1.1.1.cmml" xref="S4.SS2.52.52.52.4.m1.1.1">59.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.52.52.52.4.m1.1c">59.87</annotation></semantics></math></td>
</tr>
<tr id="S4.SS2.56.56.56" class="ltx_tr">
<td id="S4.SS2.56.56.56.5" class="ltx_td ltx_align_left ltx_border_bb">Mistral-7B-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S4.SS2.53.53.53.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.SS2.53.53.53.1.m1.1" class="ltx_Math" alttext="58.00" display="inline"><semantics id="S4.SS2.53.53.53.1.m1.1a"><mn id="S4.SS2.53.53.53.1.m1.1.1" xref="S4.SS2.53.53.53.1.m1.1.1.cmml">58.00</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.53.53.53.1.m1.1b"><cn type="float" id="S4.SS2.53.53.53.1.m1.1.1.cmml" xref="S4.SS2.53.53.53.1.m1.1.1">58.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.53.53.53.1.m1.1c">58.00</annotation></semantics></math></td>
<td id="S4.SS2.54.54.54.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.SS2.54.54.54.2.m1.1" class="ltx_Math" alttext="13.80" display="inline"><semantics id="S4.SS2.54.54.54.2.m1.1a"><mn id="S4.SS2.54.54.54.2.m1.1.1" xref="S4.SS2.54.54.54.2.m1.1.1.cmml">13.80</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.54.54.54.2.m1.1b"><cn type="float" id="S4.SS2.54.54.54.2.m1.1.1.cmml" xref="S4.SS2.54.54.54.2.m1.1.1">13.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.54.54.54.2.m1.1c">13.80</annotation></semantics></math></td>
<td id="S4.SS2.55.55.55.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.SS2.55.55.55.3.m1.1" class="ltx_Math" alttext="18.33" display="inline"><semantics id="S4.SS2.55.55.55.3.m1.1a"><mn id="S4.SS2.55.55.55.3.m1.1.1" xref="S4.SS2.55.55.55.3.m1.1.1.cmml">18.33</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.55.55.55.3.m1.1b"><cn type="float" id="S4.SS2.55.55.55.3.m1.1.1.cmml" xref="S4.SS2.55.55.55.3.m1.1.1">18.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.55.55.55.3.m1.1c">18.33</annotation></semantics></math></td>
<td id="S4.SS2.56.56.56.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.SS2.56.56.56.4.m1.1" class="ltx_Math" alttext="64.21" display="inline"><semantics id="S4.SS2.56.56.56.4.m1.1a"><mn id="S4.SS2.56.56.56.4.m1.1.1" xref="S4.SS2.56.56.56.4.m1.1.1.cmml">64.21</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.56.56.56.4.m1.1b"><cn type="float" id="S4.SS2.56.56.56.4.m1.1.1.cmml" xref="S4.SS2.56.56.56.4.m1.1.1">64.21</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.56.56.56.4.m1.1c">64.21</annotation></semantics></math></td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.SS2.56.58.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.SS2.56.59.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Empirical Evaluation on LingoQA.<span id="S4.SS2.56.59.2.1" class="ltx_text ltx_font_medium"> Ablation study highlighting the impact of various modifications in training recipes, dataset composition, frame count, video processing techniques and language model.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph ltx_figure_panel">
<h4 class="ltx_title ltx_title_paragraph">Stage 1: Pre-training for feature alignment.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">In the first stage, we pre-train the model on the GQA and SVIT datasets to align image features with the embedding space of the pretrained LLM. The GQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> contains more than 22M questions over 113k images. The recently introduced SVIT dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> contains 4.2M question-answer pairs over 108.1k images. We leverage initial weights from different models to accelerate the training process. We initialise the vision encoder using publicly available weights of OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, the Q-Former from BLIP2 weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, and language model from Vicuna v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stage 2: Fine-tuning for video QA.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">In the second stage, we fine-tune the model on our video question-answering Action and Scenery datasets described in Section <a href="#S3.SS3" title="3.3 Datasets ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. During the fine-tuning phase, each sample is composed of 5 frames taken from a 4-second span of video, accompanied by a QA-pair. To facilitate further exploration of autonomous driving QA, we open-source the dataset used to fine-tune LingoQA Baseline.</p>
</div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Empirical Evaluation on LingoQA</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">With the highly modular architecture of VLMs, the question remains what architectural components of the LingoQA Baseline model and dataset composition contribute the most to its performance? We conduct several ablation studies around the architecture and training paradigm described in Section <a href="#S4" title="4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. To this end, we investigate variations to the <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">training strategy</span>, <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">training data composition</span>, <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">frame count</span>, <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">video fusion methods</span>, and the use of different <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">large language models</span>. The results are obtained by having each model generate one answer per question and then compare the predicted answer to the two ground truth answers. Examples of comparisons between our baseline model’s answers and answers from other models from the ablations are presented in Appendix <a href="#A6" title="Appendix F LingoQA Baseline Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Training Strategy</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The aim of the training strategy experiments is to understand how much the pre-training and the fine-tuning steps contribute to performance. It becomes apparent that fine-tuning is essential to yield answers relevant to autonomous driving. The model fine-tuned on the LingoQA dataset has nearly double the performance of the dataset that is pre-trained on generic VQA datasets. This shows that while generalised pre-training leads to improvements, task-specific fine-tuning is still required for optimal performance.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training Datasets Mixture</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S4.SS2" title="4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> shows the contributions of our two datasets, <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">Action Dataset</span> and the <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">Scenery Dataset</span>. Both datasets proved influential in improving model performance. We show that fine-tuning on the LingoQA dataset that we open source leads to a considerable improvement compared to general pre-training only.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Impact of Frame Count</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We want to investigate the variation in VQA performance with decreasing and increasing the number of video frames fed into the model. The base model contains 5 frames over a 4-second context. The performance declined when shifting from multi-frame video to a single image representation, which can be explained by the model not getting enough information to answer questions where temporal information is crucial. The performance when increasing the number of frames to 3, 5 and 7 remains relatively consistent. We hypothesise this is due to the lack of effective video fusion and leave it to future work to investigate video-language encoders further,</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Impact of Video Fusion Strategy</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Given how crucial temporal context is for scenario understanding, this study explores three methods for integrating video frames into the LLM: <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">early-fusion</span>, <span id="S5.SS4.p1.1.2" class="ltx_text ltx_font_italic">mid-fusion</span>, and <span id="S5.SS4.p1.1.3" class="ltx_text ltx_font_italic">late-fusion</span>. The <span id="S5.SS4.p1.1.4" class="ltx_text ltx_font_italic">early-fusion</span> method employs average pooling to condense features from the vision encoder prior to their incorporation into the Q-Former, producing a unified visual feature vector for language space projection. In contrast, the <span id="S5.SS4.p1.1.5" class="ltx_text ltx_font_italic">mid-fusion</span> approach, merges video features into fixed-size tokens within the Q-Former with the cross-attention mechanism. The <span id="S5.SS4.p1.1.6" class="ltx_text ltx_font_italic">late-fusion</span> method, where individual frame embeddings from Q-Former output are fed into the LLM, allowing it to resolve temporal relationships. Our findings demonstrate that both <span id="S5.SS4.p1.1.7" class="ltx_text ltx_font_italic">mid-fusion</span> and <span id="S5.SS4.p1.1.8" class="ltx_text ltx_font_italic">late-fusion</span> are effective methods for incorporating video content into the model. <span id="S5.SS4.p1.1.9" class="ltx_text ltx_font_italic">Mid-fusion</span> allows for a greater number of context tokens through the use of a predetermined number of video embeddings. Conversely, <span id="S5.SS4.p1.1.10" class="ltx_text ltx_font_italic">late-fusion</span> shows a slightly enhanced performance by providing comprehensive frame information to the LLM.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Impact of Large Language Model</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">We investigate the impact that different Large Language Models have on the overall performance of our vision-language model. As shown in Table <a href="#S4.SS2" title="4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, the best score is achieved by the Vicuna-1.5-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> that our base model uses. In the same model family Llama-2-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> achieves comparable, but slightly lower performance. Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, despite its promise of superior performance over Llama-2, proved less effective in our fine-tuning task. Notably, OPT-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> demonstrates significantly lower performance, despite having a similar number of parameters. This discrepancy underscores the crucial role of the pretraining phase in the base language model’s effectiveness.</p>
</div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations of Lingo-Judge.</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">This work still has a few limitations that we discuss and provide guidelines as to how their effects may be mitigated. The primary limitation of our proposed Lingo-Judge is that we have not studied its ability to <span id="S6.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">generalise</span> to other datasets and to other domains. The classifier has been trained for autonomous driving evaluation and has been shown effective for this purpose. Hence, the classifier can be used for evaluation when paired with the benchmark dataset that we release with the paper but would need to be re-trained if employed on a new dataset. Second, we optimized the classifier to evaluate responses in the style provided by human annotators in the evaluation dataset. The same response style is adopted in the LingoQA training sets and the models. We leave generalization to different answering styles to future work. Notably, Lingo-Judge does rate human labels as highly truthful, providing confidence in its ability to rate typical human answers. Third, as the classifier is only trained to predict factual correctness, it cannot discern which answer of two equally correct answers humans prefer. We leave it to future work to extend the classifier to predict human preference scores directly instead of factual correctness.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Strengths of Lingo-Judge.</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">The strength of our contribution comprises proposing a classifier that is <span id="S6.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">highly correlated</span> with human inputs and <span id="S6.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">efficient</span> to run. In conjunction with the evaluation dataset that we propose, it becomes a useful tool for benchmarking vision-language models for autonomous driving on the video question answering task, which has been historically challenging to evaluate in a consistent fashion. With this contribution, autonomous driving research can be accelerated by providing a reliable, efficient, and easy-to-interpret benchmark.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset and model limitations.</h4>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px3.p1.1" class="ltx_p">One of the primary constraints is that our model operates on relatively short video segments and few frames, limiting the contextual understanding of scenarios. We also do not test for driving decisions and attention mechanisms, focusing on question-answering abilities only. We did not test the scaling in our models and focused on the most practical 7B parameter LLMs only. Our dataset and baseline are limited to information from a single front-facing car camera, excluding additional sensory inputs like LiDAR that could enrich the model’s understanding of the driving environment. Expanding the model to address the short video context, as well as adding action prediction and evaluation to the dataset and the benchmark would result in a more robust and versatile system for autonomous driving.</p>
</div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we introduced a novel benchmark for Video Question Answering for autonomous driving. The benchmark consists of a evaluation dataset, learned classifier-based metric Lingo-Judge that is highly correlated with human evaluation, a comprehensive high-quality training dataset for autonomous driving. The fast feedback from employing Lingo-Judge facilitates effective exploration in the video QA field. Additionally, the comprehensive experiments on different model combinations presented in this paper can become a foundation for further improvement of explainability in end-to-end autonomous driving systems. The LingoQA benchmark is openly released to spur further community research, providing a reliable and highly correlated evaluation method to human ratings.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was possible through the help of many colleagues across Wayve. We would like to thank Elahe Arani for the valuable discussions and feedback on the report. In particular we would like to acknowledge the support from: Anthony Hu, Miguel Sanchez Lohff, Lorenzo Bertoni, Charlie Lyons-Rothbart, Emma Wang, Harriett-Rose Follas, Kyle Esecson, Ben Foxall, Naama Zahavi, Ruben Diaz, Rudi Rankin, Tilly Pielichaty, Sarah Belghiti, Giulio D’Ippolito, Dan Reisman, Alex Persin, Fergal Cotter, Przemyslaw Mazur, Thomas Sajot, Giacomo Gallino, Alex Garcia Mayans, Tim Geypens, Robin Tweedie, Rebecca Hills.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Partners for automated vehicle education. pave poll 2020.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://pavecampaign.org/pave-poll-americans-wary-of-avs-but-say-education-and-experience-with-technology-can-build-trust/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://pavecampaign.org/pave-poll-americans-wary-of-avs-but-say-education-and-experience-with-technology-can-build-trust/</a><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-10-12.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
What’s going on with the open llm leaderboard?
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/blog/evaluating-mmlu-leaderboard</a><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-10-22.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Satanjeev Banerjee and Alon Lavie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 65–72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.03079</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">VLMo: Unified vision-language pre-training with mixture-of-modality-experts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Rt-1: Robotics transformer for real-world control at scale, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Driving with llms: Fusing object-level vector modality for explainable autonomous driving, 2023.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">End-to-end autonomous driving: Challenges and frontiers, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James Bradbury, and Weicheng Kuo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Pali: A jointly-scaled multilingual language-image model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learnining Representation</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Pranav Singh Chib and Pravendra Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Recent advancements in end-to-end autonomous driving using deep learning: A survey, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
DriveLM Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Drivelm: Drive on language.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/OpenDriveLab/DriveLM" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/OpenDriveLab/DriveLM</a><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Language modeling is compression, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool, and Marie Francine Moens.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Talk2car: Taking control of your self-driving car.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 2088–2098, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Palm-e: An embodied multimodal language model, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Vectornet: Encoding hd maps and agent dynamics from vectorized representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 11525–11533, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
”Jeffrey Hawke, Haibo E, Vijay Badrinarayanan, and Alex Kendall ”.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">”reimagining an autonomous vehicle”, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Model-based imitation learning for urban driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, volume 35, pages 20703–20716. Curran Associates, Inc., 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Lora: Low-rank adaptation of large language models, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Drew A. Hudson and Christopher D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Openclip, July 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">If you use this software, please cite it as below.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Sarthak Jain and Byron C Wallace.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Attention is not explanation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1902.10186</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Mistral 7b, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Adapt: Action-aware driving caption transformer, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Jinkyu Kim and John Canny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Interpretable learning for self-driving cars by visualizing causal attention, 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Textual explanations for self-driving vehicles, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2022.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Chin-Yew Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">ROUGE: A package for automatic evaluation of summaries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Text Summarization Branches Out</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Stephanie Lin, Jacob Hilton, and Owain Evans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Truthfulqa: Measuring how models mimic human falsehoods, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Visual instruction tuning, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Drama: Joint risk localization and captioning in driving, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Gpt-driver: Learning to drive with gpt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
The Language Archive Nijmegen: Max Planck Institute for Psycholinguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Elan, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Bleu: a method for automatic evaluation of machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.14836</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Behzad Dariush, Chiho Choi, and Mykel Kochenderfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Rank2tell: A multimodal driving dataset for joint importance ranking and reasoning, 2023.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, and Mingyu Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Languagempc: Large language models as decision makers for autonomous driving, 2023.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Llama: Open and efficient foundation language models, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Llama 2: Open foundation and fine-tuned chat models, 2023.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Cider: Consensus-based image description evaluation, 2015.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Image as a foreign language: BEiT pretraining for vision and vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, and Yuan Cao Yulia Tsvetkov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Simvlm: Simple visual language model pretraining with weak supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learnining Representation</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, and Botian Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">On the road with gpt-4v(ision): Early explorations of visual-language model on autonomous driving, 2023.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Wei Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">From automation to autonomy and autonomous vehicles: Challenges and opportunities for human-computer interaction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Interactions</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, 28(1):48–53, dec 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, and Nuno Vasconcelos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Explainable object-induced action decision for autonomous vehicles, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, and Hengshuang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Drivegpt4: Interpretable end-to-end autonomous driving via large language model, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Unified contrastive learning in image-text-label space, 2022.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Coca: Contrastive captioners are image-text foundation models, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Haotian* Zhang, Pengchuan* Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Glipv2: Unifying localization and vision-language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Opt: Open pre-trained transformer language models, 2022.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Bo Zhao, Boya Wu, and Tiejun Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Svit: Scaling up visual instruction tuning, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>LingoQA Dataset Examples</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Further examples on the capabilties existent in the training and the evaluation datasets are shown in Figure <a href="#A1.F4" title="Figure 4 ‣ Appendix A LingoQA Dataset Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The <span id="A1.p1.1.1" class="ltx_text ltx_font_italic">scenery</span> dataset contains highly descriptive elements, such as object colours, junction type, construction zones, traffic lights, and the road layout. The <span id="A1.p1.1.2" class="ltx_text ltx_font_italic">action</span> dataset is complementary and focused on driving competencies, such as the impact of traffic lights on driving and interactions with other road agents. The <span id="A1.p1.1.3" class="ltx_text ltx_font_italic">evaluation</span> dataset contains a broad range of questions aimed to test competencies relevant for autonomous driving.</p>
</div>
<figure id="A1.F4" class="ltx_figure"><img src="/html/2312.14115/assets/img/blurred_dataset_samples.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="472" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F4.6.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="A1.F4.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">LingoQA dataset examples.<span id="A1.F4.7.2.1" class="ltx_text ltx_font_medium"> From left to right: <span id="A1.F4.7.2.1.1" class="ltx_text ltx_font_italic">scenery</span> dataset, <span id="A1.F4.7.2.1.2" class="ltx_text ltx_font_italic">action</span> dataset, and <span id="A1.F4.7.2.1.3" class="ltx_text ltx_font_italic">evaluation</span> dataset.</span></span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Lingo-Judge Examples</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We present additional qualitative examples from our evaluation dataset in Table <a href="#A2.T5" title="Table 5 ‣ Appendix B Lingo-Judge Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, alongside predictions from our base model and corresponding metrics for each individual sample. Metrics based on <span id="A2.p1.1.1" class="ltx_text ltx_font_italic">n-gram</span> matching such as CIDEr tend to be error-prone. For example, expressions that have the same meaning, but entirely different words, are marked as not similar at all, such as <span id="A2.p1.1.2" class="ltx_text ltx_font_italic">“None”</span> and <span id="A2.p1.1.3" class="ltx_text ltx_font_italic">“There are no cars.”</span>. Sentences with minor but significant differences are graded as highly similar, despite having opposite meanings, such as <span id="A2.p1.1.4" class="ltx_text ltx_font_italic">“The traffic lights are showing green”</span> and <span id="A2.p1.1.5" class="ltx_text ltx_font_italic">“The traffic lights are showing red”</span>. Lingo-Judge demonstrates robustness against these varied expressions and subtle changes. Lingo-Judge also has limitations, primarily seen when establishing the correctness of the answer would require extra context from the videos. These examples can be seen in Table <a href="#A2.T6" title="Table 6 ‣ Appendix B Lingo-Judge Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">We qualitatively compare our classifier to GPT-4 ratings. These examples are shown in Figure <a href="#A2.F5" title="Figure 5 ‣ Appendix B Lingo-Judge Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In this situation, GPT-4 is misled by the fact that the model answer contains partially correct information. The GPT-4 assessment states that <span id="A2.p2.1.1" class="ltx_text ltx_font_italic">“The student correctly identified the presence of a traffic light”</span> and, despite the colours not being correct, further explains that <span id="A2.p2.1.2" class="ltx_text ltx_font_italic">“and accurately stated its colour”</span>. This highlights some challenges faced by GPT-4 when trying to rate the truthfulness of an answer. Lingo-Judge correctly identifies that the statements described by the model are false.</p>
</div>
<figure id="A2.T5" class="ltx_table">
<table id="A2.T5.2" class="ltx_tabular ltx_align_middle">
<tr id="A2.T5.2.1" class="ltx_tr">
<td id="A2.T5.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Question</span></span>
</span>
</td>
<td id="A2.T5.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.1.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Label</span></span>
</span>
</td>
<td id="A2.T5.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.1.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Prediction</span></span>
</span>
</td>
<td id="A2.T5.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.1.4.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="A2.T5.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.1.5.1" class="ltx_text" style="font-size:90%;">GPT4</span></td>
<td id="A2.T5.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.1.6.1" class="ltx_text" style="font-size:90%;">GPT4-CoT</span></td>
<td id="A2.T5.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.1.7.1" class="ltx_text" style="font-size:90%;">L-J Prob.</span></td>
<td id="A2.T5.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.1.8.1" class="ltx_text" style="font-size:90%;">L-J Class.</span></td>
</tr>
<tr id="A2.T5.2.2" class="ltx_tr">
<td id="A2.T5.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.2.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">How many pedestrians are crossing the road?</span></span>
</span>
</td>
<td id="A2.T5.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.2.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Zero pedestrians</span></span>
</span>
</td>
<td id="A2.T5.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.2.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">There are no pedestrians crossing the road.</span></span>
</span>
</td>
<td id="A2.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.2.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">23.18</span></td>
<td id="A2.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.2.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.2.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.2.7.1" class="ltx_text" style="font-size:90%;">0.96</span></td>
<td id="A2.T5.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.2.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">True</span></td>
</tr>
<tr id="A2.T5.2.3" class="ltx_tr">
<td id="A2.T5.2.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.3.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.3.1.1.1.1" class="ltx_text" style="font-size:90%;">What is the road speed limit?</span></span>
</span>
</td>
<td id="A2.T5.2.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.3.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.3.2.1.1.1" class="ltx_text" style="font-size:90%;">20 mph - it is written on the road</span></span>
</span>
</td>
<td id="A2.T5.2.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.3.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.3.3.1.1.1" class="ltx_text" style="font-size:90%;">The road speed limit is 20 mph.</span></span>
</span>
</td>
<td id="A2.T5.2.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.3.4.1" class="ltx_text" style="font-size:90%;color:#FF8000;">97.91</span></td>
<td id="A2.T5.2.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.3.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">4</span></td>
<td id="A2.T5.2.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.3.6.1" class="ltx_text" style="font-size:90%;color:#FF8000;">3</span></td>
<td id="A2.T5.2.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.3.7.1" class="ltx_text" style="font-size:90%;">0.95</span></td>
<td id="A2.T5.2.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.3.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">True</span></td>
</tr>
<tr id="A2.T5.2.4" class="ltx_tr">
<td id="A2.T5.2.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.4.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">How many cars are driving in your direction?</span></span>
</span>
</td>
<td id="A2.T5.2.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.4.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.4.2.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="A2.T5.2.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.4.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.4.3.1.1.1" class="ltx_text" style="font-size:90%;">There are no cars driving in my direction.</span></span>
</span>
</td>
<td id="A2.T5.2.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.4.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">0.11</span></td>
<td id="A2.T5.2.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.4.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">4</span></td>
<td id="A2.T5.2.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.4.6.1" class="ltx_text" style="font-size:90%;color:#FF8000;">3</span></td>
<td id="A2.T5.2.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.4.7.1" class="ltx_text" style="font-size:90%;">0.96</span></td>
<td id="A2.T5.2.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.4.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">True</span></td>
</tr>
<tr id="A2.T5.2.5" class="ltx_tr">
<td id="A2.T5.2.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.5.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Which vehicle should you follow if any?</span></span>
</span>
</td>
<td id="A2.T5.2.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.5.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.5.2.1.1.1" class="ltx_text" style="font-size:90%;">The motorcyclist.</span></span>
</span>
</td>
<td id="A2.T5.2.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.5.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.5.3.1.1.1" class="ltx_text" style="font-size:90%;">If any, I should follow the motorcycle ahead.</span></span>
</span>
</td>
<td id="A2.T5.2.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.5.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">0.42</span></td>
<td id="A2.T5.2.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.5.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">4</span></td>
<td id="A2.T5.2.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.5.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.5.7.1" class="ltx_text" style="font-size:90%;">0.95</span></td>
<td id="A2.T5.2.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.5.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">True</span></td>
</tr>
<tr id="A2.T5.2.6" class="ltx_tr">
<td id="A2.T5.2.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.6.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.6.1.1.1.1" class="ltx_text" style="font-size:90%;">What is the current action and its justification? Answer in the form “action, justification”</span></span>
</span>
</td>
<td id="A2.T5.2.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.6.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.6.2.1.1.1" class="ltx_text" style="font-size:90%;">Slow down, there is a stationary van infront of us</span></span>
</span>
</td>
<td id="A2.T5.2.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.6.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.6.3.1.1.1" class="ltx_text" style="font-size:90%;">I am decelerating because of the stationary truck ahead.</span></span>
</span>
</td>
<td id="A2.T5.2.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.6.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">34.79</span></td>
<td id="A2.T5.2.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.6.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.6.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.6.7.1" class="ltx_text" style="font-size:90%;">0.96</span></td>
<td id="A2.T5.2.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.6.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">True</span></td>
</tr>
<tr id="A2.T5.2.7" class="ltx_tr">
<td id="A2.T5.2.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.7.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.7.1.1.1.1" class="ltx_text" style="font-size:90%;">What is the current action and its justification? Answer in the form “action, justification”</span></span>
</span>
</td>
<td id="A2.T5.2.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.7.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.7.2.1.1.1" class="ltx_text" style="font-size:90%;">Stop, Red light</span></span>
</span>
</td>
<td id="A2.T5.2.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.7.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.7.3.1.1.1" class="ltx_text" style="font-size:90%;">I am stopping because the traffic lights to go straight are red.</span></span>
</span>
</td>
<td id="A2.T5.2.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.7.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">18.62</span></td>
<td id="A2.T5.2.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.7.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.7.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">5</span></td>
<td id="A2.T5.2.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.7.7.1" class="ltx_text" style="font-size:90%;">0.95</span></td>
<td id="A2.T5.2.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.7.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">True</span></td>
</tr>
<tr id="A2.T5.2.8" class="ltx_tr">
<td id="A2.T5.2.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.8.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.8.1.1.1.1" class="ltx_text" style="font-size:90%;">How many cyclists can you see?</span></span>
</span>
</td>
<td id="A2.T5.2.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.8.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.8.2.1.1.1" class="ltx_text" style="font-size:90%;">I can see 3 cyclists</span></span>
</span>
</td>
<td id="A2.T5.2.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.8.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.8.3.1.1.1" class="ltx_text" style="font-size:90%;">I can see two cyclists.</span></span>
</span>
</td>
<td id="A2.T5.2.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.8.4.1" class="ltx_text" style="font-size:90%;color:#FF8000;">150.29</span></td>
<td id="A2.T5.2.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.8.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">1</span></td>
<td id="A2.T5.2.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.8.6.1" class="ltx_text" style="font-size:90%;color:#FF8000;">2</span></td>
<td id="A2.T5.2.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.8.7.1" class="ltx_text" style="font-size:90%;">0.05</span></td>
<td id="A2.T5.2.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.8.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">False</span></td>
</tr>
<tr id="A2.T5.2.9" class="ltx_tr">
<td id="A2.T5.2.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.9.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.9.1.1.1.1" class="ltx_text" style="font-size:90%;">What color are the traffic lights showing?</span></span>
</span>
</td>
<td id="A2.T5.2.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.9.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.9.2.1.1.1" class="ltx_text" style="font-size:90%;">The traffic lights are showing green</span></span>
</span>
</td>
<td id="A2.T5.2.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.9.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.9.3.1.1.1" class="ltx_text" style="font-size:90%;">The traffic lights are showing red.</span></span>
</span>
</td>
<td id="A2.T5.2.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.9.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">329.36</span></td>
<td id="A2.T5.2.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.9.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">0</span></td>
<td id="A2.T5.2.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.9.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">1</span></td>
<td id="A2.T5.2.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.9.7.1" class="ltx_text" style="font-size:90%;">0.05</span></td>
<td id="A2.T5.2.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.9.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">False</span></td>
</tr>
<tr id="A2.T5.2.10" class="ltx_tr">
<td id="A2.T5.2.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.10.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.10.1.1.1.1" class="ltx_text" style="font-size:90%;">What action are you taking with respect to the cyclist?</span></span>
</span>
</td>
<td id="A2.T5.2.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.10.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.10.2.1.1.1" class="ltx_text" style="font-size:90%;">Overtaking them on the right and keeping the speed</span></span>
</span>
</td>
<td id="A2.T5.2.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.10.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.10.3.1.1.1" class="ltx_text" style="font-size:90%;">I am overtaking the cyclist on the left.</span></span>
</span>
</td>
<td id="A2.T5.2.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.10.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">349.52</span></td>
<td id="A2.T5.2.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.10.5.1" class="ltx_text" style="font-size:90%;color:#FF8000;">2</span></td>
<td id="A2.T5.2.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.10.6.1" class="ltx_text" style="font-size:90%;color:#FF8000;">2</span></td>
<td id="A2.T5.2.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.10.7.1" class="ltx_text" style="font-size:90%;">0.10</span></td>
<td id="A2.T5.2.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.10.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">False</span></td>
</tr>
<tr id="A2.T5.2.11" class="ltx_tr">
<td id="A2.T5.2.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.11.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.11.1.1.1.1" class="ltx_text" style="font-size:90%;">In which direction is the bus driving?</span></span>
</span>
</td>
<td id="A2.T5.2.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.11.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.11.2.1.1.1" class="ltx_text" style="font-size:90%;">The bus is driving in the opposite direction</span></span>
</span>
</td>
<td id="A2.T5.2.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.11.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.11.3.1.1.1" class="ltx_text" style="font-size:90%;">The bus is driving in the oncoming direction.</span></span>
</span>
</td>
<td id="A2.T5.2.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.11.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">404.65</span></td>
<td id="A2.T5.2.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.11.5.1" class="ltx_text" style="font-size:90%;color:#FF0000;">4</span></td>
<td id="A2.T5.2.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.11.6.1" class="ltx_text" style="font-size:90%;color:#FF0000;">5</span></td>
<td id="A2.T5.2.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.11.7.1" class="ltx_text" style="font-size:90%;">0.31</span></td>
<td id="A2.T5.2.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.11.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">False</span></td>
</tr>
<tr id="A2.T5.2.12" class="ltx_tr">
<td id="A2.T5.2.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.12.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.12.1.1.1.1" class="ltx_text" style="font-size:90%;">Are there any parked car on the side of the road?</span></span>
</span>
</td>
<td id="A2.T5.2.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.12.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.12.2.1.1.1" class="ltx_text" style="font-size:90%;">Yes, there are two cars parked on the right of the road</span></span>
</span>
</td>
<td id="A2.T5.2.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.12.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.12.3.1.1.1" class="ltx_text" style="font-size:90%;">No, there are no parked cars on either side of the road.</span></span>
</span>
</td>
<td id="A2.T5.2.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.12.4.1" class="ltx_text" style="font-size:90%;color:#FF8000;">142.40</span></td>
<td id="A2.T5.2.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.12.5.1" class="ltx_text" style="font-size:90%;color:#00FF00;">0</span></td>
<td id="A2.T5.2.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.12.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">0</span></td>
<td id="A2.T5.2.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.12.7.1" class="ltx_text" style="font-size:90%;">0.05</span></td>
<td id="A2.T5.2.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.12.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">False</span></td>
</tr>
<tr id="A2.T5.2.13" class="ltx_tr">
<td id="A2.T5.2.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.13.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T5.2.13.1.1.1.1" class="ltx_text" style="font-size:90%;">Is acceleration necessary in this situation? If so, provide the reason.</span></span>
</span>
</td>
<td id="A2.T5.2.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.13.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.13.2.1.1.1" class="ltx_text" style="font-size:90%;">No. We should decelerate in this situation because there is a vehicle stopping ahead of us.</span></span>
</span>
</td>
<td id="A2.T5.2.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T5.2.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T5.2.13.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T5.2.13.3.1.1.1" class="ltx_text" style="font-size:90%;">No, acceleration is not necessary in this situation as I am already driving at the speed limit.</span></span>
</span>
</td>
<td id="A2.T5.2.13.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.13.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">177.96</span></td>
<td id="A2.T5.2.13.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.13.5.1" class="ltx_text" style="font-size:90%;color:#FF8000;">3</span></td>
<td id="A2.T5.2.13.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.13.6.1" class="ltx_text" style="font-size:90%;color:#FF8000;">3</span></td>
<td id="A2.T5.2.13.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.13.7.1" class="ltx_text" style="font-size:90%;">0.31</span></td>
<td id="A2.T5.2.13.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T5.2.13.8.1" class="ltx_text" style="font-size:90%;color:#00FF00;">False</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="A2.T5.12.1" class="ltx_text ltx_font_bold">Qualitative comparison of metrics.</span> Questions and labels from our evaluation dataset along with exemplary predictions and their corresponding sample-level metrics. Notable is that the classifier (here “L-J”) can pick up slight but crucial differences between ground truth and prediction to correctly identify a wrong prediction, such as “The traffic lights are green” and “The traffic lights are red”, in which case CIDEr still shows high agreement. <span id="A2.T5.13.2" class="ltx_text" style="color:#00FF00;">Green</span> color indicates agreement with human judgement, while <span id="A2.T5.14.3" class="ltx_text" style="color:#FF8000;">orange</span> and <span id="A2.T5.15.4" class="ltx_text" style="color:#FF0000;">red</span> show disagreement.</figcaption>
</figure>
<figure id="A2.T6" class="ltx_table">
<table id="A2.T6.2" class="ltx_tabular ltx_align_middle">
<tr id="A2.T6.2.1" class="ltx_tr">
<td id="A2.T6.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T6.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Question</span></span>
</span>
</td>
<td id="A2.T6.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.1.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Label</span></span>
</span>
</td>
<td id="A2.T6.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.1.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Prediction</span></span>
</span>
</td>
<td id="A2.T6.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.1.4.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="A2.T6.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.1.5.1" class="ltx_text" style="font-size:90%;">GPT4</span></td>
<td id="A2.T6.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.1.6.1" class="ltx_text" style="font-size:90%;">GPT4-CoT</span></td>
<td id="A2.T6.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.1.7.1" class="ltx_text" style="font-size:90%;">L-J Prob.</span></td>
<td id="A2.T6.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.1.8.1" class="ltx_text" style="font-size:90%;">L-J Class.</span></td>
</tr>
<tr id="A2.T6.2.2" class="ltx_tr">
<td id="A2.T6.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.2.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T6.2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Is any vehicle stopped at the zebra crossing?</span></span>
</span>
</td>
<td id="A2.T6.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.2.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">There is no zebra crossing.</span></span>
</span>
</td>
<td id="A2.T6.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.2.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">No, there are no vehicles stopped at the zebra crossing.</span></span>
</span>
</td>
<td id="A2.T6.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.2.4.1" class="ltx_text" style="font-size:90%;color:#FF0000;">106.19</span></td>
<td id="A2.T6.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.2.5.1" class="ltx_text" style="font-size:90%;color:#FF0000;">4</span></td>
<td id="A2.T6.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.2.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">2</span></td>
<td id="A2.T6.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.2.7.1" class="ltx_text" style="font-size:90%;">0.93</span></td>
<td id="A2.T6.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.2.8.1" class="ltx_text" style="font-size:90%;color:#FF0000;">True</span></td>
</tr>
<tr id="A2.T6.2.3" class="ltx_tr">
<td id="A2.T6.2.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.3.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T6.2.3.1.1.1.1" class="ltx_text" style="font-size:90%;">What elements are capturing your attention and what is the reason behind it?</span></span>
</span>
</td>
<td id="A2.T6.2.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.3.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.3.2.1.1.1" class="ltx_text" style="font-size:90%;">I am making sure there is no vehicle on the left lane I am making my lane change to. I am also looking at the traffic light to make sure it stays green while I am crossing the intersection.</span></span>
</span>
</td>
<td id="A2.T6.2.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.3.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.3.3.1.1.1" class="ltx_text" style="font-size:90%;">I am paying attention to the bus lane on the left because I need to maintain a safe distance from it.</span></span>
</span>
</td>
<td id="A2.T6.2.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.3.4.1" class="ltx_text" style="font-size:90%;color:#00FF00;">0.91</span></td>
<td id="A2.T6.2.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.3.5.1" class="ltx_text" style="font-size:90%;color:#FF8000;">3</span></td>
<td id="A2.T6.2.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.3.6.1" class="ltx_text" style="font-size:90%;color:#00FF00;">1</span></td>
<td id="A2.T6.2.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.3.7.1" class="ltx_text" style="font-size:90%;">0.57</span></td>
<td id="A2.T6.2.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.3.8.1" class="ltx_text" style="font-size:90%;color:#FF0000;">True</span></td>
</tr>
<tr id="A2.T6.2.4" class="ltx_tr">
<td id="A2.T6.2.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.4.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A2.T6.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">What is the current action and its justification? Answer in the form ”action, justification”</span></span>
</span>
</td>
<td id="A2.T6.2.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.4.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.4.2.1.1.1" class="ltx_text" style="font-size:90%;">We are taking a left turn, because we are at a t-junction. Then we keep a safe distance from the cyclists in front.</span></span>
</span>
</td>
<td id="A2.T6.2.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A2.T6.2.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T6.2.4.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T6.2.4.3.1.1.1" class="ltx_text" style="font-size:90%;">I am decelerating to keep a safe distance from the cyclist ahead of me.</span></span>
</span>
</td>
<td id="A2.T6.2.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.4.4.1" class="ltx_text" style="font-size:90%;color:#FF8000;">46.45</span></td>
<td id="A2.T6.2.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.4.5.1" class="ltx_text" style="font-size:90%;color:#FF8000;">2</span></td>
<td id="A2.T6.2.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.4.6.1" class="ltx_text" style="font-size:90%;color:#FF8000;">2</span></td>
<td id="A2.T6.2.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.4.7.1" class="ltx_text" style="font-size:90%;">0.32</span></td>
<td id="A2.T6.2.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.2.4.8.1" class="ltx_text" style="font-size:90%;color:#FF0000;">False</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="A2.T6.12.1" class="ltx_text ltx_font_bold">Failure Cases of Lingo-Judge.</span> Examples where Lingo-Judge makes a wrong judgement about the correctness of the model prediction. <span id="A2.T6.13.2" class="ltx_text" style="color:#00FF00;">Green</span> color indicates agreement with human judgement, while <span id="A2.T6.14.3" class="ltx_text" style="color:#FF8000;">orange</span> and <span id="A2.T6.15.4" class="ltx_text" style="color:#FF0000;">red</span> show disagreement.</figcaption>
</figure>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2312.14115/assets/img/classifier_examples.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="583" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A2.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Classifier examples.<span id="A2.F5.4.2.1" class="ltx_text ltx_font_medium"> Examples of Lingo-Judge outputs compared to GPT-4.</span></span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>GPT-4 Grading</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">In this section we provide an overview of the implementation details for the evaluation method using GPT-4 with and without Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> prompting.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p"><span id="A3.p2.1.1" class="ltx_text ltx_font_bold">GPT-4 with CoT.</span> In order to evaluate a model’s answer with GPT-4 and CoT prompting, we first provide GPT-4 with the question and one or more valid answers for the questions, and ask it to come up with a strategy to evaluate new answers to this question. We then provide GPT-4 with the model’s answer and ask it to evaluate the answer using the strategy it proposed in the previous step. Finally, we ask GPT-4 to give the model a grade between 0 and 5, where 5 means the answer is perfect. The prompt used is shown in Figure <a href="#A3.F6" title="Figure 6 ‣ Appendix C GPT-4 Grading ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p"><span id="A3.p3.1.1" class="ltx_text ltx_font_bold">GPT-4 without CoT.</span> When evaluating model outputs without CoT prompting, we provide GPT-4 with the question, one or more valid answers for the questions, and the model predictions and we directly ask GPT-4 to give the model a grade between 0 and 5, without the intermeidate reasoning steps. The prompt used is shown in Figure <a href="#A3.F7" title="Figure 7 ‣ Appendix C GPT-4 Grading ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.1" class="ltx_p">We emit concurrent requests to our Azure’s GPT-4 deployment in order to max-out the limit of 40k tokens per minute. GPT-4 without CoT prompting required more than 13 minutes to perform the evaluation, and GPT-4 with CoT prompting requires more than 50 minutes.</p>
</div>
<figure id="A3.F6" class="ltx_figure"><img src="/html/2312.14115/assets/img/gpt4_cot.png" id="A3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="651" height="735" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A3.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">GPT-4 with Chain of Thought (CoT) prompting.<span id="A3.F6.4.2.1" class="ltx_text ltx_font_medium"> First, GPT-4 is provided with the question and ground truth answers, and asked to come up with a strategy for testing the answer. Second, GPT-4 is provided with the model answer and is prompted to evaluate the accuracy of the response based on the previously defined strategy. Finally, GPT-4 is asked to provide a grade for the student.</span></span></figcaption>
</figure>
<figure id="A3.F7" class="ltx_figure"><img src="/html/2312.14115/assets/img/gpt4_no_cot.png" id="A3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="651" height="506" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A3.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">GPT-4 without Chain of Thought (CoT) prompting.<span id="A3.F7.4.2.1" class="ltx_text ltx_font_medium"> GPT-4 is provided with a prompt that contains the question, the ground truth answers, and the model response, and is requested to directly provide a grade for the student.</span></span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Lingo-Judge Correlation Study</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We show that Lingo-Judge exhibits a higher correlation to human judgment than commonly-used language-based metrics, and than GPT-4.
To do so, we computed the scores of 15 different models and 2 groups of human labellers on the questions in our evaluation dataset using Lingo-Judge, GPT-4, BLEU4, METEOR and CIDEr. These scores are reported in Table <a href="#A4.T7" title="Table 7 ‣ Appendix D Lingo-Judge Correlation Study ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">We then computed the Pearson and Spearman correlation coefficients between these metrics and the human evaluation.
The <span id="A4.p2.1.1" class="ltx_text ltx_font_bold">Pearson correlation coefficient</span> measures the strength of the linear correlation between the human evaluation and a metric score, while the <span id="A4.p2.1.2" class="ltx_text ltx_font_bold">Spearman rank correlation coefficient</span> measures the monotonic relationship between the human evaluation and the metric. The higher the Spearman coefficient, the better a metric is at ranking answers in the same order as our human evaluators.
To compute the confidence intervals, we use the Fisher transformation with a 95% confidence level.</p>
</div>
<div id="A4.p3" class="ltx_para">
<p id="A4.p3.1" class="ltx_p">In Figure <a href="#A4.F8" title="Figure 8 ‣ Appendix D Lingo-Judge Correlation Study ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the metric scores are plotted against the human evaluators’ grades (from 0 to 1). In red is the least-squares regression of the linear relationship between the metric and the human-assigned grades. Figure <a href="#A4.F9" title="Figure 9 ‣ Appendix D Lingo-Judge Correlation Study ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the value of both correlation coefficients for each of the 5 metrics, as well as their confidence interval bounds. We note than not only does Lingo-Judge provided higher correlation, it also provides tighter confidence intervals than the other metrics.</p>
</div>
<figure id="A4.T7" class="ltx_table">
<table id="A4.T7.108" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A4.T7.6.6" class="ltx_tr">
<td id="A4.T7.6.6.7" class="ltx_td ltx_border_tt"></td>
<td id="A4.T7.6.6.8" class="ltx_td ltx_border_tt"></td>
<td id="A4.T7.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Lingo-Judge [%] <math id="A4.T7.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.1.1.1.m1.1a"><mo stretchy="false" id="A4.T7.1.1.1.m1.1.1" xref="A4.T7.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.1.1.1.m1.1b"><ci id="A4.T7.1.1.1.m1.1.1.cmml" xref="A4.T7.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="A4.T7.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">BLEU <math id="A4.T7.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.2.2.2.m1.1a"><mo stretchy="false" id="A4.T7.2.2.2.m1.1.1" xref="A4.T7.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.2.2.2.m1.1b"><ci id="A4.T7.2.2.2.m1.1.1.cmml" xref="A4.T7.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="A4.T7.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">METEOR <math id="A4.T7.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.3.3.3.m1.1a"><mo stretchy="false" id="A4.T7.3.3.3.m1.1.1" xref="A4.T7.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.3.3.3.m1.1b"><ci id="A4.T7.3.3.3.m1.1.1.cmml" xref="A4.T7.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="A4.T7.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">CIDEr <math id="A4.T7.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.4.4.4.m1.1a"><mo stretchy="false" id="A4.T7.4.4.4.m1.1.1" xref="A4.T7.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.4.4.4.m1.1b"><ci id="A4.T7.4.4.4.m1.1.1.cmml" xref="A4.T7.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="A4.T7.5.5.5" class="ltx_td ltx_align_center ltx_border_tt">GPT-4 <math id="A4.T7.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.5.5.5.m1.1a"><mo stretchy="false" id="A4.T7.5.5.5.m1.1.1" xref="A4.T7.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.5.5.5.m1.1b"><ci id="A4.T7.5.5.5.m1.1.1.cmml" xref="A4.T7.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="A4.T7.6.6.6" class="ltx_td ltx_align_center ltx_border_tt">Human <math id="A4.T7.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.6.6.6.m1.1a"><mo stretchy="false" id="A4.T7.6.6.6.m1.1.1" xref="A4.T7.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.6.6.6.m1.1b"><ci id="A4.T7.6.6.6.m1.1.1.cmml" xref="A4.T7.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="A4.T7.12.12" class="ltx_tr">
<td id="A4.T7.12.12.7" class="ltx_td ltx_align_left ltx_border_t" rowspan="13"><span id="A4.T7.12.12.7.1" class="ltx_text ltx_font_bold ltx_font_italic">Models</span></td>
<td id="A4.T7.12.12.8" class="ltx_td ltx_align_left ltx_border_t">Model A</td>
<td id="A4.T7.7.7.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.7.7.1.m1.1" class="ltx_Math" alttext="59.6" display="inline"><semantics id="A4.T7.7.7.1.m1.1a"><mn id="A4.T7.7.7.1.m1.1.1" xref="A4.T7.7.7.1.m1.1.1.cmml">59.6</mn><annotation-xml encoding="MathML-Content" id="A4.T7.7.7.1.m1.1b"><cn type="float" id="A4.T7.7.7.1.m1.1.1.cmml" xref="A4.T7.7.7.1.m1.1.1">59.6</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.7.7.1.m1.1c">59.6</annotation></semantics></math></td>
<td id="A4.T7.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.8.8.2.m1.1" class="ltx_Math" alttext="15.45" display="inline"><semantics id="A4.T7.8.8.2.m1.1a"><mn id="A4.T7.8.8.2.m1.1.1" xref="A4.T7.8.8.2.m1.1.1.cmml">15.45</mn><annotation-xml encoding="MathML-Content" id="A4.T7.8.8.2.m1.1b"><cn type="float" id="A4.T7.8.8.2.m1.1.1.cmml" xref="A4.T7.8.8.2.m1.1.1">15.45</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.8.8.2.m1.1c">15.45</annotation></semantics></math></td>
<td id="A4.T7.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.9.9.3.m1.1" class="ltx_Math" alttext="18.36" display="inline"><semantics id="A4.T7.9.9.3.m1.1a"><mn id="A4.T7.9.9.3.m1.1.1" xref="A4.T7.9.9.3.m1.1.1.cmml">18.36</mn><annotation-xml encoding="MathML-Content" id="A4.T7.9.9.3.m1.1b"><cn type="float" id="A4.T7.9.9.3.m1.1.1.cmml" xref="A4.T7.9.9.3.m1.1.1">18.36</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.9.9.3.m1.1c">18.36</annotation></semantics></math></td>
<td id="A4.T7.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.10.10.4.m1.1" class="ltx_Math" alttext="66.32" display="inline"><semantics id="A4.T7.10.10.4.m1.1a"><mn id="A4.T7.10.10.4.m1.1.1" xref="A4.T7.10.10.4.m1.1.1.cmml">66.32</mn><annotation-xml encoding="MathML-Content" id="A4.T7.10.10.4.m1.1b"><cn type="float" id="A4.T7.10.10.4.m1.1.1.cmml" xref="A4.T7.10.10.4.m1.1.1">66.32</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.10.10.4.m1.1c">66.32</annotation></semantics></math></td>
<td id="A4.T7.11.11.5" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.11.11.5.m1.1" class="ltx_Math" alttext="3.23" display="inline"><semantics id="A4.T7.11.11.5.m1.1a"><mn id="A4.T7.11.11.5.m1.1.1" xref="A4.T7.11.11.5.m1.1.1.cmml">3.23</mn><annotation-xml encoding="MathML-Content" id="A4.T7.11.11.5.m1.1b"><cn type="float" id="A4.T7.11.11.5.m1.1.1.cmml" xref="A4.T7.11.11.5.m1.1.1">3.23</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.11.11.5.m1.1c">3.23</annotation></semantics></math></td>
<td id="A4.T7.12.12.6" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.12.12.6.m1.1" class="ltx_Math" alttext="0.571" display="inline"><semantics id="A4.T7.12.12.6.m1.1a"><mn id="A4.T7.12.12.6.m1.1.1" xref="A4.T7.12.12.6.m1.1.1.cmml">0.571</mn><annotation-xml encoding="MathML-Content" id="A4.T7.12.12.6.m1.1b"><cn type="float" id="A4.T7.12.12.6.m1.1.1.cmml" xref="A4.T7.12.12.6.m1.1.1">0.571</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.12.12.6.m1.1c">0.571</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.18.18" class="ltx_tr">
<td id="A4.T7.18.18.7" class="ltx_td ltx_align_left">Model B</td>
<td id="A4.T7.13.13.1" class="ltx_td ltx_align_center"><math id="A4.T7.13.13.1.m1.1" class="ltx_Math" alttext="59.6" display="inline"><semantics id="A4.T7.13.13.1.m1.1a"><mn id="A4.T7.13.13.1.m1.1.1" xref="A4.T7.13.13.1.m1.1.1.cmml">59.6</mn><annotation-xml encoding="MathML-Content" id="A4.T7.13.13.1.m1.1b"><cn type="float" id="A4.T7.13.13.1.m1.1.1.cmml" xref="A4.T7.13.13.1.m1.1.1">59.6</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.13.13.1.m1.1c">59.6</annotation></semantics></math></td>
<td id="A4.T7.14.14.2" class="ltx_td ltx_align_center"><math id="A4.T7.14.14.2.m1.1" class="ltx_Math" alttext="15.16" display="inline"><semantics id="A4.T7.14.14.2.m1.1a"><mn id="A4.T7.14.14.2.m1.1.1" xref="A4.T7.14.14.2.m1.1.1.cmml">15.16</mn><annotation-xml encoding="MathML-Content" id="A4.T7.14.14.2.m1.1b"><cn type="float" id="A4.T7.14.14.2.m1.1.1.cmml" xref="A4.T7.14.14.2.m1.1.1">15.16</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.14.14.2.m1.1c">15.16</annotation></semantics></math></td>
<td id="A4.T7.15.15.3" class="ltx_td ltx_align_center"><math id="A4.T7.15.15.3.m1.1" class="ltx_Math" alttext="18.84" display="inline"><semantics id="A4.T7.15.15.3.m1.1a"><mn id="A4.T7.15.15.3.m1.1.1" xref="A4.T7.15.15.3.m1.1.1.cmml">18.84</mn><annotation-xml encoding="MathML-Content" id="A4.T7.15.15.3.m1.1b"><cn type="float" id="A4.T7.15.15.3.m1.1.1.cmml" xref="A4.T7.15.15.3.m1.1.1">18.84</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.15.15.3.m1.1c">18.84</annotation></semantics></math></td>
<td id="A4.T7.16.16.4" class="ltx_td ltx_align_center"><math id="A4.T7.16.16.4.m1.1" class="ltx_Math" alttext="65.11" display="inline"><semantics id="A4.T7.16.16.4.m1.1a"><mn id="A4.T7.16.16.4.m1.1.1" xref="A4.T7.16.16.4.m1.1.1.cmml">65.11</mn><annotation-xml encoding="MathML-Content" id="A4.T7.16.16.4.m1.1b"><cn type="float" id="A4.T7.16.16.4.m1.1.1.cmml" xref="A4.T7.16.16.4.m1.1.1">65.11</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.16.16.4.m1.1c">65.11</annotation></semantics></math></td>
<td id="A4.T7.17.17.5" class="ltx_td ltx_align_center"><math id="A4.T7.17.17.5.m1.1" class="ltx_Math" alttext="3.16" display="inline"><semantics id="A4.T7.17.17.5.m1.1a"><mn id="A4.T7.17.17.5.m1.1.1" xref="A4.T7.17.17.5.m1.1.1.cmml">3.16</mn><annotation-xml encoding="MathML-Content" id="A4.T7.17.17.5.m1.1b"><cn type="float" id="A4.T7.17.17.5.m1.1.1.cmml" xref="A4.T7.17.17.5.m1.1.1">3.16</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.17.17.5.m1.1c">3.16</annotation></semantics></math></td>
<td id="A4.T7.18.18.6" class="ltx_td ltx_align_center"><math id="A4.T7.18.18.6.m1.1" class="ltx_Math" alttext="0.564" display="inline"><semantics id="A4.T7.18.18.6.m1.1a"><mn id="A4.T7.18.18.6.m1.1.1" xref="A4.T7.18.18.6.m1.1.1.cmml">0.564</mn><annotation-xml encoding="MathML-Content" id="A4.T7.18.18.6.m1.1b"><cn type="float" id="A4.T7.18.18.6.m1.1.1.cmml" xref="A4.T7.18.18.6.m1.1.1">0.564</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.18.18.6.m1.1c">0.564</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.24.24" class="ltx_tr">
<td id="A4.T7.24.24.7" class="ltx_td ltx_align_left">Model C</td>
<td id="A4.T7.19.19.1" class="ltx_td ltx_align_center"><math id="A4.T7.19.19.1.m1.1" class="ltx_Math" alttext="57.4" display="inline"><semantics id="A4.T7.19.19.1.m1.1a"><mn id="A4.T7.19.19.1.m1.1.1" xref="A4.T7.19.19.1.m1.1.1.cmml">57.4</mn><annotation-xml encoding="MathML-Content" id="A4.T7.19.19.1.m1.1b"><cn type="float" id="A4.T7.19.19.1.m1.1.1.cmml" xref="A4.T7.19.19.1.m1.1.1">57.4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.19.19.1.m1.1c">57.4</annotation></semantics></math></td>
<td id="A4.T7.20.20.2" class="ltx_td ltx_align_center"><math id="A4.T7.20.20.2.m1.1" class="ltx_Math" alttext="14.87" display="inline"><semantics id="A4.T7.20.20.2.m1.1a"><mn id="A4.T7.20.20.2.m1.1.1" xref="A4.T7.20.20.2.m1.1.1.cmml">14.87</mn><annotation-xml encoding="MathML-Content" id="A4.T7.20.20.2.m1.1b"><cn type="float" id="A4.T7.20.20.2.m1.1.1.cmml" xref="A4.T7.20.20.2.m1.1.1">14.87</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.20.20.2.m1.1c">14.87</annotation></semantics></math></td>
<td id="A4.T7.21.21.3" class="ltx_td ltx_align_center"><math id="A4.T7.21.21.3.m1.1" class="ltx_Math" alttext="18.52" display="inline"><semantics id="A4.T7.21.21.3.m1.1a"><mn id="A4.T7.21.21.3.m1.1.1" xref="A4.T7.21.21.3.m1.1.1.cmml">18.52</mn><annotation-xml encoding="MathML-Content" id="A4.T7.21.21.3.m1.1b"><cn type="float" id="A4.T7.21.21.3.m1.1.1.cmml" xref="A4.T7.21.21.3.m1.1.1">18.52</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.21.21.3.m1.1c">18.52</annotation></semantics></math></td>
<td id="A4.T7.22.22.4" class="ltx_td ltx_align_center"><math id="A4.T7.22.22.4.m1.1" class="ltx_Math" alttext="65.49" display="inline"><semantics id="A4.T7.22.22.4.m1.1a"><mn id="A4.T7.22.22.4.m1.1.1" xref="A4.T7.22.22.4.m1.1.1.cmml">65.49</mn><annotation-xml encoding="MathML-Content" id="A4.T7.22.22.4.m1.1b"><cn type="float" id="A4.T7.22.22.4.m1.1.1.cmml" xref="A4.T7.22.22.4.m1.1.1">65.49</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.22.22.4.m1.1c">65.49</annotation></semantics></math></td>
<td id="A4.T7.23.23.5" class="ltx_td ltx_align_center"><math id="A4.T7.23.23.5.m1.1" class="ltx_Math" alttext="3.08" display="inline"><semantics id="A4.T7.23.23.5.m1.1a"><mn id="A4.T7.23.23.5.m1.1.1" xref="A4.T7.23.23.5.m1.1.1.cmml">3.08</mn><annotation-xml encoding="MathML-Content" id="A4.T7.23.23.5.m1.1b"><cn type="float" id="A4.T7.23.23.5.m1.1.1.cmml" xref="A4.T7.23.23.5.m1.1.1">3.08</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.23.23.5.m1.1c">3.08</annotation></semantics></math></td>
<td id="A4.T7.24.24.6" class="ltx_td ltx_align_center"><math id="A4.T7.24.24.6.m1.1" class="ltx_Math" alttext="0.563" display="inline"><semantics id="A4.T7.24.24.6.m1.1a"><mn id="A4.T7.24.24.6.m1.1.1" xref="A4.T7.24.24.6.m1.1.1.cmml">0.563</mn><annotation-xml encoding="MathML-Content" id="A4.T7.24.24.6.m1.1b"><cn type="float" id="A4.T7.24.24.6.m1.1.1.cmml" xref="A4.T7.24.24.6.m1.1.1">0.563</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.24.24.6.m1.1c">0.563</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.30.30" class="ltx_tr">
<td id="A4.T7.30.30.7" class="ltx_td ltx_align_left">Model D</td>
<td id="A4.T7.25.25.1" class="ltx_td ltx_align_center"><math id="A4.T7.25.25.1.m1.1" class="ltx_Math" alttext="58.2" display="inline"><semantics id="A4.T7.25.25.1.m1.1a"><mn id="A4.T7.25.25.1.m1.1.1" xref="A4.T7.25.25.1.m1.1.1.cmml">58.2</mn><annotation-xml encoding="MathML-Content" id="A4.T7.25.25.1.m1.1b"><cn type="float" id="A4.T7.25.25.1.m1.1.1.cmml" xref="A4.T7.25.25.1.m1.1.1">58.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.25.25.1.m1.1c">58.2</annotation></semantics></math></td>
<td id="A4.T7.26.26.2" class="ltx_td ltx_align_center"><math id="A4.T7.26.26.2.m1.1" class="ltx_Math" alttext="14.51" display="inline"><semantics id="A4.T7.26.26.2.m1.1a"><mn id="A4.T7.26.26.2.m1.1.1" xref="A4.T7.26.26.2.m1.1.1.cmml">14.51</mn><annotation-xml encoding="MathML-Content" id="A4.T7.26.26.2.m1.1b"><cn type="float" id="A4.T7.26.26.2.m1.1.1.cmml" xref="A4.T7.26.26.2.m1.1.1">14.51</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.26.26.2.m1.1c">14.51</annotation></semantics></math></td>
<td id="A4.T7.27.27.3" class="ltx_td ltx_align_center"><math id="A4.T7.27.27.3.m1.1" class="ltx_Math" alttext="18.59" display="inline"><semantics id="A4.T7.27.27.3.m1.1a"><mn id="A4.T7.27.27.3.m1.1.1" xref="A4.T7.27.27.3.m1.1.1.cmml">18.59</mn><annotation-xml encoding="MathML-Content" id="A4.T7.27.27.3.m1.1b"><cn type="float" id="A4.T7.27.27.3.m1.1.1.cmml" xref="A4.T7.27.27.3.m1.1.1">18.59</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.27.27.3.m1.1c">18.59</annotation></semantics></math></td>
<td id="A4.T7.28.28.4" class="ltx_td ltx_align_center"><math id="A4.T7.28.28.4.m1.1" class="ltx_Math" alttext="66.02" display="inline"><semantics id="A4.T7.28.28.4.m1.1a"><mn id="A4.T7.28.28.4.m1.1.1" xref="A4.T7.28.28.4.m1.1.1.cmml">66.02</mn><annotation-xml encoding="MathML-Content" id="A4.T7.28.28.4.m1.1b"><cn type="float" id="A4.T7.28.28.4.m1.1.1.cmml" xref="A4.T7.28.28.4.m1.1.1">66.02</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.28.28.4.m1.1c">66.02</annotation></semantics></math></td>
<td id="A4.T7.29.29.5" class="ltx_td ltx_align_center"><math id="A4.T7.29.29.5.m1.1" class="ltx_Math" alttext="3.15" display="inline"><semantics id="A4.T7.29.29.5.m1.1a"><mn id="A4.T7.29.29.5.m1.1.1" xref="A4.T7.29.29.5.m1.1.1.cmml">3.15</mn><annotation-xml encoding="MathML-Content" id="A4.T7.29.29.5.m1.1b"><cn type="float" id="A4.T7.29.29.5.m1.1.1.cmml" xref="A4.T7.29.29.5.m1.1.1">3.15</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.29.29.5.m1.1c">3.15</annotation></semantics></math></td>
<td id="A4.T7.30.30.6" class="ltx_td ltx_align_center"><math id="A4.T7.30.30.6.m1.1" class="ltx_Math" alttext="0.559" display="inline"><semantics id="A4.T7.30.30.6.m1.1a"><mn id="A4.T7.30.30.6.m1.1.1" xref="A4.T7.30.30.6.m1.1.1.cmml">0.559</mn><annotation-xml encoding="MathML-Content" id="A4.T7.30.30.6.m1.1b"><cn type="float" id="A4.T7.30.30.6.m1.1.1.cmml" xref="A4.T7.30.30.6.m1.1.1">0.559</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.30.30.6.m1.1c">0.559</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.36.36" class="ltx_tr">
<td id="A4.T7.36.36.7" class="ltx_td ltx_align_left">Model E</td>
<td id="A4.T7.31.31.1" class="ltx_td ltx_align_center"><math id="A4.T7.31.31.1.m1.1" class="ltx_Math" alttext="59.0" display="inline"><semantics id="A4.T7.31.31.1.m1.1a"><mn id="A4.T7.31.31.1.m1.1.1" xref="A4.T7.31.31.1.m1.1.1.cmml">59.0</mn><annotation-xml encoding="MathML-Content" id="A4.T7.31.31.1.m1.1b"><cn type="float" id="A4.T7.31.31.1.m1.1.1.cmml" xref="A4.T7.31.31.1.m1.1.1">59.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.31.31.1.m1.1c">59.0</annotation></semantics></math></td>
<td id="A4.T7.32.32.2" class="ltx_td ltx_align_center"><math id="A4.T7.32.32.2.m1.1" class="ltx_Math" alttext="14.42" display="inline"><semantics id="A4.T7.32.32.2.m1.1a"><mn id="A4.T7.32.32.2.m1.1.1" xref="A4.T7.32.32.2.m1.1.1.cmml">14.42</mn><annotation-xml encoding="MathML-Content" id="A4.T7.32.32.2.m1.1b"><cn type="float" id="A4.T7.32.32.2.m1.1.1.cmml" xref="A4.T7.32.32.2.m1.1.1">14.42</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.32.32.2.m1.1c">14.42</annotation></semantics></math></td>
<td id="A4.T7.33.33.3" class="ltx_td ltx_align_center"><math id="A4.T7.33.33.3.m1.1" class="ltx_Math" alttext="18.58" display="inline"><semantics id="A4.T7.33.33.3.m1.1a"><mn id="A4.T7.33.33.3.m1.1.1" xref="A4.T7.33.33.3.m1.1.1.cmml">18.58</mn><annotation-xml encoding="MathML-Content" id="A4.T7.33.33.3.m1.1b"><cn type="float" id="A4.T7.33.33.3.m1.1.1.cmml" xref="A4.T7.33.33.3.m1.1.1">18.58</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.33.33.3.m1.1c">18.58</annotation></semantics></math></td>
<td id="A4.T7.34.34.4" class="ltx_td ltx_align_center"><math id="A4.T7.34.34.4.m1.1" class="ltx_Math" alttext="66.95" display="inline"><semantics id="A4.T7.34.34.4.m1.1a"><mn id="A4.T7.34.34.4.m1.1.1" xref="A4.T7.34.34.4.m1.1.1.cmml">66.95</mn><annotation-xml encoding="MathML-Content" id="A4.T7.34.34.4.m1.1b"><cn type="float" id="A4.T7.34.34.4.m1.1.1.cmml" xref="A4.T7.34.34.4.m1.1.1">66.95</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.34.34.4.m1.1c">66.95</annotation></semantics></math></td>
<td id="A4.T7.35.35.5" class="ltx_td ltx_align_center"><math id="A4.T7.35.35.5.m1.1" class="ltx_Math" alttext="3.14" display="inline"><semantics id="A4.T7.35.35.5.m1.1a"><mn id="A4.T7.35.35.5.m1.1.1" xref="A4.T7.35.35.5.m1.1.1.cmml">3.14</mn><annotation-xml encoding="MathML-Content" id="A4.T7.35.35.5.m1.1b"><cn type="float" id="A4.T7.35.35.5.m1.1.1.cmml" xref="A4.T7.35.35.5.m1.1.1">3.14</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.35.35.5.m1.1c">3.14</annotation></semantics></math></td>
<td id="A4.T7.36.36.6" class="ltx_td ltx_align_center"><math id="A4.T7.36.36.6.m1.1" class="ltx_Math" alttext="0.553" display="inline"><semantics id="A4.T7.36.36.6.m1.1a"><mn id="A4.T7.36.36.6.m1.1.1" xref="A4.T7.36.36.6.m1.1.1.cmml">0.553</mn><annotation-xml encoding="MathML-Content" id="A4.T7.36.36.6.m1.1b"><cn type="float" id="A4.T7.36.36.6.m1.1.1.cmml" xref="A4.T7.36.36.6.m1.1.1">0.553</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.36.36.6.m1.1c">0.553</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.42.42" class="ltx_tr">
<td id="A4.T7.42.42.7" class="ltx_td ltx_align_left">Model F</td>
<td id="A4.T7.37.37.1" class="ltx_td ltx_align_center"><math id="A4.T7.37.37.1.m1.1" class="ltx_Math" alttext="58.0" display="inline"><semantics id="A4.T7.37.37.1.m1.1a"><mn id="A4.T7.37.37.1.m1.1.1" xref="A4.T7.37.37.1.m1.1.1.cmml">58.0</mn><annotation-xml encoding="MathML-Content" id="A4.T7.37.37.1.m1.1b"><cn type="float" id="A4.T7.37.37.1.m1.1.1.cmml" xref="A4.T7.37.37.1.m1.1.1">58.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.37.37.1.m1.1c">58.0</annotation></semantics></math></td>
<td id="A4.T7.38.38.2" class="ltx_td ltx_align_center"><math id="A4.T7.38.38.2.m1.1" class="ltx_Math" alttext="14.82" display="inline"><semantics id="A4.T7.38.38.2.m1.1a"><mn id="A4.T7.38.38.2.m1.1.1" xref="A4.T7.38.38.2.m1.1.1.cmml">14.82</mn><annotation-xml encoding="MathML-Content" id="A4.T7.38.38.2.m1.1b"><cn type="float" id="A4.T7.38.38.2.m1.1.1.cmml" xref="A4.T7.38.38.2.m1.1.1">14.82</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.38.38.2.m1.1c">14.82</annotation></semantics></math></td>
<td id="A4.T7.39.39.3" class="ltx_td ltx_align_center"><math id="A4.T7.39.39.3.m1.1" class="ltx_Math" alttext="18.89" display="inline"><semantics id="A4.T7.39.39.3.m1.1a"><mn id="A4.T7.39.39.3.m1.1.1" xref="A4.T7.39.39.3.m1.1.1.cmml">18.89</mn><annotation-xml encoding="MathML-Content" id="A4.T7.39.39.3.m1.1b"><cn type="float" id="A4.T7.39.39.3.m1.1.1.cmml" xref="A4.T7.39.39.3.m1.1.1">18.89</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.39.39.3.m1.1c">18.89</annotation></semantics></math></td>
<td id="A4.T7.40.40.4" class="ltx_td ltx_align_center"><math id="A4.T7.40.40.4.m1.1" class="ltx_Math" alttext="65.43" display="inline"><semantics id="A4.T7.40.40.4.m1.1a"><mn id="A4.T7.40.40.4.m1.1.1" xref="A4.T7.40.40.4.m1.1.1.cmml">65.43</mn><annotation-xml encoding="MathML-Content" id="A4.T7.40.40.4.m1.1b"><cn type="float" id="A4.T7.40.40.4.m1.1.1.cmml" xref="A4.T7.40.40.4.m1.1.1">65.43</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.40.40.4.m1.1c">65.43</annotation></semantics></math></td>
<td id="A4.T7.41.41.5" class="ltx_td ltx_align_center"><math id="A4.T7.41.41.5.m1.1" class="ltx_Math" alttext="3.11" display="inline"><semantics id="A4.T7.41.41.5.m1.1a"><mn id="A4.T7.41.41.5.m1.1.1" xref="A4.T7.41.41.5.m1.1.1.cmml">3.11</mn><annotation-xml encoding="MathML-Content" id="A4.T7.41.41.5.m1.1b"><cn type="float" id="A4.T7.41.41.5.m1.1.1.cmml" xref="A4.T7.41.41.5.m1.1.1">3.11</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.41.41.5.m1.1c">3.11</annotation></semantics></math></td>
<td id="A4.T7.42.42.6" class="ltx_td ltx_align_center"><math id="A4.T7.42.42.6.m1.1" class="ltx_Math" alttext="0.552" display="inline"><semantics id="A4.T7.42.42.6.m1.1a"><mn id="A4.T7.42.42.6.m1.1.1" xref="A4.T7.42.42.6.m1.1.1.cmml">0.552</mn><annotation-xml encoding="MathML-Content" id="A4.T7.42.42.6.m1.1b"><cn type="float" id="A4.T7.42.42.6.m1.1.1.cmml" xref="A4.T7.42.42.6.m1.1.1">0.552</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.42.42.6.m1.1c">0.552</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.48.48" class="ltx_tr">
<td id="A4.T7.48.48.7" class="ltx_td ltx_align_left">Model G</td>
<td id="A4.T7.43.43.1" class="ltx_td ltx_align_center"><math id="A4.T7.43.43.1.m1.1" class="ltx_Math" alttext="54.8" display="inline"><semantics id="A4.T7.43.43.1.m1.1a"><mn id="A4.T7.43.43.1.m1.1.1" xref="A4.T7.43.43.1.m1.1.1.cmml">54.8</mn><annotation-xml encoding="MathML-Content" id="A4.T7.43.43.1.m1.1b"><cn type="float" id="A4.T7.43.43.1.m1.1.1.cmml" xref="A4.T7.43.43.1.m1.1.1">54.8</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.43.43.1.m1.1c">54.8</annotation></semantics></math></td>
<td id="A4.T7.44.44.2" class="ltx_td ltx_align_center"><math id="A4.T7.44.44.2.m1.1" class="ltx_Math" alttext="14.41" display="inline"><semantics id="A4.T7.44.44.2.m1.1a"><mn id="A4.T7.44.44.2.m1.1.1" xref="A4.T7.44.44.2.m1.1.1.cmml">14.41</mn><annotation-xml encoding="MathML-Content" id="A4.T7.44.44.2.m1.1b"><cn type="float" id="A4.T7.44.44.2.m1.1.1.cmml" xref="A4.T7.44.44.2.m1.1.1">14.41</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.44.44.2.m1.1c">14.41</annotation></semantics></math></td>
<td id="A4.T7.45.45.3" class="ltx_td ltx_align_center"><math id="A4.T7.45.45.3.m1.1" class="ltx_Math" alttext="17.86" display="inline"><semantics id="A4.T7.45.45.3.m1.1a"><mn id="A4.T7.45.45.3.m1.1.1" xref="A4.T7.45.45.3.m1.1.1.cmml">17.86</mn><annotation-xml encoding="MathML-Content" id="A4.T7.45.45.3.m1.1b"><cn type="float" id="A4.T7.45.45.3.m1.1.1.cmml" xref="A4.T7.45.45.3.m1.1.1">17.86</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.45.45.3.m1.1c">17.86</annotation></semantics></math></td>
<td id="A4.T7.46.46.4" class="ltx_td ltx_align_center"><math id="A4.T7.46.46.4.m1.1" class="ltx_Math" alttext="64.67" display="inline"><semantics id="A4.T7.46.46.4.m1.1a"><mn id="A4.T7.46.46.4.m1.1.1" xref="A4.T7.46.46.4.m1.1.1.cmml">64.67</mn><annotation-xml encoding="MathML-Content" id="A4.T7.46.46.4.m1.1b"><cn type="float" id="A4.T7.46.46.4.m1.1.1.cmml" xref="A4.T7.46.46.4.m1.1.1">64.67</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.46.46.4.m1.1c">64.67</annotation></semantics></math></td>
<td id="A4.T7.47.47.5" class="ltx_td ltx_align_center"><math id="A4.T7.47.47.5.m1.1" class="ltx_Math" alttext="2.98" display="inline"><semantics id="A4.T7.47.47.5.m1.1a"><mn id="A4.T7.47.47.5.m1.1.1" xref="A4.T7.47.47.5.m1.1.1.cmml">2.98</mn><annotation-xml encoding="MathML-Content" id="A4.T7.47.47.5.m1.1b"><cn type="float" id="A4.T7.47.47.5.m1.1.1.cmml" xref="A4.T7.47.47.5.m1.1.1">2.98</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.47.47.5.m1.1c">2.98</annotation></semantics></math></td>
<td id="A4.T7.48.48.6" class="ltx_td ltx_align_center"><math id="A4.T7.48.48.6.m1.1" class="ltx_Math" alttext="0.529" display="inline"><semantics id="A4.T7.48.48.6.m1.1a"><mn id="A4.T7.48.48.6.m1.1.1" xref="A4.T7.48.48.6.m1.1.1.cmml">0.529</mn><annotation-xml encoding="MathML-Content" id="A4.T7.48.48.6.m1.1b"><cn type="float" id="A4.T7.48.48.6.m1.1.1.cmml" xref="A4.T7.48.48.6.m1.1.1">0.529</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.48.48.6.m1.1c">0.529</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.54.54" class="ltx_tr">
<td id="A4.T7.54.54.7" class="ltx_td ltx_align_left">Model H</td>
<td id="A4.T7.49.49.1" class="ltx_td ltx_align_center"><math id="A4.T7.49.49.1.m1.1" class="ltx_Math" alttext="50.0" display="inline"><semantics id="A4.T7.49.49.1.m1.1a"><mn id="A4.T7.49.49.1.m1.1.1" xref="A4.T7.49.49.1.m1.1.1.cmml">50.0</mn><annotation-xml encoding="MathML-Content" id="A4.T7.49.49.1.m1.1b"><cn type="float" id="A4.T7.49.49.1.m1.1.1.cmml" xref="A4.T7.49.49.1.m1.1.1">50.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.49.49.1.m1.1c">50.0</annotation></semantics></math></td>
<td id="A4.T7.50.50.2" class="ltx_td ltx_align_center"><math id="A4.T7.50.50.2.m1.1" class="ltx_Math" alttext="13.29" display="inline"><semantics id="A4.T7.50.50.2.m1.1a"><mn id="A4.T7.50.50.2.m1.1.1" xref="A4.T7.50.50.2.m1.1.1.cmml">13.29</mn><annotation-xml encoding="MathML-Content" id="A4.T7.50.50.2.m1.1b"><cn type="float" id="A4.T7.50.50.2.m1.1.1.cmml" xref="A4.T7.50.50.2.m1.1.1">13.29</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.50.50.2.m1.1c">13.29</annotation></semantics></math></td>
<td id="A4.T7.51.51.3" class="ltx_td ltx_align_center"><math id="A4.T7.51.51.3.m1.1" class="ltx_Math" alttext="17.44" display="inline"><semantics id="A4.T7.51.51.3.m1.1a"><mn id="A4.T7.51.51.3.m1.1.1" xref="A4.T7.51.51.3.m1.1.1.cmml">17.44</mn><annotation-xml encoding="MathML-Content" id="A4.T7.51.51.3.m1.1b"><cn type="float" id="A4.T7.51.51.3.m1.1.1.cmml" xref="A4.T7.51.51.3.m1.1.1">17.44</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.51.51.3.m1.1c">17.44</annotation></semantics></math></td>
<td id="A4.T7.52.52.4" class="ltx_td ltx_align_center"><math id="A4.T7.52.52.4.m1.1" class="ltx_Math" alttext="59.87" display="inline"><semantics id="A4.T7.52.52.4.m1.1a"><mn id="A4.T7.52.52.4.m1.1.1" xref="A4.T7.52.52.4.m1.1.1.cmml">59.87</mn><annotation-xml encoding="MathML-Content" id="A4.T7.52.52.4.m1.1b"><cn type="float" id="A4.T7.52.52.4.m1.1.1.cmml" xref="A4.T7.52.52.4.m1.1.1">59.87</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.52.52.4.m1.1c">59.87</annotation></semantics></math></td>
<td id="A4.T7.53.53.5" class="ltx_td ltx_align_center"><math id="A4.T7.53.53.5.m1.1" class="ltx_Math" alttext="2.88" display="inline"><semantics id="A4.T7.53.53.5.m1.1a"><mn id="A4.T7.53.53.5.m1.1.1" xref="A4.T7.53.53.5.m1.1.1.cmml">2.88</mn><annotation-xml encoding="MathML-Content" id="A4.T7.53.53.5.m1.1b"><cn type="float" id="A4.T7.53.53.5.m1.1.1.cmml" xref="A4.T7.53.53.5.m1.1.1">2.88</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.53.53.5.m1.1c">2.88</annotation></semantics></math></td>
<td id="A4.T7.54.54.6" class="ltx_td ltx_align_center"><math id="A4.T7.54.54.6.m1.1" class="ltx_Math" alttext="0.520" display="inline"><semantics id="A4.T7.54.54.6.m1.1a"><mn id="A4.T7.54.54.6.m1.1.1" xref="A4.T7.54.54.6.m1.1.1.cmml">0.520</mn><annotation-xml encoding="MathML-Content" id="A4.T7.54.54.6.m1.1b"><cn type="float" id="A4.T7.54.54.6.m1.1.1.cmml" xref="A4.T7.54.54.6.m1.1.1">0.520</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.54.54.6.m1.1c">0.520</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.60.60" class="ltx_tr">
<td id="A4.T7.60.60.7" class="ltx_td ltx_align_left">Model I</td>
<td id="A4.T7.55.55.1" class="ltx_td ltx_align_center"><math id="A4.T7.55.55.1.m1.1" class="ltx_Math" alttext="53.0" display="inline"><semantics id="A4.T7.55.55.1.m1.1a"><mn id="A4.T7.55.55.1.m1.1.1" xref="A4.T7.55.55.1.m1.1.1.cmml">53.0</mn><annotation-xml encoding="MathML-Content" id="A4.T7.55.55.1.m1.1b"><cn type="float" id="A4.T7.55.55.1.m1.1.1.cmml" xref="A4.T7.55.55.1.m1.1.1">53.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.55.55.1.m1.1c">53.0</annotation></semantics></math></td>
<td id="A4.T7.56.56.2" class="ltx_td ltx_align_center"><math id="A4.T7.56.56.2.m1.1" class="ltx_Math" alttext="14.63" display="inline"><semantics id="A4.T7.56.56.2.m1.1a"><mn id="A4.T7.56.56.2.m1.1.1" xref="A4.T7.56.56.2.m1.1.1.cmml">14.63</mn><annotation-xml encoding="MathML-Content" id="A4.T7.56.56.2.m1.1b"><cn type="float" id="A4.T7.56.56.2.m1.1.1.cmml" xref="A4.T7.56.56.2.m1.1.1">14.63</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.56.56.2.m1.1c">14.63</annotation></semantics></math></td>
<td id="A4.T7.57.57.3" class="ltx_td ltx_align_center"><math id="A4.T7.57.57.3.m1.1" class="ltx_Math" alttext="17.98" display="inline"><semantics id="A4.T7.57.57.3.m1.1a"><mn id="A4.T7.57.57.3.m1.1.1" xref="A4.T7.57.57.3.m1.1.1.cmml">17.98</mn><annotation-xml encoding="MathML-Content" id="A4.T7.57.57.3.m1.1b"><cn type="float" id="A4.T7.57.57.3.m1.1.1.cmml" xref="A4.T7.57.57.3.m1.1.1">17.98</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.57.57.3.m1.1c">17.98</annotation></semantics></math></td>
<td id="A4.T7.58.58.4" class="ltx_td ltx_align_center"><math id="A4.T7.58.58.4.m1.1" class="ltx_Math" alttext="64.45" display="inline"><semantics id="A4.T7.58.58.4.m1.1a"><mn id="A4.T7.58.58.4.m1.1.1" xref="A4.T7.58.58.4.m1.1.1.cmml">64.45</mn><annotation-xml encoding="MathML-Content" id="A4.T7.58.58.4.m1.1b"><cn type="float" id="A4.T7.58.58.4.m1.1.1.cmml" xref="A4.T7.58.58.4.m1.1.1">64.45</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.58.58.4.m1.1c">64.45</annotation></semantics></math></td>
<td id="A4.T7.59.59.5" class="ltx_td ltx_align_center"><math id="A4.T7.59.59.5.m1.1" class="ltx_Math" alttext="2.96" display="inline"><semantics id="A4.T7.59.59.5.m1.1a"><mn id="A4.T7.59.59.5.m1.1.1" xref="A4.T7.59.59.5.m1.1.1.cmml">2.96</mn><annotation-xml encoding="MathML-Content" id="A4.T7.59.59.5.m1.1b"><cn type="float" id="A4.T7.59.59.5.m1.1.1.cmml" xref="A4.T7.59.59.5.m1.1.1">2.96</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.59.59.5.m1.1c">2.96</annotation></semantics></math></td>
<td id="A4.T7.60.60.6" class="ltx_td ltx_align_center"><math id="A4.T7.60.60.6.m1.1" class="ltx_Math" alttext="0.510" display="inline"><semantics id="A4.T7.60.60.6.m1.1a"><mn id="A4.T7.60.60.6.m1.1.1" xref="A4.T7.60.60.6.m1.1.1.cmml">0.510</mn><annotation-xml encoding="MathML-Content" id="A4.T7.60.60.6.m1.1b"><cn type="float" id="A4.T7.60.60.6.m1.1.1.cmml" xref="A4.T7.60.60.6.m1.1.1">0.510</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.60.60.6.m1.1c">0.510</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.66.66" class="ltx_tr">
<td id="A4.T7.66.66.7" class="ltx_td ltx_align_left">Model J</td>
<td id="A4.T7.61.61.1" class="ltx_td ltx_align_center"><math id="A4.T7.61.61.1.m1.1" class="ltx_Math" alttext="52.6" display="inline"><semantics id="A4.T7.61.61.1.m1.1a"><mn id="A4.T7.61.61.1.m1.1.1" xref="A4.T7.61.61.1.m1.1.1.cmml">52.6</mn><annotation-xml encoding="MathML-Content" id="A4.T7.61.61.1.m1.1b"><cn type="float" id="A4.T7.61.61.1.m1.1.1.cmml" xref="A4.T7.61.61.1.m1.1.1">52.6</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.61.61.1.m1.1c">52.6</annotation></semantics></math></td>
<td id="A4.T7.62.62.2" class="ltx_td ltx_align_center"><math id="A4.T7.62.62.2.m1.1" class="ltx_Math" alttext="12.17" display="inline"><semantics id="A4.T7.62.62.2.m1.1a"><mn id="A4.T7.62.62.2.m1.1.1" xref="A4.T7.62.62.2.m1.1.1.cmml">12.17</mn><annotation-xml encoding="MathML-Content" id="A4.T7.62.62.2.m1.1b"><cn type="float" id="A4.T7.62.62.2.m1.1.1.cmml" xref="A4.T7.62.62.2.m1.1.1">12.17</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.62.62.2.m1.1c">12.17</annotation></semantics></math></td>
<td id="A4.T7.63.63.3" class="ltx_td ltx_align_center"><math id="A4.T7.63.63.3.m1.1" class="ltx_Math" alttext="17.59" display="inline"><semantics id="A4.T7.63.63.3.m1.1a"><mn id="A4.T7.63.63.3.m1.1.1" xref="A4.T7.63.63.3.m1.1.1.cmml">17.59</mn><annotation-xml encoding="MathML-Content" id="A4.T7.63.63.3.m1.1b"><cn type="float" id="A4.T7.63.63.3.m1.1.1.cmml" xref="A4.T7.63.63.3.m1.1.1">17.59</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.63.63.3.m1.1c">17.59</annotation></semantics></math></td>
<td id="A4.T7.64.64.4" class="ltx_td ltx_align_center"><math id="A4.T7.64.64.4.m1.1" class="ltx_Math" alttext="50.45" display="inline"><semantics id="A4.T7.64.64.4.m1.1a"><mn id="A4.T7.64.64.4.m1.1.1" xref="A4.T7.64.64.4.m1.1.1.cmml">50.45</mn><annotation-xml encoding="MathML-Content" id="A4.T7.64.64.4.m1.1b"><cn type="float" id="A4.T7.64.64.4.m1.1.1.cmml" xref="A4.T7.64.64.4.m1.1.1">50.45</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.64.64.4.m1.1c">50.45</annotation></semantics></math></td>
<td id="A4.T7.65.65.5" class="ltx_td ltx_align_center"><math id="A4.T7.65.65.5.m1.1" class="ltx_Math" alttext="3.00" display="inline"><semantics id="A4.T7.65.65.5.m1.1a"><mn id="A4.T7.65.65.5.m1.1.1" xref="A4.T7.65.65.5.m1.1.1.cmml">3.00</mn><annotation-xml encoding="MathML-Content" id="A4.T7.65.65.5.m1.1b"><cn type="float" id="A4.T7.65.65.5.m1.1.1.cmml" xref="A4.T7.65.65.5.m1.1.1">3.00</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.65.65.5.m1.1c">3.00</annotation></semantics></math></td>
<td id="A4.T7.66.66.6" class="ltx_td ltx_align_center"><math id="A4.T7.66.66.6.m1.1" class="ltx_Math" alttext="0.509" display="inline"><semantics id="A4.T7.66.66.6.m1.1a"><mn id="A4.T7.66.66.6.m1.1.1" xref="A4.T7.66.66.6.m1.1.1.cmml">0.509</mn><annotation-xml encoding="MathML-Content" id="A4.T7.66.66.6.m1.1b"><cn type="float" id="A4.T7.66.66.6.m1.1.1.cmml" xref="A4.T7.66.66.6.m1.1.1">0.509</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.66.66.6.m1.1c">0.509</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.72.72" class="ltx_tr">
<td id="A4.T7.72.72.7" class="ltx_td ltx_align_left">Model K</td>
<td id="A4.T7.67.67.1" class="ltx_td ltx_align_center"><math id="A4.T7.67.67.1.m1.1" class="ltx_Math" alttext="53.0" display="inline"><semantics id="A4.T7.67.67.1.m1.1a"><mn id="A4.T7.67.67.1.m1.1.1" xref="A4.T7.67.67.1.m1.1.1.cmml">53.0</mn><annotation-xml encoding="MathML-Content" id="A4.T7.67.67.1.m1.1b"><cn type="float" id="A4.T7.67.67.1.m1.1.1.cmml" xref="A4.T7.67.67.1.m1.1.1">53.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.67.67.1.m1.1c">53.0</annotation></semantics></math></td>
<td id="A4.T7.68.68.2" class="ltx_td ltx_align_center"><math id="A4.T7.68.68.2.m1.1" class="ltx_Math" alttext="13.20" display="inline"><semantics id="A4.T7.68.68.2.m1.1a"><mn id="A4.T7.68.68.2.m1.1.1" xref="A4.T7.68.68.2.m1.1.1.cmml">13.20</mn><annotation-xml encoding="MathML-Content" id="A4.T7.68.68.2.m1.1b"><cn type="float" id="A4.T7.68.68.2.m1.1.1.cmml" xref="A4.T7.68.68.2.m1.1.1">13.20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.68.68.2.m1.1c">13.20</annotation></semantics></math></td>
<td id="A4.T7.69.69.3" class="ltx_td ltx_align_center"><math id="A4.T7.69.69.3.m1.1" class="ltx_Math" alttext="18.03" display="inline"><semantics id="A4.T7.69.69.3.m1.1a"><mn id="A4.T7.69.69.3.m1.1.1" xref="A4.T7.69.69.3.m1.1.1.cmml">18.03</mn><annotation-xml encoding="MathML-Content" id="A4.T7.69.69.3.m1.1b"><cn type="float" id="A4.T7.69.69.3.m1.1.1.cmml" xref="A4.T7.69.69.3.m1.1.1">18.03</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.69.69.3.m1.1c">18.03</annotation></semantics></math></td>
<td id="A4.T7.70.70.4" class="ltx_td ltx_align_center"><math id="A4.T7.70.70.4.m1.1" class="ltx_Math" alttext="54.90" display="inline"><semantics id="A4.T7.70.70.4.m1.1a"><mn id="A4.T7.70.70.4.m1.1.1" xref="A4.T7.70.70.4.m1.1.1.cmml">54.90</mn><annotation-xml encoding="MathML-Content" id="A4.T7.70.70.4.m1.1b"><cn type="float" id="A4.T7.70.70.4.m1.1.1.cmml" xref="A4.T7.70.70.4.m1.1.1">54.90</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.70.70.4.m1.1c">54.90</annotation></semantics></math></td>
<td id="A4.T7.71.71.5" class="ltx_td ltx_align_center"><math id="A4.T7.71.71.5.m1.1" class="ltx_Math" alttext="3.04" display="inline"><semantics id="A4.T7.71.71.5.m1.1a"><mn id="A4.T7.71.71.5.m1.1.1" xref="A4.T7.71.71.5.m1.1.1.cmml">3.04</mn><annotation-xml encoding="MathML-Content" id="A4.T7.71.71.5.m1.1b"><cn type="float" id="A4.T7.71.71.5.m1.1.1.cmml" xref="A4.T7.71.71.5.m1.1.1">3.04</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.71.71.5.m1.1c">3.04</annotation></semantics></math></td>
<td id="A4.T7.72.72.6" class="ltx_td ltx_align_center"><math id="A4.T7.72.72.6.m1.1" class="ltx_Math" alttext="0.500" display="inline"><semantics id="A4.T7.72.72.6.m1.1a"><mn id="A4.T7.72.72.6.m1.1.1" xref="A4.T7.72.72.6.m1.1.1.cmml">0.500</mn><annotation-xml encoding="MathML-Content" id="A4.T7.72.72.6.m1.1b"><cn type="float" id="A4.T7.72.72.6.m1.1.1.cmml" xref="A4.T7.72.72.6.m1.1.1">0.500</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.72.72.6.m1.1c">0.500</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.78.78" class="ltx_tr">
<td id="A4.T7.78.78.7" class="ltx_td ltx_align_left">Model L</td>
<td id="A4.T7.73.73.1" class="ltx_td ltx_align_center"><math id="A4.T7.73.73.1.m1.1" class="ltx_Math" alttext="51.2" display="inline"><semantics id="A4.T7.73.73.1.m1.1a"><mn id="A4.T7.73.73.1.m1.1.1" xref="A4.T7.73.73.1.m1.1.1.cmml">51.2</mn><annotation-xml encoding="MathML-Content" id="A4.T7.73.73.1.m1.1b"><cn type="float" id="A4.T7.73.73.1.m1.1.1.cmml" xref="A4.T7.73.73.1.m1.1.1">51.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.73.73.1.m1.1c">51.2</annotation></semantics></math></td>
<td id="A4.T7.74.74.2" class="ltx_td ltx_align_center"><math id="A4.T7.74.74.2.m1.1" class="ltx_Math" alttext="14.69" display="inline"><semantics id="A4.T7.74.74.2.m1.1a"><mn id="A4.T7.74.74.2.m1.1.1" xref="A4.T7.74.74.2.m1.1.1.cmml">14.69</mn><annotation-xml encoding="MathML-Content" id="A4.T7.74.74.2.m1.1b"><cn type="float" id="A4.T7.74.74.2.m1.1.1.cmml" xref="A4.T7.74.74.2.m1.1.1">14.69</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.74.74.2.m1.1c">14.69</annotation></semantics></math></td>
<td id="A4.T7.75.75.3" class="ltx_td ltx_align_center"><math id="A4.T7.75.75.3.m1.1" class="ltx_Math" alttext="17.83" display="inline"><semantics id="A4.T7.75.75.3.m1.1a"><mn id="A4.T7.75.75.3.m1.1.1" xref="A4.T7.75.75.3.m1.1.1.cmml">17.83</mn><annotation-xml encoding="MathML-Content" id="A4.T7.75.75.3.m1.1b"><cn type="float" id="A4.T7.75.75.3.m1.1.1.cmml" xref="A4.T7.75.75.3.m1.1.1">17.83</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.75.75.3.m1.1c">17.83</annotation></semantics></math></td>
<td id="A4.T7.76.76.4" class="ltx_td ltx_align_center"><math id="A4.T7.76.76.4.m1.1" class="ltx_Math" alttext="64.51" display="inline"><semantics id="A4.T7.76.76.4.m1.1a"><mn id="A4.T7.76.76.4.m1.1.1" xref="A4.T7.76.76.4.m1.1.1.cmml">64.51</mn><annotation-xml encoding="MathML-Content" id="A4.T7.76.76.4.m1.1b"><cn type="float" id="A4.T7.76.76.4.m1.1.1.cmml" xref="A4.T7.76.76.4.m1.1.1">64.51</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.76.76.4.m1.1c">64.51</annotation></semantics></math></td>
<td id="A4.T7.77.77.5" class="ltx_td ltx_align_center"><math id="A4.T7.77.77.5.m1.1" class="ltx_Math" alttext="2.91" display="inline"><semantics id="A4.T7.77.77.5.m1.1a"><mn id="A4.T7.77.77.5.m1.1.1" xref="A4.T7.77.77.5.m1.1.1.cmml">2.91</mn><annotation-xml encoding="MathML-Content" id="A4.T7.77.77.5.m1.1b"><cn type="float" id="A4.T7.77.77.5.m1.1.1.cmml" xref="A4.T7.77.77.5.m1.1.1">2.91</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.77.77.5.m1.1c">2.91</annotation></semantics></math></td>
<td id="A4.T7.78.78.6" class="ltx_td ltx_align_center"><math id="A4.T7.78.78.6.m1.1" class="ltx_Math" alttext="0.485" display="inline"><semantics id="A4.T7.78.78.6.m1.1a"><mn id="A4.T7.78.78.6.m1.1.1" xref="A4.T7.78.78.6.m1.1.1.cmml">0.485</mn><annotation-xml encoding="MathML-Content" id="A4.T7.78.78.6.m1.1b"><cn type="float" id="A4.T7.78.78.6.m1.1.1.cmml" xref="A4.T7.78.78.6.m1.1.1">0.485</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.78.78.6.m1.1c">0.485</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.84.84" class="ltx_tr">
<td id="A4.T7.84.84.7" class="ltx_td ltx_align_left">Model M</td>
<td id="A4.T7.79.79.1" class="ltx_td ltx_align_center"><math id="A4.T7.79.79.1.m1.1" class="ltx_Math" alttext="43.2" display="inline"><semantics id="A4.T7.79.79.1.m1.1a"><mn id="A4.T7.79.79.1.m1.1.1" xref="A4.T7.79.79.1.m1.1.1.cmml">43.2</mn><annotation-xml encoding="MathML-Content" id="A4.T7.79.79.1.m1.1b"><cn type="float" id="A4.T7.79.79.1.m1.1.1.cmml" xref="A4.T7.79.79.1.m1.1.1">43.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.79.79.1.m1.1c">43.2</annotation></semantics></math></td>
<td id="A4.T7.80.80.2" class="ltx_td ltx_align_center"><math id="A4.T7.80.80.2.m1.1" class="ltx_Math" alttext="13.76" display="inline"><semantics id="A4.T7.80.80.2.m1.1a"><mn id="A4.T7.80.80.2.m1.1.1" xref="A4.T7.80.80.2.m1.1.1.cmml">13.76</mn><annotation-xml encoding="MathML-Content" id="A4.T7.80.80.2.m1.1b"><cn type="float" id="A4.T7.80.80.2.m1.1.1.cmml" xref="A4.T7.80.80.2.m1.1.1">13.76</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.80.80.2.m1.1c">13.76</annotation></semantics></math></td>
<td id="A4.T7.81.81.3" class="ltx_td ltx_align_center"><math id="A4.T7.81.81.3.m1.1" class="ltx_Math" alttext="17.37" display="inline"><semantics id="A4.T7.81.81.3.m1.1a"><mn id="A4.T7.81.81.3.m1.1.1" xref="A4.T7.81.81.3.m1.1.1.cmml">17.37</mn><annotation-xml encoding="MathML-Content" id="A4.T7.81.81.3.m1.1b"><cn type="float" id="A4.T7.81.81.3.m1.1.1.cmml" xref="A4.T7.81.81.3.m1.1.1">17.37</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.81.81.3.m1.1c">17.37</annotation></semantics></math></td>
<td id="A4.T7.82.82.4" class="ltx_td ltx_align_center"><math id="A4.T7.82.82.4.m1.1" class="ltx_Math" alttext="60.36" display="inline"><semantics id="A4.T7.82.82.4.m1.1a"><mn id="A4.T7.82.82.4.m1.1.1" xref="A4.T7.82.82.4.m1.1.1.cmml">60.36</mn><annotation-xml encoding="MathML-Content" id="A4.T7.82.82.4.m1.1b"><cn type="float" id="A4.T7.82.82.4.m1.1.1.cmml" xref="A4.T7.82.82.4.m1.1.1">60.36</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.82.82.4.m1.1c">60.36</annotation></semantics></math></td>
<td id="A4.T7.83.83.5" class="ltx_td ltx_align_center"><math id="A4.T7.83.83.5.m1.1" class="ltx_Math" alttext="2.67" display="inline"><semantics id="A4.T7.83.83.5.m1.1a"><mn id="A4.T7.83.83.5.m1.1.1" xref="A4.T7.83.83.5.m1.1.1.cmml">2.67</mn><annotation-xml encoding="MathML-Content" id="A4.T7.83.83.5.m1.1b"><cn type="float" id="A4.T7.83.83.5.m1.1.1.cmml" xref="A4.T7.83.83.5.m1.1.1">2.67</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.83.83.5.m1.1c">2.67</annotation></semantics></math></td>
<td id="A4.T7.84.84.6" class="ltx_td ltx_align_center"><math id="A4.T7.84.84.6.m1.1" class="ltx_Math" alttext="0.371" display="inline"><semantics id="A4.T7.84.84.6.m1.1a"><mn id="A4.T7.84.84.6.m1.1.1" xref="A4.T7.84.84.6.m1.1.1.cmml">0.371</mn><annotation-xml encoding="MathML-Content" id="A4.T7.84.84.6.m1.1b"><cn type="float" id="A4.T7.84.84.6.m1.1.1.cmml" xref="A4.T7.84.84.6.m1.1.1">0.371</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.84.84.6.m1.1c">0.371</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.90.90" class="ltx_tr">
<td id="A4.T7.90.90.7" class="ltx_td"></td>
<td id="A4.T7.90.90.8" class="ltx_td ltx_align_left">Model N</td>
<td id="A4.T7.85.85.1" class="ltx_td ltx_align_center"><math id="A4.T7.85.85.1.m1.1" class="ltx_Math" alttext="35.8" display="inline"><semantics id="A4.T7.85.85.1.m1.1a"><mn id="A4.T7.85.85.1.m1.1.1" xref="A4.T7.85.85.1.m1.1.1.cmml">35.8</mn><annotation-xml encoding="MathML-Content" id="A4.T7.85.85.1.m1.1b"><cn type="float" id="A4.T7.85.85.1.m1.1.1.cmml" xref="A4.T7.85.85.1.m1.1.1">35.8</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.85.85.1.m1.1c">35.8</annotation></semantics></math></td>
<td id="A4.T7.86.86.2" class="ltx_td ltx_align_center"><math id="A4.T7.86.86.2.m1.1" class="ltx_Math" alttext="13.18" display="inline"><semantics id="A4.T7.86.86.2.m1.1a"><mn id="A4.T7.86.86.2.m1.1.1" xref="A4.T7.86.86.2.m1.1.1.cmml">13.18</mn><annotation-xml encoding="MathML-Content" id="A4.T7.86.86.2.m1.1b"><cn type="float" id="A4.T7.86.86.2.m1.1.1.cmml" xref="A4.T7.86.86.2.m1.1.1">13.18</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.86.86.2.m1.1c">13.18</annotation></semantics></math></td>
<td id="A4.T7.87.87.3" class="ltx_td ltx_align_center"><math id="A4.T7.87.87.3.m1.1" class="ltx_Math" alttext="15.67" display="inline"><semantics id="A4.T7.87.87.3.m1.1a"><mn id="A4.T7.87.87.3.m1.1.1" xref="A4.T7.87.87.3.m1.1.1.cmml">15.67</mn><annotation-xml encoding="MathML-Content" id="A4.T7.87.87.3.m1.1b"><cn type="float" id="A4.T7.87.87.3.m1.1.1.cmml" xref="A4.T7.87.87.3.m1.1.1">15.67</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.87.87.3.m1.1c">15.67</annotation></semantics></math></td>
<td id="A4.T7.88.88.4" class="ltx_td ltx_align_center"><math id="A4.T7.88.88.4.m1.1" class="ltx_Math" alttext="56.07" display="inline"><semantics id="A4.T7.88.88.4.m1.1a"><mn id="A4.T7.88.88.4.m1.1.1" xref="A4.T7.88.88.4.m1.1.1.cmml">56.07</mn><annotation-xml encoding="MathML-Content" id="A4.T7.88.88.4.m1.1b"><cn type="float" id="A4.T7.88.88.4.m1.1.1.cmml" xref="A4.T7.88.88.4.m1.1.1">56.07</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.88.88.4.m1.1c">56.07</annotation></semantics></math></td>
<td id="A4.T7.89.89.5" class="ltx_td ltx_align_center"><math id="A4.T7.89.89.5.m1.1" class="ltx_Math" alttext="2.41" display="inline"><semantics id="A4.T7.89.89.5.m1.1a"><mn id="A4.T7.89.89.5.m1.1.1" xref="A4.T7.89.89.5.m1.1.1.cmml">2.41</mn><annotation-xml encoding="MathML-Content" id="A4.T7.89.89.5.m1.1b"><cn type="float" id="A4.T7.89.89.5.m1.1.1.cmml" xref="A4.T7.89.89.5.m1.1.1">2.41</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.89.89.5.m1.1c">2.41</annotation></semantics></math></td>
<td id="A4.T7.90.90.6" class="ltx_td ltx_align_center"><math id="A4.T7.90.90.6.m1.1" class="ltx_Math" alttext="0.361" display="inline"><semantics id="A4.T7.90.90.6.m1.1a"><mn id="A4.T7.90.90.6.m1.1.1" xref="A4.T7.90.90.6.m1.1.1.cmml">0.361</mn><annotation-xml encoding="MathML-Content" id="A4.T7.90.90.6.m1.1b"><cn type="float" id="A4.T7.90.90.6.m1.1.1.cmml" xref="A4.T7.90.90.6.m1.1.1">0.361</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.90.90.6.m1.1c">0.361</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.96.96" class="ltx_tr">
<td id="A4.T7.96.96.7" class="ltx_td"></td>
<td id="A4.T7.96.96.8" class="ltx_td ltx_align_left">Model O</td>
<td id="A4.T7.91.91.1" class="ltx_td ltx_align_center"><math id="A4.T7.91.91.1.m1.1" class="ltx_Math" alttext="33.6" display="inline"><semantics id="A4.T7.91.91.1.m1.1a"><mn id="A4.T7.91.91.1.m1.1.1" xref="A4.T7.91.91.1.m1.1.1.cmml">33.6</mn><annotation-xml encoding="MathML-Content" id="A4.T7.91.91.1.m1.1b"><cn type="float" id="A4.T7.91.91.1.m1.1.1.cmml" xref="A4.T7.91.91.1.m1.1.1">33.6</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.91.91.1.m1.1c">33.6</annotation></semantics></math></td>
<td id="A4.T7.92.92.2" class="ltx_td ltx_align_center"><math id="A4.T7.92.92.2.m1.1" class="ltx_Math" alttext="8.33" display="inline"><semantics id="A4.T7.92.92.2.m1.1a"><mn id="A4.T7.92.92.2.m1.1.1" xref="A4.T7.92.92.2.m1.1.1.cmml">8.33</mn><annotation-xml encoding="MathML-Content" id="A4.T7.92.92.2.m1.1b"><cn type="float" id="A4.T7.92.92.2.m1.1.1.cmml" xref="A4.T7.92.92.2.m1.1.1">8.33</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.92.92.2.m1.1c">8.33</annotation></semantics></math></td>
<td id="A4.T7.93.93.3" class="ltx_td ltx_align_center"><math id="A4.T7.93.93.3.m1.1" class="ltx_Math" alttext="14.33" display="inline"><semantics id="A4.T7.93.93.3.m1.1a"><mn id="A4.T7.93.93.3.m1.1.1" xref="A4.T7.93.93.3.m1.1.1.cmml">14.33</mn><annotation-xml encoding="MathML-Content" id="A4.T7.93.93.3.m1.1b"><cn type="float" id="A4.T7.93.93.3.m1.1.1.cmml" xref="A4.T7.93.93.3.m1.1.1">14.33</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.93.93.3.m1.1c">14.33</annotation></semantics></math></td>
<td id="A4.T7.94.94.4" class="ltx_td ltx_align_center"><math id="A4.T7.94.94.4.m1.1" class="ltx_Math" alttext="39.16" display="inline"><semantics id="A4.T7.94.94.4.m1.1a"><mn id="A4.T7.94.94.4.m1.1.1" xref="A4.T7.94.94.4.m1.1.1.cmml">39.16</mn><annotation-xml encoding="MathML-Content" id="A4.T7.94.94.4.m1.1b"><cn type="float" id="A4.T7.94.94.4.m1.1.1.cmml" xref="A4.T7.94.94.4.m1.1.1">39.16</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.94.94.4.m1.1c">39.16</annotation></semantics></math></td>
<td id="A4.T7.95.95.5" class="ltx_td ltx_align_center"><math id="A4.T7.95.95.5.m1.1" class="ltx_Math" alttext="2.07" display="inline"><semantics id="A4.T7.95.95.5.m1.1a"><mn id="A4.T7.95.95.5.m1.1.1" xref="A4.T7.95.95.5.m1.1.1.cmml">2.07</mn><annotation-xml encoding="MathML-Content" id="A4.T7.95.95.5.m1.1b"><cn type="float" id="A4.T7.95.95.5.m1.1.1.cmml" xref="A4.T7.95.95.5.m1.1.1">2.07</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.95.95.5.m1.1c">2.07</annotation></semantics></math></td>
<td id="A4.T7.96.96.6" class="ltx_td ltx_align_center"><math id="A4.T7.96.96.6.m1.1" class="ltx_Math" alttext="0.279" display="inline"><semantics id="A4.T7.96.96.6.m1.1a"><mn id="A4.T7.96.96.6.m1.1.1" xref="A4.T7.96.96.6.m1.1.1.cmml">0.279</mn><annotation-xml encoding="MathML-Content" id="A4.T7.96.96.6.m1.1b"><cn type="float" id="A4.T7.96.96.6.m1.1.1.cmml" xref="A4.T7.96.96.6.m1.1.1">0.279</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.96.96.6.m1.1c">0.279</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.102.102" class="ltx_tr">
<td id="A4.T7.102.102.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A4.T7.102.102.7.1" class="ltx_text ltx_font_bold ltx_font_italic">Humans</span></td>
<td id="A4.T7.102.102.8" class="ltx_td ltx_align_left ltx_border_t">Human labellers group A</td>
<td id="A4.T7.97.97.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.97.97.1.m1.1" class="ltx_Math" alttext="96.6" display="inline"><semantics id="A4.T7.97.97.1.m1.1a"><mn id="A4.T7.97.97.1.m1.1.1" xref="A4.T7.97.97.1.m1.1.1.cmml">96.6</mn><annotation-xml encoding="MathML-Content" id="A4.T7.97.97.1.m1.1b"><cn type="float" id="A4.T7.97.97.1.m1.1.1.cmml" xref="A4.T7.97.97.1.m1.1.1">96.6</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.97.97.1.m1.1c">96.6</annotation></semantics></math></td>
<td id="A4.T7.98.98.2" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.98.98.2.m1.1" class="ltx_Math" alttext="81.04" display="inline"><semantics id="A4.T7.98.98.2.m1.1a"><mn id="A4.T7.98.98.2.m1.1.1" xref="A4.T7.98.98.2.m1.1.1.cmml">81.04</mn><annotation-xml encoding="MathML-Content" id="A4.T7.98.98.2.m1.1b"><cn type="float" id="A4.T7.98.98.2.m1.1.1.cmml" xref="A4.T7.98.98.2.m1.1.1">81.04</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.98.98.2.m1.1c">81.04</annotation></semantics></math></td>
<td id="A4.T7.99.99.3" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.99.99.3.m1.1" class="ltx_Math" alttext="52.92" display="inline"><semantics id="A4.T7.99.99.3.m1.1a"><mn id="A4.T7.99.99.3.m1.1.1" xref="A4.T7.99.99.3.m1.1.1.cmml">52.92</mn><annotation-xml encoding="MathML-Content" id="A4.T7.99.99.3.m1.1b"><cn type="float" id="A4.T7.99.99.3.m1.1.1.cmml" xref="A4.T7.99.99.3.m1.1.1">52.92</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.99.99.3.m1.1c">52.92</annotation></semantics></math></td>
<td id="A4.T7.100.100.4" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.100.100.4.m1.1" class="ltx_Math" alttext="361.77" display="inline"><semantics id="A4.T7.100.100.4.m1.1a"><mn id="A4.T7.100.100.4.m1.1.1" xref="A4.T7.100.100.4.m1.1.1.cmml">361.77</mn><annotation-xml encoding="MathML-Content" id="A4.T7.100.100.4.m1.1b"><cn type="float" id="A4.T7.100.100.4.m1.1.1.cmml" xref="A4.T7.100.100.4.m1.1.1">361.77</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.100.100.4.m1.1c">361.77</annotation></semantics></math></td>
<td id="A4.T7.101.101.5" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.101.101.5.m1.1" class="ltx_Math" alttext="4.68" display="inline"><semantics id="A4.T7.101.101.5.m1.1a"><mn id="A4.T7.101.101.5.m1.1.1" xref="A4.T7.101.101.5.m1.1.1.cmml">4.68</mn><annotation-xml encoding="MathML-Content" id="A4.T7.101.101.5.m1.1b"><cn type="float" id="A4.T7.101.101.5.m1.1.1.cmml" xref="A4.T7.101.101.5.m1.1.1">4.68</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.101.101.5.m1.1c">4.68</annotation></semantics></math></td>
<td id="A4.T7.102.102.6" class="ltx_td ltx_align_center ltx_border_t"><math id="A4.T7.102.102.6.m1.1" class="ltx_Math" alttext="0.934" display="inline"><semantics id="A4.T7.102.102.6.m1.1a"><mn id="A4.T7.102.102.6.m1.1.1" xref="A4.T7.102.102.6.m1.1.1.cmml">0.934</mn><annotation-xml encoding="MathML-Content" id="A4.T7.102.102.6.m1.1b"><cn type="float" id="A4.T7.102.102.6.m1.1.1.cmml" xref="A4.T7.102.102.6.m1.1.1">0.934</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.102.102.6.m1.1c">0.934</annotation></semantics></math></td>
</tr>
<tr id="A4.T7.108.108" class="ltx_tr">
<td id="A4.T7.108.108.7" class="ltx_td ltx_align_left ltx_border_bb">Human labellers group B</td>
<td id="A4.T7.103.103.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="A4.T7.103.103.1.m1.1" class="ltx_Math" alttext="91.2" display="inline"><semantics id="A4.T7.103.103.1.m1.1a"><mn id="A4.T7.103.103.1.m1.1.1" xref="A4.T7.103.103.1.m1.1.1.cmml">91.2</mn><annotation-xml encoding="MathML-Content" id="A4.T7.103.103.1.m1.1b"><cn type="float" id="A4.T7.103.103.1.m1.1.1.cmml" xref="A4.T7.103.103.1.m1.1.1">91.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.103.103.1.m1.1c">91.2</annotation></semantics></math></td>
<td id="A4.T7.104.104.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="A4.T7.104.104.2.m1.1" class="ltx_Math" alttext="61.72" display="inline"><semantics id="A4.T7.104.104.2.m1.1a"><mn id="A4.T7.104.104.2.m1.1.1" xref="A4.T7.104.104.2.m1.1.1.cmml">61.72</mn><annotation-xml encoding="MathML-Content" id="A4.T7.104.104.2.m1.1b"><cn type="float" id="A4.T7.104.104.2.m1.1.1.cmml" xref="A4.T7.104.104.2.m1.1.1">61.72</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.104.104.2.m1.1c">61.72</annotation></semantics></math></td>
<td id="A4.T7.105.105.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="A4.T7.105.105.3.m1.1" class="ltx_Math" alttext="42.57" display="inline"><semantics id="A4.T7.105.105.3.m1.1a"><mn id="A4.T7.105.105.3.m1.1.1" xref="A4.T7.105.105.3.m1.1.1.cmml">42.57</mn><annotation-xml encoding="MathML-Content" id="A4.T7.105.105.3.m1.1b"><cn type="float" id="A4.T7.105.105.3.m1.1.1.cmml" xref="A4.T7.105.105.3.m1.1.1">42.57</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.105.105.3.m1.1c">42.57</annotation></semantics></math></td>
<td id="A4.T7.106.106.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="A4.T7.106.106.4.m1.1" class="ltx_Math" alttext="267.87" display="inline"><semantics id="A4.T7.106.106.4.m1.1a"><mn id="A4.T7.106.106.4.m1.1.1" xref="A4.T7.106.106.4.m1.1.1.cmml">267.87</mn><annotation-xml encoding="MathML-Content" id="A4.T7.106.106.4.m1.1b"><cn type="float" id="A4.T7.106.106.4.m1.1.1.cmml" xref="A4.T7.106.106.4.m1.1.1">267.87</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.106.106.4.m1.1c">267.87</annotation></semantics></math></td>
<td id="A4.T7.107.107.5" class="ltx_td ltx_align_center ltx_border_bb"><math id="A4.T7.107.107.5.m1.1" class="ltx_Math" alttext="4.3" display="inline"><semantics id="A4.T7.107.107.5.m1.1a"><mn id="A4.T7.107.107.5.m1.1.1" xref="A4.T7.107.107.5.m1.1.1.cmml">4.3</mn><annotation-xml encoding="MathML-Content" id="A4.T7.107.107.5.m1.1b"><cn type="float" id="A4.T7.107.107.5.m1.1.1.cmml" xref="A4.T7.107.107.5.m1.1.1">4.3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.107.107.5.m1.1c">4.3</annotation></semantics></math></td>
<td id="A4.T7.108.108.6" class="ltx_td ltx_align_center ltx_border_bb"><math id="A4.T7.108.108.6.m1.1" class="ltx_Math" alttext="0.894" display="inline"><semantics id="A4.T7.108.108.6.m1.1a"><mn id="A4.T7.108.108.6.m1.1.1" xref="A4.T7.108.108.6.m1.1.1.cmml">0.894</mn><annotation-xml encoding="MathML-Content" id="A4.T7.108.108.6.m1.1b"><cn type="float" id="A4.T7.108.108.6.m1.1.1.cmml" xref="A4.T7.108.108.6.m1.1.1">0.894</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.108.108.6.m1.1c">0.894</annotation></semantics></math></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A4.T7.113.2.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="A4.T7.110.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Correlation study metrics.<span id="A4.T7.110.1.1" class="ltx_text ltx_font_medium"> Metrics from different models on our evaluation dataset used in the correlation study in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Evaluation Metric ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For reference, we also present metrics for answers provided by human labellers. “Human” is the average of inference output scores in range <math id="A4.T7.110.1.1.m1.2" class="ltx_Math" alttext="\left[0,1\right]" display="inline"><semantics id="A4.T7.110.1.1.m1.2b"><mrow id="A4.T7.110.1.1.m1.2.3.2" xref="A4.T7.110.1.1.m1.2.3.1.cmml"><mo id="A4.T7.110.1.1.m1.2.3.2.1" xref="A4.T7.110.1.1.m1.2.3.1.cmml">[</mo><mn id="A4.T7.110.1.1.m1.1.1" xref="A4.T7.110.1.1.m1.1.1.cmml">0</mn><mo id="A4.T7.110.1.1.m1.2.3.2.2" xref="A4.T7.110.1.1.m1.2.3.1.cmml">,</mo><mn id="A4.T7.110.1.1.m1.2.2" xref="A4.T7.110.1.1.m1.2.2.cmml">1</mn><mo id="A4.T7.110.1.1.m1.2.3.2.3" xref="A4.T7.110.1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.T7.110.1.1.m1.2c"><interval closure="closed" id="A4.T7.110.1.1.m1.2.3.1.cmml" xref="A4.T7.110.1.1.m1.2.3.2"><cn type="integer" id="A4.T7.110.1.1.m1.1.1.cmml" xref="A4.T7.110.1.1.m1.1.1">0</cn><cn type="integer" id="A4.T7.110.1.1.m1.2.2.cmml" xref="A4.T7.110.1.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.110.1.1.m1.2d">\left[0,1\right]</annotation></semantics></math> where 0 is worst and 1 is best, as described in section <a href="#S3.SS2" title="3.2 Evaluation Metric ‣ 3 LingoQA Benchmark ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</span></span></figcaption>
</figure>
<figure id="A4.F8" class="ltx_figure"><img src="/html/2312.14115/assets/img/correlation_trends.png" id="A4.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="343" height="584" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A4.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Correlation trends.<span id="A4.F8.4.2.1" class="ltx_text ltx_font_medium"> Correlation trends of the average grade of models compared to the average human-grades, for different metrics.</span></span></figcaption>
</figure>
<figure id="A4.F9" class="ltx_figure"><img src="/html/2312.14115/assets/img/correlation_coefficients.png" id="A4.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="343" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A4.F9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Correlation coefficients<span id="A4.F9.4.2.1" class="ltx_text ltx_font_medium">. Correlation coefficients of the average grade of different models vs. the average human-grades, for different metrics.</span></span></figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Training Parameters</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">In this sections we present further details on the training parameters used for the LingoQA Baseline. The training process consists of a pre-training stage, and a fine-tuning stage. Table <a href="#A5.T8" title="Table 8 ‣ Appendix E Training Parameters ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the parameters for pre-training and fine-tuning respectively. The datasets are sampled with equal weight for both pre-training and fine-tuning. The overall training time was  20h for pre-training and  5h for fine-tuning on an NVIDIA A100 8GPU 80GB machine.</p>
</div>
<figure id="A5.T8" class="ltx_table">
<table id="A5.T8.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A5.T8.2.3" class="ltx_tr">
<td id="A5.T8.2.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Parameter</td>
<td id="A5.T8.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Pre-training</td>
<td id="A5.T8.2.3.3" class="ltx_td ltx_align_center ltx_border_tt">Fine-tuning</td>
</tr>
<tr id="A5.T8.2.4" class="ltx_tr">
<td id="A5.T8.2.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Precision</td>
<td id="A5.T8.2.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">bf16</td>
<td id="A5.T8.2.4.3" class="ltx_td ltx_align_center ltx_border_t">bf16</td>
</tr>
<tr id="A5.T8.2.5" class="ltx_tr">
<td id="A5.T8.2.5.1" class="ltx_td ltx_align_center ltx_border_r">Warm-up steps</td>
<td id="A5.T8.2.5.2" class="ltx_td ltx_align_center ltx_border_r">1000</td>
<td id="A5.T8.2.5.3" class="ltx_td ltx_align_center">1000</td>
</tr>
<tr id="A5.T8.2.6" class="ltx_tr">
<td id="A5.T8.2.6.1" class="ltx_td ltx_align_center ltx_border_r">Maximum steps</td>
<td id="A5.T8.2.6.2" class="ltx_td ltx_align_center ltx_border_r">100000</td>
<td id="A5.T8.2.6.3" class="ltx_td ltx_align_center">10000</td>
</tr>
<tr id="A5.T8.2.7" class="ltx_tr">
<td id="A5.T8.2.7.1" class="ltx_td ltx_align_center ltx_border_r">Batch size</td>
<td id="A5.T8.2.7.2" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="A5.T8.2.7.3" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="A5.T8.2.8" class="ltx_tr">
<td id="A5.T8.2.8.1" class="ltx_td ltx_align_center ltx_border_r">Gradient acc. steps</td>
<td id="A5.T8.2.8.2" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="A5.T8.2.8.3" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A5.T8.2.2" class="ltx_tr">
<td id="A5.T8.2.2.3" class="ltx_td ltx_align_center ltx_border_r">Learning rate</td>
<td id="A5.T8.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><math id="A5.T8.1.1.1.m1.1" class="ltx_Math" alttext="5*10^{-5}" display="inline"><semantics id="A5.T8.1.1.1.m1.1a"><mrow id="A5.T8.1.1.1.m1.1.1" xref="A5.T8.1.1.1.m1.1.1.cmml"><mn id="A5.T8.1.1.1.m1.1.1.2" xref="A5.T8.1.1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="A5.T8.1.1.1.m1.1.1.1" xref="A5.T8.1.1.1.m1.1.1.1.cmml">∗</mo><msup id="A5.T8.1.1.1.m1.1.1.3" xref="A5.T8.1.1.1.m1.1.1.3.cmml"><mn id="A5.T8.1.1.1.m1.1.1.3.2" xref="A5.T8.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A5.T8.1.1.1.m1.1.1.3.3" xref="A5.T8.1.1.1.m1.1.1.3.3.cmml"><mo id="A5.T8.1.1.1.m1.1.1.3.3a" xref="A5.T8.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="A5.T8.1.1.1.m1.1.1.3.3.2" xref="A5.T8.1.1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A5.T8.1.1.1.m1.1b"><apply id="A5.T8.1.1.1.m1.1.1.cmml" xref="A5.T8.1.1.1.m1.1.1"><times id="A5.T8.1.1.1.m1.1.1.1.cmml" xref="A5.T8.1.1.1.m1.1.1.1"></times><cn type="integer" id="A5.T8.1.1.1.m1.1.1.2.cmml" xref="A5.T8.1.1.1.m1.1.1.2">5</cn><apply id="A5.T8.1.1.1.m1.1.1.3.cmml" xref="A5.T8.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A5.T8.1.1.1.m1.1.1.3.1.cmml" xref="A5.T8.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="A5.T8.1.1.1.m1.1.1.3.2.cmml" xref="A5.T8.1.1.1.m1.1.1.3.2">10</cn><apply id="A5.T8.1.1.1.m1.1.1.3.3.cmml" xref="A5.T8.1.1.1.m1.1.1.3.3"><minus id="A5.T8.1.1.1.m1.1.1.3.3.1.cmml" xref="A5.T8.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="A5.T8.1.1.1.m1.1.1.3.3.2.cmml" xref="A5.T8.1.1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.1.1.1.m1.1c">5*10^{-5}</annotation></semantics></math></td>
<td id="A5.T8.2.2.2" class="ltx_td ltx_align_center"><math id="A5.T8.2.2.2.m1.1" class="ltx_Math" alttext="5*10^{-5}" display="inline"><semantics id="A5.T8.2.2.2.m1.1a"><mrow id="A5.T8.2.2.2.m1.1.1" xref="A5.T8.2.2.2.m1.1.1.cmml"><mn id="A5.T8.2.2.2.m1.1.1.2" xref="A5.T8.2.2.2.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="A5.T8.2.2.2.m1.1.1.1" xref="A5.T8.2.2.2.m1.1.1.1.cmml">∗</mo><msup id="A5.T8.2.2.2.m1.1.1.3" xref="A5.T8.2.2.2.m1.1.1.3.cmml"><mn id="A5.T8.2.2.2.m1.1.1.3.2" xref="A5.T8.2.2.2.m1.1.1.3.2.cmml">10</mn><mrow id="A5.T8.2.2.2.m1.1.1.3.3" xref="A5.T8.2.2.2.m1.1.1.3.3.cmml"><mo id="A5.T8.2.2.2.m1.1.1.3.3a" xref="A5.T8.2.2.2.m1.1.1.3.3.cmml">−</mo><mn id="A5.T8.2.2.2.m1.1.1.3.3.2" xref="A5.T8.2.2.2.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A5.T8.2.2.2.m1.1b"><apply id="A5.T8.2.2.2.m1.1.1.cmml" xref="A5.T8.2.2.2.m1.1.1"><times id="A5.T8.2.2.2.m1.1.1.1.cmml" xref="A5.T8.2.2.2.m1.1.1.1"></times><cn type="integer" id="A5.T8.2.2.2.m1.1.1.2.cmml" xref="A5.T8.2.2.2.m1.1.1.2">5</cn><apply id="A5.T8.2.2.2.m1.1.1.3.cmml" xref="A5.T8.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="A5.T8.2.2.2.m1.1.1.3.1.cmml" xref="A5.T8.2.2.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="A5.T8.2.2.2.m1.1.1.3.2.cmml" xref="A5.T8.2.2.2.m1.1.1.3.2">10</cn><apply id="A5.T8.2.2.2.m1.1.1.3.3.cmml" xref="A5.T8.2.2.2.m1.1.1.3.3"><minus id="A5.T8.2.2.2.m1.1.1.3.3.1.cmml" xref="A5.T8.2.2.2.m1.1.1.3.3"></minus><cn type="integer" id="A5.T8.2.2.2.m1.1.1.3.3.2.cmml" xref="A5.T8.2.2.2.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.2.2.2.m1.1c">5*10^{-5}</annotation></semantics></math></td>
</tr>
<tr id="A5.T8.2.9" class="ltx_tr">
<td id="A5.T8.2.9.1" class="ltx_td ltx_align_center ltx_border_r">Learning rate scheduler</td>
<td id="A5.T8.2.9.2" class="ltx_td ltx_align_center ltx_border_r">cosine</td>
<td id="A5.T8.2.9.3" class="ltx_td ltx_align_center">cosine</td>
</tr>
<tr id="A5.T8.2.10" class="ltx_tr">
<td id="A5.T8.2.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Weight decay</td>
<td id="A5.T8.2.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.1</td>
<td id="A5.T8.2.10.3" class="ltx_td ltx_align_center ltx_border_bb">0.1</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A5.T8.5.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="A5.T8.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Training parameters.<span id="A5.T8.6.2.1" class="ltx_text ltx_font_medium"> This table shows the training parameters utilised for the pre-training and for the fine-tuning stages respectively.</span></span></figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>LingoQA Baseline Examples</h2>

<figure id="A6.F10" class="ltx_figure"><img src="/html/2312.14115/assets/img/baseline_examples.png" id="A6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="617" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A6.F10.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Examples of model outputs on the LingoQA benchmark.<span id="A6.F10.4.2.1" class="ltx_text ltx_font_medium"> We compare the baseline with a model that has not been fine-tuned on the LingoQA dataset, a model fine-tuned on the action dataset only, and a model fine-tuned on the scenery dataset only. This shows qualitatively how the baseline can handle both action justification as well as descriptive tasks by combining the strengths of both datasets.</span></span></figcaption>
</figure>
<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">We qualitatively showcase the impact of our proposed LingoQA dataset. Figure <a href="#A6.F10" title="Figure 10 ‣ Appendix F LingoQA Baseline Examples ‣ Acknowledgements ‣ 7 Conclusion ‣ Dataset and model limitations. ‣ 6 Discussion ‣ 5.5 Impact of Large Language Model ‣ 5 Empirical Evaluation on LingoQA ‣ Stage 2: Fine-tuning for video QA. ‣ Stage 1: Pre-training for feature alignment. ‣ 4.2 Training Recipe ‣ 4 Model Methodology ‣ LingoQA: Video Question Answering for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> compares three models: a model that is not fine-tuned on any LingoQA datasets, one that is fine-tuned on the <span id="A6.p1.1.1" class="ltx_text ltx_font_italic">action</span> dataset only, one on the <span id="A6.p1.1.2" class="ltx_text ltx_font_italic">scenery</span> dataset only, and the baseline that is trained with both. Two questions are asked, one focused on perception only, and one focused on action justification. The action only model performs well at answering action-related questions, but not perception. The scenery only model performs well at perception tasks, but not action justification. The baseline exhibits good performance on both.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.14114" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.14115" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.14115">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.14115" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.14116" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 12:29:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
