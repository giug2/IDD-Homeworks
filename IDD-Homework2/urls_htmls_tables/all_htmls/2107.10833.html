<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.10833] Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</title><meta property="og:description" content="Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images.
In this work,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.10833">

<!--Generated on Fri Mar  8 14:50:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Real-ESRGAN: Training Real-World Blind Super-Resolution 
<br class="ltx_break">with Pure Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xintao Wang<sup id="id11.9.id1" class="ltx_sup"><span id="id11.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>     Liangbin Xie<sup id="id12.10.id2" class="ltx_sup"><span id="id12.10.id2.1" class="ltx_text ltx_font_italic">∗2,3</span></sup>     Chao Dong<sup id="id13.11.id3" class="ltx_sup"><span id="id13.11.id3.1" class="ltx_text ltx_font_italic">2,4</span></sup>     Ying Shan<sup id="id14.12.id4" class="ltx_sup"><span id="id14.12.id4.1" class="ltx_text ltx_font_italic">1</span></sup> 
<br class="ltx_break">
<sup id="id15.13.id5" class="ltx_sup"><span id="id15.13.id5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">1</span></sup><span id="id8.8.3" class="ltx_text" style="font-size:90%;">Applied Research Center (ARC), Tencent PCG 
<br class="ltx_break">
<sup id="id8.8.3.1" class="ltx_sup"><span id="id8.8.3.1.1" class="ltx_text ltx_font_italic">2</span></sup>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences 
<br class="ltx_break">
<sup id="id8.8.3.2" class="ltx_sup"><span id="id8.8.3.2.1" class="ltx_text ltx_font_italic">3</span></sup>University of Chinese Academy of Sciences     <sup id="id8.8.3.3" class="ltx_sup"><span id="id8.8.3.3.1" class="ltx_text ltx_font_italic">4</span></sup>Shanghai AI Laboratory 
<br class="ltx_break">
<span id="id8.8.3.4" class="ltx_text ltx_font_typewriter">{xintaowang, yingsshan}@tencent.com    {lb.xie, chao.dong}@siat.ac.cn
<br class="ltx_break"><a target="_blank" href="https://github.com/xinntao/Real-ESRGAN" title="" class="ltx_ref ltx_url" style="font-size:111%;">https://github.com/xinntao/Real-ESRGAN</a></span></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>*Liangbin Xie is an intern in Applied Research Center, Tencent PCG</span></span></span>
<p id="id16.id1" class="ltx_p">Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images.
In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data.
Specifically,
a high-order degradation modeling process is introduced to better simulate complex real-world degradations.
We also consider the common ringing and overshoot artifacts in the synthesis process.
In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics.
Extensive comparisons have shown its superior visual performance than prior works on various real datasets.
We also provide efficient implementations to synthesize training pairs on the fly.</p>
</div>
<div id="id10" class="ltx_logical-block">
<div id="id10.p1" class="ltx_para">
<img src="/html/2107.10833/assets/x1.png" id="id9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="218" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.4.2" class="ltx_text" style="font-size:90%;">Comparisons of bicubic-upsampled, ESRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and our Real-ESRGAN results on real-life images. The Real-ESRGAN model trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images. (<span id="S0.F1.4.2.1" class="ltx_text ltx_font_bold">Zoom in for best view</span>)</span></figcaption>
</figure>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Single image super-resolution (SR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> is an active research topic, which aims at reconstructing a high-resolution (HR) image from its low-resolution (LR) counterpart.
Since the pioneering work of SRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, deep convolution neural network (CNN) approaches have brought prosperous developments in the SR field.
However, most approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> assume an <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">ideal bicubic downsampling kernel</span>, which is different from real degradations. This degradation mismatch makes those approaches unpractical in real-world scenarios.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Blind super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, on the contrary, aims to restore low-resolution images suffering from <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">unknown and complex degradations</span>. Existing approaches can be roughly categorized into <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">explicit modeling</span> and <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">implicit modeling</span>, according to the underlying degradation process.
Classical degradation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, which consists of blur, downsampling, noise and JPEG compression (more details in Sec. <a href="#S3.SS1" title="3.1 Classical Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), is widely adopted in explicit modeling methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
However, the real-world degradations are usually too complex to be modeled with a simple combination of multiple degradations. Thus, these methods will easily fail in real-world samples.
Implicit modeling methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> utilize data distribution learning with Generative Adversarial Network (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to obtain the degradation model. Yet, they are limited to the degradations within training datasets, and could not generalize well to out-of-distribution images.
Readers are encouraged to refer to a recent blind SR survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for a more comprehensive taxonomy.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we aim to extend the powerful ESRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> to restore general real-world LR images by synthesizing training pairs with a more practical degradation process.
The real complex degradations usually come from <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">complicate combinations of different degradation processes</span>, such as imaging system of cameras, image editing, and Internet transmission.
For example, when we take a photo with our cellphones, the photos may have several degradations, such as camera blur, sensor noise, sharpening artifacts, and JPEG compression.
We then do some editing and upload to a social media app, which introduces further compression and unpredictable noises.
The above process becomes more complicated when the image is shared several times on the Internet.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This motivates us to extend the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">classical “first-order” degradation model</span> to <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">“high-order” degradation modeling</span> for real-world degradations, <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.4" class="ltx_text"></span>, the degradations are modeled with several repeated degradation processes, each process being the classical degradation model.
Empirically, we adopt a <span id="S1.p4.1.5" class="ltx_text ltx_font_italic">second-order degradation process</span> for a good balance between simplicity and effectiveness.
A recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> also proposes a random shuffling strategy to synthesize more practical degradations. However, it still involves a fixed number of degradation processes, and whether all the shuffled degradations are useful or not is unclear.
Instead, high-order degradation modeling is more flexible and attempts to mimic the real degradation generation process.
We further incorporate <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mi id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S1.p4.1.m1.1.1.1a" xref="S1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S1.p4.1.m1.1.1.4" xref="S1.p4.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S1.p4.1.m1.1.1.1b" xref="S1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S1.p4.1.m1.1.1.5" xref="S1.p4.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><times id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1"></times><ci id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2">𝑠</ci><ci id="S1.p4.1.m1.1.1.3.cmml" xref="S1.p4.1.m1.1.1.3">𝑖</ci><ci id="S1.p4.1.m1.1.1.4.cmml" xref="S1.p4.1.m1.1.1.4">𝑛</ci><ci id="S1.p4.1.m1.1.1.5.cmml" xref="S1.p4.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">sinc</annotation></semantics></math> filters in the synthesis process to simulate the <span id="S1.p4.1.6" class="ltx_text ltx_font_bold">common ringing and overshoot artifacts</span>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As the degradation space is much larger than ESRGAN, the training also becomes challenging.
Specifically, 1) the discriminator requires a more powerful capability to discriminate realness from complex training outputs, while the gradient feedback from the discriminator needs to be more accurate for local detail enhancement.
Therefore, we improve the VGG-style discriminator in ESRGAN to an <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">U-Net design</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
2) The U-Net structure and complicate degradations also increase the training instability. Thus, we employ the <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">spectral normalization (SN) regularization</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to stabilize the training dynamics.
Equipped with the dedicated improvements, we are able to easily train our Real-ESRGAN and achieve a good balance of local detail enhancement and artifact suppression.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To summarize, in this work,
<span id="S1.p6.1.1" class="ltx_text ltx_font_bold">1)</span> we propose a high-order degradation process to model practical degradations, and utilize <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mi id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S1.p6.1.m1.1.1.3" xref="S1.p6.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S1.p6.1.m1.1.1.1a" xref="S1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S1.p6.1.m1.1.1.4" xref="S1.p6.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S1.p6.1.m1.1.1.1b" xref="S1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S1.p6.1.m1.1.1.5" xref="S1.p6.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><times id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1"></times><ci id="S1.p6.1.m1.1.1.2.cmml" xref="S1.p6.1.m1.1.1.2">𝑠</ci><ci id="S1.p6.1.m1.1.1.3.cmml" xref="S1.p6.1.m1.1.1.3">𝑖</ci><ci id="S1.p6.1.m1.1.1.4.cmml" xref="S1.p6.1.m1.1.1.4">𝑛</ci><ci id="S1.p6.1.m1.1.1.5.cmml" xref="S1.p6.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">sinc</annotation></semantics></math> filters to model common ringing and overshoot artifacts.
<span id="S1.p6.1.2" class="ltx_text ltx_font_bold">2)</span> We employ several essential modifications (<em id="S1.p6.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p6.1.4" class="ltx_text"></span>, U-Net discriminator with spectral normalization) to increase discriminator capability and stabilize the training dynamics.
<span id="S1.p6.1.5" class="ltx_text ltx_font_bold">3)</span> Real-ESRGAN trained with pure synthetic data is able to restore most real-world images and achieve better visual performance than previous works, making it more practical in real-world applications.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">The image super-resolution</span> field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> has witnessed a variety of developments since SRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
To achieve visually-pleasing results, generative adversarial network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> is usually employed as loss supervisions to push the solutions closer to the natural manifold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
Most methods assume a bicubic downsampling kernel and usually fail in real images.
Recent works also incorporate reinforcement learning or GAN prior to image restoration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">There have been several excellent explorations in blind SR.
The first category involves explicit degradation representations and typically consists of two components: degradation prediction and conditional restoration.
The above two components are performed either separately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> or jointly (iteratively) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
These approaches rely on predefined degradation representations (<em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.2" class="ltx_text"></span>, degradation types and levels), and usually consider simple synthetic degradations.
Moreover, inaccurate degradation estimations will inevitably result in artifacts.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Another category is to obtain/generate training pairs as close to real data as possible, and then train a unified network to address blind SR.
The training pairs are usually 1) captured with specific cameras followed by tedious alignments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>; 2) or directly learned from unpaired data with cycle consistency loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>; 3) or synthesized with estimated blur kernels and extracted noise patches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
However, 1) the captured data is only constrained to degradations associated with specific cameras, and thus could not well generalize to other real images; 2) learning fine-grained degradations with unpaired data is challenging, and the results are usually unsatisfactory.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Degradation models.</span>
Classical degradation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is widely adopted in blind SR methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
Yet, real-world degradations are usually too complex to be explicitly modeled.
Thus, implicit modeling attempts to learn a degradation generation process within networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
In this work, we propose a flexible high-order degradation model to synthesize more practical degradations.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Classical Degradation Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">Blind SR aims to restore high-resolution images from low-resolution ones with unknown and complex degradations.
The classical degradation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is usually adopted to synthesize the low-resolution input.
Generally, the ground-truth image <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{y}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\bm{y}</annotation></semantics></math> is first convolved with blur kernel <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\bm{k}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝒌</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝒌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\bm{k}</annotation></semantics></math>. Then, a downsampling operation with scale factor <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">r</annotation></semantics></math> is performed. The low-resolution <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\bm{x}</annotation></semantics></math> is obtained by adding noise <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\bm{n}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">𝒏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝒏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\bm{n}</annotation></semantics></math>. Finally, JPEG compression is also adopted, as it is widely-used in real-world images.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\vspace{-0.2cm}\bm{x}=\mathcal{D}(\bm{y})=[(\bm{y}\circledast\bm{k})\downarrow_{r}+\bm{n}]_{\mathtt{JPEG}}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">𝒙</mi><mo id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.5" xref="S3.E1.m1.2.2.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.5.2" xref="S3.E1.m1.2.2.1.1.5.2.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.5.1" xref="S3.E1.m1.2.2.1.1.5.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.5.3.2" xref="S3.E1.m1.2.2.1.1.5.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.5.3.2.1" xref="S3.E1.m1.2.2.1.1.5.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝒚</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.5.3.2.2" xref="S3.E1.m1.2.2.1.1.5.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.6" xref="S3.E1.m1.2.2.1.1.6.cmml">=</mo><msub id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">𝒚</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">⊛</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">𝒌</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.2.cmml">↓</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.3.cmml">r</mi></msub><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.3a" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">+</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.cmml">𝒏</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.2.1.cmml">]</mo></mrow><mi id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml">𝙹𝙿𝙴𝙶</mi></msub></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><and id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1"></and><apply id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4"></eq><ci id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">𝒙</ci><apply id="S3.E1.m1.2.2.1.1.5.cmml" xref="S3.E1.m1.2.2.1.1.5"><times id="S3.E1.m1.2.2.1.1.5.1.cmml" xref="S3.E1.m1.2.2.1.1.5.1"></times><ci id="S3.E1.m1.2.2.1.1.5.2.cmml" xref="S3.E1.m1.2.2.1.1.5.2">𝒟</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝒚</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.6.cmml" xref="S3.E1.m1.2.2.1.1.6"></eq><share href="#S3.E1.m1.2.2.1.1.5.cmml" id="S3.E1.m1.2.2.1.1d.cmml" xref="S3.E1.m1.2.2.1"></share><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.2">↓</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.3">𝑟</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1"><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1">⊛</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝒚</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3">𝒌</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3"><plus id="S3.E1.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3"></plus><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.2">𝒏</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3">𝙹𝙿𝙴𝙶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\vspace{-0.2cm}\bm{x}=\mathcal{D}(\bm{y})=[(\bm{y}\circledast\bm{k})\downarrow_{r}+\bm{n}]_{\mathtt{JPEG}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.9" class="ltx_p">where <math id="S3.SS1.p1.6.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.p1.6.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><ci id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">\mathcal{D}</annotation></semantics></math> denotes the degradation process. In the following, we briefly revisit these commonly-used degradations. The detailed settings are specified in Sec. <a href="#S4.SS1" title="4.1 Datasets and Implementation ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. More descriptions and examples are in Appendix. <a href="#A1" title="Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.

<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS1.p1.9.1" class="ltx_text ltx_font_bold">Blur.</span>
We typically model blur degradation as a convolution with a linear blur filter (kernel).
Isotropic and anisotropic Gaussian filters are common choices.
For a Gaussian blur kernel <math id="S3.SS1.p1.7.m2.1" class="ltx_Math" alttext="\bm{k}" display="inline"><semantics id="S3.SS1.p1.7.m2.1a"><mi id="S3.SS1.p1.7.m2.1.1" xref="S3.SS1.p1.7.m2.1.1.cmml">𝒌</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.1b"><ci id="S3.SS1.p1.7.m2.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1">𝒌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.1c">\bm{k}</annotation></semantics></math> with a kernel size of <math id="S3.SS1.p1.8.m3.1" class="ltx_Math" alttext="2t+1" display="inline"><semantics id="S3.SS1.p1.8.m3.1a"><mrow id="S3.SS1.p1.8.m3.1.1" xref="S3.SS1.p1.8.m3.1.1.cmml"><mrow id="S3.SS1.p1.8.m3.1.1.2" xref="S3.SS1.p1.8.m3.1.1.2.cmml"><mn id="S3.SS1.p1.8.m3.1.1.2.2" xref="S3.SS1.p1.8.m3.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m3.1.1.2.1" xref="S3.SS1.p1.8.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.8.m3.1.1.2.3" xref="S3.SS1.p1.8.m3.1.1.2.3.cmml">t</mi></mrow><mo id="S3.SS1.p1.8.m3.1.1.1" xref="S3.SS1.p1.8.m3.1.1.1.cmml">+</mo><mn id="S3.SS1.p1.8.m3.1.1.3" xref="S3.SS1.p1.8.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m3.1b"><apply id="S3.SS1.p1.8.m3.1.1.cmml" xref="S3.SS1.p1.8.m3.1.1"><plus id="S3.SS1.p1.8.m3.1.1.1.cmml" xref="S3.SS1.p1.8.m3.1.1.1"></plus><apply id="S3.SS1.p1.8.m3.1.1.2.cmml" xref="S3.SS1.p1.8.m3.1.1.2"><times id="S3.SS1.p1.8.m3.1.1.2.1.cmml" xref="S3.SS1.p1.8.m3.1.1.2.1"></times><cn type="integer" id="S3.SS1.p1.8.m3.1.1.2.2.cmml" xref="S3.SS1.p1.8.m3.1.1.2.2">2</cn><ci id="S3.SS1.p1.8.m3.1.1.2.3.cmml" xref="S3.SS1.p1.8.m3.1.1.2.3">𝑡</ci></apply><cn type="integer" id="S3.SS1.p1.8.m3.1.1.3.cmml" xref="S3.SS1.p1.8.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m3.1c">2t+1</annotation></semantics></math>, its <math id="S3.SS1.p1.9.m4.4" class="ltx_Math" alttext="(i,j)\in[-t,t]" display="inline"><semantics id="S3.SS1.p1.9.m4.4a"><mrow id="S3.SS1.p1.9.m4.4.4" xref="S3.SS1.p1.9.m4.4.4.cmml"><mrow id="S3.SS1.p1.9.m4.4.4.3.2" xref="S3.SS1.p1.9.m4.4.4.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.9.m4.4.4.3.2.1" xref="S3.SS1.p1.9.m4.4.4.3.1.cmml">(</mo><mi id="S3.SS1.p1.9.m4.1.1" xref="S3.SS1.p1.9.m4.1.1.cmml">i</mi><mo id="S3.SS1.p1.9.m4.4.4.3.2.2" xref="S3.SS1.p1.9.m4.4.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.9.m4.2.2" xref="S3.SS1.p1.9.m4.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS1.p1.9.m4.4.4.3.2.3" xref="S3.SS1.p1.9.m4.4.4.3.1.cmml">)</mo></mrow><mo id="S3.SS1.p1.9.m4.4.4.2" xref="S3.SS1.p1.9.m4.4.4.2.cmml">∈</mo><mrow id="S3.SS1.p1.9.m4.4.4.1.1" xref="S3.SS1.p1.9.m4.4.4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.9.m4.4.4.1.1.2" xref="S3.SS1.p1.9.m4.4.4.1.2.cmml">[</mo><mrow id="S3.SS1.p1.9.m4.4.4.1.1.1" xref="S3.SS1.p1.9.m4.4.4.1.1.1.cmml"><mo id="S3.SS1.p1.9.m4.4.4.1.1.1a" xref="S3.SS1.p1.9.m4.4.4.1.1.1.cmml">−</mo><mi id="S3.SS1.p1.9.m4.4.4.1.1.1.2" xref="S3.SS1.p1.9.m4.4.4.1.1.1.2.cmml">t</mi></mrow><mo id="S3.SS1.p1.9.m4.4.4.1.1.3" xref="S3.SS1.p1.9.m4.4.4.1.2.cmml">,</mo><mi id="S3.SS1.p1.9.m4.3.3" xref="S3.SS1.p1.9.m4.3.3.cmml">t</mi><mo stretchy="false" id="S3.SS1.p1.9.m4.4.4.1.1.4" xref="S3.SS1.p1.9.m4.4.4.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m4.4b"><apply id="S3.SS1.p1.9.m4.4.4.cmml" xref="S3.SS1.p1.9.m4.4.4"><in id="S3.SS1.p1.9.m4.4.4.2.cmml" xref="S3.SS1.p1.9.m4.4.4.2"></in><interval closure="open" id="S3.SS1.p1.9.m4.4.4.3.1.cmml" xref="S3.SS1.p1.9.m4.4.4.3.2"><ci id="S3.SS1.p1.9.m4.1.1.cmml" xref="S3.SS1.p1.9.m4.1.1">𝑖</ci><ci id="S3.SS1.p1.9.m4.2.2.cmml" xref="S3.SS1.p1.9.m4.2.2">𝑗</ci></interval><interval closure="closed" id="S3.SS1.p1.9.m4.4.4.1.2.cmml" xref="S3.SS1.p1.9.m4.4.4.1.1"><apply id="S3.SS1.p1.9.m4.4.4.1.1.1.cmml" xref="S3.SS1.p1.9.m4.4.4.1.1.1"><minus id="S3.SS1.p1.9.m4.4.4.1.1.1.1.cmml" xref="S3.SS1.p1.9.m4.4.4.1.1.1"></minus><ci id="S3.SS1.p1.9.m4.4.4.1.1.1.2.cmml" xref="S3.SS1.p1.9.m4.4.4.1.1.1.2">𝑡</ci></apply><ci id="S3.SS1.p1.9.m4.3.3.cmml" xref="S3.SS1.p1.9.m4.3.3">𝑡</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m4.4c">(i,j)\in[-t,t]</annotation></semantics></math> element is sampled from a Gaussian distribution, formally:</p>
<table id="A3.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\displaystyle\vspace{-0.4cm}\bm{k}(i,j)" display="inline"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.3" xref="S3.E2.m1.2.3.cmml"><mi id="S3.E2.m1.2.3.2" xref="S3.E2.m1.2.3.2.cmml">𝒌</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.1" xref="S3.E2.m1.2.3.1.cmml">​</mo><mrow id="S3.E2.m1.2.3.3.2" xref="S3.E2.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.3.3.2.1" xref="S3.E2.m1.2.3.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">i</mi><mo id="S3.E2.m1.2.3.3.2.2" xref="S3.E2.m1.2.3.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.E2.m1.2.3.3.2.3" xref="S3.E2.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.3.cmml" xref="S3.E2.m1.2.3"><times id="S3.E2.m1.2.3.1.cmml" xref="S3.E2.m1.2.3.1"></times><ci id="S3.E2.m1.2.3.2.cmml" xref="S3.E2.m1.2.3.2">𝒌</ci><interval closure="open" id="S3.E2.m1.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑖</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑗</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle\vspace{-0.4cm}\bm{k}(i,j)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.4" class="ltx_Math" alttext="\displaystyle=\frac{1}{N}\exp(-\frac{1}{2}\bm{C}^{T}\bm{\Sigma}^{-1}\bm{C}),\quad\bm{C}=[i,j]^{T}," display="inline"><semantics id="S3.E2.m2.4a"><mrow id="S3.E2.m2.4.4.1"><mrow id="S3.E2.m2.4.4.1.1.2" xref="S3.E2.m2.4.4.1.1.3.cmml"><mrow id="S3.E2.m2.4.4.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.cmml"><mi id="S3.E2.m2.4.4.1.1.1.1.3" xref="S3.E2.m2.4.4.1.1.1.1.3.cmml"></mi><mo id="S3.E2.m2.4.4.1.1.1.1.2" xref="S3.E2.m2.4.4.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.4.4.1.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m2.4.4.1.1.1.1.1.3" xref="S3.E2.m2.4.4.1.1.1.1.1.3.cmml"><mfrac id="S3.E2.m2.4.4.1.1.1.1.1.3a" xref="S3.E2.m2.4.4.1.1.1.1.1.3.cmml"><mn id="S3.E2.m2.4.4.1.1.1.1.1.3.2" xref="S3.E2.m2.4.4.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m2.4.4.1.1.1.1.1.3.3" xref="S3.E2.m2.4.4.1.1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0.167em" rspace="0em" id="S3.E2.m2.4.4.1.1.1.1.1.2" xref="S3.E2.m2.4.4.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.4.4.1.1.1.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">exp</mi><mo id="S3.E2.m2.4.4.1.1.1.1.1.1.1a" xref="S3.E2.m2.4.4.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1a" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mstyle displaystyle="true" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml"><mfrac id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2a" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml"><mn id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml">1</mn><mn id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml">2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝑪</mi><mi id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1a" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.2.cmml">𝚺</mi><mrow id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.cmml"><mo id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3a" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.cmml">−</mo><mn id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.2" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1b" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.5" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.5.cmml">𝑪</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.4.4.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em" id="S3.E2.m2.4.4.1.1.2.3" xref="S3.E2.m2.4.4.1.1.3a.cmml">,</mo><mrow id="S3.E2.m2.4.4.1.1.2.2" xref="S3.E2.m2.4.4.1.1.2.2.cmml"><mi id="S3.E2.m2.4.4.1.1.2.2.2" xref="S3.E2.m2.4.4.1.1.2.2.2.cmml">𝑪</mi><mo id="S3.E2.m2.4.4.1.1.2.2.1" xref="S3.E2.m2.4.4.1.1.2.2.1.cmml">=</mo><msup id="S3.E2.m2.4.4.1.1.2.2.3" xref="S3.E2.m2.4.4.1.1.2.2.3.cmml"><mrow id="S3.E2.m2.4.4.1.1.2.2.3.2.2" xref="S3.E2.m2.4.4.1.1.2.2.3.2.1.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.1.1.2.2.3.2.2.1" xref="S3.E2.m2.4.4.1.1.2.2.3.2.1.cmml">[</mo><mi id="S3.E2.m2.2.2" xref="S3.E2.m2.2.2.cmml">i</mi><mo id="S3.E2.m2.4.4.1.1.2.2.3.2.2.2" xref="S3.E2.m2.4.4.1.1.2.2.3.2.1.cmml">,</mo><mi id="S3.E2.m2.3.3" xref="S3.E2.m2.3.3.cmml">j</mi><mo stretchy="false" id="S3.E2.m2.4.4.1.1.2.2.3.2.2.3" xref="S3.E2.m2.4.4.1.1.2.2.3.2.1.cmml">]</mo></mrow><mi id="S3.E2.m2.4.4.1.1.2.2.3.3" xref="S3.E2.m2.4.4.1.1.2.2.3.3.cmml">T</mi></msup></mrow></mrow><mo id="S3.E2.m2.4.4.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.4b"><apply id="S3.E2.m2.4.4.1.1.3.cmml" xref="S3.E2.m2.4.4.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m2.4.4.1.1.3a.cmml" xref="S3.E2.m2.4.4.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E2.m2.4.4.1.1.1.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1"><eq id="S3.E2.m2.4.4.1.1.1.1.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.4.4.1.1.1.1.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.3">absent</csymbol><apply id="S3.E2.m2.4.4.1.1.1.1.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1"><times id="S3.E2.m2.4.4.1.1.1.1.1.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.2"></times><apply id="S3.E2.m2.4.4.1.1.1.1.1.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.3"><divide id="S3.E2.m2.4.4.1.1.1.1.1.3.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.3"></divide><cn type="integer" id="S3.E2.m2.4.4.1.1.1.1.1.3.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m2.4.4.1.1.1.1.1.3.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1"><exp id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"></exp><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2"><times id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2"><divide id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2"></divide><cn type="integer" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.2">1</cn><cn type="integer" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.2.3">2</cn></apply><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.2">𝑪</ci><ci id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.3.3">𝑇</ci></apply><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4">superscript</csymbol><ci id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.2">𝚺</ci><apply id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3"><minus id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3"></minus><cn type="integer" id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.4.3.2">1</cn></apply></apply><ci id="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.5.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1.1.1.1.1.2.5">𝑪</ci></apply></apply></apply></apply></apply><apply id="S3.E2.m2.4.4.1.1.2.2.cmml" xref="S3.E2.m2.4.4.1.1.2.2"><eq id="S3.E2.m2.4.4.1.1.2.2.1.cmml" xref="S3.E2.m2.4.4.1.1.2.2.1"></eq><ci id="S3.E2.m2.4.4.1.1.2.2.2.cmml" xref="S3.E2.m2.4.4.1.1.2.2.2">𝑪</ci><apply id="S3.E2.m2.4.4.1.1.2.2.3.cmml" xref="S3.E2.m2.4.4.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m2.4.4.1.1.2.2.3.1.cmml" xref="S3.E2.m2.4.4.1.1.2.2.3">superscript</csymbol><interval closure="closed" id="S3.E2.m2.4.4.1.1.2.2.3.2.1.cmml" xref="S3.E2.m2.4.4.1.1.2.2.3.2.2"><ci id="S3.E2.m2.2.2.cmml" xref="S3.E2.m2.2.2">𝑖</ci><ci id="S3.E2.m2.3.3.cmml" xref="S3.E2.m2.3.3">𝑗</ci></interval><ci id="S3.E2.m2.4.4.1.1.2.2.3.3.cmml" xref="S3.E2.m2.4.4.1.1.2.2.3.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.4c">\displaystyle=\frac{1}{N}\exp(-\frac{1}{2}\bm{C}^{T}\bm{\Sigma}^{-1}\bm{C}),\quad\bm{C}=[i,j]^{T},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.12" class="ltx_p">where <math id="S3.SS1.p1.10.m1.1" class="ltx_Math" alttext="\bm{\Sigma}" display="inline"><semantics id="S3.SS1.p1.10.m1.1a"><mi id="S3.SS1.p1.10.m1.1.1" xref="S3.SS1.p1.10.m1.1.1.cmml">𝚺</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m1.1b"><ci id="S3.SS1.p1.10.m1.1.1.cmml" xref="S3.SS1.p1.10.m1.1.1">𝚺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m1.1c">\bm{\Sigma}</annotation></semantics></math> is the covariance matrix; <math id="S3.SS1.p1.11.m2.1" class="ltx_Math" alttext="\bm{C}" display="inline"><semantics id="S3.SS1.p1.11.m2.1a"><mi id="S3.SS1.p1.11.m2.1.1" xref="S3.SS1.p1.11.m2.1.1.cmml">𝑪</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m2.1b"><ci id="S3.SS1.p1.11.m2.1.1.cmml" xref="S3.SS1.p1.11.m2.1.1">𝑪</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m2.1c">\bm{C}</annotation></semantics></math> is the spatial coordinates; <math id="S3.SS1.p1.12.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.12.m3.1a"><mi id="S3.SS1.p1.12.m3.1.1" xref="S3.SS1.p1.12.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m3.1b"><ci id="S3.SS1.p1.12.m3.1.1.cmml" xref="S3.SS1.p1.12.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m3.1c">N</annotation></semantics></math> is the normalization constant.
The covariance matrix could be further represented as follows:</p>
<table id="A3.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\vspace{-0.2cm}\bm{\Sigma}" display="inline"><semantics id="S3.E3.m1.1a"><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">𝚺</mi><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝚺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\vspace{-0.2cm}\bm{\Sigma}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.3" class="ltx_Math" alttext="\displaystyle=\bm{R}\begin{bmatrix}\sigma_{1}^{2}&amp;0\\
0&amp;\sigma_{2}^{2}\end{bmatrix}\bm{R}^{T},\quad\text{($\bm{R}$ is the rotation matrix)}" display="inline"><semantics id="S3.E3.m2.3a"><mrow id="S3.E3.m2.3.3" xref="S3.E3.m2.3.3.cmml"><mi id="S3.E3.m2.3.3.3" xref="S3.E3.m2.3.3.3.cmml"></mi><mo id="S3.E3.m2.3.3.2" xref="S3.E3.m2.3.3.2.cmml">=</mo><mrow id="S3.E3.m2.3.3.1.1" xref="S3.E3.m2.3.3.1.2.cmml"><mrow id="S3.E3.m2.3.3.1.1.1" xref="S3.E3.m2.3.3.1.1.1.cmml"><mi id="S3.E3.m2.3.3.1.1.1.2" xref="S3.E3.m2.3.3.1.1.1.2.cmml">𝑹</mi><mo lspace="0em" rspace="0em" id="S3.E3.m2.3.3.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.cmml">​</mo><mrow id="S3.E3.m2.1.1a.3" xref="S3.E3.m2.1.1a.2.cmml"><mo id="S3.E3.m2.1.1a.3.1" xref="S3.E3.m2.1.1a.2.1.cmml">[</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E3.m2.1.1.1.1" xref="S3.E3.m2.1.1.1.1.cmml"><mtr id="S3.E3.m2.1.1.1.1a" xref="S3.E3.m2.1.1.1.1.cmml"><mtd id="S3.E3.m2.1.1.1.1b" xref="S3.E3.m2.1.1.1.1.cmml"><msubsup id="S3.E3.m2.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m2.1.1.1.1.1.1.1.2.2" xref="S3.E3.m2.1.1.1.1.1.1.1.2.2.cmml">σ</mi><mn id="S3.E3.m2.1.1.1.1.1.1.1.2.3" xref="S3.E3.m2.1.1.1.1.1.1.1.2.3.cmml">1</mn><mn id="S3.E3.m2.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mtd><mtd id="S3.E3.m2.1.1.1.1c" xref="S3.E3.m2.1.1.1.1.cmml"><mn id="S3.E3.m2.1.1.1.1.1.2.1" xref="S3.E3.m2.1.1.1.1.1.2.1.cmml">0</mn></mtd></mtr><mtr id="S3.E3.m2.1.1.1.1d" xref="S3.E3.m2.1.1.1.1.cmml"><mtd id="S3.E3.m2.1.1.1.1e" xref="S3.E3.m2.1.1.1.1.cmml"><mn id="S3.E3.m2.1.1.1.1.2.1.1" xref="S3.E3.m2.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E3.m2.1.1.1.1f" xref="S3.E3.m2.1.1.1.1.cmml"><msubsup id="S3.E3.m2.1.1.1.1.2.2.1" xref="S3.E3.m2.1.1.1.1.2.2.1.cmml"><mi id="S3.E3.m2.1.1.1.1.2.2.1.2.2" xref="S3.E3.m2.1.1.1.1.2.2.1.2.2.cmml">σ</mi><mn id="S3.E3.m2.1.1.1.1.2.2.1.2.3" xref="S3.E3.m2.1.1.1.1.2.2.1.2.3.cmml">2</mn><mn id="S3.E3.m2.1.1.1.1.2.2.1.3" xref="S3.E3.m2.1.1.1.1.2.2.1.3.cmml">2</mn></msubsup></mtd></mtr></mtable><mo id="S3.E3.m2.1.1a.3.2" xref="S3.E3.m2.1.1a.2.1.cmml">]</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m2.3.3.1.1.1.1a" xref="S3.E3.m2.3.3.1.1.1.1.cmml">​</mo><msup id="S3.E3.m2.3.3.1.1.1.3" xref="S3.E3.m2.3.3.1.1.1.3.cmml"><mi id="S3.E3.m2.3.3.1.1.1.3.2" xref="S3.E3.m2.3.3.1.1.1.3.2.cmml">𝑹</mi><mi id="S3.E3.m2.3.3.1.1.1.3.3" xref="S3.E3.m2.3.3.1.1.1.3.3.cmml">T</mi></msup></mrow><mo rspace="1.167em" id="S3.E3.m2.3.3.1.1.2" xref="S3.E3.m2.3.3.1.2.cmml">,</mo><mrow id="S3.E3.m2.2.2.1a" xref="S3.E3.m2.2.2.1ac.cmml"><mtext id="S3.E3.m2.2.2.1aa" xref="S3.E3.m2.2.2.1ac.cmml">(</mtext><mi id="S3.E3.m2.2.2.1.m1.1.1" xref="S3.E3.m2.2.2.1.m1.1.1.cmml">𝑹</mi><mtext id="S3.E3.m2.2.2.1ab" xref="S3.E3.m2.2.2.1ac.cmml"> is the rotation matrix)</mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.3b"><apply id="S3.E3.m2.3.3.cmml" xref="S3.E3.m2.3.3"><eq id="S3.E3.m2.3.3.2.cmml" xref="S3.E3.m2.3.3.2"></eq><csymbol cd="latexml" id="S3.E3.m2.3.3.3.cmml" xref="S3.E3.m2.3.3.3">absent</csymbol><list id="S3.E3.m2.3.3.1.2.cmml" xref="S3.E3.m2.3.3.1.1"><apply id="S3.E3.m2.3.3.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1"><times id="S3.E3.m2.3.3.1.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1"></times><ci id="S3.E3.m2.3.3.1.1.1.2.cmml" xref="S3.E3.m2.3.3.1.1.1.2">𝑹</ci><apply id="S3.E3.m2.1.1a.2.cmml" xref="S3.E3.m2.1.1a.3"><csymbol cd="latexml" id="S3.E3.m2.1.1a.2.1.cmml" xref="S3.E3.m2.1.1a.3.1">matrix</csymbol><matrix id="S3.E3.m2.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1"><matrixrow id="S3.E3.m2.1.1.1.1a.cmml" xref="S3.E3.m2.1.1.1.1"><apply id="S3.E3.m2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.2.2">𝜎</ci><cn type="integer" id="S3.E3.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.2.3">1</cn></apply><cn type="integer" id="S3.E3.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E3.m2.1.1.1.1.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.1.2.1">0</cn></matrixrow><matrixrow id="S3.E3.m2.1.1.1.1b.cmml" xref="S3.E3.m2.1.1.1.1"><cn type="integer" id="S3.E3.m2.1.1.1.1.2.1.1.cmml" xref="S3.E3.m2.1.1.1.1.2.1.1">0</cn><apply id="S3.E3.m2.1.1.1.1.2.2.1.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.2.2.1.1.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1">superscript</csymbol><apply id="S3.E3.m2.1.1.1.1.2.2.1.2.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.2.2.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.2.2.1.2.2.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1.2.2">𝜎</ci><cn type="integer" id="S3.E3.m2.1.1.1.1.2.2.1.2.3.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1.2.3">2</cn></apply><cn type="integer" id="S3.E3.m2.1.1.1.1.2.2.1.3.cmml" xref="S3.E3.m2.1.1.1.1.2.2.1.3">2</cn></apply></matrixrow></matrix></apply><apply id="S3.E3.m2.3.3.1.1.1.3.cmml" xref="S3.E3.m2.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.3.1.cmml" xref="S3.E3.m2.3.3.1.1.1.3">superscript</csymbol><ci id="S3.E3.m2.3.3.1.1.1.3.2.cmml" xref="S3.E3.m2.3.3.1.1.1.3.2">𝑹</ci><ci id="S3.E3.m2.3.3.1.1.1.3.3.cmml" xref="S3.E3.m2.3.3.1.1.1.3.3">𝑇</ci></apply></apply><ci id="S3.E3.m2.2.2.1ac.cmml" xref="S3.E3.m2.2.2.1a"><mrow id="S3.E3.m2.2.2.1a.cmml" xref="S3.E3.m2.2.2.1a"><mtext id="S3.E3.m2.2.2.1aa.cmml" xref="S3.E3.m2.2.2.1a">(</mtext><mi id="S3.E3.m2.2.2.1.m1.1.1.cmml" xref="S3.E3.m2.2.2.1.m1.1.1">𝑹</mi><mtext id="S3.E3.m2.2.2.1ab.cmml" xref="S3.E3.m2.2.2.1a"> is the rotation matrix)</mtext></mrow></ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.3c">\displaystyle=\bm{R}\begin{bmatrix}\sigma_{1}^{2}&amp;0\\
0&amp;\sigma_{2}^{2}\end{bmatrix}\bm{R}^{T},\quad\text{($\bm{R}$ is the rotation matrix)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m1.4" class="ltx_Math" alttext="\displaystyle=\begin{bmatrix}cos\theta&amp;-sin\theta\\
sin\theta&amp;cos\theta\end{bmatrix}\begin{bmatrix}\sigma_{1}^{2}&amp;0\\
0&amp;\sigma_{2}^{2}\end{bmatrix}\begin{bmatrix}cos\theta&amp;sin\theta\\
-sin\theta&amp;cos\theta\end{bmatrix}," display="inline"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.4.1" xref="S3.E4.m1.4.4.1.1.cmml"><mrow id="S3.E4.m1.4.4.1.1" xref="S3.E4.m1.4.4.1.1.cmml"><mi id="S3.E4.m1.4.4.1.1.2" xref="S3.E4.m1.4.4.1.1.2.cmml"></mi><mo id="S3.E4.m1.4.4.1.1.1" xref="S3.E4.m1.4.4.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.4.4.1.1.3" xref="S3.E4.m1.4.4.1.1.3.cmml"><mrow id="S3.E4.m1.1.1a.3" xref="S3.E4.m1.1.1a.2.cmml"><mo id="S3.E4.m1.1.1a.3.1" xref="S3.E4.m1.1.1a.2.1.cmml">[</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mtr id="S3.E4.m1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1b" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.1.1b" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.5" xref="S3.E4.m1.1.1.1.1.1.1.1.5.cmml">θ</mi></mrow></mtd><mtd id="S3.E4.m1.1.1.1.1c" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.2.1a" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">−</mo><mrow id="S3.E4.m1.1.1.1.1.1.2.1.2" xref="S3.E4.m1.1.1.1.1.1.2.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.2" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.2.1.2.1" xref="S3.E4.m1.1.1.1.1.1.2.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.3" xref="S3.E4.m1.1.1.1.1.1.2.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.2.1.2.1a" xref="S3.E4.m1.1.1.1.1.1.2.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.4" xref="S3.E4.m1.1.1.1.1.1.2.1.2.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.2.1.2.1b" xref="S3.E4.m1.1.1.1.1.1.2.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.2.1.2.5" xref="S3.E4.m1.1.1.1.1.1.2.1.2.5.cmml">θ</mi></mrow></mrow></mtd></mtr><mtr id="S3.E4.m1.1.1.1.1d" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1e" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.1.1" xref="S3.E4.m1.1.1.1.1.2.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2.1.1.2" xref="S3.E4.m1.1.1.1.1.2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.1.1.1" xref="S3.E4.m1.1.1.1.1.2.1.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.2.1.1.3" xref="S3.E4.m1.1.1.1.1.2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.1.1.1a" xref="S3.E4.m1.1.1.1.1.2.1.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.2.1.1.4" xref="S3.E4.m1.1.1.1.1.2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.1.1.1b" xref="S3.E4.m1.1.1.1.1.2.1.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.2.1.1.5" xref="S3.E4.m1.1.1.1.1.2.1.1.5.cmml">θ</mi></mrow></mtd><mtd id="S3.E4.m1.1.1.1.1f" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.2.1" xref="S3.E4.m1.1.1.1.1.2.2.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.2" xref="S3.E4.m1.1.1.1.1.2.2.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.2.2.1.3" xref="S3.E4.m1.1.1.1.1.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.1.1a" xref="S3.E4.m1.1.1.1.1.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.2.2.1.4" xref="S3.E4.m1.1.1.1.1.2.2.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.1.1b" xref="S3.E4.m1.1.1.1.1.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.2.2.1.5" xref="S3.E4.m1.1.1.1.1.2.2.1.5.cmml">θ</mi></mrow></mtd></mtr></mtable><mo id="S3.E4.m1.1.1a.3.2" xref="S3.E4.m1.1.1a.2.1.cmml">]</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.1.1.3.1" xref="S3.E4.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S3.E4.m1.2.2a.3" xref="S3.E4.m1.2.2a.2.cmml"><mo id="S3.E4.m1.2.2a.3.1" xref="S3.E4.m1.2.2a.2.1.cmml">[</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mtr id="S3.E4.m1.2.2.1.1a" xref="S3.E4.m1.2.2.1.1.cmml"><mtd id="S3.E4.m1.2.2.1.1b" xref="S3.E4.m1.2.2.1.1.cmml"><msubsup id="S3.E4.m1.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.2.2.cmml">σ</mi><mn id="S3.E4.m1.2.2.1.1.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.2.3.cmml">1</mn><mn id="S3.E4.m1.2.2.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.3.cmml">2</mn></msubsup></mtd><mtd id="S3.E4.m1.2.2.1.1c" xref="S3.E4.m1.2.2.1.1.cmml"><mn id="S3.E4.m1.2.2.1.1.1.2.1" xref="S3.E4.m1.2.2.1.1.1.2.1.cmml">0</mn></mtd></mtr><mtr id="S3.E4.m1.2.2.1.1d" xref="S3.E4.m1.2.2.1.1.cmml"><mtd id="S3.E4.m1.2.2.1.1e" xref="S3.E4.m1.2.2.1.1.cmml"><mn id="S3.E4.m1.2.2.1.1.2.1.1" xref="S3.E4.m1.2.2.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S3.E4.m1.2.2.1.1f" xref="S3.E4.m1.2.2.1.1.cmml"><msubsup id="S3.E4.m1.2.2.1.1.2.2.1" xref="S3.E4.m1.2.2.1.1.2.2.1.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.1.2.2" xref="S3.E4.m1.2.2.1.1.2.2.1.2.2.cmml">σ</mi><mn id="S3.E4.m1.2.2.1.1.2.2.1.2.3" xref="S3.E4.m1.2.2.1.1.2.2.1.2.3.cmml">2</mn><mn id="S3.E4.m1.2.2.1.1.2.2.1.3" xref="S3.E4.m1.2.2.1.1.2.2.1.3.cmml">2</mn></msubsup></mtd></mtr></mtable><mo id="S3.E4.m1.2.2a.3.2" xref="S3.E4.m1.2.2a.2.1.cmml">]</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.1.1.3.1a" xref="S3.E4.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S3.E4.m1.3.3a.3" xref="S3.E4.m1.3.3a.2.cmml"><mo id="S3.E4.m1.3.3a.3.1" xref="S3.E4.m1.3.3a.2.1.cmml">[</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><mtr id="S3.E4.m1.3.3.1.1a" xref="S3.E4.m1.3.3.1.1.cmml"><mtd id="S3.E4.m1.3.3.1.1b" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.1.1.1a" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.1.1.4" xref="S3.E4.m1.3.3.1.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.1.1.1b" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.1.1.5" xref="S3.E4.m1.3.3.1.1.1.1.1.5.cmml">θ</mi></mrow></mtd><mtd id="S3.E4.m1.3.3.1.1c" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.2.1" xref="S3.E4.m1.3.3.1.1.1.2.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2.1.2" xref="S3.E4.m1.3.3.1.1.1.2.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.2.1.1" xref="S3.E4.m1.3.3.1.1.1.2.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.2.1.3" xref="S3.E4.m1.3.3.1.1.1.2.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.2.1.1a" xref="S3.E4.m1.3.3.1.1.1.2.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.2.1.4" xref="S3.E4.m1.3.3.1.1.1.2.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.2.1.1b" xref="S3.E4.m1.3.3.1.1.1.2.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.2.1.5" xref="S3.E4.m1.3.3.1.1.1.2.1.5.cmml">θ</mi></mrow></mtd></mtr><mtr id="S3.E4.m1.3.3.1.1d" xref="S3.E4.m1.3.3.1.1.cmml"><mtd id="S3.E4.m1.3.3.1.1e" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.2.1.1" xref="S3.E4.m1.3.3.1.1.2.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.2.1.1a" xref="S3.E4.m1.3.3.1.1.2.1.1.cmml">−</mo><mrow id="S3.E4.m1.3.3.1.1.2.1.1.2" xref="S3.E4.m1.3.3.1.1.2.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.2.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.1.1.2.1" xref="S3.E4.m1.3.3.1.1.2.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.2.1.1.2.3" xref="S3.E4.m1.3.3.1.1.2.1.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.1.1.2.1a" xref="S3.E4.m1.3.3.1.1.2.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.2.1.1.2.4" xref="S3.E4.m1.3.3.1.1.2.1.1.2.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.1.1.2.1b" xref="S3.E4.m1.3.3.1.1.2.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.2.1.1.2.5" xref="S3.E4.m1.3.3.1.1.2.1.1.2.5.cmml">θ</mi></mrow></mrow></mtd><mtd id="S3.E4.m1.3.3.1.1f" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.2.2.1" xref="S3.E4.m1.3.3.1.1.2.2.1.cmml"><mi id="S3.E4.m1.3.3.1.1.2.2.1.2" xref="S3.E4.m1.3.3.1.1.2.2.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.2.1.1" xref="S3.E4.m1.3.3.1.1.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.2.2.1.3" xref="S3.E4.m1.3.3.1.1.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.2.1.1a" xref="S3.E4.m1.3.3.1.1.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.2.2.1.4" xref="S3.E4.m1.3.3.1.1.2.2.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.2.1.1b" xref="S3.E4.m1.3.3.1.1.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.2.2.1.5" xref="S3.E4.m1.3.3.1.1.2.2.1.5.cmml">θ</mi></mrow></mtd></mtr></mtable><mo id="S3.E4.m1.3.3a.3.2" xref="S3.E4.m1.3.3a.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E4.m1.4.4.1.2" xref="S3.E4.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.4.1.1.cmml" xref="S3.E4.m1.4.4.1"><eq id="S3.E4.m1.4.4.1.1.1.cmml" xref="S3.E4.m1.4.4.1.1.1"></eq><csymbol cd="latexml" id="S3.E4.m1.4.4.1.1.2.cmml" xref="S3.E4.m1.4.4.1.1.2">absent</csymbol><apply id="S3.E4.m1.4.4.1.1.3.cmml" xref="S3.E4.m1.4.4.1.1.3"><times id="S3.E4.m1.4.4.1.1.3.1.cmml" xref="S3.E4.m1.4.4.1.1.3.1"></times><apply id="S3.E4.m1.1.1a.2.cmml" xref="S3.E4.m1.1.1a.3"><csymbol cd="latexml" id="S3.E4.m1.1.1a.2.1.cmml" xref="S3.E4.m1.1.1a.3.1">matrix</csymbol><matrix id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><matrixrow id="S3.E4.m1.1.1.1.1a.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"></times><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">𝑜</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.4">𝑠</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.5.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.5">𝜃</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1"><minus id="S3.E4.m1.1.1.1.1.1.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1"></minus><apply id="S3.E4.m1.1.1.1.1.1.2.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2"><times id="S3.E4.m1.1.1.1.1.1.2.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.1"></times><ci id="S3.E4.m1.1.1.1.1.1.2.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.2">𝑠</ci><ci id="S3.E4.m1.1.1.1.1.1.2.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.3">𝑖</ci><ci id="S3.E4.m1.1.1.1.1.1.2.1.2.4.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.4">𝑛</ci><ci id="S3.E4.m1.1.1.1.1.1.2.1.2.5.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1.2.5">𝜃</ci></apply></apply></matrixrow><matrixrow id="S3.E4.m1.1.1.1.1b.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1"><times id="S3.E4.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.1"></times><ci id="S3.E4.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.2">𝑠</ci><ci id="S3.E4.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.3">𝑖</ci><ci id="S3.E4.m1.1.1.1.1.2.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.4">𝑛</ci><ci id="S3.E4.m1.1.1.1.1.2.1.1.5.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1.5">𝜃</ci></apply><apply id="S3.E4.m1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1"><times id="S3.E4.m1.1.1.1.1.2.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1"></times><ci id="S3.E4.m1.1.1.1.1.2.2.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.2">𝑐</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.3">𝑜</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.4.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.4">𝑠</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.5.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.5">𝜃</ci></apply></matrixrow></matrix></apply><apply id="S3.E4.m1.2.2a.2.cmml" xref="S3.E4.m1.2.2a.3"><csymbol cd="latexml" id="S3.E4.m1.2.2a.2.1.cmml" xref="S3.E4.m1.2.2a.3.1">matrix</csymbol><matrix id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1"><matrixrow id="S3.E4.m1.2.2.1.1a.cmml" xref="S3.E4.m1.2.2.1.1"><apply id="S3.E4.m1.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.2.2">𝜎</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.2.3">1</cn></apply><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E4.m1.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.2.1">0</cn></matrixrow><matrixrow id="S3.E4.m1.2.2.1.1b.cmml" xref="S3.E4.m1.2.2.1.1"><cn type="integer" id="S3.E4.m1.2.2.1.1.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.1.1">0</cn><apply id="S3.E4.m1.2.2.1.1.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.2.2.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.2.2">𝜎</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.2.2.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.2.3">2</cn></apply><cn type="integer" id="S3.E4.m1.2.2.1.1.2.2.1.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.1.3">2</cn></apply></matrixrow></matrix></apply><apply id="S3.E4.m1.3.3a.2.cmml" xref="S3.E4.m1.3.3a.3"><csymbol cd="latexml" id="S3.E4.m1.3.3a.2.1.cmml" xref="S3.E4.m1.3.3a.3.1">matrix</csymbol><matrix id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1"><matrixrow id="S3.E4.m1.3.3.1.1a.cmml" xref="S3.E4.m1.3.3.1.1"><apply id="S3.E4.m1.3.3.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"></times><ci id="S3.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.2">𝑐</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.3">𝑜</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.4.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.4">𝑠</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.5.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.5">𝜃</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1"><times id="S3.E4.m1.3.3.1.1.1.2.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1.1"></times><ci id="S3.E4.m1.3.3.1.1.1.2.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1.2">𝑠</ci><ci id="S3.E4.m1.3.3.1.1.1.2.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1.3">𝑖</ci><ci id="S3.E4.m1.3.3.1.1.1.2.1.4.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1.4">𝑛</ci><ci id="S3.E4.m1.3.3.1.1.1.2.1.5.cmml" xref="S3.E4.m1.3.3.1.1.1.2.1.5">𝜃</ci></apply></matrixrow><matrixrow id="S3.E4.m1.3.3.1.1b.cmml" xref="S3.E4.m1.3.3.1.1"><apply id="S3.E4.m1.3.3.1.1.2.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1"><minus id="S3.E4.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1"></minus><apply id="S3.E4.m1.3.3.1.1.2.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.2"><times id="S3.E4.m1.3.3.1.1.2.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.2.1"></times><ci id="S3.E4.m1.3.3.1.1.2.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.2.2">𝑠</ci><ci id="S3.E4.m1.3.3.1.1.2.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.2.3">𝑖</ci><ci id="S3.E4.m1.3.3.1.1.2.1.1.2.4.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.2.4">𝑛</ci><ci id="S3.E4.m1.3.3.1.1.2.1.1.2.5.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.2.5">𝜃</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1"><times id="S3.E4.m1.3.3.1.1.2.2.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1"></times><ci id="S3.E4.m1.3.3.1.1.2.2.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.2">𝑐</ci><ci id="S3.E4.m1.3.3.1.1.2.2.1.3.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.3">𝑜</ci><ci id="S3.E4.m1.3.3.1.1.2.2.1.4.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.4">𝑠</ci><ci id="S3.E4.m1.3.3.1.1.2.2.1.5.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.5">𝜃</ci></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">\displaystyle=\begin{bmatrix}cos\theta&amp;-sin\theta\\
sin\theta&amp;cos\theta\end{bmatrix}\begin{bmatrix}\sigma_{1}^{2}&amp;0\\
0&amp;\sigma_{2}^{2}\end{bmatrix}\begin{bmatrix}cos\theta&amp;sin\theta\\
-sin\theta&amp;cos\theta\end{bmatrix},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.18" class="ltx_p">where <math id="S3.SS1.p1.13.m1.1" class="ltx_Math" alttext="\sigma_{1}" display="inline"><semantics id="S3.SS1.p1.13.m1.1a"><msub id="S3.SS1.p1.13.m1.1.1" xref="S3.SS1.p1.13.m1.1.1.cmml"><mi id="S3.SS1.p1.13.m1.1.1.2" xref="S3.SS1.p1.13.m1.1.1.2.cmml">σ</mi><mn id="S3.SS1.p1.13.m1.1.1.3" xref="S3.SS1.p1.13.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m1.1b"><apply id="S3.SS1.p1.13.m1.1.1.cmml" xref="S3.SS1.p1.13.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m1.1.1.1.cmml" xref="S3.SS1.p1.13.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.13.m1.1.1.2.cmml" xref="S3.SS1.p1.13.m1.1.1.2">𝜎</ci><cn type="integer" id="S3.SS1.p1.13.m1.1.1.3.cmml" xref="S3.SS1.p1.13.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m1.1c">\sigma_{1}</annotation></semantics></math> and <math id="S3.SS1.p1.14.m2.1" class="ltx_Math" alttext="\sigma_{2}" display="inline"><semantics id="S3.SS1.p1.14.m2.1a"><msub id="S3.SS1.p1.14.m2.1.1" xref="S3.SS1.p1.14.m2.1.1.cmml"><mi id="S3.SS1.p1.14.m2.1.1.2" xref="S3.SS1.p1.14.m2.1.1.2.cmml">σ</mi><mn id="S3.SS1.p1.14.m2.1.1.3" xref="S3.SS1.p1.14.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m2.1b"><apply id="S3.SS1.p1.14.m2.1.1.cmml" xref="S3.SS1.p1.14.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m2.1.1.1.cmml" xref="S3.SS1.p1.14.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.14.m2.1.1.2.cmml" xref="S3.SS1.p1.14.m2.1.1.2">𝜎</ci><cn type="integer" id="S3.SS1.p1.14.m2.1.1.3.cmml" xref="S3.SS1.p1.14.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m2.1c">\sigma_{2}</annotation></semantics></math> are the standard deviation along the two principal axes (<em id="S3.SS1.p1.18.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p1.18.2" class="ltx_text"></span>, eigenvalues of the covariance matrix); <math id="S3.SS1.p1.15.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p1.15.m3.1a"><mi id="S3.SS1.p1.15.m3.1.1" xref="S3.SS1.p1.15.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m3.1b"><ci id="S3.SS1.p1.15.m3.1.1.cmml" xref="S3.SS1.p1.15.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m3.1c">\theta</annotation></semantics></math> is the rotation degree.
When <math id="S3.SS1.p1.16.m4.1" class="ltx_Math" alttext="\sigma_{1}=\sigma_{2}" display="inline"><semantics id="S3.SS1.p1.16.m4.1a"><mrow id="S3.SS1.p1.16.m4.1.1" xref="S3.SS1.p1.16.m4.1.1.cmml"><msub id="S3.SS1.p1.16.m4.1.1.2" xref="S3.SS1.p1.16.m4.1.1.2.cmml"><mi id="S3.SS1.p1.16.m4.1.1.2.2" xref="S3.SS1.p1.16.m4.1.1.2.2.cmml">σ</mi><mn id="S3.SS1.p1.16.m4.1.1.2.3" xref="S3.SS1.p1.16.m4.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS1.p1.16.m4.1.1.1" xref="S3.SS1.p1.16.m4.1.1.1.cmml">=</mo><msub id="S3.SS1.p1.16.m4.1.1.3" xref="S3.SS1.p1.16.m4.1.1.3.cmml"><mi id="S3.SS1.p1.16.m4.1.1.3.2" xref="S3.SS1.p1.16.m4.1.1.3.2.cmml">σ</mi><mn id="S3.SS1.p1.16.m4.1.1.3.3" xref="S3.SS1.p1.16.m4.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m4.1b"><apply id="S3.SS1.p1.16.m4.1.1.cmml" xref="S3.SS1.p1.16.m4.1.1"><eq id="S3.SS1.p1.16.m4.1.1.1.cmml" xref="S3.SS1.p1.16.m4.1.1.1"></eq><apply id="S3.SS1.p1.16.m4.1.1.2.cmml" xref="S3.SS1.p1.16.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m4.1.1.2.1.cmml" xref="S3.SS1.p1.16.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.16.m4.1.1.2.2.cmml" xref="S3.SS1.p1.16.m4.1.1.2.2">𝜎</ci><cn type="integer" id="S3.SS1.p1.16.m4.1.1.2.3.cmml" xref="S3.SS1.p1.16.m4.1.1.2.3">1</cn></apply><apply id="S3.SS1.p1.16.m4.1.1.3.cmml" xref="S3.SS1.p1.16.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m4.1.1.3.1.cmml" xref="S3.SS1.p1.16.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.16.m4.1.1.3.2.cmml" xref="S3.SS1.p1.16.m4.1.1.3.2">𝜎</ci><cn type="integer" id="S3.SS1.p1.16.m4.1.1.3.3.cmml" xref="S3.SS1.p1.16.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m4.1c">\sigma_{1}=\sigma_{2}</annotation></semantics></math>, <math id="S3.SS1.p1.17.m5.1" class="ltx_Math" alttext="\bm{k}" display="inline"><semantics id="S3.SS1.p1.17.m5.1a"><mi id="S3.SS1.p1.17.m5.1.1" xref="S3.SS1.p1.17.m5.1.1.cmml">𝒌</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m5.1b"><ci id="S3.SS1.p1.17.m5.1.1.cmml" xref="S3.SS1.p1.17.m5.1.1">𝒌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m5.1c">\bm{k}</annotation></semantics></math> is an isotropic Gaussian blur kernel; otherwise <math id="S3.SS1.p1.18.m6.1" class="ltx_Math" alttext="\bm{k}" display="inline"><semantics id="S3.SS1.p1.18.m6.1a"><mi id="S3.SS1.p1.18.m6.1.1" xref="S3.SS1.p1.18.m6.1.1.cmml">𝒌</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m6.1b"><ci id="S3.SS1.p1.18.m6.1.1.cmml" xref="S3.SS1.p1.18.m6.1.1">𝒌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m6.1c">\bm{k}</annotation></semantics></math> is an anisotropic kernel.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.3" class="ltx_p"><span id="S3.SS1.p2.3.1" class="ltx_text ltx_font_bold">Discussion.</span>
Though Gaussian blur kernels are widely used to model blur degradation,
they may not well approximate real camera blur.
To include more diverse kernel shapes, we further adopt generalized Gaussian blur kernels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and a plateau-shaped distribution. Their probability density function (pdf) are <math id="S3.SS1.p2.1.m1.1" class="ltx_math_unparsed" alttext="\frac{1}{N}\exp(-\frac{1}{2}(\bm{C}^{T}\bm{\Sigma}^{-1}\bm{C})^{\beta}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1b"><mfrac id="S3.SS1.p2.1.m1.1.1"><mn id="S3.SS1.p2.1.m1.1.1.2">1</mn><mi id="S3.SS1.p2.1.m1.1.1.3">N</mi></mfrac><mi id="S3.SS1.p2.1.m1.1.2">exp</mi><mrow id="S3.SS1.p2.1.m1.1.3"><mo stretchy="false" id="S3.SS1.p2.1.m1.1.3.1">(</mo><mo lspace="0em" id="S3.SS1.p2.1.m1.1.3.2">−</mo><mfrac id="S3.SS1.p2.1.m1.1.3.3"><mn id="S3.SS1.p2.1.m1.1.3.3.2">1</mn><mn id="S3.SS1.p2.1.m1.1.3.3.3">2</mn></mfrac><msup id="S3.SS1.p2.1.m1.1.3.4"><mrow id="S3.SS1.p2.1.m1.1.3.4.2"><mo stretchy="false" id="S3.SS1.p2.1.m1.1.3.4.2.1">(</mo><msup id="S3.SS1.p2.1.m1.1.3.4.2.2"><mi id="S3.SS1.p2.1.m1.1.3.4.2.2.2">𝑪</mi><mi id="S3.SS1.p2.1.m1.1.3.4.2.2.3">T</mi></msup><msup id="S3.SS1.p2.1.m1.1.3.4.2.3"><mi id="S3.SS1.p2.1.m1.1.3.4.2.3.2">𝚺</mi><mrow id="S3.SS1.p2.1.m1.1.3.4.2.3.3"><mo id="S3.SS1.p2.1.m1.1.3.4.2.3.3a">−</mo><mn id="S3.SS1.p2.1.m1.1.3.4.2.3.3.2">1</mn></mrow></msup><mi id="S3.SS1.p2.1.m1.1.3.4.2.4">𝑪</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.1.3.4.2.5">)</mo></mrow><mi id="S3.SS1.p2.1.m1.1.3.4.3">β</mi></msup></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\frac{1}{N}\exp(-\frac{1}{2}(\bm{C}^{T}\bm{\Sigma}^{-1}\bm{C})^{\beta}</annotation></semantics></math>, and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\frac{1}{N}\frac{1}{1+(\bm{C}^{T}\bm{\Sigma}^{-1}\bm{C})^{\beta}}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.2" xref="S3.SS1.p2.2.m2.1.2.cmml"><mfrac id="S3.SS1.p2.2.m2.1.2.2" xref="S3.SS1.p2.2.m2.1.2.2.cmml"><mn id="S3.SS1.p2.2.m2.1.2.2.2" xref="S3.SS1.p2.2.m2.1.2.2.2.cmml">1</mn><mi id="S3.SS1.p2.2.m2.1.2.2.3" xref="S3.SS1.p2.2.m2.1.2.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.2.1" xref="S3.SS1.p2.2.m2.1.2.1.cmml">​</mo><mfrac id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">1</mn><mrow id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.3.cmml">1</mn><mo id="S3.SS1.p2.2.m2.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.2.cmml">+</mo><msup id="S3.SS1.p2.2.m2.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.cmml"><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml"><msup id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.2.cmml">𝑪</mi><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.2.cmml">𝚺</mi><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.cmml"><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3a" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.cmml">−</mo><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1a" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.4" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.4.cmml">𝑪</mi></mrow><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS1.p2.2.m2.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.3.cmml">β</mi></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.2.cmml" xref="S3.SS1.p2.2.m2.1.2"><times id="S3.SS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.1"></times><apply id="S3.SS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2"><divide id="S3.SS1.p2.2.m2.1.2.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.2"></divide><cn type="integer" id="S3.SS1.p2.2.m2.1.2.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2">1</cn><ci id="S3.SS1.p2.2.m2.1.2.2.3.cmml" xref="S3.SS1.p2.2.m2.1.2.2.3">𝑁</ci></apply><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><divide id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1"></divide><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">1</cn><apply id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"><plus id="S3.SS1.p2.2.m2.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.2"></plus><cn type="integer" id="S3.SS1.p2.2.m2.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.3">1</cn><apply id="S3.SS1.p2.2.m2.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.1"></times><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.2">𝑪</ci><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.2">𝚺</ci><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3"><minus id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3"></minus><cn type="integer" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.3.3.2">1</cn></apply></apply><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.4.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.4">𝑪</ci></apply><ci id="S3.SS1.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.3">𝛽</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\frac{1}{N}\frac{1}{1+(\bm{C}^{T}\bm{\Sigma}^{-1}\bm{C})^{\beta}}</annotation></semantics></math>, respectively. <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\beta</annotation></semantics></math> is the shape parameter.
Empirically, we find that including these blur kernels could produce sharper outputs for several real samples.

<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS1.p2.3.2" class="ltx_text ltx_font_bold">Noise.</span>
We consider two commonly-used noise types – 1) additive Gaussian noise and 2) Poisson noise.
Addictive Gaussian noise has a probability density function equal to that of the Gaussian distribution.
The noise intensity is controlled by the standard deviation (<em id="S3.SS1.p2.3.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p2.3.4" class="ltx_text"></span>, sigma value) of the Gaussian distribution.
When each channel of RGB images has independent sampled noise, the synthetic noise is color noise.
We also synthesize gray noise by employing the same sampled noise to all three channels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Poisson noise follows the Poisson distribution. It is usually used to approximately model the sensor noise caused by statistical quantum fluctuations, that is, variation in the number of photons sensed at a given exposure level.
Poisson noise has an intensity proportional to the image intensity, and the noises at different pixels are independent.

<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Resize (Downsampling).</span>
Downsampling is a basic operation for synthesizing low-resolution images in SR.
More generally, we consider both downsamping and upsampling, <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p3.1.3" class="ltx_text"></span>, the resize operation.
There are several resize algorithms - nearest-neighbor interpolation, area resize, bilinear interpolation, and bicubic interpolation. Different resize operations bring in different effects - some produce blurry results while some may output over-sharp images with overshoot artifacts.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In order to include more diverse and complex resize effects, we consider a random resize operation from the above choices. As nearest-neighbor interpolation introduces the misalignment issue, we exclude it and only consider the area, bilinear and bicubic operations.

<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">JPEG compression.</span>
JPEG compression is a commonly used technique of lossy compression for digital images.
It first converts images into the YCbCr color space and downsamples the chroma channels.
Images are then split into <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mn id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">8</cn><cn type="integer" id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">8\times 8</annotation></semantics></math> blocks and each block is transformed with a two-dimensional discrete cosine transform (DCT), followed by a quantization of DCT coefficients. More details of JPEG compression algorithms can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
Unpleasing block artifacts are usually introduced by the JPEG compression.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.3" class="ltx_p">The quality of compressed images is determined by a quality factor <math id="S3.SS1.p5.1.m1.2" class="ltx_Math" alttext="q\in[0,100]" display="inline"><semantics id="S3.SS1.p5.1.m1.2a"><mrow id="S3.SS1.p5.1.m1.2.3" xref="S3.SS1.p5.1.m1.2.3.cmml"><mi id="S3.SS1.p5.1.m1.2.3.2" xref="S3.SS1.p5.1.m1.2.3.2.cmml">q</mi><mo id="S3.SS1.p5.1.m1.2.3.1" xref="S3.SS1.p5.1.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p5.1.m1.2.3.3.2" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p5.1.m1.2.3.3.2.1" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">0</mn><mo id="S3.SS1.p5.1.m1.2.3.3.2.2" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p5.1.m1.2.2" xref="S3.SS1.p5.1.m1.2.2.cmml">100</mn><mo stretchy="false" id="S3.SS1.p5.1.m1.2.3.3.2.3" xref="S3.SS1.p5.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.2b"><apply id="S3.SS1.p5.1.m1.2.3.cmml" xref="S3.SS1.p5.1.m1.2.3"><in id="S3.SS1.p5.1.m1.2.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.1"></in><ci id="S3.SS1.p5.1.m1.2.3.2.cmml" xref="S3.SS1.p5.1.m1.2.3.2">𝑞</ci><interval closure="closed" id="S3.SS1.p5.1.m1.2.3.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.3.2"><cn type="integer" id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">0</cn><cn type="integer" id="S3.SS1.p5.1.m1.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2">100</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.2c">q\in[0,100]</annotation></semantics></math>, where a lower <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">q</annotation></semantics></math> indicates a higher compression ratio and worse quality.
We use the PyTorch implementation - <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="\mathtt{DiffJPEG}" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">\mathtt{DiffJPEG}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2107.10833/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.5.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.1" class="ltx_text" style="font-size:90%;">Overview of the pure synthetic data generation adopted in Real-ESRGAN. It utilizes a second-order degradation process to model more practical degradations, where each degradation process adopts the classical degradation model.
The detailed choices for blur , resize, noise and JPEG compression are listed. We also employ <math id="S3.F2.3.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.F2.3.1.m1.1b"><mrow id="S3.F2.3.1.m1.1.1" xref="S3.F2.3.1.m1.1.1.cmml"><mi id="S3.F2.3.1.m1.1.1.2" xref="S3.F2.3.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.F2.3.1.m1.1.1.1" xref="S3.F2.3.1.m1.1.1.1.cmml">​</mo><mi id="S3.F2.3.1.m1.1.1.3" xref="S3.F2.3.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F2.3.1.m1.1.1.1b" xref="S3.F2.3.1.m1.1.1.1.cmml">​</mo><mi id="S3.F2.3.1.m1.1.1.4" xref="S3.F2.3.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.F2.3.1.m1.1.1.1c" xref="S3.F2.3.1.m1.1.1.1.cmml">​</mo><mi id="S3.F2.3.1.m1.1.1.5" xref="S3.F2.3.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.m1.1c"><apply id="S3.F2.3.1.m1.1.1.cmml" xref="S3.F2.3.1.m1.1.1"><times id="S3.F2.3.1.m1.1.1.1.cmml" xref="S3.F2.3.1.m1.1.1.1"></times><ci id="S3.F2.3.1.m1.1.1.2.cmml" xref="S3.F2.3.1.m1.1.1.2">𝑠</ci><ci id="S3.F2.3.1.m1.1.1.3.cmml" xref="S3.F2.3.1.m1.1.1.3">𝑖</ci><ci id="S3.F2.3.1.m1.1.1.4.cmml" xref="S3.F2.3.1.m1.1.1.4">𝑛</ci><ci id="S3.F2.3.1.m1.1.1.5.cmml" xref="S3.F2.3.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.m1.1d">sinc</annotation></semantics></math> filter to synthesize common ringing and overshoot artifacts.</span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2107.10833/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="365" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text" style="font-size:90%;">Models trained with synthetic data of classical degradation model could resolve some real samples (Left). Yet, they amplify noises or introduce ringing artifacts for complex real-world images (Right). Zoom in for best view</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>High-order Degradation Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">When we adopt the above classical degradation model to synthesize training pairs, the trained model could indeed handle some real samples. However, it still can not resolve some complicated degradations in the real world, especially the unknown noises and complex artifacts (see Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Classical Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
It is because that the synthetic low-resolution images still have a large gap with realistic degraded images.
We thus extend the classical degradation model to a high-order degradation process to model more practical degradations.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The classical degradation model only includes a fixed number of basic degradations, which can be regarded as a first-order modeling.
However, the real-life degradation processes are quite diverse, and usually comprise a series of procedures including imaging system of cameras, image editing, Internet transmission, <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">etc</em>.
For instance, when we want to restore a low-quality image download from the Internet, its underlying degradation involves
a complicated combination of different degradation processes.
Specifically, the original image might be taken with a cellphone many years ago, which inevitably contains degradations such as camera blur, sensor noise, low resolution and JPEG compression.
The image was then edited with sharpening and resize operations, bringing in overshoot and blur artifacts.
After that, it was uploaded to some social media applications, which introduces a further compression and unpredictable noises. As the digital transmission will also bring artifacts, this process becomes more complicated when the image spreads several times on the Internet.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2107.10833/assets/x4.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="112" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.7.3.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.5.2" class="ltx_text" style="font-size:90%;">Real-ESRGAN adopts the same generator network as that in ESRGAN. For the scale factor of <math id="S3.F4.4.1.m1.1" class="ltx_Math" alttext="\times 2" display="inline"><semantics id="S3.F4.4.1.m1.1b"><mrow id="S3.F4.4.1.m1.1.1" xref="S3.F4.4.1.m1.1.1.cmml"><mi id="S3.F4.4.1.m1.1.1.2" xref="S3.F4.4.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.F4.4.1.m1.1.1.1" xref="S3.F4.4.1.m1.1.1.1.cmml">×</mo><mn id="S3.F4.4.1.m1.1.1.3" xref="S3.F4.4.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.4.1.m1.1c"><apply id="S3.F4.4.1.m1.1.1.cmml" xref="S3.F4.4.1.m1.1.1"><times id="S3.F4.4.1.m1.1.1.1.cmml" xref="S3.F4.4.1.m1.1.1.1"></times><csymbol cd="latexml" id="S3.F4.4.1.m1.1.1.2.cmml" xref="S3.F4.4.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.F4.4.1.m1.1.1.3.cmml" xref="S3.F4.4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.1.m1.1d">\times 2</annotation></semantics></math> and <math id="S3.F4.5.2.m2.1" class="ltx_Math" alttext="\times 1" display="inline"><semantics id="S3.F4.5.2.m2.1b"><mrow id="S3.F4.5.2.m2.1.1" xref="S3.F4.5.2.m2.1.1.cmml"><mi id="S3.F4.5.2.m2.1.1.2" xref="S3.F4.5.2.m2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.F4.5.2.m2.1.1.1" xref="S3.F4.5.2.m2.1.1.1.cmml">×</mo><mn id="S3.F4.5.2.m2.1.1.3" xref="S3.F4.5.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.5.2.m2.1c"><apply id="S3.F4.5.2.m2.1.1.cmml" xref="S3.F4.5.2.m2.1.1"><times id="S3.F4.5.2.m2.1.1.1.cmml" xref="S3.F4.5.2.m2.1.1.1"></times><csymbol cd="latexml" id="S3.F4.5.2.m2.1.1.2.cmml" xref="S3.F4.5.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S3.F4.5.2.m2.1.1.3.cmml" xref="S3.F4.5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.2.m2.1d">\times 1</annotation></semantics></math>, it first employs a pixel-unshuffle operation to reduce spatial size and re-arrange information to the channel dimension.</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2107.10833/assets/x5.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="263" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.7.2.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Top<span id="S3.F5.3.1.2" class="ltx_text ltx_font_medium">: Real samples suffering from ringing and overshoot artifacts. </span>Bottom<span id="S3.F5.3.1.1" class="ltx_text ltx_font_medium">: Examples of <math id="S3.F5.3.1.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.F5.3.1.1.m1.1b"><mrow id="S3.F5.3.1.1.m1.1.1" xref="S3.F5.3.1.1.m1.1.1.cmml"><mi id="S3.F5.3.1.1.m1.1.1.2" xref="S3.F5.3.1.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.F5.3.1.1.m1.1.1.1" xref="S3.F5.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.F5.3.1.1.m1.1.1.3" xref="S3.F5.3.1.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F5.3.1.1.m1.1.1.1b" xref="S3.F5.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.F5.3.1.1.m1.1.1.4" xref="S3.F5.3.1.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.F5.3.1.1.m1.1.1.1c" xref="S3.F5.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.F5.3.1.1.m1.1.1.5" xref="S3.F5.3.1.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.3.1.1.m1.1c"><apply id="S3.F5.3.1.1.m1.1.1.cmml" xref="S3.F5.3.1.1.m1.1.1"><times id="S3.F5.3.1.1.m1.1.1.1.cmml" xref="S3.F5.3.1.1.m1.1.1.1"></times><ci id="S3.F5.3.1.1.m1.1.1.2.cmml" xref="S3.F5.3.1.1.m1.1.1.2">𝑠</ci><ci id="S3.F5.3.1.1.m1.1.1.3.cmml" xref="S3.F5.3.1.1.m1.1.1.3">𝑖</ci><ci id="S3.F5.3.1.1.m1.1.1.4.cmml" xref="S3.F5.3.1.1.m1.1.1.4">𝑛</ci><ci id="S3.F5.3.1.1.m1.1.1.5.cmml" xref="S3.F5.3.1.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.3.1.1.m1.1d">sinc</annotation></semantics></math> kernels (kernel size 21) and the corresponding filtered images. Zoom in for best view</span></span></figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Such a complicated deterioration process could not be modeled with the classical first-order model. Thus, we propose a high-order degradation model. An <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">n</annotation></semantics></math>-order model involves <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">n</annotation></semantics></math> repeated degradation processes (as shown in Eq. <a href="#S3.E5" title="In 3.2 High-order Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), where each degradation process adopts the classical degradation model (Eq. <a href="#S3.E1" title="In 3.1 Classical Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) with the same procedure but different hyper-parameters.
Note that the “high-order” here is different from that used in mathematical functions. It mainly refers to the implementation time of the same operation.
The random shuffling strategy in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> may also include repeated degradation processes (<em id="S3.SS2.p3.2.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.p3.2.2" class="ltx_text"></span>, double blur or JPEG).
But we highlight that the high-order degradation process is the key, indicating that not all the shuffled degradations are necessary.
In order to keep the image resolution in a reasonable range, the downsampling operation in Eq. <a href="#S3.E1" title="In 3.1 Classical Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is replaced with a random resize operation.
Empirically, we adopt a second-order degradation process, as it could resolve most real cases while keeping simplicity.
Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Classical Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the overall pipeline of our pure synthetic data generation pipeline.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="\bm{x}=\mathcal{D}^{n}(\bm{y})=(\mathcal{D}_{n}\circ\cdots\circ\mathcal{D}_{2}\circ\mathcal{D}_{1})(\bm{y})." display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><mi id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml">𝒙</mi><mo id="S3.E5.m1.3.3.1.1.4" xref="S3.E5.m1.3.3.1.1.4.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.5" xref="S3.E5.m1.3.3.1.1.5.cmml"><msup id="S3.E5.m1.3.3.1.1.5.2" xref="S3.E5.m1.3.3.1.1.5.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.3.3.1.1.5.2.2" xref="S3.E5.m1.3.3.1.1.5.2.2.cmml">𝒟</mi><mi id="S3.E5.m1.3.3.1.1.5.2.3" xref="S3.E5.m1.3.3.1.1.5.2.3.cmml">n</mi></msup><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.5.1" xref="S3.E5.m1.3.3.1.1.5.1.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.5.3.2" xref="S3.E5.m1.3.3.1.1.5.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.5.3.2.1" xref="S3.E5.m1.3.3.1.1.5.cmml">(</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">𝒚</mi><mo stretchy="false" id="S3.E5.m1.3.3.1.1.5.3.2.2" xref="S3.E5.m1.3.3.1.1.5.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.3.3.1.1.6" xref="S3.E5.m1.3.3.1.1.6.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.cmml">𝒟</mi><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.3.cmml">n</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.3.3.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml">∘</mo><mi mathvariant="normal" id="S3.E5.m1.3.3.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.cmml">⋯</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.3.3.1.1.1.1.1.1.1a" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml">∘</mo><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.4" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.3.3.1.1.1.1.1.1.4.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4.2.cmml">𝒟</mi><mn id="S3.E5.m1.3.3.1.1.1.1.1.1.4.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4.3.cmml">2</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.3.3.1.1.1.1.1.1.1b" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml">∘</mo><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.5" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.3.3.1.1.1.1.1.1.5.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5.2.cmml">𝒟</mi><mn id="S3.E5.m1.3.3.1.1.1.1.1.1.5.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.3.2.1" xref="S3.E5.m1.3.3.1.1.1.cmml">(</mo><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">𝒚</mi><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.3.2.2" xref="S3.E5.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><and id="S3.E5.m1.3.3.1.1a.cmml" xref="S3.E5.m1.3.3.1"></and><apply id="S3.E5.m1.3.3.1.1b.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.4.cmml" xref="S3.E5.m1.3.3.1.1.4"></eq><ci id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3">𝒙</ci><apply id="S3.E5.m1.3.3.1.1.5.cmml" xref="S3.E5.m1.3.3.1.1.5"><times id="S3.E5.m1.3.3.1.1.5.1.cmml" xref="S3.E5.m1.3.3.1.1.5.1"></times><apply id="S3.E5.m1.3.3.1.1.5.2.cmml" xref="S3.E5.m1.3.3.1.1.5.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.5.2.1.cmml" xref="S3.E5.m1.3.3.1.1.5.2">superscript</csymbol><ci id="S3.E5.m1.3.3.1.1.5.2.2.cmml" xref="S3.E5.m1.3.3.1.1.5.2.2">𝒟</ci><ci id="S3.E5.m1.3.3.1.1.5.2.3.cmml" xref="S3.E5.m1.3.3.1.1.5.2.3">𝑛</ci></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝒚</ci></apply></apply><apply id="S3.E5.m1.3.3.1.1c.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.6.cmml" xref="S3.E5.m1.3.3.1.1.6"></eq><share href="#S3.E5.m1.3.3.1.1.5.cmml" id="S3.E5.m1.3.3.1.1d.cmml" xref="S3.E5.m1.3.3.1"></share><apply id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"><times id="S3.E5.m1.3.3.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.2"></times><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1"><compose id="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1"></compose><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.2">𝒟</ci><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.3">𝑛</ci></apply><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3">⋯</ci><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.4.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.4.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4.2">𝒟</ci><cn type="integer" id="S3.E5.m1.3.3.1.1.1.1.1.1.4.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.4.3">2</cn></apply><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5.2">𝒟</ci><cn type="integer" id="S3.E5.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.5.3">1</cn></apply></apply><ci id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2">𝒚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">\bm{x}=\mathcal{D}^{n}(\bm{y})=(\mathcal{D}_{n}\circ\cdots\circ\mathcal{D}_{2}\circ\mathcal{D}_{1})(\bm{y}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.1" class="ltx_p">It is worth noting that the improved high-order degradation process is not perfect and could not cover the whole degradation space in the real world. Instead, it merely extends the solvable degradation boundary of previous blind SR methods through modifying the data synthesis process. Several typical limitation scenarios can be found in Fig. <a href="#S4.F11" title="Figure 11 ‣ 4.4 Limitations ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Ringing and overshoot artifacts</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Ringing artifacts often appear as spurious edges near sharp transitions in an image. They visually look like bands or “ghosts” near edges.
Overshoot artifacts are usually combined with ringing artifacts, which manifest themselves as an increased jump at the edge transition.
The main cause of these artifacts is that the signal is bandlimited without high frequencies.
These artifacts are very common and usually produced by a sharping algorithm, JPEG compression, <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>.
Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 High-order Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (Top) shows some real samples suffering from ringing and overshoot artifacts.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">We employ the <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1a" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.1.m1.1.1.4" xref="S3.SS3.p2.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1b" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.1.m1.1.1.5" xref="S3.SS3.p2.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑠</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑖</ci><ci id="S3.SS3.p2.1.m1.1.1.4.cmml" xref="S3.SS3.p2.1.m1.1.1.4">𝑛</ci><ci id="S3.SS3.p2.1.m1.1.1.5.cmml" xref="S3.SS3.p2.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">sinc</annotation></semantics></math> filter, an idealized filter that cuts off high frequencies, to synthesize ringing and overshoot artifacts for training pairs.
The <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.1a" xref="S3.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.2.m2.1.1.4" xref="S3.SS3.p2.2.m2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.1.1.1b" xref="S3.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.2.m2.1.1.5" xref="S3.SS3.p2.2.m2.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><times id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></times><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑠</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑖</ci><ci id="S3.SS3.p2.2.m2.1.1.4.cmml" xref="S3.SS3.p2.2.m2.1.1.4">𝑛</ci><ci id="S3.SS3.p2.2.m2.1.1.5.cmml" xref="S3.SS3.p2.2.m2.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">sinc</annotation></semantics></math> filter kernel can be expressed as<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use the implementation in <a target="_blank" href="https://dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter" title="" class="ltx_ref ltx_href">this url</a>.</span></span></span>:</p>
<table id="A3.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\displaystyle\bm{k}(i,j)" display="inline"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.3" xref="S3.E6.m1.2.3.cmml"><mi id="S3.E6.m1.2.3.2" xref="S3.E6.m1.2.3.2.cmml">𝒌</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.3.1" xref="S3.E6.m1.2.3.1.cmml">​</mo><mrow id="S3.E6.m1.2.3.3.2" xref="S3.E6.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.E6.m1.2.3.3.2.1" xref="S3.E6.m1.2.3.3.1.cmml">(</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">i</mi><mo id="S3.E6.m1.2.3.3.2.2" xref="S3.E6.m1.2.3.3.1.cmml">,</mo><mi id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.E6.m1.2.3.3.2.3" xref="S3.E6.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.3.cmml" xref="S3.E6.m1.2.3"><times id="S3.E6.m1.2.3.1.cmml" xref="S3.E6.m1.2.3.1"></times><ci id="S3.E6.m1.2.3.2.cmml" xref="S3.E6.m1.2.3.2">𝒌</ci><interval closure="open" id="S3.E6.m1.2.3.3.1.cmml" xref="S3.E6.m1.2.3.3.2"><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑖</ci><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">𝑗</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\displaystyle\bm{k}(i,j)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6.m2.1" class="ltx_Math" alttext="\displaystyle=\frac{\omega_{c}}{2\pi\sqrt{i^{2}+j^{2}}}J_{1}(\omega_{c}\sqrt{i^{2}+j^{2}})," display="inline"><semantics id="S3.E6.m2.1a"><mrow id="S3.E6.m2.1.1.1" xref="S3.E6.m2.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1" xref="S3.E6.m2.1.1.1.1.cmml"><mi id="S3.E6.m2.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.3.cmml"></mi><mo id="S3.E6.m2.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.2.cmml">=</mo><mrow id="S3.E6.m2.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E6.m2.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.3.cmml"><mfrac id="S3.E6.m2.1.1.1.1.1.3a" xref="S3.E6.m2.1.1.1.1.1.3.cmml"><msub id="S3.E6.m2.1.1.1.1.1.3.2" xref="S3.E6.m2.1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.3.2.2" xref="S3.E6.m2.1.1.1.1.1.3.2.2.cmml">ω</mi><mi id="S3.E6.m2.1.1.1.1.1.3.2.3" xref="S3.E6.m2.1.1.1.1.1.3.2.3.cmml">c</mi></msub><mrow id="S3.E6.m2.1.1.1.1.1.3.3" xref="S3.E6.m2.1.1.1.1.1.3.3.cmml"><mn id="S3.E6.m2.1.1.1.1.1.3.3.2" xref="S3.E6.m2.1.1.1.1.1.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.3.3.1" xref="S3.E6.m2.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6.m2.1.1.1.1.1.3.3.3" xref="S3.E6.m2.1.1.1.1.1.3.3.3.cmml">π</mi><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.3.3.1a" xref="S3.E6.m2.1.1.1.1.1.3.3.1.cmml">​</mo><msqrt id="S3.E6.m2.1.1.1.1.1.3.3.4" xref="S3.E6.m2.1.1.1.1.1.3.3.4.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.3.3.4.2" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.cmml"><msup id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.2" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.2.cmml">i</mi><mn id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.3" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.3.cmml">2</mn></msup><mo id="S3.E6.m2.1.1.1.1.1.3.3.4.2.1" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.1.cmml">+</mo><msup id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.cmml"><mi id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.2" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.2.cmml">j</mi><mn id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.3" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.3.cmml">2</mn></msup></mrow></msqrt></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.2.cmml">​</mo><msub id="S3.E6.m2.1.1.1.1.1.4" xref="S3.E6.m2.1.1.1.1.1.4.cmml"><mi id="S3.E6.m2.1.1.1.1.1.4.2" xref="S3.E6.m2.1.1.1.1.1.4.2.cmml">J</mi><mn id="S3.E6.m2.1.1.1.1.1.4.3" xref="S3.E6.m2.1.1.1.1.1.4.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.2a" xref="S3.E6.m2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m2.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E6.m2.1.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2.2.cmml">ω</mi><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.cmml">​</mo><msqrt id="S3.E6.m2.1.1.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.cmml"><msup id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.2.cmml">i</mi><mn id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.3.cmml">2</mn></msup><mo id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.1.cmml">+</mo><msup id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.2.cmml">j</mi><mn id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.3.cmml">2</mn></msup></mrow></msqrt></mrow><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m2.1.1.1.2" xref="S3.E6.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.1b"><apply id="S3.E6.m2.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1"><eq id="S3.E6.m2.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E6.m2.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.3">absent</csymbol><apply id="S3.E6.m2.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1"><times id="S3.E6.m2.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.2"></times><apply id="S3.E6.m2.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3"><divide id="S3.E6.m2.1.1.1.1.1.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3"></divide><apply id="S3.E6.m2.1.1.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.2.2">𝜔</ci><ci id="S3.E6.m2.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3.2.3">𝑐</ci></apply><apply id="S3.E6.m2.1.1.1.1.1.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3"><times id="S3.E6.m2.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.E6.m2.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.2">2</cn><ci id="S3.E6.m2.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.3">𝜋</ci><apply id="S3.E6.m2.1.1.1.1.1.3.3.4.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4"><root id="S3.E6.m2.1.1.1.1.1.3.3.4a.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4"></root><apply id="S3.E6.m2.1.1.1.1.1.3.3.4.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2"><plus id="S3.E6.m2.1.1.1.1.1.3.3.4.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.1"></plus><apply id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.2">𝑖</ci><cn type="integer" id="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.2.3">2</cn></apply><apply id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.2">𝑗</ci><cn type="integer" id="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3.4.2.3.3">2</cn></apply></apply></apply></apply></apply><apply id="S3.E6.m2.1.1.1.1.1.4.cmml" xref="S3.E6.m2.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.4.1.cmml" xref="S3.E6.m2.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.4.2.cmml" xref="S3.E6.m2.1.1.1.1.1.4.2">𝐽</ci><cn type="integer" id="S3.E6.m2.1.1.1.1.1.4.3.cmml" xref="S3.E6.m2.1.1.1.1.1.4.3">1</cn></apply><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1"><times id="S3.E6.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2.2">𝜔</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3"><root id="S3.E6.m2.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3"></root><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2"><plus id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.1"></plus><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.2">𝑖</ci><cn type="integer" id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.2.3">2</cn></apply><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.2">𝑗</ci><cn type="integer" id="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.3.2.3.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.1c">\displaystyle=\frac{\omega_{c}}{2\pi\sqrt{i^{2}+j^{2}}}J_{1}(\omega_{c}\sqrt{i^{2}+j^{2}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.6" class="ltx_p">where <math id="S3.SS3.p2.3.m1.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S3.SS3.p2.3.m1.2a"><mrow id="S3.SS3.p2.3.m1.2.3.2" xref="S3.SS3.p2.3.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m1.2.3.2.1" xref="S3.SS3.p2.3.m1.2.3.1.cmml">(</mo><mi id="S3.SS3.p2.3.m1.1.1" xref="S3.SS3.p2.3.m1.1.1.cmml">i</mi><mo id="S3.SS3.p2.3.m1.2.3.2.2" xref="S3.SS3.p2.3.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.p2.3.m1.2.2" xref="S3.SS3.p2.3.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS3.p2.3.m1.2.3.2.3" xref="S3.SS3.p2.3.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m1.2b"><interval closure="open" id="S3.SS3.p2.3.m1.2.3.1.cmml" xref="S3.SS3.p2.3.m1.2.3.2"><ci id="S3.SS3.p2.3.m1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1">𝑖</ci><ci id="S3.SS3.p2.3.m1.2.2.cmml" xref="S3.SS3.p2.3.m1.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m1.2c">(i,j)</annotation></semantics></math> is the kernel coordinate; <math id="S3.SS3.p2.4.m2.1" class="ltx_Math" alttext="\omega_{c}" display="inline"><semantics id="S3.SS3.p2.4.m2.1a"><msub id="S3.SS3.p2.4.m2.1.1" xref="S3.SS3.p2.4.m2.1.1.cmml"><mi id="S3.SS3.p2.4.m2.1.1.2" xref="S3.SS3.p2.4.m2.1.1.2.cmml">ω</mi><mi id="S3.SS3.p2.4.m2.1.1.3" xref="S3.SS3.p2.4.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m2.1b"><apply id="S3.SS3.p2.4.m2.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m2.1.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m2.1.1.2.cmml" xref="S3.SS3.p2.4.m2.1.1.2">𝜔</ci><ci id="S3.SS3.p2.4.m2.1.1.3.cmml" xref="S3.SS3.p2.4.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m2.1c">\omega_{c}</annotation></semantics></math> is the cutoff frequency; and <math id="S3.SS3.p2.5.m3.1" class="ltx_Math" alttext="J_{1}" display="inline"><semantics id="S3.SS3.p2.5.m3.1a"><msub id="S3.SS3.p2.5.m3.1.1" xref="S3.SS3.p2.5.m3.1.1.cmml"><mi id="S3.SS3.p2.5.m3.1.1.2" xref="S3.SS3.p2.5.m3.1.1.2.cmml">J</mi><mn id="S3.SS3.p2.5.m3.1.1.3" xref="S3.SS3.p2.5.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m3.1b"><apply id="S3.SS3.p2.5.m3.1.1.cmml" xref="S3.SS3.p2.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m3.1.1.1.cmml" xref="S3.SS3.p2.5.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m3.1.1.2.cmml" xref="S3.SS3.p2.5.m3.1.1.2">𝐽</ci><cn type="integer" id="S3.SS3.p2.5.m3.1.1.3.cmml" xref="S3.SS3.p2.5.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m3.1c">J_{1}</annotation></semantics></math> is
the first order Bessel function of the first kind.
Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 High-order Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (Bottom) shows <math id="S3.SS3.p2.6.m4.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.SS3.p2.6.m4.1a"><mrow id="S3.SS3.p2.6.m4.1.1" xref="S3.SS3.p2.6.m4.1.1.cmml"><mi id="S3.SS3.p2.6.m4.1.1.2" xref="S3.SS3.p2.6.m4.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m4.1.1.1" xref="S3.SS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.6.m4.1.1.3" xref="S3.SS3.p2.6.m4.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m4.1.1.1a" xref="S3.SS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.6.m4.1.1.4" xref="S3.SS3.p2.6.m4.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m4.1.1.1b" xref="S3.SS3.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.6.m4.1.1.5" xref="S3.SS3.p2.6.m4.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m4.1b"><apply id="S3.SS3.p2.6.m4.1.1.cmml" xref="S3.SS3.p2.6.m4.1.1"><times id="S3.SS3.p2.6.m4.1.1.1.cmml" xref="S3.SS3.p2.6.m4.1.1.1"></times><ci id="S3.SS3.p2.6.m4.1.1.2.cmml" xref="S3.SS3.p2.6.m4.1.1.2">𝑠</ci><ci id="S3.SS3.p2.6.m4.1.1.3.cmml" xref="S3.SS3.p2.6.m4.1.1.3">𝑖</ci><ci id="S3.SS3.p2.6.m4.1.1.4.cmml" xref="S3.SS3.p2.6.m4.1.1.4">𝑛</ci><ci id="S3.SS3.p2.6.m4.1.1.5.cmml" xref="S3.SS3.p2.6.m4.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m4.1c">sinc</annotation></semantics></math> filters with different cutoff frequencies, and their corresponding filtered images. It is observed that it could well synthesize ringing and overshoot artifacts (especially introduced by over-sharp effects). These artifacts are visually similar to those in the first two real samples in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 High-order Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (Top).</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2107.10833/assets/x6.png" id="S3.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.4.2" class="ltx_text" style="font-size:90%;">Architecture of the U-Net discriminator with spectral normalization.</span></figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p">We adopt <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.1a" xref="S3.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.1.m1.1.1.4" xref="S3.SS3.p3.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.1b" xref="S3.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.1.m1.1.1.5" xref="S3.SS3.p3.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝑠</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝑖</ci><ci id="S3.SS3.p3.1.m1.1.1.4.cmml" xref="S3.SS3.p3.1.m1.1.1.4">𝑛</ci><ci id="S3.SS3.p3.1.m1.1.1.5.cmml" xref="S3.SS3.p3.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">sinc</annotation></semantics></math> filters in two places: the blurring process and the last step of the synthesis.
The order of the last <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.1a" xref="S3.SS3.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.1.1.4" xref="S3.SS3.p3.2.m2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.1b" xref="S3.SS3.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.1.1.5" xref="S3.SS3.p3.2.m2.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><times id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1"></times><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑠</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑖</ci><ci id="S3.SS3.p3.2.m2.1.1.4.cmml" xref="S3.SS3.p3.2.m2.1.1.4">𝑛</ci><ci id="S3.SS3.p3.2.m2.1.1.5.cmml" xref="S3.SS3.p3.2.m2.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">sinc</annotation></semantics></math> filter and JPEG compression is randomly exchanged to cover a larger degradation space, as some images may be first over-sharpened (with overshoot artifacts) and then have JPEG compression; while some images may first do JPEG compression followed by sharpening operation.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Networks and Training</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.3" class="ltx_p"><span id="S3.SS4.p1.3.1" class="ltx_text ltx_font_bold">ESRGAN generator</span>. We adopt the same generator (SR network) as ESRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, <em id="S3.SS4.p1.3.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS4.p1.3.3" class="ltx_text"></span>, a deep network with several residual-in-residual dense blocks (RRDB), as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 High-order Degradation Model ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
We also extend the original <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\times 4" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\times 4</annotation></semantics></math> ESRGAN architecture to perform super-resolution with a scale factor of <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\times 2" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.2.m2.1.1.1" xref="S3.SS4.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><times id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></times><csymbol cd="latexml" id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\times 2</annotation></semantics></math> and <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\times 1" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mrow id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.3.m3.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><times id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1"></times><csymbol cd="latexml" id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">absent</csymbol><cn type="integer" id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\times 1</annotation></semantics></math>. As ESRGAN is a heavy network, we first employ the pixel-unshuffle (an inverse operation of pixel-shuffle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>) to reduce the spatial size and enlarge the channel size before feeding inputs into the main ESRGAN architecture. Thus, the most calculation is performed in a smaller resolution space, which can reduce the GPU memory and computational resources consumption.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">U-Net discriminator with spectral normalization (SN).</span>
As Real-ESRGAN aims to address a much larger degradation space than ESRGAN, the original design of discriminator in ESRGAN is no longer suitable.
Specifically, the discriminator in Real-ESRGAN requires a greater discriminative power for complex training outputs.
Instead of discriminating global styles, it also needs to produce accurate gradient feedback for local textures.
Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, we also improve the VGG-style discriminator in ESRGAN to an U-Net design with skip connections (Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.3 Ringing and overshoot artifacts ‣ 3 Methodology ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The U-Net outputs realness values for each pixel, and can provide detailed per-pixel feedback to the generator.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">In the meanwhile, the U-Net structure and complicate degradations also increase the training instability. We employ the spectral normalization regularization  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to stabilize the training dynamics. Moreover, we observe that spectral normalization is also beneficial to alleviate the over-sharp and annoying artifacts introduced by GAN training.
With those adjustments, we are able to easily train the Real-ESRGAN and achieve a good balance of local detail enhancement and artifact suppression.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">The training process</span> is divided into two stages. First, we train a PSNR-oriented model with the L1 loss.
The obtained model is named by <span id="S3.SS4.p4.1.2" class="ltx_text ltx_font_italic">Real-ESRNet</span>.
We then use the trained PSNR-oriented model as an initialization of the generator, and train the <span id="S3.SS4.p4.1.3" class="ltx_text ltx_font_italic">Real-ESRGAN</span> with a combination of L1 loss, perceptual loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and GAN loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Implementation</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.7" class="ltx_p"><span id="S4.SS1.p1.7.1" class="ltx_text ltx_font_bold">Training details.</span>
Similar to ESRGAN, we adopt DIV2K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, Flickr2K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and OutdoorSceneTraining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> datasets for training.
The training HR patch size is set to 256. We train our models with four NVIDIA V100 GPUs with a total batch size of 48.
We employ Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Real-ESRNet is finetuned from ESRGAN for faster convergence. We train Real-ESRNet for <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="1000K" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">1000</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">1000</cn><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">1000K</annotation></semantics></math> iterations with learning rate <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="2\times 10^{-4}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml"><mn id="S4.SS1.p1.2.m2.1.1.3.2" xref="S4.SS1.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p1.2.m2.1.1.3.3" xref="S4.SS1.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p1.2.m2.1.1.3.3a" xref="S4.SS1.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p1.2.m2.1.1.3.3.2" xref="S4.SS1.p1.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><times id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">2</cn><apply id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3"><minus id="S4.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">2\times 10^{-4}</annotation></semantics></math> while training Real-ESRGAN for <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="400K" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mn id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><times id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">400</cn><ci id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">400K</annotation></semantics></math> iterations with learning rate <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">×</mo><msup id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mn id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml"><mo id="S4.SS1.p1.4.m4.1.1.3.3a" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p1.4.m4.1.1.3.3.2" xref="S4.SS1.p1.4.m4.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">1</cn><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.2">10</cn><apply id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3"><minus id="S4.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">1\times 10^{-4}</annotation></semantics></math>.
We adopt exponential moving average (EMA) for more stable training and better performance.
Real-ESRGAN is trained with a combination of L1 loss, perceptual loss and GAN loss, with weights <math id="S4.SS1.p1.5.m5.3" class="ltx_Math" alttext="\{1,1,0.1\}" display="inline"><semantics id="S4.SS1.p1.5.m5.3a"><mrow id="S4.SS1.p1.5.m5.3.4.2" xref="S4.SS1.p1.5.m5.3.4.1.cmml"><mo stretchy="false" id="S4.SS1.p1.5.m5.3.4.2.1" xref="S4.SS1.p1.5.m5.3.4.1.cmml">{</mo><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">1</mn><mo id="S4.SS1.p1.5.m5.3.4.2.2" xref="S4.SS1.p1.5.m5.3.4.1.cmml">,</mo><mn id="S4.SS1.p1.5.m5.2.2" xref="S4.SS1.p1.5.m5.2.2.cmml">1</mn><mo id="S4.SS1.p1.5.m5.3.4.2.3" xref="S4.SS1.p1.5.m5.3.4.1.cmml">,</mo><mn id="S4.SS1.p1.5.m5.3.3" xref="S4.SS1.p1.5.m5.3.3.cmml">0.1</mn><mo stretchy="false" id="S4.SS1.p1.5.m5.3.4.2.4" xref="S4.SS1.p1.5.m5.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.3b"><set id="S4.SS1.p1.5.m5.3.4.1.cmml" xref="S4.SS1.p1.5.m5.3.4.2"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">1</cn><cn type="integer" id="S4.SS1.p1.5.m5.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2">1</cn><cn type="float" id="S4.SS1.p1.5.m5.3.3.cmml" xref="S4.SS1.p1.5.m5.3.3">0.1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.3c">\{1,1,0.1\}</annotation></semantics></math>, respectively.
We use the <math id="S4.SS1.p1.6.m6.2" class="ltx_Math" alttext="\{\mathtt{conv1},...\mathtt{conv5}\}" display="inline"><semantics id="S4.SS1.p1.6.m6.2a"><mrow id="S4.SS1.p1.6.m6.2.2.1" xref="S4.SS1.p1.6.m6.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.p1.6.m6.2.2.1.2" xref="S4.SS1.p1.6.m6.2.2.2.cmml">{</mo><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">𝚌𝚘𝚗𝚟𝟷</mi><mo id="S4.SS1.p1.6.m6.2.2.1.3" xref="S4.SS1.p1.6.m6.2.2.2.cmml">,</mo><mrow id="S4.SS1.p1.6.m6.2.2.1.1" xref="S4.SS1.p1.6.m6.2.2.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.p1.6.m6.2.2.1.1.2" xref="S4.SS1.p1.6.m6.2.2.1.1.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.2.2.1.1.1" xref="S4.SS1.p1.6.m6.2.2.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.2.2.1.1.3" xref="S4.SS1.p1.6.m6.2.2.1.1.3.cmml">𝚌𝚘𝚗𝚟𝟻</mi></mrow><mo stretchy="false" id="S4.SS1.p1.6.m6.2.2.1.4" xref="S4.SS1.p1.6.m6.2.2.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.2b"><set id="S4.SS1.p1.6.m6.2.2.2.cmml" xref="S4.SS1.p1.6.m6.2.2.1"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">𝚌𝚘𝚗𝚟𝟷</ci><apply id="S4.SS1.p1.6.m6.2.2.1.1.cmml" xref="S4.SS1.p1.6.m6.2.2.1.1"><times id="S4.SS1.p1.6.m6.2.2.1.1.1.cmml" xref="S4.SS1.p1.6.m6.2.2.1.1.1"></times><ci id="S4.SS1.p1.6.m6.2.2.1.1.2.cmml" xref="S4.SS1.p1.6.m6.2.2.1.1.2">…</ci><ci id="S4.SS1.p1.6.m6.2.2.1.1.3.cmml" xref="S4.SS1.p1.6.m6.2.2.1.1.3">𝚌𝚘𝚗𝚟𝟻</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.2c">\{\mathtt{conv1},...\mathtt{conv5}\}</annotation></semantics></math> feature maps (with weights <math id="S4.SS1.p1.7.m7.5" class="ltx_Math" alttext="\{0.1,0.1,1,1,1\}" display="inline"><semantics id="S4.SS1.p1.7.m7.5a"><mrow id="S4.SS1.p1.7.m7.5.6.2" xref="S4.SS1.p1.7.m7.5.6.1.cmml"><mo stretchy="false" id="S4.SS1.p1.7.m7.5.6.2.1" xref="S4.SS1.p1.7.m7.5.6.1.cmml">{</mo><mn id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">0.1</mn><mo id="S4.SS1.p1.7.m7.5.6.2.2" xref="S4.SS1.p1.7.m7.5.6.1.cmml">,</mo><mn id="S4.SS1.p1.7.m7.2.2" xref="S4.SS1.p1.7.m7.2.2.cmml">0.1</mn><mo id="S4.SS1.p1.7.m7.5.6.2.3" xref="S4.SS1.p1.7.m7.5.6.1.cmml">,</mo><mn id="S4.SS1.p1.7.m7.3.3" xref="S4.SS1.p1.7.m7.3.3.cmml">1</mn><mo id="S4.SS1.p1.7.m7.5.6.2.4" xref="S4.SS1.p1.7.m7.5.6.1.cmml">,</mo><mn id="S4.SS1.p1.7.m7.4.4" xref="S4.SS1.p1.7.m7.4.4.cmml">1</mn><mo id="S4.SS1.p1.7.m7.5.6.2.5" xref="S4.SS1.p1.7.m7.5.6.1.cmml">,</mo><mn id="S4.SS1.p1.7.m7.5.5" xref="S4.SS1.p1.7.m7.5.5.cmml">1</mn><mo stretchy="false" id="S4.SS1.p1.7.m7.5.6.2.6" xref="S4.SS1.p1.7.m7.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.5b"><set id="S4.SS1.p1.7.m7.5.6.1.cmml" xref="S4.SS1.p1.7.m7.5.6.2"><cn type="float" id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">0.1</cn><cn type="float" id="S4.SS1.p1.7.m7.2.2.cmml" xref="S4.SS1.p1.7.m7.2.2">0.1</cn><cn type="integer" id="S4.SS1.p1.7.m7.3.3.cmml" xref="S4.SS1.p1.7.m7.3.3">1</cn><cn type="integer" id="S4.SS1.p1.7.m7.4.4.cmml" xref="S4.SS1.p1.7.m7.4.4">1</cn><cn type="integer" id="S4.SS1.p1.7.m7.5.5.cmml" xref="S4.SS1.p1.7.m7.5.5">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.5c">\{0.1,0.1,1,1,1\}</annotation></semantics></math>) before activation in the pre-trained VGG19 network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as the perceptual loss. Our implementation is based on the BasicSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.9" class="ltx_p"><span id="S4.SS1.p2.9.1" class="ltx_text ltx_font_bold">Degradation details.</span>
We employ a second-order degradation model for a good balance of simplicity and effectiveness.
Unless otherwise specified, the two degradation processes have the same settings.
We adopt Gaussian kernels, generalized Gaussian kernels and plateau-shaped kernels, with a probability of <math id="S4.SS1.p2.1.m1.3" class="ltx_Math" alttext="\{0.7,0.15,0.15\}" display="inline"><semantics id="S4.SS1.p2.1.m1.3a"><mrow id="S4.SS1.p2.1.m1.3.4.2" xref="S4.SS1.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S4.SS1.p2.1.m1.3.4.2.1" xref="S4.SS1.p2.1.m1.3.4.1.cmml">{</mo><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">0.7</mn><mo id="S4.SS1.p2.1.m1.3.4.2.2" xref="S4.SS1.p2.1.m1.3.4.1.cmml">,</mo><mn id="S4.SS1.p2.1.m1.2.2" xref="S4.SS1.p2.1.m1.2.2.cmml">0.15</mn><mo id="S4.SS1.p2.1.m1.3.4.2.3" xref="S4.SS1.p2.1.m1.3.4.1.cmml">,</mo><mn id="S4.SS1.p2.1.m1.3.3" xref="S4.SS1.p2.1.m1.3.3.cmml">0.15</mn><mo stretchy="false" id="S4.SS1.p2.1.m1.3.4.2.4" xref="S4.SS1.p2.1.m1.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.3b"><set id="S4.SS1.p2.1.m1.3.4.1.cmml" xref="S4.SS1.p2.1.m1.3.4.2"><cn type="float" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">0.7</cn><cn type="float" id="S4.SS1.p2.1.m1.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2">0.15</cn><cn type="float" id="S4.SS1.p2.1.m1.3.3.cmml" xref="S4.SS1.p2.1.m1.3.3">0.15</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.3c">\{0.7,0.15,0.15\}</annotation></semantics></math>. The blur kernel size is randomly selected from <math id="S4.SS1.p2.2.m2.2" class="ltx_math_unparsed" alttext="\{7,9,...21" display="inline"><semantics id="S4.SS1.p2.2.m2.2a"><mrow id="S4.SS1.p2.2.m2.2b"><mo stretchy="false" id="S4.SS1.p2.2.m2.2.3">{</mo><mn id="S4.SS1.p2.2.m2.1.1">7</mn><mo id="S4.SS1.p2.2.m2.2.4">,</mo><mn id="S4.SS1.p2.2.m2.2.2">9</mn><mo id="S4.SS1.p2.2.m2.2.5">,</mo><mi mathvariant="normal" id="S4.SS1.p2.2.m2.2.6">…</mi><mn id="S4.SS1.p2.2.m2.2.7">21</mn></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.2c">\{7,9,...21</annotation></semantics></math>}. Blur standard deviation <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\sigma</annotation></semantics></math> is sampled from <math id="S4.SS1.p2.4.m4.2" class="ltx_Math" alttext="[0.2,3]" display="inline"><semantics id="S4.SS1.p2.4.m4.2a"><mrow id="S4.SS1.p2.4.m4.2.3.2" xref="S4.SS1.p2.4.m4.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.4.m4.2.3.2.1" xref="S4.SS1.p2.4.m4.2.3.1.cmml">[</mo><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">0.2</mn><mo id="S4.SS1.p2.4.m4.2.3.2.2" xref="S4.SS1.p2.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.4.m4.2.2" xref="S4.SS1.p2.4.m4.2.2.cmml">3</mn><mo stretchy="false" id="S4.SS1.p2.4.m4.2.3.2.3" xref="S4.SS1.p2.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.2b"><interval closure="closed" id="S4.SS1.p2.4.m4.2.3.1.cmml" xref="S4.SS1.p2.4.m4.2.3.2"><cn type="float" id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">0.2</cn><cn type="integer" id="S4.SS1.p2.4.m4.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2">3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.2c">[0.2,3]</annotation></semantics></math> (<math id="S4.SS1.p2.5.m5.2" class="ltx_Math" alttext="[0.2,1.5]" display="inline"><semantics id="S4.SS1.p2.5.m5.2a"><mrow id="S4.SS1.p2.5.m5.2.3.2" xref="S4.SS1.p2.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.5.m5.2.3.2.1" xref="S4.SS1.p2.5.m5.2.3.1.cmml">[</mo><mn id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">0.2</mn><mo id="S4.SS1.p2.5.m5.2.3.2.2" xref="S4.SS1.p2.5.m5.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.5.m5.2.2" xref="S4.SS1.p2.5.m5.2.2.cmml">1.5</mn><mo stretchy="false" id="S4.SS1.p2.5.m5.2.3.2.3" xref="S4.SS1.p2.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.2b"><interval closure="closed" id="S4.SS1.p2.5.m5.2.3.1.cmml" xref="S4.SS1.p2.5.m5.2.3.2"><cn type="float" id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">0.2</cn><cn type="float" id="S4.SS1.p2.5.m5.2.2.cmml" xref="S4.SS1.p2.5.m5.2.2">1.5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.2c">[0.2,1.5]</annotation></semantics></math> for the second degradation process). Shape parameter <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mi id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><ci id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\beta</annotation></semantics></math> is sampled from <math id="S4.SS1.p2.7.m7.2" class="ltx_Math" alttext="[0.5,4]" display="inline"><semantics id="S4.SS1.p2.7.m7.2a"><mrow id="S4.SS1.p2.7.m7.2.3.2" xref="S4.SS1.p2.7.m7.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.7.m7.2.3.2.1" xref="S4.SS1.p2.7.m7.2.3.1.cmml">[</mo><mn id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml">0.5</mn><mo id="S4.SS1.p2.7.m7.2.3.2.2" xref="S4.SS1.p2.7.m7.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.7.m7.2.2" xref="S4.SS1.p2.7.m7.2.2.cmml">4</mn><mo stretchy="false" id="S4.SS1.p2.7.m7.2.3.2.3" xref="S4.SS1.p2.7.m7.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.2b"><interval closure="closed" id="S4.SS1.p2.7.m7.2.3.1.cmml" xref="S4.SS1.p2.7.m7.2.3.2"><cn type="float" id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1">0.5</cn><cn type="integer" id="S4.SS1.p2.7.m7.2.2.cmml" xref="S4.SS1.p2.7.m7.2.2">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.2c">[0.5,4]</annotation></semantics></math> and <math id="S4.SS1.p2.8.m8.2" class="ltx_Math" alttext="[1,2]" display="inline"><semantics id="S4.SS1.p2.8.m8.2a"><mrow id="S4.SS1.p2.8.m8.2.3.2" xref="S4.SS1.p2.8.m8.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.8.m8.2.3.2.1" xref="S4.SS1.p2.8.m8.2.3.1.cmml">[</mo><mn id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml">1</mn><mo id="S4.SS1.p2.8.m8.2.3.2.2" xref="S4.SS1.p2.8.m8.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.8.m8.2.2" xref="S4.SS1.p2.8.m8.2.2.cmml">2</mn><mo stretchy="false" id="S4.SS1.p2.8.m8.2.3.2.3" xref="S4.SS1.p2.8.m8.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.2b"><interval closure="closed" id="S4.SS1.p2.8.m8.2.3.1.cmml" xref="S4.SS1.p2.8.m8.2.3.2"><cn type="integer" id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1">1</cn><cn type="integer" id="S4.SS1.p2.8.m8.2.2.cmml" xref="S4.SS1.p2.8.m8.2.2">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.2c">[1,2]</annotation></semantics></math> for generalized Gaussian and plateau-shaped kernels, respectively. We also use <math id="S4.SS1.p2.9.m9.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S4.SS1.p2.9.m9.1a"><mrow id="S4.SS1.p2.9.m9.1.1" xref="S4.SS1.p2.9.m9.1.1.cmml"><mi id="S4.SS1.p2.9.m9.1.1.2" xref="S4.SS1.p2.9.m9.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.9.m9.1.1.1" xref="S4.SS1.p2.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS1.p2.9.m9.1.1.3" xref="S4.SS1.p2.9.m9.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.9.m9.1.1.1a" xref="S4.SS1.p2.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS1.p2.9.m9.1.1.4" xref="S4.SS1.p2.9.m9.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.9.m9.1.1.1b" xref="S4.SS1.p2.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS1.p2.9.m9.1.1.5" xref="S4.SS1.p2.9.m9.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.1b"><apply id="S4.SS1.p2.9.m9.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1"><times id="S4.SS1.p2.9.m9.1.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1.1"></times><ci id="S4.SS1.p2.9.m9.1.1.2.cmml" xref="S4.SS1.p2.9.m9.1.1.2">𝑠</ci><ci id="S4.SS1.p2.9.m9.1.1.3.cmml" xref="S4.SS1.p2.9.m9.1.1.3">𝑖</ci><ci id="S4.SS1.p2.9.m9.1.1.4.cmml" xref="S4.SS1.p2.9.m9.1.1.4">𝑛</ci><ci id="S4.SS1.p2.9.m9.1.1.5.cmml" xref="S4.SS1.p2.9.m9.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.1c">sinc</annotation></semantics></math> kernel with a probability of 0.1. We skip the second blur degradation with a probability of 0.2.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.7" class="ltx_p">We employ Gaussian noises and Poisson noises with a probability of <math id="S4.SS1.p3.1.m1.2" class="ltx_Math" alttext="\{0.5,0.5\}" display="inline"><semantics id="S4.SS1.p3.1.m1.2a"><mrow id="S4.SS1.p3.1.m1.2.3.2" xref="S4.SS1.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.2.3.2.1" xref="S4.SS1.p3.1.m1.2.3.1.cmml">{</mo><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">0.5</mn><mo id="S4.SS1.p3.1.m1.2.3.2.2" xref="S4.SS1.p3.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.1.m1.2.2" xref="S4.SS1.p3.1.m1.2.2.cmml">0.5</mn><mo stretchy="false" id="S4.SS1.p3.1.m1.2.3.2.3" xref="S4.SS1.p3.1.m1.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.2b"><set id="S4.SS1.p3.1.m1.2.3.1.cmml" xref="S4.SS1.p3.1.m1.2.3.2"><cn type="float" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">0.5</cn><cn type="float" id="S4.SS1.p3.1.m1.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2">0.5</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.2c">\{0.5,0.5\}</annotation></semantics></math>. The noise sigma range and Poisson noise scale are set to <math id="S4.SS1.p3.2.m2.2" class="ltx_Math" alttext="[1,30]" display="inline"><semantics id="S4.SS1.p3.2.m2.2a"><mrow id="S4.SS1.p3.2.m2.2.3.2" xref="S4.SS1.p3.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.2.m2.2.3.2.1" xref="S4.SS1.p3.2.m2.2.3.1.cmml">[</mo><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">1</mn><mo id="S4.SS1.p3.2.m2.2.3.2.2" xref="S4.SS1.p3.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.2.m2.2.2" xref="S4.SS1.p3.2.m2.2.2.cmml">30</mn><mo stretchy="false" id="S4.SS1.p3.2.m2.2.3.2.3" xref="S4.SS1.p3.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.2b"><interval closure="closed" id="S4.SS1.p3.2.m2.2.3.1.cmml" xref="S4.SS1.p3.2.m2.2.3.2"><cn type="integer" id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">1</cn><cn type="integer" id="S4.SS1.p3.2.m2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2">30</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.2c">[1,30]</annotation></semantics></math> and <math id="S4.SS1.p3.3.m3.2" class="ltx_Math" alttext="[0.05,3]" display="inline"><semantics id="S4.SS1.p3.3.m3.2a"><mrow id="S4.SS1.p3.3.m3.2.3.2" xref="S4.SS1.p3.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.3.m3.2.3.2.1" xref="S4.SS1.p3.3.m3.2.3.1.cmml">[</mo><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">0.05</mn><mo id="S4.SS1.p3.3.m3.2.3.2.2" xref="S4.SS1.p3.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.3.m3.2.2" xref="S4.SS1.p3.3.m3.2.2.cmml">3</mn><mo stretchy="false" id="S4.SS1.p3.3.m3.2.3.2.3" xref="S4.SS1.p3.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.2b"><interval closure="closed" id="S4.SS1.p3.3.m3.2.3.1.cmml" xref="S4.SS1.p3.3.m3.2.3.2"><cn type="float" id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">0.05</cn><cn type="integer" id="S4.SS1.p3.3.m3.2.2.cmml" xref="S4.SS1.p3.3.m3.2.2">3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.2c">[0.05,3]</annotation></semantics></math>, respectively (<math id="S4.SS1.p3.4.m4.2" class="ltx_Math" alttext="[1,25]" display="inline"><semantics id="S4.SS1.p3.4.m4.2a"><mrow id="S4.SS1.p3.4.m4.2.3.2" xref="S4.SS1.p3.4.m4.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.4.m4.2.3.2.1" xref="S4.SS1.p3.4.m4.2.3.1.cmml">[</mo><mn id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">1</mn><mo id="S4.SS1.p3.4.m4.2.3.2.2" xref="S4.SS1.p3.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.4.m4.2.2" xref="S4.SS1.p3.4.m4.2.2.cmml">25</mn><mo stretchy="false" id="S4.SS1.p3.4.m4.2.3.2.3" xref="S4.SS1.p3.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.2b"><interval closure="closed" id="S4.SS1.p3.4.m4.2.3.1.cmml" xref="S4.SS1.p3.4.m4.2.3.2"><cn type="integer" id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">1</cn><cn type="integer" id="S4.SS1.p3.4.m4.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2">25</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.2c">[1,25]</annotation></semantics></math> and <math id="S4.SS1.p3.5.m5.2" class="ltx_Math" alttext="[0.05,2.5]" display="inline"><semantics id="S4.SS1.p3.5.m5.2a"><mrow id="S4.SS1.p3.5.m5.2.3.2" xref="S4.SS1.p3.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.5.m5.2.3.2.1" xref="S4.SS1.p3.5.m5.2.3.1.cmml">[</mo><mn id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">0.05</mn><mo id="S4.SS1.p3.5.m5.2.3.2.2" xref="S4.SS1.p3.5.m5.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.5.m5.2.2" xref="S4.SS1.p3.5.m5.2.2.cmml">2.5</mn><mo stretchy="false" id="S4.SS1.p3.5.m5.2.3.2.3" xref="S4.SS1.p3.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.2b"><interval closure="closed" id="S4.SS1.p3.5.m5.2.3.1.cmml" xref="S4.SS1.p3.5.m5.2.3.2"><cn type="float" id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">0.05</cn><cn type="float" id="S4.SS1.p3.5.m5.2.2.cmml" xref="S4.SS1.p3.5.m5.2.2">2.5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.2c">[0.05,2.5]</annotation></semantics></math> for the second degradation process).
The gray noise probability is set to 0.4.
JPEG compression quality factor is set to <math id="S4.SS1.p3.6.m6.2" class="ltx_Math" alttext="[30,95]" display="inline"><semantics id="S4.SS1.p3.6.m6.2a"><mrow id="S4.SS1.p3.6.m6.2.3.2" xref="S4.SS1.p3.6.m6.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p3.6.m6.2.3.2.1" xref="S4.SS1.p3.6.m6.2.3.1.cmml">[</mo><mn id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml">30</mn><mo id="S4.SS1.p3.6.m6.2.3.2.2" xref="S4.SS1.p3.6.m6.2.3.1.cmml">,</mo><mn id="S4.SS1.p3.6.m6.2.2" xref="S4.SS1.p3.6.m6.2.2.cmml">95</mn><mo stretchy="false" id="S4.SS1.p3.6.m6.2.3.2.3" xref="S4.SS1.p3.6.m6.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.2b"><interval closure="closed" id="S4.SS1.p3.6.m6.2.3.1.cmml" xref="S4.SS1.p3.6.m6.2.3.2"><cn type="integer" id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">30</cn><cn type="integer" id="S4.SS1.p3.6.m6.2.2.cmml" xref="S4.SS1.p3.6.m6.2.2">95</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.2c">[30,95]</annotation></semantics></math>.
The final <math id="S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S4.SS1.p3.7.m7.1a"><mrow id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mi id="S4.SS1.p3.7.m7.1.1.2" xref="S4.SS1.p3.7.m7.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.7.m7.1.1.1" xref="S4.SS1.p3.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS1.p3.7.m7.1.1.3" xref="S4.SS1.p3.7.m7.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.7.m7.1.1.1a" xref="S4.SS1.p3.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS1.p3.7.m7.1.1.4" xref="S4.SS1.p3.7.m7.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.7.m7.1.1.1b" xref="S4.SS1.p3.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS1.p3.7.m7.1.1.5" xref="S4.SS1.p3.7.m7.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><times id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1.1"></times><ci id="S4.SS1.p3.7.m7.1.1.2.cmml" xref="S4.SS1.p3.7.m7.1.1.2">𝑠</ci><ci id="S4.SS1.p3.7.m7.1.1.3.cmml" xref="S4.SS1.p3.7.m7.1.1.3">𝑖</ci><ci id="S4.SS1.p3.7.m7.1.1.4.cmml" xref="S4.SS1.p3.7.m7.1.1.4">𝑛</ci><ci id="S4.SS1.p3.7.m7.1.1.5.cmml" xref="S4.SS1.p3.7.m7.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">sinc</annotation></semantics></math> filter is applied with a probability of 0.8.
More details can be found in the <a target="_blank" href="https://github.com/xinntao/Real-ESRGAN" title="" class="ltx_ref ltx_href">released codes</a>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Training pair pool.</span>
In order to improve the training efficiency, all degradation processes are implemented in PyTorch with CUDA acceleration, so that we are able to synthesize training pairs on the fly.
However, batch processing limits the diversity of synthetic degradations in a batch. For example, samples in a batch could not have different resize scaling factors.
Therefore, we employ a training pair pool to increase the degradation diversity in a batch.
At each iteration, the training samples are randomly selected from the training pair poor to form a training batch.
We set the pool size to 180 in our implementation.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Sharpen ground-truth images during training</span>.
We further show a training trick to visually improve the sharpness, while not introducing visible artifacts.
A typical way of sharpening images is to employ a post-process algorithm, such as unsharp masking (USM).
However, this algorithm tends to introduce overshoot artifacts.
We empirically find that sharpening ground-truth images during training could achieve a better balance of sharpness and overshoot artifact suppression.
We denote the model trained with sharped ground-truth images as Real-ESRGAN<span id="S4.SS1.p5.1.2" class="ltx_text ltx_font_bold">+</span> (comparisons are shown in Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.2 Comparisons with Prior Works ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparisons with Prior Works</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare our Real-ESRGAN with several state-of-the-art methods, including ESRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, DAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, CDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and BSRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
We test on several diverse testing datasets with real-world images, including RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, DRealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, OST300 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, DPED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, ADE20K validation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and images from Internet.
Since existing metrics for perceptual quality cannot well reflect the actual human perceptual preferences on the fine-grained scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, we present several representative visual samples in Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.2 Comparisons with Prior Works ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
The quantitative results are also included in the Appendix. <a href="#A2" title="Appendix B Quantitative Comparisons ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for reference.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">It can be observed from Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.2 Comparisons with Prior Works ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> that our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness.
Specifically, the first sample contains overshoot artifacts (white edges around letters). Directly upsampling will inevitably amplify those artifacts (<em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.p2.1.2" class="ltx_text"></span>, DAN and BSRGAN). Real-ESRGAN takes such common artifacts into consideration and simulates them with <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1a" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.4" xref="S4.SS2.p2.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1b" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.5" xref="S4.SS2.p2.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑠</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝑖</ci><ci id="S4.SS2.p2.1.m1.1.1.4.cmml" xref="S4.SS2.p2.1.m1.1.1.4">𝑛</ci><ci id="S4.SS2.p2.1.m1.1.1.5.cmml" xref="S4.SS2.p2.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">sinc</annotation></semantics></math> filter, thus effectively removing ringing and overshoot artifacts.
The second sample contains unknown and complicated degradations. Most algorithms can not effectively eliminate them while Real-ESRGAN trained with second-order degradation processes could.
Real-ESRGAN is also capable of restoring more realistic textures (<em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.p2.1.4" class="ltx_text"></span>, brick, mountain and tree textures) for real-world samples, while other methods either fail to remove degradations or add unnatural textures (<em id="S4.SS2.p2.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.p2.1.6" class="ltx_text"></span>, RealSR and BSRGAN).</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2107.10833/assets/x7.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="548" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.5.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.6.2" class="ltx_text" style="font-size:90%;">Qualitative comparisons on several representative real-world samples with upsampling scale factor of 4. Our <span id="S4.F7.6.2.1" class="ltx_text">Real-ESRGAN</span> outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness.
Other methods may either fail to remove overshoot (the 1st sample) and complicated artifacts (the 2nd sample), or fail to restore realistic and natural textures for various scenes (the 3rd, 4th, 5th samples).
(<span id="S4.F7.6.2.2" class="ltx_text ltx_font_bold">Zoom in for best view</span>)</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Second-order degradation model</span>.
We conduct ablation studies of degradations on Real-ESRNet, as it is more controllable and can better reflect the influence of degradations.
We replace the second-order process in Real-ESRNet with the classical degradation model to generate training pairs.
As shown in Fig. <a href="#S4.F8" title="Figure 8 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (Top), models trained with classical first-order degradation model cannot effectively remove noise on the wall or blur in the wheat field, while Real-ESRNet can handle these cases.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.2" class="ltx_p"><span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_bold ltx_font_italic">sinc<span id="S4.SS3.p2.2.1.1" class="ltx_text ltx_font_upright"> filters</span></span>.
If <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1a" xref="S4.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.4" xref="S4.SS3.p2.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1b" xref="S4.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.5" xref="S4.SS3.p2.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝑠</ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑖</ci><ci id="S4.SS3.p2.1.m1.1.1.4.cmml" xref="S4.SS3.p2.1.m1.1.1.4">𝑛</ci><ci id="S4.SS3.p2.1.m1.1.1.5.cmml" xref="S4.SS3.p2.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">sinc</annotation></semantics></math> filters are not employed during training, the restored results will amplify the ringing and overshoot artifacts that existed in the input images, as shown in Fig. <a href="#S4.F8" title="Figure 8 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (Bottom), especially around the text and lines. In contrast, models trained with <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m2.1.1.1a" xref="S4.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.2.m2.1.1.4" xref="S4.SS3.p2.2.m2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m2.1.1.1b" xref="S4.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.2.m2.1.1.5" xref="S4.SS3.p2.2.m2.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><times id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></times><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝑠</ci><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝑖</ci><ci id="S4.SS3.p2.2.m2.1.1.4.cmml" xref="S4.SS3.p2.2.m2.1.1.4">𝑛</ci><ci id="S4.SS3.p2.2.m2.1.1.5.cmml" xref="S4.SS3.p2.2.m2.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">sinc</annotation></semantics></math> filters can remove those artifacts.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">U-Net discriminator with SN regularization</span>.
We first employ the ESRGAN setting including the VGG-style discriminator and its loss weights. However, we can observe from Fig. <a href="#S4.F9" title="Figure 9 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, this model cannot restore detailed textures (bricks and bushes) and even brings unpleasant artifacts in bush branches. Using a U-Net design could improve local details. Yet, it introduces unnatural textures and also increases training instability. SN regularization could improve restored textures while stabilizing training dynamics.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">More complicated blur kernels</span>.
We remove the generalized Gaussian kernel and plateau-shaped kernel in blur synthesis. As shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, on some real samples, the model cannot remove blur and recover sharp edges as Real-ESRGAN do. Nevertheless, on most samples, their differences are marginal, indicating that the widely-used Gaussian kernels with a high-order degradation process can already cover a large real blur space. As we can still observe slightly better performance, we adopt those more complicated blur kernels in Real-ESRGAN.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2107.10833/assets/x8.png" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="233" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.7.2.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Top<span id="S4.F8.3.1.2" class="ltx_text ltx_font_medium">: Real-ESRNet results w/ and w/o second-order degradation process. </span>Bottom<span id="S4.F8.3.1.1" class="ltx_text ltx_font_medium">: Real-ESRNet results w/ and w/o <math id="S4.F8.3.1.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S4.F8.3.1.1.m1.1b"><mrow id="S4.F8.3.1.1.m1.1.1" xref="S4.F8.3.1.1.m1.1.1.cmml"><mi id="S4.F8.3.1.1.m1.1.1.2" xref="S4.F8.3.1.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.F8.3.1.1.m1.1.1.1" xref="S4.F8.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.F8.3.1.1.m1.1.1.3" xref="S4.F8.3.1.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.F8.3.1.1.m1.1.1.1b" xref="S4.F8.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.F8.3.1.1.m1.1.1.4" xref="S4.F8.3.1.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.F8.3.1.1.m1.1.1.1c" xref="S4.F8.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.F8.3.1.1.m1.1.1.5" xref="S4.F8.3.1.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F8.3.1.1.m1.1c"><apply id="S4.F8.3.1.1.m1.1.1.cmml" xref="S4.F8.3.1.1.m1.1.1"><times id="S4.F8.3.1.1.m1.1.1.1.cmml" xref="S4.F8.3.1.1.m1.1.1.1"></times><ci id="S4.F8.3.1.1.m1.1.1.2.cmml" xref="S4.F8.3.1.1.m1.1.1.2">𝑠</ci><ci id="S4.F8.3.1.1.m1.1.1.3.cmml" xref="S4.F8.3.1.1.m1.1.1.3">𝑖</ci><ci id="S4.F8.3.1.1.m1.1.1.4.cmml" xref="S4.F8.3.1.1.m1.1.1.4">𝑛</ci><ci id="S4.F8.3.1.1.m1.1.1.5.cmml" xref="S4.F8.3.1.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F8.3.1.1.m1.1d">sinc</annotation></semantics></math> filters. Zoom in for best view</span></span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2107.10833/assets/x9.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="231" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.4.2" class="ltx_text" style="font-size:90%;">Ablation on the discriminator design. Zoom in for best view</span></figcaption>
</figure>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2107.10833/assets/x10.png" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.4.2" class="ltx_text" style="font-size:90%;">Ablation on using more blur kernels (generalized blur and plateau-shaped kernels). Zoom in for best view</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Limitations</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Though Real-ESRGAN is able to restore most real-world images, it still has some limitations.
As shown in Fig. <a href="#S4.F11" title="Figure 11 ‣ 4.4 Limitations ‣ 4 Experiments ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>,
1) some restored images (especially building and indoor scenes) have twisted lines due to aliasing issues.
2) GAN training introduces unpleasant artifacts on some samples.
3) It could not remove out-of-distribution complicated degradations in the real world. Even worse, it may amplify these artifacts.
These drawbacks have great impact on the practical application of Real-ESRGAN, which are in urgent need to address in future works.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2107.10833/assets/x11.png" id="S4.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="278" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.4.2" class="ltx_text" style="font-size:90%;">Limitations: 1) twisted lines; 2) unpleasant artifacts caused by GAN training; 3) unknown and out-of-distribution degradations. Zoom in for best view</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we train the practical Real-ESRGAN for real-world blind super-resolution with pure synthetic training pairs.
In order to synthesize more practical degradations, we propose a high-order degradation process and employ <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="sinc" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1a" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.4" xref="S5.p1.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1b" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.p1.1.m1.1.1.5" xref="S5.p1.1.m1.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝑠</ci><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">𝑖</ci><ci id="S5.p1.1.m1.1.1.4.cmml" xref="S5.p1.1.m1.1.1.4">𝑛</ci><ci id="S5.p1.1.m1.1.1.5.cmml" xref="S5.p1.1.m1.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">sinc</annotation></semantics></math> filters to model common ringing and overshoot artifacts.
We also utilize a U-Net discriminator with spectral normalization regularization to increase discriminator capability and stabilize the training dynamics.
Real-ESRGAN trained with synthetic data is able to enhance details while removing annoying artifacts for most real-world images.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgement.</span> This work is partially supported by National Natural Science Foundation of China (61906184), the Shanghai Committee of Science and Technology, China (Grant No. 21DZ1100800 and 21DZ1100100).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Eirikur Agustsson and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Ntire 2017 challenge on single image super-resolution: Dataset and
study.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Sefi Bell-Kligler, Assaf Shocher, and Michal Irani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Blind super-resolution kernel estimation using an internal-gan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">The 2018 pirm challenge on perceptual image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCVW</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yochai Blau and Tomer Michaeli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">The perception-distortion tradeoff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Toward real-world single image super-resolution: A new benchmark and
a new model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Kelvin C.K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Glean: Generative latent bank for large-factor image
super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Second-order attention network for single image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Learning a deep convolutional network for image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Image super-resolution using deep convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE TPAMI</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 38(2):295–307, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Michael Elad and Arie Feuer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Restoration of a single superresolution image from several blurred,
noisy, and undersampled measured images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on image processing</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 6(12):1646–1658, 1997.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Manuel Fritsche, Shuhang Gu, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Frequency separation for real-world super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCVW</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Daniel Glasner, Shai Bagon, and Michal Irani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Super-resolution from a single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Blind super-resolution with iterative kernel correction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deep backprojection networks for super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc
Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Dslr-quality photos on mobile devices with deep convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Real-world super-resolution via kernel estimation and noise
injection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Perceptual losses for real-time style transfer and super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Accurate image super-resolution using very deep convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Deeply-recursive convolutional network for image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Diederik Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Deep laplacian pyramid networks for fast and accurate
super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew
Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,
Zehan Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Photo-realistic single image super-resolution using a generative
adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew
Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,
Zehan Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Photo-realistic single image super-resolution using a generative
adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Enhanced deep residual networks for single image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiaoand, and Chao Dong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Blind image super-resolution: A survey and beyond.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2107.03055</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Ce Liu and Deqing Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">On bayesian adaptive video super resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">,
36(2):346–360, 2013.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Non-local recurrent network for image restoration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yu-Qi Liu, Xin Du, Hui-Liang Shen, and Shu-Jie Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Estimating generalized gaussian blur kernels for out-of-focus image
deblurring.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on circuits and systems for video technology</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Michael R Lomnitz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Diffjpeg.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/mlomnitz/DiffJPEG" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/mlomnitz/DiffJPEG</a><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Andreas Lugmayr, Martin Danelljan, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning for real-world super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCVW</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, and Tieniu Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Unfolding the alternating optimization for blind super resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Tomer Michaeli and Michal Irani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Nonparametric blind super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Anish Mittal, Rajiv Soundararajan, and Alan C Bovik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Making a completely blind image quality analyzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal Process. Lett.</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 20(3):209–212, 2013.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Spectral normalization for generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita, and Seon Joo Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">A holistic approach to cross-channel image noise modeling and its
application to image denoising.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Medical image computing and
computer-assisted intervention</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Mehdi SM Sajjadi, Bernhard Schölkopf, and Michael Hirsch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Enhancenet: Single image super-resolution through automated texture
synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Edgar Schonfeld, Bernt Schiele, and Anna Khoreva.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">A u-net based discriminator for generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P Aitken,
Rob Bishop, Daniel Rueckert, and Zehan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Real-time single image and video super-resolution using an efficient
sub-pixel convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Richard Shin and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Jpeg-resistant adversarial images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS Workshop on Machine Learning and Computer Security</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Ying Tai, Jian Yang, and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Image super-resolution via deep recursive residual network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee
Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Ntire 2017 challenge on single image super-resolution: Methods and
results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An,
and Yulan Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Unsupervised degradation representation learning for blind
super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Towards real-world blind face restoration with generative facial
prior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Ke Yu, Kelvin C.K. Chan, Chao Dong, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Basicsr.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/xinntao/BasicSR" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/xinntao/BasicSR</a><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Recovering realistic texture in image super-resolution by deep
spatial feature transform.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and
Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Esrgan: Enhanced super-resolution generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCVW</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Pengxu Wei, Ziwei Xie, Hannan Lu, ZongYuan Zhan, Qixiang Ye amd Wangmeng Zuo,
and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Component divide-and-conquer for real-world image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Yitong Yan, Chuangchuang Liu, Changyou Chen, Xianfang Sun, Longcun Jin, Peng
Xinyi, and Xiang Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Fine-grained attention and feature-sharing generative adversarial
networks for single image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Path-restore: Learning network path selection for image restoration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1904.10343</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Unsupervised image super-resolution using cycle-in-cycle generative
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Designing a practical degradation model for deep blind image
super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.14006</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Kai Zhang, Wangmeng Zuo, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Learning a single convolutional super-resolution network for multiple
degradations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Image super-resolution using very deep residual channel attention
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Residual dense network for image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Semantic understanding of scenes through the ade20k dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Ruofan Zhou and Sabine Susstrunk.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Kernel modeling super-resolution on real low-resolution images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para ltx_noindent">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Appendix</span></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of Classical Degradation Model</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we provide more details (especially examples) of each degradation type used in the classical degradation model.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Blur</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Isotropic and anisotropic Gaussian filters are the common choices for blur kernels. We show several Gaussian kernels and their corresponding blurry images in Fig. <a href="#A1.F12" title="Figure 12 ‣ A.1 Blur ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure id="A1.F12" class="ltx_figure"><img src="/html/2107.10833/assets/x12.png" id="A1.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="214" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A1.F12.4.2" class="ltx_text" style="font-size:90%;">Examples of Gaussian kernels (kernel size 21) and their corresponding blurry images. Zoom in for best view</span></figcaption>
</figure>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">To include more diverse kernel shapes, we further adopt generalized Gaussian blur kernels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and a plateau-shaped distribution.
Fig. <a href="#A1.F13" title="Figure 13 ‣ A.1 Blur ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows how the shape parameter <math id="A1.SS1.p2.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A1.SS1.p2.1.m1.1a"><mi id="A1.SS1.p2.1.m1.1.1" xref="A1.SS1.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><ci id="A1.SS1.p2.1.m1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">\beta</annotation></semantics></math> controls kernel shapes.
Empirically, we found that including these blur kernels produces sharper outputs for several real samples.</p>
</div>
<figure id="A1.F13" class="ltx_figure"><img src="/html/2107.10833/assets/x13.png" id="A1.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A1.F13.4.2" class="ltx_text" style="font-size:90%;">Blur kernels with different shape parameters in general Gaussian distribution and plateau-shaped distribution. Zoom in for best view</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Noise</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Fig. <a href="#A1.F14" title="Figure 14 ‣ A.2 Noise ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> depicts the additive Gaussian noise and Poisson noise.
Poisson noise has an intensity proportional to the image intensity, and the noises at different pixels are independent of one another.
As shown in Fig. <a href="#A1.F14" title="Figure 14 ‣ A.2 Noise ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, the Poisson noise has low noise intensity in dark areas.</p>
</div>
<figure id="A1.F14" class="ltx_figure"><img src="/html/2107.10833/assets/x14.png" id="A1.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A1.F14.4.2" class="ltx_text" style="font-size:90%;">Visual comparisons of Gaussian and Poisson noises. Poisson noise has low noise intensity in dark areas. Zoom in for best view</span></figcaption>
</figure>
<figure id="A1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="A1.T1.3.2" class="ltx_text" style="font-size:90%;">NIQE scores on several diverse testing datasets with real-world images. The lower, the better.</span></figcaption>
<div id="A1.T1.4" class="ltx_inline-block ltx_transformed_outer" style="width:496.9pt;height:66.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-286.5pt,38.6pt) scale(0.464371777385773,0.464371777385773) ;">
<table id="A1.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T1.4.1.1.1" class="ltx_tr">
<th id="A1.T1.4.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="A1.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Bicubic</th>
<th id="A1.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ESRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</th>
<th id="A1.T1.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<th id="A1.T1.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="A1.T1.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<th id="A1.T1.4.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">BSRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<th id="A1.T1.4.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T1.4.1.1.1.8.1" class="ltx_text ltx_font_bold">Real-ESRGAN</span></th>
<th id="A1.T1.4.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T1.4.1.1.1.9.1" class="ltx_text ltx_font_bold">Real-ESRGAN+</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T1.4.1.2.1" class="ltx_tr">
<th id="A1.T1.4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">RealSR-Canon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="A1.T1.4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">6.1269</td>
<td id="A1.T1.4.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">6.7715</td>
<td id="A1.T1.4.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">6.5282</td>
<td id="A1.T1.4.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">6.8692</td>
<td id="A1.T1.4.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">6.1488</td>
<td id="A1.T1.4.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">5.7489</td>
<td id="A1.T1.4.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t">4.5899</td>
<td id="A1.T1.4.1.2.1.9" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T1.4.1.2.1.9.1" class="ltx_text ltx_font_bold">4.5314</span></td>
</tr>
<tr id="A1.T1.4.1.3.2" class="ltx_tr">
<th id="A1.T1.4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RealSR-Nikon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="A1.T1.4.1.3.2.2" class="ltx_td ltx_align_left">6.3607</td>
<td id="A1.T1.4.1.3.2.3" class="ltx_td ltx_align_left">6.7480</td>
<td id="A1.T1.4.1.3.2.4" class="ltx_td ltx_align_left">6.6063</td>
<td id="A1.T1.4.1.3.2.5" class="ltx_td ltx_align_left">6.7390</td>
<td id="A1.T1.4.1.3.2.6" class="ltx_td ltx_align_left">6.3265</td>
<td id="A1.T1.4.1.3.2.7" class="ltx_td ltx_align_left">5.9920</td>
<td id="A1.T1.4.1.3.2.8" class="ltx_td ltx_align_left">5.0753</td>
<td id="A1.T1.4.1.3.2.9" class="ltx_td ltx_align_left"><span id="A1.T1.4.1.3.2.9.1" class="ltx_text ltx_font_bold">5.0247</span></td>
</tr>
<tr id="A1.T1.4.1.4.3" class="ltx_tr">
<th id="A1.T1.4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DRealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<td id="A1.T1.4.1.4.3.2" class="ltx_td ltx_align_left">6.5766</td>
<td id="A1.T1.4.1.4.3.3" class="ltx_td ltx_align_left">8.6335</td>
<td id="A1.T1.4.1.4.3.4" class="ltx_td ltx_align_left">7.0720</td>
<td id="A1.T1.4.1.4.3.5" class="ltx_td ltx_align_left">7.7213</td>
<td id="A1.T1.4.1.4.3.6" class="ltx_td ltx_align_left">6.6359</td>
<td id="A1.T1.4.1.4.3.7" class="ltx_td ltx_align_left">6.1362</td>
<td id="A1.T1.4.1.4.3.8" class="ltx_td ltx_align_left">4.9796</td>
<td id="A1.T1.4.1.4.3.9" class="ltx_td ltx_align_left"><span id="A1.T1.4.1.4.3.9.1" class="ltx_text ltx_font_bold">4.8458</span></td>
</tr>
<tr id="A1.T1.4.1.5.4" class="ltx_tr">
<th id="A1.T1.4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DPED-iphone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="A1.T1.4.1.5.4.2" class="ltx_td ltx_align_left">6.0121</td>
<td id="A1.T1.4.1.5.4.3" class="ltx_td ltx_align_left">5.7363</td>
<td id="A1.T1.4.1.5.4.4" class="ltx_td ltx_align_left">6.1414</td>
<td id="A1.T1.4.1.5.4.5" class="ltx_td ltx_align_left">5.5855</td>
<td id="A1.T1.4.1.5.4.6" class="ltx_td ltx_align_left">6.2738</td>
<td id="A1.T1.4.1.5.4.7" class="ltx_td ltx_align_left">5.9906</td>
<td id="A1.T1.4.1.5.4.8" class="ltx_td ltx_align_left">5.4352</td>
<td id="A1.T1.4.1.5.4.9" class="ltx_td ltx_align_left"><span id="A1.T1.4.1.5.4.9.1" class="ltx_text ltx_font_bold">5.2631</span></td>
</tr>
<tr id="A1.T1.4.1.6.5" class="ltx_tr">
<th id="A1.T1.4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OST300 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</th>
<td id="A1.T1.4.1.6.5.2" class="ltx_td ltx_align_left">4.4440</td>
<td id="A1.T1.4.1.6.5.3" class="ltx_td ltx_align_left">3.5245</td>
<td id="A1.T1.4.1.6.5.4" class="ltx_td ltx_align_left">5.0232</td>
<td id="A1.T1.4.1.6.5.5" class="ltx_td ltx_align_left">4.5715</td>
<td id="A1.T1.4.1.6.5.6" class="ltx_td ltx_align_left">4.7441</td>
<td id="A1.T1.4.1.6.5.7" class="ltx_td ltx_align_left">4.1662</td>
<td id="A1.T1.4.1.6.5.8" class="ltx_td ltx_align_left">2.8659</td>
<td id="A1.T1.4.1.6.5.9" class="ltx_td ltx_align_left"><span id="A1.T1.4.1.6.5.9.1" class="ltx_text ltx_font_bold">2.8191</span></td>
</tr>
<tr id="A1.T1.4.1.7.6" class="ltx_tr">
<th id="A1.T1.4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ImageNet val <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="A1.T1.4.1.7.6.2" class="ltx_td ltx_align_left">7.4985</td>
<td id="A1.T1.4.1.7.6.3" class="ltx_td ltx_align_left"><span id="A1.T1.4.1.7.6.3.1" class="ltx_text ltx_font_bold">3.6474</span></td>
<td id="A1.T1.4.1.7.6.4" class="ltx_td ltx_align_left">6.0932</td>
<td id="A1.T1.4.1.7.6.5" class="ltx_td ltx_align_left">3.8303</td>
<td id="A1.T1.4.1.7.6.6" class="ltx_td ltx_align_left">7.0441</td>
<td id="A1.T1.4.1.7.6.7" class="ltx_td ltx_align_left">4.3528</td>
<td id="A1.T1.4.1.7.6.8" class="ltx_td ltx_align_left">4.8580</td>
<td id="A1.T1.4.1.7.6.9" class="ltx_td ltx_align_left">4.6448</td>
</tr>
<tr id="A1.T1.4.1.8.7" class="ltx_tr">
<th id="A1.T1.4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">ADE20K val <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</th>
<td id="A1.T1.4.1.8.7.2" class="ltx_td ltx_align_left ltx_border_b">7.5239</td>
<td id="A1.T1.4.1.8.7.3" class="ltx_td ltx_align_left ltx_border_b">3.6905</td>
<td id="A1.T1.4.1.8.7.4" class="ltx_td ltx_align_left ltx_border_b">6.3839</td>
<td id="A1.T1.4.1.8.7.5" class="ltx_td ltx_align_left ltx_border_b"><span id="A1.T1.4.1.8.7.5.1" class="ltx_text ltx_font_bold">3.4102</span></td>
<td id="A1.T1.4.1.8.7.6" class="ltx_td ltx_align_left ltx_border_b">6.9219</td>
<td id="A1.T1.4.1.8.7.7" class="ltx_td ltx_align_left ltx_border_b">3.9434</td>
<td id="A1.T1.4.1.8.7.8" class="ltx_td ltx_align_left ltx_border_b">3.7886</td>
<td id="A1.T1.4.1.8.7.9" class="ltx_td ltx_align_left ltx_border_b">3.5778</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Resize</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">There are several resize algorithms.
We compare the following resize operations: nearest-neighbor interpolation, area resize, bilinear interpolation and bicubic interpolation.
We examine the different effects of these resize operations. We first downsample an image by a scale factor of four and then upsample to its original size. Different downsampling and upsampling algorithms are performed, and the results of different combinations are shown in Fig. <a href="#A1.F15" title="Figure 15 ‣ A.3 Resize ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>.
It is observed that different resize operations result in very different effects - some produce blurry results while some may output over-sharp images with overshoot artifacts.</p>
</div>
<figure id="A1.F15" class="ltx_figure"><img src="/html/2107.10833/assets/x15.png" id="A1.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="460" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A1.F15.4.2" class="ltx_text" style="font-size:90%;">Effects of different combinations of down- and up-sampling algorithms. The images are first downsampled by a scale factor of four and then upsampled to its original size. Zoom in for best view</span></figcaption>
</figure>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>JPEG compression</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.4" class="ltx_p">We use the PyTorch implementation - <math id="A1.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathtt{DiffJPEG}" display="inline"><semantics id="A1.SS4.p1.1.m1.1a"><mi id="A1.SS4.p1.1.m1.1.1" xref="A1.SS4.p1.1.m1.1.1.cmml">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.1.m1.1b"><ci id="A1.SS4.p1.1.m1.1.1.cmml" xref="A1.SS4.p1.1.m1.1.1">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.1.m1.1c">\mathtt{DiffJPEG}</annotation></semantics></math>.
We observe that the compressed images by <math id="A1.SS4.p1.2.m2.1" class="ltx_Math" alttext="\mathtt{DiffJPEG}" display="inline"><semantics id="A1.SS4.p1.2.m2.1a"><mi id="A1.SS4.p1.2.m2.1.1" xref="A1.SS4.p1.2.m2.1.1.cmml">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.2.m2.1b"><ci id="A1.SS4.p1.2.m2.1.1.cmml" xref="A1.SS4.p1.2.m2.1.1">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.2.m2.1c">\mathtt{DiffJPEG}</annotation></semantics></math> are a bit different from those compressed by the <math id="A1.SS4.p1.3.m3.1" class="ltx_Math" alttext="\mathtt{cv2}" display="inline"><semantics id="A1.SS4.p1.3.m3.1a"><mi id="A1.SS4.p1.3.m3.1.1" xref="A1.SS4.p1.3.m3.1.1.cmml">𝚌𝚟𝟸</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.3.m3.1b"><ci id="A1.SS4.p1.3.m3.1.1.cmml" xref="A1.SS4.p1.3.m3.1.1">𝚌𝚟𝟸</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.3.m3.1c">\mathtt{cv2}</annotation></semantics></math> package. Fig. <a href="#A1.F16" title="Figure 16 ‣ A.4 JPEG compression ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows the typical JPEG compression artifacts and the difference caused by using different packages.
Such a difference may bring an extra gap between synthetic and real samples.
In this work, we only adopt <math id="A1.SS4.p1.4.m4.1" class="ltx_Math" alttext="\mathtt{DiffJPEG}" display="inline"><semantics id="A1.SS4.p1.4.m4.1a"><mi id="A1.SS4.p1.4.m4.1.1" xref="A1.SS4.p1.4.m4.1.1.cmml">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.4.m4.1b"><ci id="A1.SS4.p1.4.m4.1.1.cmml" xref="A1.SS4.p1.4.m4.1.1">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.4.m4.1c">\mathtt{DiffJPEG}</annotation></semantics></math> for simplicity, and this difference will be addressed later.</p>
</div>
<figure id="A1.F16" class="ltx_figure"><img src="/html/2107.10833/assets/x16.png" id="A1.F16.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F16.9.4.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="A1.F16.7.3" class="ltx_text" style="font-size:90%;">JPEG compressed images by <math id="A1.F16.5.1.m1.1" class="ltx_Math" alttext="\mathtt{cv2}" display="inline"><semantics id="A1.F16.5.1.m1.1b"><mi id="A1.F16.5.1.m1.1.1" xref="A1.F16.5.1.m1.1.1.cmml">𝚌𝚟𝟸</mi><annotation-xml encoding="MathML-Content" id="A1.F16.5.1.m1.1c"><ci id="A1.F16.5.1.m1.1.1.cmml" xref="A1.F16.5.1.m1.1.1">𝚌𝚟𝟸</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F16.5.1.m1.1d">\mathtt{cv2}</annotation></semantics></math> and <math id="A1.F16.6.2.m2.1" class="ltx_Math" alttext="\mathtt{DiffJPEG}" display="inline"><semantics id="A1.F16.6.2.m2.1b"><mi id="A1.F16.6.2.m2.1.1" xref="A1.F16.6.2.m2.1.1.cmml">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</mi><annotation-xml encoding="MathML-Content" id="A1.F16.6.2.m2.1c"><ci id="A1.F16.6.2.m2.1.1.cmml" xref="A1.F16.6.2.m2.1.1">𝙳𝚒𝚏𝚏𝙹𝙿𝙴𝙶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F16.6.2.m2.1d">\mathtt{DiffJPEG}</annotation></semantics></math>, with quality factor <math id="A1.F16.7.3.m3.1" class="ltx_Math" alttext="q=50" display="inline"><semantics id="A1.F16.7.3.m3.1b"><mrow id="A1.F16.7.3.m3.1.1" xref="A1.F16.7.3.m3.1.1.cmml"><mi id="A1.F16.7.3.m3.1.1.2" xref="A1.F16.7.3.m3.1.1.2.cmml">q</mi><mo id="A1.F16.7.3.m3.1.1.1" xref="A1.F16.7.3.m3.1.1.1.cmml">=</mo><mn id="A1.F16.7.3.m3.1.1.3" xref="A1.F16.7.3.m3.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F16.7.3.m3.1c"><apply id="A1.F16.7.3.m3.1.1.cmml" xref="A1.F16.7.3.m3.1.1"><eq id="A1.F16.7.3.m3.1.1.1.cmml" xref="A1.F16.7.3.m3.1.1.1"></eq><ci id="A1.F16.7.3.m3.1.1.2.cmml" xref="A1.F16.7.3.m3.1.1.2">𝑞</ci><cn type="integer" id="A1.F16.7.3.m3.1.1.3.cmml" xref="A1.F16.7.3.m3.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F16.7.3.m3.1d">q=50</annotation></semantics></math>. They produces slightly different results. Zoom in for best view</span></figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Quantitative Comparisons</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We provide the non-reference image quality assessment - NIQE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> for reference. Note that existing metrics for perceptual quality cannot well reflect the actual human perceptual preferences on the fine-grained scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">We compare our Real-ESRGAN with several state-of-the-art methods, including ESRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, DAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, CDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and BSRGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
We test on several diverse testing datasets with real-world images, including RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, DRealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, OST300 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, DPED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, ImageNet validation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and ADE20K validation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
The results are shown in Tab. <a href="#A1.T1" title="Table 1 ‣ A.2 Noise ‣ Appendix A Details of Classical Degradation Model ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Though our Real-ESRGAN+ does not optimize for NIQE scores, it sill produces lower NIQE scores on most testing datasets.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>More Qualitative Comparisons</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We show more qualitative comparisons with previous works. As shown in Fig. <a href="#A3.F17" title="Figure 17 ‣ Appendix C More Qualitative Comparisons ‣ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, our <span id="A3.p1.1.1" class="ltx_text">Real-ESRGAN</span> outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness.
Other methods typically fail to remove complicated artifacts (the 1st sample) and overshoot artifacts (the 2nd, 3rd sample), or fail to restore realistic and natural textures for various scenes (the 4th, 5th samples).</p>
</div>
<figure id="A3.F17" class="ltx_figure"><img src="/html/2107.10833/assets/x17.png" id="A3.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="558" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F17.5.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="A3.F17.6.2" class="ltx_text" style="font-size:90%;">Qualitative comparisons on several representative real-world samples with upsampling scale factor of 4. Our <span id="A3.F17.6.2.1" class="ltx_text">Real-ESRGAN</span> outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness.
Other methods typically fail to remove complicated artifacts (the 1st sample) and overshoot artifacts (the 2nd, 3rd sample), or fail to restore realistic and natural textures for various scenes (the 4th, 5th samples).
(<span id="A3.F17.6.2.2" class="ltx_text ltx_font_bold">Zoom in for best view</span>)</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.10832" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.10833" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.10833">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.10833" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.10834" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 14:50:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
