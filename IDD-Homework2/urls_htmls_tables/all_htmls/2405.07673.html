<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation</title>
<!--Generated on Tue May 14 14:38:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.07673v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S1" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S2" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S2.SS0.SSS0.Px1" title="In 2. Related Work ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">NMT Robustness and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S2.SS0.SSS0.Px2" title="In 2. Related Work ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">Multilingual NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S2.SS0.SSS0.Px3" title="In 2. Related Work ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">Multilingual NMT Robustness</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S3" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Robustness and Evaluation Protocol</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data Collection and Annotation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.SS1" title="In 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.SS2" title="In 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Data Filtering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.SS2.SSS0.Px1" title="In 4.2. Data Filtering ‣ 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">Pre-filter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.SS2.SSS0.Px2" title="In 4.2. Data Filtering ‣ 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">OOV Filter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.SS2.SSS0.Px3" title="In 4.2. Data Filtering ‣ 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">Language Model Filter</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.SS3" title="In 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Noise Identification and Annotation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S5" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.SS1" title="In 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Evaluation Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.SS2" title="In 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Evaluation Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.SS1" title="In 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Effect of Model Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.SS2" title="In 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Effect of Noise Types</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.SS3" title="In 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Relationship between Automatic and Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.SS4" title="In 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Relationship between Sentence Length, Noise Types, and Model Sizes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S8" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusions and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#Sx2.SS0.SSS0.Px1" title="In Ethics Statement ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">Data Privacy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#Sx2.SS0.SSS0.Px2" title="In Ethics Statement ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title">Social Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S9" title="In An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Bibliographical References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages. In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise. To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation. This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes. We conduct both automatic and human evaluations. Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators. The dataset is publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tjunlp-lab/ID-ZH-MTRobustEval" title="">https://github.com/tjunlp-lab/ID-ZH-MTRobustEval</a>.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id4.id1.1">Keywords: </span>Multilingual Neural Machine Translation, Robustness, Evaluation</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1"></span></p>
</div>
<div class="ltx_logical-block" id="id3">
<div class="ltx_para" id="id3.p1">
<p class="ltx_p ltx_align_center" id="id3.p1.1"><span class="ltx_text ltx_font_bold" id="id3.p1.1.1" style="font-size:144%;">An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<table class="ltx_tabular ltx_centering ltx_align_top" id="id2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id2.2.2">
<td class="ltx_td ltx_align_center" id="id2.2.2.2"><span class="ltx_text ltx_font_bold" id="id2.2.2.2.2" style="font-size:120%;">Supryadi, Leiyu Pan, Deyi Xiong<sup class="ltx_sup" id="id2.2.2.2.2.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id2.2.2.2.2.2.1">∗</span></sup><span class="ltx_note ltx_role_thanks" id="id2.2.2.2.2.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><sup class="ltx_sup" id="id2.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id2.2.2.2.2.1.1.1">∗</span></sup>Corresponding author.</span></span></span></span></td>
</tr>
<tr class="ltx_tr" id="id2.2.3.1">
<td class="ltx_td ltx_align_center" id="id2.2.3.1.1">College of Intelligence and Computing, Tianjin University</td>
</tr>
<tr class="ltx_tr" id="id2.2.4.2">
<td class="ltx_td ltx_align_center" id="id2.2.4.2.1">Tianjin, China</td>
</tr>
<tr class="ltx_tr" id="id2.2.5.3">
<td class="ltx_td ltx_align_center" id="id2.2.5.3.1">{supryadi, lypan, dyxiong}@tju.edu.cn</td>
</tr>
</tbody>
</table>
<p class="ltx_p ltx_align_center" id="id3.p1.2"><span class="ltx_text ltx_font_italic" id="id3.p1.2.1">Abstract content</span></p>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent years have witnessed that neural machine translation (NMT) achieves a remarkable progress in both high- and low-resource language translation. For the former aspect, translation quality is substantially improved for many high-resource language pairs (e.g., Chinese-English, French-English) over the years, which has been tracked by yearly WMT evaluation <cite class="ltx_cite ltx_citemacro_cite">Bojar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib5" title="">2018</a>)</cite>. Human parity has even been reached for some language pairs in terms of certain evaluation protocols (<cite class="ltx_cite ltx_citemacro_citep">Hassan et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib9" title="">2018</a>; Barrault et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib2" title="">2019</a></cite>). For the latter aspect, to improve translation quality of low-resource languages, massively multilingual neural machine translation (MMNMT) has been explored with growing interest, which enables knowledge transfer from high-resource languages to low-resource languages (<cite class="ltx_cite ltx_citemacro_citep">Aharoni et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib1" title="">2019</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib8" title="">2021</a>; Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib7" title="">2022</a></cite>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Conversely, NMT still faces challenges related to robustness, particularly in handling noise <cite class="ltx_cite ltx_citemacro_citep">(Belinkov and Bisk, <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib3" title="">2018</a>)</cite> and adapting to domain shifts <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib14" title="">2022</a>)</cite>. In this study, we aim to delve into the translation robustness of Indonesian-Chinese within the context of massively multilingual NMT. Our specific objectives include understanding: 1) the patterns of the relationship between translation error types and noise types, and 2) how these patterns change across various MMNMT model sizes, ranging from models with millions to billions of parameters. Such an investigation holds significant importance in advancing our understanding of the robustness of Indonesian-Chinese translation, which remains an underexplored area, and in the development of MMNMT models.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To empirically study these patterns and relations, we use the open-sourced NLLB-200 <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib7" title="">2022</a>)</cite> models as our MMNMT models. We curate an Indonesian-to-Chinese translation robustness evaluation dataset that consists of 1001 sentence pairs. Both languages are among the top-20 most spoken languages in the world but the parallel resources for them are very limited. We crawl noisy Indonesian sentences from social medias and manually translate them into Chinese with the collaboration between source language local speaker and the expert of the target language.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We manually identify noises in the source language and categorize them into 10 groups. These noisy source sentences are then automatically translated into Chinese with four NLLB-200 models of different sizes, where translation errors in translated target sentences are detected and classified into 10 categories. In addition to automatic evaluation of translation results with BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib20" title="">2002</a>)</cite> and CHRF++ <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib22" title="">2017</a>)</cite>, we also conduct human evaluation with multidimensional quality metric (MQM<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="www.themqm.org" title="">www.themqm.org</a></span></span></span>).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Robustness evaluation and analysis protocol.</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The contributions of our work are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We empirically evaluate the robustness for Indonesian-Chinese translation. To the best of our knowledge, this is the first attempt to study the robustness of Indonesian-Chinese translation based on MMNMT models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We curate a new noisy parallel dataset on Indonesian-Chinese translation for such evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We manually identify noise types in the dataset and translation error types in translations generated by the NLLB-200 models, study the relation patterns of them and examine the changes of these patterns across different model sizes.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">NMT Robustness and Evaluation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Robustness is of paramount importance for neural machine translation (NMT), especially when NMT systems are deployed in real-world applications. A wide variety of efforts have been dedicated to enhancing the robustness of NMT. Among them, black-box methods are widely explored <cite class="ltx_cite ltx_citemacro_cite">Pinnis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib21" title="">2017</a>); Karpukhin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib12" title="">2019</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib16" title="">2019</a>); Wallace et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib26" title="">2020</a>); Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib24" title="">2021</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib30" title="">2021</a>)</cite>. Alternatively, white-box methods, employing gradient-based approaches, have also been proposed <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib6" title="">2019</a>)</cite>. Moreover, empirical evidence suggests that attacking NMT from the source side yields greater effectiveness <cite class="ltx_cite ltx_citemacro_cite">Zeng and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib29" title="">2021</a>)</cite>. These methods usually employ synthetic noise to improve robustness.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">In the context of robustness towards natural noise, the MTNT dataset <cite class="ltx_cite ltx_citemacro_citep">(Michel and Neubig, <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib17" title="">2018</a>)</cite> is designed, originating from noisy data collected from Reddit<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="www.reddit.com" title="">www.reddit.com</a></span></span></span> comments. This dataset comprises three different languages: English, French, and Japanese. In a similar vein, for the assessment of French-English translation robustness, noisy data have also been gathered from restaurant reviews <cite class="ltx_cite ltx_citemacro_citep">(Berard et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib4" title="">2019</a>)</cite>. Additionally, for the evaluation of Chinese-English translation robustness, a dialogue dataset has been created as the natural noise data <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib27" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1">Partially inspired by <cite class="ltx_cite ltx_citemacro_citet">Michel and Neubig (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib17" title="">2018</a>)</cite>, we have curated a novel robustness evaluation dataset. However, our dataset differs significantly from <cite class="ltx_cite ltx_citemacro_citet">Michel and Neubig (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib17" title="">2018</a>)</cite> in three aspects. Firstly, our primary focus lies in assessing the robustness of Indonesian-Chinese translation from two geographically distant languages. Secondly, our noisy data is derived from Twitter comments rather than Reddit, encompassing a broader spectrum of topics. Thirdly, we have manually identified and annotated different noise types for each sentence pair, enabling a more targeted evaluation of noise-specific robustness.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multilingual NMT</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Multilingual neural machine translation (MNMT) has garnered growing interest in recent years owing to its capacity to facilitate the deployment of NMT systems supporting multiple languages, knowledge transfer between languages <cite class="ltx_cite ltx_citemacro_cite">Sun and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib25" title="">2022</a>)</cite>, and zero-shot translation capabilities, among others <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib28" title="">2021</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib15" title="">2023</a>)</cite>. To enable knowledge transfer across an extensive array of languages, including 100 or more languages, research has delved into massively multilingual neural machine translation (MMNMT) <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib11" title="">2017</a>); Jin and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib10" title="">2022</a>)</cite>. This exploration has evolved from English-centric models <cite class="ltx_cite ltx_citemacro_cite">Aharoni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib1" title="">2019</a>)</cite> to models extending beyond English-centric approaches, such as M2M-100 <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib8" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">Among those non-English-centric models, NLLB-200 <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib7" title="">2022</a>)</cite> has recently been open-sourced, which encompasses 200 languages and 40,000 translation directions, supported by a model with up to 54 billion parameters trained on a huge amount of natural and synthesized data. In this study, we employ NLLB-200 models to assess the robustness of MMNMT on non-English languages using our curated dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Multilingual NMT Robustness</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">The robustness of multilingual NMT also been evaluated recently <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib19" title="">2023</a>)</cite>. A variety of noises at the character-, word-, and multiple levels have been explored for the study of multilingual NMT robustness. It has been observed that the robustness of multilingual NMT can be transferred across languages. In contrast to previous study, this research specifically focuses evaluating the robustness of MMNMT towards naturally occurring noise, which is categorized into 10 distinct types.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Robustness and Evaluation Protocol</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose a general protocol for evaluating and analyzing MMNMT robustness towards naturally occurring noises, which is model- and language-independent. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, the protocol consists of three main stages.</p>
</div>
<div class="ltx_para" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.ix1.p1.1.1">Data Collection and Annotation</span>: In this initial stage, we commence by identifying suitable sources for collecting noisy data. Once the sources are determined, data is extracted from these sources. We employ automatic noise detection methods to filter the extracted data, retaining only the noisy portions. In our study, this extraction and detection process yields a high-quality monolingual Indonesian corpus that incorporates naturally occurring noise. Each sentence in this corpus is then labeled with its associated noise category. It is worth noting that each sentence may be annotated with multiple noise categories.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.ix2.p1.1.1">Translation</span>: The collected source corpus is then translated into the target language by human translators, adhering to a noise translation convention to ensure consistency in the translation of noisy fragments throughout the entire corpus. These manual translations serve as reference translations for both automatic evaluation and manual analysis. To assess and analyze the robustness of specific MMNMT models, the source corpus is also automatically translated into the target language by these MMNMT models.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.ix3.p1.1.1">Evaluation and Analysis</span>: In this stage, we carry out both automatic and human evaluations. We manually identify translation errors and categorize them according to multidimensional quality metrics (MQM) (specifically level-1 error types). With annotated noise types and translation error types, we can conduct a thorough and comprehensive analysis of the robustness issues observed in MMNMT models.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We will detail the data collection and annotation procedure in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4" title="4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, translation in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S5" title="5. Translation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> and evaluation in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6" title="6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>. In-depth analysis results are presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7" title="7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:455.2pt;height:64.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-185.8pt,26.0pt) scale(0.55057713770928,0.55057713770928) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Indonesia Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1" style="color:#E74C3C;">Spelling/Typo</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1" style="color:#E67E22;">Grammar</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1" style="color:#FF9FF3;">Spoken Language</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.5.1" style="color:#7BED9F;">Slang</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.6.1" style="color:#27AE60;">Proper Noun</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.7.1" style="color:#48DBFB;">Dialect</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.8.1" style="color:#2E86DE;">Code Switching</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.9.1" style="color:#2980B9;">Jargon</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.10.1" style="color:#9B59B6;">Emojis</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.11.1" style="color:#D35400;">Slurs</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.1">Makasih <span class="ltx_text" id="S3.T1.1.1.2.2.1.1" style="color:#48DBFB;">kang</span> sial. Berkat <span class="ltx_text" id="S3.T1.1.1.2.2.1.2" style="color:#2E86DE;">lu</span> dukung <span class="ltx_text" id="S3.T1.1.1.2.2.1.3" style="color:#27AE60;">prancis</span>
</td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.4"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.6"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.9"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.2.2.11"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.1.3.3.1">argentina jadi juara. Sekali lagi terima kasih sudah <span class="ltx_text" id="S3.T1.1.1.3.3.1.1" style="color:#FF9FF3;">bikin</span>
</td>
<td class="ltx_td" id="S3.T1.1.1.3.3.2"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.3"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.4"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.5"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.6"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.7"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.8"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.9"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.10"></td>
<td class="ltx_td" id="S3.T1.1.1.3.3.11"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.1.4.4.1">prancis sial</td>
<td class="ltx_td" id="S3.T1.1.1.4.4.2"></td>
<td class="ltx_td" id="S3.T1.1.1.4.4.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.4">V</td>
<td class="ltx_td" id="S3.T1.1.1.4.4.5"></td>
<td class="ltx_td" id="S3.T1.1.1.4.4.6"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.7">V</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.8">V</td>
<td class="ltx_td" id="S3.T1.1.1.4.4.9"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.10">V</td>
<td class="ltx_td" id="S3.T1.1.1.4.4.11"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.1">
<span class="ltx_text" id="S3.T1.1.1.5.5.1.1" style="color:#E67E22;">alah</span> ribet amat urusan ucap natal <span class="ltx_text" id="S3.T1.1.1.5.5.1.2" style="color:#7BED9F;">ga</span> ucap <span class="ltx_text" id="S3.T1.1.1.5.5.1.3" style="color:#E74C3C;">nata</span>,</td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.4"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.6"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.9"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.5.5.11"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.1.1.6.6.1">noh negara lain udah <span class="ltx_text" id="S3.T1.1.1.6.6.1.1" style="color:#FF9FF3;">mikirin</span> hidup di <span class="ltx_text" id="S3.T1.1.1.6.6.1.2" style="color:#27AE60;">mars</span>.</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.2">V</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.3">V</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.4">V</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.5">V</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.6">V</td>
<td class="ltx_td" id="S3.T1.1.1.6.6.7"></td>
<td class="ltx_td" id="S3.T1.1.1.6.6.8"></td>
<td class="ltx_td" id="S3.T1.1.1.6.6.9"></td>
<td class="ltx_td" id="S3.T1.1.1.6.6.10"></td>
<td class="ltx_td" id="S3.T1.1.1.6.6.11"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.1">Gara2 <span class="ltx_text" id="S3.T1.1.1.7.7.1.1" style="color:#FF9FF3;">ngomen</span> denny Akun<span class="ltx_text" id="S3.T1.1.1.7.7.1.2" style="color:#E67E22;">.ku</span> dihanguskan <span class="ltx_text" id="S3.T1.1.1.7.7.1.3" style="color:#D35400;">njir</span>
</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.2"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.3">V</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.4">V</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.5"></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.6"></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.7"></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.8"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.9">V</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.10"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.7.7.11">V</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Noise identification and annotation.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Data Collection and Annotation</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Data Collection</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We collected raw social media comments from Twitter. To obtain these comments, we utilized Tweepy<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tweepy" title="">https://github.com/tweepy</a></span></span></span>, a Python library for accessing the Twitter API. Given our focus on the Indonesian language, the comments are crawled using popular Twitter accounts from Indonesia as keywords. The collection period for these comments spanned one week, from 13 December 2022 to 20 December 2022. The final dataset contains a total of 25,973 comments.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Data Filtering</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">After the collection of these comments, we employed filtering methods to detect noisy comments, following the approach outlined by <cite class="ltx_cite ltx_citemacro_citet">Michel and Neubig (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib17" title="">2018</a>)</cite> in the MTNT dataset. We utilized three filtering methods for this purpose.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Pre-filter</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We performed a pre-filtering process on the collected raw data in three steps, with the aim of retaining only naturally occurring noises:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p" id="S4.I1.ix1.p1.1">Removing comments containing URLs.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span>
<div class="ltx_para" id="S4.I1.ix2.p1">
<p class="ltx_p" id="S4.I1.ix2.p1.1">Removing comments from users where their usernames contain “bot” or “AutoModerator”.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span>
<div class="ltx_para" id="S4.I1.ix3.p1">
<p class="ltx_p" id="S4.I1.ix3.p1.1">Removing comments written in other languages. We use Python library Langid.py<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/saffsd/langid.py" title="">https://github.com/saffsd/langid.py</a></span></span></span> to detect non-Indonesian languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">OOV Filter</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">For robustness evaluation, we aimed to make the corpus as noisy as possible, considering out-of-vocabulary (OOV) words as a form of noise. To introduce unknown words and add noise to the sentences, we created a dictionary using a contrast corpus. Our contrast corpus comprises the Indonesian section of WMT20news-commentary-v15<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.statmt.org/wmt20" title="">https://www.statmt.org/wmt20</a></span></span></span> and OpenSubtitles<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.opensubtitles.com" title="">https://www.opensubtitles.com</a></span></span></span>. Using the fairseq tool <cite class="ltx_cite ltx_citemacro_cite">Ott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib18" title="">2019</a>)</cite>, we generate a dictionary containing 5,000 words. Then, we only keep those comments that contain at least one OOV word.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Language Model Filter</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">In the final step, we employed an n-gram language model to further identify noisy comments. We tokenized both the contrast corpus and the collected comments using Byte-Pair Encoding (BPE) with SentencePiece <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib13" title="">2018</a>)</cite>. We then trained a 5-gram Kneser-Ney smoothed language model on the segmented contrast corpus. This trained model is used to calculate language model scores for the comments, normalized by sentence length. We selected comments within a specific score interval, specifically the range between the first and third quartiles of normalized language model scores in the comment corpus. However, we ensured that the normalized language model score of a retained comment was smaller than the third quartile of normalized language model scores in the contrast corpus. This approach strikes a balance, ensuring that each kept comment contains a certain amount of noise without being overly noisy, which differs from the method used in MTNT.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p2.1">After applying the three filters to the collected raw data, we retained 1,001 comments in the final dataset. The average sentence length of these retained comments is 10 words, with a standard deviation of 6.6. The shortest comment consists of 1 word, while the longest comment comprises 48 words.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="250" id="S4.F2.g1" src="x2.png" width="304"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Statistics of different noise types in the curated dataset.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.3.   Noise Identification and Annotation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We use a noise taxonomy similar to that used in MTNT, which consists of:</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Spelling/typographical errors</span>: Comments contain incorrectly spelled or typed words.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Grammatical errors</span>: Comments are not grammatically written.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Spoken language</span>: Comments are written in the style of spoken language.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i4.p1.1.1">Internet slang</span>: Comments contain trending words in internet/social media.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i5.p1.1.1">Proper nouns</span>: Proper nouns, e.g., entities of place, person, are incorrectly written.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i6.p1">
<p class="ltx_p" id="S4.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i6.p1.1.1">Dialects</span>: Comments contain Indonesia dialects, e.g., Javanese, Acehnese, and Balinese.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i7.p1">
<p class="ltx_p" id="S4.I2.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i7.p1.1.1">Code switching</span>: Comments contain more than one language.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i8.p1">
<p class="ltx_p" id="S4.I2.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i8.p1.1.1">Jargon</span>: Comments include specific words used in certain areas of life (environment).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i9.p1">
<p class="ltx_p" id="S4.I2.i9.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i9.p1.1.1">Emojis</span>: Comments contain emojis for feeling expression.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i10.p1">
<p class="ltx_p" id="S4.I2.i10.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i10.p1.1.1">Slurs</span>: Improper words are used to insult people.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">In the corpus, we detected each instance of noise and classified them into their respective noise types, as previously described. It is worth emphasizing that a sentence may contain multiple types of noise, and we annotate each of these noise types for the sentence accordingly.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">The example of our noise identifications and annotations is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S3.T1" title="Table 1 ‣ 3. Robustness and Evaluation Protocol ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. In the first sentence, “kan” is a Sundanese dialect with the meaning “brother”, and “lu” is a Hokkien dialect meaning “you”. These two words indicate the presence of code switching in the sentence. Additionally, the word “prancis” is a proper noun referring to “France”, which should be capitalized as “Prancis”. Towards the end of the sentence, the spoken word “bikin” should be replaced with the written word “buat”, and an emoticon is present at the end of the sentence.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">In the second sentence, the first word “alah” is an interjection expressing “complaining”. There is a missing conjunction “and” between “ucap natal” and “ga ucap natal”. “Ga” is a slang term that translates to “no” in English. The word “nata” appears to be a typographical error and should be corrected to “Natal”, which means “Christmas”. Similarly, “mikirin” is a spoken language form and should be written as “memikirkan”, which means “thinking”. Finally, “Mars” is a proper noun referring to the name of a planet.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:455.2pt;height:132pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.3pt,24.7pt) scale(0.727451660045004,0.727451660045004) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1">Error Type</span></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.3.1">Description</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.2.1"><span class="ltx_text" id="S4.T2.1.1.2.2.1.1">Terminology</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.1.1.2.2.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.2.3">Inconsistency and accuracy issues of the terminology.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.3.1" rowspan="4"><span class="ltx_text" id="S4.T2.1.1.3.3.1.1">Accuracy</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.3.3.2">Mistranslation</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.3.3">Target content that does not accurately represent the source content.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.1.4.4.1">Omission</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.4.4.2">The target content is missing from the translation that is present in the source.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.1.5.5.1">Addition</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.5.2">Target content that includes content not present in the source.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.1.6.6.1">Untranslated</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.6.6.2">The text in source content is left untranslated in the target content.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.7">
<td class="ltx_td" id="S4.T2.1.1.7.7.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.1.7.7.2">Hallucination</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.7.7.3">The translation is very different or irrevelant with the source.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.8.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.1.8.8.1.1">Fluency</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.8.8.2">Grammar</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.8.3">The translation result violates the grammatical rules of the target language.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.1.9.9.1">Punctuation</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.9.9.2">Incorrect punctuation for the locale or style.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.10.10.1"><span class="ltx_text" id="S4.T2.1.1.10.10.1.1">Local Convention</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.1.1.10.10.2"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.10.10.3">The translation violates locale-specific content or formatting requirements.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.11.11.1"><span class="ltx_text" id="S4.T2.1.1.11.11.1.1">Audience Appropriateness</span></td>
<td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.1.11.11.2"></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.11.11.3">The use of content in the translation that is invalid or inappropriate for the target audience.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>MQM hierarchy.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1" style="width:455.2pt;height:63.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.6pt,1.5pt) scale(0.955333987879392,0.955333987879392) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1">Severity Level</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.2.1.1">Critical</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.1.2">The errors that significantly affect translation usability, understandability, and meaning.</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.1.3.2.1">Major</th>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.3.2.2">Errors that would impact usability or understandability of the translation.</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T3.1.1.4.3.1">Minor</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.1.1.4.3.2">Errors that would not impact the usability or understandability of the translation.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>MQM severity levels.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">In the last sentence, “ngomen” is a spoken language form that should be written as “mengomentari”, which means “to give comments”. The use of “.k” is a grammar error since the sentence is not finished, but a full stop is placed in the middle. Furthermore, it is important to note that “njir” is a derogatory slang term in the Indonesian language that dehumanizes individuals by comparing them to dogs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">The statistics for annotated noise types are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.F2" title="Figure 2 ‣ Language Model Filter ‣ 4.2. Data Filtering ‣ 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. The most prevalent noise type is grammatical noise, which is observed in 647 comments. Additionally, the spoken language and slang noise types are also prominently represented. This observation aligns well with our expectations for social media texts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Translation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The annotated corpus is then translated into Chinese manually, creating a parallel corpus that acts as a benchmark testbed for evaluating robustness. We enlisted language experts proficient in both the source and target languages for this task. Through collaboration with experts in both languages, the sentences’ meanings are conveyed with greater accuracy, making the translations more comprehensible to the reader. These translations were reviewed and proofread by professional translator to ensure translation consistency, particularly in noisy parts of the corpus. To curate a high-quality parallel corpus, it is important to establish guidelines ensuring consistency in the translation results. Our guidelines for the parallel corpus translation include:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Translating punctuation according to the target language convention and normalizing its usage.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Translating idioms to convey their meaning and ensure reader comprehension.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Correcting grammar errors in the source sentences during translation, maintaining proper grammar.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">Standardizing the translation of proper nouns (names of people, places, organizations, products) across the entire corpus.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">Preserving emojis in the translation.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1">Standardizing the translation of slang throughout the corpus.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">For machine translation, we utilized NLLB-200, a recently released MMNMT model. We employed four variants of the NLLB-200 model for automatically translating the collected dataset: NLLB-200-Distilled 600M, NLLB-200-Distilled 1.3B, NLLB-200 1.3B, and NLLB-200 3.3B. The results of machine translation will be evaluated and analyzed.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Evaluation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">An evaluation is conducted to compare the translation results of NLLB across different model sizes. By performing human and machine translations on the corpus, we conducted both automatic and human evaluations to assess the robustness of the NLLB-200 model against naturally occurring noise.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.1">Evaluation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.2.1">NLLB-200-Distilled 600M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.3.1">NLLB-200-Distilled 1.3B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.4.1">NLLB-200 1.3B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.5.1">NLLB-200 3.3B</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.1.2.1.1">BLEU</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.2">11.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.3">10.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.4">12.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S6.T4.1.2.1.5.1">14.04</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.1.3.2.1">CHRF++</th>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.2">9.56</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.3">9.34</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.4">10.33</td>
<td class="ltx_td ltx_align_center" id="S6.T4.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S6.T4.1.3.2.5.1">11.23</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.1.4.3.1">MQM</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.4.3.2">2.21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.4.3.3">5.54</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.4.3.4">10.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S6.T4.1.4.3.5.1">12.17</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
BLEU, CHRF++, and MQM scores of translation results yielded by different models on the dataset.
</figcaption>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">6.1.   Evaluation Settings</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">For each NLLB model translation results, we conduct the automatic and human evaluation. The source and reference used for the evaluation is from the curated Indonesian-Chinese dataset.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">For automatic evaluation, we use automatic metrics of BLEU and CHRF++, which collectively measure both word- and character-level translation quality. We use SacreBLEU tool for calculating the BLEU and CHRF++ score <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#bib.bib23" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Additionally, human evaluation is conducted to enhance the evaluation results. To achieve this, we utilize various types of translation errors from the multidimensional quality metric (MQM) framework, which are categorized into five groups: <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.1">terminology</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2">accuracy</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.3">fluency</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.4">local convention</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.5">audience appropriateness</span>. Accuracy encompasses translation errors such as <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.6">mistranslation</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.7">omission</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.8">addition</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.9">untranslated</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.10">hallucination</span>. Fluency covers translation errors related to <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.11">grammar</span> and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.12">punctuation</span>. In NMT, there is a possibility that the NMT system may produce strange or irrelevant translations. Therefore, in our experiment, we introduce a new error type called “hallucination”, which is not included in the MQM framework. The detailed hierarchy of the error types is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.T2" title="Table 2 ‣ 4.3. Noise Identification and Annotation ‣ 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Among the ten previously mentioned error types (ET), each one is initially given a standard error weight of 1. However, in the case of hallucination, we assign a weight of 3. This choice is grounded in the recognition that hallucination represents an exceptionally critical error, as it involves a substantial departure in meaning from the source sentence.</p>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<p class="ltx_p" id="S6.SS1.p5.1">Each error type (ET) has three severity levels: <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.1">minor</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.2">major</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.3">critical</span>, with multiplier scores of 1, 5, and 10, respectively. The detail explanation of each severity level is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S4.T3" title="Table 3 ‣ 4.3. Noise Identification and Annotation ‣ 4. Data Collection and Annotation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. We manually identify translation errors for each target translation and annotate the corresponding translation error type and severity level. Additionally, we permit the annotation of multiple translation error types for each translation if different translation errors are found in the target translation.</p>
</div>
<div class="ltx_para" id="S6.SS1.p6">
<p class="ltx_p" id="S6.SS1.p6.1">Following the annotation process, we proceed to calculate the Overall Quality Score (OQS) using the MQM framework. To begin, we calculate the Error Type Penalty Total (ETPT) for each error type, as defined in Equation <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.E1" title="In 6.1. Evaluation Settings ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. Subsequently, the OQS is derived by evaluating the relationship between ETPT and the Evaluation Word Count (EWC), as described in Equation <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.E2" title="In 6.1. Evaluation Settings ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. The EWC denotes the total count of words present in the source language corpus.</p>
</div>
<div class="ltx_para" id="S6.SS1.p7">
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\rm ETPT=({ET}_{{minor}}+ET_{{major}}\times 5+\\
\rm ET_{critical}\times 10)\times ET_{{weight}}\end{split}" class="ltx_Math" display="block" id="S6.E1.m1.20"><semantics id="S6.E1.m1.20a"><mtable displaystyle="true" id="S6.E1.m1.19.19" rowspacing="0pt" xref="S6.E1.m1.20.20.1.cmml"><mtr id="S6.E1.m1.19.19a" xref="S6.E1.m1.20.20.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S6.E1.m1.19.19b" xref="S6.E1.m1.20.20.1.cmml"><mrow id="S6.E1.m1.11.11.11.11.11" xref="S6.E1.m1.20.20.1.cmml"><mi id="S6.E1.m1.1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.1.cmml">ETPT</mi><mo id="S6.E1.m1.2.2.2.2.2.2" xref="S6.E1.m1.2.2.2.2.2.2.cmml">=</mo><mrow id="S6.E1.m1.11.11.11.11.11.12" xref="S6.E1.m1.20.20.1.cmml"><mo id="S6.E1.m1.3.3.3.3.3.3" stretchy="false" xref="S6.E1.m1.20.20.1.cmml">(</mo><msub id="S6.E1.m1.11.11.11.11.11.12.1" xref="S6.E1.m1.20.20.1.cmml"><mi id="S6.E1.m1.4.4.4.4.4.4" xref="S6.E1.m1.4.4.4.4.4.4.cmml">ET</mi><mi id="S6.E1.m1.5.5.5.5.5.5.1" xref="S6.E1.m1.5.5.5.5.5.5.1.cmml">minor</mi></msub><mo id="S6.E1.m1.6.6.6.6.6.6" xref="S6.E1.m1.6.6.6.6.6.6.cmml">+</mo><msub id="S6.E1.m1.11.11.11.11.11.12.2" xref="S6.E1.m1.20.20.1.cmml"><mi id="S6.E1.m1.7.7.7.7.7.7" xref="S6.E1.m1.7.7.7.7.7.7.cmml">ET</mi><mi id="S6.E1.m1.8.8.8.8.8.8.1" xref="S6.E1.m1.8.8.8.8.8.8.1.cmml">major</mi></msub><mo id="S6.E1.m1.9.9.9.9.9.9" lspace="0.222em" rspace="0.222em" xref="S6.E1.m1.9.9.9.9.9.9.cmml">×</mo><mn id="S6.E1.m1.10.10.10.10.10.10" xref="S6.E1.m1.10.10.10.10.10.10.cmml">5</mn><mo id="S6.E1.m1.11.11.11.11.11.11" xref="S6.E1.m1.20.20.1.cmml">+</mo></mrow></mrow></mtd></mtr><mtr id="S6.E1.m1.19.19c" xref="S6.E1.m1.20.20.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S6.E1.m1.19.19d" xref="S6.E1.m1.20.20.1.cmml"><mrow id="S6.E1.m1.19.19.19.8.8" xref="S6.E1.m1.20.20.1.cmml"><msub id="S6.E1.m1.19.19.19.8.8.9" xref="S6.E1.m1.20.20.1.cmml"><mi id="S6.E1.m1.12.12.12.1.1.1" xref="S6.E1.m1.12.12.12.1.1.1.cmml">ET</mi><mi id="S6.E1.m1.13.13.13.2.2.2.1" xref="S6.E1.m1.13.13.13.2.2.2.1.cmml">critical</mi></msub><mo id="S6.E1.m1.14.14.14.3.3.3" lspace="0.222em" rspace="0.222em" xref="S6.E1.m1.14.14.14.3.3.3.cmml">×</mo><mn id="S6.E1.m1.15.15.15.4.4.4" xref="S6.E1.m1.15.15.15.4.4.4.cmml">10</mn><mo id="S6.E1.m1.16.16.16.5.5.5" rspace="0.055em" stretchy="false" xref="S6.E1.m1.20.20.1.cmml">)</mo><mo id="S6.E1.m1.17.17.17.6.6.6" rspace="0.222em" xref="S6.E1.m1.17.17.17.6.6.6.cmml">×</mo><mi id="S6.E1.m1.18.18.18.7.7.7" xref="S6.E1.m1.18.18.18.7.7.7.cmml">ET</mi><msub id="S6.E1.m1.19.19.19.8.8.10" xref="S6.E1.m1.20.20.1.cmml"><mi id="S6.E1.m1.19.19.19.8.8.10a" xref="S6.E1.m1.20.20.1.cmml"></mi><mi id="S6.E1.m1.19.19.19.8.8.8.1" xref="S6.E1.m1.19.19.19.8.8.8.1.cmml">weight</mi></msub></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S6.E1.m1.20b"><apply id="S6.E1.m1.20.20.1.cmml" xref="S6.E1.m1.19.19"><eq id="S6.E1.m1.2.2.2.2.2.2.cmml" xref="S6.E1.m1.2.2.2.2.2.2"></eq><ci id="S6.E1.m1.1.1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1.1.1">ETPT</ci><apply id="S6.E1.m1.20.20.1.1.cmml" xref="S6.E1.m1.19.19"><times id="S6.E1.m1.17.17.17.6.6.6.cmml" xref="S6.E1.m1.17.17.17.6.6.6"></times><apply id="S6.E1.m1.20.20.1.1.1.1.1.cmml" xref="S6.E1.m1.19.19"><plus id="S6.E1.m1.6.6.6.6.6.6.cmml" xref="S6.E1.m1.6.6.6.6.6.6"></plus><apply id="S6.E1.m1.20.20.1.1.1.1.1.2.cmml" xref="S6.E1.m1.19.19"><csymbol cd="ambiguous" id="S6.E1.m1.20.20.1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.19.19">subscript</csymbol><ci id="S6.E1.m1.4.4.4.4.4.4.cmml" xref="S6.E1.m1.4.4.4.4.4.4">ET</ci><ci id="S6.E1.m1.5.5.5.5.5.5.1.cmml" xref="S6.E1.m1.5.5.5.5.5.5.1">minor</ci></apply><apply id="S6.E1.m1.20.20.1.1.1.1.1.3.cmml" xref="S6.E1.m1.19.19"><times id="S6.E1.m1.9.9.9.9.9.9.cmml" xref="S6.E1.m1.9.9.9.9.9.9"></times><apply id="S6.E1.m1.20.20.1.1.1.1.1.3.2.cmml" xref="S6.E1.m1.19.19"><csymbol cd="ambiguous" id="S6.E1.m1.20.20.1.1.1.1.1.3.2.1.cmml" xref="S6.E1.m1.19.19">subscript</csymbol><ci id="S6.E1.m1.7.7.7.7.7.7.cmml" xref="S6.E1.m1.7.7.7.7.7.7">ET</ci><ci id="S6.E1.m1.8.8.8.8.8.8.1.cmml" xref="S6.E1.m1.8.8.8.8.8.8.1">major</ci></apply><cn id="S6.E1.m1.10.10.10.10.10.10.cmml" type="integer" xref="S6.E1.m1.10.10.10.10.10.10">5</cn></apply><apply id="S6.E1.m1.20.20.1.1.1.1.1.4.cmml" xref="S6.E1.m1.19.19"><times id="S6.E1.m1.14.14.14.3.3.3.cmml" xref="S6.E1.m1.14.14.14.3.3.3"></times><apply id="S6.E1.m1.20.20.1.1.1.1.1.4.2.cmml" xref="S6.E1.m1.19.19"><csymbol cd="ambiguous" id="S6.E1.m1.20.20.1.1.1.1.1.4.2.1.cmml" xref="S6.E1.m1.19.19">subscript</csymbol><ci id="S6.E1.m1.12.12.12.1.1.1.cmml" xref="S6.E1.m1.12.12.12.1.1.1">ET</ci><ci id="S6.E1.m1.13.13.13.2.2.2.1.cmml" xref="S6.E1.m1.13.13.13.2.2.2.1">critical</ci></apply><cn id="S6.E1.m1.15.15.15.4.4.4.cmml" type="integer" xref="S6.E1.m1.15.15.15.4.4.4">10</cn></apply></apply><apply id="S6.E1.m1.20.20.1.1.3.cmml" xref="S6.E1.m1.19.19"><csymbol cd="ambiguous" id="S6.E1.m1.20.20.1.1.3.1.cmml" xref="S6.E1.m1.19.19">subscript</csymbol><ci id="S6.E1.m1.18.18.18.7.7.7.cmml" xref="S6.E1.m1.18.18.18.7.7.7">ET</ci><ci id="S6.E1.m1.19.19.19.8.8.8.1.cmml" xref="S6.E1.m1.19.19.19.8.8.8.1">weight</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.20c">\begin{split}\rm ETPT=({ET}_{{minor}}+ET_{{major}}\times 5+\\
\rm ET_{critical}\times 10)\times ET_{{weight}}\end{split}</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.20d">start_ROW start_CELL roman_ETPT = ( roman_ET start_POSTSUBSCRIPT roman_minor end_POSTSUBSCRIPT + roman_ET start_POSTSUBSCRIPT roman_major end_POSTSUBSCRIPT × 5 + end_CELL end_ROW start_ROW start_CELL roman_ET start_POSTSUBSCRIPT roman_critical end_POSTSUBSCRIPT × 10 ) × roman_ET start_POSTSUBSCRIPT roman_weight end_POSTSUBSCRIPT end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.p8">
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\rm{OQS}=\left(1-\frac{\sum_{\it{i}}{(ETPT_{\it{i}})}}{EWC}\right)\times 100" class="ltx_Math" display="block" id="S6.E2.m1.2"><semantics id="S6.E2.m1.2a"><mrow id="S6.E2.m1.2.2" xref="S6.E2.m1.2.2.cmml"><mi id="S6.E2.m1.2.2.3" xref="S6.E2.m1.2.2.3.cmml">OQS</mi><mo id="S6.E2.m1.2.2.2" xref="S6.E2.m1.2.2.2.cmml">=</mo><mrow id="S6.E2.m1.2.2.1" xref="S6.E2.m1.2.2.1.cmml"><mrow id="S6.E2.m1.2.2.1.1.1" xref="S6.E2.m1.2.2.1.1.1.1.cmml"><mo id="S6.E2.m1.2.2.1.1.1.2" xref="S6.E2.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S6.E2.m1.2.2.1.1.1.1" xref="S6.E2.m1.2.2.1.1.1.1.cmml"><mn id="S6.E2.m1.2.2.1.1.1.1.2" xref="S6.E2.m1.2.2.1.1.1.1.2.cmml">1</mn><mo id="S6.E2.m1.2.2.1.1.1.1.1" xref="S6.E2.m1.2.2.1.1.1.1.1.cmml">−</mo><mfrac id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.cmml"><msub id="S6.E2.m1.1.1.1.2" xref="S6.E2.m1.1.1.1.2.cmml"><mo id="S6.E2.m1.1.1.1.2.2" xref="S6.E2.m1.1.1.1.2.2.cmml">∑</mo><mi id="S6.E2.m1.1.1.1.2.3" xref="S6.E2.m1.1.1.1.2.3.cmml">i</mi></msub><mrow id="S6.E2.m1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.cmml"><mo id="S6.E2.m1.1.1.1.1.1.2" lspace="0em" stretchy="false" xref="S6.E2.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S6.E2.m1.1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.1.cmml"><mi id="S6.E2.m1.1.1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.1.1.2.cmml">ETPT</mi><mi id="S6.E2.m1.1.1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S6.E2.m1.1.1.1.1.1.3" stretchy="false" xref="S6.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mi id="S6.E2.m1.1.1.3" xref="S6.E2.m1.1.1.3.cmml">EWC</mi></mfrac></mrow><mo id="S6.E2.m1.2.2.1.1.1.3" rspace="0.055em" xref="S6.E2.m1.2.2.1.1.1.1.cmml">)</mo></mrow><mo id="S6.E2.m1.2.2.1.2" rspace="0.222em" xref="S6.E2.m1.2.2.1.2.cmml">×</mo><mn id="S6.E2.m1.2.2.1.3" xref="S6.E2.m1.2.2.1.3.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.2b"><apply id="S6.E2.m1.2.2.cmml" xref="S6.E2.m1.2.2"><eq id="S6.E2.m1.2.2.2.cmml" xref="S6.E2.m1.2.2.2"></eq><ci id="S6.E2.m1.2.2.3.cmml" xref="S6.E2.m1.2.2.3">OQS</ci><apply id="S6.E2.m1.2.2.1.cmml" xref="S6.E2.m1.2.2.1"><times id="S6.E2.m1.2.2.1.2.cmml" xref="S6.E2.m1.2.2.1.2"></times><apply id="S6.E2.m1.2.2.1.1.1.1.cmml" xref="S6.E2.m1.2.2.1.1.1"><minus id="S6.E2.m1.2.2.1.1.1.1.1.cmml" xref="S6.E2.m1.2.2.1.1.1.1.1"></minus><cn id="S6.E2.m1.2.2.1.1.1.1.2.cmml" type="integer" xref="S6.E2.m1.2.2.1.1.1.1.2">1</cn><apply id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"><divide id="S6.E2.m1.1.1.2.cmml" xref="S6.E2.m1.1.1"></divide><apply id="S6.E2.m1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"><apply id="S6.E2.m1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.2">subscript</csymbol><sum id="S6.E2.m1.1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.1.2.2"></sum><ci id="S6.E2.m1.1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.1.2.3">𝑖</ci></apply><apply id="S6.E2.m1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1">subscript</csymbol><ci id="S6.E2.m1.1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.1.1.2">ETPT</ci><ci id="S6.E2.m1.1.1.1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="S6.E2.m1.1.1.3.cmml" xref="S6.E2.m1.1.1.3">EWC</ci></apply></apply><cn id="S6.E2.m1.2.2.1.3.cmml" type="integer" xref="S6.E2.m1.2.2.1.3">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.2c">\rm{OQS}=\left(1-\frac{\sum_{\it{i}}{(ETPT_{\it{i}})}}{EWC}\right)\times 100</annotation><annotation encoding="application/x-llamapun" id="S6.E2.m1.2d">roman_OQS = ( 1 - divide start_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( roman_ETPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG roman_EWC end_ARG ) × 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_table" id="S6.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.1" style="width:455.2pt;height:415.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-143.1pt,130.6pt) scale(0.613937650508979,0.613937650508979) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1">Model Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.2.1">Error Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.3.1">Spell/Typo Error</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.4.1">Grammar</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.5.1">Spoken</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.6.1">Slang</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.7.1">Proper</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.8.1">Dialect</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.9.1">Code switch</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.10.1">Jargon</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.11.1">Emojis</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T5.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.12.1">Slurs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.1" rowspan="10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.2.1.1.1">NLLB-200-Distilled 600M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.2.1.2.1">Terminology</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.3">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.4">26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.5">23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.6">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.7">11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.8">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.9">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.2.1.10.1">29</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.11">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.2.1.12">1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.3.2.1.1">Mistranslation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.2">80</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.3.2.3.1">396</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.4">292</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.5">369</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.6">172</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.7">83</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.8">147</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.9">85</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.10">108</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.3.2.11">47</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.4.3.1.1">Omission</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.2">64</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.4.3.3.1">335</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.4">259</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.5">317</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.6">146</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.7">83</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.8">137</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.9">96</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.10">102</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.4.3.11">34</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.5.4.1.1">Addition</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.2">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.5.4.3.1">31</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.4">24</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.5">29</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.6">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.7">5</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.8">7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.9">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.10">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.5.4.11">3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.6.5.1.1">Untranslated</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.2">7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.6.5.3.1">32</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.4">20</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.5">28</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.6">28</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.7">5</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.8">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.9">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.10">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.6.5.11">1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.7.6">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.7.6.1.1">Hallucination</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.2">13</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.3">63</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.4">58</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.7.6.5.1">81</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.6">23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.7">28</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.8">37</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.9">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.10">27</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.7.6.11">13</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.8.7">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.8.7.1.1">Grammar</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.2">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.8.7.3.1">38</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.4">16</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.5">32</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.6">15</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.7">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.8">15</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.9">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.10">15</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.8.7.11">3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.9.8">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.9.8.1.1">Punctuation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.2">18</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.3">89</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.4">70</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.9.8.5.1">101</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.6">39</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.7">26</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.8">39</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.9">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.10">30</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.9.8.11">14</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.10.9">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.10.9.1.1">Local Convention</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.2">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.10.9.3.1">5</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.4">2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.5">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.6">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.7">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.8">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.9">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.10">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.10.9.11">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.11.10">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.11.10.1.1">Audience Appropriateness</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.2">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.3">13</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.4">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.11.10.5.1">19</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.6">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.7">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.8">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.9">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.10">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.11.10.11">1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.1" rowspan="10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.12.11.1.1">NLLB-200-Distilled 1.3B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.12.11.2.1">Terminology</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.3">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.4">18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.5">21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.6">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.8">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.9">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.12.11.10.1">26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.11">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12.11.12">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.13.12">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.13.12.1.1">Mistranslation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.2">64</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.13.12.3.1">339</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.4">255</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.5">315</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.6">154</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.7">82</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.8">135</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.9">92</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.10">101</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.13.12.11">39</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.14.13">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.14.13.1.1">Omission</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.2">66</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.14.13.3.1">349</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.4">256</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.5">343</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.6">152</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.7">82</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.8">138</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.9">92</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.10">119</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.14.13.11">30</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.15.14">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.15.14.1.1">Addition</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.2">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.3">47</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.4">41</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.15.14.5.1">49</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.6">28</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.7">13</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.8">17</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.9">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.10">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.15.14.11">5</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.16.15">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.16.15.1.1">Untranslated</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.2">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.3">23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.4">14</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.16.15.5.1">24</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.6">19</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.7">7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.8">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.9">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.10">5</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.16.15.11">3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.17.16">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.17.16.1.1">Hallucination</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.2">14</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.3">82</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.4">67</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.17.16.5.1">85</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.6">29</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.7">22</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.8">37</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.9">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.10">25</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.17.16.11">17</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.18.17">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.18.17.1.1">Grammar</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.2">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.18.17.3.1">42</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.4">25</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.5">38</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.6">24</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.7">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.8">13</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.9">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.10">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.18.17.11">5</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.19.18">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.19.18.1.1">Punctuation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.2">29</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.19.18.3.1">138</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.4">93</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.5">131</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.6">52</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.7">32</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.8">50</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.9">30</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.10">46</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.19.18.11">15</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.20.19">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.20.19.1.1">Local Convention</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.2">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.20.19.3.1">3</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.4">2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.5">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.6">2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.7">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.8">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.9">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.10">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.20.19.11">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.21.20">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.21.20.1.1">Audience Appropriateness</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.2">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.3">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.4">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.21.20.5.1">15</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.6">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.7">2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.8">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.9">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.10">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.21.20.11">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.1" rowspan="10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.22.21.1.1">NLLB-200 1.3B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.22.21.2.1">Terminology</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.4">14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.5">17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.6">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.8">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.9">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.22.21.10.1">22</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.11">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.22.21.12">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.23.22">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.23.22.1.1">Mistranslation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.2">76</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.23.22.3.1">361</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.4">270</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.5">346</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.6">178</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.7">84</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.8">148</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.9">92</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.10">104</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.23.22.11">39</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.24.23">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.24.23.1.1">Omission</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.2">69</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.24.23.3.1">330</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.4">240</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.5">295</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.6">137</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.7">82</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.8">138</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.9">92</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.10">102</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.24.23.11">32</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.25.24">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.25.24.1.1">Addition</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.2">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.3">16</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.4">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.25.24.5.1">19</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.6">7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.7">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.8">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.9">5</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.10">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.25.24.11">2</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.26.25">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.26.25.1.1">Untranslated</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.2">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.26.25.3.1">35</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.4">23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.5">32</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.6">30</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.7">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.8">15</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.9">10</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.10">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.26.25.11">4</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.27.26">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.27.26.1.1">Hallucination</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.2">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.3">64</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.4">65</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.27.26.5.1">85</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.6">21</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.7">23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.8">32</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.9">14</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.10">25</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.27.26.11">13</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.28.27">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.28.27.1.1">Grammar</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.2">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.28.27.3.1">46</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.4">26</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.5">37</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.6">21</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.7">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.8">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.9">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.10">18</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.28.27.11">2</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.29.28">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.29.28.1.1">Punctuation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.2">25</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.3">104</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.4">73</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.29.28.5.1">105</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.6">32</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.7">34</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.8">54</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.9">22</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.10">35</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.29.28.11">20</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.30.29">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.30.29.1.1">Local Convention</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.30.29.2.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.30.29.3.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.4">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.30.29.5.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.30.29.6.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.7">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.8">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.9">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.10">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.30.29.11">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.31.30">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.31.30.1.1">Audience Appropriateness</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.2">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.3">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.4">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.31.30.5.1">15</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.6">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.7">1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.8">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.9">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.10">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.31.30.11">1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.32.31">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T5.1.1.32.31.1" rowspan="10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.32.31.1.1">NLLB-200 3.3B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.32.31.2.1">Terminology</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.3">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.4">19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.5">20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.6">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.8">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.9">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.32.31.10.1">27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.11">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.32.31.12">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.33.32">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.33.32.1.1">Mistranslation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.2">76</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.33.32.3.1">352</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.4">270</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.5">344</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.6">161</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.7">89</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.8">155</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.9">89</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.10">110</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.33.32.11">43</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.34.33">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.34.33.1.1">Omission</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.2">70</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.34.33.3.1">347</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.4">251</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.5">330</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.6">153</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.7">81</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.8">135</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.9">91</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.10">103</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.34.33.11">33</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.35.34">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.35.34.1.1">Addition</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.2">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.3">20</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.4">24</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.35.34.5.1">24</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.6">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.7">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.8">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.9">4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.10">6</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.35.34.11">2</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.36.35">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.36.35.1.1">Untranslated</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.2">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.3">34</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.4">23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.36.35.5.1">34</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.6">26</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.7">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.8">15</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.9">12</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.10">9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.36.35.11">1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.37.36">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.37.36.1.1">Hallucination</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.2">5</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.3">60</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.4">48</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.37.36.5.1">67</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.6">22</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.7">14</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.8">23</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.9">8</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.10">16</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.37.36.11">12</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.38.37">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.38.37.1.1">Grammar</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.2">7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.38.37.3.1">31</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.4">18</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.5">22</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.6">14</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.7">3</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.8">7</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.9">11</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.10">13</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.38.37.11">4</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.39.38">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.39.38.1.1">Punctuation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.2">20</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.39.38.3.1">88</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.4">59</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.5">82</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.6">41</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.7">24</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.8">36</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.9">15</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.10">33</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.39.38.11">12</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.40.39">
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.40.39.1.1">Local Convention</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.2">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.40.39.3.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.40.39.4.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.40.39.5.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.40.39.6.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.7">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.8">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.9"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.40.39.9.1">2</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.10">0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.1.40.39.11">0</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1.41.40">
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.41.40.1.1">Audience Appropriateness</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.3">8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.4">9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.41.40.5.1">14</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.6">4</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.7">2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.8">3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.9">2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.10">3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T5.1.1.41.40.11">1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The number of translation error types corresponding to different models and noise types on the dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="384" id="S6.F3.g1" src="x3.png" width="288"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The change of translation error types with the increment of model parameters.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">6.2.   Evaluation Results</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">First, we conducted an automated evaluation of Indonesian to Chinese translations produced by various models using the BLEU and CHRF++ metrics. The results are represented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.T4" title="Table 4 ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>. Additionally, the results of human evaluation using MQM are also included in Table  <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.T4" title="Table 4 ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>. It can be observed that the performance of the models improves as their size increases.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">In the human evaluation, Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.T5" title="Table 5 ‣ 6.1. Evaluation Settings ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> shows the occurrences of translation error types corresponding to different noise types across various models. Based on these findings, we conducted further analysis of the evaluation results in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7" title="7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Analysis</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">7.1.   Effect of Model Size</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.F3" title="Figure 3 ‣ 6.1. Evaluation Settings ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the variation in translation error types as the number of model parameters increases. We conducted analysis for each model size in comparison to the subsequent larger model size.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">In the comparison between distilled 600M and distilled 1.3B model, the number of <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1">mistranslation</span> errors in the distilled 1.3B model is considerably lower than those in the distilled 600M model. But the number of <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.2">omission</span>, <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.3">addition</span>, and <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.4">punctuation</span> translation error types in the distilled 1.3B model is notably higher compared to the distilled 600M model. We speculate that the model is becoming more proficient in addressing mistranslation. However, it may encounter issues with yielding target translations incompletely or even producing an excessive translation.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">Furthermore, when comparing the performance of the distilled 1.3B and 1.3B models, we observed that despite having the same amount of parameters, the 1.3B model exhibits more consistent performance across the 10 translation error types. On the other hand, the distilled 1.3B model demonstrates strong performance in terms of <span class="ltx_text ltx_font_italic" id="S7.SS1.p3.1.1">mistranslation</span>, but relatively weaker performance in other translation errors. Hence, we conclude that model distillation may enhance the model’s proficiency in specific areas while potentially compromising its capability in other areas. Conversely, a model trained directly without distillation may offer a more balanced performance across all areas.</p>
</div>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">As the model size expands to 3.3B, there is a reduction in the occurence of most error types, with the exception of <span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.1">mistranslation</span>, which remains stable, and <span class="ltx_text ltx_font_italic" id="S7.SS1.p4.1.2">addition</span>, which experiences a substantial increase. This suggests that the increase in model size may lead the MMNMT model to generate additional information.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">7.2.   Effect of Noise Types</h3>
<figure class="ltx_figure" id="S7.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S7.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The heatmap of the occurences each of translation errors types according to the noise types.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S6.T5" title="Table 5 ‣ 6.1. Evaluation Settings ‣ 6. Evaluation ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, the occurrence of translation error types based on different noise types is similar across various model sizes. With the exception of <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1">addition</span>, <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.2">untranslated</span>, and <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.3">punctuation</span>, several models are affected by grammar or slang. However, the differences in counts are not significant. Therefore, we aggregate the results from the 4 models for further analysis.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.F4" title="Figure 4 ‣ 7.2. Effect of Noise Types ‣ 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> shows a heatmap that assists in analyzing the occurrence of each translation error type based on the noise types present in the source sentences. The values in the heatmap represent the combined translation results from 4 models, which have been normalized according to each translation error type. The occurrence of <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1">terminology</span> errors primarily arises from existing jargon in the source sentences. Translation errors related to <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.2">accuracy</span>, <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.3">fluency</span>, and <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.4">local convention</span> primarily arise from slang and grammar noise within the source sentences. Furthermore, errors in <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.5">audience appropriateness</span> predominantly result from slang utilized in the source sentences.</p>
</div>
<figure class="ltx_table" id="S7.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T6.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S7.T6.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.2.1">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.3.1">CHRF++</span></td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S7.T6.1.2.2.1">MQM</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S7.T6.1.2.2.2">0.8576</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S7.T6.1.2.2.3">0.8741</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>
Pearson’s correlation coefficient between automatically evaluated indicators and human evaluated indicators.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">7.3.   Relationship between Automatic and Human Evaluation</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">It is evident that as the model size increases, both BLEU and CHRF++ scores exhibit a corresponding increase. Moreover, in average, they consistently align with human evaluation results, as measured by MQM. However, there is a slight divergence in the case of the NLLB-200-Distilled-1.3B model size, where there is a slight degradation in both BLEU and CHRF scores. To quantitatively evaluate the correlation between automatic and human assessment metrics, we calculated Pearson’s correlation coefficient between the scores obtained from the automatic evaluation indicators and the number of translation errors identified by the human evaluation metrics. The results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.T6" title="Table 6 ‣ 7.2. Effect of Noise Types ‣ 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>. Based on these findings, we can conclude that CHRF++ demonstrates a stronger correlation with human evaluation when compared to the BLEU score, signifying that CHRF++ serves as a more dependable automatic evaluation metric.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">7.4.   Relationship between Sentence Length, Noise Types, and Model Sizes</h3>
<figure class="ltx_figure" id="S7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S7.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The changing trend of the number of translation errors along with the change of model parameters on short sentences. Dot represents the downward of the trends and square represents the upward of the trends.</figcaption>
</figure>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S7.F6.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The changing trend of the number of translation errors along with the change of model parameters on long sentences. Dot represents the downward of the trends and square represents the upward of the trends.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">When labelling the model translation results with translation error types, we found that there are several differences in the distribution of translation error types for short and long sentences. Thus, we analyzed short and long sentences separately and observed the correlations and differences. We first took the average length of sentences as the threshold value to differentiate long sentences from short sentences. Then we counted the number of translation errors corresponding to different noise types of different models, and fit the number of translation errors generated by different models under different noise types according to the linear regression method. If the primary term coefficient is less than -5 or greater than 0, it means that the number of translation errors has an obvious decreasing or increasing trend as the number of model parameters increases, and we will focus on these cases.</p>
</div>
<div class="ltx_para" id="S7.SS4.p2">
<p class="ltx_p" id="S7.SS4.p2.1">The results are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.F5" title="Figure 5 ‣ 7.4. Relationship between Sentence Length, Noise Types, and Model Sizes ‣ 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.F6" title="Figure 6 ‣ 7.4. Relationship between Sentence Length, Noise Types, and Model Sizes ‣ 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>. The dots on the line represent a clear downward trend in the line, corresponding to a primary term coefficient less than -5. Conversely, the squares represent an upward trend in the line, corresponding to a primary term coefficient greater than 0. The transparency of both the dots and squares indicates the magnitude of these trends, with greater transparency reflecting stronger upward or downward trends.</p>
</div>
<div class="ltx_para" id="S7.SS4.p3">
<p class="ltx_p" id="S7.SS4.p3.1">First, we focus on the analysis of short sentences. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.07673v1#S7.F5" title="Figure 5 ‣ 7.4. Relationship between Sentence Length, Noise Types, and Model Sizes ‣ 7. Analysis ‣ An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, it can be observed that as the model size increases, a clear reduction in the number of translation errors related to grammar, slang, spoken language, and proper nouns noise types is evident. Furthermore, there is a noteworthy decline in the number of translation errors attributed to slang and spoken noise types for <span class="ltx_text ltx_font_italic" id="S7.SS4.p3.1.1">punctuation</span>, indicating an enhancement in fluency. Nevertheless, concerning accuracy, a consistent upward trend is noticeable in the number of translation errors stemming from specific noise types. For instance, <span class="ltx_text ltx_font_italic" id="S7.SS4.p3.1.2">omission</span> errors associated with slang, dialect, and slurs noise contribute to this ascending trend.</p>
</div>
<div class="ltx_para" id="S7.SS4.p4">
<p class="ltx_p" id="S7.SS4.p4.1">In the case of long sentences, a noticeable pattern emerges as the model size increases. We observed a substantial reduction in the number of translation errors associated with slang, grammar, and spoken language noise types, particularly in terms of accuracy. However, there was a tendency for an increase in <span class="ltx_text ltx_font_italic" id="S7.SS4.p4.1.1">untranslated</span> translation errors. In terms of fluency, we observed a diminishing trend in <span class="ltx_text ltx_font_italic" id="S7.SS4.p4.1.2">punctuation</span> errors. Upon comparing these findings, it becomes evident that longer sentences display improved performance with larger models when contrasted with shorter sentences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Conclusions and Future Work</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The robustness of NMT still poses challenges, especially towards natural occuring noises. Based on research findings, it can be concluded that the size of the model significantly impacts translation performance. In terms of noise types, it is evident that spelling and typographical errors can lead to inaccuracies, fluency issues, and translation errors related to terminology. Larger models perform better on longer sentences.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">In the future, we aim to evaluate the robustness of low-resource languages using benchmark datasets. Additionally, given the emergence of large language models (LLMs), we plan to delve into evaluating their performance in translation tasks in comparison to traditional NMT models.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our experiments primarily rely on a curated Indonesian-Chinese parallel corpus crawled from Twitter comments with various types of noise. The dataset covers translations only from Indonesian to Chinese and serves as the evaluation benchmark. The dataset size is relatively small for training but is suitable for robustness evaluation. Due to limitation in computational resources, the NLLB-200 54B model is not used in this research.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Ethics Statement</h2>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data Privacy</h4>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px1.p1.1">The curated noisy parallel corpus of Indonesian-Chinese is openly available for research purposes. One concern regarding this data is that it is obtained by crawling Twitter comments, raising privacy concerns for Twitter users. In order to protect the privacy of Twitter users, we have removed user IDs and usernames from the dataset. In Twitter comments, it is common for users to tag other users by their usernames. To address this, we have systematically removed any text that includes the "@" prefix, thereby effectively eliminating tagged usernames.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Social Impact</h4>
<div class="ltx_para" id="Sx2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Sx2.SS0.SSS0.Px2.p1.1">However, it is essential to note that the dataset is sourced from social media comments, where certain comments may not be appropriate for all audiences, including those containing hate speech or offensive language. In our experiments, we maintain these comments for the purpose of robustness evaluation, as they are also considered as a form of natural noise. It is crucial to take this aspect into account before utilizing our parallel corpus for other studies.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Acknowledgments</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">The present research was supported by the Natural Science Foundation of Xinjiang Uygur Autonomous Region (No. 2022D01D43). We would like to appreciate the insightful comments provided by anonymous reviewers. We extend our gratitude to Ms. Yu for her assistance in proofreading the translation of the parallel corpus. Additionally, we gratitudely acknowledge the support of the Chinese Government Scholarship (CGS).</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.   Bibliographical References</h2>
<div class="ltx_para" id="S9.p1">
<span class="ltx_ERROR undefined" id="S9.p1.1">\c@NAT@ctr</span>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2019)</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1388" title="">Massively multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 3874–3884, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2019)</span>
<span class="ltx_bibblock">
Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5301" title="">Findings of the 2019 conference on machine translation (WMT19)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</em>, pages 1–61, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov and Bisk (2018)</span>
<span class="ltx_bibblock">
Yonatan Belinkov and Yonatan Bisk. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=BJ8vJebC-" title="">Synthetic and natural noise both break neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berard et al. (2019)</span>
<span class="ltx_bibblock">
Alexandre Berard, Ioan Calapodescu, Marc Dymetman, Claude Roux, Jean-Luc Meunier, and Vassilina Nikoulina. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-5617" title="">Machine translation of restaurant reviews: New corpus for domain adaptation and robustness</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 3rd Workshop on Neural Generation and Translation</em>, pages 168–176, Hong Kong. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2018)</span>
<span class="ltx_bibblock">
Ondřej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6401" title="">Findings of the 2018 conference on machine translation (WMT18)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 272–303, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2019)</span>
<span class="ltx_bibblock">
Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1425" title="">Robust neural machine translation with doubly adversarial inputs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4324–4333, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2207.04672.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://dl.acm.org/doi/pdf/10.5555/3546258.3546365" title="">Beyond english-centric multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">J. Mach. Learn. Res.</em>, 22(1).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan et al. (2018)</span>
<span class="ltx_bibblock">
Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1803.05567" title="">Achieving human parity on automatic chinese to english news translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/1803.05567.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin and Xiong (2022)</span>
<span class="ltx_bibblock">
Renren Jin and Deyi Xiong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.458" title="">Informative language representation learning for massively multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 5158–5174, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00065" title="">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Transactions of the Association for Computational Linguistics</em>, 5:339–351.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2019)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, and Marjan Ghazvininejad. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-5506" title="">Training on synthetic noise improves robustness to natural noise in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</em>, pages 42–47, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2022)</span>
<span class="ltx_bibblock">
Wen Lai, Jindřich Libovický, and Alexander Fraser. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.461" title="">Improving both domain robustness and domain adaptability in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 5191–5204, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Shangjie Li, Xiangpeng Wei, Shaolin Zhu, Jun Xie, Baosong Yang, and Deyi Xiong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.303" title="">MMNMT: Modularizing multilingual neural machine translation with flexibly assembled MoE and dense blocks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 4978–4990, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1291" title="">Robust neural machine translation with joint textual and phonetic embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 3044–3049, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michel and Neubig (2018)</span>
<span class="ltx_bibblock">
Paul Michel and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1050" title="">MTNT: A testbed for machine translation of noisy text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4009" title="">fairseq: A fast, extensible toolkit for sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</em>, pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023)</span>
<span class="ltx_bibblock">
Leiyu Pan, Supryadi, and Deyi Xiong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.940" title="">Is robustness transferable across languages in multilingual neural machine translation?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14114–14125, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">BLEU: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinnis et al. (2017)</span>
<span class="ltx_bibblock">
Mārcis Pinnis, Rihards Krišlauks, Daiga Deksne, and Toms Miks. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://link.springer.com/content/pdf/10.1007/978-3-319-64206-2_27.pdf" title="">Neural machine translation for morphologically rich languages with improved sub-word units and synthetic data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Text, Speech, and Dialogue</em>, pages 237–245, Cham. Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2017)</span>
<span class="ltx_bibblock">
Maja Popović. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4770" title="">chrF++: words helping character n-grams</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 612–618, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.aclweb.org/anthology/W18-6319" title="">A call for clarity in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186–191, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2021)</span>
<span class="ltx_bibblock">
Wenjie Qin, Xiang Li, Yuhui Sun, Deyi Xiong, Jianwei Cui, and Bin Wang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP39728.2021.9413586" title="">Modeling homophone noise for robust neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 7533–7537.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Xiong (2022)</span>
<span class="ltx_bibblock">
Haoran Sun and Deyi Xiong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.447" title="">Language branch gated multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 5046–5053, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace et al. (2020)</span>
<span class="ltx_bibblock">
Eric Wallace, Mitchell Stern, and Dawn Song. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.446" title="">Imitation attacks and defenses for black-box machine translation systems</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 5531–5546, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, and Deyi Xiong. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-industry.14" title="">Autocorrect in the process of translation — multi-task learning improves dialogue machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</em>, pages 105–112, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2021)</span>
<span class="ltx_bibblock">
Hongfei Xu, Qiuhui Liu, Josef van Genabith, and Deyi Xiong. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-short.46" title="">Modeling task-aware MIMO cardinality for efficient multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, pages 361–367, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng and Xiong (2021)</span>
<span class="ltx_bibblock">
Zhiyuan Zeng and Deyi Xiong. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-short.58" title="">An empirical study on adversarial attack on NMT: Languages and positions matter</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, pages 454–460, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Xinze Zhang, Junzhe Zhang, Zhenhua Chen, and Kun He. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.153" title="">Crafting adversarial examples for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 1967–1977, Online. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1"></p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue May 14 14:38:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
