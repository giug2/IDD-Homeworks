<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Vision-Based Learning for Drones: A Survey
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Jiaping Xiao
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Rangya Zhang
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Yuhang Zhang
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    and Mir Feroskhan
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_ERROR undefined" id="id1.1.id1">
     \IEEEmembership
    </span>
    Member, IEEE
   </span>
   <span class="ltx_author_notes">
    J. Xiao, R. Zhang, Y. Zhang and M. Feroskhan are with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore 639798, Singapore (e-mail: jiaping001@e.ntu.edu.sg; rangya001@e.ntu.edu.sg; yuhang002@e.ntu.edu.sg; mir.feroskhan@ntu.edu.sg).
    <span class="ltx_text ltx_font_italic" id="id2.2.id1">
     (Corresponding author: Mir Feroskhan.)
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id3.id1">
   Drones as advanced cyber-physical systems are undergoing a transformative shift with the advent of vision-based learning, a field that is rapidly gaining prominence due to its profound impact on drone autonomy and functionality. Different from existing task-specific surveys, this review offers a comprehensive overview of vision-based learning in drones, emphasizing its pivotal role in enhancing their operational capabilities under various scenarios. We start by elucidating the fundamental principles of vision-based learning, highlighting how it significantly improves drones’ visual perception and decision-making processes. We then categorize vision-based control methods into indirect, semi-direct, and end-to-end approaches from the perception-control perspective. We further explore various applications of vision-based drones with learning capabilities, ranging from single-agent systems to more complex multi-agent and heterogeneous system scenarios, and underscore the challenges and innovations characterizing each area. Finally, we explore open questions and potential solutions, paving the way for ongoing research and development in this dynamic and rapidly evolving field. With growing large language models (LLMs) and embodied intelligence, vision-based learning for drones provides a promising but challenging road towards artificial general intelligence (AGI) in 3D physical world.
  </p>
 </div>
 <div class="ltx_para" id="p1">
  <span class="ltx_ERROR undefined" id="p1.1">
   {IEEEkeywords}
  </span>
  <p class="ltx_p" id="p1.2">
   Drones, learning systems, robotics learning, embodied intelligence.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <span class="ltx_ERROR undefined" id="S1.p1.1">
    \IEEEPARstart
   </span>
   <p class="ltx_p" id="S1.p1.2">
    Drones are intelligent cyber-physical systems (CPS)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    that rely on functional sensing, system communication, real-time computation, and flight control to achieve perception and autonomous flight. Due to their high autonomy and maneuverability, drones
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    have been widely used in various missions, such as industrial inspection
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    , precision agriculture
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    , parcel delivery
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    and search-and-rescue
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    (some applications are shown in Fig.
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ). As such, drones in smart cities are an essential feature of the service industry of tomorrow. For future applications, many novel drones and functions are under development with the advent of advanced materials (adhesive and flexible materials), miniaturization of electronic and optical components (sensors, microprocessors), onboard computers (Nividia Jetson, Intel NUC, Raspeberry Pi, etc.), batteries, and localization systems (SLAM, UWB, GPS, etc.). Meanwhile, the functionality of drones is becoming more complex and intelligent due to the rapid advancement of artificial intelligence (AI) and onboard computation capability. From the perspective of innovation, there are three main directions for the future development of drones. Specifically:
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S1.F1.g1" src="/html/2312.05019/assets/x1.png" width="261"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">
      Figure 1
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">
     Applications of vision-based drones. (a) Parcel delivery; (b) Photography; (c) Precision agriculture; (d) Power grid inspection.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p2">
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">
        The minimization of drones
       </span>
       . Micro and nano drones are capable of accomplishing missions at a low cost and without space constraints. The small autonomous drones raised the interest of scientists to obtain more inspiration from biology, such as bees
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib9" title="">
         9
        </a>
        ]
       </cite>
       and flipping birds
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib10" title="">
         10
        </a>
        ]
       </cite>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">
        The novel designs of drones
       </span>
       . Structure and aerodynamic design enable drones to obtain increased maneuverability and improved flight performance. Tilting, morphing, and folding structures and actuators are widely studied in drone design and control
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib11" title="">
         11
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib7" title="">
         7
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib12" title="">
         12
        </a>
        ]
       </cite>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">
        The autonomy of drones
       </span>
       . Drone autonomy achieves autonomous navigation and task execution. It requires real-time perception and onboard computation capabilities. Drones that integrate visual sensors, efficient online planning, and learning-based decision-making algorithms have notably enhanced autonomy and intelligence
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib13" title="">
         13
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib14" title="">
         14
        </a>
        ]
       </cite>
       , even beating world-level champions in drone racing with a vision-based learning system called Swift
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib15" title="">
         15
        </a>
        ]
       </cite>
       .
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Recently, to improve drone autonomy, vision-based learning drones, which combine advanced sensing with learning capabilities, are attracting more insights (see the rapid growth trend in Fig.
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    ). With such capabilities, drones are even emerging towards artificial general intelligence (AGI) in the 3D physical world, especially when integrated with rapid-growing large language models (LLMs)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    and embodied intelligence
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    . Existing vision-based drone-related surveys focus only on specific tasks and applications, such as UAV navigation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib18" title="">
      18
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      19
     </a>
     ]
    </cite>
    , vision-based UAV landing
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ]
    </cite>
    , obstacle avoidance
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      21
     </a>
     ]
    </cite>
    , vision-based inspection with UAVs
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ]
    </cite>
    , and autonomous drone racing
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    , which limits the understanding of vision-based learning in drones from a holistic perspective. Therefore, this survey provides a comprehensive review of vision-based learning for drones to deliver a more general view of current drone autonomy technologies, including background, visual perception, vision-based control, applications and challenges, and open questions with potential solutions. To summarize, our main contributions are:
   </p>
   <ul class="ltx_itemize" id="S1.I2">
    <li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I2.i1.p1">
      <p class="ltx_p" id="S1.I2.i1.p1.1">
       We discussed the development of vision-based drones with learning capabilities and analyzed the core components, especially visual perception and machine learning applied in drones. We further highlighted object detection with visual perception and how it benefits drone applications.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I2.i2.p1">
      <p class="ltx_p" id="S1.I2.i2.p1.1">
       We discussed the current state of vision-based control methods for drones and categorized them into indirect, semi-direct, and end-to-end methods from the perception-control perspective. This perspective helps to understand vision-based control methods and differentiate them with better features.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I2.i3.p1">
      <p class="ltx_p" id="S1.I2.i3.p1.1">
       We summarized the applications of vision-based learning drones in single-agent systems, multi-agent systems, and heterogeneous systems and discussed the corresponding challenges in different applications.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I2.i4.p1">
      <p class="ltx_p" id="S1.I2.i4.p1.1">
       We explored several open questions that can hinder the development and applicability of vision-based learning for drones. Furthermore, we discuss potential solutions for each question.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <figure class="ltx_figure" id="S1.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="196" id="S1.F2.g1" src="/html/2312.05019/assets/figures/number-2013-2022.png" width="339"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">
      Figure 2
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">
     Number of related publications in Google Scholar using keyword “vision-based learning drones”.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">
     Organization:
    </span>
    The rest of this survey is organized as follows: Section
    <a class="ltx_ref" href="#S2" title="2 Background ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    discusses the concept of vision-based learning drones and their core components; Section
    <a class="ltx_ref" href="#S3" title="3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    summarizes object detection with visual perception and its application to vision-based drones; We introduce the vision-based control methods for drones and categorize them in Section
    <a class="ltx_ref" href="#S4" title="4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    ; The applications and challenges of vision-based learning for drones are discussed in Section
    <a class="ltx_ref" href="#S5" title="5 Applications and Challenges ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    ; We list the open questions faced by vision-based learning drones and potential solutions in Section
    <a class="ltx_ref" href="#S6" title="6 Open Questions and Potential Solutions ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    ; Section
    <a class="ltx_ref" href="#S7" title="7 Conclusion ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      7
     </span>
    </a>
    summarizes and concludes this survey.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="131" id="S1.F3.g1" src="/html/2312.05019/assets/x2.png" width="261"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F3.2.1.1" style="font-size:90%;">
      Figure 3
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F3.3.2" style="font-size:90%;">
     General framework of vision-based drones
    </span>
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Background
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Vision-based Learning Drones
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     A typical vision-based drone consists of three parts (see Fig.
     <a class="ltx_ref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     ): (1)
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">
      Visual perception
     </span>
     : sensing the environment around the drone via monocular cameras or stereo cameras; (2)
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.2">
      Image processing
     </span>
     : extracting features from an observed image sequence and output specific patterns or information, such as navigation information, depth information, object information; (3)
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.3">
      Flight controller
     </span>
     : generating high-level and low-level commands for drones to perform assigned missions. Image processing and flight controllers are generally conducted on the onboard computer, while visual perception relies on the performance of visual sensors. Vision-based drones have been widely used in traditional missions such as environmental exploration
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib25" title="">
       25
      </a>
      ]
     </cite>
     , navigation
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     , and obstacle avoidance
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib26" title="">
       26
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     . With efficient image processing and simple path planners, they can avoid dynamic obstacles effectively (see Fig.
     <a class="ltx_ref" href="#S2.F4" title="Figure 4 ‣ 2.1 Vision-based Learning Drones ‣ 2 Background ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     ).
    </p>
   </div>
   <figure class="ltx_figure" id="S2.F4">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S2.F4.sf1.g1" src="/html/2312.05019/assets/figures/moving_gate.jpg" width="617"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F4.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S2.F4.sf2.g1" src="/html/2312.05019/assets/figures/moving_ball.png" width="561"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F4.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">
       Figure 4
      </span>
      :
     </span>
     <span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">
      Vision-based control for drones’ obstacle avoidance in simple dynamic environments. (a) Drone racing in a dynamic environment with moving gates
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib26" title="">
        26
       </a>
       ]
      </cite>
      ; (b) A drone avoiding a ball thrown to it with event cameras
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib27" title="">
        27
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     Currently, vision-based learning drones, which utilize visual sensors and efficient learning algorithms, have achieved remarkable advanced performance in a series of standardized visual perception and decision-making tasks, such as agile flight control
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     , navigation
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       29
      </a>
      ]
     </cite>
     and obstacle avoidance
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     . Various cases have showcased the power of learning algorithms in improving the agility and perception capabilities of vision-based drones. For instance, using only depth cameras, inertial measurement units (IMU) and a lightweight onboard computer, the vision-based learning drone in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     succeeded in performing high-speed flight in unseen and unstructured wild environments. The controller of the drone in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     was trained from a high-fidelity simulation and transferred to a physical platform. Following that, the Swift system was developed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     to achieve world champion-level autonomous drone racing with a tracking camera and a shallow neural network. Such kinds of vision-based learning drones are leading the future of drones due to their perception and learning capabilities in complex environments. Similarly, with event cameras and a trained narrow neural network-EVDodgeNet, the vision-based learning drone in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib30" title="">
       30
      </a>
      ]
     </cite>
     was able to dodge multiple dynamic obstacles (balls) during flight. Afterwards, to improve the perception onboard in the real world, an uncertainty estimation module was trained with the Ajna network
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib31" title="">
       31
      </a>
      ]
     </cite>
     , which significantly increased the generalization capability of learning-based control for drones. The power of deep learning in handling uncertain information frees traditional approaches from complex computation with necessary accurate modeling.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Visual Perception
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Visual perception for drones is the ability of drones to perceive their surroundings and their own states through the extraction of necessary features for specific tasks with visual sensors. Light detection and ranging (LIDAR) and cameras are commonly used sensors to perceive the surrounding environment for drones.
    </p>
   </div>
   <figure class="ltx_figure" id="S2.F5">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S2.F5.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="383" id="S2.F5.sf1.g1" src="/html/2312.05019/assets/figures/Velodyne-scanner-Courtesy-of-M-Shand_W640.jpg" width="538"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F5.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel" id="S2.F5.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="383" id="S2.F5.sf2.g1" src="/html/2312.05019/assets/figures/survey-lidar-UAV_feature.jpg" width="538"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F5.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">
       Figure 5
      </span>
      :
     </span>
     <span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;">
      LIDAR on drones for visual perception. (a) A typical surrounding LIDAR; (b) Generated point cloud with LIDAR.
     </span>
    </figcaption>
   </figure>
   <section class="ltx_subsubsection" id="S2.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.2.1
     </span>
     LIght Detection And Ranging (LIDAR)
    </h4>
    <div class="ltx_para" id="S2.SS2.SSS1.p1">
     <p class="ltx_p" id="S2.SS2.SSS1.p1.1">
      LIDAR is a kind of active range sensor that relies on the calculated time of flight (TOF) between the transmitted and received beams (laser) to estimate the distance between the robot and the reflected surface of objects
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib32" title="">
        32
       </a>
       ]
      </cite>
      . Based on the scanning mechanism, LIDAR can be divided into solid-state LIDAR, which has a fixed field of view without moving parts, and surrounding LIDAR, which spins to provide a 360-degree horizontal view. Surrounding LIDAR is also referred to as “laser scanning” or “3D scanning”, which creates a 3D representation of the explored environment using eye-safe laser beams. A typical LIDAR (see Fig.
      <a class="ltx_ref" href="#S2.F5" title="Figure 5 ‣ 2.2 Visual Perception ‣ 2 Background ‣ Vision-Based Learning for Drones: A Survey">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      ) consists of laser emitters, laser receivers, and a spinning motor. The vertical field of view (FOV) of a LIDAR is determined by the number of vertical arrays of lasers. For instance, a vertical array of 16 lasers scanning 30 degrees gives a vertical resolution of 2 degrees in a typical configuration. LIDAR has recently been used on drones for mapping
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib33" title="">
        33
       </a>
       ]
      </cite>
      , power grid inspection
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib34" title="">
        34
       </a>
       ]
      </cite>
      , pose estimation
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib35" title="">
        35
       </a>
       ]
      </cite>
      and object detection
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib36" title="">
        36
       </a>
       ]
      </cite>
      . LIDAR provides sufficient and accurate depth information for drones to navigate in cluttered environments. However, it is bulky and power-hungry and does not fit within the payload restrictions of agile autonomous drones. Meanwhile, using raycast representation in a simulation environment makes it hard to match the inputs of a real LIDAR device, which brings many challenges for the Sim2Real transfer when a learning approach is considered.
     </p>
    </div>
    <figure class="ltx_figure" id="S2.F6">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F6.sf1">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S2.F6.sf1.g1" src="/html/2312.05019/assets/figures/vio.png" width="538"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S2.F6.sf1.2.1.1" style="font-size:90%;">
           (a)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F6.sf2">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="470" id="S2.F6.sf2.g1" src="/html/2312.05019/assets/figures/event_camera.jpeg" width="538"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S2.F6.sf2.2.1.1" style="font-size:90%;">
           (b)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="S2.F6.2.1.1" style="font-size:90%;">
        Figure 6
       </span>
       :
      </span>
      <span class="ltx_text" id="S2.F6.3.2" style="font-size:90%;">
       Visual perception with cameras for drones. (a) Visual Odometry for drones’ positioning; (b) Object detection for drones’ obstacle avoidance with event camera
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib27" title="">
         27
        </a>
        ]
       </cite>
       .
      </span>
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S2.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.2.2
     </span>
     Camera
    </h4>
    <div class="ltx_para" id="S2.SS2.SSS2.p1">
     <p class="ltx_p" id="S2.SS2.SSS2.p1.1">
      Compared to LIDAR, cameras provide a cheap and lightweight way for drones to perceive the environment. Cameras are external passive sensors used to monitor the drone’s geometric and dynamic relationship to its task, environment or the objects that it is handling. Cameras are commonly used perception sensors for drones to sense environment information, such as objects’ position and a point cloud map of the environment. In contrast to the motion capture system, which can only broadcast global geometric and dynamic pose information within a limited space from an offboard synchronized system, cameras enable a drone to fly without space constraints. Cameras can provide positioning for drone navigation in GPS-denied environments via visual odometry (VO)
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib37" title="">
        37
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib38" title="">
        38
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib39" title="">
        39
       </a>
       ]
      </cite>
      and visual simultaneous localization and mapping systems (V-SLAM)
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib40" title="">
        40
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib41" title="">
        41
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib42" title="">
        42
       </a>
       ]
      </cite>
      . Meanwhile, object detection and depth estimation can be performed with cameras to obtain the relative positions and sizes of obstacles. However, to avoid dynamic obstacles, even physical attacks like bird chasing, the agile vision-based drone poses fundamental challenges to visual perception. Motion blur, sparse texture environments, and unbalanced lighting conditions can cause the loss of feature detection in the VIO and object detection. LIDAR and event cameras
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib43" title="">
        43
       </a>
       ]
      </cite>
      can partially address these challenges. However, LIDAR and event cameras are either too bulky or too expensive for agile drone applications. Considering the agility requirement of physical attack avoidance, lightweight dual-fisheye cameras are used for visual perception. With dual fisheye cameras, the drone can achieve better navigation capability
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib44" title="">
        44
       </a>
       ]
      </cite>
      and omnidirectional visual perception
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib45" title="">
        45
       </a>
       ]
      </cite>
      . Some sensor fusion and state estimation techniques are required to alleviate the accuracy loss brought by the motion blur.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Machine Learning
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     Recently, machine learning (ML), especially deep learning (DL), has attracted much attention from various fields and has been widely applied to robotics for environmental exploration
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib46" title="">
       46
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib47" title="">
       47
      </a>
      ]
     </cite>
     , navigation in unknown environments
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib49" title="">
       49
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
      ]
     </cite>
     , obstacle avoidance, and intelligent control
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib51" title="">
       51
      </a>
      ]
     </cite>
     . In the domain of drones, learning-based methods have also achieved promising success, particularly for deep reinforcement learning (DRL)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib52" title="">
       52
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib53" title="">
       53
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib54" title="">
       54
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib55" title="">
       55
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib56" title="">
       56
      </a>
      ]
     </cite>
     . In
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib54" title="">
       54
      </a>
      ]
     </cite>
     , a curriculum learning augmented end-to-end reinforcement learning was proposed for a UAV to fly through a narrow gap in the real world. A vision-based end-to-end learning method was successfully developed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     to fly agile quadrotors through complex wild and human-made environments with only onboard sensing and computation capabilities, such as depth information. A visual drone swarm was developed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib56" title="">
       56
      </a>
      ]
     </cite>
     to perform collaborative target search with adaptive curriculum embedded multistage learning. These works verified the marvelous power of learning-based methods on drone applications, which pushes the agility and cooperation of drones to a level that classical approaches can hardly reach. Different from the classical approaches relying on separate mapping, localization, and planning, learning-based methods map the observations, such as the visual information or localization of obstacles, to commands directly without further planning. This greatly helps drones handle uncertain information in operations. However, learning-based methods require massive experiences and training datasets to obtain good generalization capability, which poses another challenge in deployment over unknown environments.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Object Detection with Visual Perception
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Object detection is a pivotal module in vision-based learning drones when handling complex missions such as inspection, avoidance, and search and rescue. Object detection is to find out all the objects of interest in the image and determine their position and size
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib57" title="">
      57
     </a>
     ]
    </cite>
    . Object detection is one of the core problems in the field of computer vision (CV). Nowadays, the applications of object detection include face detection, pedestrian detection, vehicle detection, and terrain detection in remote sensing images. Object detection has always been one of the most challenging problems in the field of CV due to the different appearances, shapes, and poses of various objects, as well as the interference of factors such as illumination and occlusion during imaging. At present, the object detection algorithm can be roughly divided into two categories:
    <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">
     multi-stage (two-stage) algorithm
    </span>
    , whose idea is to first generate candidate regions and then perform classification, and
    <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">
     one-stage algorithm
    </span>
    , the idea of which is to directly apply the algorithm to the input image and output the categories and corresponding positions. Beyond that, to retrieve 3D positions, depth estimation has been a popular research subbranch related to object detection whether using monocular
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib58" title="">
      58
     </a>
     ]
    </cite>
    or stereo depth estimation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib59" title="">
      59
     </a>
     ]
    </cite>
    . For a very long time, the core neural network module (backbone) of object detection has been the convolutional neural network (CNN)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib60" title="">
      60
     </a>
     ]
    </cite>
    . CNN is a classic neural network in image processing that originates from the study of the human optic nerve system. The main idea is to convolve the image with the convolution kernel to obtain a series of reorganization features, and these reorganization features represent the important information of the image. As such, CNN not only has the ability to recognize the image but also effectively decreases the requirement for computing resources. Recently, vision transformers (ViTs)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib61" title="">
      61
     </a>
     ]
    </cite>
    , originally proposed for image classification tasks, have been extended to the realm of object detection
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib62" title="">
      62
     </a>
     ]
    </cite>
    . These models demonstrate superior performance by utilizing the self-attention mechanism, which processes visual information non-locally
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib63" title="">
      63
     </a>
     ]
    </cite>
    . However, a major limitation of ViTs is their high computational demand. This presents difficulties in achieving real-time inference, particularly on platforms with limited resources like drones.
   </p>
  </div>
  <figure class="ltx_figure" id="S3.F7">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_2">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F7.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="361" id="S3.F7.sf1.g1" src="/html/2312.05019/assets/figures/R-CNN_Stanford.png" width="538"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S3.F7.sf1.2.1.1" style="font-size:90%;">
         (a)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_2">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F7.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="344" id="S3.F7.sf2.g1" src="/html/2312.05019/assets/figures/fast_RCNN.png" width="538"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S3.F7.sf2.2.1.1" style="font-size:90%;">
         (b)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S3.F7.2.1.1" style="font-size:90%;">
      Figure 7
     </span>
     :
    </span>
    <span class="ltx_text" id="S3.F7.3.2" style="font-size:90%;">
     (a) R-CNN neural network architecture
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib64" title="">
       64
      </a>
      ]
     </cite>
     ; (b) Fast R-CNN neural network architecture
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib65" title="">
       65
      </a>
      ]
     </cite>
     .
    </span>
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Multi-stage Algorithms
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Classic multi-stage algorithms include RCNN (Region-based Convolutional Neural Network)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib64" title="">
       64
      </a>
      ]
     </cite>
     , Fast R-CNN
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib65" title="">
       65
      </a>
      ]
     </cite>
     , Faster R-CNN
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib66" title="">
       66
      </a>
      ]
     </cite>
     . Multi-stage algorithms can basically meet the accuracy requirements in real-life scenarios, but the model is more complex and cannot be really applied to scenarios with high-efficiency requirements. In the R-CNN structure
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib64" title="">
       64
      </a>
      ]
     </cite>
     , it is necessary to first give some regional proposals (RP), then use the convolutional layer for feature extraction, and then classify the regions according to these features. That is, the object detection problem is transformed into an image classification problem. The R-CNN model is very intuitive, but the disadvantage is that it is too slow, and the output is obtained via training multiple Support Vector Machines (SVMs). To solve the problem of slow training speed, the Fast R-CNN model is proposed (Fig.
     <a class="ltx_ref" href="#S3.F7.sf2" title="In Figure 7 ‣ 3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       7(b)
      </span>
     </a>
     ). This model has two improvements to R-CNN: (1) first use the convolutional layer to perform feature selection on the image so that only one convolutional layer can be used to obtain RP; (2) convert training multiple SVMs to use only one fully-connected layer and a softmax layer. These techniques greatly improve the computation speed but still fail to address the efficiency issue of the Selective Search Algorithm (SSA) for RP.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.2">
     Faster R-CNN is an improvement on the basis of Fast R-CNN. In order to solve the problem of SSA, the SSA that generates RP in Fast R-CNN is replaced by a Region Proposal Network (RPN) and uses a model that integrates RP generation, feature extraction, object classification and object box regression. RPN is a fully convolutional network that simultaneously predicts object boundaries at each location. RPN is trained end-to-end to generate high-quality region proposals, which are then detected by Fast R-CNN. At the same time, RPN and Fast R-CNN share convolutional features. Meanwhile, in the feature extraction stage, Faster R-CNN uses a convolutional neural network. The model achieves
     <math alttext="73.2\%" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1">
      <semantics id="S3.SS1.p2.1.m1.1a">
       <mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">
        <mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">
         73.2
        </mn>
        <mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b">
        <apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">
         <csymbol cd="latexml" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS1.p2.1.m1.1.1.2.cmml" type="float" xref="S3.SS1.p2.1.m1.1.1.2">
          73.2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">
        73.2\%
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="70.4\%" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1">
      <semantics id="S3.SS1.p2.2.m2.1a">
       <mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">
        <mn id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">
         70.4
        </mn>
        <mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b">
        <apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">
         <csymbol cd="latexml" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS1.p2.2.m2.1.1.2.cmml" type="float" xref="S3.SS1.p2.2.m2.1.1.2">
          70.4
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">
        70.4\%
       </annotation>
      </semantics>
     </math>
     mean Average Precision (mAP) per category on the PASCAL VOC 2007 and 2012 datasets, respectively. Faster R-CNN has been greatly improved in speed than Fast R-CNN, and the accuracy has reached the state-of-the-art (SOTA), and it also fully developed an end-to-end object detection framework. However, Faster R-CNN still cannot achieve real-time object detection. Besides, after obtaining RP, it requires heavy computation for each RP classification.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F8">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="269" id="S3.F8.g1" src="/html/2312.05019/assets/figures/faster_RCNN.png" width="299"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F8.2.1.1" style="font-size:90%;">
       Figure 8
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F8.3.2" style="font-size:90%;">
      Faster RCNN neural network architecture
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib66" title="">
        66
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    One-stage Algorithms
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     One-stage algorithms such as the Single Shot Multibox Detector (SSD) model
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib67" title="">
       67
      </a>
      ]
     </cite>
     and the YOLO series models
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib68" title="">
       68
      </a>
      ]
     </cite>
     are generally slightly less accurate than the two-stage algorithms, but have simpler architectures, which can facilitate end-to-end training and are more suitable for real-time object detection. The basic process of YOLO (see Fig.
     <a class="ltx_ref" href="#S3.F9" title="Figure 9 ‣ 3.2 One-stage Algorithms ‣ 3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     ) is divided into three phases, namely, zooming the image, passing the image through a full convolutional neural network, and using maximum value suppression (NMS). The main advantages of the YOLO model are that it is fast, with few background errors via global processing, and it has good generalization performance. Meanwhile, YOLO can formulate the detection task as a unified, end-to-end regression problem, and simultaneously obtain the location and classification by processing the image only once. But there are also some problems with the YOLO, such as rough mesh, which will limit YOLO’s performance over small objects. However, the subsequent YOLOv3, YOLOv5, YOLOX
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib69" title="">
       69
      </a>
      ]
     </cite>
     and YOLOv8 improved the network on the basis of the original YOLO and achieved better detection results.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F9">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="122" id="S3.F9.g1" src="/html/2312.05019/assets/figures/yolo.jpg" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F9.2.1.1" style="font-size:90%;">
       Figure 9
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F9.3.2" style="font-size:90%;">
      YOLO network architecture
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib68" title="">
        68
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S3.F10">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S3.F10.g1" src="/html/2312.05019/assets/figures/ssd_architecture.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F10.2.1.1" style="font-size:90%;">
       Figure 10
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F10.3.2" style="font-size:90%;">
      SSD network architecture
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib67" title="">
        67
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     SSD is another classic one-stage object detection algorithm. The flowchart of SSD is (1) first to extract features from the image through a CNN, (2) generate feature maps, (3) extract feature maps of multiple layers, and then (4) generate default boxes at each point of the feature map. Finally, (5) all the generated default boxes are collected and filtered using NMS. The neural network architecture of SSD is shown in Fig.
     <a class="ltx_ref" href="#S3.F10" title="Figure 10 ‣ 3.2 One-stage Algorithms ‣ 3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     . SSD uses more convolutional layers than YOLO, which means that SSD has more feature maps. At the same time, SSD uses different convolution segments based on the VGG model to output feature maps to the regressor, which tries to improve the detection accuracy over small objects.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     The aforementioned multi-stage algorithms and one-stage algorithms have their own advantages and disadvantages. Multi-stage algorithms achieve high detection accuracy, but bring more computing overhead and repeated detection. The one-stage model generally consists of a basic network (Backbone Network) and a Detection Head. The former is used as a feature extractor to give representations of different sizes and abstraction levels of images; the latter one learns classification and location associations based on these representations and a supervised dataset. The two tasks of category prediction and position regression, which are responsible for detecting the head, are often carried out in parallel, formulating a multi-task loss function for joint training. There is only one class prediction and position regression, and most of the weights are shared. Hence, one-stage algorithms are more time-efficient at the cost of accuracy. In any case, with the continuous development of deep learning in the field of computer vision, object detection algorithms are also constantly learning from and improving each other.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Vision Transformer
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     ViTs have emerged as the most active research field in object detection tasks recently, with models like Swin-Transformer
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib62" title="">
       62
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib70" title="">
       70
      </a>
      ]
     </cite>
     , ViTdet
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib71" title="">
       71
      </a>
      ]
     </cite>
     , and DINO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib72" title="">
       72
      </a>
      ]
     </cite>
     leading the forefront. Unlike conventional CNNs, ViTs leverage self-attention mechanisms to process image patches as sequences, offering a more flexible representation of spatial hierarchies. The core mechanism of these models involves dividing an image into a sequence of patches and applying Transformer encoders
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib73" title="">
       73
      </a>
      ]
     </cite>
     to capture complex dependencies between them. This process enables ViTs to efficiently learn global context, which is pivotal in understanding comprehensive scene layouts and object relations. For instance, the Swin-Transformer
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib62" title="">
       62
      </a>
      ]
     </cite>
     introduces a hierarchical structure with shifted windows, enhancing the model’s ability to capture both local and global features. In the following, the Swin-Transformer was scaled to Swin-Transformer V2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib70" title="">
       70
      </a>
      ]
     </cite>
     with the capability of training high-resolution images (see Fig.
     <a class="ltx_ref" href="#S3.F11" title="Figure 11 ‣ 3.3 Vision Transformer ‣ 3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     ).
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F11">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="181" id="S3.F11.g1" src="/html/2312.05019/assets/figures/swint2.png" width="329"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F11.2.1.1" style="font-size:90%;">
       Figure 11
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F11.3.2" style="font-size:90%;">
      Swin Transformer V2 framework
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib70" title="">
        70
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     The primary advantages of ViTs in object detection are their scalability to large datasets and superior performance in capturing long-range dependencies. This makes them particularly effective in scenarios where contextual understanding is crucial. Additionally, ViTs demonstrate strong transfer learning capabilities, performing well across various domains with minimal fine-tuning. However, challenges with ViTs include their computational intensity due to self-attention mechanisms, particularly when processing high-resolution images. This can limit their deployment in real-time applications where computational resources are constrained. Additionally, ViTs often require large-scale datasets for pre-training to achieve optimal performance, which can be a limitation in data-scarce environments. Despite these challenges, ongoing advancements in ViT architectures, such as the development of efficient attention mechanisms
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib74" title="">
       74
      </a>
      ]
     </cite>
     and hybrid CNN-Transformer models
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib75" title="">
       75
      </a>
      ]
     </cite>
     , continue to enhance their applicability and performance in diverse object detection tasks.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F12">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="446" id="S3.F12.g1" src="/html/2312.05019/assets/figures/air-air_object_detection.png" width="538"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F12.2.1.1" style="font-size:90%;">
       Figure 12
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F12.3.2" style="font-size:90%;">
      Air-to-air object detection of micro-UAVs with a monocular camera
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib76" title="">
        76
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS3.p3">
    <p class="ltx_p" id="S3.SS3.p3.1">
     When applying object detection algorithms to drone applications, it is necessary to find the best balance between computation speed and accuracy. Besides, massive drone datasets are required for training and testing. Zheng Ye
     <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.1">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib76" title="">
       76
      </a>
      ]
     </cite>
     collected an air-to-air drone dataset “Det-Fly” (see Fig.
     <a class="ltx_ref" href="#S3.F12" title="Figure 12 ‣ 3.3 Vision Transformer ‣ 3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       12
      </span>
     </a>
     ) and evaluated air-to-air object detection of a micro-UAV with eight different object detection algorithms, namely RetinaNet
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib77" title="">
       77
      </a>
      ]
     </cite>
     , SSD, Faster R-CNN, YOLOv3
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib78" title="">
       78
      </a>
      ]
     </cite>
     , FPN
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib79" title="">
       79
      </a>
      ]
     </cite>
     , Cascade R-CNN
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib80" title="">
       80
      </a>
      ]
     </cite>
     and Grid R-CNN
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib81" title="">
       81
      </a>
      ]
     </cite>
     . The evaluation results in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib76" title="">
       76
      </a>
      ]
     </cite>
     showed that the overall performance of Cascade R-CNN and Grid R-CNN is superior compared to the others. However, the YOLOv3 provides the fastest inference speed among others. Wei Xun
     <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.2">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib82" title="">
       82
      </a>
      ]
     </cite>
     conducted another investigation into drone detection, employing the YOLOv3 architecture and deploying the model on the NVIDIA Jetson TX2 platform. They collected a dataset comprising 1435 images featuring various UAVs, including drones, hexacopters, and quadcopters. Utilizing custom-trained weights, the YOLOv3 model demonstrated proficiency in drone detection within images. However, the deployment of this trained model faced constraints due to the limited computation capacity of the Jetson TX2, which posed challenges for effective real-time application.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F13">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="142" id="S3.F13.g1" src="/html/2312.05019/assets/figures/methods.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F13.2.1.1" style="font-size:90%;">
       Figure 13
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F13.3.2" style="font-size:90%;">
      Vision-based control methods for drone applications. Based on the ways of visual perception and control, the methods can be divided into indirect methods, semi-direct methods, and end-to-end methods.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS3.p4">
    <p class="ltx_p" id="S3.SS3.p4.1">
     In agile flight, computation speed is more important than accuracy since real-time object detection is required to avoid obstacles swiftly. Therefore, a simple gate detector and a filter algorithm are adopted as the basis of the visual perception of Swift
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     . Considering the agility of the drone, a stabilization module is required to obtain more robust and accurate object detection and tracking results in real-time flight. Moreover, in the drone datasets covered in existing works, each image only includes a single UAV. To classify and detect different classes of drones in multi-drone systems, a new dataset of multiple types of drones has to be built from scratch. Furthermore, the dataset can be adapted to capture adversary drones in omnidirectional visual perception to enhance avoidance capability.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Vision-based Control
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    Vision-based control for robotics has been widely studied in recent years, whether for ground robots or aerial robotics such as drones. For drones flying in a GPS-denied environment, visual inertial odometry (VIO)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib39" title="">
      39
     </a>
     ]
    </cite>
    and visual simultaneous localization and mapping systems (SLAM)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib40" title="">
      40
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib41" title="">
      41
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib42" title="">
      42
     </a>
     ]
    </cite>
    have been preferred choices for navigation. Meanwhile, in a clustered environment, research on obstacle avoidance
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib28" title="">
      28
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib83" title="">
      83
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib84" title="">
      84
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    based on visual perception has attracted much attention in the past few years. Obstacle avoidance has been a main task for vision-based control as well as for the current learning algorithms of drones.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    From the perspective of how drones obtain visual perception (perception end) and how drones generate control commands from visual perception (control end), existing vision-based control methods can be categorized into indirect methods, semi-direct methods, and end-to-end methods. The relationship between these three categories is illustrated in Fig.
    <a class="ltx_ref" href="#S3.F13" title="Figure 13 ‣ 3.3 Vision Transformer ‣ 3 Object Detection with Visual Perception ‣ Vision-Based Learning for Drones: A Survey">
     <span class="ltx_text ltx_ref_tag">
      13
     </span>
    </a>
    . In the following, we will discuss and evaluate these methods in three different categories, respectively.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Indirect Methods
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     Indirect methods
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib85" title="">
       85
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib86" title="">
       86
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib87" title="">
       87
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib88" title="">
       88
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib89" title="">
       89
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib90" title="">
       90
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib91" title="">
       91
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib92" title="">
       92
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib93" title="">
       93
      </a>
      ]
     </cite>
     refer to extracting features from images or videos to generate visual odometry, depth maps, and 3D point cloud maps for drones to perform path planning based on traditional optimization algorithms (see Fig.
     <a class="ltx_ref" href="#S4.F14" title="Figure 14 ‣ 4.1 Indirect Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     ). Obstacle states, such as 3D shape, position, and velocity, are detected and mapped before a maneuver is taken. Once online maps are built or obstacles are located, the drone can generate a feasible path or take actions to avoid obstacles.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     SOTA indirect methods generally divide the mission into several subtasks, namely
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">
      perception
     </span>
     ,
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">
      mapping
     </span>
     and
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.3">
      planning
     </span>
     . On the perception side, depth images are always required to generate corresponding distance and position information for navigation. A depth image is a grey-level or color image that can represent the distance between the surfaces of objects from the viewpoint of the agent. Fig.
     <a class="ltx_ref" href="#S4.F15" title="Figure 15 ‣ 4.1 Indirect Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       15
      </span>
     </a>
     shows color images and corresponding depth images from a drone’s viewpoint. The illuminance is proportional to the distance from the camera. A lighter color denotes a nearer surface, and darker areas mean further surfaces. A depth map provides the necessary distance information for drones to make decisions to avoid static and dynamic obstacles. Currently, off-the-shelf RGB-D cameras, such as the Intel RealSense depth camera D415, the ZED 2 stereo camera, and the Structure Core depth camera, are widely used for drone applications. Therefore, traditional obstacle avoidance methods can treat depth information as a direct input. However, for omnidirectional perception in wide-view scenarios, efficient onboard monocular depth estimation is always required, which is a challenge to address with existing methods.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     On the mapping side, a point cloud map
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib94" title="">
       94
      </a>
      ]
     </cite>
     or Octmap
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib95" title="">
       95
      </a>
      ]
     </cite>
     , representing a set of data points in a 3D space is commonly generated. Each point has its own Cartesian coordinates and can be used to represent a 3D shape or an object. A 3D point cloud map is not from the view of a drone but constructs a global 3D map that provides global environmental information for a drone to fly. The point-cloud map can be generated from a LIDAR scanner or many overlapped images combined with depth information. An illustration of an original scene and a point cloud map are shown in Fig.
     <a class="ltx_ref" href="#S4.F14" title="Figure 14 ‣ 4.1 Indirect Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     , where the drone can travel around without colliding with static obstacles.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     Planning is a basic requirement for a vision-based drone to avoid obstacles. Within the indirect methods, planning can be further divided into two categories: one is offline methods based on high-resolution maps and pre-known position information, such as Dijkstra’s algorithm
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib96" title="">
       96
      </a>
      ]
     </cite>
     , A-star
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib97" title="">
       97
      </a>
      ]
     </cite>
     , RRT-connect
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib98" title="">
       98
      </a>
      ]
     </cite>
     and sequential convex optimization
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib99" title="">
       99
      </a>
      ]
     </cite>
     ; the other is online methods based on real-time visual perception and decision-making. Online methods can be further categorized into online path planning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib90" title="">
       90
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib91" title="">
       91
      </a>
      ]
     </cite>
     and artificial potential field (APF) methods
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib100" title="">
       100
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p5">
    <p class="ltx_p" id="S4.SS1.p5.2">
     Most vision-based drones rely on online methods. Compared to offline methods, which require an accurate pre-built global map, online methods provide advanced maneuvering capabilities for drones, especially in a dynamic environment. Currently, due to the advantages of optimization and prediction capabilities, online path planning methods have become the preferred choice for drone obstacle avoidance. For instance, in the SOTA work
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib90" title="">
       90
      </a>
      ]
     </cite>
     , Zhou Boyu
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p5.2.1">
      et al.
     </span>
     introduced a robust and efficient motion planning system called Fast-Planner for a vision-based drone to perform high-speed flight in an unknown cluttered environment. The key contributions of this work are a robust and efficient planning scheme incorporating path searching, B-spline optimization, and time adjustment to generate feasible and safe trajectories for vision-based drones’ obstacle avoidance. Using only onboard vision-based perception and computing, this work demonstrated agile drone navigation in unexplored indoor and outdoor environments. However, this approach can only achieve maximum speeds of
     <math alttext="3m/s" class="ltx_Math" display="inline" id="S4.SS1.p5.1.m1.1">
      <semantics id="S4.SS1.p5.1.m1.1a">
       <mrow id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">
        <mrow id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">
         <mn id="S4.SS1.p5.1.m1.1.1.2.2" xref="S4.SS1.p5.1.m1.1.1.2.2.cmml">
          3
         </mn>
         <mo id="S4.SS1.p5.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="S4.SS1.p5.1.m1.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="S4.SS1.p5.1.m1.1.1.2.3" xref="S4.SS1.p5.1.m1.1.1.2.3.cmml">
          m
         </mi>
        </mrow>
        <mo id="S4.SS1.p5.1.m1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.cmml">
         /
        </mo>
        <mi id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml">
         s
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b">
        <apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">
         <divide id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1">
         </divide>
         <apply id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">
          <times id="S4.SS1.p5.1.m1.1.1.2.1.cmml" xref="S4.SS1.p5.1.m1.1.1.2.1">
          </times>
          <cn id="S4.SS1.p5.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS1.p5.1.m1.1.1.2.2">
           3
          </cn>
          <ci id="S4.SS1.p5.1.m1.1.1.2.3.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3">
           𝑚
          </ci>
         </apply>
         <ci id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3">
          𝑠
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">
        3m/s
       </annotation>
      </semantics>
     </math>
     and requires
     <math alttext="7.3ms" class="ltx_Math" display="inline" id="S4.SS1.p5.2.m2.1">
      <semantics id="S4.SS1.p5.2.m2.1a">
       <mrow id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">
        <mn id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">
         7.3
        </mn>
        <mo id="S4.SS1.p5.2.m2.1.1.1" lspace="0em" rspace="0em" xref="S4.SS1.p5.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml">
         m
        </mi>
        <mo id="S4.SS1.p5.2.m2.1.1.1a" lspace="0em" rspace="0em" xref="S4.SS1.p5.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S4.SS1.p5.2.m2.1.1.4" xref="S4.SS1.p5.2.m2.1.1.4.cmml">
         s
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b">
        <apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">
         <times id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1.1">
         </times>
         <cn id="S4.SS1.p5.2.m2.1.1.2.cmml" type="float" xref="S4.SS1.p5.2.m2.1.1.2">
          7.3
         </cn>
         <ci id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3">
          𝑚
         </ci>
         <ci id="S4.SS1.p5.2.m2.1.1.4.cmml" xref="S4.SS1.p5.2.m2.1.1.4">
          𝑠
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">
        7.3ms
       </annotation>
      </semantics>
     </math>
     for computation in each step. To improve the flight performance and save computation time, Zhou Xin
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p5.2.2">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     provided a Euclidean Signed Distance Field (ESDF)-free gradient-based planning framework solution, EGO-Planner, for drone autonomous navigation in unknown obstacle-rich situations. Compared to the Fast-Planner, the EGO-Planner achieved faster speeds and saved a lot of computation time. However, these online path planning methods require bulky visual sensors, such as RGBD cameras or LIDAR, and a powerful onboard computer for the complex numerical calculation to obtain a local or global optimal trajectory.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F14">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="94" id="S4.F14.g1" src="/html/2312.05019/assets/figures/indirect_method.png" width="329"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F14.2.1.1" style="font-size:90%;">
       Figure 14
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F14.3.2" style="font-size:90%;">
      A vision-based drone is traversing through a cluttered indoor environment with generated point cloud maps and online path planning
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib90" title="">
        90
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S4.F15">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S4.F15.g1" src="/html/2312.05019/assets/figures/depth_image.png" width="329"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F15.2.1.1" style="font-size:90%;">
       Figure 15
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F15.3.2" style="font-size:90%;">
      Depth maps generated from the viewpoint of a drone
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib101" title="">
        101
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS1.p6">
    <p class="ltx_p" id="S4.SS1.p6.1">
     In contrast to online path planning, the artificial potential field methods require less computation resources and can well cope with dynamic obstacle avoidance using limited sensor information. The APF algorithm is one of the algorithms in the robot path planning approach that uses attractive force to achieve the objective position and repulsive force to avoid obstacles in an unknown environment
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib102" title="">
       102
      </a>
      ]
     </cite>
     . Falanga
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p6.1.1">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     developed an efficient and fast control strategy based on the artificial potential field method to avoid fast approaching dynamic obstacles. The obstacles in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     are represented as repulsive fields that decay over time, and the repulsive forces are generated from the first-order derivation of the repulsive fields at each time step. However, the repulsive forces computed only reach substantial values when the obstacle is very close, which may lead to unstable and aggressive behavior. Besides, artificial potential field methods are heuristic methods that cannot guarantee global optimization and robustness for drones. Hence, it is not ideal to adopt potential field methods to navigate through cluttered environments.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F16">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F16.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S4.F16.sf1.g1" src="/html/2312.05019/assets/figures/APF_fields.jpg" width="534"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S4.F16.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F16.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S4.F16.sf2.g1" src="/html/2312.05019/assets/figures/APF_fields_path_planning.jpg" width="628"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S4.F16.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F16.2.1.1" style="font-size:90%;">
       Figure 16
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F16.3.2" style="font-size:90%;">
      Artificial potential field (APF) methods in drones’ obstacle avoidance
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib100" title="">
        100
       </a>
       ]
      </cite>
      (a) Generated artificial potential field; (b) Flight trajectory based on the APF.
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    End-to-end Methods
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     In contrast to the indirect methods, which divide the whole mission into multiple sub-tasks, such as perception, mapping, and planning, end-to-end methods
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib103" title="">
       103
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib104" title="">
       104
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib105" title="">
       105
      </a>
      ]
     </cite>
     combine computer vision and reinforcement learning (RL) to map the visual observations to actions directly. RL
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib106" title="">
       106
      </a>
      ]
     </cite>
     is a technique for mapping the state (observation) space to the action space in order to maximize a long-term return with given rewards. The learner is not explicitly told what action to carry out but must figure out which action will yield the highest reward during the exploring process. A typical RL model (see Fig.
     <a class="ltx_ref" href="#S4.F18" title="Figure 18 ‣ 4.2 End-to-end Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       18
      </span>
     </a>
     ) features agents, the environment, reward functions, action, and state space. The policy model achieves convergent status via constant interactions between the agents and the environment, where the reward function guides the training process.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F17">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F17.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S4.F17.sf1.g1" src="/html/2312.05019/assets/figures/drl1.jpg" width="638"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S4.F17.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F17.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S4.F17.sf2.g1" src="/html/2312.05019/assets/figures/drl2.png" width="625"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S4.F17.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F17.2.1.1" style="font-size:90%;">
       Figure 17
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F17.3.2" style="font-size:90%;">
      Quadrotor drone flying in a wild environment with end-to-end reinforcement learning
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib28" title="">
        28
       </a>
       ]
      </cite>
      . (a) Training process in the simulation platform; (b) Real flight test in the wild snow environment.
     </span>
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S4.F18">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="185" id="S4.F18.g1" src="/html/2312.05019/assets/figures/reinforcement-learning.jpg" width="479"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F18.2.1.1" style="font-size:90%;">
       Figure 18
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F18.3.2" style="font-size:90%;">
      The reinforcement learning process.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     With end-to-end methods, the visual perception of drones is encoded by a deep neural network into an observation vector of the policy network. The mapping process is usually trained offline with abundant data, which requires high-performance computers and simulation platforms. The training data set is collected from expert flight demonstrations for imitation learning or from simulations for online training. To better generalize the performance of a trained neural network model, scenario randomization (domain randomization) is essential during the training process. Malik Aqeel Anwar
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib104" title="">
       104
      </a>
      ]
     </cite>
     presented an end-to-end reinforcement learning approach called NAVREN-RL to navigate a quadrotor drone in an indoor environment with expert data and knowledge-based data aggregation. The reward function in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib104" title="">
       104
      </a>
      ]
     </cite>
     was formulated from a ground truth depth image and a generated depth image. Loquercio
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     developed an end-to-end approach that can autonomously guide a quadrotor drone through complex wild and human-made environments at high speeds with purely onboard visual perception (depth image) and computation. The neural network policy was trained in a high-fidelity simulation environment with massive expert knowledge data. While end-to-end methods provide us with a straightforward way to generate obstacle avoidance policies for drones, they require massive training data with domain randomization (usually counted in the millions) to obtain acceptable generalization capabilities. Meanwhile, without expert knowledge data, it is challenging for the neural network policy to update its weights when the reward space is sparse. To implement end-to-end methods, we commonly consider the following aspects: neural network architecture, training process, and Sim2Real transfer.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F19">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F19.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="S4.F19.sf1.g1" src="/html/2312.05019/assets/figures/nn-target.png" width="579"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S4.F19.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F19.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="S4.F19.sf2.g1" src="/html/2312.05019/assets/figures/nn-target2.png" width="550"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S4.F19.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F19.2.1.1" style="font-size:90%;">
       Figure 19
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F19.3.2" style="font-size:90%;">
      The neural network architectures of end-to-end reinforcement learning methods. (a) Target driven navigation with ResNet50 encoder
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
       ]
      </cite>
      ; (b) Target driven visual navigation for a robot with shallow CNN encoder
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib50" title="">
        50
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">
      Neural Network Architecture:
     </span>
     The neural network architecture is the core component in the end-to-end methods, which determines the computation efficiency and intelligence level of the policy. In the end-to-end method, the input of the neural network architecture is the image raw data (RGB/RGBD), and the output is the action vector an agent needs to take. The images are encoded into a vector and then concatenated with other normalized observations to form an input vector of a policy network. For the image encoder, there are many pre-trained neural network architectures that can be considered, such as ResNet
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib107" title="">
       107
      </a>
      ]
     </cite>
     , VGG
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib108" title="">
       108
      </a>
      ]
     </cite>
     , and nature Convolutional Neural Network (CNN)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib109" title="">
       109
      </a>
      ]
     </cite>
     . Zhu
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ]
     </cite>
     developed a target-driven visual navigation approach for robots with end-to-end deep reinforcement learning, where a pre-trained ResNet-50 is used to encode the image into a feature vector. In
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
      ]
     </cite>
     , a more data-efficient image encoder with 5 layers was designed for target-driven visual navigation for robots with end-to-end imitation learning. Before designing the neural network architecture, we first need to determine the observation space and action space of the task. The training efficiency and space complexity are the two aspects we need to consider in the designing process.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">
      Training Process:
     </span>
     The training process is the most time-consuming part of the end-to-end learning methods. It requires the use of gradient information from the loss function to update the weights of the neural network and improve the policy model. There are two main training algorithms for reinforcement learning, namely the on-policy algorithm and the off-policy algorithm. On-policy algorithms attempt to evaluate or improve the policy that is used to make decisions, i.e., the behavior policy for generating actions is the same as the target policy for learning. When the agent is exploring the environment, on-policy algorithms are more stable than off-policy algorithms when the agent is exploring the environment. SARSA (State-Action-Reward-State-Action experiences to update the Q-values)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib106" title="">
       106
      </a>
      ]
     </cite>
     is an on-policy reinforcement learning algorithm that estimates the value function of the policy being carried out. PPO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib110" title="">
       110
      </a>
      ]
     </cite>
     is another efficient on-policy algorithm widely used in reinforcement learning. By compensating for the fact that more probable actions are going to be taken more often, PPO addresses the issue of high variance and low convergence with policy gradient methods. To avoid large weights’ vibration, a “clipped”
policy gradient update formulation is designed.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F20">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F20.g1" src="/html/2312.05019/assets/figures/nn-target3.jpg" width="583"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F20.2.1.1" style="font-size:90%;">
       Figure 20
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F20.3.2" style="font-size:90%;">
      Training process used in
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib28" title="">
        28
       </a>
       ]
      </cite>
      to fly an agile drone through forest.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS2.p5">
    <p class="ltx_p" id="S4.SS2.p5.1">
     In contrast, off-policy algorithms evaluate or improve a policy different from that used to generate the action currently. The off-policy algorithms such as Q-learning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib111" title="">
       111
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib112" title="">
       112
      </a>
      ]
     </cite>
     try to learn a greedy policy in every step. Off-policy algorithms perform better at movement predictions, especially in an unknown environment. However, off-policy algorithms can be very unstable due to the unstatic environments the offline data seldom covers. Imitation learning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib113" title="">
       113
      </a>
      ]
     </cite>
     is another form of off-policy algorithm that tries to mimic demonstrated behavior in a given task. Through training from demonstrations, imitation learning can save remarkable exploration costs.
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     adopted imitation learning to speed up the training process. The whole training process is illustrated in Fig.
     <a class="ltx_ref" href="#S4.F20" title="Figure 20 ‣ 4.2 End-to-end Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       20
      </span>
     </a>
     . With this privileged expert knowledge, the policy can be trained to find a time-efficient trajectory to avoid obstacles.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p6">
    <p class="ltx_p" id="S4.SS2.p6.1">
     For multi-agent systems, multi-agent reinforcement learning (MARL) is widely studied to encourage agent collaboration. In MARL, the credit assignment is a crucial issue that determines the contribution of each agent to the group’s success or failure. COMA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib114" title="">
       114
      </a>
      ]
     </cite>
     is a baseline method that uses a centralized critic to estimate the action-value function, and for each agent, it computes a counterfactual advantage function to represent the value difference, which can determine the contribution of each agent. However, COMA still does not fully address the model complexity issue. QMIX
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib115" title="">
       115
      </a>
      ]
     </cite>
     is well developed to address the scalability issue by decomposing the global action-value function into individual agent’s value functions. However, QMIX assumes the environment is fully observable and may not be able to handle scenarios with continuous action space. Hence, attention mechanism-enabled MARL is a promising direction to address the variable observations. Besides, to balance individual and team reward, a MARL with mixed credit assignment algorithm, POCA-Mix, was proposed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib116" title="">
       116
      </a>
      ]
     </cite>
     to achieve collaborative multi-target search with a visual drone swarm.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p7">
    <p class="ltx_p" id="S4.SS2.p7.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.1">
      Sim2Real Transfer:
     </span>
     For complex tasks, the policy neural networks are usually trained on simulation platforms such as AirSim
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib117" title="">
       117
      </a>
      ]
     </cite>
     , Unity
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib118" title="">
       118
      </a>
      ]
     </cite>
     or Gazebo
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib119" title="">
       119
      </a>
      ]
     </cite>
     . The differences between the simulation and the real environment are non-negligible. Sim2Real is the way to deploy the neural network model trained in a simulation environment on a real physical agent. For deployment, it is essential to validate the generalization capability of trained neural network models. A wide variety of Sim2Real techniques
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib120" title="">
       120
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib121" title="">
       121
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib122" title="">
       122
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib123" title="">
       123
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib124" title="">
       124
      </a>
      ]
     </cite>
     have been developed to improve the generalization and transfer capabilities of models. Domain randomization
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib123" title="">
       123
      </a>
      ]
     </cite>
     is one of these techniques to improve the generalization capability of the trained neural network to unseen environments. Domain randomization is a method of trying to discover a representation that can be used in a variety of scenes or domains. Existing domain randomization techniques
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib54" title="">
       54
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     for drones’ obstacle avoidance include position randomization, depth image noise randomization, texture randomization, size randomization, etc. Therefore, we need to apply domain randomization in our training process to enhance the generalization capability of the trained model in deployment.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F21">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F21.g1" src="/html/2312.05019/assets/figures/semi-direct.png" width="449"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F21.2.1.1" style="font-size:90%;">
       Figure 21
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F21.3.2" style="font-size:90%;">
      A drone avoids the obstacles in the forest with semi-direct methods
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib83" title="">
        83
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Semi-direct Methods
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Compared to the end-to-end methods, which generate actions from image raw data directly, semi-direct methods
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib83" title="">
       83
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib125" title="">
       125
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib126" title="">
       126
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ]
     </cite>
     introduce an intermediate phase for drones to take actions from visual perception, aiming to improve the generalization and transfer capabilities of the methods over unseen environments. There are two ways to design the semi-direct method architectures: one is to generate the required information from image processing (such as the relative positions of obstacles from object detection and tracking or the point cloud map) and train the control policy with deep reinforcement learning; another is to obtain the required states (such as depth image) directly from the raw image data with deep learning and avoid the obstacles using numerical or heuristic methods. These two methods can be denoted as indirect (front end)-direct (back end) methods and direct (front end)-indirect (back end) methods.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">
       Table 1
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">
      Summary of Vision-Based Control Methods in Drone Technology
     </span>
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T1.4.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.1.1.1">
        <span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1">
         Method Type
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.1.1.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.1.2.1">
         <span class="ltx_p" id="S4.T1.4.1.1.2.1.1" style="width:65.0pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.2.1.1.1">
           Perception End
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.1.1.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.1.3.1">
         <span class="ltx_p" id="S4.T1.4.1.1.3.1.1" style="width:65.0pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.3.1.1.1">
           Control End
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.1.1.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.1.4.1">
         <span class="ltx_p" id="S4.T1.4.1.1.4.1.1" style="width:43.4pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.4.1.1.1">
           Key Studies
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.1.1.5">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.1.1.5.1">
         <span class="ltx_p" id="S4.T1.4.1.1.5.1.1" style="width:151.8pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.5.1.1.1">
           Description and Applicability
          </span>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.4.2.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.2.2.1" rowspan="2">
        <span class="ltx_text" id="S4.T1.4.2.2.1.1">
         Indirect Methods
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.2.2.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.2.2.1">
         <span class="ltx_p" id="S4.T1.4.2.2.2.1.1" style="width:65.0pt;">
          Depth maps, 3D point cloud maps
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.2.2.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.2.3.1">
         <span class="ltx_p" id="S4.T1.4.2.2.3.1.1" style="width:65.0pt;">
          Traditional optimization algorithms
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.2.2.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.2.4.1">
         <span class="ltx_p" id="S4.T1.4.2.2.4.1.1" style="width:43.4pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib85" title="">
            85
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib86" title="">
            86
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib87" title="">
            87
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib91" title="">
            91
           </a>
           ]
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.2.2.5">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.2.2.5.1">
         <span class="ltx_p" id="S4.T1.4.2.2.5.1.1" style="width:151.8pt;">
          Focus on generating visual odometry, depth maps, and 3D point cloud maps for path planning. Suitable for safety-critical tasks with accurate models.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.4.3.3">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.3.3.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.3.1.1">
         <span class="ltx_p" id="S4.T1.4.3.3.1.1.1" style="width:65.0pt;">
          Depth maps, 3D point cloud maps
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.3.3.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.3.2.1">
         <span class="ltx_p" id="S4.T1.4.3.3.2.1.1" style="width:65.0pt;">
          Online path planning, APF, etc.
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.3.3.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.3.3.1">
         <span class="ltx_p" id="S4.T1.4.3.3.3.1.1" style="width:43.4pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib27" title="">
            27
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib92" title="">
            92
           </a>
           ]
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.3.3.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.3.3.4.1">
         <span class="ltx_p" id="S4.T1.4.3.3.4.1.1" style="width:151.8pt;">
          Utilize depth information for real-time visual perception and decision-making in dynamic environments. Suitable for rapid response tasks with certain information.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.4.4.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.4.4.1" rowspan="2">
        <span class="ltx_text" id="S4.T1.4.4.4.1.1">
         End-to-End Methods
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.4.4.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.4.2.1">
         <span class="ltx_p" id="S4.T1.4.4.4.2.1.1" style="width:65.0pt;">
          Visual observations encoded by DNNs
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.4.4.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.4.3.1">
         <span class="ltx_p" id="S4.T1.4.4.4.3.1.1" style="width:65.0pt;">
          RL for action mapping
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.4.4.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.4.4.1">
         <span class="ltx_p" id="S4.T1.4.4.4.4.1.1" style="width:43.4pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib56" title="">
            56
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib103" title="">
            103
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib104" title="">
            104
           </a>
           ]
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.4.4.5">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.4.4.5.1">
         <span class="ltx_p" id="S4.T1.4.4.4.5.1.1" style="width:151.8pt;">
          Combine deep learning for visual perception with RL for direct action response. Suitable for complex tasks with uncertain information.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.4.5.5">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.5.5.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.5.1.1">
         <span class="ltx_p" id="S4.T1.4.5.5.1.1.1" style="width:65.0pt;">
          Encoded visual observations
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.5.5.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.5.2.1">
         <span class="ltx_p" id="S4.T1.4.5.5.2.1.1" style="width:65.0pt;">
          Imitation learning and online training
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.5.5.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.5.3.1">
         <span class="ltx_p" id="S4.T1.4.5.5.3.1.1" style="width:43.4pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib127" title="">
            127
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib28" title="">
            28
           </a>
           ]
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.5.5.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.5.5.4.1">
         <span class="ltx_p" id="S4.T1.4.5.5.4.1.1" style="width:151.8pt;">
          Use deep learning for visual encoding and train the control system with expert demonstrations and online training. Suitable for sample-efficient tasks with expert demonstrations.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.4.6.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.4.6.6.1" rowspan="2">
        <span class="ltx_text" id="S4.T1.4.6.6.1.1">
         Semi-Direct Methods
        </span>
       </th>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.6.6.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.6.6.2.1">
         <span class="ltx_p" id="S4.T1.4.6.6.2.1.1" style="width:65.0pt;">
          Intermediate features from image processing
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.6.6.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.6.6.3.1">
         <span class="ltx_p" id="S4.T1.4.6.6.3.1.1" style="width:65.0pt;">
          DRL for action
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.6.6.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.6.6.4.1">
         <span class="ltx_p" id="S4.T1.4.6.6.4.1.1" style="width:43.4pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib83" title="">
            83
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib125" title="">
            125
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib15" title="">
            15
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib128" title="">
            128
           </a>
           ]
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.4.6.6.5">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.6.6.5.1">
         <span class="ltx_p" id="S4.T1.4.6.6.5.1.1" style="width:151.8pt;">
          Extract intermediate features like relative positions or velocities and use DRL for action decisions. Suitable for complex tasks with high generalization requirement.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.4.7.7">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.7.7.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.7.7.1.1">
         <span class="ltx_p" id="S4.T1.4.7.7.1.1.1" style="width:65.0pt;">
          Raw image data for depth images or obstacle tracking
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.7.7.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.7.7.2.1">
         <span class="ltx_p" id="S4.T1.4.7.7.2.1.1" style="width:65.0pt;">
          Numerical/Heuristic methods for obstacle avoidance
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.7.7.3">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.7.7.3.1">
         <span class="ltx_p" id="S4.T1.4.7.7.3.1.1" style="width:43.4pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib126" title="">
            126
           </a>
           ]
          </cite>
          ,
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib45" title="">
            45
           </a>
           ]
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.7.7.4">
        <span class="ltx_inline-block ltx_align_top" id="S4.T1.4.7.7.4.1">
         <span class="ltx_p" id="S4.T1.4.7.7.4.1.1" style="width:151.8pt;">
          Utilize direct image data to obtain necessary states for obstacle avoidance using non-learning-based methods. Suitable for tasks where robust stereo information is not available.
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">
      Indirect-direct methods
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib83" title="">
       83
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib125" title="">
       125
      </a>
      ]
     </cite>
     firstly obtain intermediate features such as relative position or velocity of the obstacles from image processing and then use this intermediate feature information as observations to train the policy neural network via deep reinforcement learning. Indirect-direct methods generally rely on designing suitable intermediate features. In
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib83" title="">
       83
      </a>
      ]
     </cite>
     , the features related to depth cues such as Radon features (30 dimensional), structure tensor statistics (15 dimensional), Laws’ masks (8 dimensional), and optical flow (5 dimensional) were extracted and concatenated into a single feature vector as visual observation. Together with the other nine additional features, the control policy was trained with imitation learning to navigate the drone through a dense forest environment (see Fig.
     <a class="ltx_ref" href="#S4.F21" title="Figure 21 ‣ 4.2 End-to-end Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       21
      </span>
     </a>
     ).
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F22">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="105" id="S4.F22.g1" src="/html/2312.05019/assets/figures/drone-chase.png" width="329"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F22.2.1.1" style="font-size:90%;">
       Figure 22
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F22.3.2" style="font-size:90%;">
      A drone is tracking and chasing a target drone with object detection and deep reinforcement learning
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib125" title="">
        125
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS3.p3">
    <p class="ltx_p" id="S4.SS3.p3.1">
     Moulay
     <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib125" title="">
       125
      </a>
      ]
     </cite>
     proposed a semi-direct vision-based learning control policy for UAV pursuit-evasion. Firstly, a deep object detector (YOLOv2) and a search area proposal (SAP) were used to predict the relative position of the target UAV in the next frame for target tracking. Afterward, deep reinforcement learning (see Fig.
     <a class="ltx_ref" href="#S4.F22" title="Figure 22 ‣ 4.3 Semi-direct Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       22
      </span>
     </a>
     ) was adopted to predict the actions the follower UAV needs to perform to track the target UAV. Indirect-direct methods are able to improve the generalization capability of policy neural networks, but at the cost of heavy computation and time overhead.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p4">
    <p class="ltx_p" id="S4.SS3.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">
      Direct-indirect methods
     </span>
     try to obtain depth images
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib129" title="">
       129
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib130" title="">
       130
      </a>
      ]
     </cite>
     and track obstacles
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib131" title="">
       131
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib132" title="">
       132
      </a>
      ]
     </cite>
     from training and use non-learning-based methods, such as path planning or APF to avoid obstacles. Direct-indirect methods can be applied to a microlight drone with only monocular vision, but they require a lot of training data to obtain depth images or 3D poses of the obstacles. Michele
     <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.2">
      et al.
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib132" title="">
       132
      </a>
      ]
     </cite>
     developed an object detection system to detect obstacles at a very long range and at a very high speed, without certain assumptions on the type of motion. With a deep neural network trained on real and synthetic image data, fast, robust and consistent depth information can be used for drones’ obstacle avoidance. Direct-indirect methods address the ego drift problem of monocular depth estimation using Structure from Motion (SfM) and provide a direct way to get depth information from the image. However, the massive train dataset and limited generalization capability are the main challenges for their further applications.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F23">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="97" id="S4.F23.g1" src="/html/2312.05019/assets/figures/depth_monocular.png" width="329"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F23.2.1.1" style="font-size:90%;">
       Figure 23
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F23.3.2" style="font-size:90%;">
      Via deep learning, a 3D space generated from the monocular image for obstacle avoidance
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib132" title="">
        132
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS3.p5">
    <p class="ltx_p" id="S4.SS3.p5.1">
     In summary, the field of vision-based control for drones encompasses a variety of methods, each with its own unique approach to perception and control. Indirect methods rely on traditional optimization algorithms and depth or 3D point cloud maps for navigation and obstacle avoidance. End-to-end methods leverage deep neural networks for visual perception and utilize reinforcement learning for direct action mapping. Semi-direct methods balance between computational efficiency and generalization by using intermediate features from image processing and a combination of DRL and heuristic methods for action generation. A comprehensive overview of these methods, along with key studies in each category, is summarized in Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.3 Semi-direct Methods ‣ 4 Vision-based Control ‣ Vision-Based Learning for Drones: A Survey">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , which provides a detailed comparison of their perception and control strategies.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Applications and Challenges
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Single Drone Application
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     The versatility of single drones is increasingly recognized in a variety of challenging environments. These unmanned vehicles, with their inherent advantages, are effectively employed in critical areas such as hazardous environment detection and search and rescue operations. Single drone applications in vision-based learning primarily involve tasks like obstacle avoidance
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib30" title="">
       30
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     , surveillance
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib133" title="">
       133
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib134" title="">
       134
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib135" title="">
       135
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib136" title="">
       136
      </a>
      ]
     </cite>
     , search-and-rescue operations
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib137" title="">
       137
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib138" title="">
       138
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib139" title="">
       139
      </a>
      ]
     </cite>
     , environmental monitoring
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib140" title="">
       140
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib141" title="">
       141
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib142" title="">
       142
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib143" title="">
       143
      </a>
      ]
     </cite>
     , industrial inspection
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib144" title="">
       144
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib145" title="">
       145
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib23" title="">
       23
      </a>
      ]
     </cite>
     and autonomous racing
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib146" title="">
       146
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib147" title="">
       147
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib148" title="">
       148
      </a>
      ]
     </cite>
     . Each field, while benefiting from the unique capabilities of drones, also presents its own set of challenges and areas for development.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S5.SS1.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.1.1
     </span>
     Obstacle Avoidance
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS1.p1">
     <p class="ltx_p" id="S5.SS1.SSS1.p1.1">
      The development of obstacle avoidance capabilities in drones, especially for vision-based control systems, poses significant challenges. Recent studies have primarily focused on static or simple dynamic environments, where obstacle paths are predictable
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib26" title="">
        26
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib28" title="">
        28
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib149" title="">
        149
       </a>
       ]
      </cite>
      . However, complex scenarios involving unpredictable physical attacks from birds or intelligent adversaries remain largely unaddressed. For instance,
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib26" title="">
        26
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib150" title="">
        150
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib30" title="">
        30
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib27" title="">
        27
       </a>
       ]
      </cite>
      have explored basic dynamic obstacle avoidance but do not account for adversarial environments. To effectively handle such threats, drones require advanced features like omnidirectional visual perception and agile maneuvering capabilities. Current research, however, is limited in addressing these needs, underscoring the necessity for further development in drone technology to enhance evasion strategies against smart, unpredictable adversaries.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS1.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.1.2
     </span>
     Surveillance
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS2.p1">
     <p class="ltx_p" id="S5.SS1.SSS2.p1.1">
      While drones play a pivotal role in surveillance tasks, their deployment is not without challenges. Key obstacles include managing high data processing loads and addressing the limitations of onboard computational resources. In addressing these challenges, the study by Singh
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib133" title="">
        133
       </a>
       ]
      </cite>
      presented a real-time drone surveillance system used to identify violent individuals in public areas. The proposed study was facilitated by cloud processing of drone images to address the challenge of slow and memory-intensive computations while still maintaining onboard short-term navigation capabilities. Additionally, in the study
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib136" title="">
        136
       </a>
       ]
      </cite>
      , a drone-based crowd surveillance system was tested to achieve the goal of saving scarce energy of the drone battery. This approach involved offloading video data processing from the drones by employing the Mobile Edge Computing (MEC) method. Nevertheless, while off-board processing diminishes computational demands and energy consumption, it inevitably heightens the need for data transmission. Addressing the challenge of achieving real-time surveillance in environments with limited signal connectivity is an additional critical issue that requires resolution.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS1.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.1.3
     </span>
     Search and Rescue
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS3.p1">
     <p class="ltx_p" id="S5.SS1.SSS3.p1.1">
      In the field of search and rescue operations, a number of challenges exist that hinder the development of drone technology. A primary challenge faced by drones is extracting maximum useful information from limited data sources. This is crucial for improving the efficiency and success rate of these missions. Goodrich
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS3.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib137" title="">
        137
       </a>
       ]
      </cite>
      address this by developing a contour search algorithm designed to optimize video data analysis, enhancing the capability to identify key elements swiftly. However, incorporating temporal information into this algorithm introduces additional computational demands. These increased requirements present new challenges, such as the need for more powerful processing capabilities and potentially greater energy consumption.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS1.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.1.4
     </span>
     Environmental Monitoring
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS4.p1">
     <p class="ltx_p" id="S5.SS1.SSS4.p1.1">
      A major challenge in the application of drones for environmental monitoring lies in efficiently collecting high-resolution data while navigating the constraints of battery life, flight duration, and diverse weather conditions. Addressing this, Senthilnath
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS4.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib140" title="">
        140
       </a>
       ]
      </cite>
      showcased the use of fixed-wing and VTOL (Vertical Take-Off and Landing) drones in vegetation analysis, focusing on the challenge of detailed mapping through spectral-spatial classification methods. In another study, Lu
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS4.p1.1.2">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib143" title="">
        143
       </a>
       ]
      </cite>
      demonstrated the utility of drones for species classification in grasslands, contributing to the development of methodologies for drone-acquired imagery processing, which is crucial for environmental assessment and management. While these studies represent significant steps in drone applications for environmental monitoring, several challenges persist. Future research may need to address the problems of improving the drones’ resilience to diverse environmental conditions, and extend their operational range and duration to comprehensively cover extensive and varied landscapes.
     </p>
    </div>
    <figure class="ltx_figure" id="S5.F24">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F24.sf1">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F24.sf1.g1" src="/html/2312.05019/assets/figures/single_application_1_surveillance.jpg" width="556"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S5.F24.sf1.2.1.1" style="font-size:90%;">
           (a)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F24.sf2">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F24.sf2.g1" src="/html/2312.05019/assets/figures/single_application_2_rescue.jpg" width="556"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S5.F24.sf2.2.1.1" style="font-size:90%;">
           (b)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_break">
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F24.sf3">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F24.sf3.g1" src="/html/2312.05019/assets/figures/single_application_3_environmental_monitoring.jpg" width="556"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S5.F24.sf3.2.1.1" style="font-size:90%;">
           (c)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F24.sf4">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F24.sf4.g1" src="/html/2312.05019/assets/figures/swift.jpg" width="556"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S5.F24.sf4.2.1.1" style="font-size:90%;">
           (d)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="S5.F24.2.1.1" style="font-size:90%;">
        Figure 24
       </span>
       :
      </span>
      <span class="ltx_text" id="S5.F24.3.2" style="font-size:90%;">
       Applications of single drone. (a) Surveillance
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib133" title="">
         133
        </a>
        ]
       </cite>
       ; (b) Search and Rescue
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib151" title="">
         151
        </a>
        ]
       </cite>
       ; (c) Environmental Monitoring
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib143" title="">
         143
        </a>
        ]
       </cite>
       ; (d) Autonomous Racing
       <cite class="ltx_cite ltx_citemacro_cite">
        [
        <a class="ltx_ref" href="#bib.bib15" title="">
         15
        </a>
        ]
       </cite>
       .
      </span>
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S5.SS1.SSS5">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.1.5
     </span>
     Industrial Inspection
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS5.p1">
     <p class="ltx_p" id="S5.SS1.SSS5.p1.1">
      In industrial inspection, drones face key challenges like safely navigating complex environments and conducting precise measurements in the presence of various disturbances. Kim
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS5.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib144" title="">
        144
       </a>
       ]
      </cite>
      addressed the challenge of autonomous navigation by using drones for proximity measurement among construction entities, enhancing safety in the construction industry. Additionally, Khuc
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS5.p1.1.2">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib145" title="">
        145
       </a>
       ]
      </cite>
      focused on precise structural health inspection with drones, especially in high or inaccessible locations. Despite these advancements in autonomous navigation and measurement accuracy, maintaining data accuracy and reliability in industrial settings with interference from machinery, electromagnetic fields, and physical obstacles continues to be a significant challenge, necessitating further research and development in this domain.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS1.SSS6">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.1.6
     </span>
     Autonomous Racing
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS6.p1">
     <p class="ltx_p" id="S5.SS1.SSS6.p1.1">
      In autonomous drone racing, the central challenge is reducing delays exist in visual information processing and decision making and enhancing the adaptability of perception networks. In
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib146" title="">
        146
       </a>
       ]
      </cite>
      , a novel sensor fusion method was proposed to enable high-speed autonomous racing for mini-drones. This work also addressed issues with occasional large outliers and vision delays commonly encountered in fast drone racing. Another work
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib147" title="">
        147
       </a>
       ]
      </cite>
      introduced an innovative approach to drone control, where a deep neural network (DNN) was used to fuse trajectories from multiple controllers. In the latest work
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib15" title="">
        15
       </a>
       ]
      </cite>
      , the vision-based drone outperformed world champions in the racing task, purely relying on onboard perception and a trained neural network. The primary challenge in autonomous drone racing, as identified in these studies, lies in the need for improved adaptability of perception networks to various environments and textures, which is crucial for the high-speed demands of the sport.
     </p>
    </div>
    <div class="ltx_para" id="S5.SS1.SSS6.p2">
     <p class="ltx_p" id="S5.SS1.SSS6.p2.1">
      Overall, the primary challenges in single drone applications include limited battery life, which restricts operational duration, and the need for effective obstacle avoidance in varied environments. Additionally, limitations in data processing capabilities affect real-time decision-making and adaptability. Advanced technological solutions are essential to overcome these challenges, ensuring that single drones can operate efficiently and reliably in diverse scenarios and paving the way for future innovations.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Multi-Drone Application
   </h3>
   <figure class="ltx_figure" id="S5.F25">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F25.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F25.sf1.g1" src="/html/2312.05019/assets/figures/multi_applications_1_surveying.jpg" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F25.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F25.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F25.sf2.g1" src="/html/2312.05019/assets/figures/multi_applications_2_tracking.jpg" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F25.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F25.sf3">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F25.sf3.g1" src="/html/2312.05019/assets/figures/multi_applications_3_surveillance.png" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F25.sf3.2.1.1" style="font-size:90%;">
          (c)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F25.sf4">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F25.sf4.g1" src="/html/2312.05019/assets/figures/multi_applications_4_disaster.png" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F25.sf4.2.1.1" style="font-size:90%;">
          (d)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S5.F25.2.1.1" style="font-size:90%;">
       Figure 25
      </span>
      :
     </span>
     <span class="ltx_text" id="S5.F25.3.2" style="font-size:90%;">
      Applications of multi-drone. (a) Coordinated Surveying
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib152" title="">
        152
       </a>
       ]
      </cite>
      ; (b) Cooperative Tracking
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib153" title="">
        153
       </a>
       ]
      </cite>
      ; (c) Synchronized Monitoring
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib154" title="">
        154
       </a>
       ]
      </cite>
      ; (d) Disaster Response
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib155" title="">
        155
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     While single drones offer convenience, their limited monitoring range has prompted interest in multi-drone collaboration. This approach seeks to overcome range limitations by leveraging the collective capabilities of multiple drones for broader, more efficient operations. Multi-drone applications, encompassing activities such as coordinated surveying
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib156" title="">
       156
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib157" title="">
       157
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib152" title="">
       152
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib158" title="">
       158
      </a>
      ]
     </cite>
     , cooperative tracking
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib153" title="">
       153
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib159" title="">
       159
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib160" title="">
       160
      </a>
      ]
     </cite>
     , synchronized monitoring
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib154" title="">
       154
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib161" title="">
       161
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib162" title="">
       162
      </a>
      ]
     </cite>
     , and disaster response
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib163" title="">
       163
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib164" title="">
       164
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib151" title="">
       151
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib56" title="">
       56
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib155" title="">
       155
      </a>
      ]
     </cite>
     , bring the added complexity of inter-drone communication, coordination and real-time data integration. These applications leverage the combined capabilities of multiple drones to achieve greater efficiency and coverage than single drone operations.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S5.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.2.1
     </span>
     Coordinated Surveying
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS1.p1">
     <p class="ltx_p" id="S5.SS2.SSS1.p1.1">
      In coordinated surveying, several challenges are prominent: merging diverse data from individual drones, and addressing computational demands in cooperative process. These challenges were tackled by some works. In
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib156" title="">
        156
       </a>
       ]
      </cite>
      , a monocular visual odometry algorithm was used to enable autonomous onboard control with cooperative localization and mapping. This work addressed the challenges of coordinating and merging different maps constructed by each drone platform and what’s more, the computational bottlenecks typically associated with 3D RGB-D cooperative SLAM. Micro-air vehicles also play an outstanding role in the field of coordinated surveying. Similarly, in
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib158" title="">
        158
       </a>
       ]
      </cite>
      , a sensor fusion scheme was proposed to improve the accuracy of localization in MAV fleets. Ensuring that each MAV effectively contributes to the overall perception of the environment poses a challenge addressed in this work. Both methods depend on effective collaboration and communication among multiple agents. However, maintaining stable communication between drones remains a critical and unresolved issue in multi-drone operations.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.2.2
     </span>
     Cooperative Tracking
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS2.p1">
     <p class="ltx_p" id="S5.SS2.SSS2.p1.1">
      In the field of multi-drone object tracking, navigating complex operational environments and overcoming limited communication bandwidth are significant challenges. These topics have also garnered significant research interest. In the study
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib159" title="">
        159
       </a>
       ]
      </cite>
      , researchers developed a system for cooperative surveillance and tracking in urban settings, specifically tackling the issue of collision avoidance among drones. Additionally, another work by Farmani
      <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib160" title="">
        160
       </a>
       ]
      </cite>
      explores a decentralized tracking system for drones, focusing on overcoming limited communication bandwidth and intermittent connectivity challenges. However, a persistent difficulty in this field is navigating complex external environments. Effective path planning and avoiding obstacles during multi-drone operations remain crucial challenges that require ongoing attention and innovation.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.2.3
     </span>
     Synchronized Monitoring
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS3.p1">
     <p class="ltx_p" id="S5.SS2.SSS3.p1.1">
      In synchronized monitoring missions with multiple drones, the focus is on how to effectively allocate tasks among drones and improve overall mission efficiency, as well as overcoming computational limitations. Gu
      <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS3.p1.1.1">
       et al.
      </span>
      in
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib154" title="">
        154
       </a>
       ]
      </cite>
      developed a small target detection and tracking model using data fusion, establishing a cooperative network for synchronized monitoring. This study also addresses task distribution challenges and efficiency optimization. Moreover,
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib161" title="">
        161
       </a>
       ]
      </cite>
      explores implementing deep neural network (DNN) models on computationally limited drones, focusing on reducing classification latency in collaborative drone systems. However, issues like collision avoidance in complex environments and signal interference in multi-drone systems are not comprehensively addressed in these studies.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS2.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.2.4
     </span>
     Disaster Response
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS4.p1">
     <p class="ltx_p" id="S5.SS2.SSS4.p1.1">
      In the domain of multi-drone systems for disaster response, challenges such as autonomous navigation, communication limitations, and real-time decision-making are paramount. To address these problems, Tang
      <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS4.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib163" title="">
        163
       </a>
       ]
      </cite>
      introduced a tracking-learning-detection framework with an advanced flocking strategy for exploration and search missions. However, the study does not examine the crucial aspect of task distribution and optimization for enhancing mission efficiency in multi-drone disaster response. This shortcoming points to an essential area for further research, as effective task management is key to utilizing the full capabilities of multi-drone systems, particularly in the dynamic and urgent context of disaster scenarios.
     </p>
    </div>
    <div class="ltx_para" id="S5.SS2.SSS4.p2">
     <p class="ltx_p" id="S5.SS2.SSS4.p2.1">
      The primary challenges in multi-drone applications include maintaining stable and reliable communication links, collision avoidance between drones, and effective distribution of tasks to optimize the overall mission efficiency. Additionally, issues like signal interference and managing the flight paths of multiple drones simultaneously are significant hurdles. Addressing these challenges is vital for unlocking the full potential of multi-drone systems, paving the way for advancements in various application domains.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Heterogeneous Systems Application
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     As the complexity and uncertainty of task scenarios escalate, it becomes increasingly challenging for a single robot or even a homogeneous multi-robot system to efficiently adapt to diverse environments. Consequently, in recent years, heterogeneous multi-robot systems (HMRS) have emerged as a focal point of research within the community
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib165" title="">
       165
      </a>
      ]
     </cite>
     . In the domain of UAV applications, HMRS mainly refers to communication-enabled networks that integrate various UAVs with other intelligent robotic platforms, such as unmanned ground vehicles (UGVs) and unmanned surface vehicles (USVs). This integration facilitates a diverse range of applications, leveraging the unique capabilities of each system to enhance overall operational efficiency. These systems execute a range of tasks, either individually or in a collaborative manner. The inherently heterogeneous scheduling approach of HMRS significantly enhances feasibility and adaptability, thereby effectively tackling a series of demanding tasks across different environments. Consequently, applications of HMRS are rapidly evolving; examples include but are not limited to localization and path planning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib166" title="">
       166
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib167" title="">
       167
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib168" title="">
       168
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib169" title="">
       169
      </a>
      ]
     </cite>
     , precise landing
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib170" title="">
       170
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib171" title="">
       171
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib172" title="">
       172
      </a>
      ]
     </cite>
     , and comprehensive inspection and detection
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib173" title="">
       173
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib174" title="">
       174
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib175" title="">
       175
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F26">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F26.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F26.sf1.g1" src="/html/2312.05019/assets/figures/HMRS_Planning.jpg" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F26.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F26.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F26.sf2.g1" src="/html/2312.05019/assets/figures/HMRS_Landing.jpg" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F26.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F26.sf3">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S5.F26.sf3.g1" src="/html/2312.05019/assets/figures/HMRS_Inspection.jpg" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F26.sf3.2.1.1" style="font-size:90%;">
          (c)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F26.sf4">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="598" id="S5.F26.sf4.g1" src="/html/2312.05019/assets/figures/HMRS_Detection.jpg" width="556"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S5.F26.sf4.2.1.1" style="font-size:90%;">
          (d)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S5.F26.2.1.1" style="font-size:90%;">
       Figure 26
      </span>
      :
     </span>
     <span class="ltx_text" id="S5.F26.3.2" style="font-size:90%;">
      Applications of heterogeneous systems. (a) UAV-UGV path planning
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib166" title="">
        166
       </a>
       ]
      </cite>
      ; (b) UAV-UGV precise landing
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib170" title="">
        170
       </a>
       ]
      </cite>
      ; (c) UAV-UGV inventory inspection
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib173" title="">
        173
       </a>
       ]
      </cite>
      ; (d) UAV-UGV object detection
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib174" title="">
        174
       </a>
       ]
      </cite>
      .
     </span>
    </figcaption>
   </figure>
   <section class="ltx_subsubsection" id="S5.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.3.1
     </span>
     Localization and Path Planning
    </h4>
    <div class="ltx_para" id="S5.SS3.SSS1.p1">
     <p class="ltx_p" id="S5.SS3.SSS1.p1.1">
      The UAV-UGV cooperation system can address the challenges of GPS denial and limited sensing range in UGVs. In this system, UAVs provide essential auxiliary information for UGV localization and path planning, relying solely on cost-effective cameras and wireless communication channels. Niu
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib166" title="">
        166
       </a>
       ]
      </cite>
      introduced a framework wherein a single UAV served multiple UGVs, achieving optimal path planning based on aerial imagery. This approach surpassed traditional heuristic path planning algorithms in performance. Furthermore, Liu
      <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib167" title="">
        167
       </a>
       ]
      </cite>
      presented a joint UAV-UGV architecture designed to overcome frequent target occlusion issues encountered by single-ground platforms. This architecture enabled accurate and dynamic target localization, leveraging visual inputs from UAVs.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.3.2
     </span>
     Precise Landing
    </h4>
    <div class="ltx_para" id="S5.SS3.SSS2.p1">
     <p class="ltx_p" id="S5.SS3.SSS2.p1.1">
      Given the potential necessity for battery recharging and emergency maintenance of UAVs during extended missions, the UAV-UGV heterogeneous system can facilitate UAV landings. This design reduces reliance on manual intervention while enhancing the UAVs’ capacity for prolonged, uninterrupted operation. In the study
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib170" title="">
        170
       </a>
       ]
      </cite>
      , a vision-based heterogeneous system was proposed to address the challenge of UAVs’ temporary landings during long-range inspections. This system accomplished precise target geolocation and safe landings in the absence of GPS data by detecting QR codes mounted on UGVs. Additionally, Xu
      <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS2.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib171" title="">
        171
       </a>
       ]
      </cite>
      illustrated the application of UAV heterogeneous systems for landing on USVs. A similar approach was explored for UAVs’ target localization and landing, leveraging QR code recognition on USVs. Collectively, these studies underscored the feasibility and adaptability of the heterogeneous landing system across different platforms.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S5.SS3.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      5.3.3
     </span>
     Inspection and Detection
    </h4>
    <div class="ltx_para" id="S5.SS3.SSS3.p1">
     <p class="ltx_p" id="S5.SS3.SSS3.p1.1">
      Heterogeneous UAV systems present an effective solution to overcome the limitations of background clutter and incoherent target interference often encountered in single-ground vision detection platforms. By leveraging the expansive field of view and swift scanning capabilities of UAVs, in conjunction with the endurance and high accuracy of UGVs, such heterogeneous systems can achieve time-efficient and accurate target inspection and detection in specific applications. For instance, Kalinov
      <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS3.p1.1.1">
       et al.
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib173" title="">
        173
       </a>
       ]
      </cite>
      introduced a heterogeneous inventory management system, pairing a ground robot with a UAV. In this system, the ground robot determined motion trajectories by deploying the SLAM algorithm, while the UAV, with its high maneuverability, was tasked with scanning barcodes. Furthermore, Pretto
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib175" title="">
        175
       </a>
       ]
      </cite>
      developed a heterogeneous farming system to enhance agricultural automation. This innovative system utilized the aerial perspective of the UAV to assist in farmland segmentation and the classification of crops from weeds, significantly contributing to the advancement of automated farming practices.
     </p>
    </div>
    <div class="ltx_para" id="S5.SS3.SSS3.p2">
     <p class="ltx_p" id="S5.SS3.SSS3.p2.1">
      To sum up, most of the applications above primarily focus on single UAV to single UGV or single UAV to multiple UGV configurations, with few scenarios designed for multiple UAVs interacting with multiple UGVs. It is evident that there remains significant research potential in the realm of vision-based, multi-agent-to-multi-agent heterogeneous systems. Key areas such as communication and data integration within heterogeneous systems, coordination and control in dynamic and unpredictable environments, and individual agents’ autonomy and decision-making capabilities warrant further exploration.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Open Questions and Potential Solutions
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    Despite significant advancements in the domain of vision-based learning for drones, numerous challenges remain that impede the pace of development and real-world applicability of these methods. These challenges span various aspects, from data collection and simulation accuracy to operational efficiency and safety concerns.
   </p>
  </div>
  <section class="ltx_subsection" id="S6.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.1
    </span>
    Dataset
   </h3>
   <div class="ltx_para" id="S6.SS1.p1">
    <p class="ltx_p" id="S6.SS1.p1.1">
     A major impediment in the field is the absence of a comprehensive, public dataset analogous to Open X-Embodiment
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib176" title="">
       176
      </a>
      ]
     </cite>
     in robotic manipulation. This unified dataset should ideally encompass a wide range of scenarios and tasks to facilitate generalizable learning. The current reliance on domain-specific datasets like “Anti-UAV”
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib177" title="">
       177
      </a>
      ]
     </cite>
     and “SUAV-DATA”
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib178" title="">
       178
      </a>
      ]
     </cite>
     limits the scope and applicability of research. A potential solution is the collaborative development of a diverse, multi-purpose dataset by academic and industry stakeholders, incorporating various environmental, weather, and lighting conditions.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.2
    </span>
    Simulator
   </h3>
   <div class="ltx_para" id="S6.SS2.p1">
    <p class="ltx_p" id="S6.SS2.p1.1">
     While simulators are vital for training and validating vision-based drone models, their realism and accuracy often fall short of replicating real-world complexities. This gap hampers the transition from simulation to actual deployment. Meanwhile, there is no unified simulator covering most of the drone tasks, resulting in repetitive domain-specific simulator development
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib117" title="">
       117
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib179" title="">
       179
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib180" title="">
       180
      </a>
      ]
     </cite>
     . Drawing inspiration from the self-driving car domain, the integration of off-the-shelf and highly flexible simulators such as CARLA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib181" title="">
       181
      </a>
      ]
     </cite>
     could be a solution. These simulators, known for their advanced features in realistic traffic simulation and diverse environmental conditions, can provide more authentic and varied data for training. Adapting such simulators to drone-specific scenarios could greatly enhance the quality of training and testing environments.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.3
    </span>
    Sample Efficiency
   </h3>
   <div class="ltx_para" id="S6.SS3.p1">
    <p class="ltx_p" id="S6.SS3.p1.1">
     Enhancing sample efficiency in machine learning models for drones is crucial, particularly in environments where data collection is hazardous or impractical. Even though simulators are available for generating training data, there are still challenges in ensuring the realism and diversity of these simulated environments. The gap between simulated and real-world data can lead to performance discrepancies when models are deployed in actual scenarios. Developing algorithms that leverage transfer learning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib182" title="">
       182
      </a>
      ]
     </cite>
     , few-shot learning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib183" title="">
       183
      </a>
      ]
     </cite>
     , and synthetic data generation
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib184" title="">
       184
      </a>
      ]
     </cite>
     could provide significant strides in learning efficiently from limited datasets. These approaches aim to bridge the gap between simulation and reality, enhancing the applicability and robustness of machine learning models in diverse and dynamic real-world situations.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.4
    </span>
    Inference Speed
   </h3>
   <div class="ltx_para" id="S6.SS4.p1">
    <p class="ltx_p" id="S6.SS4.p1.1">
     Balancing inference speed with accuracy is a critical challenge for drones operating in dynamic environments. The key lies in optimizing machine learning models for edge computing, enabling drones to process data and make decisions swiftly. Techniques like model pruning
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib185" title="">
       185
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib186" title="">
       186
      </a>
      ]
     </cite>
     , quantization
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib187" title="">
       187
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib188" title="">
       188
      </a>
      ]
     </cite>
     , distillation
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib189" title="">
       189
      </a>
      ]
     </cite>
     and the development of specialized hardware accelerators can play a pivotal role in this regard.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.5
    </span>
    Real World Deployment
   </h3>
   <div class="ltx_para" id="S6.SS5.p1">
    <p class="ltx_p" id="S6.SS5.p1.1">
     Transitioning from controlled simulation environments to real-world deployment (Sim2Real) involves addressing unpredictability in environmental conditions, regulatory compliance, and adaptability to diverse operational contexts. Domain randomization
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib123" title="">
       123
      </a>
      ]
     </cite>
     tries to address the Sim2Real issue in a certain way but is limited to predicted scenarios with known domain distributions. Developing robust and adaptive algorithms capable of on-the-fly learning and decision-making, along with rigorous field testing under varied conditions, can aid in overcoming these challenges.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS6">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.6
    </span>
    Embodied Intelligence in Open World
   </h3>
   <div class="ltx_para" id="S6.SS6.p1">
    <p class="ltx_p" id="S6.SS6.p1.1">
     Existing vision-based learning methods for drones require explicit task descriptions and formal constraints, while in an open world, it is hard to provide all necessary formulations at the beginning to find the optimal solution. For instance, in a complex search and rescue mission, the drone can only find the targets first and conduct rescue based on the information collected. In each stage, the task may change, and there is no prior explicit problem at the start. Human interactions are necessary during this mission. With large language models and embodied intelligence, the potential of drone autonomy can be greatly increased. Through interactions in the open world
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib190" title="">
       190
      </a>
      ]
     </cite>
     or provide few-shot imitation
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib191" title="">
       191
      </a>
      ]
     </cite>
     , vision-based learning can emerge with full autonomy for drone applications.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS7">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.7
    </span>
    Safety and Security
   </h3>
   <div class="ltx_para" id="S6.SS7.p1">
    <p class="ltx_p" id="S6.SS7.p1.1">
     Ensuring the safety and security of drone operations is paramount, especially in densely populated or sensitive areas. This includes not only physical safety but also cybersecurity concerns
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib192" title="">
       192
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib193" title="">
       193
      </a>
      ]
     </cite>
     . The security aspect extends beyond data protection, including the resilience of drones to adversarial attacks. Such attacks could take various forms, from signal jamming to deceptive inputs aimed at misleading vision-based systems and DRL algorithms
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib194" title="">
       194
      </a>
      ]
     </cite>
     . Addressing these concerns requires a multifaceted approach. Firstly, incorporating advanced cryptographic techniques ensures data integrity and secure communication. Secondly, implementing anomaly detection systems can help identify and mitigate unusual patterns indicative of adversarial interference. Moreover, improving the robustness of learning models against adversarial attacks and investigating the explainability of designed models
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib195" title="">
       195
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib196" title="">
       196
      </a>
      ]
     </cite>
     are imperative. Lastly, regular updates and patches to the drone’s software, based on the latest threat intelligence, can fortify its defenses against evolving cyber threats.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    This comprehensive survey has thoroughly explored the rapidly growing field of vision-based learning for drones, particularly emphasizing their evolving role in multi-drone systems and complex environments such as search and rescue missions and adversarial settings. The investigation revealed that drones are increasingly becoming sophisticated, autonomous systems capable of intricate tasks, largely driven by advancements in AI, machine learning, and sensor technology. The exploration of micro and nano drones, innovative structural designs, and enhanced autonomy stand out as key trends shaping the future of drone technology. Crucially, the integration of visual perception with machine learning algorithms, including deep reinforcement learning, opens up new avenues for drones to operate with greater efficiency and intelligence. These capabilities are particularly pertinent in the context of object detection and decision-making processes, vital for complex drone operations. The survey categorized vision-based control methods into indirect, semi-direct, and end-to-end methods, offering a nuanced understanding of how drones perceive and interact with their environment. Applications of vision-based learning drones, spanning from single-agent to multi-agent and heterogeneous systems, demonstrate their versatility and potential in various sectors, including agriculture, industrial inspection, and emergency response. However, this expansion also brings forth challenges such as data processing limitations, real-time decision-making, and ensuring robustness in diverse operational scenarios.
   </p>
  </div>
  <div class="ltx_para" id="S7.p2">
   <p class="ltx_p" id="S7.p2.1">
    The survey highlighted open questions and potential solutions in the field, stressing the need for comprehensive datasets, realistic simulators, improved sample efficiency, and faster inference speeds. Addressing these challenges is crucial for the effective deployment of drones in real-world scenarios. Safety and security, especially in the context of adversarial environments, remain paramount concerns that need ongoing attention. While significant progress has been made in vision-based learning for drones, the journey towards fully autonomous, intelligent, and reliable systems, even AGI in the physical world, is ongoing. Future research and development in this field hold the promise of revolutionizing various industries, pushing the boundaries of what’s possible with drone technology in complex and dynamic environments.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     R. Rajkumar, I. Lee, L. Sha, and J. Stankovic, “Cyber-physical systems: the
next computing revolution,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      Design Automation Conference
     </em>
     .   IEEE, 2010, pp. 731–736.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     R. Baheti and H. Gill, “Cyber-physical systems,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      The impact of control
technology
     </em>
     , vol. 12, no. 1, pp. 161–166, 2011.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     M. Hassanalian and A. Abdelkefi, “Classifications, applications, and design
challenges of drones: A review,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      Progress in Aerospace Sciences
     </em>
     ,
vol. 91, pp. 99–131, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     P. Nooralishahi, C. Ibarra-Castanedo, S. Deane, F. López, S. Pant,
M. Genest, N. P. Avdelidis, and X. P. Maldague, “Drone-based non-destructive
inspection of industrial sites: A review and case studies,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      Drones
     </em>
     ,
vol. 5, no. 4, p. 106, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     N. J. Stehr, “Drones: The newest technology for precision agriculture,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      Natural Sciences Education
     </em>
     , vol. 44, no. 1, pp. 89–91, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     P. M. Kornatowski, M. Feroskhan, W. J. Stewart, and D. Floreano,
“Downside up:rethinking parcel position for aerial delivery,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      IEEE
Robotics and Automation Letters
     </em>
     , vol. 5, no. 3, pp. 4297–4304, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     P. M. Kornatowski, M. Feroskhan, W. J. Stewart, and D. Floreano, “A morphing
cargo drone for safe flight in proximity of humans,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      IEEE Robotics and
Automation Letters
     </em>
     , vol. 5, no. 3, pp. 4233–4240, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     D. Câmara, “Cavalry to the rescue: Drones fleet to help rescuers
operations over disasters scenarios,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      2014 IEEE Conference on
Antenna Measurements &amp; Applications (CAMA)
     </em>
     .   IEEE, 2014, pp. 1–4.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     M. Graule, P. Chirarattananon, S. Fuller, N. Jafferis, K. Ma, M. Spenko,
R. Kornbluh, and R. Wood, “Perching and takeoff of a robotic insect on
overhangs using switchable electrostatic adhesion,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Science
     </em>
     , vol.
352, no. 6288, pp. 978–982, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     D. Floreano and R. J. Wood, “Science, technology and the future of small
autonomous drones,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Nature
     </em>
     , vol. 521, no. 7553, pp. 460–466, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     J. Shu and P. Chirarattananon, “A quadrotor with an origami-inspired
protective mechanism,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      IEEE Robotics and Automation Letters
     </em>
     , vol. 4,
no. 4, p. 3820–3827, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     E. Ajanic, M. Feroskhan, S. Mintchev, F. Noca, and D. Floreano, “Bioinspired
wing a and tail morphing extends drone flight capabilities,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Sci.
Robot.
     </em>
     , vol. 5, p. eabc2897, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     X. Zhou, J. Zhu, H. Zhou, C. Xu, and F. Gao, “Ego-swarm: A fully autonomous
and decentralized quadrotor swarm system in cluttered environments,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      2021 IEEE international conference on robotics and automation
(ICRA)
     </em>
     .   IEEE, 2021, pp. 4101–4107.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     E. Kaufmann, A. Loquercio, R. Ranftl, M. Müller, V. Koltun, and
D. Scaramuzza, “Deep drone acrobatics,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      arXiv preprint
arXiv:2006.05768
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Müller, V. Koltun, and
D. Scaramuzza, “Champion-level drone racing using deep reinforcement
learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      Nature
     </em>
     , vol. 620, no. 7976, pp. 982–987, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,
J. Thomason, and A. Garg, “Progprompt: Generating situated robot task plans
using large language models,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      2023 IEEE International Conference on
Robotics and Automation (ICRA)
     </em>
     .   IEEE,
2023, pp. 11 523–11 530.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_tag_bibitem">
     [17]
    </span>
    <span class="ltx_bibblock">
     A. Gupta, S. Savarese, S. Ganguli, and L. Fei-Fei, “Embodied intelligence via
learning and evolution,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      Nature communications
     </em>
     , vol. 12, no. 1, p.
5721, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_tag_bibitem">
     [18]
    </span>
    <span class="ltx_bibblock">
     Y. Lu, Z. Xue, G.-S. Xia, and L. Zhang, “A survey on vision-based uav
navigation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      Geo-spatial information science
     </em>
     , vol. 21, no. 1, pp.
21–32, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_tag_bibitem">
     [19]
    </span>
    <span class="ltx_bibblock">
     M. Y. Arafat, M. M. Alam, and S. Moh, “Vision-based navigation techniques for
unmanned aerial vehicles: Review and challenges,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Drones
     </em>
     , vol. 7,
no. 2, p. 89, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_tag_bibitem">
     [20]
    </span>
    <span class="ltx_bibblock">
     E. Kakaletsis, C. Symeonidis, M. Tzelepi, I. Mademlis, A. Tefas, N. Nikolaidis,
and I. Pitas, “Computer vision for autonomous uav flight safety: An overview
and a vision-based safe landing pipeline example,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Acm Computing
Surveys (Csur)
     </em>
     , vol. 54, no. 9, pp. 1–37, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_tag_bibitem">
     [21]
    </span>
    <span class="ltx_bibblock">
     A. Mcfadyen and L. Mejias, “A survey of autonomous vision-based see and avoid
for unmanned aircraft systems,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      Progress in Aerospace Sciences
     </em>
     ,
vol. 80, pp. 1–17, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_tag_bibitem">
     [22]
    </span>
    <span class="ltx_bibblock">
     R. Jenssen, D. Roverso
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      et al.
     </em>
     , “Automatic autonomous vision-based power
line inspection: A review of current status and the potential role of deep
learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2">
      International Journal of Electrical Power &amp; Energy
Systems
     </em>
     , vol. 99, pp. 107–120, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_tag_bibitem">
     [23]
    </span>
    <span class="ltx_bibblock">
     B. F. Spencer Jr, V. Hoskere, and Y. Narazaki, “Advances in computer
vision-based civil infrastructure inspection and monitoring,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      Engineering
     </em>
     , vol. 5, no. 2, pp. 199–222, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_tag_bibitem">
     [24]
    </span>
    <span class="ltx_bibblock">
     D. Hanover, A. Loquercio, L. Bauersfeld, A. Romero, R. Penicka, Y. Song,
G. Cioffi, E. Kaufmann, and D. Scaramuzza, “Autonomous drone racing: A
survey,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      arXiv e-prints, pp. arXiv–2301
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_tag_bibitem">
     [25]
    </span>
    <span class="ltx_bibblock">
     B. Zhou, H. Xu, and S. Shen, “Racer: Rapid collaborative exploration with a
decentralized multi-uav system,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      IEEE Transactions on Robotics
     </em>
     ,
vol. 39, no. 3, pp. 1816–1835, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_tag_bibitem">
     [26]
    </span>
    <span class="ltx_bibblock">
     E. Kaufmann, A. Loquercio, R. Ranftl, A. Dosovitskiy, V. Koltun, and
D. Scaramuzza, “Deep drone racing: Learning agile flight in dynamic
environments,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Conference on Robot Learning
     </em>
     .   PMLR, 2018, pp. 133–145.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_tag_bibitem">
     [27]
    </span>
    <span class="ltx_bibblock">
     D. Falanga, K. Kleber, and D. Scaramuzza, “Dynamic obstacle avoidance for
quadrotors with event cameras,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Science Robotics
     </em>
     , vol. 5, no. 40,
2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_tag_bibitem">
     [28]
    </span>
    <span class="ltx_bibblock">
     A. Loquercio, E. Kaufmann, R. Ranftl, M. Müller, V. Koltun, and
D. Scaramuzza, “Learning high-speed flight in the wild,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      Science
Robotics
     </em>
     , vol. 6, no. 59, p. eabg5810, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_tag_bibitem">
     [29]
    </span>
    <span class="ltx_bibblock">
     T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular
visual-inertial state estimator,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      IEEE Transactions on Robotics
     </em>
     ,
vol. 34, no. 4, pp. 1004–1020, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_tag_bibitem">
     [30]
    </span>
    <span class="ltx_bibblock">
     N. J. Sanket, C. M. Parameshwara, C. D. Singh, A. V. Kuruttukulam,
C. Fermüller, D. Scaramuzza, and Y. Aloimonos, “Evdodgenet: Deep dynamic
obstacle dodging with event cameras,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      2020 IEEE International
Conference on Robotics and Automation (ICRA)
     </em>
     .   IEEE, 2020, pp. 10 651–10 657.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_tag_bibitem">
     [31]
    </span>
    <span class="ltx_bibblock">
     N. J. Sanket, C. D. Singh, C. Fermüller, and Y. Aloimonos, “Ajna:
Generalized deep uncertainty for minimal perception on parsimonious robots,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      Science Robotics
     </em>
     , vol. 8, no. 81, p. eadd5139, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_tag_bibitem">
     [32]
    </span>
    <span class="ltx_bibblock">
     R. Siegwart, I. R. Nourbakhsh, and D. Scaramuzza,
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      Introduction to
autonomous mobile robots
     </em>
     .   MIT press,
2011.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_tag_bibitem">
     [33]
    </span>
    <span class="ltx_bibblock">
     N. Michael, S. Shen, K. Mohta, Y. Mulgaonkar, V. Kumar, K. Nagatani, Y. Okada,
S. Kiribayashi, K. Otake, K. Yoshida
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      et al.
     </em>
     , “Collaborative mapping
of an earthquake-damaged building via ground and aerial robots,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">
      Journal of Field Robotics
     </em>
     , vol. 29, no. 5, pp. 832–841, 2012.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_tag_bibitem">
     [34]
    </span>
    <span class="ltx_bibblock">
     H. Guan, X. Sun, Y. Su, T. Hu, H. Wang, H. Wang, C. Peng, and Q. Guo,
“UAV-lidar aids automatic intelligent powerline inspection,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      International Journal of Electrical Power and Energy Systems
     </em>
     , vol.
130, p. 106987, sep 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_tag_bibitem">
     [35]
    </span>
    <span class="ltx_bibblock">
     R. Opromolla, G. Fasano, G. Rufino, and M. Grassi, “Uncooperative pose
estimation with a lidar-based system,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      Acta Astronautica
     </em>
     , vol. 110,
pp. 287–297, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_tag_bibitem">
     [36]
    </span>
    <span class="ltx_bibblock">
     Z. Wang, Z. Zhao, Z. Jin, Z. Che, J. Tang, C. Shen, and Y. Peng, “Multi-stage
fusion for multi-class 3d lidar detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      Proceedings of the
IEEE/CVF International Conference on Computer Vision
     </em>
     , 2021, pp. 3120–3128.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_tag_bibitem">
     [37]
    </span>
    <span class="ltx_bibblock">
     M. O. Aqel, M. H. Marhaban, M. I. Saripan, and N. B. Ismail, “Review of visual
odometry: types, approaches, challenges, and applications,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      SpringerPlus
     </em>
     , vol. 5, pp. 1–26, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_tag_bibitem">
     [38]
    </span>
    <span class="ltx_bibblock">
     J. Delmerico and D. Scaramuzza, “A benchmark comparison of monocular
visual-inertial odometry algorithms for flying robots,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      2018 IEEE
international conference on robotics and automation (ICRA)
     </em>
     .   IEEE, 2018, pp. 2502–2509.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_tag_bibitem">
     [39]
    </span>
    <span class="ltx_bibblock">
     D. Scaramuzza and Z. Zhang,
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      Aerial Robots, Visual-Inertial Odometry
of
     </em>
     .   Berlin, Heidelberg: Springer
Berlin Heidelberg, 2020, pp. 1–9. [Online]. Available:
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-642-41610-1_71-1" target="_blank" title="">
      https://doi.org/10.1007/978-3-642-41610-1_71-1
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_tag_bibitem">
     [40]
    </span>
    <span class="ltx_bibblock">
     R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a versatile and
accurate monocular slam system,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      IEEE transactions on robotics
     </em>
     ,
vol. 31, no. 5, pp. 1147–1163, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_tag_bibitem">
     [41]
    </span>
    <span class="ltx_bibblock">
     T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile Monocular
Visual-Inertial State Estimator,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      IEEE Transactions on Robotics
     </em>
     ,
vol. 34, no. 4, pp. 1004–1020, aug 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_tag_bibitem">
     [42]
    </span>
    <span class="ltx_bibblock">
     C. Campos, R. Elvira, J. J. Gómez Rodríguez, J. M. M. Montiel,
and J. D. Tardós, “ORB-SLAM3: An Accurate Open-Source Library for
Visual, Visual-Inertial and Multi-Map SLAM,” 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_tag_bibitem">
     [43]
    </span>
    <span class="ltx_bibblock">
     G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi,
S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      et al.
     </em>
     ,
“Event-based vision: A survey,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2">
      IEEE transactions on pattern analysis
and machine intelligence
     </em>
     , vol. 44, no. 1, pp. 154–180, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_tag_bibitem">
     [44]
    </span>
    <span class="ltx_bibblock">
     W. Gao, K. Wang, W. Ding, F. Gao, T. Qin, and S. Shen, “Autonomous aerial
robot using dual-fisheye cameras,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      Journal of Field Robotics
     </em>
     ,
vol. 37, no. 4, pp. 497–514, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_tag_bibitem">
     [45]
    </span>
    <span class="ltx_bibblock">
     V. R. Kumar, S. Yogamani, H. Rashed, G. Sitsu, C. Witt, I. Leang, S. Milz, and
P. Mäder, “Omnidet: Surround view cameras based multi-task visual
perception network for autonomous driving,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      IEEE Robotics and
Automation Letters
     </em>
     , vol. 6, no. 2, pp. 2830–2837, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_tag_bibitem">
     [46]
    </span>
    <span class="ltx_bibblock">
     A. D. Haumann, K. D. Listmann, and V. Willert, “DisCoverage: A new paradigm
for multi-robot exploration,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      Proceedings - IEEE International
Conference on Robotics and Automation
     </em>
     , 2010, pp. 929–934.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_tag_bibitem">
     [47]
    </span>
    <span class="ltx_bibblock">
     A. H. Tan, F. P. Bejarano, Y. Zhu, R. Ren, and G. Nejat, “Deep reinforcement
learning for decentralized multi-robot exploration with macro actions,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      IEEE Robotics and Automation Letters
     </em>
     , vol. 8, no. 1, pp. 272–279,
2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_tag_bibitem">
     [48]
    </span>
    <span class="ltx_bibblock">
     Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
“Target-driven visual navigation in indoor scenes using deep reinforcement
learning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      2017 IEEE international conference on robotics and
automation (ICRA)
     </em>
     .   IEEE, 2017, pp.
3357–3364.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_tag_bibitem">
     [49]
    </span>
    <span class="ltx_bibblock">
     P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and R. Hadsell,
“Learning to navigate in complex environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      5th International
Conference on Learning Representations, ICLR 2017 - Conference Track
Proceedings
     </em>
     , 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_tag_bibitem">
     [50]
    </span>
    <span class="ltx_bibblock">
     Q. Wu, X. Gong, K. Xu, D. Manocha, J. Dong, and J. Wang, “Towards
target-driven visual navigation in indoor scenes via generative imitation
learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      IEEE Robotics and Automation Letters
     </em>
     , vol. 6, no. 1, pp.
175–182, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_tag_bibitem">
     [51]
    </span>
    <span class="ltx_bibblock">
     OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew,
A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider,
N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang,
“Solving rubik’s cube with a robot hand,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      arXiv preprint
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_tag_bibitem">
     [52]
    </span>
    <span class="ltx_bibblock">
     M. Liaq and Y. Byun, “Autonomous uav navigation using reinforcement
learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">
      International Journal of Machine Learning and Computing
     </em>
     ,
vol. 9, no. 6, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_tag_bibitem">
     [53]
    </span>
    <span class="ltx_bibblock">
     C. Wu, B. Ju, Y. Wu, X. Lin, N. Xiong, G. Xu, H. Li, and X. Liang, “Uav
autonomous target search based on deep reinforcement learning in complex
disaster scene,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      IEEE Access
     </em>
     , vol. 7, pp. 117 227–117 245, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_tag_bibitem">
     [54]
    </span>
    <span class="ltx_bibblock">
     C. Xiao, P. Lu, and Q. He, “Flying through a narrow gap using end-to-end deep
reinforcement learning augmented with curriculum learning and sim2real,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      IEEE Transactions on Neural Networks and Learning Systems
     </em>
     , vol. 34,
no. 5, pp. 2701–2708, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_tag_bibitem">
     [55]
    </span>
    <span class="ltx_bibblock">
     Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, “Autonomous drone racing
with deep reinforcement learning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      2021 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)
     </em>
     .   IEEE, 2021, pp. 1205–1212.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_tag_bibitem">
     [56]
    </span>
    <span class="ltx_bibblock">
     J. Xiao, P. Pisutsin, and M. Feroskhan, “Collaborative target search with a
visual drone swarm: An adaptive curriculum embedded multistage reinforcement
learning approach,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      IEEE Transactions on Neural Networks and Learning
Systems
     </em>
     , pp. 1–15, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_tag_bibitem">
     [57]
    </span>
    <span class="ltx_bibblock">
     Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Object detection with deep
learning: A review,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      IEEE Transactions on Neural Networks and Learning
Systems
     </em>
     , vol. 30, no. 11, pp. 3212–3232, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_tag_bibitem">
     [58]
    </span>
    <span class="ltx_bibblock">
     S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. Müller, “Zoedepth:
Zero-shot transfer by combining relative and metric depth,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">
      arXiv
preprint arXiv:2302.12288
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_tag_bibitem">
     [59]
    </span>
    <span class="ltx_bibblock">
     H. Laga, L. V. Jospin, F. Boussaid, and M. Bennamoun, “A survey on deep
learning techniques for stereo-based depth estimation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">
      IEEE
Transactions on Pattern Analysis and Machine Intelligence
     </em>
     , vol. 44, no. 4,
pp. 1738–1764, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_tag_bibitem">
     [60]
    </span>
    <span class="ltx_bibblock">
     Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional
neural networks: Analysis, applications, and prospects,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">
      IEEE
Transactions on Neural Networks and Learning Systems
     </em>
     , vol. 33, no. 12, pp.
6999–7019, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_tag_bibitem">
     [61]
    </span>
    <span class="ltx_bibblock">
     A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly
     <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">
      et al.
     </em>
     ,
“An image is worth 16x16 words: Transformers for image recognition at
scale,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib61.2.2">
      arXiv preprint arXiv:2010.11929
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_tag_bibitem">
     [62]
    </span>
    <span class="ltx_bibblock">
     Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">
      Proceedings of the IEEE/CVF international conference on computer
vision
     </em>
     , 2021, pp. 10 012–10 022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_tag_bibitem">
     [63]
    </span>
    <span class="ltx_bibblock">
     Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi, J. Fan,
and Z. He, “A survey of visual transformers,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">
      IEEE Transactions on
Neural Networks and Learning Systems
     </em>
     , pp. 1–21, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_tag_bibitem">
     [64]
    </span>
    <span class="ltx_bibblock">
     R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Region-based convolutional
networks for accurate object detection and segmentation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">
      IEEE
transactions on pattern analysis and machine intelligence
     </em>
     , vol. 38, no. 1,
pp. 142–158, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib65">
    <span class="ltx_tag ltx_tag_bibitem">
     [65]
    </span>
    <span class="ltx_bibblock">
     R. Girshick, “Fast r-cnn,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">
      Proceedings of the IEEE international
conference on computer vision
     </em>
     , 2015, pp. 1440–1448.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib66">
    <span class="ltx_tag ltx_tag_bibitem">
     [66]
    </span>
    <span class="ltx_bibblock">
     S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">
      Advances in neural
information processing systems
     </em>
     , vol. 28, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib67">
    <span class="ltx_tag ltx_tag_bibitem">
     [67]
    </span>
    <span class="ltx_bibblock">
     W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">
      European conference on
computer vision
     </em>
     .   Springer, 2016, pp.
21–37.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib68">
    <span class="ltx_tag ltx_tag_bibitem">
     [68]
    </span>
    <span class="ltx_bibblock">
     J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">
      Proceedings of the IEEE
conference on computer vision and pattern recognition
     </em>
     , 2016, pp. 779–788.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib69">
    <span class="ltx_tag ltx_tag_bibitem">
     [69]
    </span>
    <span class="ltx_bibblock">
     Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Yolox: Exceeding yolo series in
2021,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">
      arXiv preprint arXiv:2107.08430
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib70">
    <span class="ltx_tag ltx_tag_bibitem">
     [70]
    </span>
    <span class="ltx_bibblock">
     Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang,
L. Dong
     <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">
      et al.
     </em>
     , “Swin transformer v2: Scaling up capacity and
resolution,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib70.2.2">
      Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition
     </em>
     , 2022, pp. 12 009–12 019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib71">
    <span class="ltx_tag ltx_tag_bibitem">
     [71]
    </span>
    <span class="ltx_bibblock">
     Y. Li, H. Mao, R. Girshick, and K. He, “Exploring plain vision transformer
backbones for object detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">
      European Conference on Computer
Vision
     </em>
     .   Springer, 2022, pp. 280–296.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib72">
    <span class="ltx_tag ltx_tag_bibitem">
     [72]
    </span>
    <span class="ltx_bibblock">
     H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. Ni, and H.-Y. Shum,
“Dino: Detr with improved denoising anchor boxes for end-to-end object
detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">
      The Eleventh International Conference on Learning
Representations
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib73">
    <span class="ltx_tag ltx_tag_bibitem">
     [73]
    </span>
    <span class="ltx_bibblock">
     A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">
      Advances in neural information processing systems
     </em>
     , vol. 30, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib74">
    <span class="ltx_tag ltx_tag_bibitem">
     [74]
    </span>
    <span class="ltx_bibblock">
     Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, “Efficient attention: Attention
with linear complexities,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">
      Proceedings of the IEEE/CVF winter
conference on applications of computer vision
     </em>
     , 2021, pp. 3531–3539.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib75">
    <span class="ltx_tag ltx_tag_bibitem">
     [75]
    </span>
    <span class="ltx_bibblock">
     M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. W. Zamir, R. M. Anwer, and
F. Shahbaz Khan, “Edgenext: efficiently amalgamated cnn-transformer
architecture for mobile vision applications,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">
      European Conference
on Computer Vision
     </em>
     .   Springer, 2022,
pp. 3–20.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib76">
    <span class="ltx_tag ltx_tag_bibitem">
     [76]
    </span>
    <span class="ltx_bibblock">
     Y. Zheng, Z. Chen, D. Lv, Z. Li, Z. Lan, and S. Zhao, “Air-to-air visual
detection of micro-uavs: An experimental evaluation of deep learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">
      IEEE Robotics and Automation Letters
     </em>
     , vol. 6, no. 2, pp. 1020–1027,
2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib77">
    <span class="ltx_tag ltx_tag_bibitem">
     [77]
    </span>
    <span class="ltx_bibblock">
     T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
dense object detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">
      Proceedings of the IEEE international
conference on computer vision
     </em>
     , 2017, pp. 2980–2988.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib78">
    <span class="ltx_tag ltx_tag_bibitem">
     [78]
    </span>
    <span class="ltx_bibblock">
     J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">
      arXiv
preprint arXiv:1804.02767
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib79">
    <span class="ltx_tag ltx_tag_bibitem">
     [79]
    </span>
    <span class="ltx_bibblock">
     T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">
      Proceedings of
the IEEE conference on computer vision and pattern recognition
     </em>
     , 2017, pp.
2117–2125.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib80">
    <span class="ltx_tag ltx_tag_bibitem">
     [80]
    </span>
    <span class="ltx_bibblock">
     Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality object
detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">
      Proceedings of the IEEE conference on computer vision
and pattern recognition
     </em>
     , 2018, pp. 6154–6162.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib81">
    <span class="ltx_tag ltx_tag_bibitem">
     [81]
    </span>
    <span class="ltx_bibblock">
     X. Lu, B. Li, Y. Yue, Q. Li, and J. Yan, “Grid r-cnn,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">
      Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     , 2019,
pp. 7363–7372.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib82">
    <span class="ltx_tag ltx_tag_bibitem">
     [82]
    </span>
    <span class="ltx_bibblock">
     D. T. Wei Xun, Y. L. Lim, and S. Srigrarom, “Drone detection using yolov3 with
transfer learning on nvidia jetson tx2,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">
      2021 Second International
Symposium on Instrumentation, Control, Artificial Intelligence, and Robotics
(ICA-SYMP)
     </em>
     , 2021, pp. 1–6.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib83">
    <span class="ltx_tag ltx_tag_bibitem">
     [83]
    </span>
    <span class="ltx_bibblock">
     S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A. Bagnell,
and M. Hebert, “Learning monocular reactive uav control in cluttered natural
environments,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">
      2013 IEEE international conference on robotics and
automation
     </em>
     .   IEEE, 2013, pp.
1765–1772.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib84">
    <span class="ltx_tag ltx_tag_bibitem">
     [84]
    </span>
    <span class="ltx_bibblock">
     L. Xie, S. Wang, A. Markham, and N. Trigoni, “Towards monocular vision based
obstacle avoidance through deep reinforcement learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">
      arXiv
preprint arXiv:1706.09829
     </em>
     , 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib85">
    <span class="ltx_tag ltx_tag_bibitem">
     [85]
    </span>
    <span class="ltx_bibblock">
     K. Mohta, M. Watterson, Y. Mulgaonkar, S. Liu, C. Qu, A. Makineni, K. Saulnier,
K. Sun, A. Zhu, J. Delmerico
     <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">
      et al.
     </em>
     , “Fast, autonomous flight in
gps-denied and cluttered environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib85.2.2">
      Journal of Field Robotics
     </em>
     ,
vol. 35, no. 1, pp. 101–120, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib86">
    <span class="ltx_tag ltx_tag_bibitem">
     [86]
    </span>
    <span class="ltx_bibblock">
     F. Gao, W. Wu, J. Pan, B. Zhou, and S. Shen, “Optimal Time Allocation for
Quadrotor Trajectory Generation,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">
      IEEE International Conference on
Intelligent Robots and Systems
     </em>
     .   Institute of Electrical and Electronics Engineers Inc., dec 2018, pp.
4715–4722.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib87">
    <span class="ltx_tag ltx_tag_bibitem">
     [87]
    </span>
    <span class="ltx_bibblock">
     F. Gao, L. Wang, B. Zhou, X. Zhou, J. Pan, and S. Shen, “Teach-repeat-replan:
A complete and robust system for aggressive flight in complex environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">
      IEEE Transactions on Robotics
     </em>
     , vol. 36, no. 5, pp. 1526–1545, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib88">
    <span class="ltx_tag ltx_tag_bibitem">
     [88]
    </span>
    <span class="ltx_bibblock">
     F. Gao, W. Wu, W. Gao, and S. Shen, “Flying on point clouds: Online
trajectory generation and autonomous navigation for quadrotors in cluttered
environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">
      Journal of Field Robotics
     </em>
     , vol. 36, no. 4, pp.
710–733, jun 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib89">
    <span class="ltx_tag ltx_tag_bibitem">
     [89]
    </span>
    <span class="ltx_bibblock">
     F. Gao, L. Wang, B. Zhou, L. Han, J. Pan, and S. Shen, “Teach-repeat-replan:
A complete and robust system for aggressive flight in complex
environments,” pp. 1526–1545, may 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib90">
    <span class="ltx_tag ltx_tag_bibitem">
     [90]
    </span>
    <span class="ltx_bibblock">
     B. Zhou, F. Gao, L. Wang, C. Liu, and S. Shen, “Robust and efficient quadrotor
trajectory generation for fast autonomous flight,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">
      IEEE Robotics and
Automation Letters
     </em>
     , vol. 4, no. 4, pp. 3529–3536, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib91">
    <span class="ltx_tag ltx_tag_bibitem">
     [91]
    </span>
    <span class="ltx_bibblock">
     B. Zhou, J. Pan, F. Gao, and S. Shen, “Raptor: Robust and perception-aware
trajectory replanning for quadrotor fast flight,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">
      IEEE Transactions on
Robotics
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib92">
    <span class="ltx_tag ltx_tag_bibitem">
     [92]
    </span>
    <span class="ltx_bibblock">
     L. Quan, Z. Zhang, X. Zhong, C. Xu, and F. Gao, “Eva-planner: Environmental
adaptive quadrotor planning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">
      2021 IEEE International Conference on
Robotics and Automation (ICRA)
     </em>
     .   IEEE,
2021, pp. 398–404.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib93">
    <span class="ltx_tag ltx_tag_bibitem">
     [93]
    </span>
    <span class="ltx_bibblock">
     Y. Zhang, Q. Yu, K. H. Low, and C. Lv, “A self-supervised monocular depth
estimation approach based on uav aerial images,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">
      2022 IEEE/AIAA
41st Digital Avionics Systems Conference (DASC)
     </em>
     .   IEEE, 2022, pp. 1–8.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib94">
    <span class="ltx_tag ltx_tag_bibitem">
     [94]
    </span>
    <span class="ltx_bibblock">
     R. B. Rusu, Z. C. Marton, N. Blodow, M. Dolha, and M. Beetz, “Towards 3d point
cloud based object maps for household environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">
      Robotics and
Autonomous Systems
     </em>
     , vol. 56, no. 11, pp. 927–941, 2008.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib95">
    <span class="ltx_tag ltx_tag_bibitem">
     [95]
    </span>
    <span class="ltx_bibblock">
     A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard,
“OctoMap: An efficient probabilistic 3D mapping framework based on
octrees,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">
      Autonomous Robots
     </em>
     , 2013, software available at
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://octomap.github.io" target="_blank" title="">
      https://octomap.github.io
     </a>
     . [Online]. Available:
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://octomap.github.io" target="_blank" title="">
      https://octomap.github.io
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib96">
    <span class="ltx_tag ltx_tag_bibitem">
     [96]
    </span>
    <span class="ltx_bibblock">
     E. W. Dijkstra, “A note on two problems in connexion with graphs,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">
      Numerische Mathematik
     </em>
     , vol. 1, pp. 269–271, 1959.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib97">
    <span class="ltx_tag ltx_tag_bibitem">
     [97]
    </span>
    <span class="ltx_bibblock">
     P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the heuristic
determination of minimum cost paths,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">
      IEEE Transactions on Systems
Science and Cybernetics
     </em>
     , vol. 4, no. 2, pp. 100–107, 1968.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib98">
    <span class="ltx_tag ltx_tag_bibitem">
     [98]
    </span>
    <span class="ltx_bibblock">
     J. J. Kuffner and S. M. LaValle, “Rrt-connect: An efficient approach to
single-query path planning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">
      Proceedings 2000 ICRA. Millennium
Conference. IEEE International Conference on Robotics and Automation.
Symposia Proceedings (Cat. No. 00CH37065)
     </em>
     , vol. 2.   IEEE, 2000, pp. 995–1001.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib99">
    <span class="ltx_tag ltx_tag_bibitem">
     [99]
    </span>
    <span class="ltx_bibblock">
     F. Augugliaro, A. P. Schoellig, and R. D’Andrea, “Generation of collision-free
trajectories for a quadrocopter fleet: A sequential convex programming
approach,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">
      2012 IEEE/RSJ international conference on Intelligent
Robots and Systems
     </em>
     .   IEEE, 2012, pp.
1917–1922.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib100">
    <span class="ltx_tag ltx_tag_bibitem">
     [100]
    </span>
    <span class="ltx_bibblock">
     I. Iswanto, A. Ma’arif, O. Wahyunggoro, and A. Imam, “Artificial potential
field algorithm implementation for quadrotor path planning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">
      Int. J.
Adv. Comput. Sci. Appl
     </em>
     , vol. 10, no. 8, pp. 575–585, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib101">
    <span class="ltx_tag ltx_tag_bibitem">
     [101]
    </span>
    <span class="ltx_bibblock">
     T. Huang, S. Zhao, L. Geng, and Q. Xu, “Unsupervised monocular depth
estimation based on residual neural network of coarse–refined feature
extractions for drone,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">
      Electronics
     </em>
     , vol. 8, no. 10, p. 1179, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib102">
    <span class="ltx_tag ltx_tag_bibitem">
     [102]
    </span>
    <span class="ltx_bibblock">
     O. Khatib, “Real-time obstacle avoidance for manipulators and mobile robots,”
in
     <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">
      Proceedings. 1985 IEEE International Conference on Robotics and
Automation
     </em>
     , vol. 2.   IEEE, 1985, pp.
500–505.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib103">
    <span class="ltx_tag ltx_tag_bibitem">
     [103]
    </span>
    <span class="ltx_bibblock">
     X. Dai, Y. Mao, T. Huang, N. Qin, D. Huang, and Y. Li, “Automatic obstacle
avoidance of quadrotor uav via cnn-based learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">
      Neurocomputing
     </em>
     ,
vol. 402, pp. 346–358, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib104">
    <span class="ltx_tag ltx_tag_bibitem">
     [104]
    </span>
    <span class="ltx_bibblock">
     M. A. Anwar and A. Raychowdhury, “Navren-rl: Learning to fly in real
environment via end-to-end deep reinforcement learning using monocular
images,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">
      2018 25th International Conference on Mechatronics and
Machine Vision in Practice (M2VIP)
     </em>
     .   IEEE, 2018, pp. 1–6.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib105">
    <span class="ltx_tag ltx_tag_bibitem">
     [105]
    </span>
    <span class="ltx_bibblock">
     Y. Zhang, K. H. Low, and C. Lyu, “Partially-observable monocular autonomous
navigation for uav through deep reinforcement learning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">
      AIAA
AVIATION 2023 Forum
     </em>
     , 2023, p. 3813.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib106">
    <span class="ltx_tag ltx_tag_bibitem">
     [106]
    </span>
    <span class="ltx_bibblock">
     R. S. Sutton and A. G. Barto,
     <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">
      Reinforcement learning: An
introduction
     </em>
     .   MIT press, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib107">
    <span class="ltx_tag ltx_tag_bibitem">
     [107]
    </span>
    <span class="ltx_bibblock">
     K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">
      Proceedings of the IEEE conference on computer vision
and pattern recognition
     </em>
     , 2016, pp. 770–778.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib108">
    <span class="ltx_tag ltx_tag_bibitem">
     [108]
    </span>
    <span class="ltx_bibblock">
     K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">
      arXiv preprint arXiv:1409.1556
     </em>
     , 2014.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib109">
    <span class="ltx_tag ltx_tag_bibitem">
     [109]
    </span>
    <span class="ltx_bibblock">
     V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,
C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,
S. Legg, and D. Hassabis, “Human-level control through deep reinforcement
learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">
      Nature
     </em>
     , vol. 518, no. 7540, pp. 529–533, feb 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib110">
    <span class="ltx_tag ltx_tag_bibitem">
     [110]
    </span>
    <span class="ltx_bibblock">
     J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
policy optimization algorithms,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">
      arXiv preprint arXiv:1707.06347
     </em>
     ,
2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib111">
    <span class="ltx_tag ltx_tag_bibitem">
     [111]
    </span>
    <span class="ltx_bibblock">
     C. J. Watkins and P. Dayan, “Q-learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">
      Machine learning
     </em>
     , vol. 8,
no. 3, pp. 279–292, 1992.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib112">
    <span class="ltx_tag ltx_tag_bibitem">
     [112]
    </span>
    <span class="ltx_bibblock">
     H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
double q-learning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">
      Proceedings of the AAAI conference on
artificial intelligence
     </em>
     , vol. 30, no. 1, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib113">
    <span class="ltx_tag ltx_tag_bibitem">
     [113]
    </span>
    <span class="ltx_bibblock">
     Z. Zhu, K. Lin, B. Dai, and J. Zhou, “Off-policy imitation learning from
observations,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">
      Advances in Neural Information Processing Systems
     </em>
     ,
vol. 33, pp. 12 402–12 413, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib114">
    <span class="ltx_tag ltx_tag_bibitem">
     [114]
    </span>
    <span class="ltx_bibblock">
     J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
“Counterfactual multi-agent policy gradients,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">
      Proceedings of the
AAAI conference on artificial intelligence
     </em>
     , vol. 32, no. 1, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib115">
    <span class="ltx_tag ltx_tag_bibitem">
     [115]
    </span>
    <span class="ltx_bibblock">
     T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
S. Whiteson, “Monotonic value function factorisation for deep multi-agent
reinforcement learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">
      The Journal of Machine Learning Research
     </em>
     ,
vol. 21, no. 1, pp. 7234–7284, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib116">
    <span class="ltx_tag ltx_tag_bibitem">
     [116]
    </span>
    <span class="ltx_bibblock">
     J. Xiao, Y. X. M. Tan, X. Zhou, and M. Feroskhan, “Learning collaborative
multi-target search for a visual drone swarm,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">
      2023 IEEE Conference
on Artificial Intelligence (CAI)
     </em>
     , 2023, pp. 5–7.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib117">
    <span class="ltx_tag ltx_tag_bibitem">
     [117]
    </span>
    <span class="ltx_bibblock">
     S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and
physical simulation for autonomous vehicles,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">
      Field and service
robotics
     </em>
     .   Springer, 2018, pp.
621–635.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib118">
    <span class="ltx_tag ltx_tag_bibitem">
     [118]
    </span>
    <span class="ltx_bibblock">
     A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy,
Y. Gao, H. Henry, M. Mattar
     <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">
      et al.
     </em>
     , “Unity: A general platform for
intelligent agents,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib118.2.2">
      arXiv preprint arXiv:1809.02627
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib119">
    <span class="ltx_tag ltx_tag_bibitem">
     [119]
    </span>
    <span class="ltx_bibblock">
     I. Zamora, N. G. Lopez, V. M. Vilches, and A. H. Cordero, “Extending the
openai gym for robotics: a toolkit for reinforcement learning using ros and
gazebo,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">
      arXiv preprint arXiv:1608.05742
     </em>
     , 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib120">
    <span class="ltx_tag ltx_tag_bibitem">
     [120]
    </span>
    <span class="ltx_bibblock">
     P. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin,
P. Abbeel, and W. Zaremba, “Transfer from simulation to real world through
learning deep inverse dynamics model,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">
      arXiv preprint
arXiv:1610.03518
     </em>
     , 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib121">
    <span class="ltx_tag ltx_tag_bibitem">
     [121]
    </span>
    <span class="ltx_bibblock">
     J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and
V. Vanhoucke, “Sim-to-real: Learning agile locomotion for quadruped
robots,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">
      arXiv preprint arXiv:1804.10332
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib122">
    <span class="ltx_tag ltx_tag_bibitem">
     [122]
    </span>
    <span class="ltx_bibblock">
     C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast
adaptation of deep networks,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">
      International conference on machine
learning
     </em>
     .   PMLR, 2017, pp. 1126–1135.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib123">
    <span class="ltx_tag ltx_tag_bibitem">
     [123]
    </span>
    <span class="ltx_bibblock">
     J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">
      2017 IEEE/RSJ international conference on intelligent
robots and systems (IROS)
     </em>
     .   IEEE,
2017, pp. 23–30.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib124">
    <span class="ltx_tag ltx_tag_bibitem">
     [124]
    </span>
    <span class="ltx_bibblock">
     O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew,
J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray
     <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">
      et al.
     </em>
     ,
“Learning dexterous in-hand manipulation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib124.2.2">
      The International Journal
of Robotics Research
     </em>
     , vol. 39, no. 1, pp. 3–20, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib125">
    <span class="ltx_tag ltx_tag_bibitem">
     [125]
    </span>
    <span class="ltx_bibblock">
     M. A. Akhloufi, S. Arola, and A. Bonnet, “Drones chasing drones: Reinforcement
learning and deep search area proposal,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">
      Drones
     </em>
     , vol. 3, no. 3,
p. 58, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib126">
    <span class="ltx_tag ltx_tag_bibitem">
     [126]
    </span>
    <span class="ltx_bibblock">
     S. Geyer and E. Johnson, “3d obstacle avoidance in adversarial environments
for unmanned aerial vehicles,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">
      AIAA Guidance, Navigation, and
Control Conference and Exhibit
     </em>
     , 2006, p. 6542.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib127">
    <span class="ltx_tag ltx_tag_bibitem">
     [127]
    </span>
    <span class="ltx_bibblock">
     F. Schilling, J. Lecoeur, F. Schiano, and D. Floreano, “Learning vision-based
flight in drone swarms by imitation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">
      IEEE Robotics and Automation
Letters
     </em>
     , vol. 4, no. 4, pp. 4523–4530, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib128">
    <span class="ltx_tag ltx_tag_bibitem">
     [128]
    </span>
    <span class="ltx_bibblock">
     Y. Xie, M. Lu, R. Peng, and P. Lu, “Learning agile flights through narrow gaps
with varying angles using onboard sensing,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">
      IEEE Robotics and
Automation Letters
     </em>
     , vol. 8, no. 9, pp. 5424–5431, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib129">
    <span class="ltx_tag ltx_tag_bibitem">
     [129]
    </span>
    <span class="ltx_bibblock">
     C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
estimation with left-right consistency,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">
      Proceedings of the IEEE
conference on computer vision and pattern recognition
     </em>
     , 2017, pp. 270–279.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib130">
    <span class="ltx_tag ltx_tag_bibitem">
     [130]
    </span>
    <span class="ltx_bibblock">
     C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, “Digging into
self-supervised monocular depth estimation,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">
      Proceedings of the
IEEE/CVF International Conference on Computer Vision
     </em>
     , 2019, pp. 3828–3838.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib131">
    <span class="ltx_tag ltx_tag_bibitem">
     [131]
    </span>
    <span class="ltx_bibblock">
     Y. Liu, L. Wang, and M. Liu, “Yolostereo3d: A step back to 2d for efficient
stereo 3d detection,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">
      2021 IEEE International Conference on
Robotics and Automation (ICRA)
     </em>
     .   IEEE,
2021, pp. 13 018–13 024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib132">
    <span class="ltx_tag ltx_tag_bibitem">
     [132]
    </span>
    <span class="ltx_bibblock">
     M. Mancini, G. Costante, P. Valigi, and T. A. Ciarfuglia, “Fast robust
monocular depth estimation for obstacle detection with fully convolutional
networks,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">
      2016 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)
     </em>
     .   IEEE,
2016, pp. 4296–4303.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib133">
    <span class="ltx_tag ltx_tag_bibitem">
     [133]
    </span>
    <span class="ltx_bibblock">
     A. Singh, D. Patil, and S. Omkar, “Eye in the sky: Real-time drone
surveillance system (dss) for violent individuals identification using
scatternet hybrid deep learning network,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">
      Proceedings of the IEEE
conference on computer vision and pattern recognition workshops
     </em>
     , 2018, pp.
1629–1637.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib134">
    <span class="ltx_tag ltx_tag_bibitem">
     [134]
    </span>
    <span class="ltx_bibblock">
     W. Li, H. Li, Q. Wu, X. Chen, and K. N. Ngan, “Simultaneously detecting and
counting dense vehicles from drone images,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">
      IEEE Transactions on
Industrial Electronics
     </em>
     , vol. 66, no. 12, pp. 9651–9662, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib135">
    <span class="ltx_tag ltx_tag_bibitem">
     [135]
    </span>
    <span class="ltx_bibblock">
     H. Zhou, H. Kong, L. Wei, D. Creighton, and S. Nahavandi, “On detecting road
regions in a single uav image,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">
      IEEE transactions on intelligent
transportation systems
     </em>
     , vol. 18, no. 7, pp. 1713–1722, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib136">
    <span class="ltx_tag ltx_tag_bibitem">
     [136]
    </span>
    <span class="ltx_bibblock">
     N. H. Motlagh, M. Bagaa, and T. Taleb, “Uav-based iot platform: A crowd
surveillance use case,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">
      IEEE Communications Magazine
     </em>
     , vol. 55, no. 2,
pp. 128–134, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib137">
    <span class="ltx_tag ltx_tag_bibitem">
     [137]
    </span>
    <span class="ltx_bibblock">
     M. A. Goodrich, B. S. Morse, D. Gerhardt, J. L. Cooper, M. Quigley, J. A.
Adams, and C. Humphrey, “Supporting wilderness search and rescue using a
camera-equipped mini uav,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">
      Journal of Field Robotics
     </em>
     , vol. 25, no.
1-2, pp. 89–110, 2008.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib138">
    <span class="ltx_tag ltx_tag_bibitem">
     [138]
    </span>
    <span class="ltx_bibblock">
     E. Lygouras, N. Santavas, A. Taitzoglou, K. Tarchanidis, A. Mitropoulos, and
A. Gasteratos, “Unsupervised human detection with an embedded vision system
on a fully autonomous uav for search and rescue operations,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">
      Sensors
     </em>
     ,
vol. 19, no. 16, p. 3542, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib139">
    <span class="ltx_tag ltx_tag_bibitem">
     [139]
    </span>
    <span class="ltx_bibblock">
     T. Tomic, K. Schmid, P. Lutz, A. Domel, M. Kassecker, E. Mair, I. L. Grixa,
F. Ruess, M. Suppa, and D. Burschka, “Toward a fully autonomous uav:
Research platform for indoor and outdoor urban search and rescue,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">
      IEEE robotics &amp; automation magazine
     </em>
     , vol. 19, no. 3, pp. 46–56,
2012.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib140">
    <span class="ltx_tag ltx_tag_bibitem">
     [140]
    </span>
    <span class="ltx_bibblock">
     J. Senthilnath, M. Kandukuri, A. Dokania, and K. Ramesh, “Application of uav
imaging platform for vegetation analysis based on spectral-spatial methods,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">
      Computers and Electronics in Agriculture
     </em>
     , vol. 140, pp. 8–24, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib141">
    <span class="ltx_tag ltx_tag_bibitem">
     [141]
    </span>
    <span class="ltx_bibblock">
     M. R. Khosravi and S. Samadi, “Bl-alm: A blind scalable edge-guided
reconstruction filter for smart environmental monitoring through green
iomt-uav networks,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">
      IEEE Transactions on Green Communications and
Networking
     </em>
     , vol. 5, no. 2, pp. 727–736, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib142">
    <span class="ltx_tag ltx_tag_bibitem">
     [142]
    </span>
    <span class="ltx_bibblock">
     C. Donmez, O. Villi, S. Berberoglu, and A. Cilek, “Computer vision-based
citrus tree detection in a cultivated environment using uav imagery,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">
      Computers and Electronics in Agriculture
     </em>
     , vol. 187, p. 106273, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib143">
    <span class="ltx_tag ltx_tag_bibitem">
     [143]
    </span>
    <span class="ltx_bibblock">
     B. Lu and Y. He, “Species classification using unmanned aerial vehicle
(uav)-acquired high spatial resolution imagery in a heterogeneous
grassland,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">
      ISPRS Journal of Photogrammetry and Remote Sensing
     </em>
     , vol.
128, pp. 73–85, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib144">
    <span class="ltx_tag ltx_tag_bibitem">
     [144]
    </span>
    <span class="ltx_bibblock">
     D. Kim, M. Liu, S. Lee, and V. R. Kamat, “Remote proximity monitoring between
mobile construction resources using camera-mounted uavs,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">
      Automation
in Construction
     </em>
     , vol. 99, pp. 168–182, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib145">
    <span class="ltx_tag ltx_tag_bibitem">
     [145]
    </span>
    <span class="ltx_bibblock">
     T. Khuc, T. A. Nguyen, H. Dao, and F. N. Catbas, “Swaying displacement
measurement for structural monitoring using computer vision and an unmanned
aerial vehicle,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">
      Measurement
     </em>
     , vol. 159, p. 107769, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib146">
    <span class="ltx_tag ltx_tag_bibitem">
     [146]
    </span>
    <span class="ltx_bibblock">
     S. Li, E. van der Horst, P. Duernay, C. De Wagter, and G. C. de Croon, “Visual
model-predictive localization for computationally efficient autonomous racing
of a 72-g drone,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">
      Journal of Field Robotics
     </em>
     , vol. 37, no. 4, pp.
667–692, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib147">
    <span class="ltx_tag ltx_tag_bibitem">
     [147]
    </span>
    <span class="ltx_bibblock">
     M. Muller, G. Li, V. Casser, N. Smith, D. L. Michels, and B. Ghanem, “Learning
a controller fusion network by online trajectory filtering for vision-based
uav racing,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">
      Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops
     </em>
     , 2019, pp. 0–0.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib148">
    <span class="ltx_tag ltx_tag_bibitem">
     [148]
    </span>
    <span class="ltx_bibblock">
     M. Muller, V. Casser, N. Smith, D. L. Michels, and B. Ghanem, “Teaching uavs
to race: End-to-end regression of agile controls in simulation,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">
      Proceedings of the European Conference on Computer Vision (ECCV)
Workshops
     </em>
     , 2018, pp. 0–0.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib149">
    <span class="ltx_tag ltx_tag_bibitem">
     [149]
    </span>
    <span class="ltx_bibblock">
     R. Penicka and D. Scaramuzza, “Minimum-time quadrotor waypoint flight in
cluttered environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">
      IEEE Robotics and Automation Letters
     </em>
     ,
vol. 7, no. 2, pp. 5719–5726, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib150">
    <span class="ltx_tag ltx_tag_bibitem">
     [150]
    </span>
    <span class="ltx_bibblock">
     E. Kaufmann, M. Gehrig, P. Foehn, R. Ranftl, A. Dosovitskiy, V. Koltun, and
D. Scaramuzza, “Beauty and the beast: Optimal methods meet learning for
drone racing,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">
      2019 International Conference on Robotics and
Automation (ICRA)
     </em>
     .   IEEE, 2019, pp.
690–696.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib151">
    <span class="ltx_tag ltx_tag_bibitem">
     [151]
    </span>
    <span class="ltx_bibblock">
     L. Xing, X. Fan, Y. Dong, Z. Xiong, L. Xing, Y. Yang, H. Bai, and C. Zhou,
“Multi-uav cooperative system for search and rescue based on yolov5,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">
      International Journal of Disaster Risk Reduction
     </em>
     , vol. 76, p. 102972,
2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib152">
    <span class="ltx_tag ltx_tag_bibitem">
     [152]
    </span>
    <span class="ltx_bibblock">
     B. Lin, L. Wu, and Y. Niu, “End-to-end vision-based cooperative target
geo-localization for multiple micro uavs,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">
      Journal of Intelligent &amp;
Robotic Systems
     </em>
     , vol. 106, no. 1, p. 13, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib153">
    <span class="ltx_tag ltx_tag_bibitem">
     [153]
    </span>
    <span class="ltx_bibblock">
     M. E. Campbell and W. W. Whitacre, “Cooperative tracking using vision
measurements on seascan uavs,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">
      IEEE Transactions on Control Systems
Technology
     </em>
     , vol. 15, no. 4, pp. 613–626, 2007.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib154">
    <span class="ltx_tag ltx_tag_bibitem">
     [154]
    </span>
    <span class="ltx_bibblock">
     J. Gu, T. Su, Q. Wang, X. Du, and M. Guizani, “Multiple moving targets
surveillance based on a cooperative network for multi-uav,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">
      IEEE
Communications Magazine
     </em>
     , vol. 56, no. 4, pp. 82–89, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib155">
    <span class="ltx_tag ltx_tag_bibitem">
     [155]
    </span>
    <span class="ltx_bibblock">
     Y. Cao, F. Qi, Y. Jing, M. Zhu, T. Lei, Z. Li, J. Xia, J. Wang, and G. Lu,
“Mission chain driven unmanned aerial vehicle swarms cooperation for the
search and rescue of outdoor injured human targets,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">
      Drones
     </em>
     , vol. 6,
no. 6, p. 138, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib156">
    <span class="ltx_tag ltx_tag_bibitem">
     [156]
    </span>
    <span class="ltx_bibblock">
     G. Loianno, J. Thomas, and V. Kumar, “Cooperative localization and mapping of
mavs using rgb-d sensors,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">
      2015 IEEE International Conference on
Robotics and Automation (ICRA)
     </em>
     .   IEEE,
2015, pp. 4021–4028.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib157">
    <span class="ltx_tag ltx_tag_bibitem">
     [157]
    </span>
    <span class="ltx_bibblock">
     P. Tong, X. Yang, Y. Yang, W. Liu, and P. Wu, “Multi-uav collaborative
absolute vision positioning and navigation: A survey and discussion,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">
      Drones
     </em>
     , vol. 7, no. 4, p. 261, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib158">
    <span class="ltx_tag ltx_tag_bibitem">
     [158]
    </span>
    <span class="ltx_bibblock">
     N. Piasco, J. Marzat, and M. Sanfourche, “Collaborative localization and
formation flying using distributed stereo-vision,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">
      2016 IEEE
International Conference on Robotics and Automation (ICRA)
     </em>
     .   IEEE, 2016, pp. 1202–1207.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib159">
    <span class="ltx_tag ltx_tag_bibitem">
     [159]
    </span>
    <span class="ltx_bibblock">
     D. Liu, X. Zhu, W. Bao, B. Fei, and J. Wu, “Smart: Vision-based method of
cooperative surveillance and tracking by multiple uavs in the urban
environment,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">
      IEEE Transactions on Intelligent Transportation
Systems
     </em>
     , vol. 23, no. 12, pp. 24 941–24 956, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib160">
    <span class="ltx_tag ltx_tag_bibitem">
     [160]
    </span>
    <span class="ltx_bibblock">
     N. Farmani, L. Sun, and D. J. Pack, “A scalable multitarget tracking system
for cooperative unmanned aerial vehicles,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">
      IEEE Transactions on
Aerospace and Electronic Systems
     </em>
     , vol. 53, no. 4, pp. 1947–1961, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib161">
    <span class="ltx_tag ltx_tag_bibitem">
     [161]
    </span>
    <span class="ltx_bibblock">
     M. Jouhari, A. K. Al-Ali, E. Baccour, A. Mohamed, A. Erbad, M. Guizani, and
M. Hamdi, “Distributed cnn inference on resource-constrained uavs for
surveillance systems: Design and optimization,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">
      IEEE Internet of
Things Journal
     </em>
     , vol. 9, no. 2, pp. 1227–1242, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib162">
    <span class="ltx_tag ltx_tag_bibitem">
     [162]
    </span>
    <span class="ltx_bibblock">
     W. J. Yun, S. Park, J. Kim, M. Shin, S. Jung, D. A. Mohaisen, and J.-H. Kim,
“Cooperative multiagent deep reinforcement learning for reliable
surveillance via autonomous multi-uav control,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">
      IEEE Transactions on
Industrial Informatics
     </em>
     , vol. 18, no. 10, pp. 7086–7096, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib163">
    <span class="ltx_tag ltx_tag_bibitem">
     [163]
    </span>
    <span class="ltx_bibblock">
     Y. Tang, Y. Hu, J. Cui, F. Liao, M. Lao, F. Lin, and R. S. Teo, “Vision-aided
multi-uav autonomous flocking in gps-denied environment,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">
      IEEE
Transactions on industrial electronics
     </em>
     , vol. 66, no. 1, pp. 616–626, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib164">
    <span class="ltx_tag ltx_tag_bibitem">
     [164]
    </span>
    <span class="ltx_bibblock">
     J. Scherer, S. Yahyanejad, S. Hayat, E. Yanmaz, T. Andre, A. Khan,
V. Vukadinovic, C. Bettstetter, H. Hellwagner, and B. Rinner, “An autonomous
multi-uav system for search and rescue,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">
      Proceedings of the first
workshop on micro aerial vehicle networks, systems, and applications for
civilian use
     </em>
     , 2015, pp. 33–38.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib165">
    <span class="ltx_tag ltx_tag_bibitem">
     [165]
    </span>
    <span class="ltx_bibblock">
     Y. Rizk, M. Awad, and E. W. Tunstel, “Cooperative heterogeneous multi-robot
systems: A survey,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">
      ACM Computing Surveys (CSUR)
     </em>
     , vol. 52, no. 2, pp.
1–31, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib166">
    <span class="ltx_tag ltx_tag_bibitem">
     [166]
    </span>
    <span class="ltx_bibblock">
     G. Niu, L. Wu, Y. Gao, and M.-O. Pun, “Unmanned aerial vehicle (uav)-assisted
path planning for unmanned ground vehicles (ugvs) via disciplined
convex-concave programming,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">
      IEEE Transactions on Vehicular
Technology
     </em>
     , vol. 71, no. 7, pp. 6996–7007, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib167">
    <span class="ltx_tag ltx_tag_bibitem">
     [167]
    </span>
    <span class="ltx_bibblock">
     D. Liu, W. Bao, X. Zhu, B. Fei, Z. Xiao, and T. Men, “Vision-aware air-ground
cooperative target localization for uav and ugv,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">
      Aerospace Science
and Technology
     </em>
     , vol. 124, p. 107525, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib168">
    <span class="ltx_tag ltx_tag_bibitem">
     [168]
    </span>
    <span class="ltx_bibblock">
     J. Li, G. Deng, C. Luo, Q. Lin, Q. Yan, and Z. Ming, “A hybrid path planning
method in unmanned air/ground vehicle (uav/ugv) cooperative systems,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">
      IEEE Transactions on Vehicular Technology
     </em>
     , vol. 65, no. 12, pp.
9585–9596, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib169">
    <span class="ltx_tag ltx_tag_bibitem">
     [169]
    </span>
    <span class="ltx_bibblock">
     L. Zhang, F. Gao, F. Deng, L. Xi, and J. Chen, “Distributed estimation of a
layered architecture for collaborative air–ground target geolocation in
outdoor environments,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">
      IEEE Transactions on Industrial Electronics
     </em>
     ,
vol. 70, no. 3, pp. 2822–2832, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib170">
    <span class="ltx_tag ltx_tag_bibitem">
     [170]
    </span>
    <span class="ltx_bibblock">
     G. Niu, Q. Yang, Y. Gao, and M.-O. Pun, “Vision-based autonomous landing for
unmanned aerial and ground vehicles cooperative systems,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">
      IEEE
robotics and automation letters
     </em>
     , vol. 7, no. 3, pp. 6234–6241, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib171">
    <span class="ltx_tag ltx_tag_bibitem">
     [171]
    </span>
    <span class="ltx_bibblock">
     Z.-C. Xu, B.-B. Hu, B. Liu, X. Wang, and H.-T. Zhang, “Vision-based autonomous
landing of unmanned aerial vehicle on a motional unmanned surface vessel,”
in
     <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">
      2020 39th Chinese Control Conference (CCC)
     </em>
     .   IEEE, 2020, pp. 6845–6850.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib172">
    <span class="ltx_tag ltx_tag_bibitem">
     [172]
    </span>
    <span class="ltx_bibblock">
     C. Hui, C. Yousheng, L. Xiaokun, and W. W. Shing, “Autonomous takeoff,
tracking and landing of a uav on a moving ugv using onboard monocular
vision,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">
      Proceedings of the 32nd Chinese control conference
     </em>
     .   IEEE, 2013, pp. 5895–5901.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib173">
    <span class="ltx_tag ltx_tag_bibitem">
     [173]
    </span>
    <span class="ltx_bibblock">
     I. Kalinov, A. Petrovsky, V. Ilin, E. Pristanskiy, M. Kurenkov, V. Ramzhaev,
I. Idrisov, and D. Tsetserukou, “Warevision: Cnn barcode detection-based uav
trajectory optimization for autonomous warehouse stocktaking,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">
      IEEE
Robotics and Automation Letters
     </em>
     , vol. 5, no. 4, pp. 6647–6653, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib174">
    <span class="ltx_tag ltx_tag_bibitem">
     [174]
    </span>
    <span class="ltx_bibblock">
     S. Minaeian, J. Liu, and Y.-J. Son, “Vision-based target detection and
localization via a team of cooperative uav and ugvs,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">
      IEEE
Transactions on systems, man, and cybernetics: systems
     </em>
     , vol. 46, no. 7, pp.
1005–1016, 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib175">
    <span class="ltx_tag ltx_tag_bibitem">
     [175]
    </span>
    <span class="ltx_bibblock">
     A. Adaptable, “Building an aerial–ground robotics system for precision
farming,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">
      IEEE ROBOTICS &amp; AUTOMATION MAGAZINE
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib176">
    <span class="ltx_tag ltx_tag_bibitem">
     [176]
    </span>
    <span class="ltx_bibblock">
     Q. Vuong, S. Levine, H. R. Walke, K. Pertsch, A. Singh, R. Doshi, C. Xu,
J. Luo, L. Tan, D. Shah
     <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">
      et al.
     </em>
     , “Open x-embodiment: Robotic learning
datasets and rt-x models,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib176.2.2">
      2nd Workshop on Language and Robot
Learning: Language as Grounding
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib177">
    <span class="ltx_tag ltx_tag_bibitem">
     [177]
    </span>
    <span class="ltx_bibblock">
     N. Jiang, K. Wang, X. Peng, X. Yu, Q. Wang, J. Xing, G. Li, G. Guo, Q. Ye,
J. Jiao, J. Zhao, and Z. Han, “Anti-uav: A large-scale benchmark for
vision-based uav tracking,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">
      IEEE Transactions on Multimedia
     </em>
     , vol. 25,
pp. 486–500, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib178">
    <span class="ltx_tag ltx_tag_bibitem">
     [178]
    </span>
    <span class="ltx_bibblock">
     Y. Zhao, Z. Ju, T. Sun, F. Dong, J. Li, R. Yang, Q. Fu, C. Lian, and P. Shan,
“Tgc-yolov5: An enhanced yolov5 drone detection model based on transformer,
gam &amp; ca attention mechanism,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">
      Drones
     </em>
     , vol. 7, no. 7, p. 446, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib179">
    <span class="ltx_tag ltx_tag_bibitem">
     [179]
    </span>
    <span class="ltx_bibblock">
     Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scaramuzza, “Flightmare: A
flexible quadrotor simulator,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">
      Conference on Robot Learning
     </em>
     .   PMLR, 2021, pp. 1147–1157.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib180">
    <span class="ltx_tag ltx_tag_bibitem">
     [180]
    </span>
    <span class="ltx_bibblock">
     J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoellig,
“Learning to fly—a gym environment with pybullet physics for reinforcement
learning of multi-agent quadcopter control,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">
      2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS)
     </em>
     , 2021, pp.
7512–7519.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib181">
    <span class="ltx_tag ltx_tag_bibitem">
     [181]
    </span>
    <span class="ltx_bibblock">
     A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla: An open
urban driving simulator,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">
      Conference on robot learning
     </em>
     .   PMLR, 2017, pp. 1–16.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib182">
    <span class="ltx_tag ltx_tag_bibitem">
     [182]
    </span>
    <span class="ltx_bibblock">
     F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, “A
comprehensive survey on transfer learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">
      Proceedings of the IEEE
     </em>
     ,
vol. 109, no. 1, pp. 43–76, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib183">
    <span class="ltx_tag ltx_tag_bibitem">
     [183]
    </span>
    <span class="ltx_bibblock">
     Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a few examples:
A survey on few-shot learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">
      ACM computing surveys (csur)
     </em>
     ,
vol. 53, no. 3, pp. 1–34, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib184">
    <span class="ltx_tag ltx_tag_bibitem">
     [184]
    </span>
    <span class="ltx_bibblock">
     J. Fonseca and F. Bacao, “Tabular and latent space synthetic data generation:
a literature review,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">
      Journal of Big Data
     </em>
     , vol. 10, no. 1, p. 115,
2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib185">
    <span class="ltx_tag ltx_tag_bibitem">
     [185]
    </span>
    <span class="ltx_bibblock">
     Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value of
network pruning,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">
      International Conference on Learning
Representations
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib186">
    <span class="ltx_tag ltx_tag_bibitem">
     [186]
    </span>
    <span class="ltx_bibblock">
     Y. Jiang, S. Wang, V. Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and
L. Tassiulas, “Model pruning enables efficient federated learning on edge
devices,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">
      IEEE Transactions on Neural Networks and Learning Systems
     </em>
     ,
2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib187">
    <span class="ltx_tag ltx_tag_bibitem">
     [187]
    </span>
    <span class="ltx_bibblock">
     Y. Zhou, S.-M. Moosavi-Dezfooli, N.-M. Cheung, and P. Frossard, “Adaptive
quantization for deep neural network,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">
      Proceedings of the AAAI
Conference on Artificial Intelligence
     </em>
     , vol. 32, no. 1, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib188">
    <span class="ltx_tag ltx_tag_bibitem">
     [188]
    </span>
    <span class="ltx_bibblock">
     L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, “Model compression and hardware
acceleration for neural networks: A comprehensive survey,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">
      Proceedings
of the IEEE
     </em>
     , vol. 108, no. 4, pp. 485–532, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib189">
    <span class="ltx_tag ltx_tag_bibitem">
     [189]
    </span>
    <span class="ltx_bibblock">
     L. Wang and K.-J. Yoon, “Knowledge distillation and student-teacher learning
for visual intelligence: A review and new outlooks,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">
      IEEE transactions
on pattern analysis and machine intelligence
     </em>
     , vol. 44, no. 6, pp.
3048–3068, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib190">
    <span class="ltx_tag ltx_tag_bibitem">
     [190]
    </span>
    <span class="ltx_bibblock">
     R. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu,
D. Terzopoulos, L. Fei-Fei
     <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">
      et al.
     </em>
     , “Mindagent: Emergent gaming
interaction,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib190.2.2">
      arXiv preprint arXiv:2309.09971
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib191">
    <span class="ltx_tag ltx_tag_bibitem">
     [191]
    </span>
    <span class="ltx_bibblock">
     A. Bhoopchand, B. Brownfield, A. Collister, A. Dal Lago, A. Edwards,
R. Everett, A. Fréchette, Y. G. Oliveira, E. Hughes, K. W. Mathewson
     <em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">
      et al.
     </em>
     , “Learning few-shot imitation as cultural transmission,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib191.2.2">
      Nature Communications
     </em>
     , vol. 14, no. 1, p. 7536, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib192">
    <span class="ltx_tag ltx_tag_bibitem">
     [192]
    </span>
    <span class="ltx_bibblock">
     J. Xiao and M. Feroskhan, “Cyber attack detection and isolation for a
quadrotor uav with modified sliding innovation sequences,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">
      IEEE
Transactions on Vehicular Technology
     </em>
     , vol. 71, no. 7, pp. 7202–7214, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib193">
    <span class="ltx_tag ltx_tag_bibitem">
     [193]
    </span>
    <span class="ltx_bibblock">
     T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber
security,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">
      IEEE Transactions on Neural Networks and Learning Systems
     </em>
     ,
vol. 34, no. 8, pp. 3779–3795, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib194">
    <span class="ltx_tag ltx_tag_bibitem">
     [194]
    </span>
    <span class="ltx_bibblock">
     I. Ilahi, M. Usama, J. Qadir, M. U. Janjua, A. Al-Fuqaha, D. T. Hoang, and
D. Niyato, “Challenges and countermeasures for adversarial attacks on deep
reinforcement learning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">
      IEEE Transactions on Artificial
Intelligence
     </em>
     , vol. 3, no. 2, pp. 90–109, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib195">
    <span class="ltx_tag ltx_tag_bibitem">
     [195]
    </span>
    <span class="ltx_bibblock">
     A. Rawal, J. McCoy, D. B. Rawat, B. M. Sadler, and R. S. Amant, “Recent
advances in trustworthy explainable artificial intelligence: Status,
challenges, and perspectives,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">
      IEEE Transactions on Artificial
Intelligence
     </em>
     , vol. 3, no. 6, pp. 852–866, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib196">
    <span class="ltx_tag ltx_tag_bibitem">
     [196]
    </span>
    <span class="ltx_bibblock">
     G. A. Vouros, “Explainable deep reinforcement learning: state of the art and
challenges,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">
      ACM Computing Surveys
     </em>
     , vol. 55, no. 5, pp. 1–39, 2022.
    </span>
   </li>
  </ul>
 </section>
</article>
