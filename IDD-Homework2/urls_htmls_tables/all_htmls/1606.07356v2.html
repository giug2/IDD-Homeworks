<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1606.07356] Analyzing the Behavior of Visual Question Answering Models</title><meta property="og:description" content="Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic method…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Analyzing the Behavior of Visual Question Answering Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Analyzing the Behavior of Visual Question Answering Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1606.07356">

<!--Generated on Tue Mar 19 20:25:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Analyzing the Behavior of Visual Question Answering Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aishwarya Agrawal<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>, Dhruv Batra<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">†,∗</span></sup>, Devi Parikh<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">†,∗</span></sup> 
<br class="ltx_break"><sup id="id9.9.id4" class="ltx_sup"><span id="id9.9.id4.1" class="ltx_text ltx_font_italic">∗</span></sup>Virginia Tech  <sup id="id10.10.id5" class="ltx_sup"><span id="id10.10.id5.1" class="ltx_text ltx_font_italic">†</span></sup>Georgia Institute of Technology
<br class="ltx_break"><span id="id11.11.id6" class="ltx_text ltx_font_typewriter">{aish, dbatra, parikh}@vt.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models – with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.</p>
<p id="id13.id2" class="ltx_p">Our behavior analysis reveals that despite recent progress, today’s VQA models are “myopic” (tend to fail on sufficiently novel instances), often “jump to conclusions” (converge on a predicted answer after ‘listening’ to just half the question), and are “stubborn” (do not change their answers across images).</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is a recently-introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Antol et al., 2015</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Geman et al., 2014</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Malinowski and Fritz, 2014</a>]</cite> problem where given an image and a natural language question (e.g., “What kind of store is this?”, “How many people are waiting in the queue?”), the task is to automatically produce an accurate natural language answer (“bakery”, “5”). A flurry of recent deep-learning based models have been proposed for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Antol et al., 2015</a>, <a href="#bib.bibx4" title="" class="ltx_ref">Chen et al., 2015</a>, <a href="#bib.bibx24" title="" class="ltx_ref">Yang et al., 2016</a>, <a href="#bib.bibx23" title="" class="ltx_ref">Xu and Saenko, 2016</a>, <a href="#bib.bibx9" title="" class="ltx_ref">Jiang et al., 2015</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Andreas et al., 2016a</a>, <a href="#bib.bibx20" title="" class="ltx_ref">Wang et al., 2015</a>, <a href="#bib.bibx10" title="" class="ltx_ref">Kafle and Kanan, 2016</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Lu et al., 2016</a>, <a href="#bib.bibx2" title="" class="ltx_ref">Andreas et al., 2016b</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Shih et al., 2016</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Kim et al., 2016</a>, <a href="#bib.bibx5" title="" class="ltx_ref">Fukui et al., 2016</a>, <a href="#bib.bibx17" title="" class="ltx_ref">Noh and Han, 2016</a>, <a href="#bib.bibx8" title="" class="ltx_ref">Ilievski et al., 2016</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Wu et al., 2016</a>, <a href="#bib.bibx22" title="" class="ltx_ref">Xiong et al., 2016</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Zhou et al., 2015</a>, <a href="#bib.bibx18" title="" class="ltx_ref">Saito et al., 2016</a>]</cite>. Curiously, the performance of most methods is clustered around 60-70% (compared to human performance of 83% on open-ended task and 91% on multiple-choice task) with a mere 5% gap between the top-9 entries on the VQA Challenge 2016.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://www.visualqa.org/challenge.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.visualqa.org/challenge.html</a></span></span></span> It seems clear that as a first step to understand these models, to meaningfully compare strengths and weaknesses of different models, to develop insights into their failure modes, and to identify the most fruitful directions for progress, it is crucial to develop techniques to understand the behavior of VQA models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we develop novel techniques to characterize the behavior of VQA models. As concrete instantiations, we analyze two VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Lu et al., 2015</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Lu et al., 2016</a>]</cite>, one each from two major classes of VQA models – with-attention and without-attention. We also analyze the winning entry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Fukui et al., 2016</a>]</cite> of the VQA Challenge 2016.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work is inspired by previous works that diagnose the failure modes of models for different tasks. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Karpathy et al., 2016</a>]</cite> constructed a series of oracles to measure the performance of a character level language model. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hoiem et al., 2012</a>]</cite> provided analysis tools to facilitate detailed and meaningful investigation of object detector performance. This paper aims to perform behavior analyses as a first step towards diagnosing errors for VQA.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Yang et al., 2016</a>]</cite> categorize the errors made by their VQA model into four categories – model focuses attention on incorrect regions, model focuses attention on appropriate regions but predicts incorrect answers, predicted answers are different from labels but might be acceptable, labels are wrong. While these are coarse but useful failure modes, we are interested in understanding the behavior of VQA models along specific dimensions – whether they generalize to novel instances, whether they listen to the entire question, whether they look at the image.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Behavior Analyses</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We analyze the behavior of VQA models along the following three dimensions –</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Generalization to novel instances:</span> We investigate whether the test instances that are incorrectly answered are the ones that are “novel” i.e., not similar to training instances. The novelty of the test instances may be in two ways – 1) the test question-image (QI) pair is “novel”, i.e., too different from training QI pairs; and 2) the test QI pair is “familiar”, but the answer required at test time is “novel”, i.e., answers seen during training are different from what needs to be produced for the test QI pairs.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Complete question understanding:</span> To investigate whether a VQA model is understanding the input question or not, we analyze if the model ‘listens’ to only first few words of the question or the entire question, if it ‘listens’ to only question (wh) words and nouns or all the words in the question.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Complete image understanding:</span> The absence of a large gap between performance of language-alone and language + vision VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Antol et al., 2015</a>]</cite> provides evidence that current VQA models seem to be heavily reliant on the language model, perhaps not really understanding the image. In order to analyze this behavior, we investigate whether the predictions of the model change across images for a given question.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.3" class="ltx_p">We present our behavioral analyses on the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Antol et al., 2015</a>]</cite>. VQA is a large-scale free-form natural-language dataset containing <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.p5.1.m1.1a"><mo id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><csymbol cd="latexml" id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">\sim</annotation></semantics></math>0.25M images, <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.p5.2.m2.1a"><mo id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><csymbol cd="latexml" id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">\sim</annotation></semantics></math>0.76M questions, and <math id="S3.p5.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.p5.3.m3.1a"><mo id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><csymbol cd="latexml" id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">\sim</annotation></semantics></math>10M answers, with open-ended and multiple-choice modalities for answering the visual questions. All the experimental results are reported on the VQA validation set using the following models trained on the VQA training set for the open-ended task –</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">CNN + LSTM based model without-attention (<span id="S3.p6.1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span>):</span> We use the best performing model of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Antol et al., 2015</a>]</cite> (code provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Lu et al., 2015</a>]</cite>), which achieves an accuracy of 54.13% on the VQA validation set. It is a two channel model – one channel processes the image (using Convolutional Neural Network (CNN) to extract image features) and the other channel processes the question (using Long Short-Term Memory (LSTM) recurrent neural network to obtain question embedding). The image and question features obtained from the two channels are combined and passed through a fully connected (FC) layer to obtain a softmax distribution over the space of answers.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_font_bold">CNN + LSTM based model with-attention (<span id="S3.p7.1.1.1" class="ltx_text ltx_font_smallcaps">ATT</span>):</span> We use the top-entry on the VQA challenge leaderboard (as of June 03, 2016) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Lu et al., 2016</a>]</cite>, which achieves an accuracy of 57.02% on the VQA validation set.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Code available at <a target="_blank" href="https://github.com/jiasenlu/HieCoAttenVQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jiasenlu/HieCoAttenVQA</a></span></span></span> This model jointly reasons about image and question attention, in a hierarchical fashion. The attended image and question features obtained from different levels of the hierarchy are combined and passed through a FC layer to obtain a softmax distribution over the space of answers.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p"><span id="S3.p8.1.1" class="ltx_text ltx_font_bold">VQA Challenge 2016 winning entry (<span id="S3.p8.1.1.1" class="ltx_text ltx_font_smallcaps">MCB</span>):</span> This is the multimodal compact bilinear (mcb) pooling model from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Fukui et al., 2016</a>]</cite> which won the real image track of the VQA Challenge 2016. This model achieves an accuracy of 60.36% on the VQA validation set.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Code available at <a target="_blank" href="https://github.com/akirafukui/vqa-mcb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/akirafukui/vqa-mcb</a></span></span></span> In this model, multimodal compact bilinear pooling is used to predict attention over image features and also to combine the attended image features with the question features. These combined features are passed through a FC layer to obtain a softmax distribution over the space of answers.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generalization to novel instances</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Do VQA models make mistakes because test instances are too different from training ones? To analyze the first type of novelty (the test QI pair is novel), we measure the correlation between test accuracy and distance of test QI pairs from its k nearest neighbor (k-NN) training QI pairs. For each test QI pair we find its k-NNs in the training set and compute the average distance between the test QI pair and its k-NNs. The k-NNs are computed in the space of combined image + question embedding (just before passing through FC layer) for all the three models (using euclidean distance metric for the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model and cosine distance metric for the <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> and <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">MCB</span> models).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The correlation between accuracy and average distance is significant (-0.41 at k=50<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>k=50 leads to highest correlation</span></span></span> for the <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model and -0.42 at k=15<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>k=15 leads to highest correlation</span></span></span> for the <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> model). A high negative correlation value tells that the model is less likely to predict correct answers for test QI pairs which are not very similar to training QI pairs, suggesting that the model is not very good at generalizing to novel test QI pairs. The correlation between accuracy and average distance is not significant for the <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_smallcaps">MCB</span> model (-0.14 at k=1<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>k=1 leads to highest correlation</span></span></span>) suggesting that <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_smallcaps">MCB</span> is better at generalizing to novel test QI pairs.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We also found that 67.5% of mistakes made by the <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">can be successfully predicted</em> by checking distance of test QI pair from its k-NN training QI pairs (66.7% for the <span id="S3.SS1.p3.1.3" class="ltx_text ltx_font_smallcaps">ATT</span> model, 55.08% for the <span id="S3.SS1.p3.1.4" class="ltx_text ltx_font_smallcaps">MCB</span> model). Thus, this analysis not only exposes a reason for mistakes made by VQA models, but also allows us to build human-like models that can predict their own oncoming failures, and potentially refuse to answer questions that are ‘too different’ from ones seen in past.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">To analyze the second type of novelty (the answer required at test time is not familiar), we compute the correlation between test accuracy and the average distance of the test ground truth (GT) answer with GT answers of its k-NN training QI pairs. The distance between answers is computed in the space of average Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Mikolov et al., 2013</a>]</cite> vectors of answers. This correlation turns out to be quite high (-0.62) for both <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> and <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> models and significant (-0.47) for the <span id="S3.SS1.p4.1.3" class="ltx_text ltx_font_smallcaps">MCB</span> model. A high negative correlation value tells that the model tends to regurgitate answers seen during training.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">These distance features are also good at predicting failures – 74.19% of failures can be predicted by checking distance of test GT answer with GT answers of its k-NN training QI pairs for <span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model (75.41% for the <span id="S3.SS1.p5.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> model, 70.17% for the <span id="S3.SS1.p5.1.3" class="ltx_text ltx_font_smallcaps">MCB</span> model). Note that unlike the previous analysis, this analysis only explains failures but cannot be used to predict failures (since it uses GT labels). See Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Generalization to novel instances ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for qualitative examples.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1606.07356/assets/figures/acc_dist_qual_compressed.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples from test set where the <span id="S3.F1.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model makes mistakes and their corresponding nearest neighbor training instances. See <a href="#Sx4" title="Appendix III: Additional qualitative examples for “generalization to novel test instances” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix III</a> for more examples.</figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">From Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Generalization to novel instances ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (row1) we can see that the test QI pair is semantically quite different from its k-NN training QI pairs ({1st, 2nd, 3rd}-NN distances are {15.05, 15.13, 15.17}, which are higher than the corresponding distances averaged across all success cases: {8.74, 9.23, 9.50.}), explaining the mistake. Row2 shows an example where the model has seen the same question in the training set (test QI pair is semantically similar to training QI pairs) but, since it has not seen “green cone” for training instances (answers seen during training are different from what needs to be produced for the test QI pair), it is unable to answer the test QI pair correctly. This shows that current models lack compositionality: the ability to combine the concepts of “cone” and “green” (both of which have been seen in training set) to answer “green cone” for the test QI pair. This compositionality is desirable and central to intelligence.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Complete question understanding</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We feed partial questions of increasing lengths (from 0-100% of question from left to right). We then compute what percentage of responses do not change when more and more words are fed.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1606.07356/assets/figures/percent_ques_len_combined_cam.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="316" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>X-axis shows length of partial question (in %) fed as input. Y-axis shows percentage of questions for which responses of these partial questions are the same as full questions and VQA accuracy of partial questions.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 Complete question understanding ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the test accuracy and percentage of questions for which responses remain same (compared to entire question) as a function of partial question length. We can see that for 40% of the questions, the <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model seems to have converged on a predicted answer after ‘listening’ to just half the question. This shows that the model is listening to first few words of the question more than the words towards the end. Also, the model has 68% of the final accuracy (54%) when making predictions based on half the original question. When making predictions just based on the image, the accuracy of the model is 24%. The <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> model seems to have converged on a predicted answer after listening to just half the question more often (49% of the time), achieving 74% of the final accuracy (57%). The <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_smallcaps">MCB</span> model converges on a predicted answer after listening to just half the question 45% of the time, achieving 67% of the final accuracy (60%). See Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Complete question understanding ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for qualitative examples.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1606.07356/assets/figures/percent_ques_len_qual_compressed.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples where the <span id="S3.F3.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model does not change its answer after first few question words. On doing so, it is correct for some cases (the extreme left example) and incorrect for other cases (the remaining three examples). See <a href="#Sx6" title="Appendix V: Additional qualitative examples for “complete question understanding” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix V</a> for more examples.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We also analyze the change in responses of the model’s predictions (see Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Complete question understanding ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), when words of a particular part-of-the-speech (POS) tag are dropped from the question. The experimental results indicate that wh-words effect the model’s decisions the most (most of the responses get changed on dropping these words from the question), and that pronouns effect the model’s decisions the least.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/1606.07356/assets/figures/pos_cam.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Percentage of questions for which responses remain same (compared to entire question) as a function of POS tags dropped from the question.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Complete image understanding</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Does a VQA model really ‘look’ at the image? To analyze this, we compute the percentage of the time (say <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">X</annotation></semantics></math>) the response does not change across images (e.g.,, answer for all images is “2”) for a given question (e.g., “How many zebras?”) and plot histogram of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">X</annotation></semantics></math> across questions (see Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.3 Complete image understanding ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). We do this analysis for questions occurring for atleast 25 images in the VQA validation set, resulting in total 263 questions. The cumulative plot indicates that for 56% questions, the <span id="S3.SS3.p1.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model outputs the same answer for at least half the images. This is fairly high, suggesting that the model is picking the same answer no matter what the image is. Promisingly, the <span id="S3.SS3.p1.2.2" class="ltx_text ltx_font_smallcaps">ATT</span> and <span id="S3.SS3.p1.2.3" class="ltx_text ltx_font_smallcaps">MCB</span> models (that do not work with a holistic entire-image representation and purportedly pay attention to specific spatial regions in an image) produce the same response for at least half the images for fewer questions (42% for the <span id="S3.SS3.p1.2.4" class="ltx_text ltx_font_smallcaps">ATT</span> model, 40% for the <span id="S3.SS3.p1.2.5" class="ltx_text ltx_font_smallcaps">MCB</span> model).</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/1606.07356/assets/figures/image_understanding_cam.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Histogram of percentage of images for which model produces same answer for a given question and its comparison with test accuracy. The cumulative plot shows the % of questions for which model produces same answer for <em id="S3.F5.4.1" class="ltx_emph ltx_font_italic">atleast</em> <math id="S3.F5.2.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.F5.2.m1.1b"><mi id="S3.F5.2.m1.1.1" xref="S3.F5.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F5.2.m1.1c"><ci id="S3.F5.2.m1.1.1.cmml" xref="S3.F5.2.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.m1.1d">x</annotation></semantics></math> % of images.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">Interestingly, the average accuracy (see the VQA accuracy plots in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.3 Complete image understanding ‣ 3 Behavior Analyses ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) for questions for which the models produce same response for <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><gt id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">&gt;</annotation></semantics></math>50% and <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mo id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><lt id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">&lt;</annotation></semantics></math>55% of the images is 56% for the <span id="S3.SS3.p2.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model (60% for the <span id="S3.SS3.p2.2.2" class="ltx_text ltx_font_smallcaps">ATT</span> model, 73% for the <span id="S3.SS3.p2.2.3" class="ltx_text ltx_font_smallcaps">MCB</span> model) which is more than the respective average accuracy on the entire VQA validation set (54.13% for the <span id="S3.SS3.p2.2.4" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model, 57.02% for the <span id="S3.SS3.p2.2.5" class="ltx_text ltx_font_smallcaps">ATT</span> model, 60.36% for the <span id="S3.SS3.p2.2.6" class="ltx_text ltx_font_smallcaps">MCB</span> model). Thus, producing the same response across images seems to be statistically favorable. Fig. <a href="#S4.F6" title="Figure 6 ‣ 4 Conclusion ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows examples where the <span id="S3.SS3.p2.2.7" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model predicts the same response across images for a given question. The first row shows examples where the model makes errors on several images by predicting the same answer for all images. The second row shows examples where the model is always correct even if it predicts the same answer across images. This is so because questions such as <em id="S3.SS3.p2.2.8" class="ltx_emph ltx_font_italic">“What covers the ground?”</em> are asked for an image in the VQA dataset only when ground is covered with snow (because subjects were looking at the image while asking questions about it). Thus, this analysis exposes label biases in the dataset. Label biases (in particular, for “yes/no” questions) have also been reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Zhang et al., 2016</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We develop novel techniques to characterize the behavior of VQA models, as a first step towards understanding these models, meaningfully comparing the strengths and weaknesses of different models, developing insights into their failure modes, and identifying the most fruitful directions for progress. Our behavior analysis reveals that despite recent progress, today’s VQA models are “myopic” (tend to fail on sufficiently novel instances), often “jump to conclusions” (converge on a predicted answer after ‘listening’ to just half the question), and are “stubborn” (do not change their answers across images), with attention based models being less “stubborn” than non-attention based models.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1606.07356/assets/figures/image_understanding_qual_compressed.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Examples where the predicted answers do not change across images for a given question. See <a href="#Sx8" title="Appendix VII: Additional qualitative examples for “complete image understanding” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix VI</a> for more examples.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">As a final thought, we note that the somewhat pathological behaviors exposed in the paper are in some sense “correct” given the model architectures and the dataset being trained on. Ignoring optimization error, the maximum-likelihood training objective is clearly intended to capture statistics of the dataset. Our motive is simply to better understand current generation models via their behaviors, and use these observations to guide future choices – do we need novel model classes? or dataset with different biases? etc. Finally, it should be clear that our use of anthropomorphic adjectives such as “stubborn”, “myopic” etc. is purely for pedagogical reasons – to easily communicate our observations to our readers. No claims are being made about today’s VQA models being human-like.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Acknowledgements.</span>
We would like to thank the EMNLP reviewers for their valuable feedback and Yash Goyal for sharing his code. This work was supported in part by:
NSF CAREER awards,
Army Research Office YIP awards,
ICTAS Junior Faculty awards,
Google Faculty Research awards, awarded to both DB and DP,
ONR grant N00014-14-1-0679,
AWS in Education Research grant,
NVIDIA GPU donation, awarded to DB,
Paul G. Allen Family Foundation Allen Distinguished Investigator award,
ONR YIP
and
Alfred P. Sloan Fellowship, awarded to DP.
The views and conclusions contained herein are those of the authors and should not be interpreted as
necessarily representing the official policies or endorsements, either expressed or implied, of the U.S.
Government or any sponsor.
</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix Overview</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">In the appendix, we provide:</p>
<ol id="Sx1.I1" class="ltx_enumerate">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">- Behavioral analysis for question-only and image-only VQA models (<a href="#Sx2" title="Appendix I: Behavioral analysis for question-only and image-only VQA models ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix I</a>).</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">II</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">- Scatter plot of average distance of test instances from nearest neighbor training instances w.r.t. VQA accuracy (<a href="#Sx3" title="Appendix II: Scatter plot of average distance of test instances from nearest neighbor training instances w.r.t. VQA accuracy ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix II</a>).</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">III</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">- Additional qualitative examples for “generalization to novel test instances” (<a href="#Sx4" title="Appendix III: Additional qualitative examples for “generalization to novel test instances” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix III</a>).</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">IV</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">- The analyses on “complete question understanding” for different question types (<a href="#Sx5" title="Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix IV</a>).</p>
</div>
</li>
<li id="Sx1.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">V</span> 
<div id="Sx1.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i5.p1.1" class="ltx_p">- Additional qualitative examples for “complete question understanding” (<a href="#Sx6" title="Appendix V: Additional qualitative examples for “complete question understanding” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix V</a>).</p>
</div>
</li>
<li id="Sx1.I1.i6" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">VI</span> 
<div id="Sx1.I1.i6.p1" class="ltx_para">
<p id="Sx1.I1.i6.p1.1" class="ltx_p">- The analyses on “complete image understanding” for different question types (<a href="#Sx7" title="Appendix VI: Analyses on “complete image understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix VI</a>).</p>
</div>
</li>
<li id="Sx1.I1.i7" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">VII</span> 
<div id="Sx1.I1.i7.p1" class="ltx_para">
<p id="Sx1.I1.i7.p1.1" class="ltx_p">- Additional qualitative examples for “complete image understanding” (<a href="#Sx8" title="Appendix VII: Additional qualitative examples for “complete image understanding” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref">Appendix VII</a>).</p>
</div>
</li>
</ol>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix I: Behavioral analysis for question-only and image-only VQA models</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We evaluated the performance of both <span id="Sx2.p1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> and <span id="Sx2.p1.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> models by just feeding in the question (and mean image embedding) and by just feeding in the image (and mean question embedding). We computed the percentage of responses that change on feeding the question as well, compared to only feeding in the image and the percentage of responses that change on feeding the image as well, compared to only feeding in the question. We found that that the responses changed much more (about 40% more) on addition of the question than they did on addition of the image. So this suggests that the VQA models are heavily driven by question rather than the image.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix II: Scatter plot of average distance of test instances from nearest neighbor training instances w.r.t. VQA accuracy </h2>

<figure id="Sx3.F7" class="ltx_figure"><img src="/html/1606.07356/assets/figures/correlation_acc_distance_v3.png" id="Sx3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Test accuracy vs. average distance of the test points from k-NN training points for the <span id="Sx3.F7.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model.</figcaption>
</figure>
<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">Fig. <a href="#Sx3.F7" title="Figure 7 ‣ Appendix II: Scatter plot of average distance of test instances from nearest neighbor training instances w.r.t. VQA accuracy ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the variation of accuracy of test point w.r.t their average distance from k-NN training points for the <span id="Sx3.p1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model. Each point in the plot represents average statistics (accuracy and average distance) for a random subset of 25 test points. We can see that for the test points with low accuracy, the average distance is higher compared to test points with high accuracy. The correlation between accuracy and average distance is significant (-0.41 at <math id="Sx3.p1.1.m1.1" class="ltx_Math" alttext="k=50" display="inline"><semantics id="Sx3.p1.1.m1.1a"><mrow id="Sx3.p1.1.m1.1.1" xref="Sx3.p1.1.m1.1.1.cmml"><mi id="Sx3.p1.1.m1.1.1.2" xref="Sx3.p1.1.m1.1.1.2.cmml">k</mi><mo id="Sx3.p1.1.m1.1.1.1" xref="Sx3.p1.1.m1.1.1.1.cmml">=</mo><mn id="Sx3.p1.1.m1.1.1.3" xref="Sx3.p1.1.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p1.1.m1.1b"><apply id="Sx3.p1.1.m1.1.1.cmml" xref="Sx3.p1.1.m1.1.1"><eq id="Sx3.p1.1.m1.1.1.1.cmml" xref="Sx3.p1.1.m1.1.1.1"></eq><ci id="Sx3.p1.1.m1.1.1.2.cmml" xref="Sx3.p1.1.m1.1.1.2">𝑘</ci><cn type="integer" id="Sx3.p1.1.m1.1.1.3.cmml" xref="Sx3.p1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.1.m1.1c">k=50</annotation></semantics></math>.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><math id="footnote7.m1.1" class="ltx_Math" alttext="k=50" display="inline"><semantics id="footnote7.m1.1b"><mrow id="footnote7.m1.1.1" xref="footnote7.m1.1.1.cmml"><mi id="footnote7.m1.1.1.2" xref="footnote7.m1.1.1.2.cmml">k</mi><mo id="footnote7.m1.1.1.1" xref="footnote7.m1.1.1.1.cmml">=</mo><mn id="footnote7.m1.1.1.3" xref="footnote7.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote7.m1.1c"><apply id="footnote7.m1.1.1.cmml" xref="footnote7.m1.1.1"><eq id="footnote7.m1.1.1.1.cmml" xref="footnote7.m1.1.1.1"></eq><ci id="footnote7.m1.1.1.2.cmml" xref="footnote7.m1.1.1.2">𝑘</ci><cn type="integer" id="footnote7.m1.1.1.3.cmml" xref="footnote7.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote7.m1.1d">k=50</annotation></semantics></math> leads to highest correlation</span></span></span>)</p>
</div>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix III: Additional qualitative examples for “generalization to novel test instances”</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">Fig. <a href="#Sx4.F8" title="Figure 8 ‣ Appendix III: Additional qualitative examples for “generalization to novel test instances” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows test QI pairs for which the <span id="Sx4.p1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model produces the correct response and their nearest neighbor QI pairs from training set. It can be seen that the nearest neighbor QI pairs from the training set are similar to the test QI pair. In addition, the GT labels in the training set are similar to the test GT label.</p>
</div>
<div id="Sx4.p2" class="ltx_para">
<p id="Sx4.p2.1" class="ltx_p">Fig. <a href="#Sx4.F9" title="Figure 9 ‣ Appendix III: Additional qualitative examples for “generalization to novel test instances” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows test QI pairs for which the <span id="Sx4.p2.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model produces incorrect response and their nearest neighbor QI pairs from training set. Some of the mistakes are probably because the test QI pair does not have similar QI pairs in the training set (rows 2, 4 and 5) while other mistakes are probably because the GT labels in the training set are not similar to the GT test label (rows 1 and 3).</p>
</div>
<figure id="Sx4.F8" class="ltx_figure"><img src="/html/1606.07356/assets/figures/combined_v2_compressed_qual1.png" id="Sx4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="655" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Test QI pairs for which the <span id="Sx4.F8.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model produces the correct response and their nearest neighbor QI pairs from training set.</figcaption>
</figure>
<figure id="Sx4.F9" class="ltx_figure"><img src="/html/1606.07356/assets/figures/combined_v2_compressed_qual2.png" id="Sx4.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="787" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Test QI pairs for which the <span id="Sx4.F9.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model produces incorrect response and their nearest neighbor QI pairs from training set.</figcaption>
</figure>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix IV: Analyses on “complete question understanding” for different question types</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">We show the breakdown of our analyses from the main paper – (i) whether the model ‘listens’ to the entire question; and (ii) which POS tags matter the most – over the three major categories of questions – “yes/no”, “number” and “other” as categorized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Antol et al., 2015</a>]</cite>. “yes/no” are questions whose answers are either “yes” or “no”, “number” are questions whose answers are numbers (e.g., “Q: How many zebras are there?”, “A: 2”), “other” are rest of the questions.</p>
</div>
<figure id="Sx5.F10" class="ltx_figure"><img src="/html/1606.07356/assets/figures/ques_len_bin.png" id="Sx5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="313" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>X-axis shows length of partial “yes/no” question (in %) fed as input. Y-axis shows percentage of “yes/no” questions for which responses of these partial “yes/no” questions are the same as full “yes/no” questions and VQA accuracy of partial “yes/no” questions.</figcaption>
</figure>
<figure id="Sx5.F11" class="ltx_figure"><img src="/html/1606.07356/assets/figures/ques_len_num.png" id="Sx5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>X-axis shows length of partial “number” question (in %) fed as input. Y-axis shows percentage of “number” questions for which responses of these partial “number” questions are the same as full “number” questions and VQA accuracy of partial “number” questions.</figcaption>
</figure>
<figure id="Sx5.F12" class="ltx_figure"><img src="/html/1606.07356/assets/figures/ques_len_other.png" id="Sx5.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>X-axis shows length of partial “other” question (in %) fed as input. Y-axis shows percentage of “other” questions for which responses of these partial “other” questions are the same as full “other” questions and VQA accuracy of partial “other” questions.</figcaption>
</figure>
<div id="Sx5.p2" class="ltx_para">
<p id="Sx5.p2.1" class="ltx_p">For “yes/no” questions, the <span id="Sx5.p2.1.1" class="ltx_text ltx_font_smallcaps">ATT</span> model seems particularly ‘jumpy’ – converging on a predicted answer listening to only the first few words of the question (see Fig. <a href="#Sx5.F10" title="Figure 10 ‣ Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). Surprisingly, the accuracy is also as much as the final accuracy (after listening to entire question) when making predictions based on first few words of the question. In contrast, the <span id="Sx5.p2.1.2" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model converges on a predicted answer later, after listening to atleast 35% of the question, achieving as much as the final accuracy after convergence. For “number” and “other” questions, both <span id="Sx5.p2.1.3" class="ltx_text ltx_font_smallcaps">ATT</span> and <span id="Sx5.p2.1.4" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model show similar trends (see Fig. <a href="#Sx5.F11" title="Figure 11 ‣ Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> for “number” and Fig. <a href="#Sx5.F12" title="Figure 12 ‣ Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> for “other”).</p>
</div>
<figure id="Sx5.F13" class="ltx_figure"><img src="/html/1606.07356/assets/figures/pos_len_bin.png" id="Sx5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="284" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Percentage of “yes/no” questions for which responses remain same (compared to entire “yes/no’ question) as a function of POS tags dropped from the “yes/no’ question.</figcaption>
</figure>
<figure id="Sx5.F14" class="ltx_figure"><img src="/html/1606.07356/assets/figures/pos_len_num.png" id="Sx5.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Percentage of “number” questions for which responses remain same (compared to entire “number” question) as a function of POS tags dropped from the “number” question.</figcaption>
</figure>
<figure id="Sx5.F15" class="ltx_figure"><img src="/html/1606.07356/assets/figures/pos_len_other.png" id="Sx5.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Percentage of “other” questions for which responses remain same (compared to entire “other” question) as a function of POS tags dropped from the “other” question.</figcaption>
</figure>
<div id="Sx5.p3" class="ltx_para">
<p id="Sx5.p3.1" class="ltx_p">It is interesting to note that VQA models are most sensitive to adjectives for “yes/no” questions (compared to wh-words for all questions) (see Fig. <a href="#Sx5.F13" title="Figure 13 ‣ Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>). This is probably because often the “yes/no” questions are about attributes of objects (e.g., “Is the cup empty?”). For “number” questions, the <span id="Sx5.p3.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model is most sensitive to adjectives whereas the <span id="Sx5.p3.1.2" class="ltx_text ltx_font_smallcaps">ATT</span> model is most sensitive to wh-words (see Fig. <a href="#Sx5.F14" title="Figure 14 ‣ Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>). For “other” questions, both the models are most sensitive to “nouns” (see Fig. <a href="#Sx5.F15" title="Figure 15 ‣ Appendix IV: Analyses on “complete question understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>).</p>
</div>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix V: Additional qualitative examples for “complete question understanding”</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">Fig. <a href="#Sx6.F16" title="Figure 16 ‣ Appendix V: Additional qualitative examples for “complete question understanding” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows examples where the <span id="Sx6.p1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model converges on a predicted answer without listening to the entire question. On doing so, the model gets the answer correct for some QI pairs (first three rows) and incorrect for others (last two rows).</p>
</div>
<figure id="Sx6.F16" class="ltx_figure"><img src="/html/1606.07356/assets/figures/combined_v2_compressed_qual3.png" id="Sx6.F16.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="677" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Examples where the <span id="Sx6.F16.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model converges on a predicted answer without listening to the entire question.</figcaption>
</figure>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix VI: Analyses on “complete image understanding” for different question types</h2>

<figure id="Sx7.F17" class="ltx_figure"><img src="/html/1606.07356/assets/figures/img_und_bin.png" id="Sx7.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Histogram of percentage of images for which model produces same answer for a given “yes/no” question. The cumulative plot shows the % of “yes/no” questions for which model produces same answer for <em id="Sx7.F17.4.1" class="ltx_emph ltx_font_italic">atleast</em> <math id="Sx7.F17.2.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="Sx7.F17.2.m1.1b"><mi id="Sx7.F17.2.m1.1.1" xref="Sx7.F17.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Sx7.F17.2.m1.1c"><ci id="Sx7.F17.2.m1.1.1.cmml" xref="Sx7.F17.2.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx7.F17.2.m1.1d">x</annotation></semantics></math> % of images.</figcaption>
</figure>
<figure id="Sx7.F18" class="ltx_figure"><img src="/html/1606.07356/assets/figures/img_und_num.png" id="Sx7.F18.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Histogram of percentage of images for which model produces same answer for a given “number” question. The cumulative plot shows the % of “number” questions for which model produces same answer for <em id="Sx7.F18.4.1" class="ltx_emph ltx_font_italic">atleast</em> <math id="Sx7.F18.2.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="Sx7.F18.2.m1.1b"><mi id="Sx7.F18.2.m1.1.1" xref="Sx7.F18.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Sx7.F18.2.m1.1c"><ci id="Sx7.F18.2.m1.1.1.cmml" xref="Sx7.F18.2.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx7.F18.2.m1.1d">x</annotation></semantics></math> % of images.</figcaption>
</figure>
<figure id="Sx7.F19" class="ltx_figure"><img src="/html/1606.07356/assets/figures/img_und_other.png" id="Sx7.F19.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Histogram of percentage of images for which model produces same answer for a given “other” question. The cumulative plot shows the % of “other” questions for which model produces same answer for <em id="Sx7.F19.4.1" class="ltx_emph ltx_font_italic">atleast</em> <math id="Sx7.F19.2.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="Sx7.F19.2.m1.1b"><mi id="Sx7.F19.2.m1.1.1" xref="Sx7.F19.2.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Sx7.F19.2.m1.1c"><ci id="Sx7.F19.2.m1.1.1.cmml" xref="Sx7.F19.2.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx7.F19.2.m1.1d">x</annotation></semantics></math> % of images.</figcaption>
</figure>
<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">Fig. <a href="#Sx7.F17" title="Figure 17 ‣ Appendix VI: Analyses on “complete image understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, Fig. <a href="#Sx7.F18" title="Figure 18 ‣ Appendix VI: Analyses on “complete image understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> and Fig. <a href="#Sx7.F19" title="Figure 19 ‣ Appendix VI: Analyses on “complete image understanding” for different question types ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> show the breakdown of percentage of questions for which the model produces same answer across images for “yes/no”, “number” and “other” respectively. The <span id="Sx7.p1.1.1" class="ltx_text ltx_font_smallcaps">ATT</span> model seems to be more “stubborn” (does not change its answers across images) for “yes/no” questions compared to the <span id="Sx7.p1.1.2" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model, and less “stubborn” for “number” questions compared to the <span id="Sx7.p1.1.3" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model.</p>
</div>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix VII: Additional qualitative examples for “complete image understanding”</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p">Fig. <a href="#Sx8.F20" title="Figure 20 ‣ Appendix VII: Additional qualitative examples for “complete image understanding” ‣ Analyzing the Behavior of Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> shows examples where the <span id="Sx8.p1.1.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model produces the same answer for atleast half the images for a given question and the accuracy achieved by the model for such QI pairs.</p>
</div>
<figure id="Sx8.F20" class="ltx_figure"><img src="/html/1606.07356/assets/figures/combined_v2_compressed_qual4.png" id="Sx8.F20.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="453" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Examples where the <span id="Sx8.F20.2.1" class="ltx_text ltx_font_smallcaps">CNN+LSTM</span> model produces the same answer for atleast half the images for each of the questions shown above. “Q” denotes the question for which model produces same response for atleast half the images, “A” denotes the answer predicted by the model (which is same for atleast half the images), “Number of Images” denotes the number of images for which the question is repeated in the VQA validation set and “Average Accuracy” is the VQA accuracy for these QI pairs (with same question but different images).</figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Andreas et al., 2016a] </span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.

</span>
<span class="ltx_bibblock">2016a.

</span>
<span class="ltx_bibblock">Deep compositional question answering with neural module networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Andreas et al., 2016b] </span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.

</span>
<span class="ltx_bibblock">2016b.

</span>
<span class="ltx_bibblock">Learning to compose neural networks for question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">NAACL</span>.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Antol et al., 2015] </span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">ICCV</span>.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chen et al., 2015] </span>
<span class="ltx_bibblock">
Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">ABC-CNN: an attention based convolutional neural network for visual
question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1511.05960.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Fukui et al., 2016] </span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Geman et al., 2014] </span>
<span class="ltx_bibblock">
Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">A Visual Turing Test for Computer Vision Systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">PNAS</span>.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hoiem et al., 2012] </span>
<span class="ltx_bibblock">
Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Diagnosing error in object detectors.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ilievski et al., 2016] </span>
<span class="ltx_bibblock">
Ilija Ilievski, Shuicheng Yan, and Jiashi Feng.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">A focused dynamic attention model for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1604.01485.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Jiang et al., 2015] </span>
<span class="ltx_bibblock">
Aiwen Jiang, Fang Wang, Fatih Porikli, and Yi Li.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Compositional memory for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1511.05676.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kafle and Kanan, 2016] </span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Answer-type prediction for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Karpathy et al., 2016] </span>
<span class="ltx_bibblock">
Andrej Karpathy, Justin Johnson, and Fei-Fei Li.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Visualizing and understanding recurrent networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">ICLR Workshop</span>.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kim et al., 2016] </span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
Ha, and Byoung-Tak Zhang.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual QA.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lu et al., 2015] </span>
<span class="ltx_bibblock">
Jiasen Lu, Xiao Lin, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Deeper lstm and normalized cnn visual question answering model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/VT-vision-lab/VQA_LSTM_CNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VT-vision-lab/VQA_LSTM_CNN</a>.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lu et al., 2016] </span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Malinowski and Fritz, 2014] </span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">A Multi-World Approach to Question Answering about Real-World Scenes
based on Uncertain Input.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">NIPS</span>.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mikolov et al., 2013] </span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">ICLR</span>.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Noh and Han, 2016] </span>
<span class="ltx_bibblock">
Hyeonwoo Noh and Bohyung Han.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Training recurrent answering units with joint loss minimization for
vqa.

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1606.03647.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Saito et al., 2016] </span>
<span class="ltx_bibblock">
Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Dualnet: Domain-invariant network for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1606.06108.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Shih et al., 2016] </span>
<span class="ltx_bibblock">
Kevin J. Shih, Saurabh Singh, and Derek Hoiem.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al., 2015] </span>
<span class="ltx_bibblock">
Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Explicit knowledge-based reasoning for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1511.02570.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wu et al., 2016] </span>
<span class="ltx_bibblock">
Qi Wu, Peng Wang, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Xiong et al., 2016] </span>
<span class="ltx_bibblock">
Caiming Xiong, Stephen Merity, and Richard Socher.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Dynamic memory networks for visual and textual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx22.1.1" class="ltx_text ltx_font_italic">ICML</span>.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Xu and Saenko, 2016] </span>
<span class="ltx_bibblock">
Huijuan Xu and Kate Saenko.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">ECCV</span>.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al., 2016] </span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 2016] </span>
<span class="ltx_bibblock">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Yin and Yang: Balancing and answering binary visual questions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.1.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhou et al., 2015] </span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1512.02167.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1606.07355" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1606.07356" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1606.07356">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1606.07356" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1606.07357" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 20:25:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
