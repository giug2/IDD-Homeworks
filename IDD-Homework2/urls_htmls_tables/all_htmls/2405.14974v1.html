<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.14974] LOVA3: Learning to Visual Question Answering, Asking and Assessment</title><meta property="og:description" content="Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LOVA3: Learning to Visual Question Answering, Asking and Assessment">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LOVA3: Learning to Visual Question Answering, Asking and Assessment">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.14974">

<!--Generated on Wed Jun  5 17:32:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id1.1" class="ltx_text ltx_font_bold">LOVA<sup id="id1.1.1" class="ltx_sup"><span id="id1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span>: Learning to Visual Question Answering, Asking and Assessment</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henry Hengyuan Zhao<sup id="id11.8.id1" class="ltx_sup">1</sup>,
Pan Zhou<sup id="id12.9.id2" class="ltx_sup"><span id="id12.9.id2.1" class="ltx_text ltx_font_italic">2,3†</span></sup>, Difei Gao<sup id="id13.10.id3" class="ltx_sup">1</sup>
Mike Zheng Shou<sup id="id14.11.id4" class="ltx_sup"><span id="id14.11.id4.1" class="ltx_text ltx_font_italic">1†</span></sup>
<br class="ltx_break"><sup id="id15.12.id5" class="ltx_sup">1</sup>Show Lab, National University of Singapore, Singapore, 
<br class="ltx_break"><sup id="id16.13.id6" class="ltx_sup">2</sup>Singapore Management University, Singapore, 
<br class="ltx_break"><sup id="id17.14.id7" class="ltx_sup">3</sup>Sea AI Lab, Singapore 
<br class="ltx_break"><a target="_blank" href="https://github.com/showlab/LOVA3" title="" class="ltx_ref ltx_href">https://github.com/showlab/LOVA3</a>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.2" class="ltx_p">Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. However, current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. To this end, we introduce <span id="id9.1.1" class="ltx_text ltx_font_bold">LOVA<sup id="id9.1.1.1" class="ltx_sup"><span id="id9.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span>, an innovative framework named “Learning tO Visual Question Answering, Asking and Assessment,” designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks <span id="id10.2.3" class="ltx_text ltx_font_bold">GenQA</span> and <span id="id10.2.4" class="ltx_text ltx_font_bold">EvalQA</span>, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called <span id="id10.2.5" class="ltx_text ltx_font_bold">EvalQABench</span>, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 validation and testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions will improve their multimodal comprehension and lead to better performance. We validate our hypothesis by training an MLLM using the <span id="id10.2.2" class="ltx_text ltx_font_bold">LOVA<sup id="id10.2.2.1" class="ltx_sup"><span id="id10.2.2.1.1" class="ltx_text ltx_font_medium">3</span></sup></span> framework and testing it on 10 multimodal benchmarks. The results demonstrate consistent performance improvements, thereby confirming the efficacy of our approach.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup"><span id="footnotex1.1.1" class="ltx_text ltx_font_italic">†</span></sup>Corresponding author</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">To acquire knowledge, we humans often answer lots of questions and then improve ourselves by comparing our answers with the ground-truth answers. As a result, this learning mechanism empowers humans with the answering ability, which allows humans to handle well many real tasks, such as visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. However, as described in the following slogan,</p>
<blockquote id="S1.p1.2" class="ltx_quote">
<p id="S1.p1.2.1" class="ltx_p">“The art of proposing a question must be held in higher value than solving it.” - Georg Cantor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></p>
</blockquote>
<p id="S1.p1.3" class="ltx_p">asking a question is very valuable and even more important than answering a question. Indeed, humans also acquire knowledge from learning to ask questions since it encourages individuals to engage more deeply with information, thereby enhancing problem-solving skills <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. In addition to asking questions, humans also improve themselves through self-evaluation: humans try to identify the correctness of the answer and thus are involved in a deep understanding of our diverse world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. These three learning mechanisms not only play a vital role in the human intelligence learning process but also empower humans with the corresponding abilities, including answering, asking, and assessing abilities. Among them, the answering ability is necessary to handle QA-like tasks; the asking ability allows AI models to interact with humans or other AI models for necessary information; and the evaluation ability assesses the solution candidates given by humans or other models with many applications, e.g., the filtering synthetic data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2405.14974/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of three abilities reveals that LLaVA1.5 excels in providing answers but struggles in asking accurate questions and assessing question-answer pairs.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Although innate to humans, apart from answering ability, two other learning mechanisms of asking and assessment remain formidable challenges for contemporary multimodal large language models (MLLMs). Even though
some advanced MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> have even achieved remarkable proficiency in handling multimodal questions pertaining to mathematics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, science <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, and commonsense knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. However, the focus of most MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> predominantly revolves around visual question answering (VQA) tasks. As a result, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, current MLLMs, e.g., the representative LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, suffer from inferior performance on asking questions and self-assess question-answer pairs (QA), which underscores their efficacy as problem-solvers and prohibits profound multimodal understanding.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To address these challenges, we introduce two essential tasks: <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">GenQA</span> and <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">EvalQA</span>, aiming at bolstering the intelligence and robustness of MLLMs. GenQA focuses on enabling the model to generate diverse question-answer (QA) pairs for images, thus equipping the MLLM with the capability to ask questions. We believe that if an MLLM can successfully generate QA pairs for challenging tasks, it indicates a higher level of problem-solving ability. Specifically, we define the GenQA task to include not only generic VQA (e.g., VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>) but also Multi-Choice VQA (MC VQA), and Multi-Turn VQA (MT) to increase the variety of data formats. Additionally, we incorporate two challenging multimodal grounding tasks into the training process: Referring Expression Comprehension (REC) and Referring Expression Generation (REG). Learning to generate the data of these grounding tasks forces the MLLM to extract fine-grained visual cues from images, such as explicit object localization and compositional relationships. This, in turn, enhances the multimodal reasoning ability of MLLMs. During training, we gather the relevant datasets for these tasks and transform them into a generative format using our proposed instruction template. EvalQA, on the other hand, involves tasking the MLLM to predict the correctness of a given visual-question-answer triplet. Recognizing the absence of datasets specifically designed to assess VQA quality, we have developed a new benchmark called <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">EvalQABench</span> for evaluating VQA data. Rather than asking humans to label such a dataset, we propose a new automatic pipeline for data construction. This benchmark comprises training, validation, and test sets, with each VQA pair accompanied by a “Yes” or “No” label indicating correctness, along with a one-sentence explanation as the feedback. For instance, <span id="S1.p3.1.4" class="ltx_text ltx_font_italic">“Yes, the oranges are not in a bag”</span>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.2" class="ltx_p">By integrating the GenQA and EvalQA tasks into the vanilla multimodal learning, we develop an effective training framework called <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">LOVA<sup id="S1.p4.1.1.1" class="ltx_sup"><span id="S1.p4.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span>. In this study, we select the SOTA MLLM LLaVA-1.5 as the backbone model for evaluation. We conduct experiments on 10 widely used multimodal benchmarks such as GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, Vizwiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, MME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, MMBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, and MM-vet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, and observe consistent improvements across these benchmarks. To summarize, our proposed LOVA<sup id="S1.p4.2.2" class="ltx_sup">3</sup> is a new framework that endows the MLLM with the ability to ask and assess and finally achieve profound multimodal understanding capability. Overall, our contributions are three folds:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.2" class="ltx_p">To the best of our knowledge, <span id="S1.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">LOVA<sup id="S1.I1.ix1.p1.1.1.1" class="ltx_sup"><span id="S1.I1.ix1.p1.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span> is the first effort to imbue the asking and assessment abilities in training a robust and intelligent MLLM. LOVA<sup id="S1.I1.ix1.p1.2.2" class="ltx_sup">3</sup> open an avenue for imitating the human abilities towards holistic intelligence for MLLM.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">We build a new benchmark <span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">EvalQABench</span> for the VQA evaluation as the first effort to advance the development of future research.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.ix3.p1.1" class="ltx_p">The experimental results demonstrate that training with LOVA<sup id="S1.I1.ix3.p1.1.1" class="ltx_sup">3</sup> consistently improves performance across several multimodal benchmarks, including MME, VizWiz, and MMBench.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multimodal Large Language Models</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Large Language Models (LLMs)  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> such as GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> demonstrate their exceptional capacity to handle a wide range of complex tasks to play an important role in assisting humans in daily life.
Equipped with these LLMs, a surge of multimodal modes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> are proposed to integrate the visual information with the pre-trained LLM decoder for diverse multimodal reasoning tasks such as image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> and visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> is a pioneering approach that proposes to project the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> visual features to the language embedding space and showcase promising performance on various multimodal benchmarks. Unlike the architecture of LLaVA, InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> employs an instruction-aware feature extractor and obtains advanced performance on various tasks. Besides focusing on traditional vision-language tasks, Shikra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, Kosmos-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, PVIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Ferret <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> pay attention to the image-region based multimodal tasks (i.e., Referring Expression Comprehension). By adopting a large-scale image-text corpus for instruction tuning, Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, CogVLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, AnyMAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and Chameleon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> achieve exceptional performance on various multimodal tasks. However, these MLLMs primarily concentrate on training the model to answer questions as effectively as possible, neglecting the significance of enabling the model to act as a questioner and a competent evaluator within the training paradigm.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual Question Answering and Generation</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Nine years ago, visual question answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> was defined and became an essential task for evaluating multimodal systems. A surge of VQA-related benchmarks  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> are emerged to advance the development of this research area, including generic VQA benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, text-based VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, knowledge-augmented VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> aimed at assisting blind people. Simutaneously, various studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> are proposed with diverse model architecture such as LSTMs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, attention network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or convolution network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Besides the VQA task, the Visual Question Generation (VQG) task was first formulated in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, which contributes a VQG dataset with each image annotated with multi-questions and benchmarking on generative models and retrieval models.  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> first employs an RNN-based encoder-decoder framework alongside model-generated captions to generate questions. After that a list of works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> are proposed for promoting this research area. Two interesting studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> pose that treating VQG as a complementary task can enhance the robustness of visual question answering. This finding reaffirms our motivation that training a model to generate diverse questions contributes to a deeper understanding of visual information, thereby improving its problem-solving capabilities.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">As for assessing the quality of VQA triplets, to the best of our knowledge, there are no efforts to do that. Therefore, we propose a new task EvalQA for the model training tailored with a new benchmark EvalQABench to advance this research area.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multimodal Benchmarks</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Traditional multimodal benchmarks focus on answering ability, such as visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, as well as other benchmarks for specialized scenarios such as scene text understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, commonsense reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>, outside knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. The recent development of MLLM posts a strong need for modernized multimodal benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> such as MME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, MMBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, SEED-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> which involve comprehensively evaluating current MLLMs on various multimodal abilities.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Unlike existing multimodal benchmarks focusing primarily on evaluating the model’s ability to answer, we introduce EvalQABench, a benchmark designed to evaluate the quality of VQA pairs, each with a binary “Yes/No” annotation. Furthermore, recognizing the lack of emphasis on providing feedback for incorrect answers in current benchmarks, we develop an LLM-based pipeline. This pipeline can automatically generate feedback, paving the way for enhanced automated data processing in the future.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce LOVA<sup id="S3.p1.1.1" class="ltx_sup">3</sup>, a new framework designed to imitate two essential abilities - asking and assessment - within multimodal learning. We delve into the specifics of addressing this challenge through GenQA data collection, EvalQA data creation, model architecture, and training.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection for GenQA</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">If one MLLM is able to successfully generate high-quality question-answer pairs based on visual input, it indicates a stronger problem-solving ability and deep visual understanding. To enable the MLLM to ask questions, it is natural for us to gather existing annotated datasets as the training corpus and then train the model to predict both questions and answers. We carefully define five main multimodal data types as listed in Tab. <a href="#S3.T1" title="Table 1 ‣ 3.1 Data Collection for GenQA ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For each data type, we gather widely used human-annotated datasets or high-quality instruction tuning datasets generated by GPT-4. We select Generic VQA tasks to generate fundamental questions, e.g., object count and object action. We incorporate Multi-choice VQA (MC VQA) and Multi-turn VQA (MT) to increase the diversity of data formats. Additionally, we include two multimodal grounding tasks: Referring Expression Comprehension (REC) and Referring Expression Generation (REG). Generating REC and REG data requires a deeper understanding of image content, enabling the model to fully comprehend visual cues. Both tasks increase the difficulty of GenQA, which helps MLLM acquire a higher level of multimodal understanding. In total, we gather 842K data for training questioning ability.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data taxonomy of GenQA, detailing the data type, name, size, and instruction prompts of each dataset.</figcaption>
<div id="S3.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:457.3pt;height:195pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-134.3pt,57.1pt) scale(0.63,0.63) ;">
<table id="S3.T1.5.5" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.5.5.6" class="ltx_tr">
<td id="S3.T1.5.5.6.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 5.7pt;">Data Type</td>
<td id="S3.T1.5.5.6.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 5.7pt;">Dataset</td>
<td id="S3.T1.5.5.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 5.7pt;">Size</td>
<td id="S3.T1.5.5.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.6.4.1.1" class="ltx_p" style="width:398.3pt;">Instruction Prompts</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.7" class="ltx_tr">
<td id="S3.T1.5.5.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;" rowspan="5"><span id="S3.T1.5.5.7.1.1" class="ltx_text">Generic VQA</span></td>
<td id="S3.T1.5.5.7.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.5.5.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">100K</td>
<td id="S3.T1.5.5.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.7.4.1.1" class="ltx_p" style="width:398.3pt;"><em id="S3.T1.5.5.7.4.1.1.1" class="ltx_emph ltx_font_italic">Note: randomly choose from 58 instruction prompts</em></span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.8" class="ltx_tr">
<td id="S3.T1.5.5.8.1" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;">GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S3.T1.5.5.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">100K</td>
<td id="S3.T1.5.5.8.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.8.3.1.1" class="ltx_p" style="width:398.3pt;">Example: Can you provide a clear question and its answer based on the image?</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.9" class="ltx_tr">
<td id="S3.T1.5.5.9.1" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;">OCR-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S3.T1.5.5.9.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">80K</td>
<td id="S3.T1.5.5.9.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
</tr>
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;">Counting20K<sup id="S3.T1.1.1.1.1.1" class="ltx_sup"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">20K</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.1" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;">LLaVA-250K<sup id="S3.T1.2.2.2.1.1" class="ltx_sup"><span id="S3.T1.2.2.2.1.1.1" class="ltx_text ltx_font_italic">†</span></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">250K</td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.3.3.3.2.1" class="ltx_text">Multi-choice VQA</span></td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.3.3.3.3.1" class="ltx_text">A-OKVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite></span></td>
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.3.3.3.4.1" class="ltx_text">17K</span></td>
<td id="S3.T1.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.3.1.1.1" class="ltx_p" style="width:398.3pt;">Can you provide a clear question and its answer based on the image?<math id="S3.T1.3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.T1.3.3.3.1.1.1.m1.1a"><mo id="S3.T1.3.3.3.1.1.1.m1.1.1" xref="S3.T1.3.3.3.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.1.1.m1.1b"><ci id="S3.T1.3.3.3.1.1.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.1.1.m1.1c">\backslash</annotation></semantics></math>nThis is a Multi-choice VQA task.</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.10" class="ltx_tr">
<td id="S3.T1.5.5.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;" rowspan="2"><span id="S3.T1.5.5.10.1.1" class="ltx_text">Multi-turn VQA</span></td>
<td id="S3.T1.5.5.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.5.5.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">83K</td>
<td id="S3.T1.5.5.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.10.4.1.1" class="ltx_p" style="width:398.3pt;">Design a conversation between you and a person asking about this photo.</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.11" class="ltx_tr">
<td id="S3.T1.5.5.11.1" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.11.1.1" class="ltx_text">GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></span></td>
<td id="S3.T1.5.5.11.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.11.2.1" class="ltx_text">72K</span></td>
<td id="S3.T1.5.5.11.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.11.3.1.1" class="ltx_p" style="width:398.3pt;">The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers.</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.12" class="ltx_tr">
<td id="S3.T1.5.5.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;" rowspan="2"><span id="S3.T1.5.5.12.1.1" class="ltx_text">REC</span></td>
<td id="S3.T1.5.5.12.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.12.2.1" class="ltx_text">VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></span></td>
<td id="S3.T1.5.5.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.12.3.1" class="ltx_text">30K</span></td>
<td id="S3.T1.5.5.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.12.4.1.1" class="ltx_p" style="width:398.3pt;"><em id="S3.T1.5.5.12.4.1.1.1" class="ltx_emph ltx_font_italic">Note: randomly choose from 58 instruction prompts with a specific task description prompt.</em></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.2" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;"><span id="S3.T1.4.4.4.2.1" class="ltx_text">RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></span></td>
<td id="S3.T1.4.4.4.3" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;"><span id="S3.T1.4.4.4.3.1" class="ltx_text">30K</span></td>
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.1.1.1" class="ltx_p" style="width:398.3pt;">Can you review the image and articulate a concise question and its answer?<math id="S3.T1.4.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.T1.4.4.4.1.1.1.m1.1a"><mo id="S3.T1.4.4.4.1.1.1.m1.1.1" xref="S3.T1.4.4.4.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.1.1.m1.1b"><ci id="S3.T1.4.4.4.1.1.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.1.1.m1.1c">\backslash</annotation></semantics></math>nThis is a Referring Expression Comprehension (REC) task. The question will express a specific region of the image. Please provide the coordinates in the answer.</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.13" class="ltx_tr">
<td id="S3.T1.5.5.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;" rowspan="2"><span id="S3.T1.5.5.13.1.1" class="ltx_text">REG</span></td>
<td id="S3.T1.5.5.13.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.13.2.1" class="ltx_text">VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></span></td>
<td id="S3.T1.5.5.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.13.3.1" class="ltx_text">30K</span></td>
<td id="S3.T1.5.5.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.13.4.1.1" class="ltx_p" style="width:398.3pt;"><em id="S3.T1.5.5.13.4.1.1.1" class="ltx_emph ltx_font_italic">Note: randomly choose from 58 instruction prompts with a specific task description prompt.</em></span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.5" class="ltx_tr">
<td id="S3.T1.5.5.5.2" class="ltx_td ltx_align_left" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.5.2.1" class="ltx_text">RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></span></td>
<td id="S3.T1.5.5.5.3" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;"><span id="S3.T1.5.5.5.3.1" class="ltx_text">30K</span></td>
<td id="S3.T1.5.5.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="S3.T1.5.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.5.1.1.1" class="ltx_p" style="width:398.3pt;">Can you review the image and articulate a concise question and its answer?<math id="S3.T1.5.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.T1.5.5.5.1.1.1.m1.1a"><mo id="S3.T1.5.5.5.1.1.1.m1.1.1" xref="S3.T1.5.5.5.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.1.1.m1.1b"><ci id="S3.T1.5.5.5.1.1.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.1.1.m1.1c">\backslash</annotation></semantics></math>nThis is a Referring Expression Generation (REG) task. The purpose of REG is to generate a unique description for a specified location.</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5.14" class="ltx_tr">
<td id="S3.T1.5.5.14.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding:0.5pt 5.7pt;">Total</td>
<td id="S3.T1.5.5.14.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding:0.5pt 5.7pt;">-</td>
<td id="S3.T1.5.5.14.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">842K</td>
<td id="S3.T1.5.5.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding:0.5pt 5.7pt;"></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Selected examples from <span id="S3.T2.11.1" class="ltx_text ltx_font_bold">EvalQABench</span> training set, including the ground truth answer, negative answer, and feedback.</figcaption>
<table id="S3.T2.9" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.9.10" class="ltx_tr">
<td id="S3.T2.9.10.1" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.10.1.1.1" class="ltx_p"><span id="S3.T2.9.10.1.1.1.1" class="ltx_text ltx_font_bold">Question Types</span></span>
</span>
</td>
<td id="S3.T2.9.10.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.10.2.1.1" class="ltx_p"><span id="S3.T2.9.10.2.1.1.1" class="ltx_text ltx_font_bold">Object</span></span>
</span>
</td>
<td id="S3.T2.9.10.3" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.10.3.1.1" class="ltx_p"><span id="S3.T2.9.10.3.1.1.1" class="ltx_text ltx_font_bold">Yes/No</span></span>
</span>
</td>
<td id="S3.T2.9.10.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.10.4.1.1" class="ltx_p"><span id="S3.T2.9.10.4.1.1.1" class="ltx_text ltx_font_bold">Counting</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.4.1.1" class="ltx_p"><span id="S3.T2.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Image</span></span>
</span>
</td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.1.1.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/3.jpg" id="S3.T2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="[Uncaptioned image]">
</span>
</td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.2.2.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/23.jpg" id="S3.T2.2.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="418" alt="[Uncaptioned image]">
</span>
</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.3.3.3.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/35.jpg" id="S3.T2.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="347" alt="[Uncaptioned image]">
</span>
</td>
</tr>
<tr id="S3.T2.9.11" class="ltx_tr">
<td id="S3.T2.9.11.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.11.1.1.1" class="ltx_p"><span id="S3.T2.9.11.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span>
</td>
<td id="S3.T2.9.11.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.11.2.1.1" class="ltx_p">What kind of flowers are on the picture to the left?</span>
</span>
</td>
<td id="S3.T2.9.11.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.11.3.1.1" class="ltx_p">Is the sun shining?</span>
</span>
</td>
<td id="S3.T2.9.11.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.11.4.1.1" class="ltx_p">How many vases are there?</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.12" class="ltx_tr">
<td id="S3.T2.9.12.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.12.1.1.1" class="ltx_p"><span id="S3.T2.9.12.1.1.1.1" class="ltx_text ltx_font_bold">Ground-truth Answer</span></span>
</span>
</td>
<td id="S3.T2.9.12.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.12.2.1.1" class="ltx_p">roses</span>
</span>
</td>
<td id="S3.T2.9.12.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.12.3.1.1" class="ltx_p">no</span>
</span>
</td>
<td id="S3.T2.9.12.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.12.4.1.1" class="ltx_p">6</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.13" class="ltx_tr">
<td id="S3.T2.9.13.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.13.1.1.1" class="ltx_p"><span id="S3.T2.9.13.1.1.1.1" class="ltx_text ltx_font_bold">Negative Answer</span></span>
</span>
</td>
<td id="S3.T2.9.13.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.13.2.1.1" class="ltx_p">pansy</span>
</span>
</td>
<td id="S3.T2.9.13.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.13.3.1.1" class="ltx_p">yes</span>
</span>
</td>
<td id="S3.T2.9.13.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.13.4.1.1" class="ltx_p">5</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.14" class="ltx_tr">
<td id="S3.T2.9.14.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.14.1.1.1" class="ltx_p"><span id="S3.T2.9.14.1.1.1.1" class="ltx_text ltx_font_bold">Feedback</span></span>
</span>
</td>
<td id="S3.T2.9.14.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.14.2.1.1" class="ltx_p">No, the left of the picture shows roses.</span>
</span>
</td>
<td id="S3.T2.9.14.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.14.3.1.1" class="ltx_p">No, the sun is not shining.</span>
</span>
</td>
<td id="S3.T2.9.14.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.14.4.1.1" class="ltx_p">No, there are 6 vases in the picture.</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.15" class="ltx_tr">
<td id="S3.T2.9.15.1" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.15.1.1.1" class="ltx_p"><span id="S3.T2.9.15.1.1.1.1" class="ltx_text ltx_font_bold">Question Types</span></span>
</span>
</td>
<td id="S3.T2.9.15.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.15.2.1.1" class="ltx_p"><span id="S3.T2.9.15.2.1.1.1" class="ltx_text ltx_font_bold">Color</span></span>
</span>
</td>
<td id="S3.T2.9.15.3" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.15.3.1.1" class="ltx_p"><span id="S3.T2.9.15.3.1.1.1" class="ltx_text ltx_font_bold">Attribute</span></span>
</span>
</td>
<td id="S3.T2.9.15.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.15.4.1.1" class="ltx_p"><span id="S3.T2.9.15.4.1.1.1" class="ltx_text ltx_font_bold">Number</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.6.6" class="ltx_tr">
<td id="S3.T2.6.6.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.6.6.4.1.1" class="ltx_p"><span id="S3.T2.6.6.4.1.1.1" class="ltx_text ltx_font_bold">Image</span></span>
</span>
</td>
<td id="S3.T2.4.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.4.4.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/73.jpg" id="S3.T2.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="398" alt="[Uncaptioned image]">
</span>
</td>
<td id="S3.T2.5.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.5.5.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/317.jpg" id="S3.T2.5.5.2.1.g1" class="ltx_graphics ltx_img_portrait" width="359" height="538" alt="[Uncaptioned image]">
</span>
</td>
<td id="S3.T2.6.6.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.6.6.3.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/48.png" id="S3.T2.6.6.3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="[Uncaptioned image]">
</span>
</td>
</tr>
<tr id="S3.T2.9.16" class="ltx_tr">
<td id="S3.T2.9.16.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.16.1.1.1" class="ltx_p"><span id="S3.T2.9.16.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span>
</td>
<td id="S3.T2.9.16.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.16.2.1.1" class="ltx_p">What color is the truck?</span>
</span>
</td>
<td id="S3.T2.9.16.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.16.3.1.1" class="ltx_p">What type of tree is on the right?</span>
</span>
</td>
<td id="S3.T2.9.16.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.16.4.1.1" class="ltx_p">What number is written on the sheep?</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.17" class="ltx_tr">
<td id="S3.T2.9.17.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.17.1.1.1" class="ltx_p"><span id="S3.T2.9.17.1.1.1.1" class="ltx_text ltx_font_bold">Ground-truth Answer</span></span>
</span>
</td>
<td id="S3.T2.9.17.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.17.2.1.1" class="ltx_p">silver</span>
</span>
</td>
<td id="S3.T2.9.17.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.17.3.1.1" class="ltx_p">cherry</span>
</span>
</td>
<td id="S3.T2.9.17.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.17.4.1.1" class="ltx_p">3</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.18" class="ltx_tr">
<td id="S3.T2.9.18.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.18.1.1.1" class="ltx_p"><span id="S3.T2.9.18.1.1.1.1" class="ltx_text ltx_font_bold">Negative Answer</span></span>
</span>
</td>
<td id="S3.T2.9.18.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.18.2.1.1" class="ltx_p">white</span>
</span>
</td>
<td id="S3.T2.9.18.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.18.3.1.1" class="ltx_p">palm</span>
</span>
</td>
<td id="S3.T2.9.18.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.18.4.1.1" class="ltx_p">5</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.19" class="ltx_tr">
<td id="S3.T2.9.19.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.19.1.1.1" class="ltx_p"><span id="S3.T2.9.19.1.1.1.1" class="ltx_text ltx_font_bold">Feedback</span></span>
</span>
</td>
<td id="S3.T2.9.19.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.19.2.1.1" class="ltx_p">No, the truck is silver.</span>
</span>
</td>
<td id="S3.T2.9.19.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.19.3.1.1" class="ltx_p">No, the tree on the right is a cherry tree.</span>
</span>
</td>
<td id="S3.T2.9.19.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.19.4.1.1" class="ltx_p">The number written on the sheep is 3.</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.20" class="ltx_tr">
<td id="S3.T2.9.20.1" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.20.1.1.1" class="ltx_p"><span id="S3.T2.9.20.1.1.1.1" class="ltx_text ltx_font_bold">Question Types</span></span>
</span>
</td>
<td id="S3.T2.9.20.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.20.2.1.1" class="ltx_p"><span id="S3.T2.9.20.2.1.1.1" class="ltx_text ltx_font_bold">Relation</span></span>
</span>
</td>
<td id="S3.T2.9.20.3" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.20.3.1.1" class="ltx_p"><span id="S3.T2.9.20.3.1.1.1" class="ltx_text ltx_font_bold">Action</span></span>
</span>
</td>
<td id="S3.T2.9.20.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.20.4.1.1" class="ltx_p"><span id="S3.T2.9.20.4.1.1.1" class="ltx_text ltx_font_bold">Other</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.9.9" class="ltx_tr">
<td id="S3.T2.9.9.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.9.4.1.1" class="ltx_p"><span id="S3.T2.9.9.4.1.1.1" class="ltx_text ltx_font_bold">Image</span></span>
</span>
</td>
<td id="S3.T2.7.7.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.7.7.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/837.jpg" id="S3.T2.7.7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="[Uncaptioned image]">
</span>
</td>
<td id="S3.T2.8.8.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.8.8.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/291.jpg" id="S3.T2.8.8.2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="366" alt="[Uncaptioned image]">
</span>
</td>
<td id="S3.T2.9.9.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.9.3.1" class="ltx_inline-block ltx_align_top"><img src="/html/2405.14974/assets/figures/EvalQABench_samples/457.jpg" id="S3.T2.9.9.3.1.g1" class="ltx_graphics ltx_img_portrait" width="359" height="539" alt="[Uncaptioned image]">
</span>
</td>
</tr>
<tr id="S3.T2.9.21" class="ltx_tr">
<td id="S3.T2.9.21.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.21.1.1.1" class="ltx_p"><span id="S3.T2.9.21.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span>
</td>
<td id="S3.T2.9.21.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.21.2.1.1" class="ltx_p">What does the woman have on her back?</span>
</span>
</td>
<td id="S3.T2.9.21.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.21.3.1.1" class="ltx_p">What are the people doing?</span>
</span>
</td>
<td id="S3.T2.9.21.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.21.4.1.1" class="ltx_p">What does the second sign say?</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.22" class="ltx_tr">
<td id="S3.T2.9.22.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.22.1.1.1" class="ltx_p"><span id="S3.T2.9.22.1.1.1.1" class="ltx_text ltx_font_bold">Ground-truth Answer</span></span>
</span>
</td>
<td id="S3.T2.9.22.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.22.2.1.1" class="ltx_p">backpack</span>
</span>
</td>
<td id="S3.T2.9.22.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.22.3.1.1" class="ltx_p">motorcycling</span>
</span>
</td>
<td id="S3.T2.9.22.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.22.4.1.1" class="ltx_p">all-war</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.23" class="ltx_tr">
<td id="S3.T2.9.23.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.23.1.1.1" class="ltx_p"><span id="S3.T2.9.23.1.1.1.1" class="ltx_text ltx_font_bold">Negative Answer</span></span>
</span>
</td>
<td id="S3.T2.9.23.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.23.2.1.1" class="ltx_p">jacket</span>
</span>
</td>
<td id="S3.T2.9.23.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.23.3.1.1" class="ltx_p">riding bikes</span>
</span>
</td>
<td id="S3.T2.9.23.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.23.4.1.1" class="ltx_p">stop</span>
</span>
</td>
</tr>
<tr id="S3.T2.9.24" class="ltx_tr">
<td id="S3.T2.9.24.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.24.1.1.1" class="ltx_p"><span id="S3.T2.9.24.1.1.1.1" class="ltx_text ltx_font_bold">Feedback</span></span>
</span>
</td>
<td id="S3.T2.9.24.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.24.2.1.1" class="ltx_p">No, the woman has a backpack on her back.</span>
</span>
</td>
<td id="S3.T2.9.24.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.24.3.1.1" class="ltx_p">No, the people in the picture are motorcycling.</span>
</span>
</td>
<td id="S3.T2.9.24.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" style="padding:-1pt 5.0pt;">
<span id="S3.T2.9.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.9.24.4.1.1" class="ltx_p">No, the second sign says “all-war”</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Creation for EvalQA</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Completing the VQA assessment often requires fine-grained and deep visual understanding. As emphasized in Sec. <a href="#S1" title="1 Introduction ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the ability to assess is often overlooked yet crucial in MLLM training. To address this gap, we introduce a new benchmark, <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">EvalQABench</span>, to address the problem of assessing visual question-answering data. Moreover, instead of merely labeling each VQA pair with “Yes/No”, we advocate for integrating <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">feedback</span> into each instance, an important aspect rarely seen in prior multimodal benchmarks. We consider training the model not only to assess the correctness of the answer but also to provide reasonable feedback that would increase the capability for multimodal understanding. EvalQABench comprises three datasets: training, validation, and test sets. As illustrated in Tab. <a href="#S3.T2" title="Table 2 ‣ 3.1 Data Collection for GenQA ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present examples of the training set from EvalQABench across various question types.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">MLLM-based Negative Answer Generation.</span> The main challenge of EvalQABench lies in constructing negative answers. When dealing with large-scale ground-truth VQA pairs, how can we automatically produce the negative answer? One viable solution is to leverage a multimodal model for this purpose. Recognizing that Fuyu-8B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is an open-source free MLLM that stands out with the exceptional ability to process high-resolution images and perform robust well on many complex tasks, rivaling the capabilities of GPT-4V. We utilize it to generate negative answers with the following prompt:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<svg id="S3.SS2.p3.pic1" class="ltx_picture" height="58" overflow="visible" version="1.1" width="600"><g transform="translate(0,58) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#4D4D4D" fill-opacity="1.0"><path d="M 0 5.91 L 0 52.09 C 0 55.36 2.64 58 5.91 58 L 594.09 58 C 597.36 58 600 55.36 600 52.09 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 52.09 C 1.97 54.27 3.73 56.03 5.91 56.03 L 594.09 56.03 C 596.27 56.03 598.03 54.27 598.03 52.09 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_p"><span id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_typewriter" style="background-color:#EBEBEB;">&lt;Img&gt;</span></span>
<span id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">This is the question: <span id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="background-color:#EBEBEB;">&lt;Q&gt;</span>. Please give me the wrong answer to this question. The answer should be a single word or phrase.<math id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n</span>
</span></foreignObject></g></g></svg>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">Here, <span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_typewriter" style="background-color:#EBEBEB;">&lt;Img&gt;</span> and <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_typewriter" style="background-color:#EBEBEB;">&lt;Q&gt;</span> are two placeholders for the image and question from ground truth VQA pair. The output of Fuyu-8B provides a negative answer, such as <span id="S3.SS2.p4.1.3" class="ltx_text ltx_font_italic">“pansy”</span>, as illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 Data Creation for EvalQA ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Manual Filtering and Error Correction.</span> Acknowledging that the Fuyu-8B model is not flawless and recognizing that no multimodal model, including GPT-4V, is perfect, we have implemented both manual filtering and error corrections, as illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.4 Training ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for the post-data processing. Through empirical analysis, we identified 4 primary types of errors. For instance, an answer generated by Fuyu-8B may be present in the question but lacks semantic relevance or is identical to the correct answer. Additionally, some incorrect answers may result from misunderstanding the question’s category, as exemplified by the example in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.4 Training ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Beyond filtering, we propose error corrections for two types of questions: “Yes/No” and “Counting”. For “Yes/No” questions, we directly substitute an incorrect answer with “Yes”. For “Counting” questions, we first verify if the English numeral matches the correct answer; if not, we replace it with a random number. After applying the above filtering and correction processes, we found that most of the incorrect samples had been removed.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2405.14974/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the proposed pipeline for generating negative answers and feedback.</figcaption>
</figure>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">LLM-based Feedback Generation.</span> With the candidate’s negative answer, we then focus on generating error feedback. We consider the feedback describing the reason for incorrectness will help the MLLM obtain a deeper understanding. We thus utilize the SOTA LLM Llama 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> to generate the feedback by reasoning the ground truth question-answer pairs with the following prompt:</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<svg id="S3.SS2.p7.pic1" class="ltx_picture" height="41.4" overflow="visible" version="1.1" width="600"><g transform="translate(0,41.4) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#4D4D4D" fill-opacity="1.0"><path d="M 0 5.91 L 0 35.49 C 0 38.75 2.64 41.4 5.91 41.4 L 594.09 41.4 C 597.36 41.4 600 38.75 600 35.49 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 35.49 C 1.97 37.66 3.73 39.43 5.91 39.43 L 594.09 39.43 C 596.27 39.43 598.03 37.66 598.03 35.49 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">Please rephrase the question and answer: <span id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="background-color:#EBEBEB;">&lt;Q&gt;</span><math id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n<span id="S3.SS2.p7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_text ltx_font_typewriter" style="background-color:#EBEBEB;">&lt;A&gt;</span> into one short description.</span>
</span></foreignObject></g></g></svg>
</div>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.p8.1" class="ltx_p">After processing by Llama 2, we can get feedback like <span id="S3.SS2.p8.1.1" class="ltx_text ltx_font_italic">“No, the left of the picture shows roses.”</span>. Moreover, we use similar manual filtering strategies that are used in the negative answer generation step to remove the noisy samples with wrong formats or empty output.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p">In summary, we start by randomly selecting 100,000 samples from 443,758 annotated VQA pairs in the VQAv2 training set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to generate negative answers. After manual filtering, this number is reduced to 61,094 samples. We then generate feedback for each sample and further filter out those with incorrect formats, resulting in a final set of 41,592 samples. For the training set of EvalQABench, we create a one-positive-one-negative format by randomly selecting 32,000 negative samples from the 41,592 filtered samples, yielding a total of 64,000 training data points. For the validation and test subsets, we follow a similar sampling procedure. We randomly select 100,000 samples from the VQAv2 validation set, resulting in 41,907 negative samples. From these, we randomly select 2,500 negative samples each for the validation set and the test set.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Architecture</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.2" class="ltx_p">In this subsection, we introduce the model architecture of LOVA<sup id="S3.SS3.p1.2.1" class="ltx_sup">3</sup>. This model is built upon the prevalent MLLM LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> with three key components: Vision Encoder, MLP Adapter, and Large Language Model. For the vision encoder, we follow LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and implement it with a pre-trained CLIP-Large vision encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> with resolution <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="336\times 336" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">336</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">336</cn><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">336</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">336\times 336</annotation></semantics></math>. For the large language model, we adopt the widely used instruction fine-tuned model, Vicuna-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Following  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, the MLP adapter is a simple two-layer MLP since such a simple design is better for reserving the visual information while achieving running efficiency. In this study, we leverage LLaVA-1.5 to build upon because of its exceptional performance and highly reproducible training and validating codes. Other outstanding MLLMs, such as CogVLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> and Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, are pre-trained on billions-scale datasets or in-house datasets. This scale of data makes the training process difficult to replicate and poses challenges in incorporating our proposed training tasks, GenQA and EvalQA.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.5" class="ltx_p">For brevity, we denote the LOVA<sup id="S3.SS4.p1.5.1" class="ltx_sup">3</sup> model as <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="F_{M}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">F</mi><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝐹</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">F_{M}</annotation></semantics></math>. Given an image <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="X_{I}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">X</mi><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝑋</ci><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">X_{I}</annotation></semantics></math>, our target is to enforce <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="F_{M}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">F</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝐹</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">F_{M}</annotation></semantics></math> to generate the response <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="X_{R}" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><msub id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">X</mi><mi id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">𝑋</ci><ci id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">X_{R}</annotation></semantics></math>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="X_{R}=F_{M}(X_{T},X_{I})," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.4.2.cmml">X</mi><mi id="S3.E1.m1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.4.3.cmml">R</mi></msub><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.1.1.1.1.2.4" xref="S3.E1.m1.1.1.1.1.2.4.cmml"><mi id="S3.E1.m1.1.1.1.1.2.4.2" xref="S3.E1.m1.1.1.1.1.2.4.2.cmml">F</mi><mi id="S3.E1.m1.1.1.1.1.2.4.3" xref="S3.E1.m1.1.1.1.1.2.4.3.cmml">M</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">T</mi></msub><mo id="S3.E1.m1.1.1.1.1.2.2.2.4" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml">X</mi><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.2.3.cmml">I</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.2.5" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><apply id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2">𝑋</ci><ci id="S3.E1.m1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.4.3">𝑅</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3"></times><apply id="S3.E1.m1.1.1.1.1.2.4.cmml" xref="S3.E1.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.4.1.cmml" xref="S3.E1.m1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.4.2.cmml" xref="S3.E1.m1.1.1.1.1.2.4.2">𝐹</ci><ci id="S3.E1.m1.1.1.1.1.2.4.3.cmml" xref="S3.E1.m1.1.1.1.1.2.4.3">𝑀</ci></apply><interval closure="open" id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">𝑇</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2">𝑋</ci><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.3">𝐼</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">X_{R}=F_{M}(X_{T},X_{I}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.9" class="ltx_p">where <math id="S3.SS4.p1.6.m1.1" class="ltx_Math" alttext="X_{T}" display="inline"><semantics id="S3.SS4.p1.6.m1.1a"><msub id="S3.SS4.p1.6.m1.1.1" xref="S3.SS4.p1.6.m1.1.1.cmml"><mi id="S3.SS4.p1.6.m1.1.1.2" xref="S3.SS4.p1.6.m1.1.1.2.cmml">X</mi><mi id="S3.SS4.p1.6.m1.1.1.3" xref="S3.SS4.p1.6.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m1.1b"><apply id="S3.SS4.p1.6.m1.1.1.cmml" xref="S3.SS4.p1.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m1.1.1.1.cmml" xref="S3.SS4.p1.6.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m1.1.1.2.cmml" xref="S3.SS4.p1.6.m1.1.1.2">𝑋</ci><ci id="S3.SS4.p1.6.m1.1.1.3.cmml" xref="S3.SS4.p1.6.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m1.1c">X_{T}</annotation></semantics></math> represents the input text. <math id="S3.SS4.p1.7.m2.1" class="ltx_Math" alttext="X_{T}" display="inline"><semantics id="S3.SS4.p1.7.m2.1a"><msub id="S3.SS4.p1.7.m2.1.1" xref="S3.SS4.p1.7.m2.1.1.cmml"><mi id="S3.SS4.p1.7.m2.1.1.2" xref="S3.SS4.p1.7.m2.1.1.2.cmml">X</mi><mi id="S3.SS4.p1.7.m2.1.1.3" xref="S3.SS4.p1.7.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m2.1b"><apply id="S3.SS4.p1.7.m2.1.1.cmml" xref="S3.SS4.p1.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m2.1.1.1.cmml" xref="S3.SS4.p1.7.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.7.m2.1.1.2.cmml" xref="S3.SS4.p1.7.m2.1.1.2">𝑋</ci><ci id="S3.SS4.p1.7.m2.1.1.3.cmml" xref="S3.SS4.p1.7.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m2.1c">X_{T}</annotation></semantics></math> can be an example of the three types: 1) VQA data, e.g., <span id="S3.SS4.p1.9.3" class="ltx_text ltx_font_italic">“What color is the pot?”</span>; 2) GenQA data like <span id="S3.SS4.p1.9.4" class="ltx_text ltx_font_italic">“Can you provide a concise question and answer based on the image?”</span>; 3) EvalQA data, such as <span id="S3.SS4.p1.9.2" class="ltx_text ltx_font_italic">“What kind of flowers are on the picture to the left?<math id="S3.SS4.p1.8.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.SS4.p1.8.1.m1.1a"><mo id="S3.SS4.p1.8.1.m1.1.1" xref="S3.SS4.p1.8.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.1.m1.1b"><ci id="S3.SS4.p1.8.1.m1.1.1.cmml" xref="S3.SS4.p1.8.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.1.m1.1c">\backslash</annotation></semantics></math>nAnswer: pansy. <math id="S3.SS4.p1.9.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.SS4.p1.9.2.m2.1a"><mo id="S3.SS4.p1.9.2.m2.1.1" xref="S3.SS4.p1.9.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.2.m2.1b"><ci id="S3.SS4.p1.9.2.m2.1.1.cmml" xref="S3.SS4.p1.9.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.2.m2.1c">\backslash</annotation></semantics></math>nPlease examine the correctness of this question and answer according to the image content. Output Yes or No with the feedback”</span>. Accordingly, the input instruction template can be unified into the following ones:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<svg id="S3.SS4.p2.pic1" class="ltx_picture" height="41.4" overflow="visible" version="1.1" width="600"><g transform="translate(0,41.4) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#4D4D4D" fill-opacity="1.0"><path d="M 0 5.91 L 0 35.49 C 0 38.75 2.64 41.4 5.91 41.4 L 594.09 41.4 C 597.36 41.4 600 38.75 600 35.49 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 35.49 C 1.97 37.66 3.73 39.43 5.91 39.43 L 594.09 39.43 C 596.27 39.43 598.03 37.66 598.03 35.49 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4" class="ltx_p"><span id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1" class="ltx_text ltx_font_typewriter">&lt;s&gt;</span>USER: <math id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="X_{I}" display="inline"><semantics id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">X</mi><mi id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">𝑋</ci><ci id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">X_{I}</annotation></semantics></math> <math id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="X_{T}" display="inline"><semantics id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><msub id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml">X</mi><mi id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><apply id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2">𝑋</ci><ci id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">X_{T}</annotation></semantics></math><math id="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1a"><mo id="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1" xref="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1b"><ci id="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1c">\backslash</annotation></semantics></math>n ASSISTANT: <math id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1" class="ltx_Math" alttext="X_{R}" display="inline"><semantics id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1a"><msub id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml"><mi id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.2" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.2.cmml">X</mi><mi id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.3" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1b"><apply id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.1.cmml" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.2.cmml" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.2">𝑋</ci><ci id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.3.cmml" xref="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1c">X_{R}</annotation></semantics></math> <span id="S3.SS4.p2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2" class="ltx_text ltx_font_typewriter">&lt;/s&gt;</span></span>
</span></foreignObject></g></g></svg>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.4" class="ltx_p">We follow previous MLLMs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and design the training objective in an autoregressive manner:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\max\sum_{i=i}^{L}\log p(X_{R}|X_{T},X_{I})=\prod_{i}^{L}p_{\theta}(x_{i}|X_{T},X_{I},X_{R,&lt;i})," display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml">max</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><munderover id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mi id="S3.E2.m1.3.3.1.1.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.3.cmml">i</mi></mrow><mi id="S3.E2.m1.3.3.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.2.3.cmml">L</mi></munderover><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.3.3.1.1.1.1.1.3a" xref="S3.E2.m1.3.3.1.1.1.1.1.3.cmml">⁡</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.3.cmml">R</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">T</mi></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.cmml">I</mi></msub></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo rspace="0.111em" id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><munderover id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.cmml">∏</mo><mi id="S3.E2.m1.3.3.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.2.3.cmml">i</mi><mi id="S3.E2.m1.3.3.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">L</mi></munderover><mrow id="S3.E2.m1.3.3.1.1.2.1" xref="S3.E2.m1.3.3.1.1.2.1.cmml"><msub id="S3.E2.m1.3.3.1.1.2.1.3" xref="S3.E2.m1.3.3.1.1.2.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.1.3.2" xref="S3.E2.m1.3.3.1.1.2.1.3.2.cmml">p</mi><mi id="S3.E2.m1.3.3.1.1.2.1.3.3" xref="S3.E2.m1.3.3.1.1.2.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.1.2" xref="S3.E2.m1.3.3.1.1.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.2.1.1.1" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.2.1.1.1.2" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.2.1.1.1.1" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.cmml"><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.2" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.2.cmml">x</mi><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.3" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.2.1.1.1.1.4" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.4.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.4.cmml"><msub id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.3.cmml">T</mi></msub><mo id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.4" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.3.cmml">I</mi></msub><mo id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.5" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.2" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.2.cmml">X</mi><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">R</mi><mo id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml"></mi><mo id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml">&lt;</mo><mi id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml">i</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.2.1.1.1.3" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"></eq><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><max id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"></max><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><ci id="S3.E2.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.3">𝑖</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3">𝐿</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3"><log id="S3.E2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.1"></log><ci id="S3.E2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.4.3">𝑅</ci></apply><list id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑇</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3">𝐼</ci></apply></list></apply></apply></apply></apply><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><apply id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2">product</csymbol><ci id="S3.E2.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.3">𝑖</ci></apply><ci id="S3.E2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3">𝐿</ci></apply><apply id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1"><times id="S3.E2.m1.3.3.1.1.2.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.2"></times><apply id="S3.E2.m1.3.3.1.1.2.1.3.cmml" xref="S3.E2.m1.3.3.1.1.2.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.3.2">𝑝</ci><ci id="S3.E2.m1.3.3.1.1.2.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.1.3.3">𝜃</ci></apply><apply id="S3.E2.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.2.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.4">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.3.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.5.3">𝑖</ci></apply><list id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.4.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3"><apply id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.1.1.1.3">𝑇</ci></apply><apply id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.2.2.2.3">𝐼</ci></apply><apply id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.1.1.1.1.3.3.3.2">𝑋</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑅</ci><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><lt id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"></lt><csymbol cd="latexml" id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2">absent</csymbol><ci id="S3.E2.m1.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3">𝑖</ci></apply></list></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\max\sum_{i=i}^{L}\log p(X_{R}|X_{T},X_{I})=\prod_{i}^{L}p_{\theta}(x_{i}|X_{T},X_{I},X_{R,&lt;i}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.3" class="ltx_p">where <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑥</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">x_{i}</annotation></semantics></math> is the current prediction token and <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">L</annotation></semantics></math> denotes the response sequence length. <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\theta</annotation></semantics></math> denotes the trainable parameters.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2405.14974/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples from the manual filtering and error correction process. <span id="S3.F3.3.1" class="ltx_text" style="color:#FF0000;">Red</span> text indicates error answers, while <span id="S3.F3.4.2" class="ltx_text" style="color:#228B22;">Green</span> text represents manually corrected answers.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Settings</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Training Datasets.</span> For the fair comparison, we utilize the 665K instruction-following dataset introduced in LLaVA1.5, combined with the 842K GenQA data as outlined in Tab. <a href="#S3.T1" title="Table 1 ‣ 3.1 Data Collection for GenQA ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and an additional 64K data comprising one-positive-one-negative pairs as described in Section <a href="#S3.SS2" title="3.2 Data Creation for EvalQA ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, totaling our training datasets. It is important to note that the datasets and annotations used in both VQA and GenQA are the same. There are no additional datasets involved, thus avoiding unfair comparisons caused by the introduction of new instruction data. For EvalQA, we adopt VQAv2 to build the training set, which is already included in the original 665K instruction dataset.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Validation Datasets.</span> We assess LOVA<sup id="S4.SS1.p2.1.2" class="ltx_sup">3</sup> on 10 widely used multimodal datasets and benchmarks. (1) VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> are two large-scale annotated VQA datasets comprising 430K and 943K instances.
(2) VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is a challenging dataset comprising 8000 instances of test-dev set. Most of the images in this dataset are blurred, making it difficult to respond. (3) ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> is a benchmark comprising  21k multimodal multiple-choice questions with diverse science topics. (4) POPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> is a benchmark for evaluating the object hallucination in the MLLM. (5) MME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, SEED-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, MMBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, LLaVA-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, MM-Vet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> are five prominent multimodal benchmarks designed to evaluate various capabilities of MLLMs, including object existence, color recognition, counting, OCR, etc.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Competitors.</span> We compare LOVA<sup id="S4.SS1.p3.1.2" class="ltx_sup">3</sup> with other SOTA models inlcuding MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>, BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, mPLUG-owl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, LLaMA-AdapterV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. We report the results from their paper or the benchmark leaderboard.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.2" class="ltx_p"><span id="S4.SS1.p4.2.1" class="ltx_text ltx_font_bold">Implementation Details.</span>
To ensure a fair comparison, we train LOVA<sup id="S4.SS1.p4.2.2" class="ltx_sup">3</sup> model without tuning any hyperparameters of LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> from its original supervised finetuing stage. The model is trained for one epoch across three tasks: VQA, GenQA, and EvalQA. Specifically, we employ the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> optimizer with a learning rate of <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mn id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml"><mn id="S4.SS1.p4.2.m2.1.1.3.2" xref="S4.SS1.p4.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p4.2.m2.1.1.3.3" xref="S4.SS1.p4.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p4.2.m2.1.1.3.3a" xref="S4.SS1.p4.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p4.2.m2.1.1.3.3.2" xref="S4.SS1.p4.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><times id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">2</cn><apply id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.3.1.cmml" xref="S4.SS1.p4.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.2.cmml" xref="S4.SS1.p4.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p4.2.m2.1.1.3.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3"><minus id="S4.SS1.p4.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">2\times 10^{-5}</annotation></semantics></math> and a total batch size of 128. The training process takes 24.5 hours on an 8 Nvidia A100 (40G) GPU setup.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on five generic tasks including VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, and POPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. The first two columns represent the results on held-in datasets marked as <sup id="S4.T3.15.1" class="ltx_sup">∗</sup>, and the last three columns represent the held-out datasets. The best result on each subtask is <span id="S4.T3.16.2" class="ltx_text ltx_font_bold">bolded</span>.</figcaption>
<div id="S4.T3.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:459.4pt;height:129.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.6pt,19.3pt) scale(0.77,0.77) ;">
<table id="S4.T3.12.10" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.12.10.11" class="ltx_tr">
<td id="S4.T3.12.10.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 2.8pt;">Method</td>
<td id="S4.T3.12.10.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 2.8pt;">Train Paradigm</td>
<td id="S4.T3.12.10.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 2.8pt;">LLM</td>
<td id="S4.T3.12.10.11.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.8pt;">
<table id="S4.T3.12.10.11.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.12.10.11.4.1.1" class="ltx_tr">
<td id="S4.T3.12.10.11.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">VQAv2</td>
</tr>
<tr id="S4.T3.12.10.11.4.1.2" class="ltx_tr">
<td id="S4.T3.12.10.11.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">test-dev</td>
</tr>
</table>
</td>
<td id="S4.T3.12.10.11.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.8pt;">
<table id="S4.T3.12.10.11.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.12.10.11.5.1.1" class="ltx_tr">
<td id="S4.T3.12.10.11.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">GQA</td>
</tr>
<tr id="S4.T3.12.10.11.5.1.2" class="ltx_tr">
<td id="S4.T3.12.10.11.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">test</td>
</tr>
</table>
</td>
<td id="S4.T3.12.10.11.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.8pt;">
<table id="S4.T3.12.10.11.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.12.10.11.6.1.1" class="ltx_tr">
<td id="S4.T3.12.10.11.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">VizWiz</td>
</tr>
<tr id="S4.T3.12.10.11.6.1.2" class="ltx_tr">
<td id="S4.T3.12.10.11.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">test-dev</td>
</tr>
</table>
</td>
<td id="S4.T3.12.10.11.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.8pt;">
<table id="S4.T3.12.10.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.12.10.11.7.1.1" class="ltx_tr">
<td id="S4.T3.12.10.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">ScienceQA</td>
</tr>
<tr id="S4.T3.12.10.11.7.1.2" class="ltx_tr">
<td id="S4.T3.12.10.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">img</td>
</tr>
</table>
</td>
<td id="S4.T3.12.10.11.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.8pt;">
<table id="S4.T3.12.10.11.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.12.10.11.8.1.1" class="ltx_tr">
<td id="S4.T3.12.10.11.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">POPE</td>
</tr>
<tr id="S4.T3.12.10.11.8.1.2" class="ltx_tr">
<td id="S4.T3.12.10.11.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.5pt 2.8pt;">avg</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T3.12.10.12" class="ltx_tr">
<td id="S4.T3.12.10.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.8pt;">BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T3.12.10.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.8pt;"><span id="S4.T3.12.10.12.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T3.12.10.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.8pt;">Vicuna-13B</td>
<td id="S4.T3.12.10.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">41.0</td>
<td id="S4.T3.12.10.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">41.3</td>
<td id="S4.T3.12.10.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">19.6</td>
<td id="S4.T3.12.10.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">61.0</td>
<td id="S4.T3.12.10.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">85.3</td>
</tr>
<tr id="S4.T3.12.10.13" class="ltx_tr">
<td id="S4.T3.12.10.13.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T3.12.10.13.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;"><span id="S4.T3.12.10.13.2.1" class="ltx_text ltx_font_italic">VQA, VQG</span></td>
<td id="S4.T3.12.10.13.3" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">Vicuna-7B</td>
<td id="S4.T3.12.10.13.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">–</td>
<td id="S4.T3.12.10.13.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">49.2</td>
<td id="S4.T3.12.10.13.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">34.5</td>
<td id="S4.T3.12.10.13.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">60.5</td>
<td id="S4.T3.12.10.13.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">–</td>
</tr>
<tr id="S4.T3.12.10.14" class="ltx_tr">
<td id="S4.T3.12.10.14.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T3.12.10.14.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;"><span id="S4.T3.12.10.14.2.1" class="ltx_text ltx_font_italic">VQA, VQG</span></td>
<td id="S4.T3.12.10.14.3" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">Vicuna-13B</td>
<td id="S4.T3.12.10.14.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">–</td>
<td id="S4.T3.12.10.14.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">49.5</td>
<td id="S4.T3.12.10.14.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">33.4</td>
<td id="S4.T3.12.10.14.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">63.1</td>
<td id="S4.T3.12.10.14.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">78.9</td>
</tr>
<tr id="S4.T3.12.10.15" class="ltx_tr">
<td id="S4.T3.12.10.15.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">IDEFICS-9B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S4.T3.12.10.15.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;"><span id="S4.T3.12.10.15.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T3.12.10.15.3" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">LlamA-7B</td>
<td id="S4.T3.12.10.15.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">50.9</td>
<td id="S4.T3.12.10.15.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">38.4</td>
<td id="S4.T3.12.10.15.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">35.5</td>
<td id="S4.T3.12.10.15.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">44.2</td>
<td id="S4.T3.12.10.15.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">–</td>
</tr>
<tr id="S4.T3.4.2.2" class="ltx_tr">
<td id="S4.T3.4.2.2.3" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T3.4.2.2.4" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;"><span id="S4.T3.4.2.2.4.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T3.4.2.2.5" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.8pt;">Qwen-7B</td>
<td id="S4.T3.3.1.1.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">78.8<sup id="S4.T3.3.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.4.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">59.3<sup id="S4.T3.4.2.2.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.4.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">35.2</td>
<td id="S4.T3.4.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">67.1</td>
<td id="S4.T3.4.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.8pt;">–</td>
</tr>
<tr id="S4.T3.6.4.4" class="ltx_tr">
<td id="S4.T3.6.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.8pt;">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T3.6.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.8pt;"><span id="S4.T3.6.4.4.4.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T3.6.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.8pt;">Vicuna-7B</td>
<td id="S4.T3.5.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">78.5<sup id="S4.T3.5.3.3.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.6.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">62.0<sup id="S4.T3.6.4.4.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.6.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">50.0</td>
<td id="S4.T3.6.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">66.8</td>
<td id="S4.T3.6.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.8pt;">85.9</td>
</tr>
<tr id="S4.T3.12.10.10" class="ltx_tr">
<td id="S4.T3.7.5.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 2.8pt;">LOVA<sup id="S4.T3.7.5.5.1.1" class="ltx_sup">3</sup>(ours)</td>
<td id="S4.T3.12.10.10.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 2.8pt;"><span id="S4.T3.12.10.10.7.1" class="ltx_text ltx_font_italic">VQA, GenQA, EvalQA</span></td>
<td id="S4.T3.12.10.10.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 2.8pt;">Vicuna-7B</td>
<td id="S4.T3.8.6.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.8pt;"><span id="S4.T3.8.6.6.2.1" class="ltx_text ltx_font_bold">80.3<math id="S4.T3.8.6.6.2.1.m1.1" class="ltx_Math" alttext="{}^{*}_{+1.8}" display="inline"><semantics id="S4.T3.8.6.6.2.1.m1.1a"><mmultiscripts id="S4.T3.8.6.6.2.1.m1.1.1" xref="S4.T3.8.6.6.2.1.m1.1.1.cmml"><mi id="S4.T3.8.6.6.2.1.m1.1.1.2.2" xref="S4.T3.8.6.6.2.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T3.8.6.6.2.1.m1.1.1a" xref="S4.T3.8.6.6.2.1.m1.1.1.cmml"></mprescripts><mrow id="S4.T3.8.6.6.2.1.m1.1.1.3" xref="S4.T3.8.6.6.2.1.m1.1.1.3.cmml"><mo id="S4.T3.8.6.6.2.1.m1.1.1.3a" xref="S4.T3.8.6.6.2.1.m1.1.1.3.cmml">+</mo><mn id="S4.T3.8.6.6.2.1.m1.1.1.3.2" xref="S4.T3.8.6.6.2.1.m1.1.1.3.2.cmml">1.8</mn></mrow><mrow id="S4.T3.8.6.6.2.1.m1.1.1b" xref="S4.T3.8.6.6.2.1.m1.1.1.cmml"></mrow><mrow id="S4.T3.8.6.6.2.1.m1.1.1c" xref="S4.T3.8.6.6.2.1.m1.1.1.cmml"></mrow><mo id="S4.T3.8.6.6.2.1.m1.1.1.2.3" xref="S4.T3.8.6.6.2.1.m1.1.1.2.3.cmml">∗</mo></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.6.2.1.m1.1b"><apply id="S4.T3.8.6.6.2.1.m1.1.1.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.8.6.6.2.1.m1.1.1.1.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1">subscript</csymbol><apply id="S4.T3.8.6.6.2.1.m1.1.1.2.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.8.6.6.2.1.m1.1.1.2.1.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T3.8.6.6.2.1.m1.1.1.2.2.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1.2.2">absent</csymbol><times id="S4.T3.8.6.6.2.1.m1.1.1.2.3.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1.2.3"></times></apply><apply id="S4.T3.8.6.6.2.1.m1.1.1.3.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1.3"><plus id="S4.T3.8.6.6.2.1.m1.1.1.3.1.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1.3"></plus><cn type="float" id="S4.T3.8.6.6.2.1.m1.1.1.3.2.cmml" xref="S4.T3.8.6.6.2.1.m1.1.1.3.2">1.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.6.2.1.m1.1c">{}^{*}_{+1.8}</annotation></semantics></math></span></td>
<td id="S4.T3.9.7.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.8pt;"><span id="S4.T3.9.7.7.3.1" class="ltx_text ltx_font_bold">63.3<math id="S4.T3.9.7.7.3.1.m1.1" class="ltx_Math" alttext="{}^{*}_{+1.3}" display="inline"><semantics id="S4.T3.9.7.7.3.1.m1.1a"><mmultiscripts id="S4.T3.9.7.7.3.1.m1.1.1" xref="S4.T3.9.7.7.3.1.m1.1.1.cmml"><mi id="S4.T3.9.7.7.3.1.m1.1.1.2.2" xref="S4.T3.9.7.7.3.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T3.9.7.7.3.1.m1.1.1a" xref="S4.T3.9.7.7.3.1.m1.1.1.cmml"></mprescripts><mrow id="S4.T3.9.7.7.3.1.m1.1.1.3" xref="S4.T3.9.7.7.3.1.m1.1.1.3.cmml"><mo id="S4.T3.9.7.7.3.1.m1.1.1.3a" xref="S4.T3.9.7.7.3.1.m1.1.1.3.cmml">+</mo><mn id="S4.T3.9.7.7.3.1.m1.1.1.3.2" xref="S4.T3.9.7.7.3.1.m1.1.1.3.2.cmml">1.3</mn></mrow><mrow id="S4.T3.9.7.7.3.1.m1.1.1b" xref="S4.T3.9.7.7.3.1.m1.1.1.cmml"></mrow><mrow id="S4.T3.9.7.7.3.1.m1.1.1c" xref="S4.T3.9.7.7.3.1.m1.1.1.cmml"></mrow><mo id="S4.T3.9.7.7.3.1.m1.1.1.2.3" xref="S4.T3.9.7.7.3.1.m1.1.1.2.3.cmml">∗</mo></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.7.3.1.m1.1b"><apply id="S4.T3.9.7.7.3.1.m1.1.1.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.9.7.7.3.1.m1.1.1.1.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1">subscript</csymbol><apply id="S4.T3.9.7.7.3.1.m1.1.1.2.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.9.7.7.3.1.m1.1.1.2.1.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T3.9.7.7.3.1.m1.1.1.2.2.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1.2.2">absent</csymbol><times id="S4.T3.9.7.7.3.1.m1.1.1.2.3.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1.2.3"></times></apply><apply id="S4.T3.9.7.7.3.1.m1.1.1.3.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1.3"><plus id="S4.T3.9.7.7.3.1.m1.1.1.3.1.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1.3"></plus><cn type="float" id="S4.T3.9.7.7.3.1.m1.1.1.3.2.cmml" xref="S4.T3.9.7.7.3.1.m1.1.1.3.2">1.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.7.3.1.m1.1c">{}^{*}_{+1.3}</annotation></semantics></math></span></td>
<td id="S4.T3.10.8.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.8pt;"><span id="S4.T3.10.8.8.4.1" class="ltx_text ltx_font_bold">53.6<sub id="S4.T3.10.8.8.4.1.1" class="ltx_sub"><span id="S4.T3.10.8.8.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+3.6</span></sub></span></td>
<td id="S4.T3.11.9.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.8pt;"><span id="S4.T3.11.9.9.5.1" class="ltx_text ltx_font_bold">68.0<sub id="S4.T3.11.9.9.5.1.1" class="ltx_sub"><span id="S4.T3.11.9.9.5.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+1.2</span></sub></span></td>
<td id="S4.T3.12.10.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.8pt;"><span id="S4.T3.12.10.10.6.1" class="ltx_text ltx_font_bold">87.4<sub id="S4.T3.12.10.10.6.1.1" class="ltx_sub"><span id="S4.T3.12.10.10.6.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+1.5</span></sub></span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results on multimodal benchmarks, including MME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and SEED-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, MMBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and LLava-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></figcaption>
<div id="S4.T4.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:448.8pt;height:107.6pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-115.6pt,27.5pt) scale(0.66,0.66) ;">
<table id="S4.T4.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.6.6.7" class="ltx_tr">
<td id="S4.T4.6.6.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T4.6.6.7.1.1" class="ltx_text">Method</span></td>
<td id="S4.T4.6.6.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T4.6.6.7.2.1" class="ltx_text">Train Paradigm</span></td>
<td id="S4.T4.6.6.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T4.6.6.7.3.1" class="ltx_text">LLM</span></td>
<td id="S4.T4.6.6.7.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T4.6.6.7.4.1" class="ltx_text">MME</span></td>
<td id="S4.T4.6.6.7.5" class="ltx_td ltx_align_center ltx_border_tt">SEED-Bench</td>
<td id="S4.T4.6.6.7.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">MMBench</td>
<td id="S4.T4.6.6.7.7" class="ltx_td ltx_align_center ltx_border_tt">LLaVA-Bench</td>
</tr>
<tr id="S4.T4.6.6.8" class="ltx_tr">
<td id="S4.T4.6.6.8.1" class="ltx_td ltx_align_center">Image</td>
<td id="S4.T4.6.6.8.2" class="ltx_td ltx_align_center">En</td>
<td id="S4.T4.6.6.8.3" class="ltx_td ltx_align_center">Cn</td>
<td id="S4.T4.6.6.8.4" class="ltx_td ltx_align_center">All</td>
</tr>
<tr id="S4.T4.6.6.9" class="ltx_tr">
<td id="S4.T4.6.6.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T4.6.6.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.6.6.9.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T4.6.6.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Vicuna-13B</td>
<td id="S4.T4.6.6.9.4" class="ltx_td ltx_align_center ltx_border_t">1293.8</td>
<td id="S4.T4.6.6.9.5" class="ltx_td ltx_align_center ltx_border_t">49.7</td>
<td id="S4.T4.6.6.9.6" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S4.T4.6.6.9.7" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S4.T4.6.6.9.8" class="ltx_td ltx_align_center ltx_border_t">38.1</td>
</tr>
<tr id="S4.T4.6.6.10" class="ltx_tr">
<td id="S4.T4.6.6.10.1" class="ltx_td ltx_align_left ltx_border_r">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T4.6.6.10.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.6.6.10.2.1" class="ltx_text ltx_font_italic">VQA, VQG</span></td>
<td id="S4.T4.6.6.10.3" class="ltx_td ltx_align_left ltx_border_r">Vicuna-7B</td>
<td id="S4.T4.6.6.10.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.10.5" class="ltx_td ltx_align_center">58.8</td>
<td id="S4.T4.6.6.10.6" class="ltx_td ltx_align_center">36.0</td>
<td id="S4.T4.6.6.10.7" class="ltx_td ltx_align_center">23.7</td>
<td id="S4.T4.6.6.10.8" class="ltx_td ltx_align_center">60.9</td>
</tr>
<tr id="S4.T4.6.6.11" class="ltx_tr">
<td id="S4.T4.6.6.11.1" class="ltx_td ltx_align_left ltx_border_r">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T4.6.6.11.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.6.6.11.2.1" class="ltx_text ltx_font_italic">VQA, VQG</span></td>
<td id="S4.T4.6.6.11.3" class="ltx_td ltx_align_left ltx_border_r">Vicuna-13B</td>
<td id="S4.T4.6.6.11.4" class="ltx_td ltx_align_center">1212.8</td>
<td id="S4.T4.6.6.11.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.11.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.11.7" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.11.8" class="ltx_td ltx_align_center">58.2</td>
</tr>
<tr id="S4.T4.6.6.12" class="ltx_tr">
<td id="S4.T4.6.6.12.1" class="ltx_td ltx_align_left ltx_border_r">mPLUG-owl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S4.T4.6.6.12.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.6.6.12.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T4.6.6.12.3" class="ltx_td ltx_align_left ltx_border_r">Llama-7B</td>
<td id="S4.T4.6.6.12.4" class="ltx_td ltx_align_center">967.3</td>
<td id="S4.T4.6.6.12.5" class="ltx_td ltx_align_center">37.9</td>
<td id="S4.T4.6.6.12.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.12.7" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.12.8" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T4.6.6.13" class="ltx_tr">
<td id="S4.T4.6.6.13.1" class="ltx_td ltx_align_left ltx_border_r">LLaMA-AdapterV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S4.T4.6.6.13.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.6.6.13.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T4.6.6.13.3" class="ltx_td ltx_align_left ltx_border_r">Llama-7B</td>
<td id="S4.T4.6.6.13.4" class="ltx_td ltx_align_center">972.7</td>
<td id="S4.T4.6.6.13.5" class="ltx_td ltx_align_center">35.2</td>
<td id="S4.T4.6.6.13.6" class="ltx_td ltx_align_center">41.0</td>
<td id="S4.T4.6.6.13.7" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.6.6.13.8" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T4.6.6.14" class="ltx_tr">
<td id="S4.T4.6.6.14.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T4.6.6.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.6.6.14.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T4.6.6.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Vicuna-7B</td>
<td id="S4.T4.6.6.14.4" class="ltx_td ltx_align_center ltx_border_t">1510.7</td>
<td id="S4.T4.6.6.14.5" class="ltx_td ltx_align_center ltx_border_t">66.2</td>
<td id="S4.T4.6.6.14.6" class="ltx_td ltx_align_center ltx_border_t">64.3</td>
<td id="S4.T4.6.6.14.7" class="ltx_td ltx_align_center ltx_border_t">58.3</td>
<td id="S4.T4.6.6.14.8" class="ltx_td ltx_align_center ltx_border_t">64.0</td>
</tr>
<tr id="S4.T4.6.6.6" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">LOVA<sup id="S4.T4.1.1.1.1.1" class="ltx_sup">3</sup>(ours)</td>
<td id="S4.T4.6.6.6.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T4.6.6.6.7.1" class="ltx_text ltx_font_italic">VQA, GenQA, EvalQA</span></td>
<td id="S4.T4.6.6.6.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Vicuna-7B</td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.2.2.1" class="ltx_text ltx_font_bold">1552.7<sub id="S4.T4.2.2.2.2.1.1" class="ltx_sub"><span id="S4.T4.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+42.0</span></sub></span></td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.3.3.3.1" class="ltx_text ltx_font_bold">67.1<sub id="S4.T4.3.3.3.3.1.1" class="ltx_sub"><span id="S4.T4.3.3.3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+0.9</span></sub></span></td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.4.4.4.4.1" class="ltx_text ltx_font_bold">66.8<sub id="S4.T4.4.4.4.4.1.1" class="ltx_sub"><span id="S4.T4.4.4.4.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+2.5</span></sub></span></td>
<td id="S4.T4.5.5.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.5.5.5.5.1" class="ltx_text ltx_font_bold">60.5<sub id="S4.T4.5.5.5.5.1.1" class="ltx_sub"><span id="S4.T4.5.5.5.5.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+2.2</span></sub></span></td>
<td id="S4.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.6.6.6.1" class="ltx_text ltx_font_bold">68.3<sub id="S4.T4.6.6.6.6.1.1" class="ltx_sub"><span id="S4.T4.6.6.6.6.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+4.3</span></sub></span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Multimodal reasoning ability on MM-Vet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. Rec denotes Recognition; Know denotes knowledge; Gen denotes Language generation; and Spat denotes Spatial awareness.</figcaption>
<div id="S4.T5.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:469.0pt;height:101.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.6pt,12.6pt) scale(0.8,0.8) ;">
<table id="S4.T5.7.7" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.7.7.8" class="ltx_tr">
<td id="S4.T5.7.7.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Method</td>
<td id="S4.T5.7.7.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Train Paradigm</td>
<td id="S4.T5.7.7.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">LLM</td>
<td id="S4.T5.7.7.8.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Rec</td>
<td id="S4.T5.7.7.8.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">OCR</td>
<td id="S4.T5.7.7.8.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Know</td>
<td id="S4.T5.7.7.8.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Gen</td>
<td id="S4.T5.7.7.8.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Spat</td>
<td id="S4.T5.7.7.8.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Total</td>
</tr>
<tr id="S4.T5.7.7.9" class="ltx_tr">
<td id="S4.T5.7.7.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S4.T5.7.7.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.9.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T5.7.7.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Vicuna-7B</td>
<td id="S4.T5.7.7.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">27.4</td>
<td id="S4.T5.7.7.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">15.0</td>
<td id="S4.T5.7.7.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">12.8</td>
<td id="S4.T5.7.7.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">13.9</td>
<td id="S4.T5.7.7.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">20.3</td>
<td id="S4.T5.7.7.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">22.1</td>
</tr>
<tr id="S4.T5.7.7.10" class="ltx_tr">
<td id="S4.T5.7.7.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T5.7.7.10.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.10.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T5.7.7.10.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Vicuna-13B</td>
<td id="S4.T5.7.7.10.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">27.5</td>
<td id="S4.T5.7.7.10.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">11.1</td>
<td id="S4.T5.7.7.10.6" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">11.8</td>
<td id="S4.T5.7.7.10.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">7.0</td>
<td id="S4.T5.7.7.10.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">16.2</td>
<td id="S4.T5.7.7.10.9" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">22.1</td>
</tr>
<tr id="S4.T5.7.7.11" class="ltx_tr">
<td id="S4.T5.7.7.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T5.7.7.11.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.11.2.1" class="ltx_text ltx_font_italic">VQA, VQG</span></td>
<td id="S4.T5.7.7.11.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Vicuna-7B</td>
<td id="S4.T5.7.7.11.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">32.4</td>
<td id="S4.T5.7.7.11.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">14.6</td>
<td id="S4.T5.7.7.11.6" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">16.5</td>
<td id="S4.T5.7.7.11.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">18.2</td>
<td id="S4.T5.7.7.11.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">18.6</td>
<td id="S4.T5.7.7.11.9" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">26.2</td>
</tr>
<tr id="S4.T5.7.7.12" class="ltx_tr">
<td id="S4.T5.7.7.12.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T5.7.7.12.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.12.2.1" class="ltx_text ltx_font_italic">VQA, VQG</span></td>
<td id="S4.T5.7.7.12.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Vicuna-13B</td>
<td id="S4.T5.7.7.12.4" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">30.8</td>
<td id="S4.T5.7.7.12.5" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">16.0</td>
<td id="S4.T5.7.7.12.6" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">9.8</td>
<td id="S4.T5.7.7.12.7" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">9.0</td>
<td id="S4.T5.7.7.12.8" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">21.1</td>
<td id="S4.T5.7.7.12.9" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">25.6</td>
</tr>
<tr id="S4.T5.7.7.13" class="ltx_tr">
<td id="S4.T5.7.7.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T5.7.7.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.13.2.1" class="ltx_text ltx_font_italic">VQA</span></td>
<td id="S4.T5.7.7.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Vicuna-7B</td>
<td id="S4.T5.7.7.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">37.0</td>
<td id="S4.T5.7.7.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">21.0</td>
<td id="S4.T5.7.7.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">17.6</td>
<td id="S4.T5.7.7.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">20.4</td>
<td id="S4.T5.7.7.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">24.9</td>
<td id="S4.T5.7.7.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">31.2</td>
</tr>
<tr id="S4.T5.7.7.7" class="ltx_tr">
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">LOVA<sup id="S4.T5.1.1.1.1.1" class="ltx_sup">3</sup>(ours)</td>
<td id="S4.T5.7.7.7.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.7.8.1" class="ltx_text ltx_font_italic">VQA, GenQA, EvalQA</span></td>
<td id="S4.T5.7.7.7.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">Vicuna-7B</td>
<td id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.2.2.2.2.1" class="ltx_text ltx_font_bold">41.5<sub id="S4.T5.2.2.2.2.1.1" class="ltx_sub"><span id="S4.T5.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+4.5</span></sub></span></td>
<td id="S4.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.3.3.3.3.1" class="ltx_text ltx_font_bold">23.6<sub id="S4.T5.3.3.3.3.1.1" class="ltx_sub"><span id="S4.T5.3.3.3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+2.6</span></sub></span></td>
<td id="S4.T5.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.4.4.4.4.1" class="ltx_text ltx_font_bold">23.9<sub id="S4.T5.4.4.4.4.1.1" class="ltx_sub"><span id="S4.T5.4.4.4.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+6.3</span></sub></span></td>
<td id="S4.T5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.5.5.5.5.1" class="ltx_text ltx_font_bold">24.6<sub id="S4.T5.5.5.5.5.1.1" class="ltx_sub"><span id="S4.T5.5.5.5.5.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+4.2</span></sub></span></td>
<td id="S4.T5.6.6.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.6.6.6.6.1" class="ltx_text ltx_font_bold">30.3<sub id="S4.T5.6.6.6.6.1.1" class="ltx_sub"><span id="S4.T5.6.6.6.6.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+5.4</span></sub></span></td>
<td id="S4.T5.7.7.7.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.7.7.7.7.1" class="ltx_text ltx_font_bold">35.2<sub id="S4.T5.7.7.7.7.1.1" class="ltx_sub"><span id="S4.T5.7.7.7.7.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+4.0</span></sub></span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Generic tasks.</span> As shown in Tab. <a href="#S4.T3" title="Table 3 ‣ 4.1 Datasets and Settings ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, LOVA<sup id="S4.SS2.p1.1.2" class="ltx_sup">3</sup> outperforms LLaVA1.5 across all five datasets and obtains 3.6% improvement on VizWiz dataset, 1.3% improvement on GQA, 1.8% improvement on VQAv2 (1,932 samples are correctly predicted), and 1.2% improvement on ScienceQA. As for the object hallucination benchmarks, our model attains 87.4% accuracy at an average of its three subsets. Remarkably, these enhancements in VQAv2 and GQA performance are achieved without any extra datasets, underscoring the significant impact of integrating GenQA and EvalQA into our training to promote performance improvements on these generic VQA tasks.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">MME, SEED-Bench, MMBench, LLaVA-Bench.</span>
In Tab. <a href="#S4.T4" title="Table 4 ‣ 4.1 Datasets and Settings ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we evaluate four prevalent multimodal benchmarks, where our LOVA<sup id="S4.SS2.p2.1.2" class="ltx_sup">3</sup> surpass LLaVA1.5 with 42.0% on MME benchmark, 0.9% increase in accuracy on SEED-Bench, 2.5% on MMBench (En), 2.2% MMBench (Cn) and 4.3% on LLaVA-Bench. Such results showcase enhanced multimodal reasoning capabilities for complex tasks compared to vanilla LLaVA1.5, which is solely trained with VQA tasks.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.3" class="ltx_p"><span id="S4.SS2.p3.3.1" class="ltx_text ltx_font_bold">MM-Vet.</span> In Tab. <a href="#S4.T5" title="Table 5 ‣ 4.1 Datasets and Settings ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we compare LOVA<sup id="S4.SS2.p3.3.2" class="ltx_sup">3</sup> with other approaches on MM-Vet, which is a challenging benchmark including numerous complex VQA samples that demand integration of several multimodal capabilities for answering. As illustrated in Tab. <a href="#S4.T5" title="Table 5 ‣ 4.1 Datasets and Settings ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the results show that our LOVA<sup id="S4.SS2.p3.3.3" class="ltx_sup">3</sup> outperforms LLaVA-1.5 by 4.0% at an average. Such improvement demonstrates the effectiveness of LOVA<sup id="S4.SS2.p3.3.4" class="ltx_sup">3</sup> in solving these challenging multimodal questions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>

<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Abaltion studies on different finetuning datasets.</figcaption>
<div id="S4.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:352.3pt;height:141.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.7pt,19.8pt) scale(0.78,0.78) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.1.1" class="ltx_text">Row</span></td>
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:-0.5pt 2.8pt;" colspan="3">Finetuning Corpus</td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.3.1" class="ltx_text">GQA</span></td>
<td id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.4.1" class="ltx_text">VizWiz</span></td>
<td id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.5.1" class="ltx_text">ScienceQA</span></td>
<td id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.6.1" class="ltx_text">POPE</span></td>
<td id="S4.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.7.1" class="ltx_text">MME</span></td>
<td id="S4.T6.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 2.8pt;" rowspan="2"><span id="S4.T6.1.1.1.8.1" class="ltx_text">Size</span></td>
</tr>
<tr id="S4.T6.1.1.2" class="ltx_tr">
<td id="S4.T6.1.1.2.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">GenQA-Generic</td>
<td id="S4.T6.1.1.2.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">GenQA-Grounding</td>
<td id="S4.T6.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">EvalQA</td>
</tr>
<tr id="S4.T6.1.1.3" class="ltx_tr">
<td id="S4.T6.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:-0.5pt 2.8pt;">0</td>
<td id="S4.T6.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 2.8pt;" colspan="3">LLaVA-1.5 (Baseline)</td>
<td id="S4.T6.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">62.0</td>
<td id="S4.T6.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">50.0</td>
<td id="S4.T6.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">66.8</td>
<td id="S4.T6.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">85.9</td>
<td id="S4.T6.1.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 2.8pt;">1510.7</td>
<td id="S4.T6.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">665K</td>
</tr>
<tr id="S4.T6.1.1.4" class="ltx_tr">
<td id="S4.T6.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:-0.5pt 2.8pt;">1</td>
<td id="S4.T6.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.4.3" class="ltx_td ltx_border_t" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.4.4" class="ltx_td ltx_border_r ltx_border_t" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">63.1</td>
<td id="S4.T6.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">53.1</td>
<td id="S4.T6.1.1.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">67.4</td>
<td id="S4.T6.1.1.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">86.9</td>
<td id="S4.T6.1.1.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 2.8pt;">1550.7</td>
<td id="S4.T6.1.1.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 2.8pt;">722K</td>
</tr>
<tr id="S4.T6.1.1.5" class="ltx_tr">
<td id="S4.T6.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:-0.5pt 2.8pt;">2</td>
<td id="S4.T6.1.1.5.2" class="ltx_td" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.5.3" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.5.4" class="ltx_td ltx_border_r" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.5.5" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">62.8</td>
<td id="S4.T6.1.1.5.6" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">50.9</td>
<td id="S4.T6.1.1.5.7" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">66.4</td>
<td id="S4.T6.1.1.5.8" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">86.6</td>
<td id="S4.T6.1.1.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">1495.8</td>
<td id="S4.T6.1.1.5.10" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">120K</td>
</tr>
<tr id="S4.T6.1.1.6" class="ltx_tr">
<td id="S4.T6.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:-0.5pt 2.8pt;">3</td>
<td id="S4.T6.1.1.6.2" class="ltx_td" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.6.3" class="ltx_td" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.6.5" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">62.8</td>
<td id="S4.T6.1.1.6.6" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">49.1</td>
<td id="S4.T6.1.1.6.7" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">67.8</td>
<td id="S4.T6.1.1.6.8" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">87.0</td>
<td id="S4.T6.1.1.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">1535.6</td>
<td id="S4.T6.1.1.6.10" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">64K</td>
</tr>
<tr id="S4.T6.1.1.7" class="ltx_tr">
<td id="S4.T6.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:-0.5pt 2.8pt;">4</td>
<td id="S4.T6.1.1.7.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.7.3" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.7.4" class="ltx_td ltx_border_r" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.7.5" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">63.3</td>
<td id="S4.T6.1.1.7.6" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">53.2</td>
<td id="S4.T6.1.1.7.7" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">67.4</td>
<td id="S4.T6.1.1.7.8" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">86.7</td>
<td id="S4.T6.1.1.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">1523.6</td>
<td id="S4.T6.1.1.7.10" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">842K</td>
</tr>
<tr id="S4.T6.1.1.8" class="ltx_tr">
<td id="S4.T6.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:-0.5pt 2.8pt;">5</td>
<td id="S4.T6.1.1.8.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.8.3" class="ltx_td" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.8.5" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;"><span id="S4.T6.1.1.8.5.1" class="ltx_text ltx_font_bold">63.7</span></td>
<td id="S4.T6.1.1.8.6" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;"><span id="S4.T6.1.1.8.6.1" class="ltx_text ltx_font_bold">54.4</span></td>
<td id="S4.T6.1.1.8.7" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">67.0</td>
<td id="S4.T6.1.1.8.8" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">86.9</td>
<td id="S4.T6.1.1.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">1520.8</td>
<td id="S4.T6.1.1.8.10" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">786K</td>
</tr>
<tr id="S4.T6.1.1.9" class="ltx_tr">
<td id="S4.T6.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:-0.5pt 2.8pt;">6</td>
<td id="S4.T6.1.1.9.2" class="ltx_td" style="padding:-0.5pt 2.8pt;"></td>
<td id="S4.T6.1.1.9.3" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.9.5" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">63.1</td>
<td id="S4.T6.1.1.9.6" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">51.1</td>
<td id="S4.T6.1.1.9.7" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">67.5</td>
<td id="S4.T6.1.1.9.8" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">86.8</td>
<td id="S4.T6.1.1.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 2.8pt;">1478.7</td>
<td id="S4.T6.1.1.9.10" class="ltx_td ltx_align_center" style="padding:-0.5pt 2.8pt;">184K</td>
</tr>
<tr id="S4.T6.1.1.10" class="ltx_tr">
<td id="S4.T6.1.1.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:-0.5pt 2.8pt;">7</td>
<td id="S4.T6.1.1.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:-0.5pt 2.8pt;">✓</td>
<td id="S4.T6.1.1.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;">63.3</td>
<td id="S4.T6.1.1.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;">53.6</td>
<td id="S4.T6.1.1.10.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;"><span id="S4.T6.1.1.10.7.1" class="ltx_text ltx_font_bold">68.0</span></td>
<td id="S4.T6.1.1.10.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;"><span id="S4.T6.1.1.10.8.1" class="ltx_text ltx_font_bold">87.4</span></td>
<td id="S4.T6.1.1.10.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:-0.5pt 2.8pt;"><span id="S4.T6.1.1.10.9.1" class="ltx_text ltx_font_bold">1552.7</span></td>
<td id="S4.T6.1.1.10.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 2.8pt;">906K</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results of multimodal large language models on the test set of <span id="S4.T7.6.1" class="ltx_text ltx_font_bold">EvalQABench (ours).</span></figcaption>
<div id="S4.T7.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:388.0pt;height:173.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.5pt,21.6pt) scale(0.8,0.8) ;">
<table id="S4.T7.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T7.4.4.5" class="ltx_tr">
<td id="S4.T7.4.4.5.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 4.0pt;" rowspan="2"><span id="S4.T7.4.4.5.1.1" class="ltx_text">Method</span></td>
<td id="S4.T7.4.4.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 4.0pt;" rowspan="2"><span id="S4.T7.4.4.5.2.1" class="ltx_text">LLM</span></td>
<td id="S4.T7.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 4.0pt;" colspan="4">Test Set</td>
</tr>
<tr id="S4.T7.4.4.6" class="ltx_tr">
<td id="S4.T7.4.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">Accuracy</td>
<td id="S4.T7.4.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">Precision</td>
<td id="S4.T7.4.4.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">F1 Score</td>
<td id="S4.T7.4.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">No (%)</td>
</tr>
<tr id="S4.T7.4.4.7" class="ltx_tr">
<td id="S4.T7.4.4.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;" colspan="6"><span id="S4.T7.4.4.7.1.1" class="ltx_text ltx_font_italic">Vision Language Pretraining Model</span></td>
</tr>
<tr id="S4.T7.4.4.8" class="ltx_tr">
<td id="S4.T7.4.4.8.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T7.4.4.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 4.0pt;">Flan-T5-XXL-11B</td>
<td id="S4.T7.4.4.8.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">58.00</td>
<td id="S4.T7.4.4.8.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">82.79</td>
<td id="S4.T7.4.4.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">32.47</td>
<td id="S4.T7.4.4.8.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">87.80</td>
</tr>
<tr id="S4.T7.4.4.9" class="ltx_tr">
<td id="S4.T7.4.4.9.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;" colspan="6"><span id="S4.T7.4.4.9.1.1" class="ltx_text ltx_font_italic">Multimodal Large Language Models</span></td>
</tr>
<tr id="S4.T7.4.4.10" class="ltx_tr">
<td id="S4.T7.4.4.10.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T7.4.4.10.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T7.4.4.10.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">38.04</td>
<td id="S4.T7.4.4.10.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">41.49</td>
<td id="S4.T7.4.4.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">48.47</td>
<td id="S4.T7.4.4.10.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">29.76</td>
</tr>
<tr id="S4.T7.4.4.11" class="ltx_tr">
<td id="S4.T7.4.4.11.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T7.4.4.11.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 4.0pt;">Vicuna-13B</td>
<td id="S4.T7.4.4.11.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">61.42</td>
<td id="S4.T7.4.4.11.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">57.60</td>
<td id="S4.T7.4.4.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">69.18</td>
<td id="S4.T7.4.4.11.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">24.82</td>
</tr>
<tr id="S4.T7.4.4.12" class="ltx_tr">
<td id="S4.T7.4.4.12.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">CogVLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S4.T7.4.4.12.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T7.4.4.12.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">60.64</td>
<td id="S4.T7.4.4.12.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">56.59</td>
<td id="S4.T7.4.4.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">69.88</td>
<td id="S4.T7.4.4.12.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">19.32</td>
</tr>
<tr id="S4.T7.4.4.13" class="ltx_tr">
<td id="S4.T7.4.4.13.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">Qwen-VL-Chat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T7.4.4.13.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 4.0pt;">Qwen-7B</td>
<td id="S4.T7.4.4.13.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">63.66</td>
<td id="S4.T7.4.4.13.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">63.48</td>
<td id="S4.T7.4.4.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">63.90</td>
<td id="S4.T7.4.4.13.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">49.34</td>
</tr>
<tr id="S4.T7.4.4.14" class="ltx_tr">
<td id="S4.T7.4.4.14.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">InternLM-XC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
<td id="S4.T7.4.4.14.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 4.0pt;">InternLM-7B</td>
<td id="S4.T7.4.4.14.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">69.58</td>
<td id="S4.T7.4.4.14.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">70.66</td>
<td id="S4.T7.4.4.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 4.0pt;">68.76</td>
<td id="S4.T7.4.4.14.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">52.62</td>
</tr>
<tr id="S4.T7.4.4.15" class="ltx_tr">
<td id="S4.T7.4.4.15.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 4.0pt;">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T7.4.4.15.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T7.4.4.15.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">64.92</td>
<td id="S4.T7.4.4.15.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">61.28</td>
<td id="S4.T7.4.4.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 4.0pt;">69.80</td>
<td id="S4.T7.4.4.15.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">33.84</td>
</tr>
<tr id="S4.T7.4.4.4" class="ltx_tr">
<td id="S4.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.5pt 4.0pt;">LOVA<sup id="S4.T7.1.1.1.1.1" class="ltx_sup">3</sup>(ours)</td>
<td id="S4.T7.4.4.4.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;"><span id="S4.T7.2.2.2.2.1" class="ltx_text ltx_font_bold">79.58<sub id="S4.T7.2.2.2.2.1.1" class="ltx_sub"><span id="S4.T7.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+14.66</span></sub></span></td>
<td id="S4.T7.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;"><span id="S4.T7.3.3.3.3.1" class="ltx_text ltx_font_bold">79.15<sub id="S4.T7.3.3.3.3.1.1" class="ltx_sub"><span id="S4.T7.3.3.3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+17.87</span></sub></span></td>
<td id="S4.T7.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.5pt 4.0pt;"><span id="S4.T7.4.4.4.4.1" class="ltx_text ltx_font_bold">79.72<sub id="S4.T7.4.4.4.4.1.1" class="ltx_sub"><span id="S4.T7.4.4.4.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">+9.92</span></sub></span></td>
<td id="S4.T7.4.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;">49.26</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">We split the data used in the GenQA task into two groups: GenQA-General and GenQA-Grounding. The findings, presented in Tab. <a href="#S4.T6" title="Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, are instrumental in investigating the contributions of GenQA and EvalQA to model efficacy. <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">(1)</span> Comparing the first four rows, one can find that both GenQA-General and EvalQA data are more effective in improving performance than GenQA-Grounding. <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_bold">(2)</span> By comparing rows 4 and 7, it demonstrates the effectiveness of EvalQA across five datasets, especially on MME. <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_bold">(3)</span> When comparing rows 6 and 7, by removing GenQA-General from the finetuning corpus, the performance drops significantly on MME and VizWiz. <span id="S4.SS3.p1.1.4" class="ltx_text ltx_font_bold">(4)</span> Compare the rows 0 and 3, one can observe that even adding 64K data into the training, there are obvious improvements in GQA, ScienceQA, and MME. By analyzing the data size, we did not introduce any new datasets for training the GenQA task. For EvalQA, we only added 32K new negative answer annotations while retaining the original questions used for training VQA capabilities. The details of the data size are provided in the right column in Tab. <a href="#S4.T6" title="Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Benchmark of EvalQABench</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.2" class="ltx_p">We report the evaluation results on our EvalQABench test set in Tab. <a href="#S4.T7" title="Table 7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> to validate the EvalQA ability of current SOTA models and LOVA<sup id="S4.SS4.p1.2.1" class="ltx_sup">3</sup>. We select BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, CogVLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, Qwen-VL-Chat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, InternLM-XC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>, and LLaVA1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> for the comparison. We ask these models to answer “Yes” or “No” strictly and record the results for calculating the Accuracy, Precision, F1 score, and No (%) metrics. Here, No (%) indicates the percentage of results classified as "No," which ideally should approximate 50% due to the one-positive-one-negative setting utilized in our test set. As indicated by the data presented in the table, BLIP2 predominantly yields "No" responses across most test instances. Among the state-of-the-art MLLMs, InternLM-XC stands out by delivering superior performance on these four metrics. Trained with EvalQA data, LOVA<sup id="S4.SS4.p1.2.2" class="ltx_sup">3</sup> shows several improvements over our baseline LLaVA1.5 by margins of 14.66%, 17.87%, and 9.92% in Accuracy, Precision, and F1 Score, respectively.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Limitations</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.2" class="ltx_p">In this work, we propose a novel multimodal framework, <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">LOVA<sup id="S5.p1.1.1.1" class="ltx_sup"><span id="S5.p1.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span>, which is capable of mimicking the human visual question answering, asking, and assessment to achieve deeper multimodal understanding. We introduce two additional training tasks, <span id="S5.p1.2.2" class="ltx_text ltx_font_bold">GenQA</span> and <span id="S5.p1.2.3" class="ltx_text ltx_font_bold">EvalQA</span>, to help MLLM acquire these abilities. We establish <span id="S5.p1.2.4" class="ltx_text ltx_font_bold">EvalQABench</span>, a novel benchmark to assess the VQA samples between multiple MLLMs. Experimental results show that LOVA<sup id="S5.p1.2.5" class="ltx_sup">3</sup> achieves superior performance across various benchmarks, including MM-Vet, SEED, and VizWiz, demonstrating the effectiveness of the two additional abilities.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.2" class="ltx_p"><span id="S5.p2.2.1" class="ltx_text ltx_font_bold">Limitations.</span> (1) Due to computational constraints, we do not test larger LLMs, such as the 13B or 34B variants. However, we believe that our LOVA<sup id="S5.p2.2.2" class="ltx_sup">3</sup> could be beneficial for larger LLMs, as other MLLMs have shown performance improvements with increased LLM scale. (2) GenQA and EvalQA as two additional tasks increase training costs, but it is inevitable for an MLLM to acquire new capabilities. (3) Due to the limited scope of instruction tuning datasets, LOVA<sup id="S5.p2.2.3" class="ltx_sup">3</sup> cannot address domain-specific multimodal tasks well, such as text-centric VQA or mathematic-relevant VQA.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.

</span>
<span class="ltx_bibblock">nocaps: novel object captioning at scale.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 6077–6086, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.12966</span>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.

</span>
<span class="ltx_bibblock">Introducing our multimodal models, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hedi Ben-Younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome.

</span>
<span class="ltx_bibblock">Mutan: Multimodal tucker fusion for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 2612–2620, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Georg Cantor.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Über unendliche, lineare Punktmannigfaltigkeiten: Arbeiten zur Mengenlehre aus den Jahren 1872–1884</span>, volume 2.

</span>
<span class="ltx_bibblock">Springer-Verlag, 2013.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.

</span>
<span class="ltx_bibblock">Honeybee: Locality-enhanced projector for multimodal llm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu.

</span>
<span class="ltx_bibblock">Position-enhanced visual instruction tuning for multimodal large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.13437</span>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.09478</span>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.

</span>
<span class="ltx_bibblock">Shikra: Unleashing multimodal llm’s referential dialogue magic.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv:2306.15195</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao.

</span>
<span class="ltx_bibblock">Are we on the right way for evaluating large vision-language models?, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv:1504.00325</span>, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Can pre-trained vision and language models answer visual information-seeking questions?

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.11713</span>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang.

</span>
<span class="ltx_bibblock">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.11416</span>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, and Xuan-Jing Huang.

</span>
<span class="ltx_bibblock">A reinforcement learning framework for natural question generation using bi-discriminators.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th International Conference on Computational Linguistics</span>, pages 1763–1774, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al.

</span>
<span class="ltx_bibblock">Mme: A comprehensive evaluation benchmark for multimodal large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv:2306.13394</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna.

</span>
<span class="ltx_bibblock">Blink: Multimodal large language models can see but not perceive.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.12390</span>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Richard Gale.

</span>
<span class="ltx_bibblock">Asking questions that matter… asking questions of value.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">International Journal for the Scholarship of teaching and learning</span>, 3(2):3, 2009.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al.

</span>
<span class="ltx_bibblock">Llama-adapter v2: Parameter-efficient visual instruction model.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv:2304.15010</span>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, and Yu Qiao.

</span>
<span class="ltx_bibblock">Sphinx-x: Scaling data and parameters for a family of multi-modal large language models, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, and Rongrong Ji.

</span>
<span class="ltx_bibblock">Cantor: Inspiring multimodal chain-of-thought of mllm, 2024.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Brian Gordon, Yonatan Bitton, Yonatan Shafir, Roopal Garg, Xi Chen, Dani Lischinski, Daniel Cohen-Or, and Idan Szpektor.

</span>
<span class="ltx_bibblock">Mismatch quest: Visual and textual feedback for image-text misalignment.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.03766</span>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao.

</span>
<span class="ltx_bibblock">Efficient multimodal learning from data-centric perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.11530</span>, 2024.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al.

</span>
<span class="ltx_bibblock">Language is not all you need: Aligning perception with language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv:2302.14045</span>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2014.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 31, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Information maximizing visual question generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 2008–2018, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">IJCV</span>, 2017.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Lisa: Reasoning segmentation via large language model.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.

</span>
<span class="ltx_bibblock">Obelics: An open web-scale filtered dataset of interleaved image-text documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</span>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.

</span>
<span class="ltx_bibblock">Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv:2301.12597</span>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Mini-gemini: Mining the potential of multi-modality vision language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.18814</span>, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">arXiv:2305.10355</span>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou.

</span>
<span class="ltx_bibblock">Visual question generation as dual task of visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 6116–6124, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Mengchen Liu, Chongyan Chen, and Danna Gurari.

</span>
<span class="ltx_bibblock">An evaluation of gpt-4v and gemini in online vqa, 2024.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al.

</span>
<span class="ltx_bibblock">Mmbench: Is your multi-modal model an all-around player?

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.06281</span>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al.

</span>
<span class="ltx_bibblock">Deepseek-vl: towards real-world vision-language understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.05525</span>, 2024.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi.

</span>
<span class="ltx_bibblock">Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 29, 2016.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">The Twelfth International Conference on Learning Representations</span>, 2024.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji.

</span>
<span class="ltx_bibblock">Cheap and quick: Efficient vision-language instruction tuning for large language models.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions about images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russakovsky.

</span>
<span class="ltx_bibblock">Point and ask: Incorporating pointing into visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.13681</span>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.

</span>
<span class="ltx_bibblock">Generation and comprehension of unambiguous object descriptions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.

</span>
<span class="ltx_bibblock">Docvqa: A dataset for vqa on document images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">WACV</span>, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">ICDAR</span>, 2019.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens Van Der Maaten.

</span>
<span class="ltx_bibblock">Learning by asking questions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 11–20, 2018.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al.

</span>
<span class="ltx_bibblock">Anymal: An efficient and scalable any-modality augmented language model.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.16058</span>, 2023.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Issey Masuda Mora, Santiago Pascual de la Puente, and X Giro-i Nieto.

</span>
<span class="ltx_bibblock">Towards automatic generation of question answer pairs from images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</span>, pages 1–2, 2016.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende.

</span>
<span class="ltx_bibblock">Generating natural questions about an image.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.06059</span>, 2016.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.

</span>
<span class="ltx_bibblock">Kosmos-2: Grounding multimodal large language models to the world.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv:2306.14824</span>, 2023.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, J. Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 123:74–93, 2015.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 2019.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S. Khan.

</span>
<span class="ltx_bibblock">Glamm: Pixel grounding large multimodal model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2024.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, and Balaji Lakshminarayanan.

</span>
<span class="ltx_bibblock">Self-evaluation improves selective generation in large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.09300</span>, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou.

</span>
<span class="ltx_bibblock">Timechat: A time-sensitive multimodal large language model for long video understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2024.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Azzurra Ruggeri and Tania Lombrozo.

</span>
<span class="ltx_bibblock">Learning by asking: How children ask questions to achieve efficient search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Proceedings of the Annual Meeting of the Cognitive Science Society</span>, volume 36, 2014.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Claude Sammut and Ranan B Banerji.

</span>
<span class="ltx_bibblock">Learning concepts by asking questions.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Machine learning: An artificial intelligence approach</span>, 2:167–192, 1986.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">A-okvqa: A benchmark for visual question answering using world knowledge.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">arXiv</span>, 2022.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano, and Patrick Gallinari.

</span>
<span class="ltx_bibblock">What bert sees: Cross-modal transfer for visual question generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.10832</span>, 2020.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.

</span>
<span class="ltx_bibblock">Cycle-consistency for robust visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 6649–6658, 2019.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
ShareGPT.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sharegpt.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sharegpt.com/</a>, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Kevin J Shih, Saurabh Singh, and Derek Hoiem.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 4613–4621, 2016.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2020.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2019.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang.

</span>
<span class="ltx_bibblock">Milebench: Benchmarking mllms in long context, 2024.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, and Can Huang.

</span>
<span class="ltx_bibblock">Textsquare: Scaling up text-centric visual instruction tuning, 2024.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Chameleon Team.

</span>
<span class="ltx_bibblock">Chameleon: Mixed-modal early-fusion foundation models, 2024.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.

</span>
<span class="ltx_bibblock">Eyes wide shut? exploring the visual shortcomings of multimodal llms.

</span>
<span class="ltx_bibblock">In <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2024.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Nihir Vedd, Zixu Wang, Marek Rei, Yishu Miao, and Lucia Specia.

</span>
<span class="ltx_bibblock">Guiding visual question generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2110.08226</span>, 2021.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.

</span>
<span class="ltx_bibblock">Cogvlm: Visual expert for pretrained language models.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al.

</span>
<span class="ltx_bibblock">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.11175</span>, 2023.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie.

</span>
<span class="ltx_bibblock">Self-evaluation guided beam search for reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Xing Xu, Tan Wang, Yang Yang, Alan Hanjalic, and Heng Tao Shen.

</span>
<span class="ltx_bibblock">Radial graph convolutional network for visual question generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>, 32(4):1654–1667, 2020.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">arXiv:2304.14178</span>, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality, 2023.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.

</span>
<span class="ltx_bibblock">Ferret: Refer and ground anything anywhere at any granularity.

</span>
<span class="ltx_bibblock">In <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">The Twelfth International Conference on Learning Representations</span>, 2024.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">ACL</span>, 2014.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Mm-vet: Evaluating large multimodal models for integrated capabilities.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.02490</span>, 2023.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock">In <span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">Proceedings of CVPR</span>, 2024.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">From recognition to cognition: Visual commonsense reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 6720–6731, 2019.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.15112</span>, 2023.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?

</span>
<span class="ltx_bibblock"><span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.14624</span>, 2024.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, and Jiawan Zhang.

</span>
<span class="ltx_bibblock">Automatic generation of grounded visual questions.

</span>
<span class="ltx_bibblock"><span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.06530</span>, 2016.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">arXiv:2205.01068</span>, 2022.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, and Navdeep Jaitly.

</span>
<span class="ltx_bibblock">How far are we from intelligent visual deductive reasoning?, 2024.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou.

</span>
<span class="ltx_bibblock">Genixer: Empowering multimodal large language models as a powerful data generator.

</span>
<span class="ltx_bibblock"><span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.06731</span>, 2023.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span>, 2015.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">arXiv:2304.10592</span>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Broader Impacts</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">In this paper, we propose a new training framework for imbuing two essential abilities into the model training. We also propose the EvalQA task with a new benchmark of 64,000 training data and 5,000 validation and testing data. In summary, this work would inspire future work to pay more attention to visual question asking and assessment. For these two tasks, there still exists some space for involving more GenQA tasks and more formulations of EvalQA.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Details of Implementation</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text ltx_font_bold">665K instruction tuning data.</span> We follow the backbone MLLM LLaVA1.5 to adopt the 665K instruction-following data into the supervise finetuning stage. We present the details of the 665K instruction data in Tab. <a href="#A2.T8" title="Table 8 ‣ Appendix B Additional Details of Implementation ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> for convenient browsing. The details include the dataset name, size, and instruction prompts.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.6" class="ltx_p"><span id="A2.p2.6.1" class="ltx_text ltx_font_bold">Model training. </span> The baseline model LLaVA1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> includes two stages: image-text alignment pertaining stage and supervised instruction tuning stage. The first stage involves only the image caption datasets for aligning two modalities by only finetuning the two-layer MLP adapter. In this paper, we are investigating the effectiveness of two additional tasks in the supervised instruction tuning stage. Thus, we use the pretraining weights of the first stage for fair comparison and train the model as in Fig. <a href="#A2.F4" title="Figure 4 ‣ Appendix B Additional Details of Implementation ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The model comprises three key components: Vision Encoder, MLP Adapter, and Large Language Model. The vision encoder is responsible for processing the input image <math id="A2.p2.1.m1.1" class="ltx_Math" alttext="X_{I}" display="inline"><semantics id="A2.p2.1.m1.1a"><msub id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml"><mi id="A2.p2.1.m1.1.1.2" xref="A2.p2.1.m1.1.1.2.cmml">X</mi><mi id="A2.p2.1.m1.1.1.3" xref="A2.p2.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><apply id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.p2.1.m1.1.1.1.cmml" xref="A2.p2.1.m1.1.1">subscript</csymbol><ci id="A2.p2.1.m1.1.1.2.cmml" xref="A2.p2.1.m1.1.1.2">𝑋</ci><ci id="A2.p2.1.m1.1.1.3.cmml" xref="A2.p2.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">X_{I}</annotation></semantics></math> to align the learned visual features with the text input <math id="A2.p2.2.m2.1" class="ltx_Math" alttext="X_{T}" display="inline"><semantics id="A2.p2.2.m2.1a"><msub id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml"><mi id="A2.p2.2.m2.1.1.2" xref="A2.p2.2.m2.1.1.2.cmml">X</mi><mi id="A2.p2.2.m2.1.1.3" xref="A2.p2.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><apply id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A2.p2.2.m2.1.1.1.cmml" xref="A2.p2.2.m2.1.1">subscript</csymbol><ci id="A2.p2.2.m2.1.1.2.cmml" xref="A2.p2.2.m2.1.1.2">𝑋</ci><ci id="A2.p2.2.m2.1.1.3.cmml" xref="A2.p2.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">X_{T}</annotation></semantics></math>. Next, the Multi-Layer Perceptron (MLP) adapter projects the visual feature <math id="A2.p2.3.m3.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="A2.p2.3.m3.1a"><msub id="A2.p2.3.m3.1.1" xref="A2.p2.3.m3.1.1.cmml"><mi id="A2.p2.3.m3.1.1.2" xref="A2.p2.3.m3.1.1.2.cmml">F</mi><mi id="A2.p2.3.m3.1.1.3" xref="A2.p2.3.m3.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p2.3.m3.1b"><apply id="A2.p2.3.m3.1.1.cmml" xref="A2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A2.p2.3.m3.1.1.1.cmml" xref="A2.p2.3.m3.1.1">subscript</csymbol><ci id="A2.p2.3.m3.1.1.2.cmml" xref="A2.p2.3.m3.1.1.2">𝐹</ci><ci id="A2.p2.3.m3.1.1.3.cmml" xref="A2.p2.3.m3.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.3.m3.1c">F_{I}</annotation></semantics></math> to <math id="A2.p2.4.m4.1" class="ltx_Math" alttext="Z_{I}" display="inline"><semantics id="A2.p2.4.m4.1a"><msub id="A2.p2.4.m4.1.1" xref="A2.p2.4.m4.1.1.cmml"><mi id="A2.p2.4.m4.1.1.2" xref="A2.p2.4.m4.1.1.2.cmml">Z</mi><mi id="A2.p2.4.m4.1.1.3" xref="A2.p2.4.m4.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p2.4.m4.1b"><apply id="A2.p2.4.m4.1.1.cmml" xref="A2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A2.p2.4.m4.1.1.1.cmml" xref="A2.p2.4.m4.1.1">subscript</csymbol><ci id="A2.p2.4.m4.1.1.2.cmml" xref="A2.p2.4.m4.1.1.2">𝑍</ci><ci id="A2.p2.4.m4.1.1.3.cmml" xref="A2.p2.4.m4.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.4.m4.1c">Z_{I}</annotation></semantics></math>. Finally, the large language model utilizes <math id="A2.p2.5.m5.1" class="ltx_Math" alttext="Z_{I}" display="inline"><semantics id="A2.p2.5.m5.1a"><msub id="A2.p2.5.m5.1.1" xref="A2.p2.5.m5.1.1.cmml"><mi id="A2.p2.5.m5.1.1.2" xref="A2.p2.5.m5.1.1.2.cmml">Z</mi><mi id="A2.p2.5.m5.1.1.3" xref="A2.p2.5.m5.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p2.5.m5.1b"><apply id="A2.p2.5.m5.1.1.cmml" xref="A2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A2.p2.5.m5.1.1.1.cmml" xref="A2.p2.5.m5.1.1">subscript</csymbol><ci id="A2.p2.5.m5.1.1.2.cmml" xref="A2.p2.5.m5.1.1.2">𝑍</ci><ci id="A2.p2.5.m5.1.1.3.cmml" xref="A2.p2.5.m5.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.5.m5.1c">Z_{I}</annotation></semantics></math> and the current text embedding <math id="A2.p2.6.m6.1" class="ltx_Math" alttext="Z_{T}" display="inline"><semantics id="A2.p2.6.m6.1a"><msub id="A2.p2.6.m6.1.1" xref="A2.p2.6.m6.1.1.cmml"><mi id="A2.p2.6.m6.1.1.2" xref="A2.p2.6.m6.1.1.2.cmml">Z</mi><mi id="A2.p2.6.m6.1.1.3" xref="A2.p2.6.m6.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p2.6.m6.1b"><apply id="A2.p2.6.m6.1.1.cmml" xref="A2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="A2.p2.6.m6.1.1.1.cmml" xref="A2.p2.6.m6.1.1">subscript</csymbol><ci id="A2.p2.6.m6.1.1.2.cmml" xref="A2.p2.6.m6.1.1.2">𝑍</ci><ci id="A2.p2.6.m6.1.1.3.cmml" xref="A2.p2.6.m6.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.6.m6.1c">Z_{T}</annotation></semantics></math> to predict the response in a left-to-right manner. During training, the data of the three tasks is mixed. By such joint training, MLLM exhibits deeper comprehension and promising reasoning ability.</p>
</div>
<figure id="A2.F4" class="ltx_figure"><img src="/html/2405.14974/assets/x4.png" id="A2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of the model training of LOVA<sup id="A2.F4.4.1" class="ltx_sup">3</sup>.</figcaption>
</figure>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.2" class="ltx_p"><span id="A2.p3.2.1" class="ltx_text ltx_font_bold">Hyperparameter.</span> The hyperparameters of LOVA<sup id="A2.p3.2.2" class="ltx_sup">3</sup> are aligned with those of LLaVA1.5 to ensure a fair comparison, as illustrated in Tab <a href="#A2.T9" title="Table 9 ‣ Appendix B Additional Details of Implementation ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The exceptional performance highlighted in Tab. 3, 4, and 5 of the main paper, achieved without any modulation of hyperparameters, demonstrates the effectiveness and robustness of our LOVA<sup id="A2.p3.2.3" class="ltx_sup">3</sup>.</p>
</div>
<figure id="A2.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>665K instruction data of LLaVA1.5. The content is from LLaVA1.5 for convenient browsing.</figcaption>
<div id="A2.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:283.5pt;height:174.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.8pt,37.4pt) scale(0.7,0.7) ;">
<table id="A2.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T8.1.1.1" class="ltx_tr">
<td id="A2.T8.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Dataset Name</span>
</span>
</td>
<td id="A2.T8.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 5.7pt;">Size</td>
<td id="A2.T8.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.1.3.1.1" class="ltx_p" style="width:256.1pt;">Instruction Prompts</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.2" class="ltx_tr">
<td id="A2.T8.1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.2.1.1.1" class="ltx_p" style="width:85.4pt;">LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">158K</td>
<td id="A2.T8.1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.2.3.1.1" class="ltx_p" style="width:256.1pt;">–</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.3" class="ltx_tr">
<td id="A2.T8.1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.3.1.1.1" class="ltx_p" style="width:85.4pt;">ShareGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">40K</td>
<td id="A2.T8.1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.3.3.1.1" class="ltx_p" style="width:256.1pt;">–</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.4" class="ltx_tr">
<td id="A2.T8.1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.4.1.1.1" class="ltx_p" style="width:85.4pt;">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">83K</td>
<td id="A2.T8.1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.4.3.1.1" class="ltx_p" style="width:256.1pt;">Answer the question using a single word or phrase.</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.5" class="ltx_tr">
<td id="A2.T8.1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.5.1.1.1" class="ltx_p" style="width:85.4pt;">GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">72K</td>
<td id="A2.T8.1.1.5.3" class="ltx_td ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
</tr>
<tr id="A2.T8.1.1.6" class="ltx_tr">
<td id="A2.T8.1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.6.1.1.1" class="ltx_p" style="width:85.4pt;">OKVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.6.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">9K</td>
<td id="A2.T8.1.1.6.3" class="ltx_td ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
</tr>
<tr id="A2.T8.1.1.7" class="ltx_tr">
<td id="A2.T8.1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.7.1.1.1" class="ltx_p" style="width:85.4pt;">OCRVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.7.2" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">80K</td>
<td id="A2.T8.1.1.7.3" class="ltx_td ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
</tr>
<tr id="A2.T8.1.1.8" class="ltx_tr">
<td id="A2.T8.1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.8.1.1.1" class="ltx_p" style="width:85.4pt;">A-OKVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">50K</td>
<td id="A2.T8.1.1.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.8.3.1.1" class="ltx_p" style="width:256.1pt;">Answer with the option’s letter from the given choices directly.</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.9" class="ltx_tr">
<td id="A2.T8.1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.9.1.1.1" class="ltx_p" style="width:85.4pt;">TextCaps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">22K</td>
<td id="A2.T8.1.1.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.9.3.1.1" class="ltx_p" style="width:256.1pt;">Provide a one-sentence caption for the provided image.</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.10" class="ltx_tr">
<td id="A2.T8.1.1.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.10.1.1.1" class="ltx_p" style="width:85.4pt;">RefCOCO<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">30K</td>
<td id="A2.T8.1.1.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.10.3.1.1" class="ltx_p" style="width:256.1pt;"><em id="A2.T8.1.1.10.3.1.1.1" class="ltx_emph ltx_font_italic">Note: randomly choose between the two formats</em></span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.11" class="ltx_tr">
<td id="A2.T8.1.1.11.1" class="ltx_td ltx_align_top" style="padding:0.5pt 5.7pt;"></td>
<td id="A2.T8.1.1.11.2" class="ltx_td ltx_border_r" style="padding:0.5pt 5.7pt;"></td>
<td id="A2.T8.1.1.11.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.11.3.1.1" class="ltx_p" style="width:256.1pt;">Provide a short description for this region.</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.12" class="ltx_tr">
<td id="A2.T8.1.1.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.12.1.1.1" class="ltx_p" style="width:85.4pt;">VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></span>
</span>
</td>
<td id="A2.T8.1.1.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">86K</td>
<td id="A2.T8.1.1.12.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.12.3.1.1" class="ltx_p" style="width:256.1pt;">Provide the bounding box coordinate of the region this sentence describes.</span>
</span>
</td>
</tr>
<tr id="A2.T8.1.1.13" class="ltx_tr">
<td id="A2.T8.1.1.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding:0.5pt 5.7pt;">
<span id="A2.T8.1.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T8.1.1.13.1.1.1" class="ltx_p" style="width:85.4pt;">Total</span>
</span>
</td>
<td id="A2.T8.1.1.13.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">665K</td>
<td id="A2.T8.1.1.13.3" class="ltx_td ltx_align_top ltx_border_bb ltx_border_t" style="padding:0.5pt 5.7pt;"></td>
</tr>
</table>
</span></div>
</figure>
<figure id="A2.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>
Hyperparameters of LOVA<sup id="A2.T9.4.1" class="ltx_sup">3</sup> are the same as the LLaVA1.5.
</figcaption>
<div id="A2.T9.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:161.0pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.1pt,16.2pt) scale(0.8,0.8) ;">
<table id="A2.T9.5.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.5.1.1" class="ltx_tr">
<td id="A2.T9.5.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 5.7pt;">Hyperparameter</td>
<td id="A2.T9.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 5.7pt;">Finetune</td>
</tr>
<tr id="A2.T9.5.1.2" class="ltx_tr">
<td id="A2.T9.5.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">batch size</td>
<td id="A2.T9.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 5.7pt;">128</td>
</tr>
<tr id="A2.T9.5.1.3" class="ltx_tr">
<td id="A2.T9.5.1.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">learning rate</td>
<td id="A2.T9.5.1.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">2e-5</td>
</tr>
<tr id="A2.T9.5.1.4" class="ltx_tr">
<td id="A2.T9.5.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">learning rate schedule</td>
<td id="A2.T9.5.1.4.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">cosine decay</td>
</tr>
<tr id="A2.T9.5.1.5" class="ltx_tr">
<td id="A2.T9.5.1.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">learning rate warmup ratio</td>
<td id="A2.T9.5.1.5.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">0.03</td>
</tr>
<tr id="A2.T9.5.1.6" class="ltx_tr">
<td id="A2.T9.5.1.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">weight decay</td>
<td id="A2.T9.5.1.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">0</td>
</tr>
<tr id="A2.T9.5.1.7" class="ltx_tr">
<td id="A2.T9.5.1.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">epoch</td>
<td id="A2.T9.5.1.7.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">1</td>
</tr>
<tr id="A2.T9.5.1.8" class="ltx_tr">
<td id="A2.T9.5.1.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">optimizer</td>
<td id="A2.T9.5.1.8.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">AdamW</td>
</tr>
<tr id="A2.T9.5.1.9" class="ltx_tr">
<td id="A2.T9.5.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 5.7pt;">DeepSpeed stage</td>
<td id="A2.T9.5.1.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 5.7pt;">3</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Details of GenQA Data</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.4" class="ltx_p"><span id="A3.p1.4.1" class="ltx_text ltx_font_bold">Generic VQA.</span>
It includes four datasets: VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, OCR-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and Counting110K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. The generic VQA data type we developed focuses on enabling the model to produce basic and general QA pairs. We incorporate VQAv2 and GQA, two fundamental VQA datasets for granting LOVA<sup id="A3.p1.4.2" class="ltx_sup">3</sup> the capability to learn how to ask questions like a human. Additionally, to increase the question diversity, we introduce two supplementary VQA tasks: counting VQA and long-response VQA.Counting110K<sup id="A3.p1.4.3" class="ltx_sup"><span id="A3.p1.4.3.1" class="ltx_text ltx_font_italic">†</span></sup>, a dataset developed in-house by reformulating the original PointQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> format, contributes to the counting type data generation. Moreover, for most of the above generic VQA with a short response, it is necessary for MLLM to learn to ask questions with long answers. Thus, we leverage the conversation subset of LLaVA-150K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and then filter out overly lengthy sentences (e.g., the word number <math id="A3.p1.3.m3.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="A3.p1.3.m3.1a"><mo id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><geq id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">\geq</annotation></semantics></math> 200.) to yield LLaVA-250K<sup id="A3.p1.4.4" class="ltx_sup"><span id="A3.p1.4.4.1" class="ltx_text ltx_font_italic">†</span></sup> with almost 250K samples.</p>
</div>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p"><span id="A3.p2.1.1" class="ltx_text ltx_font_bold">Multi-choice VQA.</span> Apart from generic VQA, there is another variant known as multi-choice VQA. This format has become increasingly popular in recent multimodal benchmarks, including ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, SEED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, MMBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and MMMU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. Such a data format can be seen as a better way to evaluate the reasoning ability of MLLMs rather than the direct text response in an open-ended way. As each MC VQA sample comprises one correct answer and three incorrect yet plausible alternatives, it will introduce a higher level of complexity for model learning. Thus, We proceed to make the LOVA<sup id="A3.p2.1.2" class="ltx_sup">3</sup> learn to produce multi-choice VQA data.</p>
</div>
<div id="A3.p3" class="ltx_para ltx_noindent">
<p id="A3.p3.1" class="ltx_p"><span id="A3.p3.1.1" class="ltx_text ltx_font_bold">Multi-turn VQA.</span>
It is also a complex multimodal data format. We incorporate this data type into the training of LOVA<sup id="A3.p3.1.2" class="ltx_sup">3</sup>, enabling it to master the art of generating varied questions within a dialogue context from a single image. Recognizing that the VQAv2 and GQA datasets offer multiple questions per image, we carefully select 83,000 and 72,000 multi-turn VQA instances from each dataset, respectively.</p>
</div>
<div id="A3.p4" class="ltx_para ltx_noindent">
<p id="A3.p4.1" class="ltx_p"><span id="A3.p4.1.1" class="ltx_text ltx_font_bold">REC and REG.</span> Recent studies such as Shikra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> have recognized the importance of making MLLM talk about the image regions for asking and answering (e.g., describing the region by giving a bounding box in an image) in grounding tasks. Being able to refer to a region precisely when asking or answering a question demonstrates a strong capability of multimodal reasoning. Possessing this capability would enhance an MLLM’s potential as an intelligent assistant in human-AI interactions by accurately identifying and referring regions of interest. Thus, besides considering the aforementioned multimodal tasks, we also consider involving grounding tasks like REC and REG to enhance the model capability related to positions. For REC, it aims to ground the region of an image with the given referring expression. About REG, the target is to give the corresponding expression when giving the exact coordinates. We randomly select 30K samples from RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, respectively.</p>
</div>
<div id="A3.p5" class="ltx_para ltx_noindent">
<p id="A3.p5.3" class="ltx_p"><span id="A3.p5.1.1" class="ltx_text ltx_font_bold">How about the asking ability of LOVA<sup id="A3.p5.1.1.1" class="ltx_sup"><span id="A3.p5.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup>?</span> We provide some examples of prompting LOVA<sup id="A3.p5.3.2" class="ltx_sup">3</sup> to generate VQA pairs as in Fig. <a href="#A4.F9" title="Figure 9 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. One can see that LOVA<sup id="A3.p5.3.3" class="ltx_sup">3</sup> is capable of asking versatile questions based on the content of the unlabeled images. Such results demonstrate the potential of the current MLLM to actively ask questions. We would like this finding to inspire future works that explore human-AI interaction in depth.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>EvalQABench</h2>

<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.1" class="ltx_p"><span id="A4.p1.1.1" class="ltx_text ltx_font_bold">Why Fuyu-8B and Llama 2?</span> To build the EvalQABench, we used two open-source models, Fuyu-8B and Llama 2, to generate negative answers and feedback, respectively. These models were chosen due to the zero financial cost of producing a training dataset. Furthermore, our empirical investigation found that Fuyu-8B and Llama 2 are capable of generating the data by following the instruction prompts described in Sec. <a href="#S3.SS2" title="3.2 Data Creation for EvalQA ‣ 3 Methodology ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Such results prove that GPT-4 is not necessary for our purpose.</p>
</div>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.1" class="ltx_p"><span id="A4.p2.1.1" class="ltx_text ltx_font_bold">Why does manual filtering and correction work?</span> The reason is that the models of Fuyu-8B and Llama 2 are two closed-formed models, which will output incorrect samples with similar patterns even set by the hyperparameters of inference mode. Therefore, we observe that use manual checking is feasible and enough to remove most of the failure cases. Moreover, GPT-4 is a closed-form model yet that exhibits error patterns.</p>
</div>
<div id="A4.p3" class="ltx_para ltx_noindent">
<p id="A4.p3.1" class="ltx_p"><span id="A4.p3.1.1" class="ltx_text ltx_font_bold">Verification of data.</span> As mentioned in the main paper, we select 100,000 samples from annotated VQA pairs of the VQAv2 training set and then use Fuyu-8B to generate negative answers for subsequent manual filtering and error correction. We obtain 61,094 filtered samples, which is approximately 61% accuracy in generating negative answers. After that, we prompt Llama 2 to produce feedback. In this process, we also manually filter the samples with incorrect formats and finally obtain 41,592 samples. It is almost 68% accuracy in creating feedback.</p>
</div>
<div id="A4.p4" class="ltx_para ltx_noindent">
<p id="A4.p4.1" class="ltx_p"><span id="A4.p4.1.1" class="ltx_text ltx_font_bold">Details of each procedure.</span> In detail, we present the amounts of each procedure in creating the EvalQABench training set in Tab. <a href="#A4.T10" title="Table 10 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Initially, we select 100,000 samples and then use Fuyu-8B to obtain 99,998 valid outcomes, and then we use manual filtering to remove 38,904 samples. We conduct error correction to 14,814 samples and then pass 61,094 samples to Llama 2 for feedback generation. After that, we adopt manual filtering to remove 19,502 samples with incorrect formats. The filtering of feedback includes overlength output or none of output.</p>
</div>
<figure id="A4.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>
Data amount details of creating EvalQABench training set.
</figcaption>
<div id="A4.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:219.5pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="A4.T10.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A4.T10.1.1.1" class="ltx_tr">
<td id="A4.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 5.7pt;">Procedures</td>
<td id="A4.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 5.7pt;">Amount</td>
</tr>
<tr id="A4.T10.1.1.2" class="ltx_tr">
<td id="A4.T10.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 5.7pt;">Raw Data</td>
<td id="A4.T10.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 5.7pt;">100,000</td>
</tr>
<tr id="A4.T10.1.1.3" class="ltx_tr">
<td id="A4.T10.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">Negative answer generation</td>
<td id="A4.T10.1.1.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">99,998</td>
</tr>
<tr id="A4.T10.1.1.4" class="ltx_tr">
<td id="A4.T10.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">Manual filtering</td>
<td id="A4.T10.1.1.4.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">61,094 (-38,904)</td>
</tr>
<tr id="A4.T10.1.1.5" class="ltx_tr">
<td id="A4.T10.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">Error Correction</td>
<td id="A4.T10.1.1.5.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">61,094 (14,814)</td>
</tr>
<tr id="A4.T10.1.1.6" class="ltx_tr">
<td id="A4.T10.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 5.7pt;">Feedback generation</td>
<td id="A4.T10.1.1.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 5.7pt;">61,094</td>
</tr>
<tr id="A4.T10.1.1.7" class="ltx_tr">
<td id="A4.T10.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 5.7pt;">Manual filtering</td>
<td id="A4.T10.1.1.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 5.7pt;">41,592 (-19,502)</td>
</tr>
</table>
</span></div>
</figure>
<div id="A4.p5" class="ltx_para ltx_noindent">
<p id="A4.p5.1" class="ltx_p"><span id="A4.p5.1.1" class="ltx_text ltx_font_bold">Data distribution across categories</span>
We provide the data distribution of the nine question types across the “Object”, “Yes/No”, “Counting”, “Color”, “Number”, “Attribute”, “Relation”, “Action”, and “Others” of the EvalQABench training set in Tab. <a href="#A4.T11" title="Table 11 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. One can observe that there are 26.7% question belongs to Others. It is noted that “Others” includes versatile questions such as “What does the image represent?”, “Who is not out of focus?”, “What does the back of the bus say?”, “What time does the clock report?”, and “How old is this man?”. These questions, with diverse scopes, bring diversity to our EvalQABench. Due to the inherent question bias in the original VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> training set, the number of questions categorized as "Number" is limited. Nevertheless, it is important to note that such biases are also present in real-world scenarios. We also provide the statics of the other seven question types in the table.</p>
</div>
<div id="A4.p6" class="ltx_para ltx_noindent">
<p id="A4.p6.1" class="ltx_p"><span id="A4.p6.1.1" class="ltx_text ltx_font_bold">Data distribution of negative answers.</span></p>
</div>
<div id="A4.p7" class="ltx_para ltx_noindent">
<p id="A4.p7.1" class="ltx_p">To analyze the data distribution of produced negative answers, we build a word cloud in Fig. <a href="#A4.F5" title="Figure 5 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. One can observe that “Yes” and “No” are the two majority negative answers due to the higher proportion of “Yes/No” questions. While colors and numbers are also the two high-frequency word types that appeared in the negative answers.</p>
</div>
<div id="A4.p8" class="ltx_para ltx_noindent">
<p id="A4.p8.1" class="ltx_p"><span id="A4.p8.1.1" class="ltx_text ltx_font_bold">Data distribution of feedback.</span></p>
</div>
<div id="A4.p9" class="ltx_para ltx_noindent">
<p id="A4.p9.1" class="ltx_p">For analyzing the distribution of feedback, we dive into three aspects: sentence length of feedback, noun counts, and verb counts as in Fig. <a href="#A4.F6" title="Figure 6 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#A4.F7" title="Figure 7 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <a href="#A4.F8" title="Figure 8 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="A4.T11" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Statistic of question types of EvalQABench training set.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="A4.T11.2" class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_top ltx_transformed_outer" style="width:216.8pt;height:198pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="A4.T11.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A4.T11.2.1.1" class="ltx_tr">
<td id="A4.T11.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Statistic</td>
<td id="A4.T11.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Number</td>
<td id="A4.T11.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Proportion</td>
</tr>
<tr id="A4.T11.2.1.2" class="ltx_tr">
<td id="A4.T11.2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Total Questions</td>
<td id="A4.T11.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">3,200</td>
<td id="A4.T11.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="A4.T11.2.1.3" class="ltx_tr">
<td id="A4.T11.2.1.3.1" class="ltx_td ltx_align_left">- Object</td>
<td id="A4.T11.2.1.3.2" class="ltx_td ltx_align_center">2,418</td>
<td id="A4.T11.2.1.3.3" class="ltx_td ltx_align_center">7.55%</td>
</tr>
<tr id="A4.T11.2.1.4" class="ltx_tr">
<td id="A4.T11.2.1.4.1" class="ltx_td ltx_align_left">- Yes/No</td>
<td id="A4.T11.2.1.4.2" class="ltx_td ltx_align_center">6,804</td>
<td id="A4.T11.2.1.4.3" class="ltx_td ltx_align_center">21.26%</td>
</tr>
<tr id="A4.T11.2.1.5" class="ltx_tr">
<td id="A4.T11.2.1.5.1" class="ltx_td ltx_align_left">- Counting</td>
<td id="A4.T11.2.1.5.2" class="ltx_td ltx_align_center">4,880</td>
<td id="A4.T11.2.1.5.3" class="ltx_td ltx_align_center">15.25%</td>
</tr>
<tr id="A4.T11.2.1.6" class="ltx_tr">
<td id="A4.T11.2.1.6.1" class="ltx_td ltx_align_left">- Color</td>
<td id="A4.T11.2.1.6.2" class="ltx_td ltx_align_center">3,756</td>
<td id="A4.T11.2.1.6.3" class="ltx_td ltx_align_center">11.73%</td>
</tr>
<tr id="A4.T11.2.1.7" class="ltx_tr">
<td id="A4.T11.2.1.7.1" class="ltx_td ltx_align_left">- Attribute</td>
<td id="A4.T11.2.1.7.2" class="ltx_td ltx_align_center">343</td>
<td id="A4.T11.2.1.7.3" class="ltx_td ltx_align_center">5.67%</td>
</tr>
<tr id="A4.T11.2.1.8" class="ltx_tr">
<td id="A4.T11.2.1.8.1" class="ltx_td ltx_align_left">- Number</td>
<td id="A4.T11.2.1.8.2" class="ltx_td ltx_align_center">1,814</td>
<td id="A4.T11.2.1.8.3" class="ltx_td ltx_align_center">1%</td>
</tr>
<tr id="A4.T11.2.1.9" class="ltx_tr">
<td id="A4.T11.2.1.9.1" class="ltx_td ltx_align_left">- Relation</td>
<td id="A4.T11.2.1.9.2" class="ltx_td ltx_align_center">2,380</td>
<td id="A4.T11.2.1.9.3" class="ltx_td ltx_align_center">7.44%</td>
</tr>
<tr id="A4.T11.2.1.10" class="ltx_tr">
<td id="A4.T11.2.1.10.1" class="ltx_td ltx_align_left">- Action</td>
<td id="A4.T11.2.1.10.2" class="ltx_td ltx_align_center">1,274</td>
<td id="A4.T11.2.1.10.3" class="ltx_td ltx_align_center">3.98%</td>
</tr>
<tr id="A4.T11.2.1.11" class="ltx_tr">
<td id="A4.T11.2.1.11.1" class="ltx_td ltx_align_left ltx_border_bb">- Other</td>
<td id="A4.T11.2.1.11.2" class="ltx_td ltx_align_center ltx_border_bb">8,331</td>
<td id="A4.T11.2.1.11.3" class="ltx_td ltx_align_center ltx_border_bb">26.03 %</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="A4.T11.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2405.14974/assets/x5.png" id="A4.T11.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="329" alt="[Uncaptioned image]">
</div>
</div>
</div>
</figure>
<figure id="A4.F5" class="ltx_figure"><img src="/html/2405.14974/assets/x6.png" id="A4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The word cloud of total negative answers.</figcaption>
</figure>
<figure id="A4.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Some failure cases of LOVA<sup id="A4.T12.7.1" class="ltx_sup">3</sup>.</figcaption>
<table id="A4.T12.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A4.T12.3.1" class="ltx_tr">
<td id="A4.T12.3.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">Image:</td>
<td id="A4.T12.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.1.1.1.1" class="ltx_p" style="width:284.5pt;"><img src="/html/2405.14974/assets/figures/textvqa_val1007.jpg" id="A4.T12.3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="265" height="189" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T12.5.4" class="ltx_tr">
<td id="A4.T12.5.4.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Question:</td>
<td id="A4.T12.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.4.2.1.1" class="ltx_p" style="width:284.5pt;">What is the time? \Reference OCR token: N, u, g0 \Answer the question using a single word or phrase.</span>
</span>
</td>
</tr>
<tr id="A4.T12.5.5" class="ltx_tr">
<td id="A4.T12.5.5.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Answer:</td>
<td id="A4.T12.5.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.5.2.1.1" class="ltx_p" style="width:284.5pt;">10:10</span>
</span>
</td>
</tr>
<tr id="A4.T12.4.2" class="ltx_tr">
<td id="A4.T12.4.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Image:</td>
<td id="A4.T12.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.2.1.1.1" class="ltx_p" style="width:284.5pt;"><img src="/html/2405.14974/assets/figures/v1_9.png" id="A4.T12.4.2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="142" height="189" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T12.5.6" class="ltx_tr">
<td id="A4.T12.5.6.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Question:</td>
<td id="A4.T12.5.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.6.2.1.1" class="ltx_p" style="width:284.5pt;">How many gallons of supreme gasoline can I get with $50?</span>
</span>
</td>
</tr>
<tr id="A4.T12.5.7" class="ltx_tr">
<td id="A4.T12.5.7.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Answer:</td>
<td id="A4.T12.5.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.7.2.1.1" class="ltx_p" style="width:284.5pt;">With $50, you can get approximately 110 gallons of supreme gasoline, as indicated by the gas station sign in the image.</span>
</span>
</td>
</tr>
<tr id="A4.T12.5.3" class="ltx_tr">
<td id="A4.T12.5.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Image:</td>
<td id="A4.T12.5.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.3.1.1.1" class="ltx_p" style="width:284.5pt;"><img src="/html/2405.14974/assets/figures/v1_48.jpg" id="A4.T12.5.3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="410" height="275" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T12.5.8" class="ltx_tr">
<td id="A4.T12.5.8.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Question:</td>
<td id="A4.T12.5.8.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.8.2.1.1" class="ltx_p" style="width:284.5pt;">In which direction does the living room window face?</span>
</span>
</td>
</tr>
<tr id="A4.T12.5.9" class="ltx_tr">
<td id="A4.T12.5.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">Answer:</td>
<td id="A4.T12.5.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="A4.T12.5.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.5.9.2.1.1" class="ltx_p" style="width:284.5pt;">The living room window faces the dining area.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.F6" class="ltx_figure"><img src="/html/2405.14974/assets/x7.png" id="A4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The distribution of the length of feedback.</figcaption>
</figure>
<figure id="A4.F7" class="ltx_figure"><img src="/html/2405.14974/assets/x8.png" id="A4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The top-30 nouns of feedback.</figcaption>
</figure>
<figure id="A4.F8" class="ltx_figure"><img src="/html/2405.14974/assets/x9.png" id="A4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The top-30 verbs of feedback.</figcaption>
</figure>
<figure id="A4.F9" class="ltx_figure"><img src="/html/2405.14974/assets/x10.png" id="A4.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="442" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>We ask the LOVA<sup id="A4.F9.4.1" class="ltx_sup">3</sup> to generate the diverse question-answer pairs.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Failure cases of EvalQABench</h2>

<div id="A5.p1" class="ltx_para ltx_noindent">
<p id="A5.p1.6" class="ltx_p">In this section, we show some failure cases of our LOVA<sup id="A5.p1.6.1" class="ltx_sup">3</sup>. As shown in Tab. <a href="#A4.T12" title="Table 12 ‣ Appendix D EvalQABench ‣ LOVA3: Learning to Visual Question Answering, Asking and Assessment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, there are three failure cases: the first case is from the TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, and the last two cases are from MM-Vet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. In the first case, when the watch is rotated -90<sup id="A5.p1.6.2" class="ltx_sup">∘</sup> and an incorrect OCR reference is provided, our LOVA<sup id="A5.p1.6.3" class="ltx_sup">3</sup> fails to give the correct time. The second example highlights the current limitations of LOVA<sup id="A5.p1.6.4" class="ltx_sup">3</sup> in mathematical calculations and multi-step reasoning. In the third example, LOVA<sup id="A5.p1.6.5" class="ltx_sup">3</sup> fails to correctly interpret the window of the living room, resulting in an incorrect answer. It is important to note that due to the limited text-centric instruction tuning datasets and mathematic relevant task-specific data in the current experimental settings, LOVA<sup id="A5.p1.6.6" class="ltx_sup">3</sup> falls short in handling text-centric VQA, and mathematic problem-solving. We believe these failure cases are primarily caused by the shortage of relevant instruction tuning datasets. This leaves room for future exploration, while our work focuses mainly on highlighting the importance and effectiveness of two additional high-level abilities for enhancing multimodal understanding in this paper. We believe that exploring the creation or collection of more mathematical or text-centric data for training will be essential for future work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.14973" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.14974" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.14974">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.14974" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.14975" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 17:32:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
