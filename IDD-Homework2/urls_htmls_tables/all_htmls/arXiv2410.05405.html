<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones</title>
<!--Generated on Mon Oct  7 17:46:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.05405v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S1" title="In SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S2" title="In SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S2.SS1" title="In II Related Works ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Visual SLAM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S2.SS2" title="In II Related Works ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Object-oriented SLAM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S2.SS3" title="In II Related Works ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">3D Reconstruction for UAV</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S2.SS4" title="In II Related Works ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Deblurring techniques in SLAM and object detection</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S3" title="In SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">System Overview: Hardware and Software</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S3.SS1" title="In III System Overview: Hardware and Software ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Object-oriented SLAM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S3.SS2" title="In III System Overview: Hardware and Software ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Deblurring with Neural Network</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4" title="In SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.SS1" title="In IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Dataset preparation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.SS2" title="In IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">DSP-SLAM parameters</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.SS3" title="In IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Monocular SLAM position and scale calibration</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.SS4" title="In IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Shape reconstruction validation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S5" title="In SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring 
<br class="ltx_break"/>for Agile Drones

<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Denis Davletshin,
Iana Zhura,
Vladislav Cheremnykh,
Mikhail Rybiyanov,
<br class="ltx_break"/>Aleksey Fedoseev, and
Dzmitry Tsetserukou
</span><span class="ltx_author_notes">The authors are with the Intelligent Space Robotics Laboratory, Center for Digital Engineering, Skolkovo Institute of Science and Technology (Skoltech), 121205 Bolshoy Boulevard 30, bld. 1, Moscow, Russia.  {denis.davletshin, iana.zhura, vladislav.cheremnykh, mikhail.rybiyanov, aleksey.fedoseev, d.tsetserukou}@skoltech.ru</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The paper focuses on the algorithm for improving the quality of 3D reconstruction and segmentation in DSP-SLAM by enhancing the RGB image quality. SharpSLAM algorithm developed by us aims to decrease the influence of high dynamic motion on visual object-oriented SLAM through image deblurring, improving all aspects of object-oriented SLAM, including localization, mapping, and object reconstruction.</p>
<p class="ltx_p" id="id2.id2">The experimental results revealed noticeable improvement in object detection quality, with F-score increased from 82.9% to 86.2% due to the higher number of features and corresponding map points. The RMSE of signed distance function has also decreased from 17.2 to 15.4 cm. Furthermore, our solution has enhanced object positioning, with an increase in the IoU from 74.5% to 75.7%. SharpSLAM algorithm has the potential to highly improve the quality of 3D reconstruction and segmentation in DSP-SLAM and to impact a wide range of fields, including robotics, autonomous vehicles, and augmented reality.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Among the increased functionality of multi-rotor unmanned aerial vehicles (UAVs), visual perception has become a valuable tool for solving the visual simultaneous localization and mapping (SLAM) problem and obtaining more information about the environment, such as object localization and reconstruction. Given the ability of SLAM to provide accurate and reliable estimates of a vehicle‚Äôs position and orientation, it has become a popular technique for multi-rotor vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib1" title="">1</a>]</cite>. In fact, Visual SLAM allows UAVs to create detailed maps of their surroundings, which can be used for a variety of tasks.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="535" id="S1.F1.g1" src="extracted/5908119/images/Main_pic.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Improvement in visual SLAM performance under the SharpSLAM with image deblurring algorithm.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Implementation of LiDARs has proven to be efficient in 3D mapping and localization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib3" title="">3</a>]</cite>. However, high cost, as well as sensitivity to reflective surfaces and weather precipitation make them unsuitable for a wide scope of indoor and outdoor applications. The implementation of light-weight and available RGB and RGB-D cameras is therefore highly demanded for a robot localization with visual SLAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib5" title="">5</a>]</cite>. However, even with the advances of the solutions based on NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib6" title="">6</a>]</cite>, cameras without depth information until recent years presented a high challenge for accurate mapping. In addition, visual feedback from the camera is often exposed to several drawbacks caused by bad lighting conditions or lack of textures on surfaces. Several algorithms were developed to overcome these challenges, e.g., DarkSLAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib7" title="">7</a>]</cite> for dim lighting and Pop-up SLAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib8" title="">8</a>]</cite> for textureless environments.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">One of the biggest challenges for RGB-based visual SLAM remains in the unpredictable blurriness of the images derived from the camera. While several control <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib9" title="">9</a>]</cite> and data processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib10" title="">10</a>]</cite> approaches were developed to compensate unwanted camera oscillation, the image blurriness still affects high-speed dynamic systems, such as UAVs. This issue especially affects the application of drones in 3D reconstruction of buildings and other large-scale objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib12" title="">12</a>]</cite> limiting the scope of applications for UAVs carrying RGB sensors.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address this issue, we propose a novel SharpSLAM algorithm that utilizes a generative adversarial networks (GAN) architecture to deblur low-quality image frames and improve the precision of the environment reconstruction. The main contribution of this work is the development and evaluation of the algorithm for both localization and reconstruction suitable for highly dynamic UAV systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Visual SLAM</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Visual SLAM is particularly useful for UAV tasks because of it is light-weight, cost-effective, and precise compared to other mapping techniques, such as Light Detection and Ranging (LiDAR).
Several algorithms were developed for visual SLAM, for example, ORB-SLAM2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib13" title="">13</a>]</cite> and most recent ORB-SLAM3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib14" title="">14</a>]</cite> achieving a high-precision result with a wide scope of monocular, stereo, and RGB-D cameras. However, the proposed algorithms still have challenges in the accuracy and completeness of 3D reconstruction, which can be caused by factors such as lighting conditions, camera calibration, and motion blur. This can lead to errors in the UAV‚Äôs perception of the environment, which can impact its ability to navigate and perform tasks effectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Object-oriented SLAM</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The simultaneous solution of both the SLAM and the deblurring problem is a useful approach for dynamic systems. One of the proposed methods to achieve it is to restore images using fast deconvolution and SLAM data. Additional features are then extracted and registered to the map, allowing SLAM to continue even with the blurred images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib15" title="">15</a>]</cite>. The aforementioned SLAM system has been validated on the TUM RGB-D dataset and used in mobile manipulators for applications involving object grasping with high accuracy for low-speed robotic arms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The inspiration of this work is the Object Oriented SLAM with Deep Shape Priors (DSP-SLAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib17" title="">17</a>]</cite>, which demonstrated full object reconstructions of high quality even from incomplete observations while maintaining a reliable global map. Evaluations of this algorithm also revealed decreased camera tracking drift and improvements in object pose and shape reconstruction compared to recent deep prior-based reconstruction techniques.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Recently, with the rise of deep learning techniques, significant improvements have been made in image quality enhancement, as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib18" title="">18</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib7" title="">7</a>]</cite>. Among these techniques, single motion deblurring based on deep learning approaches has been introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib19" title="">19</a>]</cite>. Although, it is worth noting that a more flexible and efficient approach to enhancing image quality is to utilize end-to-end GANs and the Feature Pyramid Network, which enable faster processing results, as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">It is clear that deblurring plays a crucial role in improving the accuracy of SLAM and 3D segmentation, and existing techniques have their limitations. Utilizing GANs for end-to-end image quality enhancement offers a promising avenue for future research in this area. Nevertheless, it is important to note that for UAV systems, it is not only important to accurately perform SLAM but also to accurately reconstruct 3D environments and segment objects within them. Therefore, it is crucial to analyze 3D segmentation after deblurring, as this can significantly impact the ability of UAV systems to define objects in complex environments.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">3D Reconstruction for UAV</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Obtaining 3D models of physical buildings and landscapes is often desirable for a better understanding of the robot‚Äôs surroundings. Recently, research has been conducted to explore the possibility of using 3D reconstruction and segmentation for UAV tasks. However, many of these techniques rely on LiDAR data, which can be sparse and expensive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib21" title="">21</a>]</cite>. In contrast, Visual SLAM is widely used for UAV perception tasks in complex environments and the attempts to tackle the limitations of Visual SLAM for UAV purposes have been done <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib22" title="">22</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib23" title="">23</a>]</cite>. Additionally, a novel concept of 3D reconstruction by heterogeneous swarm was proposed and is relying on visual data from on-board cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib24" title="">24</a>]</cite>. Nevertheless, the computational complexity and time consumption have been addressed.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">One of the primary challenges in visual SLAM is mitigating limitations caused by harsh environmental conditions and motion blur, which have been previously considered. Several works explore this issue, e.g., the quality assessment approach for 3D reconstruction, suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib25" title="">25</a>]</cite>. However, current techniques have not fully addressed the issue of 3D segmentation of areas of interest while simultaneously addressing the limitations of visual SLAM.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">Deblurring techniques in SLAM and object detection</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">To enhance the accuracy of SLAM and segmentation, a common approach is to preprocess the input images. In UAV systems, images are often blurred, which can result in a lack of interest points and inaccuracy of their. To address this issue, deblurring algorithms have been integrated into SLAM, showing superior performance compared to standard Visual SLAM baselines, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib26" title="">26</a>]</cite>. However, it is important to note that the impact of deblurring algorithms on 3D reconstruction, when combined with SLAM, remains largely unexplored.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">System Overview: Hardware and Software</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">SharpSLAM hardware has two components: a UAV that takes pictures and sends them to the stationary server, which is responsible for deblurring images and processing SLAM with object reconstruction (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S3.F2" title="Figure 2 ‚Ä£ III System Overview: Hardware and Software ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">2</span></a>). Once the server has finished, it sends the map and object data back to the drone. The drone is equipped with Raspberry Pi 4 with 8GB of RAM and a Raspberry Pi camera v2.1 (8 MP Sony IMX219 image sensor). We selected Pixracer R14 to serve as the flight controller for the UAV, because it provides all functionality to communicate via ROS protocols.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="660" id="S3.F2.g1" src="extracted/5908119/images/Hardware.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overview of SharpSLAM hardware architecture used for the experimental evaluation of the proposed algorithm. Arrows show direction of data exchange between modules.</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The Software consist of three blocks (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S3.F3" title="Figure 3 ‚Ä£ III System Overview: Hardware and Software ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">3</span></a>). The first block is the deblurring algorithm, which receives images from the drone and reduces the degree of motion blur. The second one is the visual SLAM based on ORB-SLAM2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib13" title="">13</a>]</cite>, which takes pictures after deblurring, provides a point cloud and takes part in robot localization. In parallel, images are also sent to the third block, which is responsible for object reconstruction. It uses 2D object segmentation from images and point cloud information to find the 3D shape and 6-DoF pose of the objects.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="729" id="S3.F3.g1" src="extracted/5908119/images/Software.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visual SLAM pipeline with SharpSLAM approach.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Object-oriented SLAM</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">DSP-SLAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib17" title="">17</a>]</cite> was chosen to serve as the object-oriented SLAM algorithm. This system applies 2D object segmentation on keyframes taken from ORB-SLAM 2, and provides map point association with objects. The process of shape reconstruction is iterative optimization. Each object is represented as a vector <math alttext="\mathbf{z}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">ùê≥</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ùê≥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">bold_z</annotation></semantics></math> that will be iteratively optimized. Map points are then processed with the DeepSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib27" title="">27</a>]</cite> neural network decoder that returns SDF for specific type of object, and it represents not fixed shape, but an entire class of shapes, such as hatchback, coupe, sedan etc. (if they were present in training datasets). DSP-SLAM algorithm utilize this decoder to optimize the following sequence:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E=\lambda_{s}E_{surf}+\lambda_{r}E_{rend}+\lambda_{c}||z||^{2}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><mi id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml">E</mi><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><mrow id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2.cmml"><msub id="S3.E1.m1.1.2.3.2.2" xref="S3.E1.m1.1.2.3.2.2.cmml"><mi id="S3.E1.m1.1.2.3.2.2.2" xref="S3.E1.m1.1.2.3.2.2.2.cmml">Œª</mi><mi id="S3.E1.m1.1.2.3.2.2.3" xref="S3.E1.m1.1.2.3.2.2.3.cmml">s</mi></msub><mo id="S3.E1.m1.1.2.3.2.1" xref="S3.E1.m1.1.2.3.2.1.cmml">‚Å¢</mo><msub id="S3.E1.m1.1.2.3.2.3" xref="S3.E1.m1.1.2.3.2.3.cmml"><mi id="S3.E1.m1.1.2.3.2.3.2" xref="S3.E1.m1.1.2.3.2.3.2.cmml">E</mi><mrow id="S3.E1.m1.1.2.3.2.3.3" xref="S3.E1.m1.1.2.3.2.3.3.cmml"><mi id="S3.E1.m1.1.2.3.2.3.3.2" xref="S3.E1.m1.1.2.3.2.3.3.2.cmml">s</mi><mo id="S3.E1.m1.1.2.3.2.3.3.1" xref="S3.E1.m1.1.2.3.2.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.1.2.3.2.3.3.3" xref="S3.E1.m1.1.2.3.2.3.3.3.cmml">u</mi><mo id="S3.E1.m1.1.2.3.2.3.3.1a" xref="S3.E1.m1.1.2.3.2.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.1.2.3.2.3.3.4" xref="S3.E1.m1.1.2.3.2.3.3.4.cmml">r</mi><mo id="S3.E1.m1.1.2.3.2.3.3.1b" xref="S3.E1.m1.1.2.3.2.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.1.2.3.2.3.3.5" xref="S3.E1.m1.1.2.3.2.3.3.5.cmml">f</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.2.3.3" xref="S3.E1.m1.1.2.3.3.cmml"><msub id="S3.E1.m1.1.2.3.3.2" xref="S3.E1.m1.1.2.3.3.2.cmml"><mi id="S3.E1.m1.1.2.3.3.2.2" xref="S3.E1.m1.1.2.3.3.2.2.cmml">Œª</mi><mi id="S3.E1.m1.1.2.3.3.2.3" xref="S3.E1.m1.1.2.3.3.2.3.cmml">r</mi></msub><mo id="S3.E1.m1.1.2.3.3.1" xref="S3.E1.m1.1.2.3.3.1.cmml">‚Å¢</mo><msub id="S3.E1.m1.1.2.3.3.3" xref="S3.E1.m1.1.2.3.3.3.cmml"><mi id="S3.E1.m1.1.2.3.3.3.2" xref="S3.E1.m1.1.2.3.3.3.2.cmml">E</mi><mrow id="S3.E1.m1.1.2.3.3.3.3" xref="S3.E1.m1.1.2.3.3.3.3.cmml"><mi id="S3.E1.m1.1.2.3.3.3.3.2" xref="S3.E1.m1.1.2.3.3.3.3.2.cmml">r</mi><mo id="S3.E1.m1.1.2.3.3.3.3.1" xref="S3.E1.m1.1.2.3.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.1.2.3.3.3.3.3" xref="S3.E1.m1.1.2.3.3.3.3.3.cmml">e</mi><mo id="S3.E1.m1.1.2.3.3.3.3.1a" xref="S3.E1.m1.1.2.3.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.1.2.3.3.3.3.4" xref="S3.E1.m1.1.2.3.3.3.3.4.cmml">n</mi><mo id="S3.E1.m1.1.2.3.3.3.3.1b" xref="S3.E1.m1.1.2.3.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1.m1.1.2.3.3.3.3.5" xref="S3.E1.m1.1.2.3.3.3.3.5.cmml">d</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.2.3.1a" xref="S3.E1.m1.1.2.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.2.3.4" xref="S3.E1.m1.1.2.3.4.cmml"><msub id="S3.E1.m1.1.2.3.4.2" xref="S3.E1.m1.1.2.3.4.2.cmml"><mi id="S3.E1.m1.1.2.3.4.2.2" xref="S3.E1.m1.1.2.3.4.2.2.cmml">Œª</mi><mi id="S3.E1.m1.1.2.3.4.2.3" xref="S3.E1.m1.1.2.3.4.2.3.cmml">c</mi></msub><mo id="S3.E1.m1.1.2.3.4.1" xref="S3.E1.m1.1.2.3.4.1.cmml">‚Å¢</mo><msup id="S3.E1.m1.1.2.3.4.3" xref="S3.E1.m1.1.2.3.4.3.cmml"><mrow id="S3.E1.m1.1.2.3.4.3.2.2" xref="S3.E1.m1.1.2.3.4.3.2.1.cmml"><mo id="S3.E1.m1.1.2.3.4.3.2.2.1" stretchy="false" xref="S3.E1.m1.1.2.3.4.3.2.1.1.cmml">‚Äñ</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">z</mi><mo id="S3.E1.m1.1.2.3.4.3.2.2.2" stretchy="false" xref="S3.E1.m1.1.2.3.4.3.2.1.1.cmml">‚Äñ</mo></mrow><mn id="S3.E1.m1.1.2.3.4.3.3" xref="S3.E1.m1.1.2.3.4.3.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><ci id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2">ùê∏</ci><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><plus id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></plus><apply id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2"><times id="S3.E1.m1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.2.3.2.1"></times><apply id="S3.E1.m1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.2.1.cmml" xref="S3.E1.m1.1.2.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.2.3.2.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2.2">ùúÜ</ci><ci id="S3.E1.m1.1.2.3.2.2.3.cmml" xref="S3.E1.m1.1.2.3.2.2.3">ùë†</ci></apply><apply id="S3.E1.m1.1.2.3.2.3.cmml" xref="S3.E1.m1.1.2.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.3.1.cmml" xref="S3.E1.m1.1.2.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.2.3.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2.3.2">ùê∏</ci><apply id="S3.E1.m1.1.2.3.2.3.3.cmml" xref="S3.E1.m1.1.2.3.2.3.3"><times id="S3.E1.m1.1.2.3.2.3.3.1.cmml" xref="S3.E1.m1.1.2.3.2.3.3.1"></times><ci id="S3.E1.m1.1.2.3.2.3.3.2.cmml" xref="S3.E1.m1.1.2.3.2.3.3.2">ùë†</ci><ci id="S3.E1.m1.1.2.3.2.3.3.3.cmml" xref="S3.E1.m1.1.2.3.2.3.3.3">ùë¢</ci><ci id="S3.E1.m1.1.2.3.2.3.3.4.cmml" xref="S3.E1.m1.1.2.3.2.3.3.4">ùëü</ci><ci id="S3.E1.m1.1.2.3.2.3.3.5.cmml" xref="S3.E1.m1.1.2.3.2.3.3.5">ùëì</ci></apply></apply></apply><apply id="S3.E1.m1.1.2.3.3.cmml" xref="S3.E1.m1.1.2.3.3"><times id="S3.E1.m1.1.2.3.3.1.cmml" xref="S3.E1.m1.1.2.3.3.1"></times><apply id="S3.E1.m1.1.2.3.3.2.cmml" xref="S3.E1.m1.1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.3.2.1.cmml" xref="S3.E1.m1.1.2.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.2.3.3.2.2.cmml" xref="S3.E1.m1.1.2.3.3.2.2">ùúÜ</ci><ci id="S3.E1.m1.1.2.3.3.2.3.cmml" xref="S3.E1.m1.1.2.3.3.2.3">ùëü</ci></apply><apply id="S3.E1.m1.1.2.3.3.3.cmml" xref="S3.E1.m1.1.2.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.3.3.1.cmml" xref="S3.E1.m1.1.2.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.2.3.3.3.2.cmml" xref="S3.E1.m1.1.2.3.3.3.2">ùê∏</ci><apply id="S3.E1.m1.1.2.3.3.3.3.cmml" xref="S3.E1.m1.1.2.3.3.3.3"><times id="S3.E1.m1.1.2.3.3.3.3.1.cmml" xref="S3.E1.m1.1.2.3.3.3.3.1"></times><ci id="S3.E1.m1.1.2.3.3.3.3.2.cmml" xref="S3.E1.m1.1.2.3.3.3.3.2">ùëü</ci><ci id="S3.E1.m1.1.2.3.3.3.3.3.cmml" xref="S3.E1.m1.1.2.3.3.3.3.3">ùëí</ci><ci id="S3.E1.m1.1.2.3.3.3.3.4.cmml" xref="S3.E1.m1.1.2.3.3.3.3.4">ùëõ</ci><ci id="S3.E1.m1.1.2.3.3.3.3.5.cmml" xref="S3.E1.m1.1.2.3.3.3.3.5">ùëë</ci></apply></apply></apply><apply id="S3.E1.m1.1.2.3.4.cmml" xref="S3.E1.m1.1.2.3.4"><times id="S3.E1.m1.1.2.3.4.1.cmml" xref="S3.E1.m1.1.2.3.4.1"></times><apply id="S3.E1.m1.1.2.3.4.2.cmml" xref="S3.E1.m1.1.2.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.4.2.1.cmml" xref="S3.E1.m1.1.2.3.4.2">subscript</csymbol><ci id="S3.E1.m1.1.2.3.4.2.2.cmml" xref="S3.E1.m1.1.2.3.4.2.2">ùúÜ</ci><ci id="S3.E1.m1.1.2.3.4.2.3.cmml" xref="S3.E1.m1.1.2.3.4.2.3">ùëê</ci></apply><apply id="S3.E1.m1.1.2.3.4.3.cmml" xref="S3.E1.m1.1.2.3.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.4.3.1.cmml" xref="S3.E1.m1.1.2.3.4.3">superscript</csymbol><apply id="S3.E1.m1.1.2.3.4.3.2.1.cmml" xref="S3.E1.m1.1.2.3.4.3.2.2"><csymbol cd="latexml" id="S3.E1.m1.1.2.3.4.3.2.1.1.cmml" xref="S3.E1.m1.1.2.3.4.3.2.2.1">norm</csymbol><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùëß</ci></apply><cn id="S3.E1.m1.1.2.3.4.3.3.cmml" type="integer" xref="S3.E1.m1.1.2.3.4.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">E=\lambda_{s}E_{surf}+\lambda_{r}E_{rend}+\lambda_{c}||z||^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_E = italic_Œª start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_s italic_u italic_r italic_f end_POSTSUBSCRIPT + italic_Œª start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_r italic_e italic_n italic_d end_POSTSUBSCRIPT + italic_Œª start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | | italic_z | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The first term takes into account a consistency between associated point cloud and reconstructed surface. The second term is a rendering loss, that enables to reconstruct object correctly even if a case of partially observed object. Also it this term is sensetive to growth of shape outside segmentation area. The third is a shape code regularization term. The output object poses are added to the optimization graph of the SLAM, so the optimization of object reconstruction affects the solution of the SLAM problem and vice versa.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Deblurring with Neural Network</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To remove motion blur from the UAV, we started with the DeblurGANv2 neural network. The Feature Pyramid Network is employed by this network. In our experiment, we made use of the Inception-ResNet-v2 architecture. MobileNet architecture should be used for real-time deblur applications as it has a faster inference time than ResNet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Dataset preparation</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The dataset for this project was collected in the parking garage of Skoltech University, featuring a Kia Soul car as the subject. The drone was flown in an elongated ring loop around the car by the drone operator, ensuring that the car was kept within the center of the camera frame. Each recording lasted for 3 minutes, recorded in a resolution of 1980x1280 with a frame rate of 15 frames per second.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">DSP-SLAM parameters</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">At the first iterations of SharpSLAM verification, we discovered that in case of cars with the shape that significantly differs from sedan configuration, DSP-SLAM may reconstruct its shape not correctly. Thus, in different scenarios of taking dataset we need to adjust DSP-SLAM algorithm. One of the most critical parameters was a number of keyframes for reconstruction initialization. The typical example of inaccurate reconstruction scenario is a start of car observation from the side. In that case if DSP-SLAM starts to find the shape too early (too small number of keyframes), the algorithm will have a point cloud only from one side, and these points probably not be enough. In the Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.F4" title="Figure 4 ‚Ä£ IV-B DSP-SLAM parameters ‚Ä£ IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">4</span></a>(b) was shown that early attempt of reconstruction causes wrong position and dimension estimation. At the Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.F4" title="Figure 4 ‚Ä£ IV-B DSP-SLAM parameters ‚Ä£ IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">4</span></a>(c), on the contrary, was shown that increased number of keyframes can improve the object pose estimation. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Empirically there was found an optimal parameter of 50 keyframes that is sufficient to observe the major part of the vehicle.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S4.F4.g1" src="extracted/5908119/images/dsp_adjust_3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Results of DSP-SLAM reconstruction for different number of keyframes waited before start reconstruction. (a) Initial camera position. (b) No. keyframes = 15, reconstruction was too early. (c) No. keyframes = 50, improved reconstruction after observing all sides of the car</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Monocular SLAM position and scale calibration</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In order to obtain quantitative results for object reconstruction and positioning, we need to have a map of our SLAM and ground truth in the same coordinate system. Therefore, we performed calibration of the scale and coordinate system with the help of a calibration chessboard Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.F5" title="Figure 5 ‚Ä£ IV-C Monocular SLAM position and scale calibration ‚Ä£ IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">5</span></a>. To get scale information, we find positions of 2 frames (P1 and P2) relative to the board, and relative to the SLAM coordinate system. After that we can simply get scale according to the following equation:</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s=\frac{\rho(P_{1cb},P_{2cb})}{\rho(P_{1cs},P_{2cs})}" class="ltx_Math" display="block" id="S4.E2.m1.4"><semantics id="S4.E2.m1.4a"><mrow id="S4.E2.m1.4.5" xref="S4.E2.m1.4.5.cmml"><mi id="S4.E2.m1.4.5.2" xref="S4.E2.m1.4.5.2.cmml">s</mi><mo id="S4.E2.m1.4.5.1" xref="S4.E2.m1.4.5.1.cmml">=</mo><mfrac id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml"><mrow id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml"><mi id="S4.E2.m1.2.2.2.4" xref="S4.E2.m1.2.2.2.4.cmml">œÅ</mi><mo id="S4.E2.m1.2.2.2.3" xref="S4.E2.m1.2.2.2.3.cmml">‚Å¢</mo><mrow id="S4.E2.m1.2.2.2.2.2" xref="S4.E2.m1.2.2.2.2.3.cmml"><mo id="S4.E2.m1.2.2.2.2.2.3" stretchy="false" xref="S4.E2.m1.2.2.2.2.3.cmml">(</mo><msub id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml">P</mi><mrow id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml"><mn id="S4.E2.m1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.cmml">1</mn><mo id="S4.E2.m1.1.1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.3.3.cmml">c</mi><mo id="S4.E2.m1.1.1.1.1.1.1.3.1a" xref="S4.E2.m1.1.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.1.1.1.1.1.1.3.4" xref="S4.E2.m1.1.1.1.1.1.1.3.4.cmml">b</mi></mrow></msub><mo id="S4.E2.m1.2.2.2.2.2.4" xref="S4.E2.m1.2.2.2.2.3.cmml">,</mo><msub id="S4.E2.m1.2.2.2.2.2.2" xref="S4.E2.m1.2.2.2.2.2.2.cmml"><mi id="S4.E2.m1.2.2.2.2.2.2.2" xref="S4.E2.m1.2.2.2.2.2.2.2.cmml">P</mi><mrow id="S4.E2.m1.2.2.2.2.2.2.3" xref="S4.E2.m1.2.2.2.2.2.2.3.cmml"><mn id="S4.E2.m1.2.2.2.2.2.2.3.2" xref="S4.E2.m1.2.2.2.2.2.2.3.2.cmml">2</mn><mo id="S4.E2.m1.2.2.2.2.2.2.3.1" xref="S4.E2.m1.2.2.2.2.2.2.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.2.2.2.2.2.2.3.3" xref="S4.E2.m1.2.2.2.2.2.2.3.3.cmml">c</mi><mo id="S4.E2.m1.2.2.2.2.2.2.3.1a" xref="S4.E2.m1.2.2.2.2.2.2.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.2.2.2.2.2.2.3.4" xref="S4.E2.m1.2.2.2.2.2.2.3.4.cmml">b</mi></mrow></msub><mo id="S4.E2.m1.2.2.2.2.2.5" stretchy="false" xref="S4.E2.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S4.E2.m1.4.4.4" xref="S4.E2.m1.4.4.4.cmml"><mi id="S4.E2.m1.4.4.4.4" xref="S4.E2.m1.4.4.4.4.cmml">œÅ</mi><mo id="S4.E2.m1.4.4.4.3" xref="S4.E2.m1.4.4.4.3.cmml">‚Å¢</mo><mrow id="S4.E2.m1.4.4.4.2.2" xref="S4.E2.m1.4.4.4.2.3.cmml"><mo id="S4.E2.m1.4.4.4.2.2.3" stretchy="false" xref="S4.E2.m1.4.4.4.2.3.cmml">(</mo><msub id="S4.E2.m1.3.3.3.1.1.1" xref="S4.E2.m1.3.3.3.1.1.1.cmml"><mi id="S4.E2.m1.3.3.3.1.1.1.2" xref="S4.E2.m1.3.3.3.1.1.1.2.cmml">P</mi><mrow id="S4.E2.m1.3.3.3.1.1.1.3" xref="S4.E2.m1.3.3.3.1.1.1.3.cmml"><mn id="S4.E2.m1.3.3.3.1.1.1.3.2" xref="S4.E2.m1.3.3.3.1.1.1.3.2.cmml">1</mn><mo id="S4.E2.m1.3.3.3.1.1.1.3.1" xref="S4.E2.m1.3.3.3.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.3.3.3.1.1.1.3.3" xref="S4.E2.m1.3.3.3.1.1.1.3.3.cmml">c</mi><mo id="S4.E2.m1.3.3.3.1.1.1.3.1a" xref="S4.E2.m1.3.3.3.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.3.3.3.1.1.1.3.4" xref="S4.E2.m1.3.3.3.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S4.E2.m1.4.4.4.2.2.4" xref="S4.E2.m1.4.4.4.2.3.cmml">,</mo><msub id="S4.E2.m1.4.4.4.2.2.2" xref="S4.E2.m1.4.4.4.2.2.2.cmml"><mi id="S4.E2.m1.4.4.4.2.2.2.2" xref="S4.E2.m1.4.4.4.2.2.2.2.cmml">P</mi><mrow id="S4.E2.m1.4.4.4.2.2.2.3" xref="S4.E2.m1.4.4.4.2.2.2.3.cmml"><mn id="S4.E2.m1.4.4.4.2.2.2.3.2" xref="S4.E2.m1.4.4.4.2.2.2.3.2.cmml">2</mn><mo id="S4.E2.m1.4.4.4.2.2.2.3.1" xref="S4.E2.m1.4.4.4.2.2.2.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.4.4.4.2.2.2.3.3" xref="S4.E2.m1.4.4.4.2.2.2.3.3.cmml">c</mi><mo id="S4.E2.m1.4.4.4.2.2.2.3.1a" xref="S4.E2.m1.4.4.4.2.2.2.3.1.cmml">‚Å¢</mo><mi id="S4.E2.m1.4.4.4.2.2.2.3.4" xref="S4.E2.m1.4.4.4.2.2.2.3.4.cmml">s</mi></mrow></msub><mo id="S4.E2.m1.4.4.4.2.2.5" stretchy="false" xref="S4.E2.m1.4.4.4.2.3.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.4b"><apply id="S4.E2.m1.4.5.cmml" xref="S4.E2.m1.4.5"><eq id="S4.E2.m1.4.5.1.cmml" xref="S4.E2.m1.4.5.1"></eq><ci id="S4.E2.m1.4.5.2.cmml" xref="S4.E2.m1.4.5.2">ùë†</ci><apply id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4"><divide id="S4.E2.m1.4.4.5.cmml" xref="S4.E2.m1.4.4"></divide><apply id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"><times id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.3"></times><ci id="S4.E2.m1.2.2.2.4.cmml" xref="S4.E2.m1.2.2.2.4">ùúå</ci><interval closure="open" id="S4.E2.m1.2.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.2"><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2">ùëÉ</ci><apply id="S4.E2.m1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3"><times id="S4.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.1"></times><cn id="S4.E2.m1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.1.1.3.2">1</cn><ci id="S4.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.3">ùëê</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.4.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.4">ùëè</ci></apply></apply><apply id="S4.E2.m1.2.2.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.2.2.1.cmml" xref="S4.E2.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.2.2.2">ùëÉ</ci><apply id="S4.E2.m1.2.2.2.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.2.2.3"><times id="S4.E2.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E2.m1.2.2.2.2.2.2.3.1"></times><cn id="S4.E2.m1.2.2.2.2.2.2.3.2.cmml" type="integer" xref="S4.E2.m1.2.2.2.2.2.2.3.2">2</cn><ci id="S4.E2.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E2.m1.2.2.2.2.2.2.3.3">ùëê</ci><ci id="S4.E2.m1.2.2.2.2.2.2.3.4.cmml" xref="S4.E2.m1.2.2.2.2.2.2.3.4">ùëè</ci></apply></apply></interval></apply><apply id="S4.E2.m1.4.4.4.cmml" xref="S4.E2.m1.4.4.4"><times id="S4.E2.m1.4.4.4.3.cmml" xref="S4.E2.m1.4.4.4.3"></times><ci id="S4.E2.m1.4.4.4.4.cmml" xref="S4.E2.m1.4.4.4.4">ùúå</ci><interval closure="open" id="S4.E2.m1.4.4.4.2.3.cmml" xref="S4.E2.m1.4.4.4.2.2"><apply id="S4.E2.m1.3.3.3.1.1.1.cmml" xref="S4.E2.m1.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.3.1.1.1.1.cmml" xref="S4.E2.m1.3.3.3.1.1.1">subscript</csymbol><ci id="S4.E2.m1.3.3.3.1.1.1.2.cmml" xref="S4.E2.m1.3.3.3.1.1.1.2">ùëÉ</ci><apply id="S4.E2.m1.3.3.3.1.1.1.3.cmml" xref="S4.E2.m1.3.3.3.1.1.1.3"><times id="S4.E2.m1.3.3.3.1.1.1.3.1.cmml" xref="S4.E2.m1.3.3.3.1.1.1.3.1"></times><cn id="S4.E2.m1.3.3.3.1.1.1.3.2.cmml" type="integer" xref="S4.E2.m1.3.3.3.1.1.1.3.2">1</cn><ci id="S4.E2.m1.3.3.3.1.1.1.3.3.cmml" xref="S4.E2.m1.3.3.3.1.1.1.3.3">ùëê</ci><ci id="S4.E2.m1.3.3.3.1.1.1.3.4.cmml" xref="S4.E2.m1.3.3.3.1.1.1.3.4">ùë†</ci></apply></apply><apply id="S4.E2.m1.4.4.4.2.2.2.cmml" xref="S4.E2.m1.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.4.4.4.2.2.2.1.cmml" xref="S4.E2.m1.4.4.4.2.2.2">subscript</csymbol><ci id="S4.E2.m1.4.4.4.2.2.2.2.cmml" xref="S4.E2.m1.4.4.4.2.2.2.2">ùëÉ</ci><apply id="S4.E2.m1.4.4.4.2.2.2.3.cmml" xref="S4.E2.m1.4.4.4.2.2.2.3"><times id="S4.E2.m1.4.4.4.2.2.2.3.1.cmml" xref="S4.E2.m1.4.4.4.2.2.2.3.1"></times><cn id="S4.E2.m1.4.4.4.2.2.2.3.2.cmml" type="integer" xref="S4.E2.m1.4.4.4.2.2.2.3.2">2</cn><ci id="S4.E2.m1.4.4.4.2.2.2.3.3.cmml" xref="S4.E2.m1.4.4.4.2.2.2.3.3">ùëê</ci><ci id="S4.E2.m1.4.4.4.2.2.2.3.4.cmml" xref="S4.E2.m1.4.4.4.2.2.2.3.4">ùë†</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.4c">s=\frac{\rho(P_{1cb},P_{2cb})}{\rho(P_{1cs},P_{2cs})}</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.4d">italic_s = divide start_ARG italic_œÅ ( italic_P start_POSTSUBSCRIPT 1 italic_c italic_b end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 italic_c italic_b end_POSTSUBSCRIPT ) end_ARG start_ARG italic_œÅ ( italic_P start_POSTSUBSCRIPT 1 italic_c italic_s end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 italic_c italic_s end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.3">where <math alttext="\rho" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mi id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">œÅ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">ùúå</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">italic_œÅ</annotation></semantics></math> is the Euclidean distance between 3D points, <math alttext="P_{cb}" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2.1"><semantics id="S4.SS3.p3.2.m2.1a"><msub id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">P</mi><mrow id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml"><mi id="S4.SS3.p3.2.m2.1.1.3.2" xref="S4.SS3.p3.2.m2.1.1.3.2.cmml">c</mi><mo id="S4.SS3.p3.2.m2.1.1.3.1" xref="S4.SS3.p3.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS3.p3.2.m2.1.1.3.3" xref="S4.SS3.p3.2.m2.1.1.3.3.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">ùëÉ</ci><apply id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3"><times id="S4.SS3.p3.2.m2.1.1.3.1.cmml" xref="S4.SS3.p3.2.m2.1.1.3.1"></times><ci id="S4.SS3.p3.2.m2.1.1.3.2.cmml" xref="S4.SS3.p3.2.m2.1.1.3.2">ùëê</ci><ci id="S4.SS3.p3.2.m2.1.1.3.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3.3">ùëè</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">P_{cb}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_c italic_b end_POSTSUBSCRIPT</annotation></semantics></math> is the position of camera with respect to chessboard, and <math alttext="P_{cs}" class="ltx_Math" display="inline" id="S4.SS3.p3.3.m3.1"><semantics id="S4.SS3.p3.3.m3.1a"><msub id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mi id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">P</mi><mrow id="S4.SS3.p3.3.m3.1.1.3" xref="S4.SS3.p3.3.m3.1.1.3.cmml"><mi id="S4.SS3.p3.3.m3.1.1.3.2" xref="S4.SS3.p3.3.m3.1.1.3.2.cmml">c</mi><mo id="S4.SS3.p3.3.m3.1.1.3.1" xref="S4.SS3.p3.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS3.p3.3.m3.1.1.3.3" xref="S4.SS3.p3.3.m3.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">ùëÉ</ci><apply id="S4.SS3.p3.3.m3.1.1.3.cmml" xref="S4.SS3.p3.3.m3.1.1.3"><times id="S4.SS3.p3.3.m3.1.1.3.1.cmml" xref="S4.SS3.p3.3.m3.1.1.3.1"></times><ci id="S4.SS3.p3.3.m3.1.1.3.2.cmml" xref="S4.SS3.p3.3.m3.1.1.3.2">ùëê</ci><ci id="S4.SS3.p3.3.m3.1.1.3.3.cmml" xref="S4.SS3.p3.3.m3.1.1.3.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">P_{cs}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.3.m3.1d">italic_P start_POSTSUBSCRIPT italic_c italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is the position of camera in SLAM coordinate system.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="602" id="S4.F5.g1" src="extracted/5908119/images/calibration_scheme.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Schematic explanation of SLAM scale and position calibration.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">After scale calibration we perform coordinate system calibration. We detected camera position relative to the board at the frame <math alttext="P_{1}" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><msub id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mi id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">P</mi><mn id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">ùëÉ</ci><cn id="S4.SS3.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">P_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> by solving Perspective-n-Point problem with OpenCV library. The position of chessboard relative to the world is known, therefore we can obtain transformation between SLAM and world coordinate systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Shape reconstruction validation</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To estimate the quality of the 3D segmented model obtained from the SharpSLAM,
reconstructed 3D models was compared with LiDAR point cloud as a ground truth car model. The intersection over union (IoU) of 3D bounding boxes and root mean square error (RMSE) of the signed distance function (SDF) over all predicted mesh points have been calculated. According to the Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.T1" title="TABLE I ‚Ä£ IV-D Shape reconstruction validation ‚Ä£ IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">I</span></a>, our approach improves the quality of object reconstruction. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05405v1#S4.F6" title="Figure 6 ‚Ä£ IV-D Shape reconstruction validation ‚Ä£ IV Experiments ‚Ä£ SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones"><span class="ltx_text ltx_ref_tag">6</span></a> it can be seen that the roof of mesh car made with Sharp-SLAM is closer to the ground truth than the same one from DSP-SLAM.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="166" id="S4.F6.g1" src="extracted/5908119/images/mesh_comparison2.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison of qualitative results relative to the ground truth.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Geometry Metrics of Reconstructed Objects.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1">Dataset</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4">F-score</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T1.1.2.2.1">Blurred</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.2.2.2">0.971</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.2.2.3">0.723</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.4.1">0.829</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.3.1">Deblurred</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.2">0.987</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.3">0.764</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.3.4.1">0.862</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T1.1.4.4.1">Dataset</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.4.4.2">No. of points</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.4.4.3">RMSE SDF, m</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.4.4.4">IoU, %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.1">Blurred</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.2">2337</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.3">0.172</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.5.4.1">74.51</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.6.6.1">Deblurred</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.6.6.2">3236</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.6.6.3">0.154</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.6.4.1">75.67</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The experiment showed a 38.46% increase in the number of points associated with the car after the deblurring process. This improvement was mainly due to reduced tracking loss in cases of blurred images. The higher number of map points also enhanced the accuracy of the initial position estimate, leading to more reliable 3D reconstruction. Additionally, the recall for car segmentation improved by 4.1%, and the F-score increased by 3.3% as a result of these enhancements. It is important to note that SLAM performed unpredictably when processing datasets with blurry images, often losing track of the camera‚Äôs trajectory and interrupting the mapping process. However, after image restoration using neural networks, these issues were no longer present in the datasets with enhanced images.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion and Future Work</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work a new visual SLAM algorithm was proposed for UAV localization and object reconstruction. The proposed approach is based on DSP-SLAM with image enhancement achieved using the DeblurGAN-V2 neural network with the MobileNet architecture, allowing UAVs to perform real-time image restoration based on RGB cameras only.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">An experiment was carried out to evaluate the improvement of the SLAM algorithm. The results revealed that, with the number of detected points corresponding to the car increased by 38,46%, the IoU showed an increase from 74.51% to 75.67%. The RMSE of the signed distance function for the trajectory decreased from 17.2 cm to 15.4 cm. The obtained results suggested that the proposed SharpSLAM approach may potentially increase the precision of object reconstruction for UAVs and other autonomous systems with a high-speed motion. In the future we are planning to optimize the SharpSLAM reconstruction pipeline to improve the initial position estimation and correction of object estimation while receiving new data. Also we will conduct additional experiments with a multi-agent SLAM system running on several UAVs as they potentially enhance coverage of observing area and hence reconstruction quality. Furthermore, different reconstruction strategies will be developed and evaluated with several objects of different size.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Research reported in this publication was financially supported by the RSF grant No. 24-41-02039.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D.¬†Sharafutdinov, M.¬†Griguletskii, P.¬†Kopanev, M.¬†Kurenkov, G.¬†Ferrer, A.¬†Burkov, A.¬†Gonnochenko, and D.¬†Tsetserukou, ‚ÄúComparison of modern open-source visual slam approaches,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Journal of Intelligent &amp; Robotic Systems</em>, vol. 107, no.¬†3, p.¬†43, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
X.¬†Niu, C.¬†Zhang, S.¬†Fu, and W.¬†Zhang, ‚ÄúResearch on the development of 3d laser slam technology,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proc. 2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI)</em>, 2021, pp. 181‚Äì185.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.¬†Hensel, M.¬†B.¬†Marinov, and M.¬†Obert, ‚Äú3d lidar based slam system evaluation with low-cost real-time kinematics gps solution,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Computation</em>, vol.¬†10, no.¬†9, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
I.¬†Abaspur¬†Kazerouni, L.¬†Fitzgerald, G.¬†Dooly, and D.¬†Toal, ‚ÄúA survey of state-of-the-art on visual slam,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Expert Syst. Appl.</em>, vol. 205, no.¬†C, Nov. 2022. [Online]. Available: https://doi.org/10.1016/j.eswa.2022.117734

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
E.¬†Kruzhkov, A.¬†Savinykh, P.¬†Karpyshev, M.¬†Kurenkov, E.¬†Yudin, A.¬†Potapov, and D.¬†Tsetserukou, ‚ÄúMeslam: Memory efficient slam based on neural fields,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proc. 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 2022, pp. 430‚Äì435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.¬†Nenashev, M.¬†Kurenkov, A.¬†Potapov, I.¬†Zhura, M.¬†Katerishich, and D.¬†Tsetserukou, ‚ÄúLoconerf: A nerf-based approach for local structure from motion for precise localization,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proc. 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 2023, pp. 641‚Äì646.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A.¬†Savinykh, M.¬†Kurenkov, E.¬†Kruzhkov, E.¬†Yudin, A.¬†Potapov, P.¬†Karpyshev, and D.¬†Tsetserukou, ‚ÄúDarkslam: Gan-assisted visual slam for reliable operation in low-light conditions,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proc. 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)</em>, 2022, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S.¬†Yang, Y.¬†Song, M.¬†Kaess, and S.¬†Scherer, ‚ÄúPop-up slam: Semantic monocular plane slam for low-texture environments,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proc. 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2016, pp. 1222‚Äì1229.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.¬†Li, M.¬†Zhong, and Y.¬†Zhao, ‚ÄúEstimation and compensation of unknown disturbance in three-axis gyro-stabilized camera mount,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Institute of Measurement and Control</em>, vol.¬†37, no.¬†6, pp. 732‚Äì745, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S.-H. Yeh, D.¬†Wang, W.¬†Yan, and D.¬†Song, ‚ÄúDetection of camera model inconsistency and the existence of optical image stabilization system,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proc. 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</em>, 2022, pp. 1358‚Äì1363.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A.¬†Filatov, M.¬†Zaslavskiy, and K.¬†Krinkin, ‚ÄúMulti-drone 3d building reconstruction method,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Mathematics</em>, vol.¬†9, no.¬†23, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.¬†Noda, Y.¬†Harazono, K.¬†Ueda, H.¬†Ishii, and H.¬†Shimoda, ‚ÄúA study on 3d reconstruction method in cooperation with a mirror-mounted autonomous drone,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proc. 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 2022, pp. 305‚Äì310.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R.¬†Mur-Artal and J.¬†D. Tard√≥s, ‚ÄúOrb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Robotics</em>, vol.¬†33, no.¬†5, pp. 1255‚Äì1262, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C.¬†Campos, R.¬†Elvira, J.¬†J.¬†G. Rodr√≠guez, J.¬†M. M.¬†Montiel, and J.¬†D.¬†Tard√≥s, ‚ÄúOrb-slam3: An accurate open-source library for visual, visual‚Äìinertial, and multimap slam,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Transactions on Robotics</em>, vol.¬†37, no.¬†6, pp. 1874‚Äì1890, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H.¬†S. Lee, J.¬†Kwon, and K.¬†M. Lee, ‚ÄúSimultaneous localization, mapping and deblurring,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proc. 2011 International Conference on Computer Vision</em>, 2011, pp. 1203‚Äì1210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J.¬†Peng, X.¬†Shi, J.¬†Wu, and Z.¬†Xiong, ‚ÄúAn object-oriented semantic slam system towards dynamic environments for mobile manipulation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proc. 2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)</em>, 2019, pp. 199‚Äì204.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.¬†Wang, M.¬†R√ºnz, and L.¬†Agapito, ‚ÄúDsp-slam: Object oriented slam with deep shape priors,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proc. 2021 International Conference on 3D Vision (3DV)</em>, 2021, pp. 1362‚Äì1371.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X.¬†Yuan, ‚ÄúDynamic scene deblurring using deep learning,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proc. 2022 IEEE 5th International Conference on Information Systems and Computer Aided Education (ICISCAE)</em>, 2022, pp. 1009‚Äì1015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J.¬†Chen, Y.¬†Shen, Q.¬†Zhu, Q.¬†Jiang, O.¬†Xie, and J.¬†Miao, ‚ÄúMotion blur processing method for visual slam system based on local residual blur discrimination network,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Journal of Mechanical Science and Technology</em>, vol.¬†36, no.¬†7, pp. 3653‚Äì3666, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O.¬†Kupyn, T.¬†Martyniuk, J.¬†Wu, and Z.¬†Wang, ‚ÄúDeblurgan-v2: Deblurring (orders-of-magnitude) faster and better,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</em>, October 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A.¬†To, M.¬†Liu, M.¬†Hazeeq Bin Muhammad¬†Hairul, J.¬†G. Davis, J.¬†S. Lee, H.¬†Hesse, and H.¬†D. Nguyen, ‚ÄúDrone-based ai and 3d reconstruction for digital twin augmentation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proc. Social Computing and Social Media: Experience Design and Social Network Analysis: 13th International Conference, SCSM 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24‚Äì29, 2021, Proceedings, Part I</em>.¬†¬†¬†Springer, 2021, pp. 511‚Äì529.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F.¬†Huang, H.¬†Yang, X.¬†Tan, S.¬†Peng, J.¬†Tao, and S.¬†Peng, ‚ÄúFast reconstruction of 3d point cloud model using visual slam on embedded uav development platform,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Remote Sensing</em>, vol.¬†12, no.¬†20, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A.¬†Steenbeek and F.¬†Nex, ‚ÄúCnn-based dense monocular visual slam for real-time uav exploration in emergency conditions,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Drones</em>, vol.¬†6, no.¬†3, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
I.¬†Zhura, D.¬†Davletshin, N.¬†D.¬†W. Mudalige, A.¬†Fedoseev, R.¬†Peter, and D.¬†Tsetserukou, ‚ÄúNeuroswarm: Multi-agent neural 3d scene reconstruction and segmentation with uav for optimal navigation of quadruped robot,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proc. 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, 2023, pp. 2525‚Äì2530.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M.¬†Dronova, V.¬†Cheremnykh, A.¬†Kotcov, A.¬†Fedoseev, and D.¬†Tsetserukou, ‚ÄúFlynerf: Nerf-based aerial mapping for high-quality 3d scene reconstruction,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2024 International Conference on Unmanned Aircraft Systems (ICUAS)</em>, 2024, pp. 1050‚Äì1055.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.¬†Guo, R.¬†Ni, and Y.¬†Zhao, ‚ÄúDeblurslam: A novel visual slam system robust in blurring scene,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proc. 2021 IEEE 7th International Conference on Virtual Reality (ICVR)</em>, 2021, pp. 62‚Äì68.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J.¬†J. Park, P.¬†Florence, J.¬†Straub, R.¬†Newcombe, and S.¬†Lovegrove, ‚ÄúDeepsdf: Learning continuous signed distance functions for shape representation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proc. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019, pp. 165‚Äì174.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 17:46:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
