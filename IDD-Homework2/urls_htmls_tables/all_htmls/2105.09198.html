<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.09198] A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning</title><meta property="og:description" content="We curated WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction. Although automatic annotation can lead to a high degree of label noise, it is aâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.09198">

<!--Generated on Mon Mar 18 17:40:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rajitha Hathurusinghe<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Isar Nejadgholi<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">2</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>, Miodrag Bolic<sup id="id10.10.id3" class="ltx_sup">3</sup> 
<br class="ltx_break"><sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">1,2,3</span></sup>University of Ottawa, Ottawa, Canada
<br class="ltx_break"><sup id="id12.12.id5" class="ltx_sup">2</sup>National Research Council Canada, Ottawa, Canada 
<br class="ltx_break"><sup id="id13.13.id6" class="ltx_sup"><span id="id13.13.id6.1" class="ltx_text ltx_font_italic">1,3</span></sup><span id="id7.7.1" class="ltx_text ltx_font_typewriter">{rhath050,Miodrag.Bolic}@uottawa.ca
<br class="ltx_break"><sup id="id7.7.1.1" class="ltx_sup"><span id="id7.7.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">2</span></sup>isar.nejadgholi@nrc-cnrc.gc.ca</span>
</span><span class="ltx_author_notes">â€ƒEqual contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">We curated WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction. Although automatic annotation can lead to a high degree of label noise, it is an inexpensive process and can generate large volumes of annotated documents. We trained a BERT-based NER model with WikiPII and showed that with an adequately large training dataset, the model can significantly decrease the cost of manual information extraction, despite the high level of label noise. In a similar approach, organizations can leverage text mining techniques to create customized annotated datasets from their historical data without sharing the raw data for human annotation. Also, we explore collaborative training of NER models through federated learning when the annotation is noisy. Our results suggest that depending on the level of trust to the ML operator and the volume of the available data, distributed training can be an effective way of training a personal information identifier in a privacy-preserved manner. Research material is available at <a target="_blank" href="https://github.com/ratmcu/wikipiifed" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ratmcu/wikipiifed</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Extraction of Personally Identifiable Information (PII) from unstructured text is a crucial task in many industries such as healthcare (e.g <cite class="ltx_cite ltx_citemacro_cite">Li and Qin (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>); Kushida etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2012</a>)</cite>) legal documents <cite class="ltx_cite ltx_citemacro_cite">Oksanen etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, mining of user-generated data <cite class="ltx_cite ltx_citemacro_cite">Mosallanezhad etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> and publication process <cite class="ltx_cite ltx_citemacro_cite">Aura etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2006</a>)</cite>. PII is a laborious task, often necessary for de-identification purposes, among other applications. For example, the extracted information can be used for indexing of documents, categorization and other applications. Identification of PII elements is a laborious task that can be automated by deploying Named Entity Recognition (NER) models <cite class="ltx_cite ltx_citemacro_cite">Hassan etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>); GraliÅ„ski etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2009</a>)</cite>. We formulate PII recognition as a NER task that extracts predefined PII entities. Our goal is to develop NER models to decrease this taskâ€™s cost by preprocessing documents before manual information extraction.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Supervised machine learning approaches such as Conditional Random Field (CRF) models, Support vector machines, and extensive feature engineering based on lexical and phrase embeddings have been used to train NER systems <cite class="ltx_cite ltx_citemacro_cite">Luo etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2015</a>); Passos etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2014</a>); Retinov and Roth (<a href="#bib.bib21" title="" class="ltx_ref">2009</a>)</cite>. With improvements of deep learning models, recurrent neural networks and specially LSTM models became the default model for training NER systems <cite class="ltx_cite ltx_citemacro_cite">Chiu and Nichols (<a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>. Recently, a combination of pre-trained transformer-based language models and linear or recurrent prediction layers achieved the state-of-the-art in most NER tasks <cite class="ltx_cite ltx_citemacro_cite">Dai etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Fraser etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Training a NER model for extraction of PII demands a massive corpus of text, rich in personal information, which raises privacy concerns in the process of data annotation and model training. Although NER models achieve high performance in cross-validation settings, the generalization of off-the-shelf models remains poor <cite class="ltx_cite ltx_citemacro_cite">Fu etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>. For training a robust PII recognizer, a customized domain-specific annotated dataset is needed <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>. In this work, our goal is to bring privacy to the front line of designing a PII extractor from dataset creation to model training.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We consider a scenario where an institution intends to build an assistant tool to decrease the cost of manual PII extraction. We assume that the institution has accumulated documents alongside their corresponding PII fields over the years, but PII elements are not necessarily marked within the text. This is the typical case for many institutions (such as hospitals and banks), which have manually extracted PII elements for years. For example, a hospital has access to patientsâ€™ names and ages for every specific health record. However, the locations of occurrences of name and age within the documents are unknown. Also, the name and age can come in various forms and lengths when mentioned in free text. To build a useful training dataset for a PII recognizer, the hospital needs to mark phrases related to name and age in the free text through text mining. However, sharing this data for annotation and training involves privacy considerations. We consider the following steps to ensure our process is compliant with the privacy of data subjects.</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Annotating the free text programmatically without the need for sharing the data for human annotation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Distributed storing of annotated documents so that the data can be kept in authorized locations.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Remote training of the PII extraction model without the need for sharing annotated documents with machine learning practitioners.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To conduct a reproducible research, we show the feasibility of the proposed approach on a dataset collected from Wikipedia and share the created dataset and results with research community. Our contributions are as following:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We create and release an automatically labeled dataset comprised of 77703 sentences from Wikipedia biography pages annotated for 5 classes of personal information.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We develop a method for remote training of a transformer-based model on distributed datasets, using PySyft platform.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We explore the impact of label noise and dataset size on the performance of remotely trained NER models.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>WikiPII Dataset</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our goal is to create and annotate a customized textual dataset for training a PII extractor (the scenario described in Section <a href="#S1" title="1 Introduction â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Approaches like snorkel <cite class="ltx_cite ltx_citemacro_cite">Ratner etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> had been embracing noise of automatic annotations and compensated the noise by adding to the volume of inexpensive data. We took the same approach for annotating Wikipedia pages and benefited from the fact that a version of entities was available in the infobox. We used the infobox to generate noisy and inexpensive data annotations, whereas snorkel uses multiple noisy parallel annotation functions and weak-supervision from alternative sources.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">We collected our data from Wikipedia biography pages because 1) they are rich in terms of PII, 2) with the infobox available on each page, they comply with our assumption of having access to extracted PII, 3) they are publicly available and can be shared and used as a benchmark for research purposes. Similar automated annotation tasks such as <cite class="ltx_cite ltx_citemacro_citet">Nothman etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2013</a>)</cite> utilized a broader set of Wikipedia pages and contain general CoNLL style <cite class="ltx_cite ltx_citemacro_cite">Sang and DeÂ Meulder (<a href="#bib.bib24" title="" class="ltx_ref">2003</a>)</cite> (location, organization, person, miscellaneous) classes of entities and does not focus on granular personal information as ours. We refer to this dataset as WikiPII and release this data for further research.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Collection </h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We scraped our raw textual data from biography page entries of living people in Wikipedia (about 900K pages).
For programmatic annotation of each pageâ€™s textual body, we first read the HTML-coded infobox and converted it to a PII element dictionary, using the BeautifulSoup<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></span></span></span> package. An example of this conversion is demonstrated in Figure <a href="#S2.F1" title="Figure 1 â€£ 2.1 Data Collection â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2105.09198/assets/html_example.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Infobox a) viewed on web page, b) in HTML format, c) converted to a dictionary</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Next, we normalized the similar entity types to acquire consistent entity types across all pages. For example, the spouseâ€™s name can come under the titlesâ€™ Spouseâ€™,â€™ Spouse(s)â€™ andâ€™ Spousesâ€™, which are all normalized to the â€˜SPâ€™ tag. After normalization, we manually inspected the entities and chose the ones with high coverage in the dataset. At last, we decided to include BD (date of birth), PR (names of parents), SP (names of spouse(s)), CH (names of children) and ED (terms of education institutes attended). Our final tags and their corresponding infobox entries are shown in Table <a href="#S2.T1" title="Table 1 â€£ 2.1 Data Collection â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We first extracted birthplace from the infobox, but places are mentioned in several formats such as town, province, country, etc. We omit this entity in the final tagging.</span></span></span>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r" style="padding-bottom:2.15277pt;">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Tag</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-bottom:2.15277pt;">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p" style="width:128.0pt;"><span id="S2.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Corresponding entries in infobox</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S2.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.1.1.1" class="ltx_p" style="width:56.9pt;">Birth Date (BD)</span>
</span>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.2.1.1" class="ltx_p" style="width:128.0pt;">â€™Bornâ€™, â€™Born:â€™</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.1.1.1" class="ltx_p" style="width:56.9pt;">Parents (PR)</span>
</span>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p" style="width:128.0pt;">â€™Parentâ€™, â€™Parent(s)â€™, â€™Parentsâ€™, â€™Fatherâ€™, â€™Fatherâ€™s nameâ€™, â€™Motherâ€™, â€™Motherâ€™s name</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.1.1.1" class="ltx_p" style="width:56.9pt;">Spouses (SP)</span>
</span>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p" style="width:128.0pt;">â€™Spouseâ€™, â€™Spouse(s)â€™, â€™Spousesâ€™</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.1.1.1" class="ltx_p" style="width:56.9pt;">Children (CH)</span>
</span>
</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p" style="width:128.0pt;">â€™Childrenâ€™</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S2.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.1.1.1" class="ltx_p" style="width:56.9pt;">Education (ED)</span>
</span>
</td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-bottom:4.30554pt;">
<span id="S2.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.2.1.1" class="ltx_p" style="width:128.0pt;">â€™Educationâ€™, â€™High schoolâ€™, â€™High school:â€™, â€™Law Schoolâ€™, â€™Schoolâ€™, â€™Schoolsâ€™, â€™Collegeâ€™, â€™College(s)â€™, â€™Collegesâ€™, â€™Alma materâ€™,â€™Almat materâ€™</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Tags included in our dataset and corresponding entities in infobox.</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2105.09198/assets/annot_ui_2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Annotator UI for manual annotation.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Labeling of Entities in Text</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Once the PII was extracted from the infobox, we had to locate them in the text and generate a tag for each word to create an annotated dataset. This stepâ€™s main challenge is that the mentions of entities in the text might be variations of the ones extracted from the infobox.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We parsed the textual body into sentences and removed citation brackets and numerals using regular expressions. For each tag shown in Table <a href="#S2.T1" title="Table 1 â€£ 2.1 Data Collection â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we develop a function that takes the extracted phrase from infobox and locates that phrase within the free text. We combine two different methods of matching to get a more accurate match. Our entities are subcategories of places, organizations, persons and dates, which SpaCy already covers. We leveraged the part-of-speech and named entity recognition capabilities of the SpaCy<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We used the <math id="footnote3.m1.1" class="ltx_Math" alttext="en\_core\_web\_lg" display="inline"><semantics id="footnote3.m1.1b"><mrow id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mi id="footnote3.m1.1.1.2" xref="footnote3.m1.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1b" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="footnote3.m1.1.1.4" xref="footnote3.m1.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1c" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.5" xref="footnote3.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1d" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.6" xref="footnote3.m1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1e" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.7" xref="footnote3.m1.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1f" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.8" xref="footnote3.m1.1.1.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1g" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="footnote3.m1.1.1.9" xref="footnote3.m1.1.1.9.cmml">_</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1h" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.10" xref="footnote3.m1.1.1.10.cmml">w</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1i" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.11" xref="footnote3.m1.1.1.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1j" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.12" xref="footnote3.m1.1.1.12.cmml">b</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1k" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="footnote3.m1.1.1.13" xref="footnote3.m1.1.1.13.cmml">_</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1l" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.14" xref="footnote3.m1.1.1.14.cmml">l</mi><mo lspace="0em" rspace="0em" id="footnote3.m1.1.1.1m" xref="footnote3.m1.1.1.1.cmml">â€‹</mo><mi id="footnote3.m1.1.1.15" xref="footnote3.m1.1.1.15.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><times id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1.1"></times><ci id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1.2">ğ‘’</ci><ci id="footnote3.m1.1.1.3.cmml" xref="footnote3.m1.1.1.3">ğ‘›</ci><ci id="footnote3.m1.1.1.4.cmml" xref="footnote3.m1.1.1.4">_</ci><ci id="footnote3.m1.1.1.5.cmml" xref="footnote3.m1.1.1.5">ğ‘</ci><ci id="footnote3.m1.1.1.6.cmml" xref="footnote3.m1.1.1.6">ğ‘œ</ci><ci id="footnote3.m1.1.1.7.cmml" xref="footnote3.m1.1.1.7">ğ‘Ÿ</ci><ci id="footnote3.m1.1.1.8.cmml" xref="footnote3.m1.1.1.8">ğ‘’</ci><ci id="footnote3.m1.1.1.9.cmml" xref="footnote3.m1.1.1.9">_</ci><ci id="footnote3.m1.1.1.10.cmml" xref="footnote3.m1.1.1.10">ğ‘¤</ci><ci id="footnote3.m1.1.1.11.cmml" xref="footnote3.m1.1.1.11">ğ‘’</ci><ci id="footnote3.m1.1.1.12.cmml" xref="footnote3.m1.1.1.12">ğ‘</ci><ci id="footnote3.m1.1.1.13.cmml" xref="footnote3.m1.1.1.13">_</ci><ci id="footnote3.m1.1.1.14.cmml" xref="footnote3.m1.1.1.14">ğ‘™</ci><ci id="footnote3.m1.1.1.15.cmml" xref="footnote3.m1.1.1.15">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">en\_core\_web\_lg</annotation></semantics></math> pipeline from <a target="_blank" href="https://spacy.io/usage/facts-figures##benchmarks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://spacy.io/usage/facts-figures##benchmarks</a>, which is highly accurate in NER task and optimized in terms of speed.</span></span></span> package to find noun chunks, person names, locations, organizations and dates. Then, depending on the type of the entity, we choose a subset of extracted phrases and use fuzzy string matching<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We utilized the implementation published at <a target="_blank" href="https://github.com/axiak/fuzzyset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/axiak/fuzzyset/</a> for fuzzy matching.</span></span></span> to find the closest phrase to our target phrase. For example, for the tag, ED (education), we extract all the organization names by SpaCy. We then use fuzzy matching to find the variations of the education institute pulled from the infobox.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<table id="S2.F3.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.F3.2.2" class="ltx_tr">
<td id="S2.F3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.F3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.F3.1.1.1.1.1" class="ltx_p" style="width:142.3pt;"><img src="/html/2105.09198/assets/annot_examples/7.jpg" id="S2.F3.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="63" alt="Refer to caption"></span>
</span>
</td>
<td id="S2.F3.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.F3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.F3.2.2.2.1.1" class="ltx_p" style="width:284.5pt;"><img src="/html/2105.09198/assets/annot_examples/8.jpg" id="S2.F3.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="393" height="26" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S2.F3.4.4" class="ltx_tr">
<td id="S2.F3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.F3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.F3.3.3.1.1.1" class="ltx_p" style="width:142.3pt;"><img src="/html/2105.09198/assets/annot_examples/9.jpg" id="S2.F3.3.3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="60" alt="Refer to caption"></span>
</span>
</td>
<td id="S2.F3.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.F3.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.F3.4.4.2.1.1" class="ltx_p" style="width:284.5pt;"><img src="/html/2105.09198/assets/annot_examples/10.jpg" id="S2.F3.4.4.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="393" height="23" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S2.F3.6.6" class="ltx_tr">
<td id="S2.F3.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.F3.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.F3.5.5.1.1.1" class="ltx_p" style="width:142.3pt;"><img src="/html/2105.09198/assets/annot_examples/5.jpg" id="S2.F3.5.5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="59" alt="Refer to caption"></span>
</span>
</td>
<td id="S2.F3.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.F3.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.F3.6.6.2.1.1" class="ltx_p" style="width:284.5pt;"><img src="/html/2105.09198/assets/annot_examples/6.jpg" id="S2.F3.6.6.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="393" height="14" alt="Refer to caption"></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of mistakes in automated annotation. </figcaption>
</figure>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;">Data split / annotation method</span>
</span>
</th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;">Pages</span>
</span>
</th>
<th id="S2.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.3.1.1" class="ltx_p" style="width:42.7pt;">Sentences</span>
</span>
</th>
<th id="S2.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.4.1.1" class="ltx_p" style="width:28.5pt;">BD</span>
</span>
</th>
<th id="S2.T2.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.5.1.1" class="ltx_p" style="width:28.5pt;">PR</span>
</span>
</th>
<th id="S2.T2.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.6.1.1" class="ltx_p" style="width:28.5pt;">SP</span>
</span>
</th>
<th id="S2.T2.1.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.7.1.1" class="ltx_p" style="width:28.5pt;">CH</span>
</span>
</th>
<th id="S2.T2.1.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-bottom:2.15277pt;">
<span id="S2.T2.1.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.8.1.1" class="ltx_p" style="width:28.5pt;">ED</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<td id="S2.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.1.1.1" class="ltx_p" style="width:113.8pt;">training/automatic</span>
</span>
</td>
<td id="S2.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.2.1.1" class="ltx_p" style="width:28.5pt;">20039</span>
</span>
</td>
<td id="S2.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.3.1.1" class="ltx_p" style="width:42.7pt;">77703</span>
</span>
</td>
<td id="S2.T2.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.4.1.1" class="ltx_p" style="width:28.5pt;">16883</span>
</span>
</td>
<td id="S2.T2.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.5.1.1" class="ltx_p" style="width:28.5pt;">6326</span>
</span>
</td>
<td id="S2.T2.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.6.1.1" class="ltx_p" style="width:28.5pt;">25163</span>
</span>
</td>
<td id="S2.T2.1.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.7.1.1" class="ltx_p" style="width:28.5pt;">10824</span>
</span>
</td>
<td id="S2.T2.1.2.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T2.1.2.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.2.1.8.1.1" class="ltx_p" style="width:28.5pt;">24365</span>
</span>
</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<td id="S2.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.1.1.1" class="ltx_p" style="width:113.8pt;">validation/automatic</span>
</span>
</td>
<td id="S2.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.2.1.1" class="ltx_p" style="width:28.5pt;">2744</span>
</span>
</td>
<td id="S2.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.3.1.1" class="ltx_p" style="width:42.7pt;">12267</span>
</span>
</td>
<td id="S2.T2.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.4.1.1" class="ltx_p" style="width:28.5pt;">2512</span>
</span>
</td>
<td id="S2.T2.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.5.1.1" class="ltx_p" style="width:28.5pt;">1509</span>
</span>
</td>
<td id="S2.T2.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.6.1.1" class="ltx_p" style="width:28.5pt;">3844</span>
</span>
</td>
<td id="S2.T2.1.3.2.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.7.1.1" class="ltx_p" style="width:28.5pt;">1846</span>
</span>
</td>
<td id="S2.T2.1.3.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.3.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.3.2.8.1.1" class="ltx_p" style="width:28.5pt;">3831</span>
</span>
</td>
</tr>
<tr id="S2.T2.1.4.3" class="ltx_tr">
<td id="S2.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.1.1.1" class="ltx_p" style="width:113.8pt;">test/automatic</span>
</span>
</td>
<td id="S2.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.2.1.1" class="ltx_p" style="width:28.5pt;">307</span>
</span>
</td>
<td id="S2.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.3.1.1" class="ltx_p" style="width:42.7pt;">2051</span>
</span>
</td>
<td id="S2.T2.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.4.1.1" class="ltx_p" style="width:28.5pt;">303</span>
</span>
</td>
<td id="S2.T2.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.5.1.1" class="ltx_p" style="width:28.5pt;">331</span>
</span>
</td>
<td id="S2.T2.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.6.1.1" class="ltx_p" style="width:28.5pt;">609</span>
</span>
</td>
<td id="S2.T2.1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.7.1.1" class="ltx_p" style="width:28.5pt;">604</span>
</span>
</td>
<td id="S2.T2.1.4.3.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T2.1.4.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.4.3.8.1.1" class="ltx_p" style="width:28.5pt;">534</span>
</span>
</td>
</tr>
<tr id="S2.T2.1.5.4" class="ltx_tr">
<td id="S2.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.1.1.1" class="ltx_p" style="width:113.8pt;">test/manual</span>
</span>
</td>
<td id="S2.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.2.1.1" class="ltx_p" style="width:28.5pt;">91</span>
</span>
</td>
<td id="S2.T2.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.3.1.1" class="ltx_p" style="width:42.7pt;">320</span>
</span>
</td>
<td id="S2.T2.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.4.1.1" class="ltx_p" style="width:28.5pt;">76</span>
</span>
</td>
<td id="S2.T2.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.5.1.1" class="ltx_p" style="width:28.5pt;">50</span>
</span>
</td>
<td id="S2.T2.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.6.1.1" class="ltx_p" style="width:28.5pt;">80</span>
</span>
</td>
<td id="S2.T2.1.5.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.7.1.1" class="ltx_p" style="width:28.5pt;">62</span>
</span>
</td>
<td id="S2.T2.1.5.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-bottom:4.30554pt;">
<span id="S2.T2.1.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.5.4.8.1.1" class="ltx_p" style="width:28.5pt;">92</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Count of pages, sentences which contain at least one of the target entities and number of mentions per each class of entities for different splits of the WikiPII dataset. </figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We used the BIO scheme for tagging of words. NER is a sequence to sequence learning task that predicts a label for each word, specifying whether the word is within or outside an entity and the entitiesâ€™ type. In the BIO format, the tags â€˜B_â€™, â€˜I_â€™, and â€˜O_â€™ mark the beginning, inside and outside of an entity, respectively. For example, â€˜B_CHâ€™ specifies the beginning of a phrase tagged as â€˜Childrenâ€™. The combination of these tags specifies the boundary and tag of the extracted entity. Therefore, error analysis of an NER task is based on errors in tag and boundary detection. These error are reflected in the evaluation metrics described in Section <a href="#S3" title="3 Evaluation of PII extraction â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Manual Annotation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To evaluate the quality of the programmatic annotation, we manually annotated a subset of the pages. We selected pages that include highest numbers of entities and made sure that the manually annotated dataset contains 50 to 100 mentions of each class. Manual annotation is done by re-annotating the entities already found by the automated annotator. A human annotator can choose to confirm, reject or correct the labels created by the automatic annotation. We designed a user interface for the manual annotation where the annotator had access to the infobox elements and their corresponding tags. An example of the designed annotator user interface is shown in Figure <a href="#S2.F2" title="Figure 2 â€£ 2.1 Data Collection â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Figure <a href="#S2.F3" title="Figure 3 â€£ 2.2 Labeling of Entities in Text â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows examples of common mistakes in the automatic annotation. Entities extracted from the infobox are shown in the left column. The yellow entities are missed by automated annotation and corrected by the human annotator. These entities are missed because they are missing from the infobox. Also, since the automatic annotation does not consider the context, it cannot resolve ambiguities. In the example of Figure <a href="#S2.F3" title="Figure 3 â€£ 2.2 Labeling of Entities in Text â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> â€˜Troyâ€™ is a city name but is tagged as CH (children) since it appears as a child name in the infobox. Also, â€˜Harvard Universityâ€™, which is tagged as an education institute, is not a PII element for the main subject of the page but an affiliation of someone else. In manual annotation, â€˜Troyâ€™ and â€˜Harvard Universityâ€™ will be corrected and not tagged as an entity.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Statistics of WikiPII dataset</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Our data source contains over 900K entries. Our annotation method could only use a little over 23K entries due to formatting changes in the Wikipedia pages where infobox is not available. We filtered the sentences that do not include any of our target entities. The dataset contains a large number of PII instances belonging to over 23K individuals worldwide belonging to 5 classes.
Separate splits of the created dataset and numbers of entities contained are presented in Table <a href="#S2.T2" title="Table 2 â€£ 2.2 Labeling of Entities in Text â€£ 2 WikiPII Dataset â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation of PII extraction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Averaged F-score is a common metric to evaluate a NER system. However, the definition of a True Positive and a True Negative prediction is not always trivial. Since identifying an entity involves finding both the span and type of the entity, some of the systemâ€™s predictions can be partially correct. Multiple evaluation schemes have been developed. Shared tasks such as IREX <cite class="ltx_cite ltx_citemacro_cite">Sekine and Isahara (<a href="#bib.bib26" title="" class="ltx_ref">1999</a>)</cite>, and CoNLL <cite class="ltx_cite ltx_citemacro_cite">Sang and DeÂ Meulder (<a href="#bib.bib24" title="" class="ltx_ref">2003</a>)</cite> only gave credit to the exacted entities with the exact type and boundary matches. Other works have adopted <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">type matching</span> or <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">partial matching</span> evaluation schemes, which reward partially correct entity extractions <cite class="ltx_cite ltx_citemacro_cite">Tsai etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2006</a>); Chinchor and Sundheim (<a href="#bib.bib3" title="" class="ltx_ref">1993</a>); SeguraÂ Bedmar etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2013</a>)</cite>. Learning-based evaluation methods are developed to predict the user experience in specific tasks <cite class="ltx_cite ltx_citemacro_cite">Nejadgholi etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.4.5.1" class="ltx_tr">
<td id="S3.T3.4.5.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.4.5.1.1.1" class="ltx_text ltx_font_bold">Predicted Entity</span></td>
<td id="S3.T3.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.4.5.1.2.1" class="ltx_text ltx_font_bold">strict</span></td>
<td id="S3.T3.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.4.5.1.3.1" class="ltx_text ltx_font_bold">exact</span></td>
<td id="S3.T3.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.4.5.1.4.1" class="ltx_text ltx_font_bold">type</span></td>
<td id="S3.T3.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.4.5.1.5.1" class="ltx_text ltx_font_bold">partial</span></td>
<td id="S3.T3.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.4.5.1.6.1" class="ltx_text ltx_font_bold">implication in PII extraction</span></td>
</tr>
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<sub id="S3.T3.1.1.1.1" class="ltx_sub"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#FF0000;">name</span></sub><span id="S3.T3.1.1.1.2" class="ltx_text" style="color:#000000;background-color:#FF7D00;">Adam London.</span>
</td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“âœ“</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“âœ“</td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“âœ“</td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“âœ“</td>
<td id="S3.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">no need for correction.</td>
</tr>
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Adam <sub id="S3.T3.2.2.1.1" class="ltx_sub"><span id="S3.T3.2.2.1.1.1" class="ltx_text ltx_font_italic" style="color:#FF0000;">name</span></sub><span id="S3.T3.2.2.1.2" class="ltx_text" style="color:#000000;background-color:#FF7D00;">London.</span>
</td>
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S3.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S3.T3.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">boundary should be corrected.</td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<sub id="S3.T3.3.3.1.1" class="ltx_sub"><span id="S3.T3.3.3.1.1.1" class="ltx_text ltx_font_italic" style="color:#FF0000;">place</span></sub><span id="S3.T3.3.3.1.2" class="ltx_text" style="color:#000000;background-color:#FF7D00;">Adam London.</span>
</td>
<td id="S3.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“âœ“</td>
<td id="S3.T3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S3.T3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">type should be corrected.</td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Adam <sub id="S3.T3.4.4.1.1" class="ltx_sub"><span id="S3.T3.4.4.1.1.1" class="ltx_text ltx_font_italic" style="color:#FF0000;">place</span></sub><span id="S3.T3.4.4.1.2" class="ltx_text" style="color:#000000;background-color:#FF7D00;">London.</span>
</td>
<td id="S3.T3.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">âœ“</td>
<td id="S3.T3.4.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">entity is located, boundary and type should be corrected.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examples of predicted entity with respect to various evaluation metrics. âœ—indicates no reward, âœ“indicates half point reward and âœ“âœ“indicates a full reward. </figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">PII extraction is a sensitive task, and a fully automatic system cannot be reliable. Instead, the output of such systems are used to augment the performance of manual PII extraction. In practice, when a human is in the loop, partial matching can reduce the manual effort of PII extraction. We adopted the metrics introduced by the MUC-5 task <cite class="ltx_cite ltx_citemacro_cite">Chinchor and Sundheim (<a href="#bib.bib3" title="" class="ltx_ref">1993</a>)</cite>, and SemEval-13 task 9 <cite class="ltx_cite ltx_citemacro_cite">SeguraÂ Bedmar etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2013</a>)</cite> and implemented the following evaluation metrics ordered in terms of strictness:</p>
</div>
<div id="S3.p3" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Strict Matching:</span> rewards a prediction only if boundary and type of entity match with gold standard label. This metric evaluates the system in a fully automated PII extraction setting.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Exact Boundary:</span> rewards a prediction if the boundary of extracted entity matches the gold standard labeling. This metric evaluates the system where the human annotator relies on boundaries predicted by the system and only corrects the label if necessary.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Type Matching:</span> rewards the strict matches and partially (<math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mo id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><times id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\times</annotation></semantics></math>0.5) rewards the extracted entities where the type is correct and boundary overlaps with the gold standard. This metric evaluates the system where the human annotator relies on types predicted by the system and only corrects the boundary if necessary.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Partial Boundary:</span> rewards strict matches and partially (<math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mo id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><times id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\times</annotation></semantics></math>0.5) rewards where the boundary overlaps with the gold standard label regardless of type. This metric evaluates the system where the human annotator relies on the location of predictions and corrects both label and boundary if necessary.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Table <a href="#S3.T3" title="Table 3 â€£ 3 Evaluation of PII extraction â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows examples of predicted entities by a PII recognizer with respect to the evaluation metrics and the cost of correction in a human-in-the-loop PII recognition task.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>PII Extraction model</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">First, we evaluate the automated annotation compared to the manual annotation. Then we use the automatically annotated train set to train a BERT-based NER model with a fully connected linear layer as the prediction layer. We then evaluate the performance of the trained PII recognizer on both automatically and manually annotated test sets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison between Manual and Automatic Annotation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate the automatic annotation, we take manual annotations as the gold standard and score the corresponding automated annotations with the metrics described in Section <a href="#S3" title="3 Evaluation of PII extraction â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Table <a href="#S4.T4" title="Table 4 â€£ 4.1 Comparison between Manual and Automatic Annotation â€£ 4 PII Extraction model â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results of this evaluation. As discussed in Section <a href="#S3" title="3 Evaluation of PII extraction â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we used different metrics to evaluate this model based on the real-application scenario. For example, partial metric evaluates the scenario where the model is used to assist the human annotator in locating the entities. We observe that the rule-based annotation tool leads to high levels of noise. With partial evaluation, we conclude that automatic annotation spots about half of the entities correctly, but the boundary and type might not be fully correct. On the other side, the strict metric indicates that about one-third of the entities are perfectly annotated. Also, the type metric is higher than the exact metric, indicating that automatic annotation performs better in predicting types than boundaries. This is expected because of the complexities and subjectivity of boundary identification.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td"></td>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">strict</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">exact</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">type</th>
<th id="S4.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">partial</th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<td id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">precision</td>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.31</td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">0.39</td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<td id="S4.T4.1.3.3.1" class="ltx_td ltx_align_center">recall</td>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center">0.45</td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center">0.46</td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center">0.57</td>
<td id="S4.T4.1.3.3.5" class="ltx_td ltx_align_center">0.65</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<td id="S4.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_b">F1-score</td>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b">0.37</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b">0.38</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b">0.47</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_b">0.54</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation of automated annotation compared to the manual annotation.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance of PII Extraction Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We fine-tuned a BERT-based NER model with the training split of the automatically annotated WikiPII dataset to build a PII recognizer and tested the trained model with the test split of automatically annotated dataset and the manually annotated test set. We choose a batch size of 128 sentences and a maximum length of 50 tokens and present the results for one epoch of training. The optimum number of epochs varies between 1 and 3 for different datasets, but for the sake of comparison we choose to run all experiments with one epoch. Table <a href="#S4.T5" title="Table 5 â€£ 4.2 Performance of PII Extraction Model â€£ 4 PII Extraction model â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results of this experiment.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We observed that despite the high level of noise in the automatically annotated training dataset the trained NER model reaches an acceptable performance. This is due to the large size of the automatically annotated dataset. As <cite class="ltx_cite ltx_citemacro_citet">Rolnick etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> showed, deep learning models are robust to label noise when the size of the dataset is adequately large. We observed that the partial metric is 80%, which indicates a significant decrease in manual cost of PII extraction. While these predictions might still need corrections of type and boundary the system can locate most of the entities. From the strict metric, we conclude that half of the PII elements are predicted correctly in label and span and do not need any correction. Comparing of the exact and type metric shows that in most cases the system predicts the label correctly and boundaries need to be corrected.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">Automatically annotated</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">Manually annotated</th>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<th id="S4.T5.1.2.2.1" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">strict</th>
<th id="S4.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">exact</th>
<th id="S4.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">type</th>
<th id="S4.T5.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">partial</th>
<th id="S4.T5.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">strict</th>
<th id="S4.T5.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">exact</th>
<th id="S4.T5.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">type</th>
<th id="S4.T5.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">partial</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.3.1" class="ltx_tr">
<td id="S4.T5.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">precision</td>
<td id="S4.T5.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.64</td>
<td id="S4.T5.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.72</td>
<td id="S4.T5.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.70</td>
<td id="S4.T5.1.3.1.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.74</td>
<td id="S4.T5.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.55</td>
<td id="S4.T5.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.56</td>
<td id="S4.T5.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">0.68</td>
<td id="S4.T5.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">0.79</td>
</tr>
<tr id="S4.T5.1.4.2" class="ltx_tr">
<td id="S4.T5.1.4.2.1" class="ltx_td ltx_align_center">recall</td>
<td id="S4.T5.1.4.2.2" class="ltx_td ltx_align_center">0.62</td>
<td id="S4.T5.1.4.2.3" class="ltx_td ltx_align_center">0.69</td>
<td id="S4.T5.1.4.2.4" class="ltx_td ltx_align_center">0.68</td>
<td id="S4.T5.1.4.2.5" class="ltx_td ltx_align_center ltx_border_rr">0.72</td>
<td id="S4.T5.1.4.2.6" class="ltx_td ltx_align_center">0.56</td>
<td id="S4.T5.1.4.2.7" class="ltx_td ltx_align_center">0.56</td>
<td id="S4.T5.1.4.2.8" class="ltx_td ltx_align_center">0.68</td>
<td id="S4.T5.1.4.2.9" class="ltx_td ltx_align_center">0.80</td>
</tr>
<tr id="S4.T5.1.5.3" class="ltx_tr">
<td id="S4.T5.1.5.3.1" class="ltx_td ltx_align_center ltx_border_b">F1-score</td>
<td id="S4.T5.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b">0.64</td>
<td id="S4.T5.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b">0.70</td>
<td id="S4.T5.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b">0.69</td>
<td id="S4.T5.1.5.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">0.73</td>
<td id="S4.T5.1.5.3.6" class="ltx_td ltx_align_center ltx_border_b">0.55</td>
<td id="S4.T5.1.5.3.7" class="ltx_td ltx_align_center ltx_border_b">0.56</td>
<td id="S4.T5.1.5.3.8" class="ltx_td ltx_align_center ltx_border_b">0.68</td>
<td id="S4.T5.1.5.3.9" class="ltx_td ltx_align_center ltx_border_b">0.80</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Test accuracy of the BERT-based NER model on both test sets</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Distributed Training </h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Modern deep learning models are known as data-hungry algorithms. In the task of PII extraction, sharing data across organizations will lead to more robust models. However, sharing of personal data in a central location involves concerns of privacy. To mitigate the risk of data breaches, we can train machine learning models in a distributed fashion while leaving the data in a location governed by the data owners. In this work, we explore Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite> for training a NER model with noisy labelled data. Federated learning involves training statistical models over remote data centers, such as mobile phones or hospitals, while keeping data localized without requiring transfer of the whole dataset to a central location.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">To implement FL, we use the PySyft framework, developed by <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">OpenMined</span> <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="http://www.openmined.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.openmined.org/</a></span></span></span>.
This framework is developed in PyTorch and provides the platform for executing tensor operations remotely <cite class="ltx_cite ltx_citemacro_cite">Ryffel etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>. PySyft has been developed under the theme <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">"Answer questions you cannot see"</span>, to perform machine learning inference with zero knowledge about the specifics of the data.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In this framework, a central entity orchestrates the training scenario. Data is maintained and tagged by its owners at a remote location. At each data location, a worker follows the commands of the central entity. The model is transferred to the remote location, and updates are completed remotely at each training iteration. Subsequently, the final model is updated by averaging weights, averaging remote gradient updates or consecutive updates at each dataset location <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Federated Training of BERT-based Model</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model pre-trained on a massive corpus of written text. BERT and a series of language models belonging to BERTâ€™s family form the backbone of todayâ€™s deep learning NLP models. The language model generates a vector representation for the input text and passes it on to the downstream task. The pre-trained language model is usually fine-tuned with the task-specific data during task-specific learning. In this work, we only use BERT-based NER model, with a fully connected linear layer as the prediction layer, but the general idea applies to other transformer-based language models.Training and testing splits are the same as the ones used in Section <a href="#S4.SS2" title="4.2 Performance of PII Extraction Model â€£ 4 PII Extraction model â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The input to a BERT model contains three tensors: Token type id (specifies single sentence or double sentence use of the model), position ids (specifies the position of the token in the sentence), and input ids (specifies the id of the word in the vocabulary) <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. Except for input ids, all the other inputs are tensors generated dynamically at the training time. The pre-trained tokenizer model generates these inputs to be based on the dimensions of the input sentence batches.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">PySyft is designed in a way that abstracts the remote tensor objects by wrapping them around an empty tensor located centrally. Wrapping the tensor is the process of maintaining an empty local tensor object, while executing tensor operations on the remote tensor through the network. This method abstracts the location separation and allows the central worker to operate on tensor objects just as they were situated centrally. One drawback of this wrapping-based abstraction is that the functions such as size querying are operated on the local empty tensor rather than the native real tensor located in the remote worker. For that reason, the PySyft framework cannot query the dimensions of the input data tensors while operating in a remote worker <cite class="ltx_cite ltx_citemacro_cite">Ryffel etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">For remote tokenization through PySyft, we modified the model to carry these inputs as static non-trainable parameters embedded in the form of tensor buffers. Using PySyft, we can move a model between the remote workers and the central worker using the API calls. Initially, these APIs were developed to handle the trainable parameters of the models among workers involved in federated learning. We contributed to the PySyft frameworkâ€™s codebase by developing a federated BERT tokenizer method, which handles the movement of non-trainable parameters and allows full remote functionality of the model. Our implementation of the <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_italic">BERT-base</span> model for remote operation will be released for further research.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training Scenarios</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We deploy two settings of FL to share training data in a privacy-preserved manner. In practice one of these scenarios might be preferred depending on how much trusted the central worker is.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">federated/central: A trusted central operator can receive data batches from remote data holders</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">federated/remote: A mistrusted central operator sends model to a remote data holders</p>
</div>
</li>
</ul>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09198/assets/central_vs_fed_remote.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09198/assets/loss_wikiner.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Training loss for central vs federated/remote on a) CoNLL-2003 and b) WikiPII dataset </figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">In scenario 1, the operator is trusted to receive data from remote sources and updates the model in the central location. Data batches from distributed sources are called using the federated training iterator. Received data batches contribute to forward pass and back-propagation operations. Then the operator discards the data batch as agreed. Here the operator has full control over the data batches, and the BERT tokenizer works in its typical mode. The only different operations with respect to central training are data transfers from the remote workers towards the central worker. These transfers might lead to information loss because of data compression. Also, from machine learning perspective, distributed data cannot be shuffled randomly and data batches might be imbalanced which has an impact on the final performance of the model.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In scenario 2, the operator is not fully trusted, so the batch of data can not be fully transferred to the central operator. Federated training iterator holds the locations of the data holders or remote workers. The model owned by the central operator is sent to the remote worker and allowed to be remotely executed. Only the central operatorâ€™s commands are allowed to reach the remote worker guaranteeing the central operator is not breaching into the data. In this scenario, we use our remote tokenization method. Interactions for this training involve sending the model, sending commands to execute the model, and receiving the trained model parameters back. Model weights are received back by the central operator after training for all the batches of data belonging to the remote worker. Then the model is sent to the other workers. An epoch is completed when the model cycles all the workers.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Performance of Model Trained with Distributed Data and Noisy Labels</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To gain insight into the impact of distributed training on NER modelsâ€™ performance, besides the WikiPII dataset, we trained our model on the widely used NER dataset, CoNLL-2003. Similar to our central training (Section <a href="#S4.SS2" title="4.2 Performance of PII Extraction Model â€£ 4 PII Extraction model â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we restrict our experiments to one epoch of training. We simulated both scenarios with only two virtual workers and recorded the training loss to investigate how remote learning impacts the modelâ€™s convergence. Also, for simplicity, we assume the remote workersâ€™ availability at all the times a central worker requests their computational resources for training, which might not be the real-world scenario and will require planning and robustness.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">We observed that the case of federated/central training does not impact the convergence of the model. Figures <a href="#S5.F4.sf1" title="In Figure 4 â€£ 5.2 Training Scenarios â€£ 5 Distributed Training â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a> and <a href="#S5.F4.sf2" title="In Figure 4 â€£ 5.2 Training Scenarios â€£ 5 Distributed Training â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> show the convergence of the loss when model trained on two workers in federated/remote scenario compared to the typical centralized training, for both CoNLL2003 and WikiPII. In the case of CoNLL2003, where the annotations are of gold-standard quality, the federated training does not significantly impact the modelâ€™s convergence. In WikiPII, with noisy labels, federated/remote training leads to higher loss function values. However, this impact is not detrimental.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.12.13.1" class="ltx_tr">
<th id="S5.T6.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T6.12.13.1.1.1" class="ltx_text ltx_font_bold">Dataset/Setting</span></th>
<th id="S5.T6.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">no. of workers</th>
<th id="S5.T6.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1 score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.12.14.1" class="ltx_tr">
<th id="S5.T6.12.14.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T6.12.14.1.1.1" class="ltx_text ltx_font_bold">CoNLL2003</span></th>
<td id="S5.T6.12.14.1.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T6.12.14.1.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T6.2.2" class="ltx_tr">
<th id="S5.T6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â Â Â  central</th>
<td id="S5.T6.2.2.3" class="ltx_td ltx_align_center">N/A</td>
<td id="S5.T6.2.2.2" class="ltx_td ltx_align_center">0.90 <math id="S5.T6.2.2.2.m1.1" class="ltx_Math" alttext="\pm 0.005" display="inline"><semantics id="S5.T6.2.2.2.m1.1a"><mrow id="S5.T6.2.2.2.m1.1.1" xref="S5.T6.2.2.2.m1.1.1.cmml"><mo id="S5.T6.2.2.2.m1.1.1a" xref="S5.T6.2.2.2.m1.1.1.cmml">Â±</mo><mn id="S5.T6.2.2.2.m1.1.1.2" xref="S5.T6.2.2.2.m1.1.1.2.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.m1.1b"><apply id="S5.T6.2.2.2.m1.1.1.cmml" xref="S5.T6.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.2.2.2.m1.1.1.1.cmml" xref="S5.T6.2.2.2.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T6.2.2.2.m1.1.1.2.cmml" xref="S5.T6.2.2.2.m1.1.1.2">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.m1.1c">\pm 0.005</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T6.4.4" class="ltx_tr">
<th id="S5.T6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â Â Â  federated/remote</th>
<td id="S5.T6.4.4.3" class="ltx_td ltx_align_center">2</td>
<td id="S5.T6.4.4.2" class="ltx_td ltx_align_center">0.85 <math id="S5.T6.4.4.2.m1.1" class="ltx_Math" alttext="\pm 0.003" display="inline"><semantics id="S5.T6.4.4.2.m1.1a"><mrow id="S5.T6.4.4.2.m1.1.1" xref="S5.T6.4.4.2.m1.1.1.cmml"><mo id="S5.T6.4.4.2.m1.1.1a" xref="S5.T6.4.4.2.m1.1.1.cmml">Â±</mo><mn id="S5.T6.4.4.2.m1.1.1.2" xref="S5.T6.4.4.2.m1.1.1.2.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.4.4.2.m1.1b"><apply id="S5.T6.4.4.2.m1.1.1.cmml" xref="S5.T6.4.4.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.4.4.2.m1.1.1.1.cmml" xref="S5.T6.4.4.2.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T6.4.4.2.m1.1.1.2.cmml" xref="S5.T6.4.4.2.m1.1.1.2">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.4.4.2.m1.1c">\pm 0.003</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T6.6.6" class="ltx_tr">
<th id="S5.T6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â Â Â  federated/central</th>
<td id="S5.T6.6.6.3" class="ltx_td ltx_align_center">2</td>
<td id="S5.T6.6.6.2" class="ltx_td ltx_align_center">0.90 <math id="S5.T6.6.6.2.m1.1" class="ltx_Math" alttext="\pm 0.008" display="inline"><semantics id="S5.T6.6.6.2.m1.1a"><mrow id="S5.T6.6.6.2.m1.1.1" xref="S5.T6.6.6.2.m1.1.1.cmml"><mo id="S5.T6.6.6.2.m1.1.1a" xref="S5.T6.6.6.2.m1.1.1.cmml">Â±</mo><mn id="S5.T6.6.6.2.m1.1.1.2" xref="S5.T6.6.6.2.m1.1.1.2.cmml">0.008</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.6.6.2.m1.1b"><apply id="S5.T6.6.6.2.m1.1.1.cmml" xref="S5.T6.6.6.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.6.6.2.m1.1.1.1.cmml" xref="S5.T6.6.6.2.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T6.6.6.2.m1.1.1.2.cmml" xref="S5.T6.6.6.2.m1.1.1.2">0.008</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.6.6.2.m1.1c">\pm 0.008</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T6.12.15.2" class="ltx_tr">
<th id="S5.T6.12.15.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T6.12.15.2.1.1" class="ltx_text ltx_font_bold">WikiPII</span></th>
<td id="S5.T6.12.15.2.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T6.12.15.2.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T6.8.8" class="ltx_tr">
<th id="S5.T6.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â Â Â  central</th>
<td id="S5.T6.8.8.3" class="ltx_td ltx_align_center">N/A</td>
<td id="S5.T6.8.8.2" class="ltx_td ltx_align_center">0.70 <math id="S5.T6.8.8.2.m1.1" class="ltx_Math" alttext="\pm 0.006" display="inline"><semantics id="S5.T6.8.8.2.m1.1a"><mrow id="S5.T6.8.8.2.m1.1.1" xref="S5.T6.8.8.2.m1.1.1.cmml"><mo id="S5.T6.8.8.2.m1.1.1a" xref="S5.T6.8.8.2.m1.1.1.cmml">Â±</mo><mn id="S5.T6.8.8.2.m1.1.1.2" xref="S5.T6.8.8.2.m1.1.1.2.cmml">0.006</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.8.8.2.m1.1b"><apply id="S5.T6.8.8.2.m1.1.1.cmml" xref="S5.T6.8.8.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.8.8.2.m1.1.1.1.cmml" xref="S5.T6.8.8.2.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T6.8.8.2.m1.1.1.2.cmml" xref="S5.T6.8.8.2.m1.1.1.2">0.006</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.8.8.2.m1.1c">\pm 0.006</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T6.10.10" class="ltx_tr">
<th id="S5.T6.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â Â Â  federated/remote</th>
<td id="S5.T6.10.10.3" class="ltx_td ltx_align_center">2</td>
<td id="S5.T6.10.10.2" class="ltx_td ltx_align_center">0.56 <math id="S5.T6.10.10.2.m1.1" class="ltx_Math" alttext="\pm 0.02" display="inline"><semantics id="S5.T6.10.10.2.m1.1a"><mrow id="S5.T6.10.10.2.m1.1.1" xref="S5.T6.10.10.2.m1.1.1.cmml"><mo id="S5.T6.10.10.2.m1.1.1a" xref="S5.T6.10.10.2.m1.1.1.cmml">Â±</mo><mn id="S5.T6.10.10.2.m1.1.1.2" xref="S5.T6.10.10.2.m1.1.1.2.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.10.10.2.m1.1b"><apply id="S5.T6.10.10.2.m1.1.1.cmml" xref="S5.T6.10.10.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.10.10.2.m1.1.1.1.cmml" xref="S5.T6.10.10.2.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T6.10.10.2.m1.1.1.2.cmml" xref="S5.T6.10.10.2.m1.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.10.10.2.m1.1c">\pm 0.02</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T6.12.12" class="ltx_tr">
<th id="S5.T6.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-bottom:4.30554pt;">Â Â Â Â Â Â  federated/central</th>
<td id="S5.T6.12.12.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-bottom:4.30554pt;">2</td>
<td id="S5.T6.12.12.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-bottom:4.30554pt;">0.70 <math id="S5.T6.12.12.2.m1.1" class="ltx_Math" alttext="\pm 0.01" display="inline"><semantics id="S5.T6.12.12.2.m1.1a"><mrow id="S5.T6.12.12.2.m1.1.1" xref="S5.T6.12.12.2.m1.1.1.cmml"><mo id="S5.T6.12.12.2.m1.1.1a" xref="S5.T6.12.12.2.m1.1.1.cmml">Â±</mo><mn id="S5.T6.12.12.2.m1.1.1.2" xref="S5.T6.12.12.2.m1.1.1.2.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.12.12.2.m1.1b"><apply id="S5.T6.12.12.2.m1.1.1.cmml" xref="S5.T6.12.12.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.12.12.2.m1.1.1.1.cmml" xref="S5.T6.12.12.2.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T6.12.12.2.m1.1.1.2.cmml" xref="S5.T6.12.12.2.m1.1.1.2">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.12.12.2.m1.1c">\pm 0.01</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Exact F1-score for central vs federated model </figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The exact F-scores trained under our FL implementations are summarized in Table <a href="#S5.T6" title="Table 6 â€£ 5.3 Performance of Model Trained with Distributed Data and Noisy Labels â€£ 5 Distributed Training â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We observed very close final model performance between the federated learning with centrally operated and typical centralized training. Federated training with the mistrusted central operator deviates from central training, with higher loss convergence values and a reduced final performance score, for both CoNLL2003 and WikiPII datasets. This observation can be explained by the loss of information in weight compression while transferring the model.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Effect of Dataset Size</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Federated learning is most useful where multiple data holders participate in the training process. In reality, different distributed sources contributing to training can carry imbalanced amounts of data and features, which can have a negative impact on the results. Here we measure the effect of increasing the dataset size by increasing the number of workers. We randomly divided the training dataset among ten workers and, starting from 2 workers, increased the number of workers participating in the training process. Figure <a href="#S5.F5" title="Figure 5 â€£ 5.4 Effect of Dataset Size â€£ 5 Distributed Training â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the change of different types of F-score as more workers are utilized, and the dataset size increases as a result. We used the federated/central scenario here, which was shown to achieve comparable performance to central training. To control for the random sampling, we repeat each experiment 10 times and average the acquired F1-score. The error plot in Figure <a href="#S5.F5" title="Figure 5 â€£ 5.4 Effect of Dataset Size â€£ 5 Distributed Training â€£ A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates this experimentâ€™s final results for all the metrics.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">In general, we observed that when the size of the noisy annotated data increases, higher performances are achieved. Since automatic labelling of data is inexpensive, generating and sharing noisy labelled data is a promising way of achieving high-quality models. However, note that the standard deviation of F-scores can be considerable. This observation indicates that the imbalances of distributed data can drastically impact the final model.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2105.09198/assets/datsize_auto_test.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="295" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>F1-scores vs. the training dataset size as more workers participate in federated/central training</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, our goal is to use the historical data accumulated in an organization to build a customized NER for a human-in-the-loop PII recognition tool. We expect that this tool can significantly decrease the cost of manual extraction by locating PII entities in the free text.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">First, we assume that the organization has access to a corpus of unstructured documents along with the structured dataset containing their corresponding PII entities. We propose that these parallel datasets can be used to create a noisy annotated training set. Our method of automatic annotation is based on matching of phrases and the raw data is not exposed to a third party for annotation. Using Wikipedia biography pages as an example, we show the feasibility of creating a noisy annotated dataset and training a PII recognition model in a privacy-preserved fashion. Our automatic annotation is inexpensive. Therefore it can generate large volumes of annotated datasets to compensate for the label noise. This is in line with previous work showing that deep learning is robust against noise when trained with massive noisy datasets <cite class="ltx_cite ltx_citemacro_cite">Rolnick etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Furthermore, we looked at the feasibility of distributed training in cases that multiple organizations have similar datasets and are willing to collaborate to build more robust models but cannot share the data due to privacy concerns. We showed that where the operator is trusted, distributed training will not affect performance regardless of the annotation quality. For both CoNLL2003 (clean annotations) and our WikiPII dataset (noisy annotations), the F-score of NER models does not suffer from distributed training. However, when the operator is not trusted, the F-score is impacted and the drop of F-score is more significant in the case of the noisy dataset. In model transfer, all the parameter tensors of the model go through the simplification, serialization and compression steps followed by decompression, de-serialization and decompression steps. We suspect this mechanism affects the precision of the weights. In future work, a rigorous analysis should be carried out to analyze the effect of object transfers in a distributed system.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Lastly, in the federated/central scenario, we showed that the increase in the dataset size is a promising way to achieve higher accuracies. Distributed training allows organizations to share their data which results in a bigger size of the data. We conclude that there is a trade-off between the drop in performance because of the distributed training and the increase in performance because of the higher volume of data.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">This work has limitations. NER is a very challenging task, and it is difficult to achieve a fully reliable NER model for a sensitive task such as PII extraction. Also, even highly accurate NER models can be vulnerable to adversarial attacks <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>. For this reason, throughout this work, we only envisioned this system to assist human annotators by locating the entities and suggest a highly likely tag. Although this system does not reach very high performance, it is still instrumental in reducing the cost of PII extraction when compared to a fully manual procedure. We only considered a BERT-based NER model, but the general idea applies to other transformer-based NER models. In future, an ensemble of different techniques should be considered to improve the utility of the system.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We propose an inexpensive and privacy-preserved method that automatically annotates parallel structured/unstructured datasets to train a customized NER models. The final models can be used to decrease the cost of manual extraction of PII elements by preprocessing the documents in a human-in-the-loop setting. Our results demonstrate that federated training is a promising tool to compensate for label noise by increasing the volume of the noisy labeled dataset.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to thank IMRSV Data Labs for partial funding and support in manual annotation. We also acknowledge Mitacs for partially funding this project.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aura etÂ al. (2006)</span>
<span class="ltx_bibblock">
Tuomas Aura, ThomasÂ A Kuhn, and Michael Roe. 2006.

</span>
<span class="ltx_bibblock">Scanning electronic documents for personally identifiable
information.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th ACM workshop on Privacy in electronic
society</em>, pages 41â€“50.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2015)</span>
<span class="ltx_bibblock">
Yukun Chen, ThomasÂ A Lasko, Qiaozhu Mei, JoshuaÂ C Denny, and Hua Xu. 2015.

</span>
<span class="ltx_bibblock">A study of active learning methods for named entity recognition in
clinical text.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Journal of biomedical informatics</em>, 58:11â€“18.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chinchor and Sundheim (1993)</span>
<span class="ltx_bibblock">
Nancy Chinchor and Beth Sundheim. 1993.

</span>
<span class="ltx_bibblock">MUC-5 evaluation metrics.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Fifth Message Understanding Conference (MUC-5):
Proceedings of a Conference Held in Baltimore, Maryland, August 25-27,
1993</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiu and Nichols (2016)</span>
<span class="ltx_bibblock">
JasonÂ PC Chiu and Eric Nichols. 2016.

</span>
<span class="ltx_bibblock">Named entity recognition with bidirectional lstm-cnns.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational
Linguistics</em>, volumeÂ 4, pages 357â€“370. MIT Press.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zhenjin Dai, Xutao Wang, Pin Ni, Yuming Li, Gangmin Li, and Xuming Bai. 2019.

</span>
<span class="ltx_bibblock">Named entity recognition using BERT-BiLSTM-CRF for chinese
electronic health records.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2019 12th international congress on image and signal
processing, biomedical engineering and informatics (cisp-bmei)</em>, pages 1â€“5.
IEEE.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraser etÂ al. (2019)</span>
<span class="ltx_bibblock">
KathleenÂ C Fraser, Isar Nejadgholi, Berry DeÂ Bruijn, Muqun Li, Astha LaPlante,
and Khaldoun ZineÂ El Abidine. 2019.

</span>
<span class="ltx_bibblock">Extracting umls concepts from medical text using general and
domain-specific deep learning models.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01274</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jinlan Fu, Pengfei Liu, and QiÂ Zhang. 2020.

</span>
<span class="ltx_bibblock">Rethinking generalization of neural models: A named entity
recognition case study.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 34, pages 7732â€“7739.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GraliÅ„ski etÂ al. (2009)</span>
<span class="ltx_bibblock">
Filip GraliÅ„ski, Krzysztof Jassem, MichaÅ‚ MarciÅ„czuk, and PaweÅ‚
Wawrzyniak. 2009.

</span>
<span class="ltx_bibblock">Named entity recognition in machine anonymization.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Recent Advances in Intelligent Information Systems</em>, pages
247â€“260.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan etÂ al. (2018)</span>
<span class="ltx_bibblock">
Fadi Hassan, Josep Domingo-Ferrer, and Jordi Soria-Comas. 2018.

</span>
<span class="ltx_bibblock">Anonymization of unstructured data via named-entity recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International conference on modeling decisions for
artificial intelligence</em>, pages 296â€“305. Springer.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kushida etÂ al. (2012)</span>
<span class="ltx_bibblock">
CleteÂ A Kushida, DeborahÂ A Nichols, Rik Jadrnicek, Ric Miller, JamesÂ K Walsh,
and Kara Griffin. 2012.

</span>
<span class="ltx_bibblock">Strategies for de-identification and anonymization of electronic
health record data for use in multicenter research studies.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Medical care</em>, 50(Suppl):S82.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tian Li, AnitÂ Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37(3):50â€“60.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Qin (2017)</span>
<span class="ltx_bibblock">
Xiao-Bai Li and Jialun Qin. 2017.

</span>
<span class="ltx_bibblock">Anonymizing and sharing medical text records.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Information Systems Research</em>, 28(2):332â€“352.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al. (2015)</span>
<span class="ltx_bibblock">
Gang Luo, Xiaojing Huang, Chin-Yew Lin, and Zaiqing Nie. 2015.

</span>
<span class="ltx_bibblock">Joint named entity recognition and disambiguation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing</em>, page 879â€“888, USA. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosallanezhad etÂ al. (2019)</span>
<span class="ltx_bibblock">
Ahmadreza Mosallanezhad, Ghazaleh Beigi, and Huan Liu. 2019.

</span>
<span class="ltx_bibblock">Deep reinforcement learning-based text anonymization against
private-attribute inference.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2360â€“2369. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nejadgholi etÂ al. (2020)</span>
<span class="ltx_bibblock">
Isar Nejadgholi, KathleenÂ C. Fraser, and BerryÂ De Bruijn. 2020.

</span>
<span class="ltx_bibblock">Extensive error analysis and a learning-based evaluation of medical
entity recognition systems to approximate user experience.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">The 19th SIGBioMed Workshop on Biomedical Language
Processing (BioNLP2020)</em>, volumeÂ 19, pages 177â€“186.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nothman etÂ al. (2013)</span>
<span class="ltx_bibblock">
Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy, and JamesÂ R Curran.
2013.

</span>
<span class="ltx_bibblock">Learning multilingual named entity recognition from wikipedia.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence</em>, volume 194, pages 151â€“175.
Elsevier.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oksanen etÂ al. (2019)</span>
<span class="ltx_bibblock">
Arttu Oksanen, JÂ Tuominen, EÂ MÃ¤kelÃ¤, MÂ Tamper, Aki Hietanen, and Eero
HyvÃ¶nen. 2019.

</span>
<span class="ltx_bibblock">Semantic finlex: Transforming, publishing, and using finnish
legislation and case law as linked open data on the web.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Knowledge of the Law in the Big Data Age</em>, 317:212â€“228.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passos etÂ al. (2014)</span>
<span class="ltx_bibblock">
Alexandre Passos, Vineet Kumar, and Andrew McCallum. 2014.

</span>
<span class="ltx_bibblock">Lexicon infused phrase embeddings for named entity resolution.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighteenth Conference on Computational
Natural Language Learning</em>, pages 78â€“86.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ratner etÂ al. (2017)</span>
<span class="ltx_bibblock">
Alexander Ratner, StephenÂ H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and
Christopher RÃ©. 2017.

</span>
<span class="ltx_bibblock">Snorkel: Rapid training data creation with weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment. International Conference
on Very Large Data Bases</em>, volumeÂ 11, page 269. NIH Public Access.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Retinov and Roth (2009)</span>
<span class="ltx_bibblock">
Lev Retinov and Dan Roth. 2009.

</span>
<span class="ltx_bibblock">Design challenges and misconceptions in named entity recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL)</em>, page 147â€“155, USA. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rolnick etÂ al. (2017)</span>
<span class="ltx_bibblock">
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. 2017.

</span>
<span class="ltx_bibblock">Deep learning is robust to massive label noise.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.10694</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel etÂ al. (2018)</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach. 2018.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.04017</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sang and DeÂ Meulder (2003)</span>
<span class="ltx_bibblock">
ErikÂ F Sang and Fien DeÂ Meulder. 2003.

</span>
<span class="ltx_bibblock">Introduction to the conll-2003 shared task: Language-independent
named entity recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Conference on Natural Language
Learning, Edmonton, Canada</em>, pages 142â€“â€“147.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SeguraÂ Bedmar etÂ al. (2013)</span>
<span class="ltx_bibblock">
Isabel SeguraÂ Bedmar, Paloma MartÃ­nez, and MarÃ­a HerreroÂ Zazo. 2013.

</span>
<span class="ltx_bibblock">Semeval-2013 task 9: Extraction of drug-drug interactions from
biomedical texts (ddiextraction 2013).

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sekine and Isahara (1999)</span>
<span class="ltx_bibblock">
Satoshi Sekine and Hitoshi Isahara. 1999.

</span>
<span class="ltx_bibblock">IREX project overview.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IREX Workshop</em>, pages 7â€“12. Citeseer.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai etÂ al. (2006)</span>
<span class="ltx_bibblock">
Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi Chou, Yu-Chun Lin, Ding He, Jieh
Hsiang, Ting-Yi Sung, and Wen-Lian Hsu. 2006.

</span>
<span class="ltx_bibblock">Various criteria in the evaluation of biomedical named entity
recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">BMC bioinformatics</em>, volumeÂ 7, pageÂ 92. Springer.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
10(2):1â€“19.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020)</span>
<span class="ltx_bibblock">
WeiÂ Emma Zhang, QuanÂ Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020.

</span>
<span class="ltx_bibblock">Adversarial attacks on deep-learning models in natural language
processing: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
11(3):1â€“41.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.09197" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.09198" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.09198">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.09198" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.09199" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 18 17:40:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
