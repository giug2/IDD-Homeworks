<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting</title>
<!--Generated on Fri Dec  8 02:46:35 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2312.04807v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S2" title="2 Related Work ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Training ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 Inference ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS3" title="3.3 Knowledge Acquisition ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Knowledge Acquisition</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px1" title="Retrieving Similar Sentence ‚Ä£ 3.3 Knowledge Acquisition ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Retrieving Similar Sentence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px2" title="Matching Terminology ‚Ä£ 3.3 Knowledge Acquisition ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Matching Terminology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px3" title="Translation Template Prediction ‚Ä£ 3.3 Knowledge Acquisition ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Translation Template Prediction</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px1" title="Corpus ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px2" title="Baseline ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px3" title="Training Data ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px4" title="Test Data ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Test Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Main Results ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="4.3 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS4" title="4.4 Effect of Similarity Threshold ùúÜ ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Effect of Similarity Threshold <math alttext="\lambda" class="ltx_Math" display="inline"><semantics><mi>Œª</mi><annotation-xml encoding="MathML-Content"><ci>ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_Œª</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS5" title="4.5 Inference Speed ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Inference Speed</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusions ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="package-alerts ltx_document" role="alert">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of <a href="https://corpora.mathweb.org/corpus/arxmliv/tex_to_html/info/loaded_file" target="_blank">supported packages</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2312.04807v1 [cs.CL] 08 Dec 2023</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ke Wang, Jun Xie<math alttext="{}^{\ast}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">‚àó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\ast}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT ‚àó end_FLOATSUPERSCRIPT</annotation></semantics></math>, Yuqi Zhang, Yu Zhao 
<br class="ltx_break"/>Alibaba Group 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id1">{wk258730,qingjing.xj,chenwei.zyq}@alibaba-inc.com,kongyu@taobao.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Improving neural machine translation (NMT) systems with prompting has achieved significant progress in recent years. In this work, we focus on how to integrate multi-knowledge, multiple types of knowledge, into NMT models to enhance the performance with prompting. We propose a unified framework, which can integrate effectively multiple types of knowledge including sentences, terminologies/phrases and translation templates into NMT models. We utilize multiple types of knowledge as prefix-prompts of input for the encoder and decoder of NMT models to guide the translation process. The approach requires no changes to the model architecture and effectively adapts to domain-specific translation without retraining. The experiments on English-Chinese and English-German translation demonstrate that our approach significantly outperform strong baselines, achieving high translation quality and terminology match accuracy.</p>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><math alttext="{}^{\ast}" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><msup id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml"><mi id="footnote1.m1.1.1b" xref="footnote1.m1.1.1.cmml"></mi><mo id="footnote1.m1.1.1.1" xref="footnote1.m1.1.1.1.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"><ci id="footnote1.m1.1.1.1.cmml" xref="footnote1.m1.1.1.1">‚àó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">{}^{\ast}</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">start_FLOATSUPERSCRIPT ‚àó end_FLOATSUPERSCRIPT</annotation></semantics></math>Corresponding author.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the workflow of translation, human translators generally utilize different types of external knowledge to simplify the process and improve translation quality and speed, such as matching terminologies and similar example sentences. The knowledge used in machine translation mainly includes high-quality bilingual sentences, a bilingual terminology dictionary and translation templates. Intuitively, it is reasonable to believe that it is beneficial for improving translation quality to integrate multiple types of knowledge into NMT models in a flexible and efficient way. However, most existing methods focus on only how to integrate a single type of knowledge into NMT models, either a terminology dictionary¬†<cite class="ltx_cite ltx_citemacro_cite">Dinu et¬†al. (<a class="ltx_ref" href="#bib.bib10" title="">2019</a>); Dougal and Lonsdale (<a class="ltx_ref" href="#bib.bib11" title="">2020</a>)</cite>, bilingual sentences¬†<cite class="ltx_cite ltx_citemacro_cite">Cao and Xiong (<a class="ltx_ref" href="#bib.bib7" title="">2018</a>); Liu et¬†al. (<a class="ltx_ref" href="#bib.bib26" title="">2019a</a>)</cite> or translation templates¬†<cite class="ltx_cite ltx_citemacro_cite">Yang et¬†al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As a primary technique to utilize a terminology dictionary, lexically constrained translation allows for explicit phrase-based constraints to be placed on target output strings¬†<cite class="ltx_cite ltx_citemacro_cite">Hu et¬†al. (<a class="ltx_ref" href="#bib.bib17" title="">2019</a>)</cite>. Several research works¬†<cite class="ltx_cite ltx_citemacro_cite">Hokamp and Liu (<a class="ltx_ref" href="#bib.bib16" title="">2017</a>); Post and Vilar (<a class="ltx_ref" href="#bib.bib34" title="">2018</a>)</cite> impose lexical constraints by modifying the beam search decoding algorithm. Another line of approach trains the model to copy the target constraints by data augmentation¬†<cite class="ltx_cite ltx_citemacro_cite">Song et¬†al. (<a class="ltx_ref" href="#bib.bib37" title="">2019</a>); Dinu et¬†al. (<a class="ltx_ref" href="#bib.bib10" title="">2019</a>); Chen et¬†al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>)</cite>. Some researchers¬†<cite class="ltx_cite ltx_citemacro_cite">Li et¬†al. (<a class="ltx_ref" href="#bib.bib24" title="">2019</a>); Wang et¬†al. (<a class="ltx_ref" href="#bib.bib44" title="">2022b</a>)</cite> introduce attention modules in the architecture of NMT models to integrate constraints. These methods using terminologies or phrases as the knowledge suffer from either high computational overheads or low terminology translation success rates.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the majority of methods that utilize sentence pairs, the most similar source-target sentence pairs are retrieved from a translation memory (TM) for the input source sentence¬†<cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a class="ltx_ref" href="#bib.bib26" title="">2019a</a>); Huang et¬†al. (<a class="ltx_ref" href="#bib.bib18" title="">2021</a>); He et¬†al. (<a class="ltx_ref" href="#bib.bib14" title="">2021</a>)</cite>. Several approaches focus on integrating a TM into statistical machine translation (SMT)¬†<cite class="ltx_cite ltx_citemacro_cite">Ma et¬†al. (<a class="ltx_ref" href="#bib.bib29" title="">2011</a>); Wang et¬†al. (<a class="ltx_ref" href="#bib.bib42" title="">2013</a>); Liu et¬†al. (<a class="ltx_ref" href="#bib.bib27" title="">2019b</a>)</cite>. Some researchers use a TM to augment an NMT model, including using n-grams from a TM to reward translation¬†<cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="#bib.bib51" title="">2018b</a>)</cite>, employing an auxiliary network to integrate similar sentences into the NMT¬†<cite class="ltx_cite ltx_citemacro_cite">Gu et¬†al. (<a class="ltx_ref" href="#bib.bib13" title="">2018</a>); Xia et¬†al. (<a class="ltx_ref" href="#bib.bib45" title="">2019</a>)</cite> and data augmentation based on TM¬†<cite class="ltx_cite ltx_citemacro_cite">Bulte and Tezcan (<a class="ltx_ref" href="#bib.bib4" title="">2019a</a>); Xu et¬†al. (<a class="ltx_ref" href="#bib.bib46" title="">2020</a>)</cite>. These methods consume considerable computational overheads in training or testing.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Although these approaches have demonstrated the benefits of combining an NMT model with a single type of knowledge, how to integrate multiple types of knowledge into NMT models remains a challenge. In this work, we propose a prompt-based neural machine translation that can integrate multiple types of knowledge including both sentences, terminologies/phrases and translation templates into NMT models in a unified framework. Inspired by¬†<cite class="ltx_cite ltx_citemacro_cite">Brown et¬†al. (<a class="ltx_ref" href="#bib.bib3" title="">2020</a>)</cite>, which has redefined different NLP tasks as fill in the blanks problems by different prompts, we concatenate the source and target side of the knowledge as prefix-prompts of input for the encoder and decoder of NMT models, respectively. During training, this model learns dynamically to incorporate helpful information from the prefixes into generating translations. At inference time, new knowledge from multiple sources can be applied in real time. The model has automatic domain adaption capability and can be extended to new domains without updating parameters. We evaluate the approach in two tasks domain adaptation and soft lexical (terminology) constraint. The metric of ‚Äòexact match‚Äô for terminology match accuracy has significantly improved compared to strong baselines both in English to German and English to Chinese translation.
This approach has shown its robustness in domain adaptation and performs better than fine-tuning when there are domain mismatch or noise data.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The contributions of this work include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a simple and effective approach to integrate multi-knowledge into NMT models with prompting.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We demonstrate that an NMT model can benefit from multiple types of knowledge simultaneously, including sentence, terminology/phrases and translation template knowledge.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">NMT is increasingly improving translation quality. However, the interpolation of the reasoning process has been less clear due to the deep neural architectures with hundreds of millions of parameters. How to guide an NMT system with user-specified different types of knowledge is an important issue of NMT applications in real world.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The first and most studied knowledge is simple constraints such as lexical constraints or in-domain dictionaries.
<cite class="ltx_cite ltx_citemacro_cite">Hokamp and Liu (<a class="ltx_ref" href="#bib.bib16" title="">2017</a>)</cite> proposes grid beam search (GBS) by modifying the decoding algorithm to add lexical constraints.¬†<cite class="ltx_cite ltx_citemacro_cite">Post and Vilar (<a class="ltx_ref" href="#bib.bib34" title="">2018</a>)</cite> introduces dynamic beam allocation to reduce the runtime complexity of GBS by dividing a fixed size of beam for candidates.¬†<cite class="ltx_cite ltx_citemacro_cite">Hu et¬†al. (<a class="ltx_ref" href="#bib.bib17" title="">2019</a>)</cite> proposes vectorized dynamic beam allocation (VDBA) to improve the efficiency of the decoding algorithm further. The beam search decoding algorithm by adding lexical constraints is still significantly slower than the beam search algorithm. Some data augmentation works propose to replace the corresponding source phrases with the target constraints¬†<cite class="ltx_cite ltx_citemacro_cite">Song et¬†al. (<a class="ltx_ref" href="#bib.bib37" title="">2019</a>)</cite>, to integrate constraints as inline annotations in the source sentence¬†<cite class="ltx_cite ltx_citemacro_cite">Dinu et¬†al. (<a class="ltx_ref" href="#bib.bib10" title="">2019</a>)</cite>, to insert target constraints using an alignment model¬†<cite class="ltx_cite ltx_citemacro_cite">Chen et¬†al. (<a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite> and to append constraints after the source sentence with a separation symbol¬†<cite class="ltx_cite ltx_citemacro_cite">Chen et¬†al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>); Jon et¬†al. (<a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>. These data augmentation methods can not guarantee the presence of the target constraints in the output.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Some works concentrate on adapting the architecture of NMT models to add lexical constraints.¬†<cite class="ltx_cite ltx_citemacro_cite">Susanto et¬†al. (<a class="ltx_ref" href="#bib.bib38" title="">2020</a>)</cite> invokes lexical constraints using a non-autoregressive decoding approach.¬†<cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="#bib.bib50" title="">2021</a>)</cite> introduces explicit phrase alignment into the translation process of NMT models by building a search space similar to phrase-based SMT.¬†<cite class="ltx_cite ltx_citemacro_cite">Li et¬†al. (<a class="ltx_ref" href="#bib.bib24" title="">2019</a>)</cite> proposes to use external continuous memory to store constraints and integrate the constraint memories into NMT models through the decoder network.¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="#bib.bib43" title="">2022a</a>)</cite> proposes a template-based method for constrained translation while maintaining the inference speed.¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="#bib.bib44" title="">2022b</a>)</cite> proposes to integrate vectorized lexical source and target constraints into attention modules of the NMT model to model constraint pairs. These methods may still suffer from low match accuracy of terminology when decoding without the VDBA algorithm.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="319" id="S2.F1.g1" src="extracted/5189224/images/model.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Knowledge integration framework with prompting and an example representation. </figcaption>
</figure>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The use of TM is very necessary for computer-aided translation¬†<cite class="ltx_cite ltx_citemacro_cite">Yamada (<a class="ltx_ref" href="#bib.bib47" title="">2011</a>)</cite> and computational approaches for machine translation¬†<cite class="ltx_cite ltx_citemacro_cite">Koehn and Senellart (<a class="ltx_ref" href="#bib.bib22" title="">2010</a>)</cite>.
Similar sentence pairs retrieved from a TM are also utilized as a type of knowledge to enhance the translation¬†<cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a class="ltx_ref" href="#bib.bib26" title="">2019a</a>); He et¬†al. (<a class="ltx_ref" href="#bib.bib14" title="">2021</a>); Khandelwal et¬†al. (<a class="ltx_ref" href="#bib.bib20" title="">2021</a>)</cite>. ¬†<cite class="ltx_cite ltx_citemacro_cite">Farajian et¬†al. (<a class="ltx_ref" href="#bib.bib12" title="">2017</a>)</cite> exploits the retrieved sentence pairs from a TM to update the generic NMT models on-the-fly.¬†<cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="#bib.bib51" title="">2018b</a>)</cite> utilizes translation pieces based on n-grams extracted from a TM during beam search by adding rewards for matched translation pieces into the NMT model output layer.¬†<cite class="ltx_cite ltx_citemacro_cite">He et¬†al. (<a class="ltx_ref" href="#bib.bib15" title="">2019</a>)</cite> proposes to add the word position information from a TM as additional rewards to guide the decoding of NMT models.
<cite class="ltx_cite ltx_citemacro_cite">Gu et¬†al. (<a class="ltx_ref" href="#bib.bib13" title="">2018</a>)</cite> uses an auxiliary network to fuse information from the source sentence and the retrieved value from a TM and then integrate it into the NMT model architecture.¬†<cite class="ltx_cite ltx_citemacro_cite">Xia et¬†al. (<a class="ltx_ref" href="#bib.bib45" title="">2019</a>)</cite> proposes to pack a TM into a compact graph corresponding to multiple words for different sentences in a TM, and then encode the packed graph into a deep representation during the decoding phase.¬†<cite class="ltx_cite ltx_citemacro_cite">Xu et¬†al. (<a class="ltx_ref" href="#bib.bib46" title="">2020</a>)</cite> utilizes data augmentation to train an NMT model whose training instances are bilingual sentences augmented with the translation retrieved from the TM. For input sentences that are not very similar to their TMs, the translation performance of these methods suffers significantly.¬†<cite class="ltx_cite ltx_citemacro_cite">He et¬†al. (<a class="ltx_ref" href="#bib.bib14" title="">2021</a>)</cite> introduces Example Layer consisting of multi-head attention and cross attention to translate any input sentences whether they are similar to their TM or no.¬†<cite class="ltx_cite ltx_citemacro_cite">Cai et¬†al. (<a class="ltx_ref" href="#bib.bib6" title="">2021</a>)</cite> extends the TM from the bilingual setting to the monolingual setting through learnable memory retrieval in a cross-lingual manner. The key idea of TM is to integrate the retrieved sentence pairs from a TM into the NMT architecture for accurate translations. Most of the works integrate the TM knowledge via model modification and the models need to be retrained when loading another TM in new domains.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The general knowledge integration to NMT is an ongoing work <cite class="ltx_cite ltx_citemacro_cite">Tang et¬†al. (<a class="ltx_ref" href="#bib.bib39" title="">2016</a>); Liu et¬†al. (<a class="ltx_ref" href="#bib.bib25" title="">2016</a>); Zhang et¬†al. (<a class="ltx_ref" href="#bib.bib49" title="">2018a</a>)</cite>.¬†<cite class="ltx_cite ltx_citemacro_cite">Yang et¬†al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite> proposes to use extracted templates from tree structures as soft target templates to incorporate the template information into the encoder-decoder framework.¬†<cite class="ltx_cite ltx_citemacro_cite">Shang et¬†al. (<a class="ltx_ref" href="#bib.bib36" title="">2021</a>)</cite> introduce to a template-based machine translation (TBMT) model to integrate the syntactic knowledge of the retrieved target template in the NMT decoder.¬†<cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="#bib.bib49" title="">2018a</a>)</cite> represents prior knowledge sources as features in a log-linear model to guide the learning process of NMT models. These approaches have demonstrated the clear benefits by incorporating different single types of knowledge into NMT models. Our approach is designed to integrate multiple types of knowledge into NMT models through an unified framework.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As shown in Figure¬†<a class="ltx_ref" href="#S2.F1" title="Figure 1 ‚Ä£ 2 Related Work ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">1</span></a>, we use source-side knowledge sequence and target-side knowledge sequence to prepend a source sentence and a target sentence as a prefix separately. The model uses multiple types of knowledge as prefix-prompts of the input to guide the process of translating target sentence.
The form is intended to lead the NMT model how to utilize relevant information from the redundant prefixes to guide the translation process for improving translation quality.
We use three types of special tokens to separate different types of knowledge sequence, source sentence and target sentence.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">[Sentence]/[Term]/[Template]: It indicates similar sentences, matching terminologies and translation templates respectively. The first token of each knowledge sequence is always the special token.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">[Input]: It is used to separate the knowledge sequence and the source sentence.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">[Output]: It is used to separate the knowledge sequence and the target sentence.
</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.8">For each sentence pair <math alttext="\left\langle\mathbf{x},\mathbf{y}\right\rangle" class="ltx_Math" display="inline" id="S3.p2.1.m1.2"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mo id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">‚ü®</mo><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">ùê±</mi><mo id="S3.p2.1.m1.2.3.2.2" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">ùê≤</mi><mo id="S3.p2.1.m1.2.3.2.3" xref="S3.p2.1.m1.2.3.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ùê±</ci><ci id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">ùê≤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">\left\langle\mathbf{x},\mathbf{y}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.2d">‚ü® bold_x , bold_y ‚ü©</annotation></semantics></math>, corresponding similar sentence pairs, matching terminologies and translation templates are concatenated into source knowledge sequence <math alttext="\mathbf{x_{k}}" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">ùê±</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">ùê§</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ùê±</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">ùê§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\mathbf{x_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> and target knowledge sequence <math alttext="\mathbf{y_{k}}" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">ùê≤</mi><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">ùê§</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">ùê≤</ci><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">ùê§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\mathbf{y_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> with the corresponding special token [Sentence], [Term] and [Template] on the source and target side, respectively.
Then the sentence pair <math alttext="\left\langle\mathbf{x},\mathbf{y}\right\rangle" class="ltx_Math" display="inline" id="S3.p2.4.m4.2"><semantics id="S3.p2.4.m4.2a"><mrow id="S3.p2.4.m4.2.3.2" xref="S3.p2.4.m4.2.3.1.cmml"><mo id="S3.p2.4.m4.2.3.2.1" xref="S3.p2.4.m4.2.3.1.cmml">‚ü®</mo><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">ùê±</mi><mo id="S3.p2.4.m4.2.3.2.2" xref="S3.p2.4.m4.2.3.1.cmml">,</mo><mi id="S3.p2.4.m4.2.2" xref="S3.p2.4.m4.2.2.cmml">ùê≤</mi><mo id="S3.p2.4.m4.2.3.2.3" xref="S3.p2.4.m4.2.3.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.2b"><list id="S3.p2.4.m4.2.3.1.cmml" xref="S3.p2.4.m4.2.3.2"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">ùê±</ci><ci id="S3.p2.4.m4.2.2.cmml" xref="S3.p2.4.m4.2.2">ùê≤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.2c">\left\langle\mathbf{x},\mathbf{y}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.2d">‚ü® bold_x , bold_y ‚ü©</annotation></semantics></math> are preprocessed as follows: the source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">bold_x</annotation></semantics></math> and target sentence <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">bold_y</annotation></semantics></math> are concatenated with source knowledge sequence <math alttext="\mathbf{x_{k}}" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><msub id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">ùê±</mi><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">ùê§</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">ùê±</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">ùê§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">\mathbf{x_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> and target knowledge sequence <math alttext="\mathbf{y_{k}}" class="ltx_Math" display="inline" id="S3.p2.8.m8.1"><semantics id="S3.p2.8.m8.1a"><msub id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml">ùê≤</mi><mi id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml">ùê§</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2">ùê≤</ci><ci id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3">ùê§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">\mathbf{y_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.8.m8.1d">bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.
An Example of the format of the input and output sequences is given in Figure¬†<a class="ltx_ref" href="#S2.F1" title="Figure 1 ‚Ä£ 2 Related Work ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">1</span></a>. When similar sentences and matching terminologies retrieved are empty, the input sequence and output sequence contain only translation template knowledge sequence. Although in this paper we integrate similar sentences, terminologies and translation templates into the NMT models, our approach can utilize more types of knowledge to improve translation performance by using this labeling strategy.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">Given a source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">bold_x</annotation></semantics></math>, the conditional probability of the corresponding target sentence <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">bold_y</annotation></semantics></math> by incorporating source and target knowledge sequence <math alttext="\left\langle\mathbf{x_{k}},\mathbf{y_{k}}\right\rangle" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.2"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.2.2" xref="S3.SS1.p1.3.m3.2.2.3.cmml"><mo id="S3.SS1.p1.3.m3.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.3.cmml">‚ü®</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">ùê±</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.3.cmml">ùê§</mi></msub><mo id="S3.SS1.p1.3.m3.2.2.2.4" xref="S3.SS1.p1.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.cmml"><mi id="S3.SS1.p1.3.m3.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.2.cmml">ùê≤</mi><mi id="S3.SS1.p1.3.m3.2.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.2.2.3.cmml">ùê§</mi></msub><mo id="S3.SS1.p1.3.m3.2.2.2.5" xref="S3.SS1.p1.3.m3.2.2.3.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><list id="S3.SS1.p1.3.m3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2"><apply id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2">ùê±</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3">ùê§</ci></apply><apply id="S3.SS1.p1.3.m3.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.2.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2">ùê≤</ci><ci id="S3.SS1.p1.3.m3.2.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.3">ùê§</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">\left\langle\mathbf{x_{k}},\mathbf{y_{k}}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.2d">‚ü® bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ‚ü©</annotation></semantics></math> is defined as follows:
</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(\mathbf{y}|\mathbf{x},\mathbf{x_{k}},\mathbf{y_{k}};\theta)=\prod_{i=1}^{n}P%
(\mathbf{y_{j}}|\mathbf{x},\mathbf{y_{&lt;j}},\mathbf{x_{k}},\mathbf{y_{k}};%
\theta)," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.3.cmml">P</mi><mo id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.4.cmml">ùê≤</mi><mo fence="false" id="S3.E1.m1.5.5.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ùê±</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">ùê±</mi><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml">ùê§</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml">ùê≤</mi><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml">ùê§</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.5" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Œ∏</mi></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.3" rspace="0.111em" xref="S3.E1.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><munderover id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml"><mo id="S3.E1.m1.5.5.1.1.2.2.2.2" movablelimits="false" xref="S3.E1.m1.5.5.1.1.2.2.2.2.cmml">‚àè</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2.3.2" xref="S3.E1.m1.5.5.1.1.2.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.5.5.1.1.2.2.2.3.1" xref="S3.E1.m1.5.5.1.1.2.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.5.5.1.1.2.2.2.3.3" xref="S3.E1.m1.5.5.1.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.3.cmml">n</mi></munderover><mrow id="S3.E1.m1.5.5.1.1.2.1" xref="S3.E1.m1.5.5.1.1.2.1.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.3" xref="S3.E1.m1.5.5.1.1.2.1.3.cmml">P</mi><mo id="S3.E1.m1.5.5.1.1.2.1.2" xref="S3.E1.m1.5.5.1.1.2.1.2.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2.cmml">ùê≤</mi><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3.cmml">ùê£</mi></msub><mo fence="false" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.4" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.4.cmml">|</mo><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">ùê±</mi><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.4" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2.cmml">ùê≤</mi><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3.cmml">ùê£</mi></mrow></msub><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.5" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2.cmml">ùê±</mi><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3.cmml">ùê§</mi></msub><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.6" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2.cmml">ùê≤</mi><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3.cmml">ùê§</mi></msub><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.7" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">Œ∏</mi></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"></eq><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.2"></times><ci id="S3.E1.m1.5.5.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3">ùëÉ</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3">conditional</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.4">ùê≤</ci><list id="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùê±</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2">ùê±</ci><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3">ùê§</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2">ùê≤</ci><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3">ùê§</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ùúÉ</ci></list></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><apply id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2">product</csymbol><apply id="S3.E1.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3"><eq id="S3.E1.m1.5.5.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3.1"></eq><ci id="S3.E1.m1.5.5.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3.2">ùëñ</ci><cn id="S3.E1.m1.5.5.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.5.5.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3">ùëõ</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1"><times id="S3.E1.m1.5.5.1.1.2.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.2"></times><ci id="S3.E1.m1.5.5.1.1.2.1.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.3">ùëÉ</ci><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.4">conditional</csymbol><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2">ùê≤</ci><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3">ùê£</ci></apply><list id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ùê±</ci><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2">ùê≤</ci><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3"><lt id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3">ùê£</ci></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2">ùê±</ci><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3">ùê§</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2">ùê≤</ci><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3">ùê§</ci></apply><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ùúÉ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">P(\mathbf{y}|\mathbf{x},\mathbf{x_{k}},\mathbf{y_{k}};\theta)=\prod_{i=1}^{n}P%
(\mathbf{y_{j}}|\mathbf{x},\mathbf{y_{&lt;j}},\mathbf{x_{k}},\mathbf{y_{k}};%
\theta),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_P ( bold_y | bold_x , bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ; italic_Œ∏ ) = ‚àè start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_P ( bold_y start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT | bold_x , bold_y start_POSTSUBSCRIPT &lt; bold_j end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ; italic_Œ∏ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.8">where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m1.1"><semantics id="S3.SS1.p1.4.m1.1a"><mi id="S3.SS1.p1.4.m1.1.1" xref="S3.SS1.p1.4.m1.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m1.1b"><ci id="S3.SS1.p1.4.m1.1.1.cmml" xref="S3.SS1.p1.4.m1.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m1.1d">italic_Œ∏</annotation></semantics></math> is a set of model parameters, <math alttext="\mathbf{y_{&lt;j}}=y_{1},...,y_{j-1}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m2.3"><semantics id="S3.SS1.p1.5.m2.3a"><mrow id="S3.SS1.p1.5.m2.3.3" xref="S3.SS1.p1.5.m2.3.3.cmml"><msub id="S3.SS1.p1.5.m2.3.3.4" xref="S3.SS1.p1.5.m2.3.3.4.cmml"><mi id="S3.SS1.p1.5.m2.3.3.4.2" xref="S3.SS1.p1.5.m2.3.3.4.2.cmml">ùê≤</mi><mrow id="S3.SS1.p1.5.m2.3.3.4.3" xref="S3.SS1.p1.5.m2.3.3.4.3.cmml"><mi id="S3.SS1.p1.5.m2.3.3.4.3.2" xref="S3.SS1.p1.5.m2.3.3.4.3.2.cmml"></mi><mo id="S3.SS1.p1.5.m2.3.3.4.3.1" xref="S3.SS1.p1.5.m2.3.3.4.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.5.m2.3.3.4.3.3" xref="S3.SS1.p1.5.m2.3.3.4.3.3.cmml">ùê£</mi></mrow></msub><mo id="S3.SS1.p1.5.m2.3.3.3" xref="S3.SS1.p1.5.m2.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.5.m2.3.3.2.2" xref="S3.SS1.p1.5.m2.3.3.2.3.cmml"><msub id="S3.SS1.p1.5.m2.2.2.1.1.1" xref="S3.SS1.p1.5.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.5.m2.2.2.1.1.1.2" xref="S3.SS1.p1.5.m2.2.2.1.1.1.2.cmml">y</mi><mn id="S3.SS1.p1.5.m2.2.2.1.1.1.3" xref="S3.SS1.p1.5.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.5.m2.3.3.2.2.3" xref="S3.SS1.p1.5.m2.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p1.5.m2.1.1" mathvariant="normal" xref="S3.SS1.p1.5.m2.1.1.cmml">‚Ä¶</mi><mo id="S3.SS1.p1.5.m2.3.3.2.2.4" xref="S3.SS1.p1.5.m2.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.5.m2.3.3.2.2.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.5.m2.3.3.2.2.2.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.2.cmml">y</mi><mrow id="S3.SS1.p1.5.m2.3.3.2.2.2.3" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.cmml"><mi id="S3.SS1.p1.5.m2.3.3.2.2.2.3.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.2.cmml">j</mi><mo id="S3.SS1.p1.5.m2.3.3.2.2.2.3.1" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.1.cmml">‚àí</mo><mn id="S3.SS1.p1.5.m2.3.3.2.2.2.3.3" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m2.3b"><apply id="S3.SS1.p1.5.m2.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3"><eq id="S3.SS1.p1.5.m2.3.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3.3"></eq><apply id="S3.SS1.p1.5.m2.3.3.4.cmml" xref="S3.SS1.p1.5.m2.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.3.3.4.1.cmml" xref="S3.SS1.p1.5.m2.3.3.4">subscript</csymbol><ci id="S3.SS1.p1.5.m2.3.3.4.2.cmml" xref="S3.SS1.p1.5.m2.3.3.4.2">ùê≤</ci><apply id="S3.SS1.p1.5.m2.3.3.4.3.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3"><lt id="S3.SS1.p1.5.m2.3.3.4.3.1.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.5.m2.3.3.4.3.2.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3.2">absent</csymbol><ci id="S3.SS1.p1.5.m2.3.3.4.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3.3">ùê£</ci></apply></apply><list id="S3.SS1.p1.5.m2.3.3.2.3.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2"><apply id="S3.SS1.p1.5.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.1.2">ùë¶</ci><cn id="S3.SS1.p1.5.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.5.m2.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.5.m2.1.1.cmml" xref="S3.SS1.p1.5.m2.1.1">‚Ä¶</ci><apply id="S3.SS1.p1.5.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.5.m2.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.2">ùë¶</ci><apply id="S3.SS1.p1.5.m2.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3"><minus id="S3.SS1.p1.5.m2.3.3.2.2.2.3.1.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.1"></minus><ci id="S3.SS1.p1.5.m2.3.3.2.2.2.3.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.2">ùëó</ci><cn id="S3.SS1.p1.5.m2.3.3.2.2.2.3.3.cmml" type="integer" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.3">1</cn></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m2.3c">\mathbf{y_{&lt;j}}=y_{1},...,y_{j-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m2.3d">bold_y start_POSTSUBSCRIPT &lt; bold_j end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_y start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT</annotation></semantics></math> denotes a sequence of translation prefix tokens at time step <math alttext="j" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m3.1"><semantics id="S3.SS1.p1.6.m3.1a"><mi id="S3.SS1.p1.6.m3.1.1" xref="S3.SS1.p1.6.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m3.1b"><ci id="S3.SS1.p1.6.m3.1.1.cmml" xref="S3.SS1.p1.6.m3.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m3.1d">italic_j</annotation></semantics></math> and <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m4.1"><semantics id="S3.SS1.p1.7.m4.1a"><mi id="S3.SS1.p1.7.m4.1.1" xref="S3.SS1.p1.7.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m4.1b"><ci id="S3.SS1.p1.7.m4.1.1.cmml" xref="S3.SS1.p1.7.m4.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m4.1d">italic_n</annotation></semantics></math> is length of the target sentence <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m5.1"><semantics id="S3.SS1.p1.8.m5.1a"><mi id="S3.SS1.p1.8.m5.1.1" xref="S3.SS1.p1.8.m5.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m5.1b"><ci id="S3.SS1.p1.8.m5.1.1.cmml" xref="S3.SS1.p1.8.m5.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m5.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m5.1d">bold_y</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Similar to the vanilla NMT, we use the maximum likelihood estimation (MLE) loss function to find a set of model parameters on training set <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ùíü</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ùíü</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">caligraphic_D</annotation></semantics></math>. In order to focus the model on learning the target sentence, we utilize only tokens from the target sentence to calculate the loss function instead of the whole output sentence that contains knowledge sequence. Formally, we minimize the following loss function:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\frac{1}{|\mathcal{D}|}\sum_{\mathbf{(x,y)}\in\mathcal{D}}-\log P(%
\mathbf{y|x,x_{k},y_{k}}{;}\theta).\\
" class="ltx_Math" display="block" id="S3.E2.m1.6"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.1.1.3" xref="S3.E2.m1.6.6.1.1.3.cmml">‚Ñí</mi><mo id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml"><mfrac id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mn id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">1</mn><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">ùíü</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.E2.m1.6.6.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.3.1.cmml">‚Å¢</mo><munder id="S3.E2.m1.6.6.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.3.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.3.2.2" movablelimits="false" rspace="0em" xref="S3.E2.m1.6.6.1.1.1.3.2.2.cmml">‚àë</mo><mrow id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml"><mrow id="S3.E2.m1.3.3.2.4.2" xref="S3.E2.m1.3.3.2.4.1.cmml"><mo id="S3.E2.m1.3.3.2.4.2.1" stretchy="false" xref="S3.E2.m1.3.3.2.4.1.cmml">(</mo><mi id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">ùê±</mi><mo id="S3.E2.m1.3.3.2.4.2.2" xref="S3.E2.m1.3.3.2.4.1.cmml">,</mo><mi id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.2.cmml">ùê≤</mi><mo id="S3.E2.m1.3.3.2.4.2.3" stretchy="false" xref="S3.E2.m1.3.3.2.4.1.cmml">)</mo></mrow><mo id="S3.E2.m1.3.3.2.3" xref="S3.E2.m1.3.3.2.3.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.2.5" xref="S3.E2.m1.3.3.2.5.cmml">ùíü</mi></mrow></munder></mrow><mo id="S3.E2.m1.6.6.1.1.1.2" lspace="0em" xref="S3.E2.m1.6.6.1.1.1.2.cmml">‚àí</mo><mrow id="S3.E2.m1.6.6.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.1.3.1.cmml">log</mi><mo id="S3.E2.m1.6.6.1.1.1.1.3a" lspace="0.167em" xref="S3.E2.m1.6.6.1.1.1.1.3.cmml">‚Å°</mo><mi id="S3.E2.m1.6.6.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.4" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.4.cmml">ùê≤</mi><mo fence="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">ùê±</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml">ùê±</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml">ùê§</mi></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.4" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.cmml">ùê≤</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3.cmml">ùê§</mi></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.5" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">Œ∏</mi></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.2" lspace="0em" xref="S3.E2.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1"><eq id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></eq><ci id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.3">‚Ñí</ci><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><minus id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.2"></minus><apply id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3"><times id="S3.E2.m1.6.6.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><divide id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1"></divide><cn id="S3.E2.m1.1.1.3.cmml" type="integer" xref="S3.E2.m1.1.1.3">1</cn><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.3"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></abs><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ùíü</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.3.2">subscript</csymbol><sum id="S3.E2.m1.6.6.1.1.1.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.3.2.2"></sum><apply id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"><in id="S3.E2.m1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.2.3"></in><interval closure="open" id="S3.E2.m1.3.3.2.4.1.cmml" xref="S3.E2.m1.3.3.2.4.2"><ci id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1">ùê±</ci><ci id="S3.E2.m1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2">ùê≤</ci></interval><ci id="S3.E2.m1.3.3.2.5.cmml" xref="S3.E2.m1.3.3.2.5">ùíü</ci></apply></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2"></times><apply id="S3.E2.m1.6.6.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3"><log id="S3.E2.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.1"></log><ci id="S3.E2.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.2">ùëÉ</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3">conditional</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.4">ùê≤</ci><list id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2"><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">ùê±</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2">ùê±</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3">ùê§</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2">ùê≤</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3">ùê§</ci></apply><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">ùúÉ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\mathcal{L}=\frac{1}{|\mathcal{D}|}\sum_{\mathbf{(x,y)}\in\mathcal{D}}-\log P(%
\mathbf{y|x,x_{k},y_{k}}{;}\theta).\\
</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.6d">caligraphic_L = divide start_ARG 1 end_ARG start_ARG | caligraphic_D | end_ARG ‚àë start_POSTSUBSCRIPT ( bold_x , bold_y ) ‚àà caligraphic_D end_POSTSUBSCRIPT - roman_log italic_P ( bold_y | bold_x , bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ; italic_Œ∏ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.1">Note that the proposed method is different from the priming method¬†<cite class="ltx_cite ltx_citemacro_cite">Bulte and Tezcan (<a class="ltx_ref" href="#bib.bib5" title="">2019b</a>); Pham et¬†al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite>. The priming techniques retrieve similar source sentences and corresponding translations, and then the similar sentence pairs can be used as an input prompt for NMT models. First, we train the model based on our proposed loss function Equation¬†<a class="ltx_ref" href="#S3.E2" title="2 ‚Ä£ 3.1 Training ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a> and the NMT model is trained with standard loss function in their works. Also, our method can be applied to multi-knowledge and their work can only be is limited to sentences.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">For model optimization, we adopt a two-stage training strategy. In the first stage, we train the standard NMT model based on the standard training objective using the original training data set. Then, in the second stage, we use a training data set constructed from multiple types of knowledge to learn the model parameters based on Equation¬†<a class="ltx_ref" href="#S3.E2" title="2 ‚Ä£ 3.1 Training ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a>. The proposed model can also be initialized with pre-trained NMT models and then trained on the training data set that contains multi-knowledge.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The model receives the whole input sequence and the target prefix composed of target knowledge sequence during decoding. Before beginning translation sequence generation, the encoder encodes the input sequence, and the decoder encodes the target prefix. The initial steps of the beam search use the given prefix <math alttext="\mathbf{y_{k}}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">ùê≤</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">ùê§</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ùê≤</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ùê§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathbf{y_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> to decode the tokens after the special separator token [Output] in forced decoding mode. The decoder can gain indirect access to whole input sequence tokens while also gaining direct access to target prefix tokens by self-attention and cross-attention mechanisms. It enables the NMT model learn to how to extract and make use of valuable information from the redundant prefixes during training, and use the prefixes to guide the translation process during inference.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Knowledge Acquisition</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We describe the methods employed in this work to how to obtain knowledge from bilingual sentences (sentence knowledge), terminology dictionaries (terminology knowledge) and translation templates (template knowledge).</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieving Similar Sentence</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.6">For each source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">bold_x</annotation></semantics></math>, we retrieve the most similar bilingual sentences <math alttext="\left\langle\mathbf{x_{s}},\mathbf{y_{s}}\right\rangle" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.2"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.2a"><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.3" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml">‚ü®</mo><msub id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3.cmml">ùê¨</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.4" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml">ùê≤</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3.cmml">ùê¨</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.5" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.2b"><list id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2"><apply id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3">ùê¨</ci></apply><apply id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2">ùê≤</ci><ci id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3">ùê¨</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.2c">\left\langle\mathbf{x_{s}},\mathbf{y_{s}}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.2d">‚ü® bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT ‚ü©</annotation></semantics></math> from sentence knowledge. We use token-based edit distance¬†<cite class="ltx_cite ltx_citemacro_cite">Levenshtein (<a class="ltx_ref" href="#bib.bib23" title="">1965</a>)</cite> to calculate the similarity score. Formally, for a given source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">bold_x</annotation></semantics></math>, similarity score <math alttext="\text{sim}(\mathbf{x},\mathbf{x_{s}})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.4.m4.2"><semantics id="S3.SS3.SSS0.Px1.p1.4.m4.2a"><mrow id="S3.SS3.SSS0.Px1.p1.4.m4.2.2" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.cmml"><mtext id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3a.cmml">sim</mtext><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml"><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">(</mo><mi id="S3.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml">ùê±</mi><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.3" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml">ùê¨</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.4" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.4.m4.2b"><apply id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2"><times id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2"></times><ci id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3a.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3"><mtext id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3">sim</mtext></ci><interval closure="open" id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1"><ci id="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1">ùê±</ci><apply id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3">ùê¨</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.4.m4.2c">\text{sim}(\mathbf{x},\mathbf{x_{s}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.4.m4.2d">sim ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT )</annotation></semantics></math> between two source sentences <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px1.p1.5.m5.1a"><mi id="S3.SS3.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.5.m5.1b"><ci id="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.5.m5.1d">bold_x</annotation></semantics></math> and <math alttext="\mathbf{x_{s}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS3.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml">ùê¨</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3">ùê¨</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.6.m6.1c">\mathbf{x_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.6.m6.1d">bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{sim}(\mathbf{x},\mathbf{x_{s}})=1-\frac{{ED}(\mathbf{x},\mathbf{x_{s}})}%
{{max}(|\mathbf{x}|,|\mathbf{x_{s}}|)}," class="ltx_Math" display="block" id="S3.E3.m1.7"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.1.cmml"><mtext id="S3.E3.m1.7.7.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.3a.cmml">sim</mtext><mo id="S3.E3.m1.7.7.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">(</mo><mi id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml">ùê±</mi><mo id="S3.E3.m1.7.7.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">,</mo><msub id="S3.E3.m1.7.7.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml">ùê±</mi><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml">ùê¨</mi></msub><mo id="S3.E3.m1.7.7.1.1.1.1.1.4" stretchy="false" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.7.7.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.7.7.1.1.3" xref="S3.E3.m1.7.7.1.1.3.cmml"><mn id="S3.E3.m1.7.7.1.1.3.2" xref="S3.E3.m1.7.7.1.1.3.2.cmml">1</mn><mo id="S3.E3.m1.7.7.1.1.3.1" xref="S3.E3.m1.7.7.1.1.3.1.cmml">‚àí</mo><mfrac id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml">E</mi><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">‚Å¢</mo><mi id="S3.E3.m1.2.2.2.5" xref="S3.E3.m1.2.2.2.5.cmml">D</mi><mo id="S3.E3.m1.2.2.2.3a" xref="S3.E3.m1.2.2.2.3.cmml">‚Å¢</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mo id="S3.E3.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">ùê±</mi><mo id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.2.cmml">,</mo><msub id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">ùê±</mi><mi id="S3.E3.m1.2.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml">ùê¨</mi></msub><mo id="S3.E3.m1.2.2.2.2.1.4" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.E3.m1.5.5.5" xref="S3.E3.m1.5.5.5.cmml"><mi id="S3.E3.m1.5.5.5.5" xref="S3.E3.m1.5.5.5.5.cmml">m</mi><mo id="S3.E3.m1.5.5.5.4" xref="S3.E3.m1.5.5.5.4.cmml">‚Å¢</mo><mi id="S3.E3.m1.5.5.5.6" xref="S3.E3.m1.5.5.5.6.cmml">a</mi><mo id="S3.E3.m1.5.5.5.4a" xref="S3.E3.m1.5.5.5.4.cmml">‚Å¢</mo><mi id="S3.E3.m1.5.5.5.7" xref="S3.E3.m1.5.5.5.7.cmml">x</mi><mo id="S3.E3.m1.5.5.5.4b" xref="S3.E3.m1.5.5.5.4.cmml">‚Å¢</mo><mrow id="S3.E3.m1.5.5.5.3.2" xref="S3.E3.m1.5.5.5.3.3.cmml"><mo id="S3.E3.m1.5.5.5.3.2.3" stretchy="false" xref="S3.E3.m1.5.5.5.3.3.cmml">(</mo><mrow id="S3.E3.m1.4.4.4.2.1.1.2" xref="S3.E3.m1.4.4.4.2.1.1.1.cmml"><mo id="S3.E3.m1.4.4.4.2.1.1.2.1" stretchy="false" xref="S3.E3.m1.4.4.4.2.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.3.3.3.1" xref="S3.E3.m1.3.3.3.1.cmml">ùê±</mi><mo id="S3.E3.m1.4.4.4.2.1.1.2.2" stretchy="false" xref="S3.E3.m1.4.4.4.2.1.1.1.1.cmml">|</mo></mrow><mo id="S3.E3.m1.5.5.5.3.2.4" xref="S3.E3.m1.5.5.5.3.3.cmml">,</mo><mrow id="S3.E3.m1.5.5.5.3.2.2.1" xref="S3.E3.m1.5.5.5.3.2.2.2.cmml"><mo id="S3.E3.m1.5.5.5.3.2.2.1.2" stretchy="false" xref="S3.E3.m1.5.5.5.3.2.2.2.1.cmml">|</mo><msub id="S3.E3.m1.5.5.5.3.2.2.1.1" xref="S3.E3.m1.5.5.5.3.2.2.1.1.cmml"><mi id="S3.E3.m1.5.5.5.3.2.2.1.1.2" xref="S3.E3.m1.5.5.5.3.2.2.1.1.2.cmml">ùê±</mi><mi id="S3.E3.m1.5.5.5.3.2.2.1.1.3" xref="S3.E3.m1.5.5.5.3.2.2.1.1.3.cmml">ùê¨</mi></msub><mo id="S3.E3.m1.5.5.5.3.2.2.1.3" stretchy="false" xref="S3.E3.m1.5.5.5.3.2.2.2.1.cmml">|</mo></mrow><mo id="S3.E3.m1.5.5.5.3.2.5" stretchy="false" xref="S3.E3.m1.5.5.5.3.3.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow><mo id="S3.E3.m1.7.7.1.2" xref="S3.E3.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1"><eq id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2"></eq><apply id="S3.E3.m1.7.7.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1"><times id="S3.E3.m1.7.7.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.2"></times><ci id="S3.E3.m1.7.7.1.1.1.3a.cmml" xref="S3.E3.m1.7.7.1.1.1.3"><mtext id="S3.E3.m1.7.7.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.3">sim</mtext></ci><interval closure="open" id="S3.E3.m1.7.7.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1"><ci id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6">ùê±</ci><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2">ùê±</ci><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3">ùê¨</ci></apply></interval></apply><apply id="S3.E3.m1.7.7.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.3"><minus id="S3.E3.m1.7.7.1.1.3.1.cmml" xref="S3.E3.m1.7.7.1.1.3.1"></minus><cn id="S3.E3.m1.7.7.1.1.3.2.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.3.2">1</cn><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><divide id="S3.E3.m1.5.5.6.cmml" xref="S3.E3.m1.5.5"></divide><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><ci id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">ùê∏</ci><ci id="S3.E3.m1.2.2.2.5.cmml" xref="S3.E3.m1.2.2.2.5">ùê∑</ci><interval closure="open" id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ùê±</ci><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.2">ùê±</ci><ci id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">ùê¨</ci></apply></interval></apply><apply id="S3.E3.m1.5.5.5.cmml" xref="S3.E3.m1.5.5.5"><times id="S3.E3.m1.5.5.5.4.cmml" xref="S3.E3.m1.5.5.5.4"></times><ci id="S3.E3.m1.5.5.5.5.cmml" xref="S3.E3.m1.5.5.5.5">ùëö</ci><ci id="S3.E3.m1.5.5.5.6.cmml" xref="S3.E3.m1.5.5.5.6">ùëé</ci><ci id="S3.E3.m1.5.5.5.7.cmml" xref="S3.E3.m1.5.5.5.7">ùë•</ci><interval closure="open" id="S3.E3.m1.5.5.5.3.3.cmml" xref="S3.E3.m1.5.5.5.3.2"><apply id="S3.E3.m1.4.4.4.2.1.1.1.cmml" xref="S3.E3.m1.4.4.4.2.1.1.2"><abs id="S3.E3.m1.4.4.4.2.1.1.1.1.cmml" xref="S3.E3.m1.4.4.4.2.1.1.2.1"></abs><ci id="S3.E3.m1.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.1">ùê±</ci></apply><apply id="S3.E3.m1.5.5.5.3.2.2.2.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1"><abs id="S3.E3.m1.5.5.5.3.2.2.2.1.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.2"></abs><apply id="S3.E3.m1.5.5.5.3.2.2.1.1.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.3.2.2.1.1.1.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.5.5.5.3.2.2.1.1.2.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1.2">ùê±</ci><ci id="S3.E3.m1.5.5.5.3.2.2.1.1.3.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1.3">ùê¨</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">\text{sim}(\mathbf{x},\mathbf{x_{s}})=1-\frac{{ED}(\mathbf{x},\mathbf{x_{s}})}%
{{max}(|\mathbf{x}|,|\mathbf{x_{s}}|)},</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.7d">sim ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT ) = 1 - divide start_ARG italic_E italic_D ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT ) end_ARG start_ARG italic_m italic_a italic_x ( | bold_x | , | bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT | ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.8">where <math alttext="ED(\mathbf{x},\mathbf{x_{s}})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.1.m1.2"><semantics id="S3.SS3.SSS0.Px1.p2.1.m1.2a"><mrow id="S3.SS3.SSS0.Px1.p2.1.m1.2.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3.cmml">E</mi><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2.cmml">‚Å¢</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4.cmml">D</mi><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2a" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2.cmml">‚Å¢</mo><mrow id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml"><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml">ùê±</mi><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3.cmml">ùê¨</mi></msub><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.4" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.1.m1.2b"><apply id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2"><times id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2"></times><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3">ùê∏</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4">ùê∑</ci><interval closure="open" id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1"><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1">ùê±</ci><apply id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3">ùê¨</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.1.m1.2c">ED(\mathbf{x},\mathbf{x_{s}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.1.m1.2d">italic_E italic_D ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT )</annotation></semantics></math> denotes the Edit Distance between <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p2.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.2.m2.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.2.m2.1d">bold_x</annotation></semantics></math> and <math alttext="\mathbf{x_{s}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p2.3.m3.1a"><msub id="S3.SS3.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml">ùê¨</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3">ùê¨</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.3.m3.1c">\mathbf{x_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.3.m3.1d">bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="|\mathbf{x}|" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.4.m4.1"><semantics id="S3.SS3.SSS0.Px1.p2.4.m4.1a"><mrow id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.cmml"><mo id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2.1" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.1.cmml">|</mo><mi id="S3.SS3.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.cmml">ùê±</mi><mo id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.4.m4.1b"><apply id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2"><abs id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2.1"></abs><ci id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1">ùê±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.4.m4.1c">|\mathbf{x}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.4.m4.1d">| bold_x |</annotation></semantics></math> is the length of <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.5.m5.1"><semantics id="S3.SS3.SSS0.Px1.p2.5.m5.1a"><mi id="S3.SS3.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.5.m5.1b"><ci id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.5.m5.1d">bold_x</annotation></semantics></math>. <math alttext="\mathbf{x_{s}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.6.m6.1"><semantics id="S3.SS3.SSS0.Px1.p2.6.m6.1a"><msub id="S3.SS3.SSS0.Px1.p2.6.m6.1.1" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3.cmml">ùê¨</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.6.m6.1b"><apply id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3">ùê¨</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.6.m6.1c">\mathbf{x_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.6.m6.1d">bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT</annotation></semantics></math> is a source sentence from the sentence knowledge. Each source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.7.m7.1"><semantics id="S3.SS3.SSS0.Px1.p2.7.m7.1a"><mi id="S3.SS3.SSS0.Px1.p2.7.m7.1.1" xref="S3.SS3.SSS0.Px1.p2.7.m7.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.7.m7.1b"><ci id="S3.SS3.SSS0.Px1.p2.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.7.m7.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.7.m7.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.7.m7.1d">bold_x</annotation></semantics></math> is compared to all the sources from the sentence knowledge using the similarity score. We ignore perfect matches and keep the single best match sentence pair if its similarity score is higher than a specified threshold¬†<math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.8.m8.1"><semantics id="S3.SS3.SSS0.Px1.p2.8.m8.1a"><mi id="S3.SS3.SSS0.Px1.p2.8.m8.1.1" xref="S3.SS3.SSS0.Px1.p2.8.m8.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.8.m8.1b"><ci id="S3.SS3.SSS0.Px1.p2.8.m8.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m8.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.8.m8.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.8.m8.1d">italic_Œª</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Matching Terminology</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.6">Terminology dictionaries specify phrase-level corresponding relationships of a sentence pair. When two matching terminologies from a sentence pair have overlapping ranges, a ‚Äòhard‚Äô match selects only one of them as matching results, such as maximum matching using the longest matching terminologies. The match strategy causes boundary errors, which could negatively impact the quality of the translation. Therefore, we adopt a ‚Äòsoft‚Äô match strategy to utilize all matching terminologies from sentence pairs. For each source <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.1.m1.1d">bold_x</annotation></semantics></math> and the corresponding target <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.2.m2.1d">bold_y</annotation></semantics></math>, we record any matching bilingual terminologies that are fully contained in both the source <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">ùê±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1">ùê±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.3.m3.1d">bold_x</annotation></semantics></math> and target <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.4.m4.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.4.m4.1d">bold_y</annotation></semantics></math>. For each sentence pair, source tokens of the matching terminologies are concatenated into <math alttext="\mathbf{x_{tm}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px2.p1.5.m5.1a"><msub id="S3.SS3.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml">ùê≠ùê¶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3">ùê≠ùê¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.5.m5.1c">\mathbf{x_{tm}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.5.m5.1d">bold_x start_POSTSUBSCRIPT bold_tm end_POSTSUBSCRIPT</annotation></semantics></math> with the special token [Term], and corresponding target tokens are similarly concatenated into <math alttext="\mathbf{y_{tm}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS3.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml">ùê≤</mi><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml">ùê≠ùê¶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2">ùê≤</ci><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3">ùê≠ùê¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.6.m6.1c">\mathbf{y_{tm}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.6.m6.1d">bold_y start_POSTSUBSCRIPT bold_tm end_POSTSUBSCRIPT</annotation></semantics></math>. The NMT model learns to automatically choose the proper terminologies based on the terminological context by redundant prefixes that contain the all matching bilingual terminologies.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Translation Template Prediction</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">To construct translation template sequence <math alttext="\mathbf{x_{tp}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml">ùê±</mi><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml">ùê≠ùê©</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2">ùê±</ci><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3">ùê≠ùê©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">\mathbf{x_{tp}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.1.m1.1d">bold_x start_POSTSUBSCRIPT bold_tp end_POSTSUBSCRIPT</annotation></semantics></math>, we follow¬†<cite class="ltx_cite ltx_citemacro_cite">Yang et¬†al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite> to extract templates from a sub-tree by pruning the nodes deeper than a specific depth on the sentence corresponding constituency-based parser tree. We gain a parallel training data using the source sentences, extracted source and target templates. The constructed data is employed to train a sequence generation model to predict target template sequence. The model is to take the source sentence and corresponding source template sequence as inputs and generate template sequence as outputs.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we validate the effectiveness of the proposed approach on translation quality and terminology match accuracy by comparing with the previous methods used only a single type of knowledge.
We evaluate translation quality with the case-insensitive detokenized SacreBLEU score¬†<cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="#bib.bib33" title="">2018</a>)</cite> and terminology match accuracy with exact match accuracy¬†<cite class="ltx_cite ltx_citemacro_cite">Anastasopoulos et¬†al. (<a class="ltx_ref" href="#bib.bib2" title="">2021</a>)</cite> which is defined as the ratio between the number of matched source term translations in the output and the total number of source terms.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Corpus</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We evaluate our approach on English-Chinese (En-Zh) and English-German (En-De) translation tasks. For English-German, we use the WMT16 dataset as the training corpus of our model, consisting of 4.5M sentence pairs. We randomly divided the corpus into 4,000 sentences for the validation set and the rest for training. For English-Chinese, we train our model on CCMT2022 Corpus, containing 8.2M sentence pairs. The WMT newsdev2017 is used as the validation set.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">We measure the effectiveness of our model on multi-domain test sets. For English-German, we use the multi-domain English-German parallel data¬†<cite class="ltx_cite ltx_citemacro_cite">Aharoni and Goldberg (<a class="ltx_ref" href="#bib.bib1" title="">2020</a>)</cite> as in-domain test sets, which include IT, Medical, Koran, and Law. For English-Chinese, We use the multi-domain English-Chinese parallel dataset¬†<cite class="ltx_cite ltx_citemacro_cite">Tian et¬†al. (<a class="ltx_ref" href="#bib.bib40" title="">2014</a>)</cite> as in-domain test sets, including Subtitles, News and Education. To distinguish the multi-domain sets for testing and the WMT16 or CCMT2022 sets for training, we call the multi-domain datasets as the in-domain training set or in-domain test set. We use only the training sets to train the models and evaluate results on in-domain test sets in our experiments. We retrieve similar sentence pairs from corresponding in-domain training sets for each in-domain test set. We used randomly selected 2,000 sentences from UM-Corpus as the validation sets of Fine-Tuning models. The sentence statistics of datasets are illustrated in Table¬†<a class="ltx_ref" href="#S4.T1" title="Table 1 ‚Ä£ Corpus ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Domain</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">Train</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">Vaild</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">Test</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.2.1" rowspan="3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T1.1.2.2.1.1">En-Zh</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Subtitles</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">298K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">579</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.3.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">News</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">448K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">1,500</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Education</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">448K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">790</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.1" rowspan="4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T1.1.5.5.1.1">En-De</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">IT</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.5.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">223K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.5.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.5.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.6.6.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Medical</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">248K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.7.7.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Law</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">467K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.1.8.8.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Koran</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.8.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">18K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.8.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.8.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The number of training, validation, and test data sets of English-German and English-Chinese multi-domains.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p3.2">For all datasets, we tokenize English and German text with Moses¬†<span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/moses-smt/mosesdecoder</span></span></span> and the Chinese text with Jieba¬†<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/fxsjy/jieba</span></span></span> tokenizer. We train a joint Byte Pair Encoding (BPE)¬†<cite class="ltx_cite ltx_citemacro_cite">Sennrich et¬†al. (<a class="ltx_ref" href="#bib.bib35" title="">2016</a>)</cite> with 32k merge operations and use a joint vocabulary for both source and target text. The models in all experiments follow the state-of-the-art Transformer base architecture¬†<cite class="ltx_cite ltx_citemacro_cite">Vaswani et¬†al. (<a class="ltx_ref" href="#bib.bib41" title="">2017</a>)</cite> implemented in the Fairseq toolkit¬†<cite class="ltx_cite ltx_citemacro_cite">Ott et¬†al. (<a class="ltx_ref" href="#bib.bib31" title="">2019</a>)</cite>. The models are trained on 4 NVIDIA V100 GPUs and optimized with Adam algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a class="ltx_ref" href="#bib.bib21" title="">2015</a>)</cite> with <math alttext="{\beta}_{1}=0.9" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p3.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p3.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2.cmml">Œ≤</mi><mn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p3.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1"><eq id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2">ùõΩ</ci><cn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3">1</cn></apply><cn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p3.1.m1.1c">{\beta}_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p3.1.m1.1d">italic_Œ≤ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="{\beta}_{2}=0.98" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.2.m2.1"><semantics id="S4.SS1.SSS0.Px1.p3.2.m2.1a"><mrow id="S4.SS1.SSS0.Px1.p3.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2.cmml">Œ≤</mi><mn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p3.2.m2.1b"><apply id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1"><eq id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2">ùõΩ</ci><cn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3">2</cn></apply><cn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml" type="float" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p3.2.m2.1c">{\beta}_{2}=0.98</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p3.2.m2.1d">italic_Œ≤ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98</annotation></semantics></math>. We set the learning rate to 0.0007. In all experiments, the dropout rate is set to 0.3 for English-German and 0.1 for English-Chinese. We use early stopping with a patience of 30 for all experiments. We averaged the last 5 checkpoints in all testing.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Baseline</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We compare our approach with the following representative baselines:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Vanilla NMT</span>¬†<cite class="ltx_cite ltx_citemacro_cite">Vaswani et¬†al. (<a class="ltx_ref" href="#bib.bib41" title="">2017</a>)</cite>: We directly train a standard Transformer base model using the training set.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Fine-Tuning</span>: The model is fine-tuned using each in-domain training data set based on vanilla NMT. As a single-domain model with an upper bound on the performance, it loses the ability of multi-domain adaption.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.3"><math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i3.p1.1.m1.1"><semantics id="S4.I1.i3.p1.1.m1.1a"><mi id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><ci id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.3.1">NN-MT</span>¬†<cite class="ltx_cite ltx_citemacro_cite">Khandelwal et¬†al. (<a class="ltx_ref" href="#bib.bib20" title="">2021</a>)</cite>: The non-parametric method combines a NMT model with token-level <math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i3.p1.2.m2.1"><semantics id="S4.I1.i3.p1.2.m2.1a"><mi id="S4.I1.i3.p1.2.m2.1.1" xref="S4.I1.i3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.2.m2.1b"><ci id="S4.I1.i3.p1.2.m2.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.2.m2.1d">italic_k</annotation></semantics></math>-nearest-neighbor(<math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i3.p1.3.m3.1"><semantics id="S4.I1.i3.p1.3.m3.1a"><mi id="S4.I1.i3.p1.3.m3.1.1" xref="S4.I1.i3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.3.m3.1b"><ci id="S4.I1.i3.p1.3.m3.1.1.cmml" xref="S4.I1.i3.p1.3.m3.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.3.m3.1d">italic_k</annotation></semantics></math>NN) by retrieving relevant token examples. It uses an in-domain training data set for domain adaptation tasks without additional training. The datastore is generated by an in-domain training set.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Priming-NMT</span>¬†<cite class="ltx_cite ltx_citemacro_cite">Pham et¬†al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite>: It only uses similar sentences as prefixes of a NMT model to force the model to generate a translation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">VecConstNMT</span>¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="#bib.bib44" title="">2022b</a>)</cite>: It vectorizes and integrates lexical constraints (matching terminologies) into NMT models by attention modules. The method outperforms several strong baselines, including the works¬†<cite class="ltx_cite ltx_citemacro_cite">Song et¬†al. (<a class="ltx_ref" href="#bib.bib37" title="">2019</a>); Chen et¬†al. (<a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">En-De</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">En-Zh</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1">Sentence</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">33.57%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">39.28%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1">Terminology</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.3.2.2">42.66%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.3.2.3">18.23%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Percentage of source sentences with similar sentences and with matching terminologies on the training sets.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:433.6pt;height:246.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.7pt,3.2pt) scale(0.974485597836756,0.974485597836756) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.2.2.3.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S4.T3.2.2.3.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.2.1">English-German</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T3.2.2.3.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.3.1">English-Chinese</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.1.1">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.2.2.4.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.2.1">Method (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.4.2.2.1.1">Knowledge</span>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.3.1">IT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.4.1">Medical</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.5.1">Law</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.2.2.4.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.6.1">Koran</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="S4.T3.2.2.4.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.7.1">Avg.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.8.1">Subtitles</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.9.1">News</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.2.2.4.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.10.1">Education</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.11" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.11.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.5.1.1" rowspan="6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T3.2.2.5.1.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.2.5.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">40.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">56.68</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.5.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">28.08</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T3.2.2.5.1.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">27.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">33.91</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.5.1.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.47</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.6.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.6.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Vanilla NMT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">23.07</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">30.72</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.6.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">10.16</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.6.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.34</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.6.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.54</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.44</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.7.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.7.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">VecConstNMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.7.3.1.1">Term.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">23.74</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">30.41</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.7.3.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">10.21</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.7.3.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.70</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.61</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.7.3.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.8.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.8.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Priming-NMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.8.4.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.93</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.94</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.8.4.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">11.41</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.8.4.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">29.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.79</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.83</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.8.4.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.83</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.15</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<math alttext="k" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">italic_k</annotation></semantics></math>NN-MT (<span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">17.66</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.1.1.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.23</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.07</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.1.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">51.32</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.57</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.9.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.9.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.9.5.1.1">Ours</span> (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.9.5.1.2">Term.+Sent.+Temp.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">32.43</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.9.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">18.89</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.9.5.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.62</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">40.13</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.9.5.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">55.25</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.10.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.2.10.6.1" rowspan="6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T3.2.2.10.6.1.1">Exact Match</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.2.10.6.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">59.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">69.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">66.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.10.6.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.45</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T3.2.2.10.6.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">58.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.10.6.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">65.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">59.14</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.11.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.11.7.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Vanilla NMT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.09</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.92</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.11.7.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">19.09</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.11.7.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.60</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">59.17</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">55.50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.11.7.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">60.75</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">58.47</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.12.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.12.8.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">VecConstNMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.12.8.1.1">Term.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.91</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.12.8.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">77.27</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.12.8.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.51</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">88.01</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.12.8.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.39</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.57</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.13.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.13.9.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Priming-NMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.13.9.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.48</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.13.9.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">15.45</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.13.9.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.08</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">66.86</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">56.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.13.9.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">66.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">63.33</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<math alttext="k" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mi id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">italic_k</annotation></semantics></math>NN-MT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.2.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.56</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">61.15</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">61.16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.55</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.61</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.30</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">63.55</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.91</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.14.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.2.2.14.10.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.14.10.1.1">Ours</span> (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.14.10.1.2">Term.+Sent.+Temp.</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">86.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.2.2.14.10.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S4.T3.2.2.14.10.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">84.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.2.2.14.10.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">90.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.74</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results on the English-German and English-Chinese multi-domain test sets, reported on BLEU and exact match accuracy of terminology. <span class="ltx_text ltx_font_italic" id="S4.T3.6.1">Term.</span>, <span class="ltx_text ltx_font_italic" id="S4.T3.7.2">Sent.</span> and <span class="ltx_text ltx_font_italic" id="S4.T3.8.3">Temp.</span> indicate terminology, sentence and template knowledge, respectively.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Training Data</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">The similar sentence pairs are extracted from an training data set using a specified similarity threshold of 0.4 in our experiments. For terminology knowledge, we extract a bilingual terminology dictionary from the training data set using a term extraction tool TM2TB¬†<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/luismond/tm2tb</span></span></span> with default parameters and use the dictionary to match each source and target sentences in the training data set by the ‚Äòsoft‚Äô match strategy. We use Stanford parser¬†<cite class="ltx_cite ltx_citemacro_cite">Manning et¬†al. (<a class="ltx_ref" href="#bib.bib30" title="">2014</a>)</cite> to generate source and target templates based on a specific depth 4 from the training data. We build our method‚Äôs training data by combining corresponding the similar sentence pair, matching terminologies and translation templates for each sentence pair from the training data set. Table¬†<a class="ltx_ref" href="#S4.T2" title="Table 2 ‚Ä£ Baseline ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a> provides the percentage of source sentences with a similar sentence pair where the score is higher than the similarity threshold of 0.4 and the percentage of sentences with matching terminologies in the training data set. We use the vanilla NMT to train our proposed models based on the training data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Test Data</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">For each test set of multi-domain sets, we retrieve similar sentence pairs from the training data set and corresponding in-domain training data set. The number of terminologies extracted by TM2TB with the default threshold of 0.9 is not sufficient to validate terminology accuracy in the in-domain test sets. Therefore, we use TM2TB with a similarity threshold of 0.7 to extract the bilingual terminologies from the in-domain test set, and then use the terminologies to match each sentence pair. We train a model based on the pre-trained model mBART¬†<cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a class="ltx_ref" href="#bib.bib28" title="">2020</a>)</cite> using source sentences, source and target templates extracted from parse tree on in-domain training data. Then we use the model to predict target templates of in-domain test data.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:433.6pt;height:197pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.1pt,10.0pt) scale(0.907680323070687,0.907680323070687) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<td class="ltx_td ltx_border_tt" colspan="2" id="S4.T4.1.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T4.1.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.2.1">English-German</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T4.1.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.3.1">English-Chinese</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.2.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.1.1">Metric</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.2.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T4.1.1.2.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.2.2.3.1">Knowledge</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.4.1">IT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.5.1">Medical</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.6.1">Law</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.2.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.7.1">Koran</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S4.T4.1.1.2.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.8.1">Avg.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.9.1">Subtitles</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.10.1">News</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.2.2.11" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.11.1">Education</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.12" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.12.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.3.1" rowspan="5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T4.1.1.3.3.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Priming-NMT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.3.3.3.1">Sent.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">11.41</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.3.3.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">29.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.83</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.12" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.4.4.1" rowspan="4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.4.1.1">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.4.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.4.4.2.1">Term.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.4.4.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">10.33</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.4.4.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">33.67</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.4.4.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">33.49</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.5.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.5.5.1.1">Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">29.49</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">13.86</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.5.5.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">39.03</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.5.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.39</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.6.6.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.6.6.1.1">Term.+Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.34</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.74</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.77</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">13.61</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.6.6.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">39.95</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.73</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.74</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.7.7.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.7.7.1.1">Term.+Sent.+Temp.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">32.43</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.7.7.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">18.89</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.7.7.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">40.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.7.7.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">55.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.42</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.1.8.8.1" rowspan="5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T4.1.1.8.8.1.1">Exact Match</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.8.8.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">VecConstNMT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.8.8.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.8.8.3.1">Term.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.87</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.8.8.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">77.27</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.8.8.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">88.01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.8.8.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.12" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.57</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.1.9.9.1" rowspan="4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.9.9.1.1">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.9.9.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.9.9.2.1">Term.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">88.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.09</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.9.9.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">87.27</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.9.9.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.9.9.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">97.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">93.88</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.10.10.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.10.10.1.1">Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">39.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">56.83</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.72</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.10.10.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">23.64</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.10.10.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.49</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.10.10.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">61.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.08</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.11.11.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.11.11.1.1">Term.+Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.05</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">86.89</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.97</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.11.11.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">78.18</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.11.11.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.31</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.91</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.11.11.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.52</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">93.25</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.1.1.12.12.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.12.12.1.1">Term.+Sent.+Temp.</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">86.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.12.12.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S4.T4.1.1.12.12.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">84.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.12.12.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">90.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.74</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation result on BLEU and exact match accuracy using only partial type of knowledge on the English-German and English-Chinese.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">Table¬†<a class="ltx_ref" href="#S4.T3" title="Table 3 ‚Ä£ Baseline ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">3</span></a> shows the BLEU and exact match accuracy on Fine-Tuning, NMT, VecConstNMT, <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math>NN-MT, Priming-NMT, and our proposed method on the English-German and English-Chinese multi-domain test data sets. Our method outperforms all baselines in terms of exact match accuracy on average, demonstrating the benefits of integrating sentence, terminology and template knowledge into NMT models. On English-German multi-domain test sets, our method improves an average of 10.74 BLEU and 49.93% exact match accuracy over vanilla NNT. Compared with Priming-NMT using sentence knowledge, our method enhances performance by up to 7.48 BLEU on average. Our method outperforms than <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_k</annotation></semantics></math>NN-MT in the IT and Koran domains.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.2">For English-Chinese multi-domain test sets, our method performs better than Fine-Tuning. The Subtitles, News and Education three domains training data contains noises, such as sentence pairs or domain mismatches. The performance improvement of Fine-Tuning relied on the quality of the in-domain the training data is not significant. Similarly, the performance of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math>NN-MT depends on training data quality, and our method achieves better BLEU in News and Education domains compared to <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_k</annotation></semantics></math>NN-MT. Therefore, our approach has stronger generalization ability and significant performance improvements in these domains compared to the baselines.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.3.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><math alttext="\lambda\geq 0.4" class="ltx_Math" display="inline" id="S4.T5.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml"><mi id="S4.T5.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.m1.1.1.2.cmml">Œª</mi><mo id="S4.T5.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.m1.1.1.1.cmml">‚â•</mo><mn id="S4.T5.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.m1.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1"><geq id="S4.T5.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1.1"></geq><ci id="S4.T5.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.m1.1.1.2">ùúÜ</ci><cn id="S4.T5.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T5.1.1.1.m1.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\lambda\geq 0.4</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.m1.1d">italic_Œª ‚â• 0.4</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><math alttext="\lambda\geq 0.5" class="ltx_Math" display="inline" id="S4.T5.2.2.2.m1.1"><semantics id="S4.T5.2.2.2.m1.1a"><mrow id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml"><mi id="S4.T5.2.2.2.m1.1.1.2" xref="S4.T5.2.2.2.m1.1.1.2.cmml">Œª</mi><mo id="S4.T5.2.2.2.m1.1.1.1" xref="S4.T5.2.2.2.m1.1.1.1.cmml">‚â•</mo><mn id="S4.T5.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1"><geq id="S4.T5.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1.1"></geq><ci id="S4.T5.2.2.2.m1.1.1.2.cmml" xref="S4.T5.2.2.2.m1.1.1.2">ùúÜ</ci><cn id="S4.T5.2.2.2.m1.1.1.3.cmml" type="float" xref="S4.T5.2.2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\lambda\geq 0.5</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.m1.1d">italic_Œª ‚â• 0.5</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.3.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><math alttext="\lambda\geq 0.6" class="ltx_Math" display="inline" id="S4.T5.3.3.3.m1.1"><semantics id="S4.T5.3.3.3.m1.1a"><mrow id="S4.T5.3.3.3.m1.1.1" xref="S4.T5.3.3.3.m1.1.1.cmml"><mi id="S4.T5.3.3.3.m1.1.1.2" xref="S4.T5.3.3.3.m1.1.1.2.cmml">Œª</mi><mo id="S4.T5.3.3.3.m1.1.1.1" xref="S4.T5.3.3.3.m1.1.1.1.cmml">‚â•</mo><mn id="S4.T5.3.3.3.m1.1.1.3" xref="S4.T5.3.3.3.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><apply id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1"><geq id="S4.T5.3.3.3.m1.1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1.1"></geq><ci id="S4.T5.3.3.3.m1.1.1.2.cmml" xref="S4.T5.3.3.3.m1.1.1.2">ùúÜ</ci><cn id="S4.T5.3.3.3.m1.1.1.3.cmml" type="float" xref="S4.T5.3.3.3.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">\lambda\geq 0.6</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.3.m1.1d">italic_Œª ‚â• 0.6</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.3.4.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">IT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.4.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">32.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.4.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.4.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">30.91</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.3.5.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Medical</th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.14</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.41</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.85</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.3.6.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Law</th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.10</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.76</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.3.7.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Koran</th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.7.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">18.89</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.7.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">17.56</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.7.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">16.38</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.3.8.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Avg</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.8.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.8.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.67</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.8.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.46</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Effect of the threshold <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T5.5.m1.1"><semantics id="S4.T5.5.m1.1b"><mi id="S4.T5.5.m1.1.1" xref="S4.T5.5.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.T5.5.m1.1c"><ci id="S4.T5.5.m1.1.1.cmml" xref="S4.T5.5.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.m1.1e">italic_Œª</annotation></semantics></math> for similar sentence retrieval on BLUE on the English to German.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this subsection, we perform ablation experiments on proposed models in order to better understand their relative importance. Table¬†<a class="ltx_ref" href="#S4.T4" title="Table 4 ‚Ä£ Test Data ‚Ä£ 4.1 Setup ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">4</span></a> shows evaluation results of the proposed model using only one or two type of knowledge on in-domain test sets. Our proposed method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Sent.</span>) using sentence knowledge outperforms the strong baseline Priming-NMT by 5.11 BLEU on English-German on average, which indicates that the loss function Equation¬†<a class="ltx_ref" href="#S3.E2" title="2 ‚Ä£ 3.1 Training ‚Ä£ 3 Approach ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a> could significantly enhance the performance during training. Our method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">Term.</span>) using terminology knowledge outperforms the strong baseline VecConstNMT in terms of exact match accuracy on average.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Our method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">Term.+Sent.</span>) using sentence and terminology knowledge achieves both BLEU and exact match accuracy improvements compared with our methods used only sentence or terminology knowledge. When sentence, terminology and template knowledge are used simultaneously, our method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">Term.+Sent.+Temp.</span>) outperforms the method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.3">Term.+Sent.</span>) using sentence and terminology knowledge by 2 BLEU on English-German and by 0.68 BLEU on average on English-Chinese on average respectively, which shows that the translation templates could effectively improve the translation performance.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.3" style="width:433.6pt;height:290.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(136.3pt,-91.4pt) scale(2.69328576898435,2.69328576898435) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T6.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">IT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">Medical</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">Law</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">Koran</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T6.3.1.2.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Term.</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">4.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">5.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.1.2.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">2.9</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.3.1.3.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Sent.</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">6.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">12.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">17.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.3.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">11.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">12.1</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.3.1.4.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Temp.</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">5.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">4.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.4.3.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">8.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">5.2</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.3.1.5.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Know.</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">13.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">21.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.5.4.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">19.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">20.2</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T6.3.1.6.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Speed</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T6.3.1.6.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Relative inference speed for our method compared to the vanilla NMT in English to German multi-domain test sets. The batch size is <math alttext="32" class="ltx_Math" display="inline" id="S4.T6.2.m1.1"><semantics id="S4.T6.2.m1.1b"><mn id="S4.T6.2.m1.1.1" xref="S4.T6.2.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.T6.2.m1.1c"><cn id="S4.T6.2.m1.1.1.cmml" type="integer" xref="S4.T6.2.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.m1.1d">32</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.m1.1e">32</annotation></semantics></math>. #Term., #Sent., #Temp., and #Know. indicate the average number of tokens of matching terminologies, similar sentences, predicted templates and whole knowledge sequences on the target, respectively.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Effect of Similarity Threshold <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS4.1.m1.1"><semantics id="S4.SS4.1.m1.1b"><mi id="S4.SS4.1.m1.1.1" xref="S4.SS4.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.1.m1.1c"><ci id="S4.SS4.1.m1.1.1.cmml" xref="S4.SS4.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.1.m1.1e">italic_Œª</annotation></semantics></math>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Most works¬†<cite class="ltx_cite ltx_citemacro_cite">Bulte and Tezcan (<a class="ltx_ref" href="#bib.bib4" title="">2019a</a>); Xu et¬†al. (<a class="ltx_ref" href="#bib.bib46" title="">2020</a>); Pham et¬†al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite> use 0.5 or 0.6 as a similarity threshold. Table¬†<a class="ltx_ref" href="#S4.T5" title="Table 5 ‚Ä£ 4.2 Main Results ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">5</span></a> shows the effect of the threshold for similar sentence
retrieval on translation quality. We find that retrieving similar sentences using a lower threshold leads to improvements. The average best performance on in-domain test sets can be achieved by our method based on the 0.4 threshold.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Inference Speed</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.2">We report the inference speed of our approach relative to the vanilla NMT in Table¬†<a class="ltx_ref" href="#S4.T6" title="Table 6 ‚Ä£ 4.3 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">6</span></a>. We use the multiple types of knowledge as prefixes of the encoder and decoder of the NMT model, increasing the extra calculating time during decoding. The speed of the proposed method using beam search is 1.6<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mo id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">‚àº</annotation></semantics></math>1.9 times slower than the vanilla NMT and mainly depends on the number of tokens of the prefixes on the target during decoding. Compared to <math alttext="k" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><mi id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><ci id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">italic_k</annotation></semantics></math>NN-MT with a generation speed that is two orders of magnitude slower than the vanilla NMT, our method is more easily acceptable in terms of inference speed.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we propose a unified framework to integrate multi-knowledge into NMT models. We utilize multiple types of knowledge as prefixes of the encoder and decoder of NMT models, which guides the NMT model‚Äôs translation process. Especially, our approaches do not actually require the model to see the domain-specific data in training. The model has automatic domain adaption capability and can be extended to new domains without updating parameters. The experimental results on multi-domain translation tasks demonstrated that incorporating multiple types of knowledge into NMT models leads to significant improvements in both translation quality and exact match accuracy.
</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Monolingual data is valuable to improve the translation quality of NMT models. In the future, we would like to integrate monolingual knowledge into the NMT model. Furthermore, our approach can be applied for tasks where there are multiple types of knowledge, such as Question Answering and Image to Text.
</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">As with the majority of studies, the design of the current approach is subject to limitations. We integrate multiple types of knowledge as additional prefixes of NMT models and add time consumption in the training and inference stages. The experimental results show the added time cost of the proposed method is acceptable. Our approach depends on multiple types of knowledge and obtaining the knowledge may be difficult in some practical applications.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni and Goldberg (2020)</span>
<span class="ltx_bibblock">
Roee Aharoni and Yoav Goldberg. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2004.02105" title="">Unsupervised domain
clusters in pretrained language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anastasopoulos et¬†al. (2021)</span>
<span class="ltx_bibblock">
Antonios Anastasopoulos, Laurent Besacier, James Cross, Matthias Gall√©,
Philipp Koehn, Vassilina Nikoulina, et¬†al. 2021.

</span>
<span class="ltx_bibblock">On the evaluation of machine translation for terminology consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2106.11891</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et¬†al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared¬†D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et¬†al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in neural information processing systems</em>,
33:1877‚Äì1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulte and Tezcan (2019a)</span>
<span class="ltx_bibblock">
Bram Bulte and Arda Tezcan. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1175" title="">Neural fuzzy repair:
Integrating fuzzy matches into neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1800‚Äì1809, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulte and Tezcan (2019b)</span>
<span class="ltx_bibblock">
Bram Bulte and Arda Tezcan. 2019b.

</span>
<span class="ltx_bibblock">Neural fuzzy repair: Integrating fuzzy matches into neural machine
translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">57th Annual Meeting of the
Association-for-Computational-Linguistics (ACL)</em>, pages 1800‚Äì1809.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et¬†al. (2021)</span>
<span class="ltx_bibblock">
Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. 2021.

</span>
<span class="ltx_bibblock">Neural machine translation with monolingual translation memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 7307‚Äì7318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao and Xiong (2018)</span>
<span class="ltx_bibblock">
Qian Cao and Deyi Xiong. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1340" title="">Encoding gated
translation memory into neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 3042‚Äì3047, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. (2021)</span>
<span class="ltx_bibblock">
Guanhua Chen, Yun Chen, and Victor¬†O.K. Li. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v35i14.17496" title="">Lexically
constrained neural machine translation with explicit alignment guidance</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
35(14):12630‚Äì12638.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. (2020)</span>
<span class="ltx_bibblock">
Guanhua Chen, Yun Chen, Yong Wang, and Victor¬†O.K. Li. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.24963/ijcai.2020/496" title="">Lexical-constraint-aware neural machine translation via data augmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Twenty-Ninth International Joint
Conference on Artificial Intelligence, IJCAI-20</em>, pages 3587‚Äì3593.
International Joint Conferences on Artificial Intelligence Organization.

</span>
<span class="ltx_bibblock">Main track.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinu et¬†al. (2019)</span>
<span class="ltx_bibblock">
Georgiana Dinu, Prashant Mathur, Marcello Federico, and Yaser Al-Onaizan. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1294" title="">Training neural machine
translation to apply terminology constraints</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3063‚Äì3068, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dougal and Lonsdale (2020)</span>
<span class="ltx_bibblock">
Duane¬†K. Dougal and Deryle Lonsdale. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.lrec-1.593" title="">Improving NMT
quality using terminology injection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Twelfth Language Resources and Evaluation
Conference</em>, pages 4820‚Äì4827, Marseille, France. European Language Resources
Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farajian et¬†al. (2017)</span>
<span class="ltx_bibblock">
M.¬†Amin Farajian, Marco Turchi, Matteo Negri, and Marcello Federico. 2017.

</span>
<span class="ltx_bibblock">Multi-domain neural machine translation through unsupervised
adaptation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Conference on Machine Translation</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et¬†al. (2018)</span>
<span class="ltx_bibblock">
Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor¬†OK Li. 2018.

</span>
<span class="ltx_bibblock">Search engine guided neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume¬†32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. (2021)</span>
<span class="ltx_bibblock">
Qiuxiang He, Guoping Huang, Qu¬†Cui, Li¬†Li, and Lemao Liu. 2021.

</span>
<span class="ltx_bibblock">Fast and accurate neural machine translation with translation memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3170‚Äì3180.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. (2019)</span>
<span class="ltx_bibblock">
Qiuxiang He, Guoping Huang, Lemao Liu, and Li¬†Li. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-32233-5_29" title="">Word position
aware translation memory for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Natural Language Processing and Chinese Computing: 8th CCF
International Conference, NLPCC 2019, Dunhuang, China, October 9‚Äì14, 2019,
Proceedings, Part I</em>, page 367‚Äì379, Berlin, Heidelberg. Springer-Verlag.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hokamp and Liu (2017)</span>
<span class="ltx_bibblock">
Chris Hokamp and Qun Liu. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1141" title="">Lexically constrained
decoding for sequence generation using grid beam search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1535‚Äì1546,
Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et¬†al. (2019)</span>
<span class="ltx_bibblock">
J.¬†Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt
Post, and Benjamin Van¬†Durme. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1090" title="">Improved lexically
constrained decoding for translation and monolingual rewriting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 839‚Äì850, Minneapolis,
Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et¬†al. (2021)</span>
<span class="ltx_bibblock">
Guoping Huang, Lemao Liu, Xing Wang, Longyue Wang, Huayang Li,
Zhaopeng Tu, Chengyan Huang, and Shuming Shi. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2105.13072" title="">TranSmart: A Practical
Interactive Machine Translation System</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv e-prints</em>, page arXiv:2105.13072.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jon et¬†al. (2021)</span>
<span class="ltx_bibblock">
Josef Jon, Jo√£o¬†Paulo Aires, Dusan Varis, and Ond≈ôej Bojar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.311" title="">End-to-end
lexically constrained machine translation for morphologically rich
languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 4019‚Äì4033,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et¬†al. (2021)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=7wCBOfJ8hJM" title="">Nearest neighbor
machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik¬†P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1412.6980" title="">Adam: A method for
stochastic optimization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Senellart (2010)</span>
<span class="ltx_bibblock">
Philipp Koehn and Jean Senellart. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2010.jec-1.4" title="">Convergence of
translation memory and statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Second Joint EM+/CNGL Workshop: Bringing
MT to the User: Research on Integrating MT in the Translation Industry</em>,
pages 21‚Äì32, Denver, Colorado, USA. Association for Machine Translation in
the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein (1965)</span>
<span class="ltx_bibblock">
Vladimir¬†I. Levenshtein. 1965.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Soviet physics. Doklady</em>, 10:707‚Äì710.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. (2019)</span>
<span class="ltx_bibblock">
Huayang Li, Guoping Huang, Deng Cai, and Lemao Liu. 2019.

</span>
<span class="ltx_bibblock">Neural machine translation with noisy lexical constraints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 28:1864‚Äì1874.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. (2016)</span>
<span class="ltx_bibblock">
Chunyang Liu, Yang Liu, Maosong Sun, Huanbo Luan, and Heng Yu. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1097" title="">Agreement-based
learning of parallel lexicons and phrases from non-parallel corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1024‚Äì1033,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. (2019a)</span>
<span class="ltx_bibblock">
Yang Liu, Kun Wang, Chengqing Zong, and Keh-Yih Su. 2019a.

</span>
<span class="ltx_bibblock">A unified framework and models for integrating translation memory
into phrase-based statistical machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Computer Speech &amp; Language</em>, 54:176‚Äì206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. (2019b)</span>
<span class="ltx_bibblock">
Yang Liu, Kun Wang, Chengqing Zong, and Keh-Yih Su. 2019b.

</span>
<span class="ltx_bibblock">A unified framework and models for integrating translation memory
into phrase-based statistical machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Comput. Speech Lang.</em>, 54:176‚Äì206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00343" title="">Multilingual denoising
pre-training for neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Transactions of the Association for Computational Linguistics</em>,
8:726‚Äì742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et¬†al. (2011)</span>
<span class="ltx_bibblock">
Yanjun Ma, Yifan He, Andy Way, and Josef van Genabith. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P11-1124" title="">Consistent translation
using discriminative learning - a translation memory-inspired approach</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies</em>, pages
1239‚Äì1248, Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning et¬†al. (2014)</span>
<span class="ltx_bibblock">
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard,
and David McClosky. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P14-5010" title="">The Stanford
CoreNLP natural language processing toolkit</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations</em>, pages 55‚Äì60, Baltimore,
Maryland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et¬†al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4009" title="">fairseq: A fast,
extensible toolkit for sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics (Demonstrations)</em>,
pages 48‚Äì53, Minneapolis, Minnesota. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et¬†al. (2020)</span>
<span class="ltx_bibblock">
Minh¬†Quang Pham, Jitao Xu, Josep-Maria Crego, Jean Senellart, and
Fran√ßois Yvon. 2020.

</span>
<span class="ltx_bibblock">Priming neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Conference on Machine Translation</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in
reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186‚Äì191, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post and Vilar (2018)</span>
<span class="ltx_bibblock">
Matt Post and David Vilar. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1119" title="">Fast lexically
constrained decoding with dynamic beam allocation for neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1314‚Äì1324, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et¬†al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1162" title="">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715‚Äì1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang et¬†al. (2021)</span>
<span class="ltx_bibblock">
Wei Shang, Chong Feng, Tianfu Zhang, and Da¬†Xu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/IJCNN52387.2021.9533734" title="">Guiding
neural machine translation with retrieved translation template</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2021 International Joint Conference on Neural Networks
(IJCNN)</em>, pages 1‚Äì7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et¬†al. (2019)</span>
<span class="ltx_bibblock">
Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, and Min Zhang. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1044" title="">Code-switching for
enhancing NMT with pre-specified translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 449‚Äì459, Minneapolis,
Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Susanto et¬†al. (2020)</span>
<span class="ltx_bibblock">
Raymond¬†Hendy Susanto, Shamil Chollampatt, and Liling Tan. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.325" title="">Lexically
constrained neural machine translation with Levenshtein transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3536‚Äì3543, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et¬†al. (2016)</span>
<span class="ltx_bibblock">
Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li, and P.¬†Yu. 2016.

</span>
<span class="ltx_bibblock">Neural machine translation with external phrase memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ArXiv</em>, abs/1606.01792.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et¬†al. (2014)</span>
<span class="ltx_bibblock">
Liang Tian, Derek¬†F. Wong, Lidia¬†S. Chao, Paulo Quaresma, Francisco Oliveira,
Yi¬†Lu, Shuo Li, Yiming Wang, and Longyue Wang. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/774_Paper.pdf" title="">UM-corpus: A large English-Chinese parallel corpus for statistical
machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC‚Äô14)</em>, pages 1837‚Äì1842, Reykjavik,
Iceland. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et¬†al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan¬†N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 31st International Conference on Neural
Information Processing Systems</em>, NIPS‚Äô17, page 6000‚Äì6010, Red Hook, NY,
USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2013)</span>
<span class="ltx_bibblock">
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P13-1002" title="">Integrating translation
memory into phrase-based machine translation during decoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 11‚Äì21, Sofia,
Bulgaria. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2022a)</span>
<span class="ltx_bibblock">
Shuo Wang, Peng Li, Zhixing Tan, Zhaopeng Tu, Maosong Sun, and Yang Liu.
2022a.

</span>
<span class="ltx_bibblock">A template-based method for constrained neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ArXiv</em>, abs/2205.11255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2022b)</span>
<span class="ltx_bibblock">
Shuo Wang, Zhixing Tan, and Yang Liu. 2022b.

</span>
<span class="ltx_bibblock">Integrating vectorized lexical constraints for neural machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2203.12210</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et¬†al. (2019)</span>
<span class="ltx_bibblock">
Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v33i01.33017297" title="">Graph based
translation memory for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the Thirty-Third AAAI Conference on
Artificial Intelligence and Thirty-First Innovative Applications of
Artificial Intelligence Conference and Ninth AAAI Symposium on Educational
Advances in Artificial Intelligence</em>, AAAI‚Äô19/IAAI‚Äô19/EAAI‚Äô19. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et¬†al. (2020)</span>
<span class="ltx_bibblock">
Jitao Xu, Josep Crego, and Jean Senellart. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.144" title="">Boosting
neural machine translation with similar translations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1580‚Äì1590, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamada (2011)</span>
<span class="ltx_bibblock">
Masaru Yamada. 2011.

</span>
<span class="ltx_bibblock">The effect of translation memory databases on productivity.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et¬†al. (2020)</span>
<span class="ltx_bibblock">
Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.531" title="">Improving
neural machine translation with soft template prediction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 5979‚Äì5989, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2018a)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, and Maosong Sun.
2018a.

</span>
<span class="ltx_bibblock">Prior knowledge integration for neural machine translation using
posterior regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:1811.01100</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2021)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, and Yang
Liu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TASLP.2021.3057831" title="">Neural machine
translation with explicit phrase alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 29:1001‚Äì1010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2018b)</span>
<span class="ltx_bibblock">
Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, and Satoshi
Nakamura. 2018b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1120" title="">Guiding neural machine
translation with retrieved translation pieces</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1325‚Äì1335, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Dec  8 02:46:35 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
