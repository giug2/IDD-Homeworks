<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation</title>
<!--Generated on Fri Jun 14 14:46:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.10091v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S1" title="In Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S2" title="In Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3" title="In Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data and methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS1" title="In 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS2" title="In 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Human evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS3" title="In 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Machine evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS4" title="In 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Computing correlations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S4" title="In Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S5" title="In Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Ethical considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S6" title="In Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoman Wang
<br class="ltx_break"/>University of Leeds
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">mlxwang@leeds.ac.uk</span> <span class="ltx_ERROR undefined" id="id2.2.id2">\And</span>Claudio Fantinuoli
<br class="ltx_break"/>KUDO
<br class="ltx_break"/>University of Mainz
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">fantinuoli@uni-mainz.de</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Assessing the performance of interpreting services is a complex task, given the nuanced nature of spoken language translation, the strategies that interpreters apply, and the diverse expectations of users. The complexity of this task become even more pronounced when automated evaluation methods are applied. This is particularly true because interpreted texts exhibit less linearity between the source and target languages due to the strategies employed by the interpreter.</p>
<p class="ltx_p" id="id5.id2">This study aims to assess the reliability of automatic metrics in evaluating simultaneous interpretations by analyzing their correlation with human evaluations. We focus on a particular feature of interpretation quality, namely translation accuracy or faithfulness. As a benchmark we use human assessments performed by language experts, and evaluate how well sentence embeddings and Large Language Models correlate with them. We quantify semantic similarity between the source and translated texts without relying on a reference translation. The results suggest GPT models, particularly GPT-3.5 with direct prompting, demonstrate the strongest correlation with human judgment in terms of semantic similarity between source and target texts, even when evaluating short textual segments. Additionally, the study reveals that the size of the context window has a notable impact on this correlation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The assessment of interpreting quality is a common practice in both professional interpreting and academic contexts. The results of these evaluations offer valuable insights for a wide range of stakeholders, including interpreter’s clients, users, practitioners, educators, certification bodies, and researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx21" title="">Han, 2022</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Assessing quality in interpretation is a complex endeavor. Quality is not only challenging to measure, but it manifests also an “elusive nature” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx3" title="">Becerra et al., 2013</a>, p. 7]</cite> making it difficult to define. The notion of quality in fact may vary from one user to another, introducing a substantial degree of subjectivity in determining what constitutes a good translation of speech. Furthermore, the criteria for quality are contingent upon the type of interpretation involved. For instance, in conference interpreting, the emphasis is generally on the quality of the interpreter’s output, encompassing aspects such as content, language, and delivery. In contrast, within community settings like social and healthcare interpreting, interactional competencies and discourse management play a crucial role in determining what quality is <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx24" title="">Kalina, 2012</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Traditionally, the assessment of interpreting performances has been carried out manually, a methodology that comes with its own set of pros and cons. On the positive side, human evaluations offer a holistic view of quality by taking into account various facets of the communication process, thereby delivering a more nuanced understanding of interpreting performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx37" title="">Pöchhacker, 2002</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx3" title="">Becerra et al., 2013</a>]</cite>. Conversely, manual assessment comes with its own set of challenges, including being labor-intensive, time-consuming and costly  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx48" title="">Wu, 2011</a>]</cite>. Furthermore, the results often have limited generalizability due to either the restricted scope of the data sampled or the inherent complexities associated with evaluating spoken translation.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In light of the limitations, there has been a growing interest to apply automatic metrics to the evaluation of interpreting performances. While traditional statistical metrics like BLEU have shown limited efficacy in capturing translation quality from a user’s perspective, the emergence of semantic vectors and pre-trained, large-scale generative language models has yielded promising results, especially in the domain of written translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx25" title="">Kocmi and Federmann, 2023</a>]</cite>. The application of these metrics is gradually extending to the field of spoken translation as well <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx17" title="">Han and Lu, 2021b</a>]</cite>. However, it must be mentioned that orally translated texts possess certain characteristics that might restrict the efficacy of employing metrics designed for written texts. Interpreters, especially in the simultaneous modality, tend to alter the text more extensively than translators, modifying the structure and omitting parts deliberately as a strategy rather than a deficiency. For example, interpreters may omit part of the original when experiencing cognitive overload, when they cannot comprehend the original message, to name just a few <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx26" title="">Korpal, 2012</a>]</cite>. This non-linearity between source and target texts renders the task of automatic evaluation even more challenging.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The adoption of easy accessible and robust automatic evaluation in interpreting offers several potential applications that could benefit a wide range of stakeholders. Firstly, the ability to provide instant feedback to trainees and practitioners would enable them to quickly assess their performance and pinpoint areas for improvement, also in real-time, creating a faster feedback loop that could substantially accelerate autonomous skill development. Secondly, automatic evaluation might aid organizations in consistently and objectively monitoring the quality of their multilingual services. Thirdly, automatic metrics that correlate with human judgments might serve as a useful tool for the continuous evaluation of machine interpretation.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This study addresses two primary questions: First, is there an automatic metric that aligns closely with human judgment and can thus be used to automate the accuracy evaluation of spoken language translation? Second, do these metrics evaluate human-generated translations, machine-generated translations, or both more effectively?</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The rest of the paper is organised as follows. In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S2" title="2 Related work ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">2</span></a> we present an overview of research in the field of automatic evaluation of interpreting performances. In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3" title="3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, we illustrate our research methodology, our data and the experimental design. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS1" title="3.1 Dataset ‣ 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a> describes the dataset created for this task. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS2" title="3.2 Human evaluation ‣ 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">3.2</span></a> describes the process for human evaluation of the translations while Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S3.SS3" title="3.3 Machine evaluation ‣ 3 Data and methodology ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a> delves on the process followed for the automatic evaluation. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S4" title="4 Results ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S5" title="5 Ethical considerations ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">5</span></a> introduces some ethical implications. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S6" title="6 Conclusions ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">6</span></a> concludes the paper with some discussion and remarks.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The evaluation of translation quality and in particular of accuracy or information fidelity, i.e. the correspondence between source-language and target-language renditions, has traditionally differed between computer science, with its tradition of abundant use of automatic metrics, and translation and interpreting community, with its focus on manual evaluation as perceived by experts and users.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In computer science, evaluation metrics such as Bilingual Evaluation Understudy (BLEU) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx34" title="">Papineni et al., 2002</a>]</cite>, National Institute of Standards and Technology (NIST) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx11" title="">Doddington, 2002</a>]</cite>, Metric for Evaluation of Translation with Explicit Ordering (METEOR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx2" title="">Banerjee and Lavie, 2005</a>]</cite>, and Translation Edit Rate (TER) have been foundational in establishing benchmarks for Machine Translation Quality Estimation (MTQE). BLEU and NIST emphasise n-gram precision, with NIST uniquely weighting distinct n-grams. METEOR integrates both recall and precision, while TER quantifies requisite edits for optimal translation alignment. However, recent scholarly discourse have suggested that these metrics, while valuable, may possess intrinsic limitations in encapsulating the multifaceted subtleties and overarching context of linguistic structures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx13" title="">Fernandes et al., 2023</a>]</cite>. This acknowledgement has precipitated the exploration of advanced, data-driven methodologies for MTQE without references. Neural networks, characterized by their bio-inspired architectures, emerge as a compelling alternative. These computational structures excel in managing voluminous datasets, discerning intricate patterns, and, crucially, accounting for the inherent complexities associated with linguistic phenomena.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In the field of neural network architectures, the potential of Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and the groundbreaking Transformer for semantic similarity computations has been explored. Of these, Transformer-based models like BERT and GPT have gained considerable academic traction due to their outstanding performance across numerous Natural Language Processing (NLP) tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx45" title="">Wang et al., 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx25" title="">Kocmi and Federmann, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx8" title="">Clark et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx49" title="">Xenouleas et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx50" title="">Yang et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx4" title="">Brown et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx22" title="">Hendy et al., 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx7" title="">Clark et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx44" title="">Vaswani et al., 2017</a>]</cite>. Central to these architectures is the concept of embeddings: dense vector representations that capture the semantic essence of words or textual segments. Within this high-dimensional space, vectors situated closely denote semantic relatedness. In translation evaluation, embeddings offer a mediating semantic layer, enabling comparisons between source and target linguistic structures. However, the embeddings landscape is complex. Models from the Universal Sentence Encoder Multilingual (USEM) to the Generative Pre-trained Transformer (GPT) produce embeddings with varied purposes. For example, USEM is geared towards retrieving semantically aligned entities, while GPT emphasizes generating context-rich linguistic constructs. These nuances highlight the need for researchers to thoughtfully choose models aligned with their specific research goals.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In interpreting studies, a subdomain of translation studies dedicated to oral translation, the traditional practice has been to assess accuracy manually, with or without references. Many of the evaluation methodologies are derived by written translation. For the assessment based on references, also known as intra-lingua assessment, the notion of ”tertium comparisons” stands as a pivotal benchmark within a particular language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx42" title="">Setton and Motta, 2007</a>]</cite>. Tracing the historical evolution of this methodology, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx5" title="">Carroll, 1966</a>]</cite> stands out as a foundational contributor. He experimented with lay people to ascertain accuracy in translations. Building on Carroll’s scale, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx43" title="">Tiselius, 2009</a>]</cite> refined the process, integrating references to spoken language and interpreting for intra-lingual assessment. These approaches, while receiving considerable acceptance from seasoned interpreters, are not without limitations. The impact of cognitive-linguistic factors can potentially alter evaluation results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx18" title="">Han and Zhao, 2021</a>]</cite>. Moreover, intra-lingual evaluations face challenges in adapting to changing contexts and demographics and may lack a universally acknowledged point of reference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx42" title="">Setton and Motta, 2007</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Academic discussions prioritise evaluation methods for gauging accuracy in inter-language interpreting. These methods range from error analysis, as seen in works by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx14" title="">Gerver, 1969</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx15" title="">Gile, 1995</a>]</cite> that identify translation inaccuracies, to propositional analysis, endorsed by researchers like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx31" title="">Mackintosh, 1983</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx27" title="">Lee, 1999a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx28" title="">Lee, 1999b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx29" title="">Lee, 2002</a>]</cite>, which examines textual accuracy. However, these methods present challenges in addressing linguistic subtleties and differing interpretations. More recent research emphasizes grading rubrics, tracing back to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx5" title="">Carroll, 1966</a>]</cite>, which outline performance across competency tiers and are validated in multiple studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx19" title="">Han, 2016</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx20" title="">Han, 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx33" title="">Nia and Modarresi, 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx46" title="">Wu et al., 2013</a>]</cite>. Yet, even with proven reliability, this rubric-based evaluation faces hurdles like the development and validation of rubric descriptors and evaluator inconsistencies.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">A few studies have explored the effectiveness of various metrics in evaluating the translation quality or interpreting performances. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx6" title="">Chung, 2020</a>]</cite>, for instance, pinpointed the strong alignment between human evaluations and scores determined by BLEU and METEOR for German-to-Korean translation. Subsequent studies by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx16" title="">Han and Lu, 2021a</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx30" title="">Lu and Han, 2022</a>]</cite> reinforced the merit of these automated tools. Han and Lu (2021) discerned that METEOR’s sentence-level evaluations resonated more with human assessments than broader, text-level evaluations. Conversely, Lu and Han’s (2022) exploration, fortified with the integration of the BERT model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx10" title="">Devlin et al., 2019</a>]</cite>, showcased substantial correlations between human and automated evaluations, underlining the potential of these metrics in assessing interpreting performances. A recent study by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx25" title="">Kocmi and Federmann, 2023</a>]</cite> employed Large Language Models like GPT to evaluate translation quality across three language pairs, concluding that only models GPT3.5 and above possess the capability for such translation quality assessment.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">While initial studies have underscored positive and moderate-to-strong correlations for MT metrics such as BLEU and METEOR, to the best of our knowledge no research has been conducted so far on the use of language models for reference-free interpreting assessment. Our study aims to fill this gap.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data and methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The dataset used for the study consists of 12 original speeches in English translated into Spanish, each lasting approximately 5 minutes. These videos were curated from a broader selection of real-life contexts, including lectures, business presentations, live tutorials, and political addresses.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The dataset is available under the Creative Commons 4.0 License at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/renawang26/Information_fidelity</span></span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Although the corpus size is inherently limited, in order to allow high quality human evaluation, the selection of videos was strategically designed to capture a spectrum of speech features. The speeches were distributed equally in terms of gender, with 6 from male speakers and 6 from female speakers. In addition, the accent of the speakers comprises both native and non-native speakers. The nature of the speeches is diversified into three categories: 4 corporate, 4 political, and 4 general presentations. The speeches comprise 3529 tokens.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">For the evaluation purpose, each video was simultaneously interpreted in two ways:</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Translation H</span>: Human professional interpreters were engaged. Three interpreters, native Spanish speakers, were involved, each responsible for translating four videos. Simultaneous interpreters were required to interpret the entire video (approximately 5 minutes) to preserve the contextual information essential for accurate interpretation. However, only 2 minutes of the videos were randomly selected for the evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Translation M</span>: Machine interpretation was carried out by the KUDO AI Speech Translator, the only system available for simultaneous translation available at the moment of writing <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">www.kudo.ai</span></span></span></span>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">All recordings were automatically transcribed, and the transcriptions were post-edited for accuracy by expert linguists proficient in both English and Spanish. The goal of this operation was to make sure that the transcripts did not contain errors of transcription. The transcriptions were manually aligned based on semantic units, a critical step due to the absence of formal punctuation commonly found in written texts. These segments are roughly comparable to sentences or smaller paragraphs. The average length of segments is 29.41 tokens.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Human evaluation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The human evaluation process was guided by the methodology proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx12" title="">Fantinuoli and Prandi, 2021</a>]</cite>, which uses a Likert scale to assess two key features of interpretation: accuracy (ability of the translation to convey the meaning of the original) and intellegibility (ability of the translation of being understandable). The two dimensions reflect the main criteria at the core of the product-oriented approach to quality evaluation in Interpreting Studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx43" title="">Tiselius, 2009</a>]</cite>. For the purposes of this study, however, the focus was exclusively on the feature of informativeness, i.e. accuracy, leaving the assessment of intellegibility and any other potential feature for future research. One of the advantages of this framework lies in its being user-centric and in line with the corpus-based evaluation already established in Interpreting Studies to assess the quality of human interpretation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The human evaluation was conducted using a diverse group of 18 evaluators. This consisted of 9 professional interpreters and 9 bilingual individuals without any translation or interpreting experience. The goal was to capture a broad and unbiased evaluation of the translations, taking into account both professional expertise and everyday bilingual proficiency. Each evaluator was assigned 4 videos to evaluate. They were informed that the translations were transcriptions of oral simultaneous interpretations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">For each speech, the raters were asked to assess on a six-point Likert scale first the intelligibility of the output (without a comparison with the source speech nor a comparison between the two outputs), then to evaluate the accuracy of the renditions by comparing each one to the source speech.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">An important feature of the evaluation process was the anonymity of the translation sources. Evaluators were not informed whether the translations were produced by a human or by a machine. This was a deliberate step to prevent any evaluation bias, ensuring that the judgment was strictly based on the quality of the translation, irrespective of the producer.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">It is important to point out that with a value of 0.0964 the interater agreement is low (“slight agreement” on a Fleiss’ Kappa scale). This aspect showcases the intrinsic complexity of objectively assessing spoken language translation due to different expectations by the evaluators about what constitutes accuracy. This is an insidious limitation of the evaluation of human interpretation (see Han 2022). While the low agreement between multiple raters is expected, it also limits the generalizability of our findings.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Machine evaluation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our approach to the machine assessment of spoken language translation is based on the concept of semantic similarity leveraging sentence embeddings and large language model prompting techniques.
The rationale behind using embeddings to measure semantic similarity is multifold, as it proffers a host of advantages, such as the provision of a continuous representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx32" title="">Mikolov et al., 2013</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx35" title="">Pennington et al., 2014</a>]</cite>, incorporation of contextual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx10" title="">Devlin et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx36" title="">Peters et al., 1802</a>]</cite>, dimensionality reduction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx40" title="">Roweis and Saul, 2000</a>]</cite>, applicability of transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx23" title="">Howard and Ruder, 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx39" title="">Raffel et al., 2020</a>]</cite>, multilingual support <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx9" title="">Conneau et al., 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx47" title="">Wu et al., 2016</a>]</cite>, interoperability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx41" title="">Ruder et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx1" title="">Artetxe et al., 2018</a>]</cite>, ease of use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx38" title="">Radford et al., 2018</a>]</cite>, and state-of-the-art performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx44" title="">Vaswani et al., 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#bib.bibx4" title="">Brown et al., 2020</a>]</cite> in NLP tasks. The advantage of using sentence embeddings over MTQE models for assessing interpreting performance lies in the ability of embeddings to evaluate without the need for references. This approach contrasts with some MTQE models, which typically depend on references for quality assessments. Furthermore, sentence embeddings are adaptable across a broad spectrum of languages and text genres, offering a versatile solution for evaluating interpreting performance. This flexibility is beneficial across different domains and contexts, whereas MTQE models often necessitate more specific training data to achieve comparable effectiveness. The fundamental operation of this methodology involves vectorising each sentence in both the source and target texts. Essentially, this means mapping each sentence to its corresponding vectors of real numbers, thereby projecting them into a shared multi-dimensional space. This conversion of textual data into numerical format empowers the machine to elaborate the semantics of the texts effectively. The subsequent step involves the calculation of cosine similarity, which serves as a measure of the similarity between each language pair.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We employed three neural network models to carry out sentence embedding: all-MiniLM-L6-v2<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</span></span></span></span>, Generative Pre-trained Transformer (GPT), and in particular GPT-Ada. model<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://platform.openai.com/docs/api-reference/embeddings</span></span></span></span>, and Universal Sentence Encoder Multilingual<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3</span></span></span></span> (USEM). By integrating the all-MiniLM-L6-v2 for its efficient, compact design suitable for multi-language applications, alongside GPT-Ada for their advanced generative capabilities and adaptability, and USEM for its extensive language coverage and cross-lingual semantic understanding, this strategy offers a balanced and comprehensive approach. The embeddings obatained werer vectorised sentences in both English and Spanish. The sentence embeddings computed with these models were later used to calculate the Cosine Similarity between the source texts and the translations.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In addition to the models for computing word vectors, we tested another method to compute semantic similarity leveraging the prompt functionality of GPT3.5<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.openai.com</span></span></span></span>. The Large Language Model was assigned the task of assessing the semantic similarity between pairs of sentences, one in English and the other in Spanish, using a scale ranging from 1 to 5. An example prompt provided was: ”Given the two sentences in English and Spanish, rate from 1 to 5 their similarity, where 1 is not similar and 5 very similar.”</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Computing correlations</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The human and automatic assessments were put together in an evaluation matrix. Pearson’s correlation coefficients were leveraged to explore the relationships between human and machine evaluations. Specifically, the correlations between human judgments and the cosine similarities derived from the embeddings of segments from the source speech and their corresponding translations (Translation H and Translation M) generated by three models. were calculated. As stated before, the overarching aim was to probe the feasibility of achieving machine-human parity in the evaluation results.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Since the neural network models chosen for this experiment have limited reliably in computing semantic vectors for long texts, we opted to establish a correlation between human and machine evaluations at the segment level. It is essential to emphasize that similarities values obtained from isolated segment pairs have intrinsic limitations since they are not able to consider accuracy across segments. Thus, the quantity of tokens utilized as context for sentence embeddings could potentially affect the model’s contextual comprehension and, subsequently, the precision of semantic similarity assessments.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">To take this into consideration, we examined the effect of “window size”, i.e. the number of segments combined into a single one. For this purpose, we computed similarities for window sizes up to five segments. The systematic variation in window size aimed to shed light on how semantic similarity between human and machine evaluations could potentially be influenced by the availability of cross-segment context.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To analyse the data, we devised charts from three perspectives, including the comparison of correlation values among evaluation methods, a comparison between Translation H and Translation M, and correlation values based on window size.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10091v1#S4.F1" title="Figure 1 ‣ 4 Results ‣ Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation"><span class="ltx_text ltx_ref_tag">1</span></a> we compare the distribution of correlation values across all machine evaluation methods, namely GPT-3.5, all-MiniLM-L6-v2, GPT-Ada, and UMSE, for both Translation H and Translation M.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The correlation with GPT-3.5 displays the highest median correlation value. The interquartile range (IQR) is also quite narrow, indicating that the correlation values for this method are consistently high and well-aligned with human evaluations. The correlation with all-MiniLM-L6-v2 has a wide IQR, showcasing varied performance. The median value is close to zero, but there are negative outliers, indicating instances where the machine evaluation is inversely related to human judgment. The correlation values with GPT-Ada are relatively consistent, with a narrow IQR. The median is slightly above 0.3, which indicates moderate alignment with human judgments. UMSE’s performance seems to be close to GPT-Ada with a median slightly above 0.3. The IQR is a bit larger, suggesting a bit more variability in the correlation values.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="164" id="S4.F1.g1" src="extracted/5667843/Figs/translation_comparison_boxplot_very_large_font.png" width="283"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Correlations among machine evaluation methods</figcaption>
</figure>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">For the comparison between Translation H and Translation M in Figure 2, paired bar charts elucidate the average correlation disparities for each machine evaluation method. GPT-3.5’s measurements exhibit robust correlation values with human judgments for both translations, although Translation H marginally outperforms Translation M. For all-MiniLM-L6-v2, the correlation of Translation H gravitates towards zero, whereas Translation M registers a negative value, implying a potential misalignment of the all-MiniLM-L6-v2’s evaluations with human perspectives, predominantly for Translation M. GPT-Ada embeddings yield nearly identical correlation values for both translations, but with Translation H slightly edging out. Intriguingly, UMSE’s embeddings produce a higher correlation value for Translation M compared to Translation H.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="S4.F2.g1" src="extracted/5667843/Figs/translation_comparison_barchart_very_large_font.png" width="283"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Correlations for Translation H and Translation M</figcaption>
</figure>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Turning to the third perspective, which examines the shift in correlation values based on window size, line charts in Figure 3 and Figure 4 offer insights into this dynamic for each machine evaluation method. For Translation H (Human Translated) in Figure 3, GPT-Ada correlation with human ratings sees a mild fluctuation across window sizes, initially decreasing from window size-1 to size-2, then slightly rising in the following window size2-5. The all-MiniLM-L6-v2 model, in contrast, exhibits a downward trend, indicating reduced agreement with human evaluations as window size grows. UMSE consistently maintains a stable correlation with human ratings, showing only minor variations across different window sizes. GPT-3.5 presents a distinct pattern; while its correlation initially drops from size-1 to size-2, it surges notably in the subsequent window, outperforming the other models.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">In observations for Translation M (Machine Translated) in Figure 4, GPT-Ada begins with a positive correlation with human ratings with size-1, but this declines as the window size expands, hinting at potential metric inconsistencies for broader contexts. The all-MiniLM-L6-v2 model’s correlation, on the other hand, commences positively by size-1 and consistently rises with the window size, pointing to a more aligned evaluation with human judgment for larger translation segments. UMSE’s performance mirrors its evaluation with human translations, maintaining stability across all window sizes and showcasing its consistent metric evaluation. In contrast, GPT-3.5’s correlation fluctuates considerably across window sizes — experiencing a drop, a subsequent rise, and then another decline — indicating a variable level of concurrence with human assessments depending on the window size.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="118" id="S4.F3.g1" src="extracted/5667843/Figs/Translation_H_chart.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Correlations for Translation H according to window size</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="118" id="S4.F4.g1" src="extracted/5667843/Figs/Translation_M_chart.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Correlation for Translation M according to window size</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Ethical considerations</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The adoption of automatic evaluation also raises ethical concerns that warrant careful consideration. One potential issue is the possibility to continuously monitor interpreters, which might infringe on privacy rights and create a sense of constant surveillance, negatively impacting job satisfaction and professional autonomy. Additionally, decisions regarding the employability of individuals could be blindly given to mechanical means, potentially leading to unjust or biased outcomes if the algorithms fail to account for contextual nuances or other essential aspects of human communication. As such, it is crucial for the language industry to carefully weigh the benefits and challenges of automatic evaluation, ensuring that ethical considerations are addressed as advancements in AI technology continue to reshape the landscape.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study aimed to analyze the correlation between automated and human evaluations of translated content. The peculiarity of this experiment is that we focused on a specific form of translation: the simultaneous interpretation of English speeches into Spanish. This mode of translation introduces unique challenges to assessment due to the nonlinear nature of the output (in spoken translation, the differences between the source and target can be more pronounced than in written translation) and varying user expectations regarding interpretation quality. We evaluated both interpretations provided by professional interpreters and those produced by a machine interpretation system. The objective was to develop a metric reflecting interpreting quality in a manner consistent with human judgment.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The direct prompting of GPT-3.5 for quality estimation on a Likert scale exhibits the highest median correlation with human evaluation. This finding establishes GPT-3.5 as the most promising tool among the evaluated methods to gauge translation quality, both for interpretations produced by humans and machines. GPT-3.5 benefits from a larger context, performing better with larger segment windows. This suggests that the model can capture and evaluate long dependencies more effectively.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Contrary to expectations, GPT-3.5’s correlation with human judgment is somewhat stronger for translations produced by professional interpreters than for machine-generated ones. This implies that GPT might be subtly more attuned to the linguistic nuances of human translation, even though it remains adept at evaluating speech translation. One possible explanation for this is that human interpreters often introduce subtle contextual, tonal, and idiomatic adjustments that are more aligned with GPT-3.5’s training on diverse data, whereas machine translations might adhere more strictly to equivalences. Looking forward, further research could explore deeper into the characteristics of the datasets used for training such models and their alignment with real-world interpretation tasks.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">This study presents several limitations. The observed low interrater agreement suggests potential inconsistencies in human evaluations, possibly affecting correlation values, and generability of the results. Furthermore, the limited scope of the sampled translations might not capture the full range of linguistic complexities inherent to interpretation. Future research should consider evaluations for higher window sizes. In light of GPT-3.5’s performance in this study, future research might explore its ability to delineate nuances of typologies of errors rather than merely providing aggregate scores</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">This study is considered a preliminary attempt to test the feasibility of applying automatic metrics to evaluate inputs from both human and machine interpreters. Before these metrics can be used in production, more research needs to be conducted.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Artetxe et al., 2018] </span>
<span class="ltx_bibblock">
Artetxe, Mikel, Gorka Labaka, and Eneko Agirre.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">arXiv preprint arXiv:1805.06297</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Banerjee and Lavie, 2005] </span>
<span class="ltx_bibblock">
Banerjee, Satanjeev and Alon Lavie.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</span>, pages 65–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Becerra et al., 2013] </span>
<span class="ltx_bibblock">
Becerra, Olalla García, Macarena Pradas Macìas, and Rafael Barranco-Droege, editors.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">Quality in interpreting. 1</span>.

</span>
<span class="ltx_bibblock">Number 120 in Interlingua. Comares, Granada.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Brown et al., 2020] </span>
<span class="ltx_bibblock">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda Askell.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">Advances in neural information processing systems</span>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Carroll, 1966] </span>
<span class="ltx_bibblock">
Carroll, John B.

</span>
<span class="ltx_bibblock">1966.

</span>
<span class="ltx_bibblock">An experiment in evaluating the quality of translations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">Mech. Transl. Comput. Linguistics</span>, 9(3-4):55–66.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Chung, 2020] </span>
<span class="ltx_bibblock">
Chung, H. Y.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Automatic evaluation of human translation: BLEU vs. METEOR.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">Lebende Sprachen</span>, 65(1):181–205.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Clark et al., 2019] </span>
<span class="ltx_bibblock">
Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">What does bert look at? an analysis of bert’s attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">arXiv preprint arXiv:1906.04341</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Clark et al., 2020] </span>
<span class="ltx_bibblock">
Clark, Kevin, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Electra: Pre-training text encoders as discriminators rather than generators.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">arXiv preprint arXiv:2003.10555</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Conneau et al., 2017] </span>
<span class="ltx_bibblock">
Conneau, Alexis, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Word translation without parallel data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">arXiv preprint arXiv:1710.04087</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Devlin et al., 2019] </span>
<span class="ltx_bibblock">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">arXiv:1810.04805 [cs]</span>, May.

</span>
<span class="ltx_bibblock">arXiv: 1810.04805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Doddington, 2002] </span>
<span class="ltx_bibblock">
Doddington, George.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Proceedings of the second international conference on Human Language Technology Research</span>, pages 138–145.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Fantinuoli and Prandi, 2021] </span>
<span class="ltx_bibblock">
Fantinuoli, Claudio and Bianca Prandi.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Towards the evaluation of automatic simultaneous speech translation from a communicative perspective.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</span>, pages 245–254, Bangkok, Thailand (online), August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Fernandes et al., 2023] </span>
<span class="ltx_bibblock">
Fernandes, Patrick, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, and Orhan Firat.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation, August.

</span>
<span class="ltx_bibblock">arXiv:2308.07286 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Gerver, 1969] </span>
<span class="ltx_bibblock">
Gerver, David.

</span>
<span class="ltx_bibblock">1969.

</span>
<span class="ltx_bibblock">The effects of source language presentation rate on the performance of simultaneous conference interpreters.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">Proceedings of the 2nd Louisville Conference on rate and/or frequency controlled speech</span>, pages 162–184. Louisville (Kty), University of Louisville.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Gile, 1995] </span>
<span class="ltx_bibblock">
Gile, Daniel.

</span>
<span class="ltx_bibblock">1995.

</span>
<span class="ltx_bibblock">Fidelity assessment in consecutive interpretation: An experiment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">Target. International Journal of Translation Studies</span>, 7(1):151–164.

</span>
<span class="ltx_bibblock">ISBN: 0924-1884 Publisher: John Benjamins.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Han and Lu, 2021a] </span>
<span class="ltx_bibblock">
Han, Chao and Xiaolei Lu.

</span>
<span class="ltx_bibblock">2021a.

</span>
<span class="ltx_bibblock">Can automated machine translation evaluation metrics be used to assess students’ interpretation in the language learning classroom?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">Computer Assisted Language Learning</span>, pages 1–24, August.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Han and Lu, 2021b] </span>
<span class="ltx_bibblock">
Han, Chao and Xiaolei Lu.

</span>
<span class="ltx_bibblock">2021b.

</span>
<span class="ltx_bibblock">Interpreting quality assessment re-imagined: The synergy between human and machine scoring.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Interpreting and Society</span>, 1(1):70–90, September.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Han and Zhao, 2021] </span>
<span class="ltx_bibblock">
Han, Chao and Xiao Zhao.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Accuracy of peer ratings on the quality of spoken-language interpreting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">Assessment &amp; Evaluation in Higher Education</span>, 46(8):1299–1313, November.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Han, 2016] </span>
<span class="ltx_bibblock">
Han, Chao.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Investigating Score Dependability in English/Chinese Interpreter Certification Performance Testing: A Generalizability Theory Approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">Language Assessment Quarterly</span>, 13(3):186–201, July.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Han, 2017] </span>
<span class="ltx_bibblock">
Han, Chao.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Using analytic rating scales to assess English/Chinese bi-directional interpretation: A longitudinal Rasch analysis of scale utility and rater behavior.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Linguistica Antverpiensia, New Series–Themes in Translation Studies</span>, 16.

</span>
<span class="ltx_bibblock">ISBN: 2295-5739.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Han, 2022] </span>
<span class="ltx_bibblock">
Han, Chao.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Interpreting testing and assessment: A state-of-the-art review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">Language Testing</span>, 39(1):30–55, January.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Hendy et al., 2023] </span>
<span class="ltx_bibblock">
Hendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">How good are gpt models at machine translation? a comprehensive evaluation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">arXiv preprint arXiv:2302.09210</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Howard and Ruder, 2018] </span>
<span class="ltx_bibblock">
Howard, Jeremy and Sebastian Ruder.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Universal language model fine-tuning for text classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">arXiv preprint arXiv:1801.06146</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Kalina, 2012] </span>
<span class="ltx_bibblock">
Kalina, Sylvia.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Quality in interpreting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">John Benjamins Publishing Company</span>, 3:134–140.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Kocmi and Federmann, 2023] </span>
<span class="ltx_bibblock">
Kocmi, Tom and Christian Federmann.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Large Language Models Are State-of-the-Art Evaluators of Translation Quality, May.

</span>
<span class="ltx_bibblock">arXiv:2302.14520 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Korpal, 2012] </span>
<span class="ltx_bibblock">
Korpal, Pawel.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Omission in simultaneous interpreting as a deliberate act.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Translation Research Projects 4</span>, pages 103–111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[Lee, 1999a] </span>
<span class="ltx_bibblock">
Lee, Tae-Hyung.

</span>
<span class="ltx_bibblock">1999a.

</span>
<span class="ltx_bibblock">Simultaneous listening and speaking in English into Korean simultaneous interpretation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">Meta: journal des traducteurs/Meta: Translators’ Journal</span>, 44(4):560–572.

</span>
<span class="ltx_bibblock">ISBN: 0026-0452 Publisher: Les Presses de l’Université de Montréal.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[Lee, 1999b] </span>
<span class="ltx_bibblock">
Lee, Tae-Hyung.

</span>
<span class="ltx_bibblock">1999b.

</span>
<span class="ltx_bibblock">Speech proportion and accuracy in simultaneous interpretation from English into Korean.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">Meta: journal des traducteurs/Meta: Translators’ Journal</span>, 44(2):260–267.

</span>
<span class="ltx_bibblock">ISBN: 0026-0452 Publisher: Les Presses de l’Université de Montréal.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[Lee, 2002] </span>
<span class="ltx_bibblock">
Lee, Tae-Hyung.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Ear voice span in English into Korean simultaneous interpretation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx29.1.1">Meta: Journal des traducteurs/Meta: Translators’ Journal</span>, 47(4):596–606.

</span>
<span class="ltx_bibblock">ISBN: 0026-0452 Publisher: Les Presses de l’Université de Montréal.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[Lu and Han, 2022] </span>
<span class="ltx_bibblock">
Lu, Xiaolei and Chao Han.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Automatic assessment of spoken-language interpreting based on machine-translation evaluation metrics: A multi-scenario exploratory study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">Interpreting</span>.

</span>
<span class="ltx_bibblock">ISBN: 1384-6647 Publisher: John Benjamins Publishing Company Amsterdam/Philadelphia.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[Mackintosh, 1983] </span>
<span class="ltx_bibblock">
Mackintosh, J.

</span>
<span class="ltx_bibblock">1983.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">RELAY INTERPRETATION: AN EXPLORATORY STUDY. University of London</span>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, unpublished MA thesis.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[Mikolov et al., 2013] </span>
<span class="ltx_bibblock">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their compositionality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">Advances in neural information processing systems</span>, 26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[Nia and Modarresi, 2019] </span>
<span class="ltx_bibblock">
Nia, Foroogh Khorami and Ghasem Modarresi.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">A Rasch-based validation of the evaluation rubric for consecutive interpreting performance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx33.1.1">Sendebar</span>, 30:221–244.

</span>
<span class="ltx_bibblock">ISBN: 2340-2415.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[Papineni et al., 2002] </span>
<span class="ltx_bibblock">
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx34.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</span>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[Pennington et al., 2014] </span>
<span class="ltx_bibblock">
Pennington, Jeffrey, Richard Socher, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx35.1.1">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</span>, pages 1532–1543.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[Peters et al., 1802] </span>
<span class="ltx_bibblock">
Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">1802.

</span>
<span class="ltx_bibblock">Deep contextualized word representations. CoRR abs/1802.05365 (2018).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx36.1.1">arXiv preprint arXiv:1802.05365</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[Pöchhacker, 2002] </span>
<span class="ltx_bibblock">
Pöchhacker, Franz.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Quality Assessment in Conference and Community Interpreting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx37.1.1">Meta</span>, 46(2):410–425, October.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[Radford et al., 2018] </span>
<span class="ltx_bibblock">
Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx38.1.1">arXiv preprint</span>.

</span>
<span class="ltx_bibblock">Publisher: OpenAI.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[Raffel et al., 2020] </span>
<span class="ltx_bibblock">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx39.1.1">The Journal of Machine Learning Research</span>, 21(1):5485–5551.

</span>
<span class="ltx_bibblock">ISBN: 1532-4435 Publisher: JMLRORG.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[Roweis and Saul, 2000] </span>
<span class="ltx_bibblock">
Roweis, Sam T. and Lawrence K. Saul.

</span>
<span class="ltx_bibblock">2000.

</span>
<span class="ltx_bibblock">Nonlinear dimensionality reduction by locally linear embedding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx40.1.1">science</span>, 290(5500):2323–2326.

</span>
<span class="ltx_bibblock">ISBN: 1095-9203 Publisher: American Association for the Advancement of Science.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[Ruder et al., 2019] </span>
<span class="ltx_bibblock">
Ruder, Sebastian, Ivan Vulić, and Anders Søgaard.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">A survey of cross-lingual word embedding models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx41.1.1">Journal of Artificial Intelligence Research</span>, 65:569–631.

</span>
<span class="ltx_bibblock">ISBN: 1076-9757.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[Setton and Motta, 2007] </span>
<span class="ltx_bibblock">
Setton, Robin and Manuela Motta.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock">Syntacrobatics: Quality and reformulation in simultaneous-with-text.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx42.1.1">Interpreting</span>, 9(2):199–230, January.

</span>
<span class="ltx_bibblock">Publisher: John Benjamins.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[Tiselius, 2009] </span>
<span class="ltx_bibblock">
Tiselius, Elisabet.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock">Revisiting Carroll’s scales.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx43.1.1">Testing and assessment in translation and interpreting studies</span>, pages 95–121.

</span>
<span class="ltx_bibblock">Publisher: John Benjamins Amsterdam.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al., 2017] </span>
<span class="ltx_bibblock">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx44.1.1">Advances in neural information processing systems</span>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al., 2023] </span>
<span class="ltx_bibblock">
Wang, Longyue, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Document-Level Machine Translation with Large Language Models, April.

</span>
<span class="ltx_bibblock">arXiv:2304.02210 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_tag_bibitem">[Wu et al., 2013] </span>
<span class="ltx_bibblock">
Wu, Jessica, M. H. Liu, and Cecilia Liao.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Analytic scoring in interpretation test: Construct validity and the halo effect.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx46.1.1">The making of a translator: Multiple perspectives</span>, 277:292.

</span>
<span class="ltx_bibblock">Publisher: Bookman Taipei.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_tag_bibitem">[Wu et al., 2016] </span>
<span class="ltx_bibblock">
Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, and Klaus Macherey.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx47.1.1">arXiv preprint arXiv:1609.08144</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx48">
<span class="ltx_tag ltx_tag_bibitem">[Wu, 2011] </span>
<span class="ltx_bibblock">
Wu, Shao-Chuan.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx48.1.1">Assessing simultaneous interpreting: A study on test reliability and examiners’ assessment behavior</span>.

</span>
<span class="ltx_bibblock">PhD Thesis, Newcastle University.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx49">
<span class="ltx_tag ltx_tag_bibitem">[Xenouleas et al., 2019] </span>
<span class="ltx_bibblock">
Xenouleas, Stratos, Prodromos Malakasiotis, Marianna Apidianaki, and Ion Androutsopoulos.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Sumqe: a bert-based summary quality estimation model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx49.1.1">arXiv preprint arXiv:1909.00578</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx50">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al., 2019] </span>
<span class="ltx_bibblock">
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, and Quoc V. Le.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Xlnet: Generalized autoregressive pretraining for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx50.1.1">Advances in neural information processing systems</span>, 32.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun 14 14:46:58 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
