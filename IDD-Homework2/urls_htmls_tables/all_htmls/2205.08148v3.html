<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.08148] Gender and Racial Bias in Visual Question Answering Datasets</title><meta property="og:description" content="Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models.
A popular task in the field is visual question answering (VQA), which aims to ans…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gender and Racial Bias in Visual Question Answering Datasets">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Gender and Racial Bias in Visual Question Answering Datasets">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.08148">

<!--Generated on Mon Mar 11 15:45:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="visual question answering,  gender stereotype,  racial stereotype,  datasets">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Gender and Racial Bias in Visual Question Answering Datasets</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yusuke Hirota
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:y-hirota@is.ids.osaka-u.ac.jp">y-hirota@is.ids.osaka-u.ac.jp</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7221-0184" title="ORCID identifier" class="ltx_ref">0000-0002-7221-0184</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Osaka University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Osaka</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">Japan</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuta Nakashima
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:n-yuta@ids.osaka-u.ac.jp">n-yuta@ids.osaka-u.ac.jp</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-8000-3567" title="ORCID identifier" class="ltx_ref">0000-0001-8000-3567</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Osaka University</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Osaka</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">Japan</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noa Garcia
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:noagarcia@ids.osaka-u.ac.jp">noagarcia@ids.osaka-u.ac.jp</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9200-6359" title="ORCID identifier" class="ltx_ref">0000-0002-9200-6359</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Osaka University</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Osaka</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">Japan</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id10.id1" class="ltx_p">Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models.
A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: <span id="id10.id1.1" class="ltx_text ltx_font_italic">e</span>.<span id="id10.id1.2" class="ltx_text ltx_font_italic">g</span>., questions about the color of a <span id="id10.id1.3" class="ltx_text ltx_font_italic">banana</span> are answered with <span id="id10.id1.4" class="ltx_text ltx_font_italic">yellow</span>, even if the banana in the image is green. If societal bias (<span id="id10.id1.5" class="ltx_text ltx_font_italic">e</span>.<span id="id10.id1.6" class="ltx_text ltx_font_italic">g</span>., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes.
For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets.
Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem <span id="id10.id1.7" class="ltx_text ltx_font_italic">before</span>, <span id="id10.id1.8" class="ltx_text ltx_font_italic">during</span>, and <span id="id10.id1.9" class="ltx_text ltx_font_italic">after</span> the dataset collection process.</p>
</div>
<div class="ltx_keywords">visual question answering, gender stereotype, racial stereotype, datasets
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>2022 ACM Conference on Fairness, Accountability, and Transparency; June 21–24, 2022; Seoul, Republic of Korea</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22), June 21–24, 2022, Seoul, Republic of Korea</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3531146.3533184</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-9352-2/22/06</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-9352-2/22/06</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics Gender</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics Race and ethnicity</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Computer vision</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language processing</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2205.08148/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Diagram of VQA. Given an image and a question about the image, a model answers the question.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The so-called vision-and-language tasks, which consist of applications that interact, process, and make decisions based on both visual and language content, present one of the greatest challenges in modern machine learning research. By construction, such applications not only need to deal with the challenges of image and text understanding but also overcome the modality gap between the visual and the language inputs. Such modality gap is non-trivial and has made tasks such as image captioning <cite class="ltx_cite ltx_citemacro_citep">(You et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2016</a>; Vinyals et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> or visual question answering <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>; Malinowski and Fritz, <a href="#bib.bib38" title="" class="ltx_ref">2014</a>)</cite> considerably popular within the computer vision (CV) <cite class="ltx_cite ltx_citemacro_citep">(Elhoseiny et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>; Wu and Gan, <a href="#bib.bib56" title="" class="ltx_ref">2021</a>; Shrivastava et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2021</a>; Achlioptas et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> and the natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(Mosbach et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>; Bugliarello et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> communities. From classic CNN-based models <cite class="ltx_cite ltx_citemacro_citep">(Vinyals et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> to the current self-attention multi-modal Transformers <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite>, the rapid progression of the field has only been possible thanks to the collection, annotation, and public distribution of datasets and benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2014</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2015</a>; Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>; Shrivastava et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2021</a>; Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2018</a>)</cite> designed specifically to train and evaluate vision-and-language models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The increased complexity of those models, which contain a high number of parameters to be trained, has made the availability of data a precious resource. At the same time, with the adoption of some of these models into real-world products, how this data represents the real world is of raising concern. For example, when minoritized groups are underrepresented in machine learning datasets, trained models can contribute to perpetuating social discrimination by producing potentially harmful outcomes. This has been the case for face recognition <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> as well as for object recognition <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite>. With respect to vision-and-language, Burns <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_citep">(Hendricks et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> and Zhao <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite> showed that image captioning models, <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">i</span>.<span id="S1.p2.1.4" class="ltx_text ltx_font_italic">e</span>., models that aim to produce a descriptive sentence of a given image, perpetuate gender and racial bias. As crucial as this is, the depth of the problem has not been fully explored, and the question about whether other tasks within vision-and-language are also affected by unfair data representations remains open.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With the aim of raising awareness and starting an open discussion, this paper investigates societal bias in visual question answering (VQA), another of the foundational tasks within the vision-and-language community. The aim of VQA is to correctly answer questions about a given image (Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), requiring understanding and associating the question and the image with potential candidate answers. One of the current challenges in VQA is that models tend to suffer from language bias <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="S1.p3.1.2" class="ltx_text ltx_font_italic">e</span>., they exploit the superficial correlations between the questions and the answers in the training set, and produce answers without looking into the visual content. For example, in the VQA 1.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite> questions starting with <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">what sport</span> are answered <span id="S1.p3.1.4" class="ltx_text ltx_font_italic">tennis</span> on the <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="41" display="inline"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">41</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn type="integer" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">41</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">41</annotation></semantics></math>% of the samples, which leads to models learning a shortcut to always produce <span id="S1.p3.1.5" class="ltx_text ltx_font_italic">tennis</span> for this type of questions. This suggests that if societal bias, such as racism or sexism, is present in the training data, it will be highly likely perpetuated by VQA models. Moreover, as most of the VQA datasets are collected by crowdsourcing by showing human annotators a photo and asking them to freely write questions and answers about it, it is reasonable to think that the biases from the annotators may be leaked into the data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Specifically, this paper analyzes gender and racial bias on five VQA datasets <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>; Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>. Our study is based on the compilation of statistics about the representation of different demographic groups, the correlation between demographic groups and answer and question distributions, and the exploration of harmful examples within each dataset. In Section <a href="#S4" title="4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we analyze gender bias in terms of men and women representation,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We realize that gender categories should be inclusive and based on the self-identity of gender. Since VQA datasets deal with gender in binary categories and according to other computer vision studies about ethics <cite class="ltx_cite ltx_citemacro_citep">(Hendricks et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019b</a>)</cite>, we consider binary gender in this analysis.</span></span></span> uncovering:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">There is a systematic imbalance on gender representation. Questions about men are about twice as frequent as those about women
in all the analyzed datasets.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.3" class="ltx_p">Answer distributions are different between women and men questions. For example, in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>, the answer <span id="S1.I1.i2.p1.3.1" class="ltx_text ltx_font_italic">skateboarding</span> appears <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="835" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mn id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">835</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><cn type="integer" id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1">835</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">835</annotation></semantics></math> times for questions about men, but only <math id="S1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S1.I1.i2.p1.2.m2.1a"><mn id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><cn type="integer" id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">60</annotation></semantics></math> times for questions about women. Furthermore, we found that specific answers are co-related with each gender, <span id="S1.I1.i2.p1.3.2" class="ltx_text ltx_font_italic">e</span>.<span id="S1.I1.i2.p1.3.3" class="ltx_text ltx_font_italic">g</span>., the percentage of answers that are <span id="S1.I1.i2.p1.3.4" class="ltx_text ltx_font_italic">blonde</span> is about <math id="S1.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S1.I1.i2.p1.3.m3.1a"><mn id="S1.I1.i2.p1.3.m3.1.1" xref="S1.I1.i2.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.3.m3.1b"><cn type="integer" id="S1.I1.i2.p1.3.m3.1.1.cmml" xref="S1.I1.i2.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.3.m3.1c">4</annotation></semantics></math> times higher in questions about women than in questions about men.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We found multiple samples that reflect traditional gender stereotypes both in men and women images.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Answers based on gender stereotypes tend to appear more frequently in samples where the answers cannot be grounded in the image content, such as <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">What is the woman thinking?</span>.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In Section <a href="#S5" title="5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we analyze VQA datasets in terms of racial bias.
Specifically, we dig into samples that explicitly mentions race or ethnicity. Our analysis shows:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">All the VQA datasets contain questions related to race. However, the ratio of such questions is very small for GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, whose questions and answers were automatically generated from the images, and for Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>, in which inappropriate samples were filtered.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">There is an imbalance on the representation of different demographic groups. For instance, in VQA 2.0, the answers <span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">White</span> or <span id="S1.I2.i2.p1.1.2" class="ltx_text ltx_font_italic">Caucasian</span> appear about <math id="S1.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="3.45" display="inline"><semantics id="S1.I2.i2.p1.1.m1.1a"><mn id="S1.I2.i2.p1.1.m1.1.1" xref="S1.I2.i2.p1.1.m1.1.1.cmml">3.45</mn><annotation-xml encoding="MathML-Content" id="S1.I2.i2.p1.1.m1.1b"><cn type="float" id="S1.I2.i2.p1.1.m1.1.1.cmml" xref="S1.I2.i2.p1.1.m1.1.1">3.45</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I2.i2.p1.1.m1.1c">3.45</annotation></semantics></math> times more frequently than the answers <span id="S1.I2.i2.p1.1.3" class="ltx_text ltx_font_italic">Black</span> or <span id="S1.I2.i2.p1.1.4" class="ltx_text ltx_font_italic">African</span>.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">The correlation between race and nationality shows a US-centric viewpoint, with <math id="S1.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="65" display="inline"><semantics id="S1.I2.i3.p1.1.m1.1a"><mn id="S1.I2.i3.p1.1.m1.1.1" xref="S1.I2.i3.p1.1.m1.1.1.cmml">65</mn><annotation-xml encoding="MathML-Content" id="S1.I2.i3.p1.1.m1.1b"><cn type="integer" id="S1.I2.i3.p1.1.m1.1.1.cmml" xref="S1.I2.i3.p1.1.m1.1.1">65</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I2.i3.p1.1.m1.1c">65</annotation></semantics></math>% of Black people being associated with American nationality in VQA 2.0.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">We found potential harmful examples related to race, ethnicity, or nationality. As in the case of gender, these samples tend to appear when answers cannot be grounded in the visual content of the image, leading annotators to answer the questions based on their own preconceptions of the world.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In Section <a href="#S6" title="6. Possible Solutions ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we identify two main sources of societal bias in VQA datasets, and propose mitigation strategies to alleviate them. The first problem is the underrepresentation of minoritized groups.
We emphasize the importance of recognizing societal bias in VQA datasets and taking measures against it, as models trained on such datasets can learn to ignore minoritized attributes and lead to shortcut learning <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Kervadec et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>.
The second problem is the presence of harmful samples in the datasets.
We propose three simple tools to remove them while considering annotation costs: 1) automatically screening to identify unanswerable questions, 2) including ethical instruction in the annotation process, and 3) creating an open platform to allow user’s feedback, so that problematic samples can be easily addressed.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Finally, we would like to note that datasets are a crucial part for the development of the field, and all existing VQA datasets have been and will be necessary and important. This paper does not aim to diminish the contribution of such datasets, but to raise awareness within their users and potential dataset developers so that mitigation measures can be taken in the future.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual question answering (VQA)</span>
VQA is the task of answering a question about an image’s visual content, which has been commonly used to evaluate the ability of a model to understand and integrate visual and language information.
In the seminal work by Agrawal <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite>, the first large-scale dataset for VQA was created, commonly known as VQA 1.0 dataset. VQA 1.0 contained challenging reasoning questions involving a diverse set of skills for the models to solve, such as object and activity recognition, counting, or space localization, among others.
Since models became progressively better at VQA 1.0, new datasets and challenges were proposed <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>; Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>; Gurari et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>; Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>; Garcia et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>, including VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>, which partly addressed the existent language bias in VQA 1.0 by increasing the diversity of answers in similar types of questions, GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, in which questions required various reasoning skills (<span id="S2.p1.1.3" class="ltx_text ltx_font_italic">e</span>.<span id="S2.p1.1.4" class="ltx_text ltx_font_italic">g</span>., spatial reasoning, logical inference) to find the correct answer, and OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, in which models needed to access external knowledge such as Wikipedia.
As a result, the field has attracted a lot of attention from researches all over the world, with a large number of models been proposed ever since <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>; Ben-younes et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Anderson et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2018</a>; Cadène et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019a</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>; Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>; Tan and Bansal, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020a</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2020</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2021</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020</a>; Hirota et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Although most of the effort has been focused on increasing the overall accuracy on the publicly available benchmarks, specially on the VQA 2.0 dataset, an important body of work <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Clark et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Cadène et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019b</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020b</a>; Niu et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> has been devoted to address <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">language bias</span>, which refers to the existence of skewed distributions of answers with respect to a certain type of question. Even though VQA is a multi-modal task, because of the language bias, models tend to learn and make inferences based on superficial correlations, such as questions about <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">bananas</span> are answered <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">yellow</span> with high probability <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>. This makes the models to ignore the visual information, which prevents generalization to out-of-distribution settings.
Manjunatha <span id="S2.p2.1.4" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_citep">(Manjunatha et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite> further examined this tendency by utilizing rule mining algorithms to correlate the questions, answers, and regions in which a model focuses. The results showed that predictions tend to contain miscellaneous rules; for example, when <span id="S2.p2.1.5" class="ltx_text ltx_font_italic">what</span> and <span id="S2.p2.1.6" class="ltx_text ltx_font_italic">brand</span> are in a question and there is a laptop in the image, the answer tends to be <span id="S2.p2.1.7" class="ltx_text ltx_font_italic">Dell</span>.
It was also observed that in <span id="S2.p2.1.8" class="ltx_text ltx_font_italic">What is he/she doing?</span> type of questions, answers for men were more diverse (<span id="S2.p2.1.9" class="ltx_text ltx_font_italic">skateboarding, snowboarding, surfing</span>), than answers for women (<span id="S2.p2.1.10" class="ltx_text ltx_font_italic">texting</span>). Although this evidence points at skewed distributions in terms of gender, <span id="S2.p2.1.11" class="ltx_text ltx_font_italic">societal bias</span> in VQA datasets have not been explicitly studied yet.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Societal bias in vision-and-language</span>
It is only in recent years that societal bias has been investigated in CV and NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019b</a>, <a href="#bib.bib53" title="" class="ltx_ref">a</a>; Thong and Snoek, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>; Jia et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Shankar et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2017</a>; Bolukbasi et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. One significant study is Buolamwini and Gebru’s work <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> on commercial face recognition applications. They demonstrated that the system’s performance varies depending on the gender and race of the individual, and in particular, misjudges women with darker skin.
In vision-and-language tasks, there has been some recent advancements, especially for image captioning <cite class="ltx_cite ltx_citemacro_citep">(Hendricks et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>; Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2021</a>; Hirota et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>.
In one of the first studies, Burns <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_citep">(Hendricks et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> showed that, instead of looking into the appearance of people, captioning models predicted gender words based on gender stereotypes in the contextual information. For example, when an image had a laptop, models generated the word <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">man</span>, even when there was only a woman.
Similarly, Zhao <span id="S2.p3.1.4" class="ltx_text ltx_font_italic">et al</span>.  <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite> studied racial and gender bias. They found imbalances in gender and race representation in the COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2014</a>)</cite>, with more than twice images of men than of women and <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="9.2" display="inline"><semantics id="S2.p3.1.m1.1a"><mn id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">9.2</mn><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><cn type="float" id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">9.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">9.2</annotation></semantics></math>x more images of lighter-skinned than darker-skinned people.
More recently, Hirota <span id="S2.p3.1.5" class="ltx_text ltx_font_italic">et al</span>.  <cite class="ltx_cite ltx_citemacro_citep">(Hirota et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> proposed a metric to quantify gender and racial bias amplification of image captioning models.
Given this context, in order to raise awareness and mitigate the damage societal bias may be causing to underrepresented communities, it is only natural to investigate whether other tasks and datasets within vision-and-language are also affected by this problem.
</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Preliminaries</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We study gender and racial bias in VQA.
We first describe the datasets under analysis in Section <a href="#S3.SS1" title="3.1. VQA datasets ‣ 3. Preliminaries ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, and then, the methodology we followed in our experiments in Section <a href="#S3.SS2" title="3.2. Methodology ‣ 3. Preliminaries ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of datasets under analysis. <sup id="S3.T1.16.1" class="ltx_sup"><span id="S3.T1.16.1.1" class="ltx_text ltx_font_italic">†</span></sup> denotes the images are from the intersection of COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2014</a>)</cite> and YFCC100M <cite class="ltx_cite ltx_citemacro_citep">(Thomee et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2016</a>)</cite> datasets. The types of answers can be: Open ended, which means the answers are written freely without vocabulary restrictions; Multiple choice, which means several candidate answers, with only one correct answer, are provided per question; or Closed vocabulary, which means the words used as answers are taken from a limited list.</figcaption>
<table id="S3.T1.14" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.14.13.1" class="ltx_tr">
<th id="S3.T1.14.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Year</th>
<th id="S3.T1.14.13.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S3.T1.14.13.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Images</th>
<th id="S3.T1.14.13.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Source</th>
<th id="S3.T1.14.13.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. QA</th>
<th id="S3.T1.14.13.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">QA Annotation</th>
<th id="S3.T1.14.13.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Answer Type</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.5.3" class="ltx_tr">
<td id="S3.T1.5.3.4" class="ltx_td ltx_align_left ltx_border_t">2016</td>
<td id="S3.T1.5.3.5" class="ltx_td ltx_align_left ltx_border_t">Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S3.T1.3.1.1" class="ltx_td ltx_align_right ltx_border_t">
<math id="S3.T1.3.1.1.m1.1" class="ltx_Math" alttext="108" display="inline"><semantics id="S3.T1.3.1.1.m1.1a"><mn id="S3.T1.3.1.1.m1.1.1" xref="S3.T1.3.1.1.m1.1.1.cmml">108</mn><annotation-xml encoding="MathML-Content" id="S3.T1.3.1.1.m1.1b"><cn type="integer" id="S3.T1.3.1.1.m1.1.1.cmml" xref="S3.T1.3.1.1.m1.1.1">108</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.1.1.m1.1c">108</annotation></semantics></math>k</td>
<td id="S3.T1.4.2.2" class="ltx_td ltx_align_left ltx_border_t">COCO<sup id="S3.T1.4.2.2.1" class="ltx_sup"><span id="S3.T1.4.2.2.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S3.T1.5.3.3" class="ltx_td ltx_align_right ltx_border_t">
<math id="S3.T1.5.3.3.m1.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S3.T1.5.3.3.m1.1a"><mn id="S3.T1.5.3.3.m1.1.1" xref="S3.T1.5.3.3.m1.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S3.T1.5.3.3.m1.1b"><cn type="float" id="S3.T1.5.3.3.m1.1.1.cmml" xref="S3.T1.5.3.3.m1.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.3.3.m1.1c">1.7</annotation></semantics></math>M</td>
<td id="S3.T1.5.3.6" class="ltx_td ltx_align_left ltx_border_t">Crowdsourcing</td>
<td id="S3.T1.5.3.7" class="ltx_td ltx_align_left ltx_border_t">Open ended</td>
</tr>
<tr id="S3.T1.7.5" class="ltx_tr">
<td id="S3.T1.7.5.3" class="ltx_td ltx_align_left">2016</td>
<td id="S3.T1.7.5.4" class="ltx_td ltx_align_left">Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S3.T1.6.4.1" class="ltx_td ltx_align_right">
<math id="S3.T1.6.4.1.m1.1" class="ltx_Math" alttext="47" display="inline"><semantics id="S3.T1.6.4.1.m1.1a"><mn id="S3.T1.6.4.1.m1.1.1" xref="S3.T1.6.4.1.m1.1.1.cmml">47</mn><annotation-xml encoding="MathML-Content" id="S3.T1.6.4.1.m1.1b"><cn type="integer" id="S3.T1.6.4.1.m1.1.1.cmml" xref="S3.T1.6.4.1.m1.1.1">47</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.4.1.m1.1c">47</annotation></semantics></math>k</td>
<td id="S3.T1.7.5.5" class="ltx_td ltx_align_left">COCO</td>
<td id="S3.T1.7.5.2" class="ltx_td ltx_align_right">
<math id="S3.T1.7.5.2.m1.1" class="ltx_Math" alttext="327" display="inline"><semantics id="S3.T1.7.5.2.m1.1a"><mn id="S3.T1.7.5.2.m1.1.1" xref="S3.T1.7.5.2.m1.1.1.cmml">327</mn><annotation-xml encoding="MathML-Content" id="S3.T1.7.5.2.m1.1b"><cn type="integer" id="S3.T1.7.5.2.m1.1.1.cmml" xref="S3.T1.7.5.2.m1.1.1">327</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.5.2.m1.1c">327</annotation></semantics></math>k</td>
<td id="S3.T1.7.5.6" class="ltx_td ltx_align_left">Crowdsourcing</td>
<td id="S3.T1.7.5.7" class="ltx_td ltx_align_left">Multiple choice</td>
</tr>
<tr id="S3.T1.9.7" class="ltx_tr">
<td id="S3.T1.9.7.3" class="ltx_td ltx_align_left">2017</td>
<td id="S3.T1.9.7.4" class="ltx_td ltx_align_left">VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S3.T1.8.6.1" class="ltx_td ltx_align_right">
<math id="S3.T1.8.6.1.m1.1" class="ltx_Math" alttext="204" display="inline"><semantics id="S3.T1.8.6.1.m1.1a"><mn id="S3.T1.8.6.1.m1.1.1" xref="S3.T1.8.6.1.m1.1.1.cmml">204</mn><annotation-xml encoding="MathML-Content" id="S3.T1.8.6.1.m1.1b"><cn type="integer" id="S3.T1.8.6.1.m1.1.1.cmml" xref="S3.T1.8.6.1.m1.1.1">204</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.6.1.m1.1c">204</annotation></semantics></math>k</td>
<td id="S3.T1.9.7.5" class="ltx_td ltx_align_left">COCO</td>
<td id="S3.T1.9.7.2" class="ltx_td ltx_align_right">
<math id="S3.T1.9.7.2.m1.1" class="ltx_Math" alttext="1.1" display="inline"><semantics id="S3.T1.9.7.2.m1.1a"><mn id="S3.T1.9.7.2.m1.1.1" xref="S3.T1.9.7.2.m1.1.1.cmml">1.1</mn><annotation-xml encoding="MathML-Content" id="S3.T1.9.7.2.m1.1b"><cn type="float" id="S3.T1.9.7.2.m1.1.1.cmml" xref="S3.T1.9.7.2.m1.1.1">1.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.7.2.m1.1c">1.1</annotation></semantics></math>M</td>
<td id="S3.T1.9.7.6" class="ltx_td ltx_align_left">Crowdsourcing</td>
<td id="S3.T1.9.7.7" class="ltx_td ltx_align_left">Open ended</td>
</tr>
<tr id="S3.T1.12.10" class="ltx_tr">
<td id="S3.T1.12.10.4" class="ltx_td ltx_align_left">2019</td>
<td id="S3.T1.12.10.5" class="ltx_td ltx_align_left">GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S3.T1.10.8.1" class="ltx_td ltx_align_right">
<math id="S3.T1.10.8.1.m1.1" class="ltx_Math" alttext="113" display="inline"><semantics id="S3.T1.10.8.1.m1.1a"><mn id="S3.T1.10.8.1.m1.1.1" xref="S3.T1.10.8.1.m1.1.1.cmml">113</mn><annotation-xml encoding="MathML-Content" id="S3.T1.10.8.1.m1.1b"><cn type="integer" id="S3.T1.10.8.1.m1.1.1.cmml" xref="S3.T1.10.8.1.m1.1.1">113</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.8.1.m1.1c">113</annotation></semantics></math>k</td>
<td id="S3.T1.11.9.2" class="ltx_td ltx_align_left">COCO<sup id="S3.T1.11.9.2.1" class="ltx_sup"><span id="S3.T1.11.9.2.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S3.T1.12.10.3" class="ltx_td ltx_align_right">
<math id="S3.T1.12.10.3.m1.1" class="ltx_Math" alttext="22.7" display="inline"><semantics id="S3.T1.12.10.3.m1.1a"><mn id="S3.T1.12.10.3.m1.1.1" xref="S3.T1.12.10.3.m1.1.1.cmml">22.7</mn><annotation-xml encoding="MathML-Content" id="S3.T1.12.10.3.m1.1b"><cn type="float" id="S3.T1.12.10.3.m1.1.1.cmml" xref="S3.T1.12.10.3.m1.1.1">22.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.10.3.m1.1c">22.7</annotation></semantics></math>M</td>
<td id="S3.T1.12.10.6" class="ltx_td ltx_align_left">Automatic</td>
<td id="S3.T1.12.10.7" class="ltx_td ltx_align_left">Closed vocabulary</td>
</tr>
<tr id="S3.T1.14.12" class="ltx_tr">
<td id="S3.T1.14.12.3" class="ltx_td ltx_align_left ltx_border_bb">2019</td>
<td id="S3.T1.14.12.4" class="ltx_td ltx_align_left ltx_border_bb">OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S3.T1.13.11.1" class="ltx_td ltx_align_right ltx_border_bb">
<math id="S3.T1.13.11.1.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.T1.13.11.1.m1.1a"><mn id="S3.T1.13.11.1.m1.1.1" xref="S3.T1.13.11.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.T1.13.11.1.m1.1b"><cn type="integer" id="S3.T1.13.11.1.m1.1.1.cmml" xref="S3.T1.13.11.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.11.1.m1.1c">14</annotation></semantics></math>k</td>
<td id="S3.T1.14.12.5" class="ltx_td ltx_align_left ltx_border_bb">COCO</td>
<td id="S3.T1.14.12.2" class="ltx_td ltx_align_right ltx_border_bb">
<math id="S3.T1.14.12.2.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.T1.14.12.2.m1.1a"><mn id="S3.T1.14.12.2.m1.1.1" xref="S3.T1.14.12.2.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.T1.14.12.2.m1.1b"><cn type="integer" id="S3.T1.14.12.2.m1.1.1.cmml" xref="S3.T1.14.12.2.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.12.2.m1.1c">14</annotation></semantics></math>k</td>
<td id="S3.T1.14.12.6" class="ltx_td ltx_align_left ltx_border_bb">Crowdsourcing</td>
<td id="S3.T1.14.12.7" class="ltx_td ltx_align_left ltx_border_bb">Open ended</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>VQA datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We analyze five standard datasets, summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3. Preliminaries ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Each dataset varies in the number of images, the number of questions, the annotation method, and the format of the answers; however, the images are from a common source, the COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2014</a>)</cite>. A detailed description for each dataset is provided below.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p"><span id="S3.SS1.p2.2.1" class="ltx_text ltx_font_bold">Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite></span>
Visual Genome contains <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="108" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">108</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">108</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">108</annotation></semantics></math>k images from the intersection of COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2014</a>)</cite> and YFCC100M <cite class="ltx_cite ltx_citemacro_citep">(Thomee et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2016</a>)</cite> datasets, and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="float" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">1.7</annotation></semantics></math> million question-answer pairs about the images. The answers are open-ended, which means they are freely written and their vocabulary is not restricted. Questions and answers were created by human annotators following three rules: 1) questions had to start with one of the six Ws: Who, Where, What, When, Why, and How, 2) ambiguous and speculative questions had to be avoided, and 3) questions had to be precise, unique, and relatable to the image, such that they had to be answerable if and only if the image was shown.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p"><span id="S3.SS1.p3.3.1" class="ltx_text ltx_font_bold">Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite></span>
Visual7W is composed of <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="327" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">327</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn type="integer" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">327</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">327</annotation></semantics></math>k QA pairs and <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="1.3" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mn id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">1.3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><cn type="float" id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">1.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">1.3</annotation></semantics></math>M human-generated multiple choice answers on top of <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="47" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mn id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">47</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><cn type="integer" id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">47</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">47</annotation></semantics></math>k COCO images. The dataset is characterized by object-level rationales and multiple choice answers, which means several candidate answers are provided per question with only one being the correct one. Each question starts with one of the seven Ws: What, Where, When, Who, Why, How, and Which. Annotators instructed to create question-answer pairs while being concise and unambiguous to avoid wordy or speculative questions. After that, other annotators check the question-answer pairs to see if an average person can answer them.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.7" class="ltx_p"><span id="S3.SS1.p4.7.1" class="ltx_text ltx_font_bold">VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite></span>
The dataset is built on COCO images and contains <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="1.1" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">1.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><cn type="float" id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">1.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">1.1</annotation></semantics></math>M question-answer pairs. The questions are categorized by question types defined by the first few words of questions (<span id="S3.SS1.p4.7.2" class="ltx_text ltx_font_italic">e</span>.<span id="S3.SS1.p4.7.3" class="ltx_text ltx_font_italic">g</span>., <span id="S3.SS1.p4.7.4" class="ltx_text ltx_font_italic">What is this</span>, <span id="S3.SS1.p4.7.5" class="ltx_text ltx_font_italic">How many</span>). The dataset is divided into training (<math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mn id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><cn type="integer" id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">80</annotation></semantics></math>k images and <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="444" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">444</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">444</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">444</annotation></semantics></math>k questions), validation (<math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mn id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><cn type="integer" id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">40</annotation></semantics></math>k images and <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="214" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mn id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">214</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><cn type="integer" id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">214</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">214</annotation></semantics></math>k questions), and test (<math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><mn id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><cn type="integer" id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">80</annotation></semantics></math>k images and <math id="S3.SS1.p4.7.m7.1" class="ltx_Math" alttext="448" display="inline"><semantics id="S3.SS1.p4.7.m7.1a"><mn id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml">448</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><cn type="integer" id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">448</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">448</annotation></semantics></math>k questions) sets. VQA 2.0 is the de facto benchmark for natural image VQA. When making the questions, annotators freely create questions that people can answer while making questions not easy. Also, annotators are instructed to ask questions that require the image to answer. After that, ten different annotators answer each question. The answers of the test set are not published, so we use training and validation sets in our analysis.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.3" class="ltx_p"><span id="S3.SS1.p5.3.1" class="ltx_text ltx_font_bold">GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite></span>
GQA is a large-scale VQA dataset with <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="113" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mn id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">113</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><cn type="integer" id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">113</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">113</annotation></semantics></math>k images from the Visual Genome dataset and <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="22.7" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mn id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">22.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><cn type="float" id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">22.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">22.7</annotation></semantics></math>M question-answer pairs. Questions require many types of reasoning which measure <span id="S3.SS1.p5.3.2" class="ltx_text ltx_font_italic">e</span>.<span id="S3.SS1.p5.3.3" class="ltx_text ltx_font_italic">g</span>., logical inference. The question-answer pairs are automatically generated using question templates and scene graph representing all the objects and relationships in the image. Hence, the answers are limited to the words in the scene graphs, which we called closed vocabulary. Due to its large-scale, we use a random subset of roughly <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mn id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><cn type="integer" id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">20</annotation></semantics></math> percent of the samples in our analysis.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.2" class="ltx_p"><span id="S3.SS1.p6.2.1" class="ltx_text ltx_font_bold">OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite></span>
The dataset is built on a part of COCO images and contains <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mn id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><cn type="integer" id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">14</annotation></semantics></math>k open-ended questions. Also, there are <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mn id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><cn type="integer" id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">10</annotation></semantics></math> knowledge categories into which each question is classified (<span id="S3.SS1.p6.2.2" class="ltx_text ltx_font_italic">e</span>.<span id="S3.SS1.p6.2.3" class="ltx_text ltx_font_italic">g</span>., Science and Technology, Cooking and Food).
The dataset comprises questions that require external knowledge (<span id="S3.SS1.p6.2.4" class="ltx_text ltx_font_italic">e</span>.<span id="S3.SS1.p6.2.5" class="ltx_text ltx_font_italic">g</span>., Wikipedia) to answer. Questions are written by human annotators following the same instructions as VQA 2.0, but the annotators are also encouraged to make questions that require external knowledge. Later, five different annotators write the answers to those questions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Methodology</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2205.08148/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of our methodology. For each VQA dataset, we automatically select the samples containing references to gender and race/ethnicity, and proceed to perform statistical analysis, answer analysis, and visual inspection.
<span id="S3.F2.5.1" class="ltx_text" style="color:#FF8C00;">Orange</span> and <span id="S3.F2.6.2" class="ltx_text" style="color:#006400;">green</span> circles represent samples with a gender reference. <span id="S3.F2.7.3" class="ltx_text" style="color:#4682B4;">Blue</span> circles are samples with a reference to race or ethnicity. <span id="S3.F2.8.4" class="ltx_text" style="color:#696969;">Gray</span> circles are other samples.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2. Methodology ‣ 3. Preliminaries ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we provide an overview of the methodology used to investigate gender and racial bias in VQA datasets. The first step is to select samples for each dataset with an explicit mention to gender (women/men questions) or race and ethnicity (racial questions). We use a rule-based approach to detect such samples. Then, we analyze gender/racial bias by comparing different sets. Concretely, we run statistics on the number of samples, analyze trends in the answers, and visually inspect for potential harmful data.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Gender Bias in VQA</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first analyze gender bias in VQA datasets. Following previous work <cite class="ltx_cite ltx_citemacro_citep">(Hendricks et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite>, we use a binary classification of gender, with the two gender categories being <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">women</span> and <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">men</span>. With a rule-based approach, we classify samples into women or men categories based on the gender words in the questions (if any).
Specifically, we first define a list of words for women and men, <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">e</span>.<span id="S4.p1.1.4" class="ltx_text ltx_font_italic">g</span>., <span id="S4.p1.1.5" class="ltx_text ltx_font_italic">woman</span>, <span id="S4.p1.1.6" class="ltx_text ltx_font_italic">girl</span>, <span id="S4.p1.1.7" class="ltx_text ltx_font_italic">she</span> for women, <span id="S4.p1.1.8" class="ltx_text ltx_font_italic">man</span>, <span id="S4.p1.1.9" class="ltx_text ltx_font_italic">boy</span>, <span id="S4.p1.1.10" class="ltx_text ltx_font_italic">he</span> for men.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The list of women/men words can be found in the appendix.</span></span></span> When a question only includes <span id="S4.p1.1.11" class="ltx_text ltx_font_italic">women words</span>, the question is classified as a <span id="S4.p1.1.12" class="ltx_text ltx_font_italic">women question</span>, and vice versa. Questions that are not classified either as women or men are excluded. Note that this analysis is based on the VQA annotator’s perceived gender, and not on gender identity. We report our findings below.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Statistics of women/men questions (Women Qs/Men Qs) in VQA datasets. MoW is the number of men questions over the number of women questions. Ratio is the number of gender questions (Num. Women Qs <math id="S4.T2.2.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T2.2.m1.1b"><mo id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><plus id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">+</annotation></semantics></math> Num. Men Qs) over the total number of questions (Num. Total Qs).</figcaption>
<table id="S4.T2.32" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.32.31.1" class="ltx_tr">
<th id="S4.T2.32.31.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Dataset</th>
<th id="S4.T2.32.31.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Men Qs</th>
<th id="S4.T2.32.31.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Women Qs</th>
<th id="S4.T2.32.31.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">MoW</th>
<th id="S4.T2.32.31.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Gender Qs</th>
<th id="S4.T2.32.31.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Total Qs</th>
<th id="S4.T2.32.31.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Ratio (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.8.6" class="ltx_tr">
<th id="S4.T2.8.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T2.3.1.1.m1.2" class="ltx_Math" alttext="111,286" display="inline"><semantics id="S4.T2.3.1.1.m1.2a"><mrow id="S4.T2.3.1.1.m1.2.3.2" xref="S4.T2.3.1.1.m1.2.3.1.cmml"><mn id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">111</mn><mo id="S4.T2.3.1.1.m1.2.3.2.1" xref="S4.T2.3.1.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.3.1.1.m1.2.2" xref="S4.T2.3.1.1.m1.2.2.cmml">286</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.2b"><list id="S4.T2.3.1.1.m1.2.3.1.cmml" xref="S4.T2.3.1.1.m1.2.3.2"><cn type="integer" id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">111</cn><cn type="integer" id="S4.T2.3.1.1.m1.2.2.cmml" xref="S4.T2.3.1.1.m1.2.2">286</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.2c">111,286</annotation></semantics></math></td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T2.4.2.2.m1.2" class="ltx_Math" alttext="56,013" display="inline"><semantics id="S4.T2.4.2.2.m1.2a"><mrow id="S4.T2.4.2.2.m1.2.3.2" xref="S4.T2.4.2.2.m1.2.3.1.cmml"><mn id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml">56</mn><mo id="S4.T2.4.2.2.m1.2.3.2.1" xref="S4.T2.4.2.2.m1.2.3.1.cmml">,</mo><mn id="S4.T2.4.2.2.m1.2.2" xref="S4.T2.4.2.2.m1.2.2.cmml">013</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.2b"><list id="S4.T2.4.2.2.m1.2.3.1.cmml" xref="S4.T2.4.2.2.m1.2.3.2"><cn type="integer" id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">56</cn><cn type="integer" id="S4.T2.4.2.2.m1.2.2.cmml" xref="S4.T2.4.2.2.m1.2.2">013</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.2c">56,013</annotation></semantics></math></td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T2.5.3.3.m1.1" class="ltx_Math" alttext="2.0" display="inline"><semantics id="S4.T2.5.3.3.m1.1a"><mn id="S4.T2.5.3.3.m1.1.1" xref="S4.T2.5.3.3.m1.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.3.m1.1b"><cn type="float" id="S4.T2.5.3.3.m1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.3.m1.1c">2.0</annotation></semantics></math></td>
<td id="S4.T2.6.4.4" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T2.6.4.4.m1.2" class="ltx_Math" alttext="167,299" display="inline"><semantics id="S4.T2.6.4.4.m1.2a"><mrow id="S4.T2.6.4.4.m1.2.3.2" xref="S4.T2.6.4.4.m1.2.3.1.cmml"><mn id="S4.T2.6.4.4.m1.1.1" xref="S4.T2.6.4.4.m1.1.1.cmml">167</mn><mo id="S4.T2.6.4.4.m1.2.3.2.1" xref="S4.T2.6.4.4.m1.2.3.1.cmml">,</mo><mn id="S4.T2.6.4.4.m1.2.2" xref="S4.T2.6.4.4.m1.2.2.cmml">299</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.4.4.m1.2b"><list id="S4.T2.6.4.4.m1.2.3.1.cmml" xref="S4.T2.6.4.4.m1.2.3.2"><cn type="integer" id="S4.T2.6.4.4.m1.1.1.cmml" xref="S4.T2.6.4.4.m1.1.1">167</cn><cn type="integer" id="S4.T2.6.4.4.m1.2.2.cmml" xref="S4.T2.6.4.4.m1.2.2">299</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.4.4.m1.2c">167,299</annotation></semantics></math></td>
<td id="S4.T2.7.5.5" class="ltx_td ltx_align_right ltx_border_t">
<math id="S4.T2.7.5.5.m1.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S4.T2.7.5.5.m1.1a"><mn id="S4.T2.7.5.5.m1.1.1" xref="S4.T2.7.5.5.m1.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S4.T2.7.5.5.m1.1b"><cn type="float" id="S4.T2.7.5.5.m1.1.1.cmml" xref="S4.T2.7.5.5.m1.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.5.5.m1.1c">1.7</annotation></semantics></math>M</td>
<td id="S4.T2.8.6.6" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T2.8.6.6.m1.1" class="ltx_Math" alttext="9.8" display="inline"><semantics id="S4.T2.8.6.6.m1.1a"><mn id="S4.T2.8.6.6.m1.1.1" xref="S4.T2.8.6.6.m1.1.1.cmml">9.8</mn><annotation-xml encoding="MathML-Content" id="S4.T2.8.6.6.m1.1b"><cn type="float" id="S4.T2.8.6.6.m1.1.1.cmml" xref="S4.T2.8.6.6.m1.1.1">9.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.6.6.m1.1c">9.8</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.14.12" class="ltx_tr">
<th id="S4.T2.14.12.7" class="ltx_td ltx_align_left ltx_th ltx_th_row">Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S4.T2.9.7.1" class="ltx_td ltx_align_right"><math id="S4.T2.9.7.1.m1.2" class="ltx_Math" alttext="10,641" display="inline"><semantics id="S4.T2.9.7.1.m1.2a"><mrow id="S4.T2.9.7.1.m1.2.3.2" xref="S4.T2.9.7.1.m1.2.3.1.cmml"><mn id="S4.T2.9.7.1.m1.1.1" xref="S4.T2.9.7.1.m1.1.1.cmml">10</mn><mo id="S4.T2.9.7.1.m1.2.3.2.1" xref="S4.T2.9.7.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.9.7.1.m1.2.2" xref="S4.T2.9.7.1.m1.2.2.cmml">641</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.7.1.m1.2b"><list id="S4.T2.9.7.1.m1.2.3.1.cmml" xref="S4.T2.9.7.1.m1.2.3.2"><cn type="integer" id="S4.T2.9.7.1.m1.1.1.cmml" xref="S4.T2.9.7.1.m1.1.1">10</cn><cn type="integer" id="S4.T2.9.7.1.m1.2.2.cmml" xref="S4.T2.9.7.1.m1.2.2">641</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.7.1.m1.2c">10,641</annotation></semantics></math></td>
<td id="S4.T2.10.8.2" class="ltx_td ltx_align_right"><math id="S4.T2.10.8.2.m1.2" class="ltx_Math" alttext="5,030" display="inline"><semantics id="S4.T2.10.8.2.m1.2a"><mrow id="S4.T2.10.8.2.m1.2.3.2" xref="S4.T2.10.8.2.m1.2.3.1.cmml"><mn id="S4.T2.10.8.2.m1.1.1" xref="S4.T2.10.8.2.m1.1.1.cmml">5</mn><mo id="S4.T2.10.8.2.m1.2.3.2.1" xref="S4.T2.10.8.2.m1.2.3.1.cmml">,</mo><mn id="S4.T2.10.8.2.m1.2.2" xref="S4.T2.10.8.2.m1.2.2.cmml">030</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.8.2.m1.2b"><list id="S4.T2.10.8.2.m1.2.3.1.cmml" xref="S4.T2.10.8.2.m1.2.3.2"><cn type="integer" id="S4.T2.10.8.2.m1.1.1.cmml" xref="S4.T2.10.8.2.m1.1.1">5</cn><cn type="integer" id="S4.T2.10.8.2.m1.2.2.cmml" xref="S4.T2.10.8.2.m1.2.2">030</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.8.2.m1.2c">5,030</annotation></semantics></math></td>
<td id="S4.T2.11.9.3" class="ltx_td ltx_align_right"><math id="S4.T2.11.9.3.m1.1" class="ltx_Math" alttext="2.1" display="inline"><semantics id="S4.T2.11.9.3.m1.1a"><mn id="S4.T2.11.9.3.m1.1.1" xref="S4.T2.11.9.3.m1.1.1.cmml">2.1</mn><annotation-xml encoding="MathML-Content" id="S4.T2.11.9.3.m1.1b"><cn type="float" id="S4.T2.11.9.3.m1.1.1.cmml" xref="S4.T2.11.9.3.m1.1.1">2.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.9.3.m1.1c">2.1</annotation></semantics></math></td>
<td id="S4.T2.12.10.4" class="ltx_td ltx_align_right"><math id="S4.T2.12.10.4.m1.2" class="ltx_Math" alttext="15,671" display="inline"><semantics id="S4.T2.12.10.4.m1.2a"><mrow id="S4.T2.12.10.4.m1.2.3.2" xref="S4.T2.12.10.4.m1.2.3.1.cmml"><mn id="S4.T2.12.10.4.m1.1.1" xref="S4.T2.12.10.4.m1.1.1.cmml">15</mn><mo id="S4.T2.12.10.4.m1.2.3.2.1" xref="S4.T2.12.10.4.m1.2.3.1.cmml">,</mo><mn id="S4.T2.12.10.4.m1.2.2" xref="S4.T2.12.10.4.m1.2.2.cmml">671</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.10.4.m1.2b"><list id="S4.T2.12.10.4.m1.2.3.1.cmml" xref="S4.T2.12.10.4.m1.2.3.2"><cn type="integer" id="S4.T2.12.10.4.m1.1.1.cmml" xref="S4.T2.12.10.4.m1.1.1">15</cn><cn type="integer" id="S4.T2.12.10.4.m1.2.2.cmml" xref="S4.T2.12.10.4.m1.2.2">671</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.10.4.m1.2c">15,671</annotation></semantics></math></td>
<td id="S4.T2.13.11.5" class="ltx_td ltx_align_right">
<math id="S4.T2.13.11.5.m1.1" class="ltx_Math" alttext="327" display="inline"><semantics id="S4.T2.13.11.5.m1.1a"><mn id="S4.T2.13.11.5.m1.1.1" xref="S4.T2.13.11.5.m1.1.1.cmml">327</mn><annotation-xml encoding="MathML-Content" id="S4.T2.13.11.5.m1.1b"><cn type="integer" id="S4.T2.13.11.5.m1.1.1.cmml" xref="S4.T2.13.11.5.m1.1.1">327</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.11.5.m1.1c">327</annotation></semantics></math>K</td>
<td id="S4.T2.14.12.6" class="ltx_td ltx_align_right"><math id="S4.T2.14.12.6.m1.1" class="ltx_Math" alttext="4.8" display="inline"><semantics id="S4.T2.14.12.6.m1.1a"><mn id="S4.T2.14.12.6.m1.1.1" xref="S4.T2.14.12.6.m1.1.1.cmml">4.8</mn><annotation-xml encoding="MathML-Content" id="S4.T2.14.12.6.m1.1b"><cn type="float" id="S4.T2.14.12.6.m1.1.1.cmml" xref="S4.T2.14.12.6.m1.1.1">4.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.12.6.m1.1c">4.8</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.20.18" class="ltx_tr">
<th id="S4.T2.20.18.7" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S4.T2.15.13.1" class="ltx_td ltx_align_right"><math id="S4.T2.15.13.1.m1.2" class="ltx_Math" alttext="64,990" display="inline"><semantics id="S4.T2.15.13.1.m1.2a"><mrow id="S4.T2.15.13.1.m1.2.3.2" xref="S4.T2.15.13.1.m1.2.3.1.cmml"><mn id="S4.T2.15.13.1.m1.1.1" xref="S4.T2.15.13.1.m1.1.1.cmml">64</mn><mo id="S4.T2.15.13.1.m1.2.3.2.1" xref="S4.T2.15.13.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.15.13.1.m1.2.2" xref="S4.T2.15.13.1.m1.2.2.cmml">990</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.15.13.1.m1.2b"><list id="S4.T2.15.13.1.m1.2.3.1.cmml" xref="S4.T2.15.13.1.m1.2.3.2"><cn type="integer" id="S4.T2.15.13.1.m1.1.1.cmml" xref="S4.T2.15.13.1.m1.1.1">64</cn><cn type="integer" id="S4.T2.15.13.1.m1.2.2.cmml" xref="S4.T2.15.13.1.m1.2.2">990</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.13.1.m1.2c">64,990</annotation></semantics></math></td>
<td id="S4.T2.16.14.2" class="ltx_td ltx_align_right"><math id="S4.T2.16.14.2.m1.2" class="ltx_Math" alttext="31,690" display="inline"><semantics id="S4.T2.16.14.2.m1.2a"><mrow id="S4.T2.16.14.2.m1.2.3.2" xref="S4.T2.16.14.2.m1.2.3.1.cmml"><mn id="S4.T2.16.14.2.m1.1.1" xref="S4.T2.16.14.2.m1.1.1.cmml">31</mn><mo id="S4.T2.16.14.2.m1.2.3.2.1" xref="S4.T2.16.14.2.m1.2.3.1.cmml">,</mo><mn id="S4.T2.16.14.2.m1.2.2" xref="S4.T2.16.14.2.m1.2.2.cmml">690</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.16.14.2.m1.2b"><list id="S4.T2.16.14.2.m1.2.3.1.cmml" xref="S4.T2.16.14.2.m1.2.3.2"><cn type="integer" id="S4.T2.16.14.2.m1.1.1.cmml" xref="S4.T2.16.14.2.m1.1.1">31</cn><cn type="integer" id="S4.T2.16.14.2.m1.2.2.cmml" xref="S4.T2.16.14.2.m1.2.2">690</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.14.2.m1.2c">31,690</annotation></semantics></math></td>
<td id="S4.T2.17.15.3" class="ltx_td ltx_align_right"><math id="S4.T2.17.15.3.m1.1" class="ltx_Math" alttext="2.1" display="inline"><semantics id="S4.T2.17.15.3.m1.1a"><mn id="S4.T2.17.15.3.m1.1.1" xref="S4.T2.17.15.3.m1.1.1.cmml">2.1</mn><annotation-xml encoding="MathML-Content" id="S4.T2.17.15.3.m1.1b"><cn type="float" id="S4.T2.17.15.3.m1.1.1.cmml" xref="S4.T2.17.15.3.m1.1.1">2.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.15.3.m1.1c">2.1</annotation></semantics></math></td>
<td id="S4.T2.18.16.4" class="ltx_td ltx_align_right"><math id="S4.T2.18.16.4.m1.2" class="ltx_Math" alttext="96,680" display="inline"><semantics id="S4.T2.18.16.4.m1.2a"><mrow id="S4.T2.18.16.4.m1.2.3.2" xref="S4.T2.18.16.4.m1.2.3.1.cmml"><mn id="S4.T2.18.16.4.m1.1.1" xref="S4.T2.18.16.4.m1.1.1.cmml">96</mn><mo id="S4.T2.18.16.4.m1.2.3.2.1" xref="S4.T2.18.16.4.m1.2.3.1.cmml">,</mo><mn id="S4.T2.18.16.4.m1.2.2" xref="S4.T2.18.16.4.m1.2.2.cmml">680</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.18.16.4.m1.2b"><list id="S4.T2.18.16.4.m1.2.3.1.cmml" xref="S4.T2.18.16.4.m1.2.3.2"><cn type="integer" id="S4.T2.18.16.4.m1.1.1.cmml" xref="S4.T2.18.16.4.m1.1.1">96</cn><cn type="integer" id="S4.T2.18.16.4.m1.2.2.cmml" xref="S4.T2.18.16.4.m1.2.2">680</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.16.4.m1.2c">96,680</annotation></semantics></math></td>
<td id="S4.T2.19.17.5" class="ltx_td ltx_align_right">
<math id="S4.T2.19.17.5.m1.1" class="ltx_Math" alttext="658" display="inline"><semantics id="S4.T2.19.17.5.m1.1a"><mn id="S4.T2.19.17.5.m1.1.1" xref="S4.T2.19.17.5.m1.1.1.cmml">658</mn><annotation-xml encoding="MathML-Content" id="S4.T2.19.17.5.m1.1b"><cn type="integer" id="S4.T2.19.17.5.m1.1.1.cmml" xref="S4.T2.19.17.5.m1.1.1">658</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.17.5.m1.1c">658</annotation></semantics></math>K</td>
<td id="S4.T2.20.18.6" class="ltx_td ltx_align_right"><math id="S4.T2.20.18.6.m1.1" class="ltx_Math" alttext="14.7" display="inline"><semantics id="S4.T2.20.18.6.m1.1a"><mn id="S4.T2.20.18.6.m1.1.1" xref="S4.T2.20.18.6.m1.1.1.cmml">14.7</mn><annotation-xml encoding="MathML-Content" id="S4.T2.20.18.6.m1.1b"><cn type="float" id="S4.T2.20.18.6.m1.1.1.cmml" xref="S4.T2.20.18.6.m1.1.1">14.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.18.6.m1.1c">14.7</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.26.24" class="ltx_tr">
<th id="S4.T2.26.24.7" class="ltx_td ltx_align_left ltx_th ltx_th_row">GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T2.21.19.1" class="ltx_td ltx_align_right"><math id="S4.T2.21.19.1.m1.2" class="ltx_Math" alttext="296,242" display="inline"><semantics id="S4.T2.21.19.1.m1.2a"><mrow id="S4.T2.21.19.1.m1.2.3.2" xref="S4.T2.21.19.1.m1.2.3.1.cmml"><mn id="S4.T2.21.19.1.m1.1.1" xref="S4.T2.21.19.1.m1.1.1.cmml">296</mn><mo id="S4.T2.21.19.1.m1.2.3.2.1" xref="S4.T2.21.19.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.21.19.1.m1.2.2" xref="S4.T2.21.19.1.m1.2.2.cmml">242</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.21.19.1.m1.2b"><list id="S4.T2.21.19.1.m1.2.3.1.cmml" xref="S4.T2.21.19.1.m1.2.3.2"><cn type="integer" id="S4.T2.21.19.1.m1.1.1.cmml" xref="S4.T2.21.19.1.m1.1.1">296</cn><cn type="integer" id="S4.T2.21.19.1.m1.2.2.cmml" xref="S4.T2.21.19.1.m1.2.2">242</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.19.1.m1.2c">296,242</annotation></semantics></math></td>
<td id="S4.T2.22.20.2" class="ltx_td ltx_align_right"><math id="S4.T2.22.20.2.m1.2" class="ltx_Math" alttext="178,518" display="inline"><semantics id="S4.T2.22.20.2.m1.2a"><mrow id="S4.T2.22.20.2.m1.2.3.2" xref="S4.T2.22.20.2.m1.2.3.1.cmml"><mn id="S4.T2.22.20.2.m1.1.1" xref="S4.T2.22.20.2.m1.1.1.cmml">178</mn><mo id="S4.T2.22.20.2.m1.2.3.2.1" xref="S4.T2.22.20.2.m1.2.3.1.cmml">,</mo><mn id="S4.T2.22.20.2.m1.2.2" xref="S4.T2.22.20.2.m1.2.2.cmml">518</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.22.20.2.m1.2b"><list id="S4.T2.22.20.2.m1.2.3.1.cmml" xref="S4.T2.22.20.2.m1.2.3.2"><cn type="integer" id="S4.T2.22.20.2.m1.1.1.cmml" xref="S4.T2.22.20.2.m1.1.1">178</cn><cn type="integer" id="S4.T2.22.20.2.m1.2.2.cmml" xref="S4.T2.22.20.2.m1.2.2">518</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.20.2.m1.2c">178,518</annotation></semantics></math></td>
<td id="S4.T2.23.21.3" class="ltx_td ltx_align_right"><math id="S4.T2.23.21.3.m1.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S4.T2.23.21.3.m1.1a"><mn id="S4.T2.23.21.3.m1.1.1" xref="S4.T2.23.21.3.m1.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S4.T2.23.21.3.m1.1b"><cn type="float" id="S4.T2.23.21.3.m1.1.1.cmml" xref="S4.T2.23.21.3.m1.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.21.3.m1.1c">1.7</annotation></semantics></math></td>
<td id="S4.T2.24.22.4" class="ltx_td ltx_align_right"><math id="S4.T2.24.22.4.m1.2" class="ltx_Math" alttext="474,760" display="inline"><semantics id="S4.T2.24.22.4.m1.2a"><mrow id="S4.T2.24.22.4.m1.2.3.2" xref="S4.T2.24.22.4.m1.2.3.1.cmml"><mn id="S4.T2.24.22.4.m1.1.1" xref="S4.T2.24.22.4.m1.1.1.cmml">474</mn><mo id="S4.T2.24.22.4.m1.2.3.2.1" xref="S4.T2.24.22.4.m1.2.3.1.cmml">,</mo><mn id="S4.T2.24.22.4.m1.2.2" xref="S4.T2.24.22.4.m1.2.2.cmml">760</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.24.22.4.m1.2b"><list id="S4.T2.24.22.4.m1.2.3.1.cmml" xref="S4.T2.24.22.4.m1.2.3.2"><cn type="integer" id="S4.T2.24.22.4.m1.1.1.cmml" xref="S4.T2.24.22.4.m1.1.1">474</cn><cn type="integer" id="S4.T2.24.22.4.m1.2.2.cmml" xref="S4.T2.24.22.4.m1.2.2">760</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.22.4.m1.2c">474,760</annotation></semantics></math></td>
<td id="S4.T2.25.23.5" class="ltx_td ltx_align_right">
<math id="S4.T2.25.23.5.m1.1" class="ltx_Math" alttext="4.3" display="inline"><semantics id="S4.T2.25.23.5.m1.1a"><mn id="S4.T2.25.23.5.m1.1.1" xref="S4.T2.25.23.5.m1.1.1.cmml">4.3</mn><annotation-xml encoding="MathML-Content" id="S4.T2.25.23.5.m1.1b"><cn type="float" id="S4.T2.25.23.5.m1.1.1.cmml" xref="S4.T2.25.23.5.m1.1.1">4.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.23.5.m1.1c">4.3</annotation></semantics></math>M</td>
<td id="S4.T2.26.24.6" class="ltx_td ltx_align_right"><math id="S4.T2.26.24.6.m1.1" class="ltx_Math" alttext="11.0" display="inline"><semantics id="S4.T2.26.24.6.m1.1a"><mn id="S4.T2.26.24.6.m1.1.1" xref="S4.T2.26.24.6.m1.1.1.cmml">11.0</mn><annotation-xml encoding="MathML-Content" id="S4.T2.26.24.6.m1.1b"><cn type="float" id="S4.T2.26.24.6.m1.1.1.cmml" xref="S4.T2.26.24.6.m1.1.1">11.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.24.6.m1.1c">11.0</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.32.30" class="ltx_tr">
<th id="S4.T2.32.30.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T2.27.25.1" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T2.27.25.1.m1.1" class="ltx_Math" alttext="868" display="inline"><semantics id="S4.T2.27.25.1.m1.1a"><mn id="S4.T2.27.25.1.m1.1.1" xref="S4.T2.27.25.1.m1.1.1.cmml">868</mn><annotation-xml encoding="MathML-Content" id="S4.T2.27.25.1.m1.1b"><cn type="integer" id="S4.T2.27.25.1.m1.1.1.cmml" xref="S4.T2.27.25.1.m1.1.1">868</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.25.1.m1.1c">868</annotation></semantics></math></td>
<td id="S4.T2.28.26.2" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T2.28.26.2.m1.1" class="ltx_Math" alttext="443" display="inline"><semantics id="S4.T2.28.26.2.m1.1a"><mn id="S4.T2.28.26.2.m1.1.1" xref="S4.T2.28.26.2.m1.1.1.cmml">443</mn><annotation-xml encoding="MathML-Content" id="S4.T2.28.26.2.m1.1b"><cn type="integer" id="S4.T2.28.26.2.m1.1.1.cmml" xref="S4.T2.28.26.2.m1.1.1">443</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.26.2.m1.1c">443</annotation></semantics></math></td>
<td id="S4.T2.29.27.3" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T2.29.27.3.m1.1" class="ltx_Math" alttext="2.0" display="inline"><semantics id="S4.T2.29.27.3.m1.1a"><mn id="S4.T2.29.27.3.m1.1.1" xref="S4.T2.29.27.3.m1.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="S4.T2.29.27.3.m1.1b"><cn type="float" id="S4.T2.29.27.3.m1.1.1.cmml" xref="S4.T2.29.27.3.m1.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.27.3.m1.1c">2.0</annotation></semantics></math></td>
<td id="S4.T2.30.28.4" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T2.30.28.4.m1.2" class="ltx_Math" alttext="1,311" display="inline"><semantics id="S4.T2.30.28.4.m1.2a"><mrow id="S4.T2.30.28.4.m1.2.3.2" xref="S4.T2.30.28.4.m1.2.3.1.cmml"><mn id="S4.T2.30.28.4.m1.1.1" xref="S4.T2.30.28.4.m1.1.1.cmml">1</mn><mo id="S4.T2.30.28.4.m1.2.3.2.1" xref="S4.T2.30.28.4.m1.2.3.1.cmml">,</mo><mn id="S4.T2.30.28.4.m1.2.2" xref="S4.T2.30.28.4.m1.2.2.cmml">311</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.30.28.4.m1.2b"><list id="S4.T2.30.28.4.m1.2.3.1.cmml" xref="S4.T2.30.28.4.m1.2.3.2"><cn type="integer" id="S4.T2.30.28.4.m1.1.1.cmml" xref="S4.T2.30.28.4.m1.1.1">1</cn><cn type="integer" id="S4.T2.30.28.4.m1.2.2.cmml" xref="S4.T2.30.28.4.m1.2.2">311</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.28.4.m1.2c">1,311</annotation></semantics></math></td>
<td id="S4.T2.31.29.5" class="ltx_td ltx_align_right ltx_border_bb">
<math id="S4.T2.31.29.5.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S4.T2.31.29.5.m1.1a"><mn id="S4.T2.31.29.5.m1.1.1" xref="S4.T2.31.29.5.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S4.T2.31.29.5.m1.1b"><cn type="integer" id="S4.T2.31.29.5.m1.1.1.cmml" xref="S4.T2.31.29.5.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.29.5.m1.1c">14</annotation></semantics></math>K</td>
<td id="S4.T2.32.30.6" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T2.32.30.6.m1.1" class="ltx_Math" alttext="9.4" display="inline"><semantics id="S4.T2.32.30.6.m1.1a"><mn id="S4.T2.32.30.6.m1.1.1" xref="S4.T2.32.30.6.m1.1.1.cmml">9.4</mn><annotation-xml encoding="MathML-Content" id="S4.T2.32.30.6.m1.1b"><cn type="float" id="S4.T2.32.30.6.m1.1.1.cmml" xref="S4.T2.32.30.6.m1.1.1">9.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.30.6.m1.1c">9.4</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Questions about men are dominant</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The statistics of the number of women and men questions for each dataset are reported in Table <a href="#S4.T2" title="Table 2 ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The number of questions about men is about twice as large as the number of questions about women in all the datasets.
This tendency is consistent with the result in <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite>, which shows that there are more than twice as many men images as women images in the COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2014</a>)</cite>. As all the datasets in Table <a href="#S4.T2" title="Table 2 ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are based on COCO images, the root of the underrepresentation of women in VQA datasets may come from the original selection of images.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Answer distributions are skewed toward each gender</h3>

<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="82" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x4.png" id="S4.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="90" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Top-20 frequent answers in VQA 2.0. <span id="S4.F3.5.1" class="ltx_text ltx_font_bold">Above</span>: Frequent answers for women questions (<span id="S4.F3.6.2" class="ltx_text" style="color:#FF8C00;">orange</span>). For the comparison, we also show the ratio of the answers over men questions (<span id="S4.F3.7.3" class="ltx_text" style="color:#006400;">green</span>). <span id="S4.F3.8.4" class="ltx_text ltx_font_bold">Below</span>: Frequent answers for men questions. As in above, we also show the ratio of answers for women questions. A large difference in ratio indicates that the answer is skewed toward certain gender.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">In Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2. Answer distributions are skewed toward each gender ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show the top-<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">20</annotation></semantics></math> answers for women (above) and men (below) questions in the VQA 2.0 dataset. We filter out yes/no and numeric answers. Each distribution is normalized by the number of questions for the corresponding gender. Comparing the two answer distributions, we can see that there are more frequent answers about sports in men questions (<span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_italic">frisbee</span>, <span id="S4.SS2.p1.2.2" class="ltx_text ltx_font_italic">skateboard</span>, <span id="S4.SS2.p1.2.3" class="ltx_text ltx_font_italic">skateboarding</span>, <span id="S4.SS2.p1.2.4" class="ltx_text ltx_font_italic">baseball</span>, <span id="S4.SS2.p1.2.5" class="ltx_text ltx_font_italic">tennis</span>, <span id="S4.SS2.p1.2.6" class="ltx_text ltx_font_italic">surfing</span>, and <span id="S4.SS2.p1.2.7" class="ltx_text ltx_font_italic">surfboard</span>) than in women questions (<span id="S4.SS2.p1.2.8" class="ltx_text ltx_font_italic">tennis</span>). Also, the differences in the ratios about sport answers are large between the two genders, perpetuating the stereotype that sport is an activity predominantly masculine.
The top-<math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">20</annotation></semantics></math> answers for women questions with a notably higher ratio than men are <span id="S4.SS2.p1.2.9" class="ltx_text ltx_font_italic">pink</span>, <span id="S4.SS2.p1.2.10" class="ltx_text ltx_font_italic">purple</span>, <span id="S4.SS2.p1.2.11" class="ltx_text ltx_font_italic">blonde</span>, and <span id="S4.SS2.p1.2.12" class="ltx_text ltx_font_italic">umbrella</span>, most of them strongly associated with the traditional gender stereotype of feminine.
We find these patterns are also exhibited in other datasets, <span id="S4.SS2.p1.2.13" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS2.p1.2.14" class="ltx_text ltx_font_italic">g</span>., Figure 1 in the appendix shows that the answers about sports are more frequent in men questions in the Visual7W dataset.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2. Answer distributions are skewed toward each gender ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the women (above) and men (below) top-20 frequent answer distribution for the specific question type <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">what is this</span> in VQA 2.0 dataset, where the skew is more prominent. Answers for men questions include multiple sports words (<span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_italic">g</span>., <span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_italic">surfing</span>, <span id="S4.SS2.p2.1.5" class="ltx_text ltx_font_italic">skateboarding</span>, <span id="S4.SS2.p2.1.6" class="ltx_text ltx_font_italic">skiing</span>), with higher ratios than those of women. On the other hand, in women questions more motionless words appear (<span id="S4.SS2.p2.1.7" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS2.p2.1.8" class="ltx_text ltx_font_italic">g</span>., <span id="S4.SS2.p2.1.9" class="ltx_text ltx_font_italic">posing</span>, <span id="S4.SS2.p2.1.10" class="ltx_text ltx_font_italic">sitting</span>, <span id="S4.SS2.p2.1.11" class="ltx_text ltx_font_italic">smiling</span>, <span id="S4.SS2.p2.1.12" class="ltx_text ltx_font_italic">talking on phone</span>).</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x5.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="88" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x6.png" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="83" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Top-20 frequent answers for the question type <span id="S4.F4.6.1" class="ltx_text ltx_font_italic">what is this</span> in VQA 2.0. <span id="S4.F4.7.2" class="ltx_text ltx_font_bold">Above</span>: Frequent answers for women questions (<span id="S4.F4.8.3" class="ltx_text" style="color:#FF8C00;">orange</span>). <span id="S4.F4.9.4" class="ltx_text ltx_font_bold">Below</span>: Frequent answers for men questions (<span id="S4.F4.10.5" class="ltx_text" style="color:#006400;">green</span>). As in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2. Answer distributions are skewed toward each gender ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the answer ratios between the two genders.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Gender-answer correlations reflect gender stereotypes and discrimination</h3>

<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x7.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="75" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x8.png" id="S4.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="85" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Top-20 answers that are co-related with women questions (<span id="S4.F5.3.1" class="ltx_text ltx_font_bold">above</span>) and men questions (<span id="S4.F5.4.2" class="ltx_text ltx_font_bold">below</span>) in the VQA 2.0 dataset.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.6" class="ltx_p">We calculate the correlation between answers and women/men questions by utilizing the bias score (BS) defined in <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite>. We adapt the definition of BS to remove the influence of the difference of the number of women/men questions. Let <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="q_{w}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑞</ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">q_{w}</annotation></semantics></math> and <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="q_{m}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝑞</ci><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">q_{m}</annotation></semantics></math> denote a women question and a men question respectively, and <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="a\in A" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mi id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">a</mi><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">∈</mo><mi id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><in id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1"></in><ci id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">𝑎</ci><ci id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">a\in A</annotation></semantics></math> the set of the answers. We filter answers that do not appear more than <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mi id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><ci id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">n</annotation></semantics></math> times<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><math id="footnote3.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="footnote3.m1.1b"><mi id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><ci id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">n</annotation></semantics></math> is different among the datasets due to the size of each dataset. The detailed setting can be found in the appendix.</span></span></span> in women/men questions.
Our BS gives the degree to which an answer <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mi id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><ci id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">a</annotation></semantics></math> is biased with respect to a men questions <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="q_{m}" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><msub id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml"><mi id="S4.SS3.p1.6.m6.1.1.2" xref="S4.SS3.p1.6.m6.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.6.m6.1.1.3" xref="S4.SS3.p1.6.m6.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><apply id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.6.m6.1.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.p1.6.m6.1.1.2.cmml" xref="S4.SS3.p1.6.m6.1.1.2">𝑞</ci><ci id="S4.SS3.p1.6.m6.1.1.3.cmml" xref="S4.SS3.p1.6.m6.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">q_{m}</annotation></semantics></math>:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.8" class="ltx_Math" alttext="\text{BS}(a,q_{m})=\frac{c(a,q_{m})}{c(a,q_{m})+rc(a,q_{w})}," display="block"><semantics id="S4.E1.m1.8a"><mrow id="S4.E1.m1.8.8.1" xref="S4.E1.m1.8.8.1.1.cmml"><mrow id="S4.E1.m1.8.8.1.1" xref="S4.E1.m1.8.8.1.1.cmml"><mrow id="S4.E1.m1.8.8.1.1.1" xref="S4.E1.m1.8.8.1.1.1.cmml"><mtext id="S4.E1.m1.8.8.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.3a.cmml">BS</mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.8.8.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.8.8.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.8.8.1.1.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.1.2.cmml">(</mo><mi id="S4.E1.m1.7.7" xref="S4.E1.m1.7.7.cmml">a</mi><mo id="S4.E1.m1.8.8.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.2.cmml">,</mo><msub id="S4.E1.m1.8.8.1.1.1.1.1.1" xref="S4.E1.m1.8.8.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.2" xref="S4.E1.m1.8.8.1.1.1.1.1.1.2.cmml">q</mi><mi id="S4.E1.m1.8.8.1.1.1.1.1.1.3" xref="S4.E1.m1.8.8.1.1.1.1.1.1.3.cmml">m</mi></msub><mo stretchy="false" id="S4.E1.m1.8.8.1.1.1.1.1.4" xref="S4.E1.m1.8.8.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.8.8.1.1.2" xref="S4.E1.m1.8.8.1.1.2.cmml">=</mo><mfrac id="S4.E1.m1.6.6" xref="S4.E1.m1.6.6.cmml"><mrow id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml"><mi id="S4.E1.m1.2.2.2.4" xref="S4.E1.m1.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.3" xref="S4.E1.m1.2.2.2.3.cmml">​</mo><mrow id="S4.E1.m1.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.2.2.1.2" xref="S4.E1.m1.2.2.2.2.2.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">a</mi><mo id="S4.E1.m1.2.2.2.2.1.3" xref="S4.E1.m1.2.2.2.2.2.cmml">,</mo><msub id="S4.E1.m1.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.1.1.cmml"><mi id="S4.E1.m1.2.2.2.2.1.1.2" xref="S4.E1.m1.2.2.2.2.1.1.2.cmml">q</mi><mi id="S4.E1.m1.2.2.2.2.1.1.3" xref="S4.E1.m1.2.2.2.2.1.1.3.cmml">m</mi></msub><mo stretchy="false" id="S4.E1.m1.2.2.2.2.1.4" xref="S4.E1.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S4.E1.m1.6.6.6" xref="S4.E1.m1.6.6.6.cmml"><mrow id="S4.E1.m1.5.5.5.3" xref="S4.E1.m1.5.5.5.3.cmml"><mi id="S4.E1.m1.5.5.5.3.3" xref="S4.E1.m1.5.5.5.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.5.3.2" xref="S4.E1.m1.5.5.5.3.2.cmml">​</mo><mrow id="S4.E1.m1.5.5.5.3.1.1" xref="S4.E1.m1.5.5.5.3.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.5.5.5.3.1.1.2" xref="S4.E1.m1.5.5.5.3.1.2.cmml">(</mo><mi id="S4.E1.m1.3.3.3.1" xref="S4.E1.m1.3.3.3.1.cmml">a</mi><mo id="S4.E1.m1.5.5.5.3.1.1.3" xref="S4.E1.m1.5.5.5.3.1.2.cmml">,</mo><msub id="S4.E1.m1.5.5.5.3.1.1.1" xref="S4.E1.m1.5.5.5.3.1.1.1.cmml"><mi id="S4.E1.m1.5.5.5.3.1.1.1.2" xref="S4.E1.m1.5.5.5.3.1.1.1.2.cmml">q</mi><mi id="S4.E1.m1.5.5.5.3.1.1.1.3" xref="S4.E1.m1.5.5.5.3.1.1.1.3.cmml">m</mi></msub><mo stretchy="false" id="S4.E1.m1.5.5.5.3.1.1.4" xref="S4.E1.m1.5.5.5.3.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.6.6.6.5" xref="S4.E1.m1.6.6.6.5.cmml">+</mo><mrow id="S4.E1.m1.6.6.6.4" xref="S4.E1.m1.6.6.6.4.cmml"><mi id="S4.E1.m1.6.6.6.4.3" xref="S4.E1.m1.6.6.6.4.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.6.4.2" xref="S4.E1.m1.6.6.6.4.2.cmml">​</mo><mi id="S4.E1.m1.6.6.6.4.4" xref="S4.E1.m1.6.6.6.4.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.6.6.6.4.2a" xref="S4.E1.m1.6.6.6.4.2.cmml">​</mo><mrow id="S4.E1.m1.6.6.6.4.1.1" xref="S4.E1.m1.6.6.6.4.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.6.6.6.4.1.1.2" xref="S4.E1.m1.6.6.6.4.1.2.cmml">(</mo><mi id="S4.E1.m1.4.4.4.2" xref="S4.E1.m1.4.4.4.2.cmml">a</mi><mo id="S4.E1.m1.6.6.6.4.1.1.3" xref="S4.E1.m1.6.6.6.4.1.2.cmml">,</mo><msub id="S4.E1.m1.6.6.6.4.1.1.1" xref="S4.E1.m1.6.6.6.4.1.1.1.cmml"><mi id="S4.E1.m1.6.6.6.4.1.1.1.2" xref="S4.E1.m1.6.6.6.4.1.1.1.2.cmml">q</mi><mi id="S4.E1.m1.6.6.6.4.1.1.1.3" xref="S4.E1.m1.6.6.6.4.1.1.1.3.cmml">w</mi></msub><mo stretchy="false" id="S4.E1.m1.6.6.6.4.1.1.4" xref="S4.E1.m1.6.6.6.4.1.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id="S4.E1.m1.8.8.1.2" xref="S4.E1.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.8b"><apply id="S4.E1.m1.8.8.1.1.cmml" xref="S4.E1.m1.8.8.1"><eq id="S4.E1.m1.8.8.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.2"></eq><apply id="S4.E1.m1.8.8.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1"><times id="S4.E1.m1.8.8.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.2"></times><ci id="S4.E1.m1.8.8.1.1.1.3a.cmml" xref="S4.E1.m1.8.8.1.1.1.3"><mtext id="S4.E1.m1.8.8.1.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.1.3">BS</mtext></ci><interval closure="open" id="S4.E1.m1.8.8.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1"><ci id="S4.E1.m1.7.7.cmml" xref="S4.E1.m1.7.7">𝑎</ci><apply id="S4.E1.m1.8.8.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.2">𝑞</ci><ci id="S4.E1.m1.8.8.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.8.8.1.1.1.1.1.1.3">𝑚</ci></apply></interval></apply><apply id="S4.E1.m1.6.6.cmml" xref="S4.E1.m1.6.6"><divide id="S4.E1.m1.6.6.7.cmml" xref="S4.E1.m1.6.6"></divide><apply id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"><times id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.3"></times><ci id="S4.E1.m1.2.2.2.4.cmml" xref="S4.E1.m1.2.2.2.4">𝑐</ci><interval closure="open" id="S4.E1.m1.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1"><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">𝑎</ci><apply id="S4.E1.m1.2.2.2.2.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.2">𝑞</ci><ci id="S4.E1.m1.2.2.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.3">𝑚</ci></apply></interval></apply><apply id="S4.E1.m1.6.6.6.cmml" xref="S4.E1.m1.6.6.6"><plus id="S4.E1.m1.6.6.6.5.cmml" xref="S4.E1.m1.6.6.6.5"></plus><apply id="S4.E1.m1.5.5.5.3.cmml" xref="S4.E1.m1.5.5.5.3"><times id="S4.E1.m1.5.5.5.3.2.cmml" xref="S4.E1.m1.5.5.5.3.2"></times><ci id="S4.E1.m1.5.5.5.3.3.cmml" xref="S4.E1.m1.5.5.5.3.3">𝑐</ci><interval closure="open" id="S4.E1.m1.5.5.5.3.1.2.cmml" xref="S4.E1.m1.5.5.5.3.1.1"><ci id="S4.E1.m1.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3.1">𝑎</ci><apply id="S4.E1.m1.5.5.5.3.1.1.1.cmml" xref="S4.E1.m1.5.5.5.3.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.5.3.1.1.1.1.cmml" xref="S4.E1.m1.5.5.5.3.1.1.1">subscript</csymbol><ci id="S4.E1.m1.5.5.5.3.1.1.1.2.cmml" xref="S4.E1.m1.5.5.5.3.1.1.1.2">𝑞</ci><ci id="S4.E1.m1.5.5.5.3.1.1.1.3.cmml" xref="S4.E1.m1.5.5.5.3.1.1.1.3">𝑚</ci></apply></interval></apply><apply id="S4.E1.m1.6.6.6.4.cmml" xref="S4.E1.m1.6.6.6.4"><times id="S4.E1.m1.6.6.6.4.2.cmml" xref="S4.E1.m1.6.6.6.4.2"></times><ci id="S4.E1.m1.6.6.6.4.3.cmml" xref="S4.E1.m1.6.6.6.4.3">𝑟</ci><ci id="S4.E1.m1.6.6.6.4.4.cmml" xref="S4.E1.m1.6.6.6.4.4">𝑐</ci><interval closure="open" id="S4.E1.m1.6.6.6.4.1.2.cmml" xref="S4.E1.m1.6.6.6.4.1.1"><ci id="S4.E1.m1.4.4.4.2.cmml" xref="S4.E1.m1.4.4.4.2">𝑎</ci><apply id="S4.E1.m1.6.6.6.4.1.1.1.cmml" xref="S4.E1.m1.6.6.6.4.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.6.4.1.1.1.1.cmml" xref="S4.E1.m1.6.6.6.4.1.1.1">subscript</csymbol><ci id="S4.E1.m1.6.6.6.4.1.1.1.2.cmml" xref="S4.E1.m1.6.6.6.4.1.1.1.2">𝑞</ci><ci id="S4.E1.m1.6.6.6.4.1.1.1.3.cmml" xref="S4.E1.m1.6.6.6.4.1.1.1.3">𝑤</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.8c">\text{BS}(a,q_{m})=\frac{c(a,q_{m})}{c(a,q_{m})+rc(a,q_{w})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS3.p1.19" class="ltx_p">where <math id="S4.SS3.p1.7.m1.2" class="ltx_Math" alttext="c(a,q)" display="inline"><semantics id="S4.SS3.p1.7.m1.2a"><mrow id="S4.SS3.p1.7.m1.2.3" xref="S4.SS3.p1.7.m1.2.3.cmml"><mi id="S4.SS3.p1.7.m1.2.3.2" xref="S4.SS3.p1.7.m1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.7.m1.2.3.1" xref="S4.SS3.p1.7.m1.2.3.1.cmml">​</mo><mrow id="S4.SS3.p1.7.m1.2.3.3.2" xref="S4.SS3.p1.7.m1.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS3.p1.7.m1.2.3.3.2.1" xref="S4.SS3.p1.7.m1.2.3.3.1.cmml">(</mo><mi id="S4.SS3.p1.7.m1.1.1" xref="S4.SS3.p1.7.m1.1.1.cmml">a</mi><mo id="S4.SS3.p1.7.m1.2.3.3.2.2" xref="S4.SS3.p1.7.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS3.p1.7.m1.2.2" xref="S4.SS3.p1.7.m1.2.2.cmml">q</mi><mo stretchy="false" id="S4.SS3.p1.7.m1.2.3.3.2.3" xref="S4.SS3.p1.7.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m1.2b"><apply id="S4.SS3.p1.7.m1.2.3.cmml" xref="S4.SS3.p1.7.m1.2.3"><times id="S4.SS3.p1.7.m1.2.3.1.cmml" xref="S4.SS3.p1.7.m1.2.3.1"></times><ci id="S4.SS3.p1.7.m1.2.3.2.cmml" xref="S4.SS3.p1.7.m1.2.3.2">𝑐</ci><interval closure="open" id="S4.SS3.p1.7.m1.2.3.3.1.cmml" xref="S4.SS3.p1.7.m1.2.3.3.2"><ci id="S4.SS3.p1.7.m1.1.1.cmml" xref="S4.SS3.p1.7.m1.1.1">𝑎</ci><ci id="S4.SS3.p1.7.m1.2.2.cmml" xref="S4.SS3.p1.7.m1.2.2">𝑞</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m1.2c">c(a,q)</annotation></semantics></math> is the number of co-occurrences of <math id="S4.SS3.p1.8.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS3.p1.8.m2.1a"><mi id="S4.SS3.p1.8.m2.1.1" xref="S4.SS3.p1.8.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m2.1b"><ci id="S4.SS3.p1.8.m2.1.1.cmml" xref="S4.SS3.p1.8.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m2.1c">a</annotation></semantics></math> and <math id="S4.SS3.p1.9.m3.1" class="ltx_Math" alttext="q_{w}" display="inline"><semantics id="S4.SS3.p1.9.m3.1a"><msub id="S4.SS3.p1.9.m3.1.1" xref="S4.SS3.p1.9.m3.1.1.cmml"><mi id="S4.SS3.p1.9.m3.1.1.2" xref="S4.SS3.p1.9.m3.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.9.m3.1.1.3" xref="S4.SS3.p1.9.m3.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m3.1b"><apply id="S4.SS3.p1.9.m3.1.1.cmml" xref="S4.SS3.p1.9.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.9.m3.1.1.1.cmml" xref="S4.SS3.p1.9.m3.1.1">subscript</csymbol><ci id="S4.SS3.p1.9.m3.1.1.2.cmml" xref="S4.SS3.p1.9.m3.1.1.2">𝑞</ci><ci id="S4.SS3.p1.9.m3.1.1.3.cmml" xref="S4.SS3.p1.9.m3.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m3.1c">q_{w}</annotation></semantics></math> or <math id="S4.SS3.p1.10.m4.1" class="ltx_Math" alttext="q_{m}" display="inline"><semantics id="S4.SS3.p1.10.m4.1a"><msub id="S4.SS3.p1.10.m4.1.1" xref="S4.SS3.p1.10.m4.1.1.cmml"><mi id="S4.SS3.p1.10.m4.1.1.2" xref="S4.SS3.p1.10.m4.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.10.m4.1.1.3" xref="S4.SS3.p1.10.m4.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.10.m4.1b"><apply id="S4.SS3.p1.10.m4.1.1.cmml" xref="S4.SS3.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.10.m4.1.1.1.cmml" xref="S4.SS3.p1.10.m4.1.1">subscript</csymbol><ci id="S4.SS3.p1.10.m4.1.1.2.cmml" xref="S4.SS3.p1.10.m4.1.1.2">𝑞</ci><ci id="S4.SS3.p1.10.m4.1.1.3.cmml" xref="S4.SS3.p1.10.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.10.m4.1c">q_{m}</annotation></semantics></math>, and <math id="S4.SS3.p1.11.m5.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.SS3.p1.11.m5.1a"><mi id="S4.SS3.p1.11.m5.1.1" xref="S4.SS3.p1.11.m5.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.11.m5.1b"><ci id="S4.SS3.p1.11.m5.1.1.cmml" xref="S4.SS3.p1.11.m5.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.11.m5.1c">r</annotation></semantics></math> is the ratio of the number of the men and women questions (<span id="S4.SS3.p1.19.1" class="ltx_text ltx_font_italic">i</span>.<span id="S4.SS3.p1.19.2" class="ltx_text ltx_font_italic">e</span>., <math id="S4.SS3.p1.12.m6.1" class="ltx_Math" alttext="r=\text{\# men questions}/\text{\# women questions}" display="inline"><semantics id="S4.SS3.p1.12.m6.1a"><mrow id="S4.SS3.p1.12.m6.1.1" xref="S4.SS3.p1.12.m6.1.1.cmml"><mi id="S4.SS3.p1.12.m6.1.1.2" xref="S4.SS3.p1.12.m6.1.1.2.cmml">r</mi><mo id="S4.SS3.p1.12.m6.1.1.1" xref="S4.SS3.p1.12.m6.1.1.1.cmml">=</mo><mrow id="S4.SS3.p1.12.m6.1.1.3" xref="S4.SS3.p1.12.m6.1.1.3.cmml"><mtext id="S4.SS3.p1.12.m6.1.1.3.2" xref="S4.SS3.p1.12.m6.1.1.3.2a.cmml"># men questions</mtext><mo id="S4.SS3.p1.12.m6.1.1.3.1" xref="S4.SS3.p1.12.m6.1.1.3.1.cmml">/</mo><mtext id="S4.SS3.p1.12.m6.1.1.3.3" xref="S4.SS3.p1.12.m6.1.1.3.3a.cmml"># women questions</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.12.m6.1b"><apply id="S4.SS3.p1.12.m6.1.1.cmml" xref="S4.SS3.p1.12.m6.1.1"><eq id="S4.SS3.p1.12.m6.1.1.1.cmml" xref="S4.SS3.p1.12.m6.1.1.1"></eq><ci id="S4.SS3.p1.12.m6.1.1.2.cmml" xref="S4.SS3.p1.12.m6.1.1.2">𝑟</ci><apply id="S4.SS3.p1.12.m6.1.1.3.cmml" xref="S4.SS3.p1.12.m6.1.1.3"><divide id="S4.SS3.p1.12.m6.1.1.3.1.cmml" xref="S4.SS3.p1.12.m6.1.1.3.1"></divide><ci id="S4.SS3.p1.12.m6.1.1.3.2a.cmml" xref="S4.SS3.p1.12.m6.1.1.3.2"><mtext id="S4.SS3.p1.12.m6.1.1.3.2.cmml" xref="S4.SS3.p1.12.m6.1.1.3.2"># men questions</mtext></ci><ci id="S4.SS3.p1.12.m6.1.1.3.3a.cmml" xref="S4.SS3.p1.12.m6.1.1.3.3"><mtext id="S4.SS3.p1.12.m6.1.1.3.3.cmml" xref="S4.SS3.p1.12.m6.1.1.3.3"># women questions</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.12.m6.1c">r=\text{\# men questions}/\text{\# women questions}</annotation></semantics></math>). If <math id="S4.SS3.p1.13.m7.2" class="ltx_Math" alttext="\text{BS}(a,q_{m})" display="inline"><semantics id="S4.SS3.p1.13.m7.2a"><mrow id="S4.SS3.p1.13.m7.2.2" xref="S4.SS3.p1.13.m7.2.2.cmml"><mtext id="S4.SS3.p1.13.m7.2.2.3" xref="S4.SS3.p1.13.m7.2.2.3a.cmml">BS</mtext><mo lspace="0em" rspace="0em" id="S4.SS3.p1.13.m7.2.2.2" xref="S4.SS3.p1.13.m7.2.2.2.cmml">​</mo><mrow id="S4.SS3.p1.13.m7.2.2.1.1" xref="S4.SS3.p1.13.m7.2.2.1.2.cmml"><mo stretchy="false" id="S4.SS3.p1.13.m7.2.2.1.1.2" xref="S4.SS3.p1.13.m7.2.2.1.2.cmml">(</mo><mi id="S4.SS3.p1.13.m7.1.1" xref="S4.SS3.p1.13.m7.1.1.cmml">a</mi><mo id="S4.SS3.p1.13.m7.2.2.1.1.3" xref="S4.SS3.p1.13.m7.2.2.1.2.cmml">,</mo><msub id="S4.SS3.p1.13.m7.2.2.1.1.1" xref="S4.SS3.p1.13.m7.2.2.1.1.1.cmml"><mi id="S4.SS3.p1.13.m7.2.2.1.1.1.2" xref="S4.SS3.p1.13.m7.2.2.1.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.13.m7.2.2.1.1.1.3" xref="S4.SS3.p1.13.m7.2.2.1.1.1.3.cmml">m</mi></msub><mo stretchy="false" id="S4.SS3.p1.13.m7.2.2.1.1.4" xref="S4.SS3.p1.13.m7.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.13.m7.2b"><apply id="S4.SS3.p1.13.m7.2.2.cmml" xref="S4.SS3.p1.13.m7.2.2"><times id="S4.SS3.p1.13.m7.2.2.2.cmml" xref="S4.SS3.p1.13.m7.2.2.2"></times><ci id="S4.SS3.p1.13.m7.2.2.3a.cmml" xref="S4.SS3.p1.13.m7.2.2.3"><mtext id="S4.SS3.p1.13.m7.2.2.3.cmml" xref="S4.SS3.p1.13.m7.2.2.3">BS</mtext></ci><interval closure="open" id="S4.SS3.p1.13.m7.2.2.1.2.cmml" xref="S4.SS3.p1.13.m7.2.2.1.1"><ci id="S4.SS3.p1.13.m7.1.1.cmml" xref="S4.SS3.p1.13.m7.1.1">𝑎</ci><apply id="S4.SS3.p1.13.m7.2.2.1.1.1.cmml" xref="S4.SS3.p1.13.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.13.m7.2.2.1.1.1.1.cmml" xref="S4.SS3.p1.13.m7.2.2.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.13.m7.2.2.1.1.1.2.cmml" xref="S4.SS3.p1.13.m7.2.2.1.1.1.2">𝑞</ci><ci id="S4.SS3.p1.13.m7.2.2.1.1.1.3.cmml" xref="S4.SS3.p1.13.m7.2.2.1.1.1.3">𝑚</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.13.m7.2c">\text{BS}(a,q_{m})</annotation></semantics></math> is close to <math id="S4.SS3.p1.14.m8.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS3.p1.14.m8.1a"><mn id="S4.SS3.p1.14.m8.1.1" xref="S4.SS3.p1.14.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.14.m8.1b"><cn type="integer" id="S4.SS3.p1.14.m8.1.1.cmml" xref="S4.SS3.p1.14.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.14.m8.1c">1</annotation></semantics></math>, then the answer <math id="S4.SS3.p1.15.m9.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS3.p1.15.m9.1a"><mi id="S4.SS3.p1.15.m9.1.1" xref="S4.SS3.p1.15.m9.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.15.m9.1b"><ci id="S4.SS3.p1.15.m9.1.1.cmml" xref="S4.SS3.p1.15.m9.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.15.m9.1c">a</annotation></semantics></math> is correlated with men questions. On the contrary, if <math id="S4.SS3.p1.16.m10.2" class="ltx_Math" alttext="\text{BS}(a,q_{m})" display="inline"><semantics id="S4.SS3.p1.16.m10.2a"><mrow id="S4.SS3.p1.16.m10.2.2" xref="S4.SS3.p1.16.m10.2.2.cmml"><mtext id="S4.SS3.p1.16.m10.2.2.3" xref="S4.SS3.p1.16.m10.2.2.3a.cmml">BS</mtext><mo lspace="0em" rspace="0em" id="S4.SS3.p1.16.m10.2.2.2" xref="S4.SS3.p1.16.m10.2.2.2.cmml">​</mo><mrow id="S4.SS3.p1.16.m10.2.2.1.1" xref="S4.SS3.p1.16.m10.2.2.1.2.cmml"><mo stretchy="false" id="S4.SS3.p1.16.m10.2.2.1.1.2" xref="S4.SS3.p1.16.m10.2.2.1.2.cmml">(</mo><mi id="S4.SS3.p1.16.m10.1.1" xref="S4.SS3.p1.16.m10.1.1.cmml">a</mi><mo id="S4.SS3.p1.16.m10.2.2.1.1.3" xref="S4.SS3.p1.16.m10.2.2.1.2.cmml">,</mo><msub id="S4.SS3.p1.16.m10.2.2.1.1.1" xref="S4.SS3.p1.16.m10.2.2.1.1.1.cmml"><mi id="S4.SS3.p1.16.m10.2.2.1.1.1.2" xref="S4.SS3.p1.16.m10.2.2.1.1.1.2.cmml">q</mi><mi id="S4.SS3.p1.16.m10.2.2.1.1.1.3" xref="S4.SS3.p1.16.m10.2.2.1.1.1.3.cmml">m</mi></msub><mo stretchy="false" id="S4.SS3.p1.16.m10.2.2.1.1.4" xref="S4.SS3.p1.16.m10.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.16.m10.2b"><apply id="S4.SS3.p1.16.m10.2.2.cmml" xref="S4.SS3.p1.16.m10.2.2"><times id="S4.SS3.p1.16.m10.2.2.2.cmml" xref="S4.SS3.p1.16.m10.2.2.2"></times><ci id="S4.SS3.p1.16.m10.2.2.3a.cmml" xref="S4.SS3.p1.16.m10.2.2.3"><mtext id="S4.SS3.p1.16.m10.2.2.3.cmml" xref="S4.SS3.p1.16.m10.2.2.3">BS</mtext></ci><interval closure="open" id="S4.SS3.p1.16.m10.2.2.1.2.cmml" xref="S4.SS3.p1.16.m10.2.2.1.1"><ci id="S4.SS3.p1.16.m10.1.1.cmml" xref="S4.SS3.p1.16.m10.1.1">𝑎</ci><apply id="S4.SS3.p1.16.m10.2.2.1.1.1.cmml" xref="S4.SS3.p1.16.m10.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.16.m10.2.2.1.1.1.1.cmml" xref="S4.SS3.p1.16.m10.2.2.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.16.m10.2.2.1.1.1.2.cmml" xref="S4.SS3.p1.16.m10.2.2.1.1.1.2">𝑞</ci><ci id="S4.SS3.p1.16.m10.2.2.1.1.1.3.cmml" xref="S4.SS3.p1.16.m10.2.2.1.1.1.3">𝑚</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.16.m10.2c">\text{BS}(a,q_{m})</annotation></semantics></math> is close to <math id="S4.SS3.p1.17.m11.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS3.p1.17.m11.1a"><mn id="S4.SS3.p1.17.m11.1.1" xref="S4.SS3.p1.17.m11.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.17.m11.1b"><cn type="integer" id="S4.SS3.p1.17.m11.1.1.cmml" xref="S4.SS3.p1.17.m11.1.1">0</cn></annotation-xml></semantics></math>, answer <math id="S4.SS3.p1.18.m12.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS3.p1.18.m12.1a"><mi id="S4.SS3.p1.18.m12.1.1" xref="S4.SS3.p1.18.m12.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.18.m12.1b"><ci id="S4.SS3.p1.18.m12.1.1.cmml" xref="S4.SS3.p1.18.m12.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.18.m12.1c">a</annotation></semantics></math> is correlated with women questions.
The difference between BS and Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2. Answer distributions are skewed toward each gender ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and  <a href="#S4.F4" title="Figure 4 ‣ 4.2. Answer distributions are skewed toward each gender ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is that the distribution of BS shows the answers that often appear only in questions of one gender.
For example, in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2. Answer distributions are skewed toward each gender ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <span id="S4.SS3.p1.19.3" class="ltx_text ltx_font_italic">black</span> is the most common answer for both gender questions, but it is not biased towards any gender, in which case the value of BS is close to <math id="S4.SS3.p1.19.m13.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS3.p1.19.m13.1a"><mn id="S4.SS3.p1.19.m13.1.1" xref="S4.SS3.p1.19.m13.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.19.m13.1b"><cn type="float" id="S4.SS3.p1.19.m13.1.1.cmml" xref="S4.SS3.p1.19.m13.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.19.m13.1c">0.5</annotation></semantics></math>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">The top-<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn type="integer" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">20</annotation></semantics></math> answers that are correlated with each gender based on BS in VQA 2.0 are shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3. Gender-answer correlations reflect gender stereotypes and discrimination ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
In a bias-free dataset, the distribution of BS would be flat, which reveals that
the VQA 2.0 dataset contains strong gender bias in their answers.
Again, many sport words (<span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS3.p2.2.2" class="ltx_text ltx_font_italic">g</span>., <span id="S4.SS3.p2.2.3" class="ltx_text ltx_font_italic">skateboarding</span>, <span id="S4.SS3.p2.2.4" class="ltx_text ltx_font_italic">baseball</span>, <span id="S4.SS3.p2.2.5" class="ltx_text ltx_font_italic">snowboarding</span>) are strongly correlated toward men questions, while no answers about sports are highly related to women questions. On the other hand, food-related words (<span id="S4.SS3.p2.2.6" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS3.p2.2.7" class="ltx_text ltx_font_italic">g</span>., <span id="S4.SS3.p2.2.8" class="ltx_text ltx_font_italic">wine</span>, <span id="S4.SS3.p2.2.9" class="ltx_text ltx_font_italic">knife</span>, <span id="S4.SS3.p2.2.10" class="ltx_text ltx_font_italic">cake</span>) are highly correlated to women questions.
This trend is common in other datasets as well; for example, in GQA (Figure 2 in the appendix), sports or outdoor words (<span id="S4.SS3.p2.2.11" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS3.p2.2.12" class="ltx_text ltx_font_italic">g</span>., <span id="S4.SS3.p2.2.13" class="ltx_text ltx_font_italic">skating</span>, <span id="S4.SS3.p2.2.14" class="ltx_text ltx_font_italic">surfing</span>, <span id="S4.SS3.p2.2.15" class="ltx_text ltx_font_italic">tan</span>) made up most of the top-<math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn type="integer" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">20</annotation></semantics></math> answers for men questions, while there are no sports or activity-related answers in the case of women, and instead, static words (<span id="S4.SS3.p2.2.16" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS3.p2.2.17" class="ltx_text ltx_font_italic">g</span>., <span id="S4.SS3.p2.2.18" class="ltx_text ltx_font_italic">umbrella</span>, <span id="S4.SS3.p2.2.19" class="ltx_text ltx_font_italic">sofa</span>, and <span id="S4.SS3.p2.2.20" class="ltx_text ltx_font_italic">posing</span>) are common.
Similarly to the results in the previous section, these results are a reflection of the real-world stereotypes that leads to gender bias and discrimination.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Topics of the questions are significantly different between women and men</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2205.08148/assets/x9.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="67" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>The degree to which each question category is biased with respect to a men questions. <span id="S4.F6.4.1" class="ltx_text" style="color:#006400;">Green</span> color denotes the category is skewed toward men questions, while <span id="S4.F6.5.2" class="ltx_text" style="color:#FF8C00;">orange</span> color denotes the category is biased toward women questions. The larger the Ratio, the more the question category is biased toward men questions, and vice versa. The <span id="S4.F6.6.3" class="ltx_text" style="color:#808080;">gray</span> dotted line at 0.5 indicates gender balance.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p">Next, we examine gender representation among different question categories. For this analysis, we use the OK-VQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, whose questions are categorized into 10 classes. <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span id="footnote4.1" class="ltx_text ltx_font_italic">Science</span> (Science and Technology), <span id="footnote4.2" class="ltx_text ltx_font_italic">Vehicles</span> (Vehicles and Transportation), <span id="footnote4.3" class="ltx_text ltx_font_italic">Sports</span> (Sports and Recreation), <span id="footnote4.4" class="ltx_text ltx_font_italic">Brands</span> (Brands, Companies and Products), <span id="footnote4.5" class="ltx_text ltx_font_italic">Plants</span> (Plants and Animals), <span id="footnote4.6" class="ltx_text ltx_font_italic">People</span> (People and Everyday life), <span id="footnote4.7" class="ltx_text ltx_font_italic">Objects</span> (Objects, Material and Clothing), <span id="footnote4.8" class="ltx_text ltx_font_italic">Cooking</span> (Cooking and Food), <span id="footnote4.9" class="ltx_text ltx_font_italic">Weather</span> (Weather and Climate), <span id="footnote4.10" class="ltx_text ltx_font_italic">Geography</span> (Geography, History, Language and Culture)</span></span></span> Specifically, we adapt Eq. (<a href="#S4.E1" title="In 4.3. Gender-answer correlations reflect gender stereotypes and discrimination ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) by replacing the answer <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">a</annotation></semantics></math> with question category <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="c_{q}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">c</mi><mi id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">𝑐</ci><ci id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">c_{q}</annotation></semantics></math>. In other words, we calculate the degree to which each question category is biased with respect to men questions.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Results in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4. Topics of the questions are significantly different between women and men ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> show that some categories are strongly correlated with gender. The most correlated category with men is Science, followed by Vehicles, and Sports. On the other hand, Geography questions (which is a category gathering geography, history, language, and culture together) are biased towards women, together with Weather, and Cooking. From these observations, we conclude that men are women are differently represented in the dataset, where men are often associated with science and technology whereas women are tied to liberal arts and cooking <cite class="ltx_cite ltx_citemacro_citep">(Van der Vleuten et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Gender-stereotypical samples</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Aside from the statistics in the preceding sections, we manually explore each dataset for potentially sexist or harmful samples. Specifically, we investigate more than <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mn id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><cn type="integer" id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">300</annotation></semantics></math> samples for each dataset and we find that the datasets in which different annotators generate the questions and the answers separately (<span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">e</span>., VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> and OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>) often contain gender-stereotypical samples. Also, Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>, which is no filtering to ensure questions are visually grounded, contains such samples.
On the contrary, GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, which automatically generates samples, and Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>, which filters out not visually grounded samples, do not contain such samples.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2205.08148/assets/x10.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="334" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Examples of gender stereotypes in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> (<span id="S4.F7.4.1" class="ltx_text ltx_font_bold">above</span>), OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> (<span id="S4.F7.5.2" class="ltx_text ltx_font_bold">middle</span>), and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> (<span id="S4.F7.6.3" class="ltx_text ltx_font_bold">below</span>). </figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Some examples, consisting of an image, a question, and ground truth answers, are shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. Gender-stereotypical samples ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
More examples can be found in the appendix. In the top-row left example,
to the question about the topic of the conversation between the two women, which cannot be inferred using visual clues, some annotators provided gender-stereotypical answers such as <span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_italic">men</span>, <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">hair</span>, and <span id="S4.SS5.p2.1.3" class="ltx_text ltx_font_italic">shopping</span>, probably based on their own prejudices about women. Similarly, in the top-row right example of Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. Gender-stereotypical samples ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the question asks what the man is looking at, but it cannot be seen in the image. In this case, the provided answers contain the words <span id="S4.SS5.p2.1.4" class="ltx_text ltx_font_italic">girl</span> and <span id="S4.SS5.p2.1.5" class="ltx_text ltx_font_italic">woman</span>.
The common denominator in these examples is that their questions are not visually grounded: there is no definite and unambiguous evidence in the image to answer.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Additionally, we find some inappropriate or directly harmful examples. For instance, the top-row middle question in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. Gender-stereotypical samples ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> has a sexual connotation about the woman in the image, and in the second-row left image in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. Gender-stereotypical samples ‣ 4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, where a woman is just standing in front of a mirror with a camera, one of the ground truth answers to the question of what is she doing is <span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_italic">flirt</span>, implying that the mere fact that a woman is standing is to seduce someone.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Discussion</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p"><span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_bold">Biased distributions </span>
We have shown that the ratio of men questions over women questions and the answer distributions are biased in all datasets.
Especially for the ratio of men questions over women questions, we know the real-world ratio of the men to women is roughly <math id="S4.SS6.p1.1.m1.1" class="ltx_Math" alttext="1:1" display="inline"><semantics id="S4.SS6.p1.1.m1.1a"><mrow id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml"><mn id="S4.SS6.p1.1.m1.1.1.2" xref="S4.SS6.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.SS6.p1.1.m1.1.1.1" xref="S4.SS6.p1.1.m1.1.1.1.cmml">:</mo><mn id="S4.SS6.p1.1.m1.1.1.3" xref="S4.SS6.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.1b"><apply id="S4.SS6.p1.1.m1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1"><ci id="S4.SS6.p1.1.m1.1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1.1">:</ci><cn type="integer" id="S4.SS6.p1.1.m1.1.1.2.cmml" xref="S4.SS6.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS6.p1.1.m1.1.1.3.cmml" xref="S4.SS6.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.1c">1:1</annotation></semantics></math>.
Hence, the skew of the ratio of men questions over women questions in VQA datasets (<span id="S4.SS6.p1.1.2" class="ltx_text ltx_font_italic">i</span>.<span id="S4.SS6.p1.1.3" class="ltx_text ltx_font_italic">e</span>., men questions are about twice as numerous as women questions) is far from the real world distribution. That is unfair underrepresentation of women and could lead machine learning models trained on these datasets to potentially ignore women. This means that models perpetuate the underrepresentation of women unveiled in the dataset, which is very problematic from the perspective of gender equality.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">As for the answer distributions, while we have shown that the answer distributions are highly skewed toward each gender, it is difficult to know the real-world distributions (<span id="S4.SS6.p2.1.1" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS6.p2.1.2" class="ltx_text ltx_font_italic">g</span>., the actual gender ratio among those who snowboard). Furthermore, realistically, the gender ratio for the answers cannot be aligned, and even if it could be, the gender bias in models might not disappear <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019b</a>; Buolamwini and Gebru, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>.
However, machine learning models trained on these datasets without considering the biased distributions can learn to ignore underrepresented combinations in the datasets (<span id="S4.SS6.p2.1.3" class="ltx_text ltx_font_italic">e</span>.<span id="S4.SS6.p2.1.4" class="ltx_text ltx_font_italic">g</span>., a woman who snowboards) and lead to shortcut learning <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Kervadec et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>.
Being aware of such bias toward each gender encourages the community to design better model architectures and training paradigms to mitigate the bias, which is a research direction to be further explored, as in <cite class="ltx_cite ltx_citemacro_citep">(Hendricks et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p"><span id="S4.SS6.p3.1.1" class="ltx_text ltx_font_bold">Gender-stereotypical examples </span>
Compared to the skewed answer distributions, gender-discriminatory samples are undoubtedly harmful.
Such samples are often found when the associated question is not visually grounded. For such questions, annotators may answer based on their gender stereotypes.
Such harmful samples are found in datasets in which not visually grounded questions are not filtered (<span id="S4.SS6.p3.1.2" class="ltx_text ltx_font_italic">i</span>.<span id="S4.SS6.p3.1.3" class="ltx_text ltx_font_italic">e</span>., VQA 2.0, OK-VQA, and Visual Genome). Also, in VQA 2.0 and OK-VQA, the annotators who create the questions are different from those who answer them. This choice is to deal with the problem of multiple possible correct answers to the same question <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite>. Although the process allows the datasets to have diverse answers, it does not require the annotators to answer to their own questions and gives room to make questions that are not visually grounded.
On the other hand, in GQA and Visual7W, while not visually grounded questions hardly exist because of the datasets construction, the diversity of answers is limited.
In conclusion, manual removal of potential harmful questions may be necessary to ensure ethical goodness.
Thus, it may be a good practice to build an efficient mechanism to report potential ethical problems and review them on a regular basis to decide whether some samples should be removed.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Racial Bias in VQA</h2>

<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Statistics of racial questions (Racial Qs) in VQA datasets. Ratio is the number of racial questions over the number of questions.</figcaption>
<table id="S5.T3.15" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.15.16.1" class="ltx_tr">
<th id="S5.T3.15.16.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Dataset</th>
<th id="S5.T3.15.16.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Racial Qs</th>
<th id="S5.T3.15.16.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Num. Total Qs</th>
<th id="S5.T3.15.16.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Ratio (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="619" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mn id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">619</mn><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><cn type="integer" id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">619</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">619</annotation></semantics></math></td>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_right ltx_border_t">
<math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mn id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><cn type="float" id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">1.7</annotation></semantics></math>M</td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><math id="S5.T3.3.3.3.m1.1" class="ltx_Math" alttext="0.43" display="inline"><semantics id="S5.T3.3.3.3.m1.1a"><mn id="S5.T3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.m1.1.1.cmml">0.43</mn><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><cn type="float" id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1">0.43</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">0.43</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.6.6" class="ltx_tr">
<th id="S5.T3.6.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S5.T3.4.4.1" class="ltx_td ltx_align_right"><math id="S5.T3.4.4.1.m1.1" class="ltx_Math" alttext="74" display="inline"><semantics id="S5.T3.4.4.1.m1.1a"><mn id="S5.T3.4.4.1.m1.1.1" xref="S5.T3.4.4.1.m1.1.1.cmml">74</mn><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.1.m1.1b"><cn type="integer" id="S5.T3.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.1.m1.1.1">74</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.1.m1.1c">74</annotation></semantics></math></td>
<td id="S5.T3.5.5.2" class="ltx_td ltx_align_right">
<math id="S5.T3.5.5.2.m1.1" class="ltx_Math" alttext="327" display="inline"><semantics id="S5.T3.5.5.2.m1.1a"><mn id="S5.T3.5.5.2.m1.1.1" xref="S5.T3.5.5.2.m1.1.1.cmml">327</mn><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.2.m1.1b"><cn type="integer" id="S5.T3.5.5.2.m1.1.1.cmml" xref="S5.T3.5.5.2.m1.1.1">327</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.2.m1.1c">327</annotation></semantics></math>K</td>
<td id="S5.T3.6.6.3" class="ltx_td ltx_align_right"><math id="S5.T3.6.6.3.m1.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="S5.T3.6.6.3.m1.1a"><mn id="S5.T3.6.6.3.m1.1.1" xref="S5.T3.6.6.3.m1.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.3.m1.1b"><cn type="float" id="S5.T3.6.6.3.m1.1.1.cmml" xref="S5.T3.6.6.3.m1.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.3.m1.1c">0.05</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.9.9" class="ltx_tr">
<th id="S5.T3.9.9.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S5.T3.7.7.1" class="ltx_td ltx_align_right"><math id="S5.T3.7.7.1.m1.1" class="ltx_Math" alttext="799" display="inline"><semantics id="S5.T3.7.7.1.m1.1a"><mn id="S5.T3.7.7.1.m1.1.1" xref="S5.T3.7.7.1.m1.1.1.cmml">799</mn><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.1.m1.1b"><cn type="integer" id="S5.T3.7.7.1.m1.1.1.cmml" xref="S5.T3.7.7.1.m1.1.1">799</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.1.m1.1c">799</annotation></semantics></math></td>
<td id="S5.T3.8.8.2" class="ltx_td ltx_align_right">
<math id="S5.T3.8.8.2.m1.1" class="ltx_Math" alttext="658" display="inline"><semantics id="S5.T3.8.8.2.m1.1a"><mn id="S5.T3.8.8.2.m1.1.1" xref="S5.T3.8.8.2.m1.1.1.cmml">658</mn><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.2.m1.1b"><cn type="integer" id="S5.T3.8.8.2.m1.1.1.cmml" xref="S5.T3.8.8.2.m1.1.1">658</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.2.m1.1c">658</annotation></semantics></math>K</td>
<td id="S5.T3.9.9.3" class="ltx_td ltx_align_right"><math id="S5.T3.9.9.3.m1.1" class="ltx_Math" alttext="0.12" display="inline"><semantics id="S5.T3.9.9.3.m1.1a"><mn id="S5.T3.9.9.3.m1.1.1" xref="S5.T3.9.9.3.m1.1.1.cmml">0.12</mn><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.3.m1.1b"><cn type="float" id="S5.T3.9.9.3.m1.1.1.cmml" xref="S5.T3.9.9.3.m1.1.1">0.12</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.3.m1.1c">0.12</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.12.12" class="ltx_tr">
<th id="S5.T3.12.12.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S5.T3.10.10.1" class="ltx_td ltx_align_right"><math id="S5.T3.10.10.1.m1.1" class="ltx_Math" alttext="85" display="inline"><semantics id="S5.T3.10.10.1.m1.1a"><mn id="S5.T3.10.10.1.m1.1.1" xref="S5.T3.10.10.1.m1.1.1.cmml">85</mn><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.1.m1.1b"><cn type="integer" id="S5.T3.10.10.1.m1.1.1.cmml" xref="S5.T3.10.10.1.m1.1.1">85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.1.m1.1c">85</annotation></semantics></math></td>
<td id="S5.T3.11.11.2" class="ltx_td ltx_align_right">
<math id="S5.T3.11.11.2.m1.1" class="ltx_Math" alttext="4.3" display="inline"><semantics id="S5.T3.11.11.2.m1.1a"><mn id="S5.T3.11.11.2.m1.1.1" xref="S5.T3.11.11.2.m1.1.1.cmml">4.3</mn><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.2.m1.1b"><cn type="float" id="S5.T3.11.11.2.m1.1.1.cmml" xref="S5.T3.11.11.2.m1.1.1">4.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.2.m1.1c">4.3</annotation></semantics></math>M</td>
<td id="S5.T3.12.12.3" class="ltx_td ltx_align_right"><math id="S5.T3.12.12.3.m1.1" class="ltx_Math" alttext="0.00" display="inline"><semantics id="S5.T3.12.12.3.m1.1a"><mn id="S5.T3.12.12.3.m1.1.1" xref="S5.T3.12.12.3.m1.1.1.cmml">0.00</mn><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.3.m1.1b"><cn type="float" id="S5.T3.12.12.3.m1.1.1.cmml" xref="S5.T3.12.12.3.m1.1.1">0.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.3.m1.1c">0.00</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.15.15" class="ltx_tr">
<th id="S5.T3.15.15.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S5.T3.13.13.1" class="ltx_td ltx_align_right ltx_border_bb"><math id="S5.T3.13.13.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.T3.13.13.1.m1.1a"><mn id="S5.T3.13.13.1.m1.1.1" xref="S5.T3.13.13.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.T3.13.13.1.m1.1b"><cn type="integer" id="S5.T3.13.13.1.m1.1.1.cmml" xref="S5.T3.13.13.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.13.13.1.m1.1c">30</annotation></semantics></math></td>
<td id="S5.T3.14.14.2" class="ltx_td ltx_align_right ltx_border_bb">
<math id="S5.T3.14.14.2.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.T3.14.14.2.m1.1a"><mn id="S5.T3.14.14.2.m1.1.1" xref="S5.T3.14.14.2.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.T3.14.14.2.m1.1b"><cn type="integer" id="S5.T3.14.14.2.m1.1.1.cmml" xref="S5.T3.14.14.2.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.14.14.2.m1.1c">14</annotation></semantics></math>K</td>
<td id="S5.T3.15.15.3" class="ltx_td ltx_align_right ltx_border_bb"><math id="S5.T3.15.15.3.m1.1" class="ltx_Math" alttext="0.21" display="inline"><semantics id="S5.T3.15.15.3.m1.1a"><mn id="S5.T3.15.15.3.m1.1.1" xref="S5.T3.15.15.3.m1.1.1.cmml">0.21</mn><annotation-xml encoding="MathML-Content" id="S5.T3.15.15.3.m1.1b"><cn type="float" id="S5.T3.15.15.3.m1.1.1.cmml" xref="S5.T3.15.15.3.m1.1.1">0.21</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.15.15.3.m1.1c">0.21</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To study racial bias, we first identify samples in the dataset with a reference to race or ethnicity. We select all the samples whose questions explicitly contain the words <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">race</span> or <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">ethnicity</span>. We refer to these samples as <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">racial samples</span> or <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">racial questions</span>. Additionally, we elaborate a list of <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">racial-related words</span> (<span id="S5.p1.1.6" class="ltx_text ltx_font_italic">e</span>.<span id="S5.p1.1.7" class="ltx_text ltx_font_italic">g</span>., <span id="S5.p1.1.8" class="ltx_text ltx_font_italic">Asian</span>, <span id="S5.p1.1.9" class="ltx_text ltx_font_italic">Caucasian</span>, <span id="S5.p1.1.10" class="ltx_text ltx_font_italic">Black</span>) and <span id="S5.p1.1.11" class="ltx_text ltx_font_italic">nationality-related words</span> (<span id="S5.p1.1.12" class="ltx_text ltx_font_italic">e</span>.<span id="S5.p1.1.13" class="ltx_text ltx_font_italic">g</span>., <span id="S5.p1.1.14" class="ltx_text ltx_font_italic">American</span>, <span id="S5.p1.1.15" class="ltx_text ltx_font_italic">Chinese</span>, <span id="S5.p1.1.16" class="ltx_text ltx_font_italic">Indian</span>) from the answers in VQA 2.0.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The complete list of words can be found in the appendix.</span></span></span> Our findings are reported below.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Most datasets contain racial words</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The number of racial samples per dataset is shown in Table <a href="#S5.T3" title="Table 3 ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. All datasets contain racial questions. However, the ratio of racial questions is very small for GQA, whose questions and answers were created automatically from the image’s scene graph, and for Visual7W, in which annotators had explicit instructions to write visually grounded questions, and inappropriate samples were filtered. In contrast, VQA 2.0, Visual Genome, and OK-VQA show higher ratios of racial questions. As the absolute number in OK-VQA is small, we conduct our analysis in the VQA 2.0 and Visual Genome.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2205.08148/assets/x11.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="77" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2205.08148/assets/x12.png" id="S5.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="77" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Top-<math id="S5.F8.2.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.F8.2.m1.1b"><mn id="S5.F8.2.m1.1.1" xref="S5.F8.2.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.F8.2.m1.1c"><cn type="integer" id="S5.F8.2.m1.1.1.cmml" xref="S5.F8.2.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F8.2.m1.1d">10</annotation></semantics></math> answers to racial questions in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> (<span id="S5.F8.5.1" class="ltx_text ltx_font_bold">left</span>) and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> (<span id="S5.F8.6.2" class="ltx_text ltx_font_bold">right</span>).</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>White people are majority, and Black people are minority</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p">First, we investigate the distribution of answers for racial questions. To remove answers that may not be related to race, ethnicity, or nationality, we filter <span id="S5.SS2.p1.2.1" class="ltx_text ltx_font_italic">yes</span>, <span id="S5.SS2.p1.2.2" class="ltx_text ltx_font_italic">no</span>, <span id="S5.SS2.p1.2.3" class="ltx_text ltx_font_italic">horse<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_upright">6</span></span><span id="footnote6.5" class="ltx_text ltx_font_upright">When </span><span id="footnote6.6" class="ltx_text">horse</span><span id="footnote6.7" class="ltx_text ltx_font_upright"> is in the answers, the questions are more likely to ask about horse race rather than race of individuals.</span></span></span></span></span> and answers to questions about colors<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>This is to avoid confusing answers about color with race.</span></span></span>. In Figure <a href="#S5.F8" title="Figure 8 ‣ 5.1. Most datasets contain racial words ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we show the top-<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mn id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><cn type="integer" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">10</annotation></semantics></math> answers to racial questions in VQA 2.0 and Visual Genome.
In both datasets, the demographic group that occupies the largest amount of answers is related to White people (<span id="S5.SS2.p1.2.4" class="ltx_text ltx_font_italic">White</span>, <span id="S5.SS2.p1.2.5" class="ltx_text ltx_font_italic">Caucasian</span>), followed by Asian people related words (<span id="S5.SS2.p1.2.6" class="ltx_text ltx_font_italic">Asian</span>, <span id="S5.SS2.p1.2.7" class="ltx_text ltx_font_italic">Chinese</span>). On the contrary, words usually associated with Black people (<span id="S5.SS2.p1.2.8" class="ltx_text ltx_font_italic">Black</span>, <span id="S5.SS2.p1.2.9" class="ltx_text ltx_font_italic">African American</span>, <span id="S5.SS2.p1.2.10" class="ltx_text ltx_font_italic">African</span>) and Hispanic people (<span id="S5.SS2.p1.2.11" class="ltx_text ltx_font_italic">Hispanic</span>) appear less frequently, showing an underrepresentation of darker-skinned people on the analyzed samples. This tendency has also been observed in other computer vision datasets, such as facial recognition (e.g., <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="79.6" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mn id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">79.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><cn type="float" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">79.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">79.6</annotation></semantics></math>% subjects are lighter-skinned in IJB-A <cite class="ltx_cite ltx_citemacro_citep">(Klare et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2015</a>; Buolamwini and Gebru, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>), or in image captioning <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite>. This imbalance can lead to poor performance on images of darker-skinned people in models trained on such datasets.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>US-centric perspective of nationality and race</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We next explore the relationship between race and nationality in the VQA 2.0 dataset. We examine the co-occurrence of nationality-related and racial-related words in the answers. We roughly categorize them into three races or ethnicities: African-oriented (with the words <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">Black</span> and <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_italic">African</span>), Asian-oriented (with the words <span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_italic">Asian</span> and <span id="S5.SS3.p1.1.4" class="ltx_text ltx_font_italic">Oriental</span>), and White-oriented (with the words <span id="S5.SS3.p1.1.5" class="ltx_text ltx_font_italic">White</span> and <span id="S5.SS3.p1.1.6" class="ltx_text ltx_font_italic">Caucasian</span>).<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>More categories are not include due to the limited number of the samples.</span></span></span></p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2205.08148/assets/x13.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="401" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Top-<math id="S5.F9.2.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S5.F9.2.m1.1b"><mn id="S5.F9.2.m1.1.1" xref="S5.F9.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.F9.2.m1.1c"><cn type="integer" id="S5.F9.2.m1.1.1.cmml" xref="S5.F9.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.2.m1.1d">5</annotation></semantics></math> answers related to nationality that co-occur with African-oriented (Black, African) (<span id="S5.F9.6.1" class="ltx_text ltx_font_bold">left</span>), Asian-oriented (Asian, Oriental) (<span id="S5.F9.7.2" class="ltx_text ltx_font_bold">middle</span>), and White-oriented (White, Caucasian) (<span id="S5.F9.8.3" class="ltx_text ltx_font_bold">right</span>) in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.3" class="ltx_p">Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3. US-centric perspective of nationality and race ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the top-<math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mn id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><cn type="integer" id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">10</annotation></semantics></math> nationalities co-occurring with each racial category.
Each racial category is strongly tied to a specific country.
The result in Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3. US-centric perspective of nationality and race ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (left) shows that about <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="65" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mn id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">65</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><cn type="integer" id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">65</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">65</annotation></semantics></math>% of Black people are considered to be American (<span id="S5.SS3.p2.3.1" class="ltx_text ltx_font_italic">i</span>.<span id="S5.SS3.p2.3.2" class="ltx_text ltx_font_italic">e</span>., <span id="S5.SS3.p2.3.3" class="ltx_text ltx_font_italic">African American</span> or <span id="S5.SS3.p2.3.4" class="ltx_text ltx_font_italic">American</span>).
As for Asian-oriented category, most of the nationality answers are Chinese with a 42% ratio, followed by Japanese and Indian with a 13% and 12% ratio, respectively (Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3. US-centric perspective of nationality and race ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (middle)).
Regarding the White-oriented category, American is the most frequent nationality answer with a <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="33" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mn id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><cn type="integer" id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">33</annotation></semantics></math>% ratio (Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3. US-centric perspective of nationality and race ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (right)). As the concept of race is highly tied to the social and cultural background of each individual <cite class="ltx_cite ltx_citemacro_citep">(Hanna et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, it is remarkable to note that the relationship between race and nationality in the analyzed datasets seems to be rooted in a United States point of view where White and Black people are associated with American nationality, and Asian people with Chinese nationality. This is probably the result of a US-centric annotation process.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Racial-stereotypical examples</h3>

<figure id="S5.F10" class="ltx_figure"><img src="/html/2205.08148/assets/x14.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="331" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Racial samples in OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> (<span id="S5.F10.4.1" class="ltx_text ltx_font_bold">above</span>), VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> (<span id="S5.F10.5.2" class="ltx_text ltx_font_bold">middle</span>), and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> (<span id="S5.F10.6.3" class="ltx_text ltx_font_bold">below</span>).</figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We manually inspect all the racial samples for all the datasets to check whether they can be potentially harmful. In addition to this, we conduct an intersectional analysis and explore more than <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mn id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><cn type="integer" id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">300</annotation></semantics></math> samples of women and men questions for each dataset in terms of racial bias. We find that there are two types of samples with racial bias: 1) racial discriminatory samples, and 2) biased judgment samples. Such examples appear in the VQA 2.0, Visual Genome, and OK-VQA datasets, and some of them are shown in Figure <a href="#S5.F10" title="Figure 10 ‣ 5.4. Racial-stereotypical examples ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. More can be found in the appendix.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.2" class="ltx_p">Some samples that fall into the <span id="S5.SS4.p2.2.1" class="ltx_text ltx_font_italic">racial discriminatory</span> category are shown in Figure <a href="#S5.F10" title="Figure 10 ‣ 5.4. Racial-stereotypical examples ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. For example, in the top-row left example, the question <span id="S5.SS4.p2.2.2" class="ltx_text ltx_font_italic">What causes the skiing pigment of the girl in the blue shirt to be so dark?</span> implies that lighter-skin is the standard. Another example is shown in Figure <a href="#S5.F10" title="Figure 10 ‣ 5.4. Racial-stereotypical examples ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> second-row right image, whose question asks about the woman’s nationality. One of the answers, <span id="S5.SS4.p2.2.3" class="ltx_text ltx_font_italic">Oriental</span>, is an outdated term that has not been used in US federal laws since <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="2016" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mn id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">2016</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><cn type="integer" id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">2016</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">2016</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Park, <a href="#bib.bib43" title="" class="ltx_ref">2016</a>)</cite>.
Still, it appears <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mn id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><cn type="integer" id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">23</annotation></semantics></math> times as an answer in VQA 2.0.
With respect to <span id="S5.SS4.p2.2.4" class="ltx_text ltx_font_italic">biased judgment</span> samples, they often appear in questions that are not visually grounded. In other words, when there is no clue in the image to identify the race, ethnicity, or nationality. For example, in Figure <a href="#S5.F10" title="Figure 10 ‣ 5.4. Racial-stereotypical examples ‣ 5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> bottom-row left image, the answer is <span id="S5.SS4.p2.2.5" class="ltx_text ltx_font_italic">White</span> even though we can only see the tips of the fingers. A similar case can be seen in the bottom-row right example.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Discussion</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_bold">Biased distributions</span>
In VQA 2.0 and Visual Genome, the number of samples related to White people is much greater than samples related to Black and Hispanic people (<span id="S5.SS5.p1.1.2" class="ltx_text ltx_font_italic">e</span>.<span id="S5.SS5.p1.1.3" class="ltx_text ltx_font_italic">g</span>., there are <math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="3.45" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mn id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">3.45</mn><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><cn type="float" id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">3.45</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">3.45</annotation></semantics></math>x more samples related to White people than Black people in VQA 2.0).
The skewed distribution of race can be problematic if models trained on those datasets are used in real world applications, as the underrepresentation of certain races or ethnicities may lead to biased answers.
Although it seems ideal to have a uniform racial distribution, race itself is a vague concept, which is strongly tied to the personal background of each individual <cite class="ltx_cite ltx_citemacro_citep">(Hanna et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>. Furthermore, aligning racial distributions alone is insufficient to remove racial bias from the models <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2019b</a>)</cite>. For these reasons, it is essential to make an effort to have racial diversity in the datasets, but at the same time, it is vital to devise learning strategies that can debias the datasets.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p"><span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_bold">Biased samples</span>
Even though the number of racial questions is relatively small, the stereotypical samples in terms of race are unquestionably harmful and should be removed from the datasets.
Datasets that have not gone through manual screening to remove samples that are not visually grounded are the most affected ones (VQA 2.0, Visual Genome, and OK-VQA). Also, the problem is accentuated in VQA 2.0 and OK-VQA, which increased the diversity in their answer set by making different annotators to answer the written questions.
As samples with racial discrimination or biased judgment of race/nationality may reflect the bias in the annotators, an unconstrained or less-constrained annotation process may be prone to contain such harmful samples. This is supported by the fact that, we could not find such samples in GQA, which automatically generates question-answer pairs, or Visual7W, which applies manual filtering to exclude questions that are not based visually grounded.
Because of this, an additional cleansing process on the samples may be a necessary strategy to remove samples with racial discrimination and biased judgment.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p"><span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_bold">Necessity versus validity of asking questions about race</span>
We take a step back and cast doubt on asking about race in the first place.
Although race has been used to categorize people for a long time, it is extremely hard to provide fixed categories in which people from different backgrounds fit together <cite class="ltx_cite ltx_citemacro_citep">(Khan and Fu, <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>.
Moreover, visual information alone is hardly sufficient to identify one’s race, ethnicity, or nationality, so asking this type of question is prone to cause biased answers based on stereotypes.
Thus, we believe that, at least in VQA datasets, questions about race should be discouraged. </p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Possible Solutions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In order to reduce the risks associated with gender and racial bias, we would like to encourage VQA researchers to increase their awareness to this problem and take steps to address it.
We specifically discuss possible solutions to address the two major problems presented in this paper: skewed distributions and harmful samples.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Skewed distributions</span>
We have shown that the analyzed VQA datasets have distributional bias related to gender (Section <a href="#S4" title="4. Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and, some of them, to race (Section <a href="#S5" title="5. Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
Aligning the distributions is tricky for many reasons, such as the existence of bias in the real-world and the ambiguity of race categorization.
Furthermore, even if the distributions could be aligned, this is not sufficient for bias-free models; models can still amplify bias.
Nevertheless, if the distributions are too skewed in gender and race, models are more likely to ignore underrepresented groups of people and increase the risk of shortcut learning.
Therefore, efforts should be made to avoid this underrepresentation.
More importantly, we encourage the users of the datasets to be aware of the distributional biases related to gender and race in VQA datasets and design models and training paradigms that can address these issues.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Harmful samples</span>
We have found that some of the VQA datasets contain harmful samples that exhibit gender or racial stereotypes.
Such samples are often found when the associated question is unanswerable from the image content. Thus, datasets that have no filtering processes, such as VQA 2.0, Visual Genome, OK-VQA, are prone to contain such samples.
Also, questions and answers themselves can be discriminatory.
The ideal solution is to remove harmful samples by conducting a manual filtering, but the cost for such a process can be extremely expensive, especially if the size of dataset is large.
So, we propose three alternative solutions to address both the ethical and the cost problems: 1) <span id="S6.p3.1.2" class="ltx_text ltx_font_italic">automatic screening</span>, 2) <span id="S6.p3.1.3" class="ltx_text ltx_font_italic">ethical instructions</span>, and 3) <span id="S6.p3.1.4" class="ltx_text ltx_font_italic">a feedback platform for users</span>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">For the automatic screening, we propose to train a model to identify unanswerable questions from images. To train such a model, a labeled dataset to identify whether a question is answerable from an image might be necessary. With a trained model, visually not grounded samples could be potentially filtered out. Although the model’s performance is not guaranteed, it could be used to ease the manual screening process as a pre-filtering step.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">The second proposed solution is to incorporate ethical instruction in the dataset’s annotation process. Ethical instructions are not commonly provided to annotators when creating VQA datasets. Nevertheless, as we have shown, VQA datasets can contain harmful samples; so instruction for annotators to be aware of making ethical questions and answers could reduce the amount of harmful samples.
In this paper, we only focused on gender and race as demographic attributes, but ethical instructions should be extended to make datasets fairer with respect to any other attributes.
</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">The last solution is to create a platform to report potential ethical problems and review them to decide whether they should be removed.
The platform should allow dataset users to report and share with dataset developers when they find harmful samples in their use or investigation of datasets.
This platform would be based on the idea of shifting from the traditional developer-driven paradigm of dataset creation to a user-participatory paradigm.
Incorporating a process that allows users to improve datasets can solve both cost and ethics issues at a high level.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We investigated gender and racial bias in VQA datasets through the compilation of statistics and the manual exploration of harmful samples.
The results showed: 1) distributions are very skewed concerning gender or race, and 2) harmful samples, denoting annotators’ gender or racial stereotypes, exist in VQA datasets.
Additionally, we discussed potential solutions.
We proposed the automatic screening of samples, the inclusion of ethical instructions in the annotations process, and the creation of a platform for receiving user’s feedback.
Through the analysis and discussion in this paper, we hope to raise awareness and encourage the VQA research community to take measures to mitigate societal bias.</p>
</div>
<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Funding</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">This research was partially supported by JST CREST Grant No. JPMJCR20D3 (JST) and JSPS KAKENHI No. JP22K12091 (JSPS). The JST and JSPS had no role in the design and conduct of the study; access and collection of data; analysis and interpretation of data; preparation, review, or approval of the manuscript; or the decision to submit the manuscript for publication. The authors declare no other financial interests.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achlioptas et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Panos Achlioptas, Zhenyu
Chen, Mohamed Elhoseiny, Angel X Chang,
Matthias Niessner, and Leonidas
Guibas. 2021.

</span>
<span class="ltx_bibblock">1st Workshop on Language for 3D Scenes.

</span>
<span class="ltx_bibblock">Workshop at CVPR 2021.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://language3dscenes.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://language3dscenes.github.io/</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv
Batra, Devi Parikh, and Aniruddha
Kembhavi. 2018.

</span>
<span class="ltx_bibblock">Don’t Just Assume; Look and Answer: Overcoming
Priors for Visual Question Answering. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE Computer Society,
4971–4980.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong
He, Chris Buehler, Damien Teney,
Mark Johnson, Stephen Gould, and
Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-Up and Top-Down Attention for Image
Captioning and Visual Question Answering. In
<em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. Computer Vision
Foundation / IEEE Computer Society, 6077–6086.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya
Agrawal, Jiasen Lu, Margaret Mitchell,
Dhruv Batra, C. Lawrence Zitnick, and
Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering. In
<em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">ICCV</em>. IEEE Computer
Society, 2425–2433.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben-younes et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hedi Ben-younes,
Rémi Cadène, Matthieu Cord,
and Nicolas Thome. 2017.

</span>
<span class="ltx_bibblock">MUTAN: Multimodal Tucker Fusion for Visual
Question Answering. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.
IEEE Computer Society, 2631–2639.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Tolga Bolukbasi, Kai-Wei
Chang, James Y. Zou, Venkatesh
Saligrama, and Adam Tauman Kalai.
2016.

</span>
<span class="ltx_bibblock">Man is to Computer Programmer as Woman is to
Homemaker? Debiasing Word Embeddings.

</span>
<span class="ltx_bibblock">(2016), 4349–4357.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Emanuele Bugliarello,
Kai-Wei Chang, Desmond Elliott,
Spandana Gella, Aishwarya Kamath,
Liunian Harold Li, Fangyu Liu,
Jonas Pfeiffer, Edoardo M. Ponti,
Krishna Srinivasan, Ivan Vulić,
Yinfei Yang, and Da Yin.
2021.

</span>
<span class="ltx_bibblock">Workshop on Multilingual Multimodal Learning.

</span>
<span class="ltx_bibblock">Workshop at ACL 2021.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://mml-workshop.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mml-workshop.github.io/</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and Gebru (2018)</span>
<span class="ltx_bibblock">
Joy Buolamwini and
Timnit Gebru. 2018.

</span>
<span class="ltx_bibblock">Gender Shades: Intersectional Accuracy Disparities
in Commercial Gender Classification. In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">FAT</em>
<em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">(Proceedings of Machine Learning Research,
Vol. 81)</em>. PMLR,
77–91.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadène et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Rémi Cadène,
Hedi Ben-younes, Matthieu Cord, and
Nicolas Thome. 2019a.

</span>
<span class="ltx_bibblock">MUREL: Multimodal Relational Reasoning for Visual
Question Answering. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
1989–1998.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadène et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Rémi Cadène,
Corentin Dancette, Hedi Ben-younes,
Matthieu Cord, and Devi Parikh.
2019b.

</span>
<span class="ltx_bibblock">RUBi: Reducing Unimodal Biases for Visual Question
Answering.

</span>
<span class="ltx_bibblock">(2019), 839–850.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Long Chen, Xin Yan,
Jun Xiao, Hanwang Zhang,
Shiliang Pu, and Yueting Zhuang.
2020b.

</span>
<span class="ltx_bibblock">Counterfactual Samples Synthesizing for Robust
Visual Question Answering. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
10797–10806.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang,
Tsung-Yi Lin, Ramakrishna Vedantam,
Saurabh Gupta, Piotr Dollár, and
C. Lawrence Zitnick. 2015.

</span>
<span class="ltx_bibblock">Microsoft COCO Captions: Data Collection and
Evaluation Server.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1504.00325
(2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie
Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu.
2020a.

</span>
<span class="ltx_bibblock">UNITER: UNiversal Image-TExt Representation
Learning. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">ECCV (30)</em>
<em id="bib.bib14.4.2" class="ltx_emph ltx_font_italic">(Lecture Notes in Computer Science,
Vol. 12375)</em>. Springer,
104–120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Mark
Yatskar, and Luke Zettlemoyer.
2019.

</span>
<span class="ltx_bibblock">Don’t Take the Easy Way Out: Ensemble Based Methods
for Avoiding Known Dataset Biases.

</span>
<span class="ltx_bibblock">(2019), 4067–4080.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elhoseiny et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Mohamed Elhoseiny,
Xin Eric Wang, Andrew Brown,
Anna Rohrbach, and Marcus Rohrbach.
2021.

</span>
<span class="ltx_bibblock">4th Workshop on Closing the Loop Between Vision and
Language.

</span>
<span class="ltx_bibblock">Workshop at ICCV 2021.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sites.google.com/view/iccv21clvl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sites.google.com/view/iccv21clvl</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Noa Garcia, Mayu Otani,
Chenhui Chu, and Yuta Nakashima.
2020.

</span>
<span class="ltx_bibblock">KnowIT VQA: Answering Knowledge-Based Questions
about Videos. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">AAAI</em>.
AAAI Press, 10826–10834.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geirhos et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Robert Geirhos,
Jörn-Henrik Jacobsen, Claudio
Michaelis, Richard S. Zemel, Wieland
Brendel, Matthias Bethge, and Felix A.
Wichmann. 2020.

</span>
<span class="ltx_bibblock">Shortcut learning in deep neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Nat. Mach. Intell.</em> 2,
11 (2020), 665–673.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot,
Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Making the V in VQA Matter: Elevating the Role
of Image Understanding in Visual Question Answering. In
<em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. IEEE Computer
Society, 6325–6334.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li,
Abigale J. Stangl, Anhong Guo,
Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P. Bigham.
2018.

</span>
<span class="ltx_bibblock">VizWiz Grand Challenge: Answering Visual Questions
From Blind People. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE Computer Society,
3608–3617.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanna et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alex Hanna, Emily Denton,
Andrew Smart, and Jamila Smith-Loud.
2020.

</span>
<span class="ltx_bibblock">Towards a critical race methodology in algorithmic
fairness. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">FAT*</em>. ACM,
501–512.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks,
Kaylee Burns, Kate Saenko,
Trevor Darrell, and Anna Rohrbach.
2018.

</span>
<span class="ltx_bibblock">Women Also Snowboard: Overcoming Bias in Captioning
Models. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">ECCV (3)</em>
<em id="bib.bib22.4.2" class="ltx_emph ltx_font_italic">(Lecture Notes in Computer Science,
Vol. 11207)</em>. Springer,
793–811.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirota et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yusuke Hirota, Noa
Garcia, Mayu Otani, Chenhui Chu,
Yuta Nakashima, Ittetsu Taniguchi, and
Takao Onoye. 2021.

</span>
<span class="ltx_bibblock">Visual Question Answering with Textual
Representations for Images. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">ICCVW</em>.
IEEE, 3147–3150.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirota et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yusuke Hirota, Yuta
Nakashima, and Noa Garcia.
2022.

</span>
<span class="ltx_bibblock">Quantifying Societal Bias Amplification in Image
Captioning. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhicheng Huang, Zhaoyang
Zeng, Yupan Huang, Bei Liu,
Dongmei Fu, and Jianlong Fu.
2021.

</span>
<span class="ltx_bibblock">Seeing Out of the Box: End-to-End Pre-Training for
Vision-Language Representation Learning. In
<em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. Computer Vision
Foundation / IEEE, 12976–12985.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and
Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock">GQA: A New Dataset for Real-World Visual
Reasoning and Compositional Question Answering. In
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">CVPR</em>. Computer Vision
Foundation / IEEE, 6700–6709.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Shengyu Jia, Tao Meng,
Jieyu Zhao, and Kai-Wei Chang.
2020.

</span>
<span class="ltx_bibblock">Mitigating Gender Bias Amplification in
Distribution by Posterior Regularization.

</span>
<span class="ltx_bibblock">(2020), 2936–2942.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Huaizu Jiang, Ishan
Misra, Marcus Rohrbach, Erik G.
Learned-Miller, and Xinlei Chen.
2020.

</span>
<span class="ltx_bibblock">In Defense of Grid Features for Visual Question
Answering. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
10264–10273.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kervadec et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Corentin Kervadec, Grigory
Antipov, Moez Baccouche, and Christian
Wolf. 2021.

</span>
<span class="ltx_bibblock">Roses Are Red, Violets Are Blue… but Should VQA
Expect Them To?. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
2776–2785.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan and Fu (2021)</span>
<span class="ltx_bibblock">
Zaid Khan and Yun Fu.
2021.

</span>
<span class="ltx_bibblock">One Label, One Billion Faces: Usage and Consistency
of Racial Categories in Computer Vision. In
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">FAccT</em>. ACM,
587–597.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun
Jun, and Byoung-Tak Zhang.
2018.

</span>
<span class="ltx_bibblock">Bilinear Attention Networks.

</span>
<span class="ltx_bibblock">(2018), 1571–1581.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klare et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Brendan F. Klare, Ben
Klein, Emma Taborsky, Austin Blanton,
Jordan Cheney, Kristen Allen,
Patrick Grother, Alan Mah,
Mark James Burge, and Anil K. Jain.
2015.

</span>
<span class="ltx_bibblock">Pushing the frontiers of unconstrained face
detection and recognition: IARPA Janus Benchmark A. In
<em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. IEEE Computer
Society, 1931–1939.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu,
Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz,
Stephanie Chen, Yannis Kalantidis,
Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li
Fei-Fei. 2017.

</span>
<span class="ltx_bibblock">Visual Genome: Connecting Language and Vision Using
Crowdsourced Dense Image Annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vis.</em> 123,
1 (2017), 32–73.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark
Yatskar, Da Yin, Cho-Jui Hsieh, and
Kai-Wei Chang. 2019.

</span>
<span class="ltx_bibblock">VisualBert: A Simple and Performant Baseline for
Vision and Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03557</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin,
Chunyuan Li, Pengchuan Zhang,
Xiaowei Hu, Lei Zhang,
Lijuan Wang, Houdong Hu,
Li Dong, Furu Wei, et al<span id="bib.bib35.3.1" class="ltx_text">.</span>
2020.

</span>
<span class="ltx_bibblock">OSCAR: Object-Semantics Aligned Pre-training
for Vision-Language Tasks. In <em id="bib.bib35.4.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael
Maire, Serge J. Belongie, James Hays,
Pietro Perona, Deva Ramanan,
Piotr Dollár, and C. Lawrence
Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context. In
<em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">ECCV (5)</em> <em id="bib.bib36.4.2" class="ltx_emph ltx_font_italic">(Lecture
Notes in Computer Science, Vol. 8693)</em>.
Springer, 740–755.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra,
Devi Parikh, and Stefan Lee.
2019.

</span>
<span class="ltx_bibblock">ViLBERT: Pretraining Task-Agnostic Visiolinguistic
Representations for Vision-and-Language Tasks.

</span>
<span class="ltx_bibblock">(2019), 13–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and
Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A Multi-World Approach to Question Answering about
Real-World Scenes based on Uncertain Input.

</span>
<span class="ltx_bibblock">(2014), 1682–1690.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manjunatha et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Varun Manjunatha, Nirat
Saini, and Larry S. Davis.
2019.

</span>
<span class="ltx_bibblock">Explicit Bias Discovery in Visual Question
Answering Models. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
9562–9571.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad
Rastegari, Ali Farhadi, and Roozbeh
Mottaghi. 2019.

</span>
<span class="ltx_bibblock">OK-VQA: A Visual Question Answering Benchmark
Requiring External Knowledge. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
3195–3204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosbach et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Marius Mosbach, Sandro
Pezzelle, Michael A. Hedderich, Dietrich
Klakow, Marie-Francine Moens, and
Zeynep Akata. 2021.

</span>
<span class="ltx_bibblock">LANTERN - The Third Workshop Beyond Vision and
Language: Integrating Real World Knowledge.

</span>
<span class="ltx_bibblock">Workshop at EACL 2021.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.lantern.uni-saarland.de/2021/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.lantern.uni-saarland.de/2021/</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yulei Niu, Kaihua Tang,
Hanwang Zhang, Zhiwu Lu,
Xian-Sheng Hua, and Ji-Rong Wen.
2021.

</span>
<span class="ltx_bibblock">Counterfactual VQA: A Cause-Effect Look at
Language Bias. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
12700–12710.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park (2016)</span>
<span class="ltx_bibblock">
Madison Park.
2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">U.S. government to stop using these words
to refer to minorities</em>.

</span>
<span class="ltx_bibblock">CNN.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://edition.cnn.com/2016/05/22/politics/obama-federal-law-minorities-references/index.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://edition.cnn.com/2016/05/22/politics/obama-federal-law-minorities-references/index.html</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shankar et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Shreya Shankar, Yoni
Halpern, Eric Breck, James Atwood,
Jimbo Wilson, and D Sculley.
2017.

</span>
<span class="ltx_bibblock">No Classification Without Representation:
Assessing Geodiversity Issues in Open Data Sets for the Developing World.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">NeurIPS Workshop</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding,
Sebastian Goodman, and Radu Soricut.
2018.

</span>
<span class="ltx_bibblock">Conceptual Captions: A Cleaned, Hypernymed, Image
Alt-text Dataset For Automatic Image Captioning. In
<em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">ACL (1)</em>. Association for
Computational Linguistics, 2556–2565.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrivastava et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ayush Shrivastava,
Yash Mukund Kant, Satwik Kottur,
Dhruv Batra, Devi Parikh, and
Aishwarya Agrawal. 2021.

</span>
<span class="ltx_bibblock">Visual Question Answering Workshop.

</span>
<span class="ltx_bibblock">Workshop at CVPR 2021.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://visualqa.org/workshop" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/workshop</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit
Bansal. 2019.

</span>
<span class="ltx_bibblock">LXMERT: Learning Cross-Modality Encoder
Representations from Transformers.

</span>
<span class="ltx_bibblock">(2019), 5099–5110.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Mengnan
Du, Yuening Li, Zirui Liu,
Na Zou, and Xia Hu.
2021.

</span>
<span class="ltx_bibblock">Mitigating Gender Bias in Captioning Systems. In
<em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">WWW</em>. ACM / IW3C2,
633–645.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomee et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Bart Thomee, David A.
Shamma, Gerald Friedland, Benjamin
Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li.
2016.

</span>
<span class="ltx_bibblock">YFCC100M: the new data in multimedia research.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 59,
2 (2016), 64–73.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thong and Snoek (2021)</span>
<span class="ltx_bibblock">
William Thong and
Cees GM Snoek. 2021.

</span>
<span class="ltx_bibblock">Feature and Label Embedding Spaces Matter in
Addressing Image Classifier Bias. In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van der Vleuten et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Maaike Van der Vleuten,
Eva Jaspers, Ineke Maas, and
Tanja van der Lippe. 2016.

</span>
<span class="ltx_bibblock">Boys’ and Girls’ Educational Choices in
Secondary Education. The Role of Gender Ideology.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Educational Studies</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinyals et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Oriol Vinyals, Alexander
Toshev, Samy Bengio, and Dumitru
Erhan. 2017.

</span>
<span class="ltx_bibblock">Show and Tell: Lessons Learned from the 2015
MSCOCO Image Captioning Challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>
39, 4 (2017),
652–663.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Mei Wang, Weihong Deng,
Jiani Hu, Xunqiang Tao, and
Yaohai Huang. 2019a.

</span>
<span class="ltx_bibblock">Racial Faces in the Wild: Reducing Racial Bias by
Information Maximization Adaptation Network. In
<em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">ICCV</em>. IEEE,
692–702.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tan Wang, Jianqiang
Huang, Hanwang Zhang, and Qianru Sun.
2020.

</span>
<span class="ltx_bibblock">Visual Commonsense R-CNN. In
<em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. Computer Vision
Foundation / IEEE, 10757–10767.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Tianlu Wang, Jieyu Zhao,
Mark Yatskar, Kai-Wei Chang, and
Vicente Ordonez. 2019b.

</span>
<span class="ltx_bibblock">Balanced Datasets Are Not Enough: Estimating and
Mitigating Gender Bias in Deep Image Representations. In
<em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">ICCV</em>. IEEE,
5309–5318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Gan (2021)</span>
<span class="ltx_bibblock">
Qi Wu and Zhe Gan.
2021.

</span>
<span class="ltx_bibblock">From VQA to VLN: Recent Advances in
Vision-and-Language Research.

</span>
<span class="ltx_bibblock">Tutorial at CVPR 2021.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://vqa2vln-tutorial.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vqa2vln-tutorial.github.io/</a>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zekun Yang, Noa Garcia,
Chenhui Chu, Mayu Otani,
Yuta Nakashima, and Haruo Takemura.
2020.

</span>
<span class="ltx_bibblock">BERT Representations for Video Question
Answering. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">WACV</em>.
IEEE, 1545–1554.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Quanzeng You, Hailin Jin,
Zhaowen Wang, Chen Fang, and
Jiebo Luo. 2016.

</span>
<span class="ltx_bibblock">Image Captioning with Semantic Attention. In
<em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. IEEE Computer
Society, 4651–4659.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun
Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.
2021.

</span>
<span class="ltx_bibblock">VinVL: Revisiting Visual Representations in
Vision-Language Models. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.
Computer Vision Foundation / IEEE,
5579–5588.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Dora Zhao, Angelina Wang,
and Olga Russakovsky. 2021.

</span>
<span class="ltx_bibblock">Understanding and Evaluating Racial Biases in Image
Captioning. In <em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.
IEEE, 14810–14820.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Jieyu Zhao, Tianlu Wang,
Mark Yatskar, Vicente Ordonez, and
Kai-Wei Chang. 2017.

</span>
<span class="ltx_bibblock">Men Also Like Shopping: Reducing Gender Bias
Amplification using Corpus-level Constraints. In
<em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">EMNLP</em>. Association for
Computational Linguistics, 2979–2989.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth,
Michael S. Bernstein, and Li
Fei-Fei. 2016.

</span>
<span class="ltx_bibblock">Visual7W: Grounded Question Answering in Images.
In <em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">CVPR</em>. IEEE Computer
Society, 4995–5004.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Gender Bias in VQA</h2>

<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gender words</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">We list the female/male words that are used to define female/male questions:
<span id="A1.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="color:#FF8000;">woman</span>, <span id="A1.SS0.SSS0.Px1.p1.1.2" class="ltx_text" style="color:#FF8000;">female</span>, <span id="A1.SS0.SSS0.Px1.p1.1.3" class="ltx_text" style="color:#FF8000;">lady</span>, <span id="A1.SS0.SSS0.Px1.p1.1.4" class="ltx_text" style="color:#FF8000;">mother</span>, <span id="A1.SS0.SSS0.Px1.p1.1.5" class="ltx_text" style="color:#FF8000;">girl</span>, <span id="A1.SS0.SSS0.Px1.p1.1.6" class="ltx_text" style="color:#FF8000;">aunt</span>,
<span id="A1.SS0.SSS0.Px1.p1.1.7" class="ltx_text" style="color:#FF8000;">wife</span>, <span id="A1.SS0.SSS0.Px1.p1.1.8" class="ltx_text" style="color:#FF8000;">actress</span>, <span id="A1.SS0.SSS0.Px1.p1.1.9" class="ltx_text" style="color:#FF8000;">princess</span>, <span id="A1.SS0.SSS0.Px1.p1.1.10" class="ltx_text" style="color:#FF8000;">waitress</span>, <span id="A1.SS0.SSS0.Px1.p1.1.11" class="ltx_text" style="color:#FF8000;">sister</span>, <span id="A1.SS0.SSS0.Px1.p1.1.12" class="ltx_text" style="color:#FF8000;">queen</span>, <span id="A1.SS0.SSS0.Px1.p1.1.13" class="ltx_text" style="color:#FF8000;">pregnant</span>, <span id="A1.SS0.SSS0.Px1.p1.1.14" class="ltx_text" style="color:#FF8000;">daughter</span>, <span id="A1.SS0.SSS0.Px1.p1.1.15" class="ltx_text" style="color:#FF8000;">girlfriend</span>, <span id="A1.SS0.SSS0.Px1.p1.1.16" class="ltx_text" style="color:#FF8000;">chairwoman</span>, <span id="A1.SS0.SSS0.Px1.p1.1.17" class="ltx_text" style="color:#FF8000;">policewoman</span>, <span id="A1.SS0.SSS0.Px1.p1.1.18" class="ltx_text" style="color:#FF8000;">she</span>, <span id="A1.SS0.SSS0.Px1.p1.1.19" class="ltx_text" style="color:#FF8000;">her</span>, <span id="A1.SS0.SSS0.Px1.p1.1.20" class="ltx_text" style="color:#FF8000;">hers</span>, <span id="A1.SS0.SSS0.Px1.p1.1.21" class="ltx_text" style="color:#FF8000;">herself</span>,
<span id="A1.SS0.SSS0.Px1.p1.1.22" class="ltx_text ltx_font_italic" style="color:#808000;">man</span>, <span id="A1.SS0.SSS0.Px1.p1.1.23" class="ltx_text ltx_font_italic" style="color:#808000;">male</span>, <span id="A1.SS0.SSS0.Px1.p1.1.24" class="ltx_text ltx_font_italic" style="color:#808000;">father</span>, <span id="A1.SS0.SSS0.Px1.p1.1.25" class="ltx_text ltx_font_italic" style="color:#808000;">gentleman</span>, <span id="A1.SS0.SSS0.Px1.p1.1.26" class="ltx_text ltx_font_italic" style="color:#808000;">boy</span>, <span id="A1.SS0.SSS0.Px1.p1.1.27" class="ltx_text ltx_font_italic" style="color:#808000;">uncle</span>, <span id="A1.SS0.SSS0.Px1.p1.1.28" class="ltx_text ltx_font_italic" style="color:#808000;">husband</span>, <span id="A1.SS0.SSS0.Px1.p1.1.29" class="ltx_text ltx_font_italic" style="color:#808000;">actor</span>, <span id="A1.SS0.SSS0.Px1.p1.1.30" class="ltx_text ltx_font_italic" style="color:#808000;">prince</span>, <span id="A1.SS0.SSS0.Px1.p1.1.31" class="ltx_text ltx_font_italic" style="color:#808000;">waiter</span>, <span id="A1.SS0.SSS0.Px1.p1.1.32" class="ltx_text ltx_font_italic" style="color:#808000;">son</span>, <span id="A1.SS0.SSS0.Px1.p1.1.33" class="ltx_text ltx_font_italic" style="color:#808000;">brother</span>, <span id="A1.SS0.SSS0.Px1.p1.1.34" class="ltx_text ltx_font_italic" style="color:#808000;">guy</span>, <span id="A1.SS0.SSS0.Px1.p1.1.35" class="ltx_text ltx_font_italic" style="color:#808000;">emperor</span>, <span id="A1.SS0.SSS0.Px1.p1.1.36" class="ltx_text ltx_font_italic" style="color:#808000;">dude</span>, <span id="A1.SS0.SSS0.Px1.p1.1.37" class="ltx_text ltx_font_italic" style="color:#808000;">cowboy</span>, <span id="A1.SS0.SSS0.Px1.p1.1.38" class="ltx_text ltx_font_italic" style="color:#808000;">boyfriend</span>, <span id="A1.SS0.SSS0.Px1.p1.1.39" class="ltx_text ltx_font_italic" style="color:#808000;">chairman</span>, <span id="A1.SS0.SSS0.Px1.p1.1.40" class="ltx_text ltx_font_italic" style="color:#808000;">policeman</span>, <span id="A1.SS0.SSS0.Px1.p1.1.41" class="ltx_text ltx_font_italic" style="color:#808000;">he</span>, <span id="A1.SS0.SSS0.Px1.p1.1.42" class="ltx_text ltx_font_italic" style="color:#808000;">his</span>, <span id="A1.SS0.SSS0.Px1.p1.1.43" class="ltx_text ltx_font_italic" style="color:#808000;">him</span>, <span id="A1.SS0.SSS0.Px1.p1.1.44" class="ltx_text ltx_font_italic" style="color:#808000;">himself</span> and their plurals.
<span id="A1.SS0.SSS0.Px1.p1.1.45" class="ltx_text" style="color:#FF8000;">Orange</span> denotes female words, whereas <span id="A1.SS0.SSS0.Px1.p1.1.46" class="ltx_text ltx_font_italic" style="color:#808000;">green</span> denotes male words.
We select the gender words by investigating the datasets manually.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Answer distributions are skewed toward each gender.</h4>

<figure id="A1.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x15.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="45" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x16.png" id="A1.F11.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="50" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x17.png" id="A1.F11.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="45" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x18.png" id="A1.F11.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="50" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x19.png" id="A1.F11.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="45" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x20.png" id="A1.F11.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="50" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x21.png" id="A1.F11.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="45" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x22.png" id="A1.F11.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="256" height="50" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Top-20 frequent answers in GQA, Visual Genome, Visual7W, and OK-VQA: Frequent answers for women questions (<span id="A1.F11.4.1" class="ltx_text" style="color:#FF8C00;">orange</span>). For the comparison, we also show the ratio of the answers over men questions (<span id="A1.F11.5.2" class="ltx_text" style="color:#006400;">green</span>). <span id="A1.F11.6.3" class="ltx_text ltx_font_bold">Below</span>: Frequent answers for men questions. As in above, we also show the ratio of answers for women questions. A large difference in ratio indicates that the answer is skewed toward certain gender.</figcaption>
</figure>
<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">In the main paper, we show the top-<math id="A1.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A1.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="A1.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="A1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.1.m1.1c">20</annotation></semantics></math> answers of VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>. In the appendix, we also show the results of the other datasets (GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>, Visual7W <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>, and OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>). As well as VQA 2.0, the distributions for women/men questions are skewed toward each gender in the other datasets.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gender-answer correlations reflect gender stereotypes and discrimination.</h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.6" class="ltx_p">Here, we describe the detailed setting for BS (Section 4.3 in the main paper). We filter answers that do not appear more than <math id="A1.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A1.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="A1.SS0.SSS0.Px3.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="A1.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.1.m1.1c">n</annotation></semantics></math> times in women/men questions. For each dataset, we use: <math id="A1.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="n=150" display="inline"><semantics id="A1.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="A1.SS0.SSS0.Px3.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">n</mi><mo id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">150</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1"><eq id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1"></eq><ci id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝑛</ci><cn type="integer" id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3">150</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.2.m2.1c">n=150</annotation></semantics></math> (VQA 2.0), <math id="A1.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="n=100" display="inline"><semantics id="A1.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="A1.SS0.SSS0.Px3.p1.3.m3.1.1" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">n</mi><mo id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">=</mo><mn id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1"><eq id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.1"></eq><ci id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.2">𝑛</ci><cn type="integer" id="A1.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="A1.SS0.SSS0.Px3.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.3.m3.1c">n=100</annotation></semantics></math> (Visual Genome), <math id="A1.SS0.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="n=100" display="inline"><semantics id="A1.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="A1.SS0.SSS0.Px3.p1.4.m4.1.1" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">n</mi><mo id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.1" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">=</mo><mn id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.3" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1"><eq id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.1"></eq><ci id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.2">𝑛</ci><cn type="integer" id="A1.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="A1.SS0.SSS0.Px3.p1.4.m4.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.4.m4.1c">n=100</annotation></semantics></math> (GQA), <math id="A1.SS0.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="n=10" display="inline"><semantics id="A1.SS0.SSS0.Px3.p1.5.m5.1a"><mrow id="A1.SS0.SSS0.Px3.p1.5.m5.1.1" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.2" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.2.cmml">n</mi><mo id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.1" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml">=</mo><mn id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.3" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.5.m5.1b"><apply id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1"><eq id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.1"></eq><ci id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.2">𝑛</ci><cn type="integer" id="A1.SS0.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="A1.SS0.SSS0.Px3.p1.5.m5.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.5.m5.1c">n=10</annotation></semantics></math> (Visual7W), <math id="A1.SS0.SSS0.Px3.p1.6.m6.1" class="ltx_Math" alttext="n=5" display="inline"><semantics id="A1.SS0.SSS0.Px3.p1.6.m6.1a"><mrow id="A1.SS0.SSS0.Px3.p1.6.m6.1.1" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.cmml"><mi id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.2" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.2.cmml">n</mi><mo id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.1" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.1.cmml">=</mo><mn id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.3" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.6.m6.1b"><apply id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1"><eq id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.1"></eq><ci id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.2.cmml" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.2">𝑛</ci><cn type="integer" id="A1.SS0.SSS0.Px3.p1.6.m6.1.1.3.cmml" xref="A1.SS0.SSS0.Px3.p1.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.6.m6.1c">n=5</annotation></semantics></math> (OK-VQA).</p>
</div>
<div id="A1.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p2.1" class="ltx_p">We show the results of BS for GQA, Visual Genome, Visual7W, and OK-VQA in Figure <a href="#A1.F12" title="Figure 12 ‣ Gender-answer correlations reflect gender stereotypes and discrimination. ‣ Appendix A Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. As in the case of VQA 2.0 in the main paper, the distributions are highly skewed toward each gender.</p>
</div>
<figure id="A1.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x23.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="42" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x24.png" id="A1.F12.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="53" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x25.png" id="A1.F12.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="46" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x26.png" id="A1.F12.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="51" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x27.png" id="A1.F12.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="56" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x28.png" id="A1.F12.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="51" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x29.png" id="A1.F12.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="45" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.08148/assets/x30.png" id="A1.F12.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="273" height="46" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>Top-20 answers that are co-related with women questions (<span id="A1.F12.3.1" class="ltx_text ltx_font_bold">above</span>) and men questions (<span id="A1.F12.4.2" class="ltx_text ltx_font_bold">below</span>) in GQA, Visual Genome, Visual7W, and OK-VQA.</figcaption>
</figure>
</section>
<section id="A1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gender-stereotypical samples</h4>

<figure id="A1.F13" class="ltx_figure"><img src="/html/2205.08148/assets/x31.png" id="A1.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Examples of gender stereotypes in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> (<span id="A1.F13.4.1" class="ltx_text ltx_font_bold">above</span>), OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> (<span id="A1.F13.5.2" class="ltx_text ltx_font_bold">middle</span>), and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> (<span id="A1.F13.6.3" class="ltx_text ltx_font_bold">below</span>)</figcaption>
</figure>
<div id="A1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px4.p1.1" class="ltx_p">In Figure <a href="#A1.F13" title="Figure 13 ‣ Gender-stereotypical samples ‣ Appendix A Gender Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, we additionally show some samples that include harmful gender stereotypes in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>, OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Racial Bias in VQA</h2>

<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Racial-related words and nationality-related words</h4>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p1.1" class="ltx_p">We show the full list of racial-related words and nationality-related words used for the analysis in the paper.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p2.1" class="ltx_p">The list of racial-related words is: <span id="A2.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">Black</span>, <span id="A2.SS0.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_italic">African</span>, <span id="A2.SS0.SSS0.Px1.p2.1.3" class="ltx_text ltx_font_italic">Africa</span>, <span id="A2.SS0.SSS0.Px1.p2.1.4" class="ltx_text ltx_font_italic">Latino</span>, <span id="A2.SS0.SSS0.Px1.p2.1.5" class="ltx_text ltx_font_italic">Latina</span>, <span id="A2.SS0.SSS0.Px1.p2.1.6" class="ltx_text ltx_font_italic">Latinx</span>, <span id="A2.SS0.SSS0.Px1.p2.1.7" class="ltx_text ltx_font_italic">Hispanic</span>, <span id="A2.SS0.SSS0.Px1.p2.1.8" class="ltx_text ltx_font_italic">White</span>, <span id="A2.SS0.SSS0.Px1.p2.1.9" class="ltx_text ltx_font_italic">Caucasian</span>, <span id="A2.SS0.SSS0.Px1.p2.1.10" class="ltx_text ltx_font_italic">Asian</span>, <span id="A2.SS0.SSS0.Px1.p2.1.11" class="ltx_text ltx_font_italic">Oriental</span>, <span id="A2.SS0.SSS0.Px1.p2.1.12" class="ltx_text ltx_font_italic">Asia</span>, <span id="A2.SS0.SSS0.Px1.p2.1.13" class="ltx_text ltx_font_italic">Native</span>, <span id="A2.SS0.SSS0.Px1.p2.1.14" class="ltx_text ltx_font_italic">Indigenous</span>, <span id="A2.SS0.SSS0.Px1.p2.1.15" class="ltx_text ltx_font_italic">Arabic</span>.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p3.1" class="ltx_p">The list of nationality-related words is: <span id="A2.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_italic">American</span>, <span id="A2.SS0.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_italic">USA</span>, <span id="A2.SS0.SSS0.Px1.p3.1.3" class="ltx_text ltx_font_italic">United States</span>, <span id="A2.SS0.SSS0.Px1.p3.1.4" class="ltx_text ltx_font_italic">African American</span>, <span id="A2.SS0.SSS0.Px1.p3.1.5" class="ltx_text ltx_font_italic">Chinese</span>, <span id="A2.SS0.SSS0.Px1.p3.1.6" class="ltx_text ltx_font_italic">China</span>, <span id="A2.SS0.SSS0.Px1.p3.1.7" class="ltx_text ltx_font_italic">Japanese</span>, <span id="A2.SS0.SSS0.Px1.p3.1.8" class="ltx_text ltx_font_italic">Japan</span>, <span id="A2.SS0.SSS0.Px1.p3.1.9" class="ltx_text ltx_font_italic">Indian</span>, <span id="A2.SS0.SSS0.Px1.p3.1.10" class="ltx_text ltx_font_italic">India</span>, <span id="A2.SS0.SSS0.Px1.p3.1.11" class="ltx_text ltx_font_italic">Mexican</span>, <span id="A2.SS0.SSS0.Px1.p3.1.12" class="ltx_text ltx_font_italic">Mexico</span>, <span id="A2.SS0.SSS0.Px1.p3.1.13" class="ltx_text ltx_font_italic">Italian</span>, <span id="A2.SS0.SSS0.Px1.p3.1.14" class="ltx_text ltx_font_italic">Italy</span>, <span id="A2.SS0.SSS0.Px1.p3.1.15" class="ltx_text ltx_font_italic">Spanish</span>, <span id="A2.SS0.SSS0.Px1.p3.1.16" class="ltx_text ltx_font_italic">German</span>, <span id="A2.SS0.SSS0.Px1.p3.1.17" class="ltx_text ltx_font_italic">French</span>, <span id="A2.SS0.SSS0.Px1.p3.1.18" class="ltx_text ltx_font_italic">France</span>, <span id="A2.SS0.SSS0.Px1.p3.1.19" class="ltx_text ltx_font_italic">English</span>, <span id="A2.SS0.SSS0.Px1.p3.1.20" class="ltx_text ltx_font_italic">British</span>, <span id="A2.SS0.SSS0.Px1.p3.1.21" class="ltx_text ltx_font_italic">England</span>, <span id="A2.SS0.SSS0.Px1.p3.1.22" class="ltx_text ltx_font_italic">Russian</span>, <span id="A2.SS0.SSS0.Px1.p3.1.23" class="ltx_text ltx_font_italic">Swiss</span>, <span id="A2.SS0.SSS0.Px1.p3.1.24" class="ltx_text ltx_font_italic">Hawaiian</span>, <span id="A2.SS0.SSS0.Px1.p3.1.25" class="ltx_text ltx_font_italic">Thai</span>, <span id="A2.SS0.SSS0.Px1.p3.1.26" class="ltx_text ltx_font_italic">Brazil</span>.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Racial-stereotypical examples</h4>

<figure id="A2.F14" class="ltx_figure"><img src="/html/2205.08148/assets/x32.png" id="A2.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Examples of racial stereotypes in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> (<span id="A2.F14.4.1" class="ltx_text ltx_font_bold">above</span>), OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> (<span id="A2.F14.5.2" class="ltx_text ltx_font_bold">middle</span>), and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> (<span id="A2.F14.6.3" class="ltx_text ltx_font_bold">below</span>)</figcaption>
</figure>
<div id="A2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px2.p1.1" class="ltx_p">In Figure <a href="#A2.F14" title="Figure 14 ‣ Racial-stereotypical examples ‣ Appendix B Racial Bias in VQA ‣ Gender and Racial Bias in Visual Question Answering Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, we additionally show some samples that include harmful racial stereotypes in VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>, OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, and Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.08147" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.08148" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.08148">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.08148" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.08149" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 15:45:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
