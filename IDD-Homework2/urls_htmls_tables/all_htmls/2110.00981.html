<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.00981] SecFL: Confidential Federated Learning using TEEs</title><meta property="og:description" content="Federated Learning (FL) is an emerging machine learning paradigm that enables multiple clients to jointly train a model to take benefits from diverse datasets from the clients without sharing their local training datas…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SecFL: Confidential Federated Learning using TEEs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SecFL: Confidential Federated Learning using TEEs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.00981">

<!--Generated on Tue Mar 19 14:28:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">
<span id="id1.id1" class="ltx_text ltx_font_medium ltx_font_smallcaps">SecFL:</span> Confidential Federated Learning using TEEs
</h1>
<div class="ltx_subtitle">
<span id="id2.id1" class="ltx_text" style="font-size:90%;">Demo paper</span>
</div>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Do Le Quoc
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">Huawei Munich Research Center <span id="id3.1.id1.1" class="ltx_text ltx_affiliation_country">Germany</span></span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christof Fetzer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">TU Dresden <span id="id4.1.id1.1" class="ltx_text ltx_affiliation_country">Germany</span></span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id5.id1" class="ltx_p"><span id="id5.id1.1" class="ltx_text" lang="en">Federated Learning (FL) is an emerging machine learning paradigm that enables multiple clients to jointly train a model to take benefits from diverse datasets from the clients without sharing their local training datasets. FL helps reduce data privacy risks. Unfortunately, FL still exist several issues regarding privacy and security. First, it is possible to leak sensitive information from the shared training parameters. Second, malicious clients can collude with each other to steal data, models from regular clients or corrupt the global training model.
To tackle these challenges, we propose <span id="id5.id1.1.1" class="ltx_text ltx_font_smallcaps">SecFL</span>- a confidential federated learning framework that leverages Trusted Execution Environments (TEEs). <span id="id5.id1.1.2" class="ltx_text ltx_font_smallcaps">SecFL</span> performs the global and local training inside TEE enclaves to ensure the confidentiality and integrity of the computations against powerful adversaries with privileged access. <span id="id5.id1.1.3" class="ltx_text ltx_font_smallcaps">SecFL</span> provides a transparent remote attestation mechanism, relying on the remote attestation provided by TEEs, to allow clients to attest the global training computation as well as the local training computation of each other. Thus, all malicious clients can be detected using the remote attestation mechanisms.</span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">FL <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite> is an emerging machine learning technique which allows participating clients to collaboratively train a joint global machine learning model without sharing their local training data. FL reduces privacy risks for the local training data which may be highly sensitive relating to personal finances, political views, health, etc. Thus, it has been used widely in the industry since it helps companies comply with regulations on issues regarding the way in which personal data is handled and processed - such as EU’s General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_citep">(Voigt and Von dem
Bussche, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite>.
The core idea of FL is that each client trains a local model, rather than sharing training data to a centralized training system which is deployed in an untrusted environment, e.g., a public cloud.
For each iteration, the clients send their local training parameters to the central system to train a global model which takes benefits from all local training data from clients. Typically, the central system aggregates the local training parameters from the clients and sends the aggregated parameters back to them. This training process is repeated until it converges or the global model reaches a certain desired accuracy. An example of FL in real-life deployment is that several hospitals collaborate to develop a shared machine learning model based on their patient data to detect a disease at an early stage. Each hospital trains its data locally, shares the local model with the central training system, and receives the global model in each iteration.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While promising at first glance, FL paradigm suffers several vulnerabilities. First, an attacker with privileged/root accesses can easily obtain the training models (<span id="S1.p2.1.1" class="ltx_text" style="position:relative; bottom:-1.0pt;">➊</span>). The attacker can also compromise the privacy of individuals in the training data by inferring it from parameters of the global model <cite class="ltx_cite ltx_citemacro_citep">(Abadi et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>.
Therefore, the training models need to be protected at rest, in transit, and in use.
Second, a large number of malicious clients may collude with each other to reveal local data and local models of the remaining clients <cite class="ltx_cite ltx_citemacro_citep">(Mugunthan
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> (<span id="S1.p2.1.2" class="ltx_text" style="position:relative; bottom:-1.0pt;">➋</span>).
Last but not least, these malicious clients can tamper their local training data or parameters updates forwarded to the central training system to corrupt the global model <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> (<span id="S1.p2.1.3" class="ltx_text" style="position:relative; bottom:-1.0pt;">➌</span>).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To handle the issues <span id="S1.p3.1.1" class="ltx_text" style="position:relative; bottom:-1.0pt;">➊</span> <span id="S1.p3.1.2" class="ltx_text" style="position:relative; bottom:-1.0pt;">➋</span>, state-of-the-art solutions rely on a privacy-preserving mechanism such as differential privacy or secure multiparty computation (MPC). The disadvantage of the differential privacy mechanism is that it reduces the performance of the global training model regarding utility or accuracy. Meanwhile, the solutions based on secure multiparty computation incur significant overhead <cite class="ltx_cite ltx_citemacro_citep">(Le Quoc et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Kunkel et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. To cope with issue <span id="S1.p3.1.3" class="ltx_text" style="position:relative; bottom:-1.0pt;">➌</span>, several Byzantine-robust federated learning mechanisms have been proposed <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Bhagoji et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. The core idea behind these mechanisms is to reduce the impact of statistical outliers during model updates in the federated learning system.
However, recent works <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Bhagoji et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> show that the mitigation of the impact is still not enough to protect the utility of the global model. The malicious clients can still affect the accuracy of the global model trained by a Byzantine-robust mechanism by carefully tampering with their model parameters sent to the central training system <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we overcome these limitations by building a confidential federated learning system called <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">SecFL</span> using TEEs, e.g., Intel SGX.
<span title="" class="ltx_glossaryref" style="color:#000000;">Trusted Execution Environment (TEE)</span> technologies, such as Intel <span title="" class="ltx_glossaryref" style="color:#000000;">Software Guard eXtensions (SGX)</span>, have gained much attention in the industry <cite class="ltx_cite ltx_citemacro_citep">(Gordon, <a href="#bib.bib10" title="" class="ltx_ref">[n.d.]</a>; Karnati, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>; Singh
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> as well as in academia <cite class="ltx_cite ltx_citemacro_citep">(Arnautov et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>; Costan and
Devadas, <a href="#bib.bib8" title="" class="ltx_ref">2016</a>; Tsai
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2017</a>; Le Quoc
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2019</a>; Ozga
et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib22" title="" class="ltx_ref">b</a>; Le Quoc et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Singh
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Krahn et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>; Ozga
et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>; Bailleu et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Ozga
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021c</a>)</cite>.
To ensure confidentiality and integrity of applications, <span title="" class="ltx_glossaryref" style="color:#000000;">TEEs</span> execute their code and data inside an encrypted memory region called <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">enclave</em>.
Adversaries with privileged access cannot read or interfere with the memory region and only the processor can decrypt and execute the application inside an enclave.
In addition, TEEs such as Intel SGX also provide a mechanism for users to verify that the <span title="" class="ltx_glossaryref" style="color:#000000;">TEE</span> is genuine and that an adversary did not alter their application running inside TEE enclaves. The verification process is called <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">Remote Attestation</em> <cite class="ltx_cite ltx_citemacro_citep">(Costan and
Devadas, <a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> and allows users to establish trust in their application running inside an enclave on a remote host.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We leverage <span title="" class="ltx_glossaryref" style="color:#000000;">TEEs</span> to handle issue <span id="S1.p5.1.1" class="ltx_text" style="position:relative; bottom:-1.0pt;">➊</span>, by providing end-to-end encryption in <span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">SecFL</span>. <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">SecFL</span> encrypts input training data and code (e.g., Python code) and performs all training computations including local training and global training insides TEE enclaves. <span id="S1.p5.1.4" class="ltx_text ltx_font_smallcaps">SecFL</span> enables all model updates via TLS connections between the enclave of clients and the enclaves of the central training computation. Thus attackers with privileged accesses cannot violate the integrity and confidentiality of the input training data, code, and models. <span id="S1.p5.1.5" class="ltx_text ltx_font_smallcaps">SecFL</span> also ensures the <span id="S1.p5.1.6" class="ltx_text ltx_font_italic">freshness</span> of the input training data, models, and, by applying an advanced asynchronous monotonic counter service <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.
We tackle issues <span id="S1.p5.1.7" class="ltx_text" style="position:relative; bottom:-1.0pt;">➋</span> <span id="S1.p5.1.8" class="ltx_text" style="position:relative; bottom:-1.0pt;">➌</span> by developing in <span id="S1.p5.1.9" class="ltx_text ltx_font_smallcaps">SecFL</span> a <span id="S1.p5.1.10" class="ltx_text ltx_font_italic">Security Policy Manager</span> component based on the remote attestation mechanism supported by TEEs <cite class="ltx_cite ltx_citemacro_citep">(Gregor
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>; Scarlata et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>. The component ensures the integrity of input data and training code, i.e., it makes sure that training computations are running with correct code, correct input data and not modified by anyone, e.g., an attacker or malicious client. This component also monitors and attests to the compliance of participated clients with the pre-defined agreement before collaborating to train the global machine learning model. In addition, <span id="S1.p5.1.11" class="ltx_text ltx_font_smallcaps">SecFL</span> can clone the global training computation and randomly take a sample of clients for the training computation. This helps to detect outliers regarding the utility which helps to solve issue <span id="S1.p5.1.12" class="ltx_text" style="position:relative; bottom:-1.0pt;">➌</span>.
Our preliminary evaluation shows that <span id="S1.p5.1.13" class="ltx_text ltx_font_smallcaps">SecFL</span> can ensure the confidentiality and integrity of federated learning computations while maintaining the same utility/accuracy of the training computations.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Confidential Federated Learning</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2110.00981/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="178" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span id="S2.F1.2.1" class="ltx_text ltx_font_smallcaps">SecFL</span> Architecture for Federated Learning.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Confidential Federated Learning ‣ SecFL: Confidential Federated Learning using TEEs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the architecture of <span id="S2.p1.1.1" class="ltx_text ltx_font_smallcaps">SecFL</span>.
The main goal of <span id="S2.p1.1.2" class="ltx_text ltx_font_smallcaps">SecFL</span> is not only to ensure the confidentiality, integrity and freshness of input data, code, and machine learning models but also to enable multiple clients (who do not necessarily trust each other) to get the benefits of collaborative training without revealing their local training data.
In <span id="S2.p1.1.3" class="ltx_text ltx_font_smallcaps">SecFL</span>, each client performs the local training also inside TEE enclaves to make sure that no one tamper input data or training code during the computations. To govern and coordinate the collaborative machine learning training computation between clients, we design in <span id="S2.p1.1.4" class="ltx_text ltx_font_smallcaps">SecFL</span> a trusted management component, called <span id="S2.p1.1.5" class="ltx_text ltx_font_italic">Security Policy Manager</span> which maintains security policies based on the agreement among all clients to define the access control over global training computation, the global training model, also the code and input data used for local training at each client. Security Policy Manager automatically and transparently performs remote attestation to make sure the local computations are running correct code, correct input data, and on correct platforms as the agreement. It only allows clients to participate in the global training after successfully performing the remote attestation process.
It also conducts the remote attestation on the enclaves that execute the global training in a cloud, to ensure that no one at the cloud provider side modify the global training aggregation computation. <span id="S2.p1.1.6" class="ltx_text ltx_font_smallcaps">SecFL</span> encrypts the training code, and Security Policy Manager only provides the key to decrypt it inside enclaves after the remote attestation. Secrets including keys for encryption/decryption in each policy are generated by the Security Policy Manager also running inside Intel SGX enclaves and cannot be seen by any human or client. Examples of the policies can be found in  <cite class="ltx_cite ltx_citemacro_citep">(Team, <a href="#bib.bib29" title="" class="ltx_ref">[n.d.]</a>; Gregor
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">After receiving the agreed security policies from clients, Security Policy Manager strictly enforces them. It only passes secrets and configuration to applications (i.e., training computations), after attesting them. The training computations are executed inside Intel SGX enclaves and associated with policies provided and pre-agreed by clients. The training computations are identified by a secure hash and the content of the files (input data) they can access. Secrets can be passed to applications as command-line arguments, environment variables, or can be injected into files. The files can contain environment variables referring to the names of secrets defined in the security policy. The variables are transparently replaced by the value of the secret when an application that is permitted to access the secrets reads the file. We design the component Security Policy Manager in the way that we can delegate the management of it to an untrusted party, e.g., a cloud provider, while clients can still trust that their security policies for protecting their properties are safely maintained and well protected. In <span id="S2.p2.1.1" class="ltx_text ltx_font_smallcaps">SecFL</span>, clients can attest Security Policy Manager component, i.e., they can verify that it runs the expected unmodified code, in a correct platform before uploading security policies.
We implement <span id="S2.p2.1.2" class="ltx_text ltx_font_smallcaps">SecFL</span> using Intel OpenFL <cite class="ltx_cite ltx_citemacro_citep">(Reina et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> — a distributed federated machine learning framework
We run the local and global training computations inside SGX enclaves using SCONE <cite class="ltx_cite ltx_citemacro_citep">(Arnautov et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> — a shielded execution framework to enable unmodified applications to run inside SGX enclaves. In the SCONE platform, the source code of an application is recompiled against a modified standard C library (SCONE libc) to facilitate the execution of system calls. The address space of the application stays within an enclave. In <span id="S2.p2.1.3" class="ltx_text ltx_font_smallcaps">SecFL</span>, the input training data and code are encrypted using the file system shield of SCONE, and then decrypted and processed inside SGX enclaves which cannot be accessed even by strong attackers with root access. We rely on our previous works <cite class="ltx_cite ltx_citemacro_citep">(Gregor
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>; Team, <a href="#bib.bib29" title="" class="ltx_ref">[n.d.]</a>)</cite> to implement Security Policy Manager. A demo of <span id="S2.p2.1.4" class="ltx_text ltx_font_smallcaps">SecFL</span> is publicly available in <cite class="ltx_cite ltx_citemacro_citep">(Quoc, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu,
Ian Goodfellow, H. Brendan McMahan,
Ilya Mironov, Kunal Talwar, and
Li Zhang. 2016.

</span>
<span class="ltx_bibblock">Deep Learning with Differential Privacy. In
<em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 ACM SIGSAC Conference on
Computer and Communications Security (CCS)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arnautov et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Sergei Arnautov, Bohdan
Trach, Franz Gregor, Thomas Knauth,
Andre Martin, Christian Priebe,
Joshua Lind, Divya Muthukumaran,
Dan O’Keeffe, Mark L.
Stillwell, David Goltzsche, Dave Eyers,
Rüdiger Kapitza, Peter Pietzuch,
and Christof Fetzer. 2016.

</span>
<span class="ltx_bibblock">SCONE: Secure Linux Containers with Intel SGX.
In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">12th USENIX Symposium on Operating Systems
Design and Implementation (OSDI)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bailleu et al<span id="bib.bib4.8.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Maurice Bailleu, Dimitra
Giantsidi, Vasilis Gavrielatos, Do
Le Quoc, Vijay Nagarajan, and Pramod
Bhatotia. 2021.

</span>
<span class="ltx_bibblock">Avocado: A Secure In-Memory Distributed Storage
System. In <em id="bib.bib4.6.6" class="ltx_emph ltx_font_italic">2021 <math id="bib.bib4.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib4.1.1.m1.1a"><mo stretchy="false" id="bib.bib4.1.1.m1.1.1" xref="bib.bib4.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib4.1.1.m1.1b"><ci id="bib.bib4.1.1.m1.1.1.cmml" xref="bib.bib4.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib4.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib4.2.2.m2.1a"><mo stretchy="false" id="bib.bib4.2.2.m2.1.1" xref="bib.bib4.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib4.2.2.m2.1b"><ci id="bib.bib4.2.2.m2.1.1.cmml" xref="bib.bib4.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.2.2.m2.1c">\}</annotation></semantics></math> Annual Technical
Conference (<math id="bib.bib4.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib4.3.3.m3.1a"><mo stretchy="false" id="bib.bib4.3.3.m3.1.1" xref="bib.bib4.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib4.3.3.m3.1b"><ci id="bib.bib4.3.3.m3.1.1.cmml" xref="bib.bib4.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib4.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib4.4.4.m4.1a"><mo stretchy="false" id="bib.bib4.4.4.m4.1.1" xref="bib.bib4.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib4.4.4.m4.1b"><ci id="bib.bib4.4.4.m4.1.1.cmml" xref="bib.bib4.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.4.4.m4.1c">\}</annotation></semantics></math><math id="bib.bib4.5.5.m5.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib4.5.5.m5.1a"><mo stretchy="false" id="bib.bib4.5.5.m5.1.1" xref="bib.bib4.5.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib4.5.5.m5.1b"><ci id="bib.bib4.5.5.m5.1.1.cmml" xref="bib.bib4.5.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.5.5.m5.1c">\{</annotation></semantics></math>ATC<math id="bib.bib4.6.6.m6.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib4.6.6.m6.1a"><mo stretchy="false" id="bib.bib4.6.6.m6.1.1" xref="bib.bib4.6.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib4.6.6.m6.1b"><ci id="bib.bib4.6.6.m6.1.1.cmml" xref="bib.bib4.6.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.6.6.m6.1c">\}</annotation></semantics></math> 21)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Arjun Nitin Bhagoji,
Supriyo Chakraborty, Prateek Mittal,
and Seraphin Calo. 2019.

</span>
<span class="ltx_bibblock">Analyzing federated learning through an adversarial
lens. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi
El Mhamdi, Rachid Guerraoui, and Julien
Stainer. 2017.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine
tolerant gradient descent. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
31st International Conference on Neural Information Processing Systems</em>.
118–128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiaoyu Cao, Jinyuan Jia,
and Neil Zhenqiang Gong.
2021.

</span>
<span class="ltx_bibblock">Provably Secure Federated Learning against
Malicious Clients. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costan and
Devadas (2016)</span>
<span class="ltx_bibblock">
Victor Costan and
Srinivas Devadas. 2016.

</span>
<span class="ltx_bibblock">Intel SGX Explained.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IACR Cryptol. ePrint Arch.</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
et al<span id="bib.bib9.6.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu
Cao, Jinyuan Jia, and Neil Gong.
2020.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to byzantine-robust
federated learning. In <em id="bib.bib9.4.4" class="ltx_emph ltx_font_italic">29th <math id="bib.bib9.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib9.1.1.m1.1a"><mo stretchy="false" id="bib.bib9.1.1.m1.1.1" xref="bib.bib9.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.1.1.m1.1b"><ci id="bib.bib9.1.1.m1.1.1.cmml" xref="bib.bib9.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib9.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib9.2.2.m2.1a"><mo stretchy="false" id="bib.bib9.2.2.m2.1.1" xref="bib.bib9.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.2.2.m2.1b"><ci id="bib.bib9.2.2.m2.1.1.cmml" xref="bib.bib9.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.2.2.m2.1c">\}</annotation></semantics></math>
Security Symposium (<math id="bib.bib9.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib9.3.3.m3.1a"><mo stretchy="false" id="bib.bib9.3.3.m3.1.1" xref="bib.bib9.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.3.3.m3.1b"><ci id="bib.bib9.3.3.m3.1.1.cmml" xref="bib.bib9.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib9.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib9.4.4.m4.1a"><mo stretchy="false" id="bib.bib9.4.4.m4.1.1" xref="bib.bib9.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.4.4.m4.1b"><ci id="bib.bib9.4.4.m4.1.1.cmml" xref="bib.bib9.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.4.4.m4.1c">\}</annotation></semantics></math> Security 20)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon ([n.d.])</span>
<span class="ltx_bibblock">
James C Gordon.
[n.d.].

</span>
<span class="ltx_bibblock">Microsoft Azure Confidential Computing with Intel
SGX.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://software.intel.com/content/www/us/en/develop/blogs/microsoft-azure-confidential-computing-with-intel-sgx.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://software.intel.com/content/www/us/en/develop/blogs/microsoft-azure-confidential-computing-with-intel-sgx.html</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: Sept 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gregor
et al<span id="bib.bib11.3.3.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Franz Gregor, Wojciech
Ozga, Sébastien Vaucher, Rafael
Pires, Sergei Arnautov, André
Martin, Valerio Schiavoni, Pascal
Felber, Christof Fetzer, et al<span id="bib.bib11.4.1" class="ltx_text">.</span>
2020.

</span>
<span class="ltx_bibblock">Trust management as a service: Enabling trusted
execution in the face of byzantine stakeholders. In
<em id="bib.bib11.5.1" class="ltx_emph ltx_font_italic">2020 50th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karnati (2018)</span>
<span class="ltx_bibblock">
Pratheek Karnati.
2018.

</span>
<span class="ltx_bibblock">Data-in-use protection on IBM Cloud using Intel SGX.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.ibm.com/cloud/blog/data-use-protection-ibm-cloud-using-intel-sgx" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ibm.com/cloud/blog/data-use-protection-ibm-cloud-using-intel-sgx</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: Sept 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Felix X Yu,
Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon.
2016.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving
communication efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krahn et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Robert Krahn, Donald
Dragoti, Franz Gregor, Do Le Quoc,
Valerio Schiavoni, Pascal Felber,
Clenimar Souza, Andrey Brito, and
Christof Fetzer. 2020.

</span>
<span class="ltx_bibblock">TEEMon: A continuous performance monitoring
framework for TEEs. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th
International Middleware Conference (Middleware)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kunkel et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Roland Kunkel, Do Le
Quoc, Franz Gregor, Sergei Arnautov,
Pramod Bhatotia, and Christof Fetzer.
2019.

</span>
<span class="ltx_bibblock">Tensorscone: A secure tensorflow framework using
intel sgx.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.04413</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Quoc et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Do Le Quoc, Franz Gregor,
Sergei Arnautov, Roland Kunkeland,
Pramod Bhatotia, and Christof Fetzer.
2020.

</span>
<span class="ltx_bibblock">secureTF: A Secure TensorFlow Framework. In
<em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th International Middleware
Conference (Middleware)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Quoc
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Do Le Quoc, Franz Gregor,
Jatinder Singh, and Christof Fetzer.
2019.

</span>
<span class="ltx_bibblock">SGX-PySpark: Secure Distributed Data Analytics. In
<em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the World Wide Web Conference
(WWW)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
André Martin, Cong
Lian, Franz Gregor, Robert Krahn,
Valerio Schiavoni, Pascal Felber, and
Christof Fetzer. 2021.

</span>
<span class="ltx_bibblock">ADAM-CS: Advanced Asynchronous Monotonic Counter
Service. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">2021 51st Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mugunthan
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Vaikkunth Mugunthan, Anton
Peraire-Bueno, and Lalana Kagal.
2020.

</span>
<span class="ltx_bibblock">PrivacyFL: A simulator for privacy-preserving and
secure federated learning. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
29th ACM International Conference on Information &amp; Knowledge Management
(CIKM)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozga
et al<span id="bib.bib20.3.3.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Wojciech Ozga, Christof
Fetzer, et al<span id="bib.bib20.4.1" class="ltx_text">.</span> 2021a.

</span>
<span class="ltx_bibblock">Perun: Confidential Multi-stakeholder Machine
Learning Framework with Hardware Acceleration Support. In
<em id="bib.bib20.5.1" class="ltx_emph ltx_font_italic">IFIP Annual Conference on Data and Applications
Security and Privacy</em>. Springer, 189–208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozga
et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wojciech Ozga, Do
Le Quoc, and Christof Fetzer.
2020.

</span>
<span class="ltx_bibblock">A practical approach for updating an
integrity-enforced operating system. In
<em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th International Middleware
Conference (Middleware)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozga
et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Wojciech Ozga, Do Le
Quoc, and Christof Fetzer.
2021b.

</span>
<span class="ltx_bibblock">Perun: Secure Multi-Stakeholder Machine Learning
Framework with GPU Support.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.16898</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozga
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2021c)</span>
<span class="ltx_bibblock">
Wojciech Ozga, Do Le
Quoc, and Christof Fetzer.
2021c.

</span>
<span class="ltx_bibblock">WELES: Policy-driven Runtime Integrity Enforcement
of Virtual Machines.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.14862</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quoc (2021)</span>
<span class="ltx_bibblock">
Do Le Quoc.
2021.

</span>
<span class="ltx_bibblock">Secure Federate Learning using SCONE and
Intel-OpenFL.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://youtu.be/J3tQcjrX3Jk" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://youtu.be/J3tQcjrX3Jk</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: Sept 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reina et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
G Anthony Reina, Alexey
Gruzdev, Patrick Foley, Olga
Perepelkina, Mansi Sharma, Igor
Davidyuk, Ilya Trushkin, Maksim
Radionov, Aleksandr Mokrov, Dmitry
Agapov, Jason Martin, Brandon Edwards,
Micah J. Sheller, Sarthak Pati,
Prakash Narayana Moorthy, Shih han Wang,
Prashant Shah, and Spyridon Bakas.
2021.

</span>
<span class="ltx_bibblock">OpenFL: An open-source framework for Federated
Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2105.06413 [cs.LG]

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scarlata et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Vinnie Scarlata, Simon
Johnson, James Beaney, and Piotr
Zmijewski. 2018.

</span>
<span class="ltx_bibblock">Supporting third party attestation for Intel SGX
with Intel data center attestation primitives.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">White paper</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh
et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jatinder Singh, Jennifer
Cobbe, Do Le Quoc, and Zahra
Tarkhani. 2020.

</span>
<span class="ltx_bibblock">Enclaves in the Clouds: Legal considerations and
broader implications.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Queue</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jatinder Singh, Jennifer
Cobbe, Do Le Quoc, and Zahra
Tarkhani. 2021.

</span>
<span class="ltx_bibblock">Enclaves in the Clouds.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 64,
5 (2021), 42–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team ([n.d.])</span>
<span class="ltx_bibblock">
Scontain Team.
[n.d.].

</span>
<span class="ltx_bibblock">CAS policy language v0.3.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sconedocs.github.io/CAS_session_lang_0_3/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sconedocs.github.io/CAS_session_lang_0_3/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: Sept 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Chia-Che Tsai, Donald E
Porter, and Mona Vij. 2017.

</span>
<span class="ltx_bibblock">Graphene-SGX: A Practical Library OS for
Unmodified Applications on SGX. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">USENIX
Annual Technical Conference (USENIXATC 17)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voigt and Von dem
Bussche (2017)</span>
<span class="ltx_bibblock">
Paul Voigt and Axel
Von dem Bussche. 2017.

</span>
<span class="ltx_bibblock">The eu general data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer
International Publishing</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Cong Xie, Oluwasanmi
Koyejo, and Indranil Gupta.
2020.

</span>
<span class="ltx_bibblock">Fall of empires: Breaking Byzantine-tolerant SGD by
inner product manipulation. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Uncertainty in
Artificial Intelligence</em>. PMLR.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.00979" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.00981" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.00981">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.00981" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.00982" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 14:28:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
