<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A comprehensive review and new taxonomy on superpixel segmentation</title>
<!--Generated on Fri Sep 27 22:50:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="superpixel,  image segmentation,  survey,  image processing" lang="en" name="keywords"/>
<base href="/html/2409.19179v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S1" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Taxonomy of superpixel methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.SS1" title="In 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Processing steps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.SS2" title="In 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Processing level of image features</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.SS3" title="In 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>The proposed taxonomy in superpixel literature</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS1" title="In 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Superpixel methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS2" title="In 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3" title="In 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation criteria</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS1" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Connectivity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS2" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Control over the number of superpixels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS3" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Boundary delineation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS4" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Color homogeneity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS5" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.5 </span>Compactness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS6" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.6 </span>Stability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS7" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.7 </span>Robustness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS8" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.8 </span>Runtime</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.SS3.SSS9" title="In 3.3. Evaluation criteria ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.9 </span>Visual quality</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="In 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Quantitative evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1.SSS1" title="In 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Object delineation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1.SSS2" title="In 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Color homogeneity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1.SSS3" title="In 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Compactness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1.SSS4" title="In 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Overall</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS2" title="In 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Runtime</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3" title="In 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Qualitative evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS1" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Path-based clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS2" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Neighborhood-based clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS3" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Dynamic center update clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS4" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Boundary evolution clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS5" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Hierarchical clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS6" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.6 </span>Deep-based clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS7" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.7 </span>Others</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3.SSS8" title="In 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.8 </span>Overall</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S5" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Superpixel segmentation methods</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Neighborhood-based clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS1" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>SLIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS2" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.2 </span>K-SLIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS3" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.3 </span>LSC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS4" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.4 </span>SCALP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS5" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.5 </span>TASP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS6" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.6 </span>MFGS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS7" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.7 </span>DSR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS8" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.8 </span>Semasuperpixel</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS1.SSS9" title="In A.1. Neighborhood-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.9 </span>AWkS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Boundary evolution clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS1" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>SEEDS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS2" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>CRS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS3" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.3 </span>ETPS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS4" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.4 </span>IBIS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS5" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.5 </span>CFBS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS6" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.6 </span>SCAC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS7" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.7 </span>LSC-Manhattan</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS2.SSS8" title="In A.2. Boundary evolution clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.8 </span>FLS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Dynamic-center-update clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS1" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.1 </span>SNIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS2" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.2 </span>FCSS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS3" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.3 </span>CONIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS4" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.4 </span>SCBP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS5" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.5 </span>A-DBSCAN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS6" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.6 </span>F-DBSCAN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS3.SSS7" title="In A.3. Dynamic-center-update clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.7 </span>DRW</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Path-based clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4.SSS1" title="In A.4. Path-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.1 </span>ERGC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4.SSS2" title="In A.4. Path-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.2 </span>ISF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4.SSS3" title="In A.4. Path-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.3 </span>RSS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4.SSS4" title="In A.4. Path-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.4 </span>DISF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4.SSS5" title="In A.4. Path-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.5 </span>ODISF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS4.SSS6" title="In A.4. Path-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.6 </span>SICLE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS5" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Hierarchical clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS5.SSS1" title="In A.5. Hierarchical clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.1 </span>SH</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS5.SSS2" title="In A.5. Hierarchical clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.2 </span>HMLI-SLIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS5.SSS3" title="In A.5. Hierarchical clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.3 </span>RISF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS5.SSS4" title="In A.5. Hierarchical clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.4 </span>UOIFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS5.SSS5" title="In A.5. Hierarchical clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.5 </span>DAL-HERS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS6" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Density-based clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS6.SSS1" title="In A.6. Density-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6.1 </span>PGDPC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS6.SSS2" title="In A.6. Density-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6.2 </span>DPS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS7" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7 </span>Sparse linear system clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS7.SSS1" title="In A.7. Sparse linear system clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7.1 </span>ANRW</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS7.SSS2" title="In A.7. Sparse linear system clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7.2 </span>GL<math alttext="l_{1/2}" class="ltx_Math" display="inline"><semantics><msub><mi>l</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝑙</ci><apply><divide></divide><cn type="integer">1</cn><cn type="integer">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex">l_{1/2}</annotation><annotation encoding="application/x-llamapun">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math>RSC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS7.SSS3" title="In A.7. Sparse linear system clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7.3 </span>SCSC</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS8" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.8 </span>Regional feature extraction, Polygonal decomposition, and Graph-based clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS8.SSS1" title="In A.8. Regional feature extraction, Polygonal decomposition, and Graph-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.8.1 </span>EAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS8.SSS2" title="In A.8. Regional feature extraction, Polygonal decomposition, and Graph-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.8.2 </span>ECCPD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS8.SSS3" title="In A.8. Regional feature extraction, Polygonal decomposition, and Graph-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.8.3 </span>ERS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS9" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.9 </span>Data distribution-based clustering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS9.SSS1" title="In A.9. Data distribution-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.9.1 </span>GMMSP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS9.SSS2" title="In A.9. Data distribution-based clustering ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.9.2 </span>gGMMSP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10" title="In Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10 </span>CNN-based methods</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS1" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.1 </span>SSN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS2" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.2 </span>E2E-SIS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS3" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.3 </span>BP-net</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS4" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.4 </span>DAFnet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS5" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.5 </span>SEN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS6" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.6 </span>LNSNet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS7" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.7 </span>ML-SGN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS8" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.8 </span>SSFCN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS9" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.9 </span>SENSS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS10" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.10 </span>AINET</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS11" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.11 </span>ss-RIM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS12" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.12 </span>EW-RIM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS13" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.13 </span>ML-RIM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1.SS10.SSS14" title="In A.10. CNN-based methods ‣ Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.10.14 </span>SIN</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Evaluation measures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3" title="In A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="In Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Number of superpixels and connectivity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="In Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Superpixels stability</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2.SSS1" title="In C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.1 </span>Object delineation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2.SSS2" title="In C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.2 </span>Color homogeneity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS3" title="In Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Robustness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS4" title="In Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Overall performance</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A comprehensive review and new taxonomy on superpixel segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Isabela Borlido Barcelos
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:isabela_borlido@hotmail.com">isabela_borlido@hotmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-7288-2485" title="ORCID identifier">0000-0001-7288-2485</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Pontifical Catholic University of Minas Gerais</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Belo Horizonte</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Minas Gerais</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">Brazil</span><span class="ltx_text ltx_affiliation_postcode" id="id5.5.id5">30535-901</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felipe de Castro Belém
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:felipe.belem@ic.unicamp.br">felipe.belem@ic.unicamp.br</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6037-5977" title="ORCID identifier">0000-0002-6037-5977</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">University of Campinas</span><span class="ltx_text ltx_affiliation_city" id="id7.2.id2">Campinas</span><span class="ltx_text ltx_affiliation_country" id="id8.3.id3">Brazil</span><span class="ltx_text ltx_affiliation_postcode" id="id9.4.id4">13083-970</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leonardo de Melo João
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:leonardo.joao@ic.unicamp.br">leonardo.joao@ic.unicamp.br</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4625-7840" title="ORCID identifier">0000-0003-4625-7840</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of Campinas</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Campinas</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">Brazil</span><span class="ltx_text ltx_affiliation_postcode" id="id13.4.id4">13083-970</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zenilton K. G. do Patrocínio Jr
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zenilton@pucminas.br">zenilton@pucminas.br</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-0804-1790" title="ORCID identifier">0000-0003-0804-1790</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id14.1.id1">Pontifical Catholic University of Minas Gerais</span><span class="ltx_text ltx_affiliation_city" id="id15.2.id2">Belo Horizonte</span><span class="ltx_text ltx_affiliation_state" id="id16.3.id3">Minas Gerais</span><span class="ltx_text ltx_affiliation_country" id="id17.4.id4">Brazil</span><span class="ltx_text ltx_affiliation_postcode" id="id18.5.id5">30535-901</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexandre Xavier Falcão
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:afalcao@ic.unicamp.br">afalcao@ic.unicamp.br</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-2914-5380" title="ORCID identifier">0000-0002-2914-5380</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">University of Campinas</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Campinas</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">Brazil</span><span class="ltx_text ltx_affiliation_postcode" id="id22.4.id4">13083-970</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Silvio Jamil Ferzoli Guimarães
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sjamil@pucminas.br">sjamil@pucminas.br</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-8522-2056" title="ORCID identifier">0000-0001-8522-2056</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id23.1.id1">Pontifical Catholic University of Minas Gerais</span><span class="ltx_text ltx_affiliation_city" id="id24.2.id2">Belo Horizonte</span><span class="ltx_text ltx_affiliation_state" id="id25.3.id3">Minas Gerais</span><span class="ltx_text ltx_affiliation_country" id="id26.4.id4">Brazil</span><span class="ltx_text ltx_affiliation_postcode" id="id27.5.id5">30535-901</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024; 9 February 2023; 20 Febuary 2024; 7 March 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id28.id1">Superpixel segmentation consists of partitioning images into regions composed of similar and connected pixels. Its methods have been widely used in many computer vision applications since it allows for reducing the workload, removing redundant information, and preserving regions with meaningful features. Due to the rapid progress in this area, the literature fails to catch up on more recent works among the compared ones and to categorize the methods according to all existing strategies. This work fills this gap by presenting a comprehensive review with new taxonomy for superpixel segmentation, in which methods are classified according to their processing steps and processing levels of image features. We revisit the recent and popular literature according to our taxonomy and evaluate 20 strategies based on nine criteria: connectivity, compactness, delineation, control over the number of superpixels, color homogeneity, robustness, running time, stability, and visual quality. Our experiments show the trends of each approach in pixel clustering and discuss individual trade-offs. Finally, we provide a new benchmark for superpixel assessment, available at https://github.com/IMScience-PPGINF-PucMinas/superpixel-benchmark.</p>
</div>
<div class="ltx_keywords">superpixel, image segmentation, survey, image processing
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3652509</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; ; </span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>CSUR</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>General and reference Surveys and overviews</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Image segmentation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Superpixel segmentation aims to divide images into homogeneous regions of connected pixels, such that unions of superpixels compose image objects. It has several benefits, such as reducing the workload (e.g., reducing millions of pixels to thousands/hundreds of superpixels) and providing higher-level content information than pixels. Consequently, methods for superpixel segmentation are used in several applications, such as object segmentation <cite class="ltx_cite ltx_citemacro_citep">(Borlido Barcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib19" title="">2021</a>; Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib69" title="">2020</a>; Sheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib105" title="">2018</a>)</cite>, anomaly detection <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib94" title="">2019</a>)</cite> semantic segmentation <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib146" title="">2018</a>)</cite>, saliency detection <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib144" title="">2019</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib147" title="">2019</a>)</cite>, and image classification <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib36" title="">2015</a>; Sellars et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib101" title="">2020</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="97" id="S1.F1.1.g1" src="x1.png" width="194"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="97" id="S1.F1.2.g1" src="x2.png" width="194"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="97" id="S1.F1.3.g1" src="x3.png" width="194"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Superpixel segmentation examples, in which superpixel borders are shown in red. Although boundary adherence, regularity, and compactness are essential properties, (a) superpixels with higher regularity and compactness have poor boundary adherence. Conversely, (b)
superpixel methods focused on boundary adherence may present irregular contours due to their sensitivity to subtle color variations.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Superpixel segmentation has a vast literature, and although previous work provided categorizations <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>; Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>; Kumar, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib62" title="">2023</a>)</cite> and benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib123" title="">2017</a>; Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>; Mathieu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib83" title="">2017</a>)</cite> to evaluate and compare methods, such works did not cover more recent approaches.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> presents three superpixel segmentation examples, in which the superpixels’ borders are shown in red.
In the literature, several authors identified the desired superpixel properties. Despite the absence of consensus, most authors agreed that superpixels must be composed of connected pixels, adhere to the objects’ borders, present smooth contours, and have regularly distributed and compact shapes <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib123" title="">2017</a>)</cite>. Moreover, the methods must be computationally efficient and generate a controllable number of superpixels. However, superpixel methods usually meet part of those criteria, which often occurs when the improvement in a property leads to worse for another property.
For instance, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>(a) has superpixels with maximum compacity and regularity, but their contours do not adhere to the object’s borders. Improving boundary adherence may negatively impact compactness (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>(c)). Some superpixel approaches try to manage this trade-off (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>(b)).
In this sense, the choice of an evaluation measure depends on the optimized property.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In contrast to the rapid progress in new superpixel strategies, the papers usually compared their proposals against classical approaches. Therefore, there are few comparisons among state-of-the-art methods, which impairs the judgment of their actual contribution.
Benchmarks usually fill this gap by offering an easy-to-use tool to compare different approaches.
The first benchmark for superpixel evaluation <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite> compared eight algorithms and evaluated object delineation and robustness to affine transformations. To overcome the biased penalty in <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Under-segmentation Error</span> (UE) measure <cite class="ltx_cite ltx_citemacro_citep">(Levinshtein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib65" title="">2009</a>)</cite> caused by the superpixel size, the authors proposed a modified UE to consider the smallest part of the superpixel leakage. Also, the evaluated superpixel methods presented similar results, demonstrating that the most appropriate methods for each task depend on the crucial characteristics of that task. In addition, algorithms less focused on compactness showed greater robustness to image transformations. Unlike Neubert and Protzel <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite>, Achanta et al. <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> demonstrated the effectiveness of <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">Simple Linear and Iterative Clustering</span> (SLIC) by comparing five superpixel methods to determine their benefits and limitations regarding their boundary adherence and efficiency. Achanta et al. <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> characterized the superpixel methods as <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">graph-based</span> and <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">gradient-ascent-based</span>. The former contains methods that model the segmentation problem based on graph theory generating superpixels by minimizing a cost function defined on the graph. The second iteratively refines its initial clusters until reaching a convergence criterion.
Although the categorization provided <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> is widely adopted in the literature on superpixels, it fails to cover recent strategies.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Schick et al. <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib99" title="">2012</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib100" title="">2014</a>)</cite> investigated the importance of compactness in superpixel segmentation. They proposed a compactness measure based on the isoperimetric coefficient <cite class="ltx_cite ltx_citemacro_citep">(Polya, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib91" title="">2020</a>)</cite> and demonstrated a trade-off between Compactness and Boundary Recall <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib82" title="">2004</a>)</cite>. The authors argued that a more accurate segmentation would not imply better overall performance. Thus, they claimed that compact superpixels better capture spatially coherent information facilitating information extraction from their boundaries.
In contrast, Stutz et al. <cite class="ltx_cite ltx_citemacro_citep">(Stutz, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib109" title="">2015</a>)</cite> explored the impact of depth information in superpixel methods in a benchmark with fifteen algorithms and two datasets.
According to their evaluation, depth inclusion may not represent improved results. Regarding visual quality, the authors settled that the high quantitative results in the delineation assessment did not necessarily reflect the segmentations’ visual quality. Mathieu et al. <cite class="ltx_cite ltx_citemacro_citep">(Mathieu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib83" title="">2017</a>)</cite> argued that more than two datasets, as used in <cite class="ltx_cite ltx_citemacro_citep">(Stutz, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib109" title="">2015</a>)</cite>, are needed for an exhaustive evaluation. They overcome this with a new dataset, called the <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Heterogeneous Size Image Dataset</span> (HSID). The HSID mainly contains large images (with millions of pixels) and allows evaluating the superpixel methods according to the image size. Using the HSID, the authors analyzed the five best superpixel methods in <cite class="ltx_cite ltx_citemacro_citep">(Stutz, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib109" title="">2015</a>)</cite> and Waterpixels <cite class="ltx_cite ltx_citemacro_citep">(Machairas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib78" title="">2015</a>)</cite> method. The evaluated methods did not achieve a satisfactory trade-off between adherence to contours, conciseness (smallest possible number of superpixels), and efficiency. Therefore, the authors argued that the superpixel method must be chosen according to the necessary superpixels’ characteristics for the desired task.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib123" title="">2017</a>)</cite> proposed a regularity measure for superpixels, allowing a quantitative regularity analysis. The authors also provided an overview of the superpixel methods and a benchmark with fifteen methods and thirteen evaluation measures, including the proposed one. In <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib123" title="">2017</a>)</cite>, the superpixel methods were categorized as clustering-based (or gradient-based) and graph-based, following the characterization in <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>. According to Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib123" title="">2017</a>)</cite>, methods based on clustering showed greater efficiency, while those based on graphs presented an improved delineation. However, the authors argued that the evaluated algorithms are hardly applicable in scenarios requiring real-time responses.
The authors in <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite> presented a more comprehensive evaluation in a benchmark with 28 superpixel algorithms with five datasets that included indoor, outdoor, and people images. In addition to the benchmark, the authors also proposed three evaluation measures independent of the number of superpixels and based on existing delineation metrics: <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">Average Miss Rate</span> (AMR), <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">Average Under-segmentation Error</span> (AUE), and <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">Average Unexplained Variation</span> (AUV). Stutz et al. <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite> evaluated the stability of superpixel methods, considering the minimum, maximum, and standard deviation of each metric; and its robustness to noise, blur, and affine transformations. Based on the categorization in <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, they also categorized superpixel methods by their high-level approach, allowing them to relate their categories to experimental results. Despite the broad categorization in <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, the authors settled that some methods in the literature are not included in their categorization. Based on the proposed evaluation, they created a ranking of the evaluated methods, in which they recommended six of them: ETPS <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>)</cite>, SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>)</cite>, ERS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib72" title="">2011</a>)</cite>, CRS <cite class="ltx_cite ltx_citemacro_citep">(Conrad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib29" title="">2013</a>)</cite>, ERGC <cite class="ltx_cite ltx_citemacro_citep">(Buyssens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib21" title="">2014</a>)</cite>, and SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Recently, the authors in <cite class="ltx_cite ltx_citemacro_citep">(Kumar, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib62" title="">2023</a>)</cite> extensively discussed various aspects of superpixel segmentation. They reviewed several classical superpixel methods and categorized them as <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">graph-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p6.1.2">clustering-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p6.1.3">watershed-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p6.1.4">energy optimization</span>, and <span class="ltx_text ltx_font_italic" id="S1.p6.1.5">wavelet-based</span> techniques. They also reviewed superpixel methods based on the classical approaches, extensively discussed them for general purposes and specific domains, and presented some commonly used datasets and evaluation measures. However, the work in <cite class="ltx_cite ltx_citemacro_citep">(Kumar, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib62" title="">2023</a>)</cite> did not perform an experimental evaluation. Although the desired attributes of superpixels were broadly discussed, which methods are more advantageous than others are still to be determined.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Other recent works discussed superpixel segmentation for specific applications, such as superpixels as pre-processing for clustering <cite class="ltx_cite ltx_citemacro_citep">(Sasmal and Dhal, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib98" title="">2023</a>)</cite> and superpixels in hyperspectral images <cite class="ltx_cite ltx_citemacro_citep">(Grewal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib47" title="">2023</a>)</cite>.
Clustering and superpixel methods categorizations were also provided in <cite class="ltx_cite ltx_citemacro_citep">(Sasmal and Dhal, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib98" title="">2023</a>)</cite>, where superpixel methods were categorized as <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">density-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">watershed-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p7.1.3">graph-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p7.1.4">path-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p7.1.5">contour evolution-based</span>, <span class="ltx_text ltx_font_italic" id="S1.p7.1.6">energy optimization-based</span>, and <span class="ltx_text ltx_font_italic" id="S1.p7.1.7">clustering-based</span> methods. In <cite class="ltx_cite ltx_citemacro_citep">(Sasmal and Dhal, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib98" title="">2023</a>)</cite>, the authors evaluated the efficacy of combining superpixels and partitional clustering approaches using SLIC as pre-processing in rosette plant images and oral histopathology images. Their results indicated that although the pre-processing based on superpixels could not improve accuracy, it reduced execution time and produced more compact, coherent, and regular image regions.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Despite the evaluations in previous benchmarks, superpixel approaches have made significant progress in recent years by introducing new strategies, making previous reviews and evaluations outdated. This work presents an overview of several superpixel segmentation strategies from both classic and recent literature. We also introduce a new benchmark that includes six superpixel methods recommended by <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite> and seventeen recent algorithms. Moreover, we provide a comprehensive assessment based on nine well-established criteria: <em class="ltx_emph ltx_font_italic" id="S1.p8.1.1">delineation</em>; <em class="ltx_emph ltx_font_italic" id="S1.p8.1.2">compactness</em>; <em class="ltx_emph ltx_font_italic" id="S1.p8.1.3">color homogeneity</em>; <em class="ltx_emph ltx_font_italic" id="S1.p8.1.4">running time</em>, <em class="ltx_emph ltx_font_italic" id="S1.p8.1.5">connectivity</em>; <em class="ltx_emph ltx_font_italic" id="S1.p8.1.6">control over the number of superpixels</em>; <em class="ltx_emph ltx_font_italic" id="S1.p8.1.7">robustness</em>; <em class="ltx_emph ltx_font_italic" id="S1.p8.1.8">stability</em>; and <em class="ltx_emph ltx_font_italic" id="S1.p8.1.9">visual quality</em>.
The results provide valuable insights into the pros and cons of the methods, supporting the choice of the most suitable one for a given application.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">This paper is organized as follows.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2" title="2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> describes the proposed taxonomy and categorizes the most recent and commonly used superpixel methods.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3" title="3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> presents the benchmark setup, including methods, datasets, and evaluation criteria.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4" title="4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> presents the obtained results in five datasets and <math alttext="23" class="ltx_Math" display="inline" id="S1.p9.1.m1.1"><semantics id="S1.p9.1.m1.1a"><mn id="S1.p9.1.m1.1.1" xref="S1.p9.1.m1.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S1.p9.1.m1.1b"><cn id="S1.p9.1.m1.1.1.cmml" type="integer" xref="S1.p9.1.m1.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.1.m1.1c">23</annotation><annotation encoding="application/x-llamapun" id="S1.p9.1.m1.1d">23</annotation></semantics></math> superpixel methods. Finally, we draw conclusions and state future work in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S5" title="5. Conclusions ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>.
In addition, we provide supplementary material with three appendices. The reader should refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1" title="Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">A</span></a> for an extensive description covering several superpixel methods. In terms of evaluation, quantitative benchmark measures are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2" title="Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">B</span></a>. Furthermore, Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3" title="Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C</span></a> provides additional results with experiments evaluating connectivity, stability, and robustness, along with a review of the overall performance concerning the clustering categories.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Taxonomy of superpixel methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Most articles categorize superpixel methods into clustering-based, graph-based, and, more recently, deep-learning proposals. Only a few recent works <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>; Kumar, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib62" title="">2023</a>)</cite> present more categories for such methods. However, while the categories in <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite> cannot represent some recent superpixel methods, the authors in <cite class="ltx_cite ltx_citemacro_citep">(Kumar, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib62" title="">2023</a>)</cite> focused mainly on classical approaches. A taxonomy based on different and non-strict aspects may be more appropriate since previous categorizations do not cover the wide variety of superpixel approaches, and the rapid advance in this area hampers the establishment of disjoint categories. Therefore, this work provides a taxonomy that categorizes methods according to their processing steps and the abstraction level of the features used. In addition, it also reports the desired superpixel properties that each method satisfies.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Processing steps</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">To provide a comprehensive taxonomy with a more natural representation, we identified that superpixel algorithms generally have up to three steps: (i) initial; (ii) main; and (iii) final processing. We identify categories that broadly define the process performed at each processing step in <math alttext="59" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">59</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><cn id="S2.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1">59</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">59</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">59</annotation></semantics></math> superpixel segmentation methods. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.F2" title="Figure 2 ‣ 2.1. Processing steps ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the categories for each processing step. For instance, in <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">Initial Processing</span>, superpixel methods usually perform image pre-processing, such as denoising or feature extraction, or the methods compute the initial algorithm setup, such as creating seeds or performing an initial segmentation.
<span class="ltx_text" id="S2.SS1.p1.1.2" style="color:#000000;"> On the other hand, the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2.1">Main Processing</span> step contains the strategy for superpixel computation, including the whole loop for superpixel generation, if any. As one may see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.F2" title="Figure 2 ‣ 2.1. Processing steps ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, some main processing categories have deep networks, which we categorize based on the pixel-superpixel assignment process and the network’s output.</span> After computing superpixels, post-processing operations (the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">Final Processing</span>) may ensure superpixel connectivity, fine-tune the segmentation, or complete the pixel-superpixel map computation.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="S2.F2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span><span class="ltx_text" id="S2.F2.2.1" style="color:#000000;"> Categories of each processing step in superpixel taxonomy.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The processing steps in our taxonomy divide superpixel approaches into specialized procedures, from which one may identify categories. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.T1" title="Table 1 ‣ 2.2. Processing level of image features ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> presents the categories of the <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">Main Processing</span> step whose clustering procedure does not use convolutional networks. Our taxonomy introduces some new categories and also reviews others. Instead of the common <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">clustering-based</span> (also called gradient-based), our taxonomy contains the <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.3">neighborhood-based</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.4">dynamic-center-update</span> clustering categories. The former performs clustering restricted to a maximum spatial distance from some reference point in the image, while the latter dynamically updates the cluster centers based on an optimization function. Furthermore, the <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.5">graph-based</span> category here relates to using graph topology instead of graph modeling. Also, similar to Stutz et al. <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, our taxonomy includes <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.6">boundary evolution</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.7">path-based</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.8">density-based</span> clustering categories. Finally, we introduce the categories <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.9">sparse linear system</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.10">data distribution-based</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.11">regional feature extraction</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.12">polygonal decomposition</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.13">hierarchical</span> clustering. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.T1" title="Table 1 ‣ 2.2. Processing level of image features ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> shows their definitions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Processing level of image features</h3>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Main processing categories excluding those based on neural networks.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1" style="font-size:80%;">Clustering categories</span></th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1.1.1" style="font-size:80%;">Explanation</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.2.1.1.1">
<tr class="ltx_tr" id="S2.T1.1.2.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.2.1.1.1.1.1"><span class="ltx_text" id="S2.T1.1.2.1.1.1.1.1.1" style="font-size:80%;">Neighborhood-based</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.2.1.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.1"><span class="ltx_text" id="S2.T1.1.2.1.2.1.1.1" style="font-size:80%;">Performs clustering based on the similarity between pixels restricted to a maximum spatial distance from some reference point in the image.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.3.2.1.1">
<tr class="ltx_tr" id="S2.T1.1.3.2.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.3.2.1.1.1.1"><span class="ltx_text" id="S2.T1.1.3.2.1.1.1.1.1" style="font-size:80%;">Boundary evolution</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.3.2.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.2.1">
<span class="ltx_p" id="S2.T1.1.3.2.2.1.1"><span class="ltx_text" id="S2.T1.1.3.2.2.1.1.1" style="font-size:80%;">These algorithms iteratively update the superpixels’ boundaries to optimize an energy function, usually using a coarse-to-fine image block strategy.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.4.3.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.4.3.1.1">
<tr class="ltx_tr" id="S2.T1.1.4.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.4.3.1.1.1.1"><span class="ltx_text" id="S2.T1.1.4.3.1.1.1.1.1" style="font-size:80%;">Dynamic-center-update</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.4.3.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_p" id="S2.T1.1.4.3.2.1.1"><span class="ltx_text" id="S2.T1.1.4.3.2.1.1.1" style="font-size:80%;">The dynamic-center-update algorithms perform clustering with a distance function based on the features of the clusters, dynamically updating their centers.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.5.4.1"><span class="ltx_text" id="S2.T1.1.5.4.1.1" style="font-size:80%;">Path-based</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.5.4.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.2.1">
<span class="ltx_p" id="S2.T1.1.5.4.2.1.1"><span class="ltx_text" id="S2.T1.1.5.4.2.1.1.1" style="font-size:80%;">Path-based approaches generate superpixels by creating paths in the image graph based on some criteria. Usually, its clustering criterion is a path-based function to optimize during clustering.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.6.5.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.6.5.1.1">
<tr class="ltx_tr" id="S2.T1.1.6.5.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.6.5.1.1.1.1"><span class="ltx_text" id="S2.T1.1.6.5.1.1.1.1.1" style="font-size:80%;">Hierarchical</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.6.5.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.2.1">
<span class="ltx_p" id="S2.T1.1.6.5.2.1.1"><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.1" style="font-size:80%;">These algorithms create regions in the image that form a hierarchical structure, obeying the criteria of locality and causality </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.2.1" style="font-size:80%;">(</span>Guigues et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib51" title="">2006</a><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.4.3" style="font-size:80%;">)</span></cite><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.5" style="font-size:80%;">.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.7.6.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.7.6.1.1">
<tr class="ltx_tr" id="S2.T1.1.7.6.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.7.6.1.1.1.1"><span class="ltx_text" id="S2.T1.1.7.6.1.1.1.1.1" style="font-size:80%;">Density-based</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.7.6.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.2.1">
<span class="ltx_p" id="S2.T1.1.7.6.2.1.1"><span class="ltx_text" id="S2.T1.1.7.6.2.1.1.1" style="font-size:80%;">These superpixel methods model the problem of computing superpixels in a problem of finding density peaks.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.8.7.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.8.7.1.1">
<tr class="ltx_tr" id="S2.T1.1.8.7.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.8.7.1.1.1.1"><span class="ltx_text" id="S2.T1.1.8.7.1.1.1.1.1" style="font-size:80%;">Sparse linear system</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.8.7.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.2.1">
<span class="ltx_p" id="S2.T1.1.8.7.2.1.1"><span class="ltx_text" id="S2.T1.1.8.7.2.1.1.1" style="font-size:80%;">Model the segmentation problem with a sparse matrix and use its properties to find superpixels.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.9.8.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.9.8.1.1">
<tr class="ltx_tr" id="S2.T1.1.9.8.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.9.8.1.1.1.1"><span class="ltx_text" id="S2.T1.1.9.8.1.1.1.1.1" style="font-size:80%;">Data distribution-based</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.9.8.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.2.1">
<span class="ltx_p" id="S2.T1.1.9.8.2.1.1"><span class="ltx_text" id="S2.T1.1.9.8.2.1.1.1" style="font-size:80%;">The approach assumes that the image pixels follow a specific distribution and perform the clustering based on this conjecture.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.10.9.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.10.9.1.1">
<tr class="ltx_tr" id="S2.T1.1.10.9.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.10.9.1.1.1.1"><span class="ltx_text" id="S2.T1.1.10.9.1.1.1.1.1" style="font-size:80%;">Regional feature extraction</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.10.9.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.10.9.2.1">
<span class="ltx_p" id="S2.T1.1.10.9.2.1.1"><span class="ltx_text" id="S2.T1.1.10.9.2.1.1.1" style="font-size:80%;">Iteratively extracts regional features to perform clustering based on these features.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.11.10.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.11.10.1.1">
<tr class="ltx_tr" id="S2.T1.1.11.10.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.11.10.1.1.1.1"><span class="ltx_text" id="S2.T1.1.11.10.1.1.1.1.1" style="font-size:80%;">Polygonal decomposition</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S2.T1.1.11.10.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.11.10.2.1">
<span class="ltx_p" id="S2.T1.1.11.10.2.1.1"><span class="ltx_text" id="S2.T1.1.11.10.2.1.1.1" style="font-size:80%;">The segmentation in these methods consists of decomposing the image into non-overlapping polygons.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S2.T1.1.12.11.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.12.11.1.1">
<tr class="ltx_tr" id="S2.T1.1.12.11.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S2.T1.1.12.11.1.1.1.1"><span class="ltx_text" id="S2.T1.1.12.11.1.1.1.1.1" style="font-size:80%;">Graph-based</span></td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S2.T1.1.12.11.2" style="width:264.6pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.11.2.1">
<span class="ltx_p" id="S2.T1.1.12.11.2.1.1"><span class="ltx_text" id="S2.T1.1.12.11.2.1.1.1" style="font-size:80%;">Perform superpixel segmentation based on graph topology.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Superpixel methods can either compute features on the fly or obtain them from other algorithms. Additionally, several approaches combine the same information differently to extract features. For instance, some methods combine local features (<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">e.g.</span>, color and pixel position) with higher-level ones (<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">e.g.</span>, edge or semantic information) in their optimization function <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib143" title="">2021a</a>; Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib126" title="">2021b</a>)</cite>. Conversely, other extracted features by only exploring local information — <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">e.g.</span>, using strategies based on graph theory or linear algebra <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib24" title="">2017a</a>; Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>; Galvão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib39" title="">2020</a>)</cite>. However, as far as we know, there was no study on the features’ impact on superpixel generation. Although such a study is beyond the scope of this work, we categorize superpixel methods based on the processing level of the features used. Since superpixel methods usually combine higher-level features with lower-level ones, we categorize them according to the highest-level features. The categories are defined as follows:</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Pixel-level features:</span> raw data resources in images — <span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.2">e.g.</span>, pixel color, position, and depth;</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Mid-level features:</span> features that can be computed based on a set of pixels, smaller than the entire image — <span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.2">e.g.</span>, patch-based feature, path-based feature, gradient, or boundary;</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">High-level features:</span> features that combine pixel properties and high-level information. The high-level information cannot be extracted from a small set of pixels. They are given directly by the user or predicted by other models — <span class="ltx_text ltx_font_italic" id="S2.I1.i3.p1.1.2">e.g.</span>, saliency map, semantic features, texture, or a desired object geometry.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>The proposed taxonomy in superpixel literature</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="574" id="S2.F3.g1" src="x5.png" width="573"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span><span class="ltx_text" id="S2.F3.2.1" style="color:#000000;"> The main processing categories in superpixel taxonomy and the methods that conform with each one.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">This section presents our taxonomy applied to superpixel literature, in which we categorize the processing steps of <span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.1">59 superpixel methods</span>. In the following, we discuss the main processing categories and the usage of deep learning in superpixel segmentation. Then, we present the complete taxonomy applied to the superpixel methods.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.F3" title="Figure 3 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the superpixel methods according to their main processing categories. The <span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">neighborhood-based</span> methods usually require an initial seed sampling, in which seeds represent superpixel centers, and a final merging step ensures connectivity since their neighborhood distance usually allows superpixels to conquer non-connected pixels <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib24" title="">2017a</a>; Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib130" title="">2021b</a>; Liu and Duan, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib70" title="">2020</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib143" title="">2021a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib126" title="">2021b</a>; Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib52" title="">2021</a>)</cite>. Also, most neighborhood-based methods manage compactness by parameter. In contrast, methods with <span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.2">boundary evolution-based</span> clustering require an initial segmentation, but they usually guarantee connectivity since only pixels at superpixels’ borders can conquer neighbor pixels <cite class="ltx_cite ltx_citemacro_citep">(Bobbia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib18" title="">2021</a>; Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>; Conrad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib29" title="">2013</a>; Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib132" title="">2020</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib140" title="">2021b</a>; Li and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib68" title="">2015</a>; Qiao and Di, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib92" title="">2022</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib89" title="">2022</a>)</cite>. Their initial grid segmentation and the restricted pixel-conquering strategy allow the creation of highly compact and regular superpixels, while the iterative coarse-to-fine block strategy improves delineation. Boundary evolution-based methods are usually more efficient than other approaches, although they usually do not produce the precise number of superpixels.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">In <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">dynamic-center-update</span> algorithms, the optimization function usually relies on the superpixel centers’ features, dynamically updating them to improve these features <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>; Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib44" title="">2021</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib58" title="">2020</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib67" title="">2021</a>; Loke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib75" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib145" title="">2021b</a>; Wang and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib124" title="">2021</a>)</cite>. Most of these methods avoid performing several iterations, updating each pixel once with a priority queue. They usually have good boundary adherence but less compactness than neighborhood-based and boundary evolution-based clustering methods. In contrast, superpixel methods with <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.2">path-based</span> clustering are usually focused on delineation rather than compactness <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>; Chai, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib22" title="">2020</a>; Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib14" title="">2022b</a>)</cite>. Similar to neighborhood-based methods, they require an initial seed sampling, but instead of superpixel centers, the seeds are the roots of the trees. In path-based strategies, superpixels are usually trees that start with a unique seed and iteratively conquer pixels according to the graph’s adjacency. Such a clustering procedure allows the development of optimization functions based on the paths (tree branches) instead of a global function, and the pixel conquering based on the graph’s adjacency guarantees connectivity.
Methods with <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.3">hierarchical</span> clustering create a hierarchical structure by iteratively merging pixels or dividing image regions <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib128" title="">2018</a>; Bejar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib10" title="">2020</a>; Di et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib31" title="">2021</a>; Galvão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib39" title="">2020</a>; Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib90" title="">2022</a>)</cite>. Instead of computing only a pre-determined number of superpixels, most of these methods extract different superpixel quantities, named scales, from the hierarchical structure. However, to improve running time and delineation, the hierarchical structure can have dense and sparse scales and, therefore, they may not produce any superpixel quantity. Similar to path-based clustering methods, the hierarchical ones usually focus on boundary adherence, and their strategy to cluster guarantees connected superpixels. Although they require a unique execution to produce all scales, the superpixel leakage at one hierarchical scale is propagated to the following ones, increasing delineation error.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">Density-based</span> methods model the problem of finding superpixels in the problem of finding density peak pixels <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib48" title="">2021</a>; Shah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib103" title="">2021</a>)</cite>. Similar to path-based and hierarchical-based methods, the density-based ones also focus on delineation, but they may not guarantee connectivity. Also, unlike most neighborhood-based and boundary evolution-based methods, density-based methods usually use non-iterative approaches and they assume that the image pixel features form peaks of density (groups of similar pixels) along the image dimension, considering them density peaks as candidates for superpixel centers. Similarly, <span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.2">data distribution-based</span> approaches assume that features in image pixels follow a specific distribution. In this work, only GMMSP <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite> performs such a strategy and considers that the image pixels follow a Gaussian distribution. GMMSP does not allow direct control over the number of superpixels and does not produce highly compact superpixels. However, its superpixels have smooth borders and low variation in size. In contrast, <span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.3">sparse linear system clustering</span> methods model pixel similarities with a sparse matrix, using algorithms based on linear algebra to solve the segmentation problem <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib119" title="">2020b</a>; Francis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib37" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib66" title="">2020</a>)</cite>. These methods usually have a higher time complexity, prioritize delineation over homogeneity, and may not guarantee connectivity.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.1">Regional feature extraction clustering</span> methods iteratively extract features from image regions and use these features to perform clustering. EAM <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib5" title="">2020</a>)</cite>, the unique superpixel method in this work with this clustering approach, performs an iterative coarse-to-fine grid segmentation based on attributes extracted from image regions. Although such a procedure is similar to boundary evolution clustering, EAM does not improve its superpixels during the iterative process. Instead, it performs a further merging stage to compose superpixels. Unlike boundary evolution clustering methods, EAM does not generate compact or regular superpixels. However, it can capture finer details by producing fewer superpixels in homogeneous regions. On the other hand, <span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.2">polygonal decomposition clustering</span> methods decompose the image into non-overlapping polygons as superpixels. In this work, only ECCPD <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib77" title="">2020</a>)</cite> uses this clustering strategy. The ECCPD has highly compact and connected superpixels compared to other clustering methods. However, it requires minutes to segment an image. The <span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.3">graph-based</span> clustering performs superpixel segmentation based on graph topology. In this work, only ERS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib72" title="">2011</a>)</cite> uses this clustering strategy. In contrast to ECCPD, ERS uses an efficient greedy algorithm to solve the problem of selecting a set of edges to find a predetermined number of connected components in a graph. ERS has a balancing term to control compactness, and the graph’s adjacency guarantees connectivity. Also, ERS can generate the exact number of desired superpixels.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S2.F4.g1" src="x6.png" width="660"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span class="ltx_text" id="S2.F4.2.1" style="color:#000000;"> The usage of neural networks in superpixel segmentation. The color of each method relates to its main processing category color in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.F3" title="Figure 3 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1">The remaining clustering categories relate to <span class="ltx_text ltx_font_bold" id="S2.SS3.p6.1.1">deep-learning networks</span>. The neural networks used for superpixels are typically deep convolutional, and they are used in the main or initial processing. Although deep learning is a popular topic in computer vision, its use for superpixel segmentation is relatively new. This delay is due to two major challenges: (i) propose differentiable operations for the pixel-superpixel association and (ii) fit the irregular superpixel lattices into regular convolutional ones. As a result, most deep-learning networks do not produce superpixels directly. Instead, they usually employ a differential clustering module in an end-to-end trainable network. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.F4" title="Figure 4 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>, superpixel segmentation methods may use deep-learning networks to (i) extract features for a non-differential clustering module, (ii) compute pixel-superpixel soft association using a differential clustering module, (iii) compute pixel-superpixel soft association directly, or (iv) compute superpixels directly. The deep networks in (i) perform an initial processing for a further clustering step.
Conversely, in (ii), the network’s training procedure usually integrates a differential clustering module, and, in this case, we consider that the network performs clustering. In this work, only FLS <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib89" title="">2022</a>)</cite> adopts a differential clustering module without integrating it into the network’s training process. Finally, the deep networks in (iii) and (iv) also perform clustering and, therefore, are part of the main processing step.</p>
</div>
<div class="ltx_para" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1">The SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite> overcomes these issues with a supervised fully convolutional network to extract image features and a differentiable clustering module based on SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> to produce a pixel-superpixel soft association. As far as we know, the proposal in <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite> was the first end-to-end trainable network for superpixel segmentation and inspired others. For instance, E2E-SIS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib121" title="">2020a</a>)</cite>, BP-net <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib142" title="">2021c</a>)</cite>, and DAFnet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib131" title="">2021a</a>)</cite> are supervised deep-based superpixel approaches that also use a differentiable clustering module based on SLIC. E2E-SIS performs multi-task learning that exploits the mutual benefit between image segmentation and superpixel segmentation. In contrast, BP-net and DAFnet generate superpixels for RGB-D and stereo images, respectively. Other approaches employ new clustering modules, such as SEN <cite class="ltx_cite ltx_citemacro_citep">(Gaur and Manjunath, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib40" title="">2020</a>)</cite> with a differential mean-shift module and LNSNet <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib148" title="">2021</a>)</cite> with a Non-iterative Clustering Module. Both are unsupervised networks, in which the former uses superpixels generated from SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite> as pseudo-ground-truth, and the latter adopts a lifelong learning strategy. Similarly, SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite> and ML-SGN <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib71" title="">2022</a>)</cite> use a U-shaped network, in which the former employs a supervised strategy that directly outputs a pixel-superpixel association map, and the latter uses an unsupervised strategy with a differential clustering based on SLIC to train a multitasking network. Inspired by SSFCN, SENSS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib120" title="">2022</a>)</cite>, and AINET <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib127" title="">2021a</a>)</cite> are also supervised U-shaped networks, in which the former improves learning ability with <span class="ltx_text ltx_font_italic" id="S2.SS3.p7.1.1">Squeeze-and-Excitation</span> blocks, and the latter employs a boundary-perceiving loss to improve boundary delineation and an <span class="ltx_text ltx_font_italic" id="S2.SS3.p7.1.2">Association Implantation</span> module to associate each pixel with its surrounding superpixels in a grid shape.
Conversely, some deep-learning methods integrate the soft pixel-superpixel assignment into the convolutional process. For instance, ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>, EW-RIM <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite>, and ML-RIM <cite class="ltx_cite ltx_citemacro_citep">(Eliasof et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib33" title="">2022</a>)</cite> use the deep image prior procedure <cite class="ltx_cite ltx_citemacro_citep">(Lempitsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib64" title="">2018</a>)</cite> to generate superpixels without image ground truth. Instead, they are trained based on clustering entropy, spatial smoothness, and reconstruction.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Recent methods for superpixel segmentation.</figcaption>
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="S2.T2.74" style="width:433.6pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.62.62.62" style="width:372.9pt;height:565.3pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-553.9pt,839.3pt) scale(0.251841261434202,0.251841261434202) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.62.62.62.62">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.63.1">
<td class="ltx_td" id="S2.T2.62.62.62.62.63.1.1"></td>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.2"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.3"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.4"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.5"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.6"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.7"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.8"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.9"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.10" rowspan="2"><span class="ltx_text" id="S2.T2.62.62.62.62.63.1.10.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.63.1.10.1.1">
<span class="ltx_tr" id="S2.T2.62.62.62.62.63.1.10.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.63.1.10.1.1.1.1">Time</span></span>
<span class="ltx_tr" id="S2.T2.62.62.62.62.63.1.10.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.63.1.10.1.1.2.1">complexity</span></span>
</span></span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.11"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.12"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.13"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.14"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.15"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S2.T2.62.62.62.62.63.1.16"><span class="ltx_text" id="S2.T2.62.62.62.62.63.1.16.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.63.1.16.1.1">
<span class="ltx_tr" id="S2.T2.62.62.62.62.63.1.16.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.63.1.16.1.1.1.1">Features</span></span>
</span></span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.63.1.17"></th>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.63.1.18"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.64.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.1.1">Method</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.3">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.3.1" style="width:6.8pt;height:37pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.0pt;transform:translate(-15.07pt,-15.07pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.3.1.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.3.1.1.1">Iterative</span></p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.4">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.4.1" style="width:8.9pt;height:27pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.0pt;transform:translate(-9.04pt,-8.07pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.4.1.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.4.1.1.1">#Iter.</span></p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.5">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.5.1" style="width:8.9pt;height:42pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.0pt;transform:translate(-16.54pt,-15.57pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.5.1.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.5.1.1.1">#Superp.</span></p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.6">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.6.1" style="width:6.8pt;height:35pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:35.0pt;transform:translate(-14.08pt,-14.08pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.6.1.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.6.1.1.1">Connec.</span></p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.7">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.7.1" style="width:8.8pt;height:42.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-16.72pt,-15.75pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.7.1.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.7.1.1.1">Compact.</span></p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.8" rowspan="2">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.8.1" style="width:8.8pt;height:33.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:33.4pt;transform:translate(-12.29pt,-11.32pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.8.1.1"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.8.1.1.1">Superv.</span></p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.9"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.9.1">Color</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.10"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.11"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.11.1">Initial processing</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.12"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.12.1">Main processing</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.13"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.13.1">Final processing</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.14"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.15">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.15.1" style="width:6.8pt;height:17.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.4pt,-5.4pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.15.1.1">Pix.</p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.16">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.16.1" style="width:6.9pt;height:20.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.3pt;transform:translate(-6.67pt,-6.67pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.16.1.1">Mid.</p>
</span></div>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.17">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.62.62.62.62.64.2.17.1" style="width:8.9pt;height:23.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:23.6pt;transform:translate(-7.36pt,-6.39pt) rotate(-90deg) ;">
<p class="ltx_p" id="S2.T2.62.62.62.62.64.2.17.1.1">High.</p>
</span></div>
</th>
<th class="ltx_td ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.18"></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T2.62.62.62.62.64.2.19"><span class="ltx_text" id="S2.T2.62.62.62.62.64.2.19.1">Inspired</span></th>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.1.1.1" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.2"><span class="ltx_text" id="S2.T2.1.1.1.1.1.2.1" style="background-color:#F0F0F0;">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite></span></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.4"><span class="ltx_text" id="S2.T2.1.1.1.1.1.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.5"><span class="ltx_text" id="S2.T2.1.1.1.1.1.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.6"><span class="ltx_text" id="S2.T2.1.1.1.1.1.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.1"><span class="ltx_text" id="S2.T2.1.1.1.1.1.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.1.1.1.1.1.1.1.1.1" style="background-color:#F0F0F0;">a</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.7"><span class="ltx_text" id="S2.T2.1.1.1.1.1.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.8"><span class="ltx_text" id="S2.T2.1.1.1.1.1.8.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.9"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.11">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.1.1.1.11.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.1.1.1.1.1.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.1.1.1.11.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.1.1.1.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.1.1.1.1.1.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.1.1.1.12.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.1.1.1.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.1.1.1.1.1.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.1.1.1.13.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.14"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.1.1.15"><span class="ltx_text" id="S2.T2.1.1.1.1.1.15.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.16"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.17"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.1.1.1.18"></td>
<td class="ltx_td ltx_nopad_r ltx_border_t" id="S2.T2.1.1.1.1.1.19"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.65.3">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.1">K-SLIC <cite class="ltx_cite ltx_citemacro_citep">(Ullah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib114" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.5">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.7">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.9">RGB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.65.3.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.65.3.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.65.3.12.1.1.1">Compute optimum K</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.65.3.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.65.3.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.65.3.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.65.3.16">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.17"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.65.3.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.65.3.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.4.4.4.4" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.4"><span class="ltx_text" id="S2.T2.4.4.4.4.4.4.1" style="background-color:#F0F0F0;">LSC <cite class="ltx_cite ltx_citemacro_citep">(Li and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib68" title="">2015</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.6"><span class="ltx_text" id="S2.T2.4.4.4.4.4.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.7"><span class="ltx_text" id="S2.T2.4.4.4.4.4.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.8"><span class="ltx_text" id="S2.T2.4.4.4.4.4.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.2.2.1"><span class="ltx_text" id="S2.T2.2.2.2.2.2.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.2.2.2.2.2.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.2.2.2.2.2.1.1.1.1" style="background-color:#F0F0F0;">a</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.9"><span class="ltx_text" id="S2.T2.4.4.4.4.4.9.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.11"><span class="ltx_text" id="S2.T2.4.4.4.4.4.11.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.3">
<math alttext="O(kn+nz)" class="ltx_Math" display="inline" id="S2.T2.3.3.3.3.3.2.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.3.3.3.3.3.2.m1.1a"><mrow id="S2.T2.3.3.3.3.3.2.m1.1.1" xref="S2.T2.3.3.3.3.3.2.m1.1.1.cmml"><mi id="S2.T2.3.3.3.3.3.2.m1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.3.3.3.3.3.2.m1.1.1.3.cmml">O</mi><mo id="S2.T2.3.3.3.3.3.2.m1.1.1.2" xref="S2.T2.3.3.3.3.3.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.cmml"><mo id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.cmml"><mrow id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.cmml"><mi id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.2.cmml">k</mi><mo id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.1" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.3" mathbackground="#F0F0F0" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.3.cmml">n</mi></mrow><mo id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.1" mathbackground="#F0F0F0" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.2" mathbackground="#F0F0F0" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.2.cmml">n</mi><mo id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.1" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.3" mathbackground="#F0F0F0" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.3.cmml">z</mi></mrow></mrow><mo id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.3" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.3.3.2.m1.1b"><apply id="S2.T2.3.3.3.3.3.2.m1.1.1.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1"><times id="S2.T2.3.3.3.3.3.2.m1.1.1.2.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.2"></times><ci id="S2.T2.3.3.3.3.3.2.m1.1.1.3.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.3">𝑂</ci><apply id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1"><plus id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.1.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.1"></plus><apply id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2"><times id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.1.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.1"></times><ci id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.2.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.2">𝑘</ci><ci id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.3.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3"><times id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.1"></times><ci id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.2">𝑛</ci><ci id="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.3.cmml" xref="S2.T2.3.3.3.3.3.2.m1.1.1.1.1.1.3.3">𝑧</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.3.3.2.m1.1c">O(kn+nz)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.3.3.3.3.2.m1.1d">italic_O ( italic_k italic_n + italic_n italic_z )</annotation></semantics></math><span class="ltx_text" id="S2.T2.4.4.4.4.4.3.1" style="background-color:#F0F0F0;"> <sup class="ltx_sup" id="S2.T2.4.4.4.4.4.3.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.4.4.4.4.4.3.1.1.1" style="background-color:#F0F0F0;">b</span></sup></span>
</td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.4.4.4.4.4.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.4.4.4.4.4.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.4.4.4.4.4.13.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.4.4.4.4.4.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.4.4.4.4.4.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.4.4.4.4.4.14.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.15">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.4.4.4.4.4.15.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.4.4.4.4.4.15.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.4.4.4.4.4.15.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4.4.17"><span class="ltx_text" id="S2.T2.4.4.4.4.4.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.18"></td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.19"></td>
<td class="ltx_td" id="S2.T2.4.4.4.4.4.20"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.4.4.4.4.4.21"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.66.4">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.1">SCALP <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.7">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.66.4.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.66.4.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.66.4.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.66.4.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.66.4.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.66.4.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.66.4.17">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.66.4.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.66.4.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.67.5" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.1"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.1.1" style="background-color:#F0F0F0;">TASP <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib130" title="">2021b</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.3"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.3.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.4"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.5"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.6"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.7"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.9"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.67.5.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.67.5.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.67.5.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.67.5.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.67.5.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.67.5.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.67.5.17"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.67.5.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.67.5.20"><span class="ltx_text" id="S2.T2.62.62.62.62.67.5.20.1" style="background-color:#F0F0F0;">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.5.5.5.5.5">
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.2">MFGS <cite class="ltx_cite ltx_citemacro_citep">(Liu and Duan, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib70" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.3"></td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.4"></td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.1">✓ <sup class="ltx_sup" id="S2.T2.5.5.5.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.5.5.5.5.5.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.7">✓</td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.10"></td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.5.5.5.5.5.12.1">
<tr class="ltx_tr" id="S2.T2.5.5.5.5.5.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.5.5.5.5.5.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.5.5.5.5.5.13.1">
<tr class="ltx_tr" id="S2.T2.5.5.5.5.5.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.5.5.5.5.5.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.5.5.5.5.5.14.1">
<tr class="ltx_tr" id="S2.T2.5.5.5.5.5.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.5.5.5.5.5.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.15"></td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.5.5.17">✓</td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.18"></td>
<td class="ltx_td" id="S2.T2.5.5.5.5.5.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.5.5.5.5.5.20">SLICO <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.6.6.6.6.6" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.2"><span class="ltx_text" id="S2.T2.6.6.6.6.6.2.1" style="background-color:#F0F0F0;">DSR <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib143" title="">2021a</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.4"><span class="ltx_text" id="S2.T2.6.6.6.6.6.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.1"><span class="ltx_text" id="S2.T2.6.6.6.6.6.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.6.6.6.6.6.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.6.6.6.6.6.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.7"><span class="ltx_text" id="S2.T2.6.6.6.6.6.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.9"><span class="ltx_text" id="S2.T2.6.6.6.6.6.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.10"></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.6.6.6.6.6.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.6.6.6.6.6.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.6.6.6.6.6.12.1.1.1">Saliency computation</td>
</tr>
<tr class="ltx_tr" id="S2.T2.6.6.6.6.6.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.6.6.6.6.6.12.1.2.1">and Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.6.6.6.6.6.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.6.6.6.6.6.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.6.6.6.6.6.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.6.6.6.6.6.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.6.6.6.6.6.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.6.6.6.6.6.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.15"></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.16"></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.6.6.18"><span class="ltx_text" id="S2.T2.6.6.6.6.6.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.6.6.6.6.6.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.6.6.6.6.6.20"><span class="ltx_text" id="S2.T2.6.6.6.6.6.20.1" style="background-color:#F0F0F0;">dSLIC <cite class="ltx_cite ltx_citemacro_citep">(Maierhofer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib79" title="">2018</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.7.7.7.7.7">
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.2">Semasuperpixel <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib126" title="">2021b</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.1">✓<sup class="ltx_sup" id="S2.T2.7.7.7.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.7.7.7.7.7.1.1.1">a</span></sup>
</td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.10"></td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.7.7.7.7.7.12.1">
<tr class="ltx_tr" id="S2.T2.7.7.7.7.7.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.7.7.7.7.7.12.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.7.7.7.7.7.12.1.1.1.1">arch:</span> Encoder-decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.7.7.7.7.7.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.7.7.7.7.7.12.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.7.7.7.7.7.12.1.2.1.1">out:</span> Semantic map</td>
</tr>
<tr class="ltx_tr" id="S2.T2.7.7.7.7.7.12.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.7.7.7.7.7.12.1.3.1">and Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.7.7.7.7.7.13.1">
<tr class="ltx_tr" id="S2.T2.7.7.7.7.7.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.7.7.7.7.7.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.7.7.7.7.7.14.1">
<tr class="ltx_tr" id="S2.T2.7.7.7.7.7.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.7.7.7.7.7.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.15"></td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.16"></td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.7.7.18">✓</td>
<td class="ltx_td" id="S2.T2.7.7.7.7.7.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.7.7.7.7.7.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.68.6" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.1"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.1.1" style="background-color:#F0F0F0;">AWkS <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib52" title="">2021</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.3"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.3.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.4"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.5"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.6"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.7"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.9"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.68.6.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.68.6.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.68.6.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.68.6.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.68.6.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.68.6.13.1.1.1">Neighborhood-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.68.6.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.68.6.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.68.6.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.68.6.16"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.16.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.17"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.68.6.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.68.6.20"><span class="ltx_text" id="S2.T2.62.62.62.62.68.6.20.1" style="background-color:#F0F0F0;">W-k-means <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib55" title="">2005</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.9.9.9.9.9">
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.3">IBIS, IBIScuda <cite class="ltx_cite ltx_citemacro_citep">(Bobbia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib18" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.5">✓</td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.8.8.8.1">✓<sup class="ltx_sup" id="S2.T2.8.8.8.8.8.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.8.8.8.8.8.1.1.1">a</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.8">✓</td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.10">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.2"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.9.9.9.9.9.2.m1.1"><semantics id="S2.T2.9.9.9.9.9.2.m1.1a"><mrow id="S2.T2.9.9.9.9.9.2.m1.1.2" xref="S2.T2.9.9.9.9.9.2.m1.1.2.cmml"><mi id="S2.T2.9.9.9.9.9.2.m1.1.2.2" xref="S2.T2.9.9.9.9.9.2.m1.1.2.2.cmml">O</mi><mo id="S2.T2.9.9.9.9.9.2.m1.1.2.1" xref="S2.T2.9.9.9.9.9.2.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.9.9.9.9.9.2.m1.1.2.3.2" xref="S2.T2.9.9.9.9.9.2.m1.1.2.cmml"><mo id="S2.T2.9.9.9.9.9.2.m1.1.2.3.2.1" stretchy="false" xref="S2.T2.9.9.9.9.9.2.m1.1.2.cmml">(</mo><mi id="S2.T2.9.9.9.9.9.2.m1.1.1" xref="S2.T2.9.9.9.9.9.2.m1.1.1.cmml">n</mi><mo id="S2.T2.9.9.9.9.9.2.m1.1.2.3.2.2" stretchy="false" xref="S2.T2.9.9.9.9.9.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.9.9.9.9.9.2.m1.1b"><apply id="S2.T2.9.9.9.9.9.2.m1.1.2.cmml" xref="S2.T2.9.9.9.9.9.2.m1.1.2"><times id="S2.T2.9.9.9.9.9.2.m1.1.2.1.cmml" xref="S2.T2.9.9.9.9.9.2.m1.1.2.1"></times><ci id="S2.T2.9.9.9.9.9.2.m1.1.2.2.cmml" xref="S2.T2.9.9.9.9.9.2.m1.1.2.2">𝑂</ci><ci id="S2.T2.9.9.9.9.9.2.m1.1.1.cmml" xref="S2.T2.9.9.9.9.9.2.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.9.9.9.9.9.2.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.9.9.9.9.9.2.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.9.9.9.9.9.12.1">
<tr class="ltx_tr" id="S2.T2.9.9.9.9.9.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.9.9.9.9.9.12.1.1.1">Grid segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.9.9.9.9.9.13.1">
<tr class="ltx_tr" id="S2.T2.9.9.9.9.9.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.9.9.9.9.9.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.9.9.9.9.9.14.1">
<tr class="ltx_tr" id="S2.T2.9.9.9.9.9.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.9.9.9.9.9.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.9.9.9.9.9.16">✓</td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.17"></td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.18"></td>
<td class="ltx_td" id="S2.T2.9.9.9.9.9.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.9.9.9.9.9.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.69.7" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.1"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.1.1" style="background-color:#F0F0F0;">SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib116" title="">2015</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.3"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.3.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.4"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.5"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.7"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.9"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.69.7.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.69.7.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.69.7.12.1.1.1">Grid segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.69.7.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.69.7.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.69.7.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.69.7.16"><span class="ltx_text" id="S2.T2.62.62.62.62.69.7.16.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.17"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.69.7.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.69.7.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.70.8">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.1">CRS <cite class="ltx_cite ltx_citemacro_citep">(Conrad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib29" title="">2013</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.7">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.9">YCrCb</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.70.8.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.70.8.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.70.8.12.1.1.1">Grid segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.70.8.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.70.8.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.70.8.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.70.8.17">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.70.8.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.70.8.20">CR <cite class="ltx_cite ltx_citemacro_citep">(Guevara et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib49" title="">2011a</a>; Mester et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib84" title="">2011a</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.71.9" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.1"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.1.1" style="background-color:#F0F0F0;">ETPS <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.3"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.3.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.4"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.5"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.6"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.7"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.9"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.9.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.71.9.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.71.9.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.71.9.12.1.1.1">Grid segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.71.9.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.71.9.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.71.9.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.71.9.16"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.16.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.17"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.71.9.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.71.9.20"><span class="ltx_text" id="S2.T2.62.62.62.62.71.9.20.1" style="background-color:#F0F0F0;">SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.72.10">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.1">CFBS <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib132" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.3">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.7">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.72.10.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.72.10.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.72.10.12.1.1.1">Grid segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.72.10.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.72.10.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.72.10.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.72.10.16">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.17"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.72.10.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.72.10.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.10.10.10.10.10" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.2"><span class="ltx_text" id="S2.T2.10.10.10.10.10.2.1" style="background-color:#F0F0F0;">SCAC <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib140" title="">2021b</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.3"></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.4"></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.1"><span class="ltx_text" id="S2.T2.10.10.10.10.10.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.10.10.10.10.10.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.10.10.10.10.10.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.6"><span class="ltx_text" id="S2.T2.10.10.10.10.10.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.7"><span class="ltx_text" id="S2.T2.10.10.10.10.10.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.9"><span class="ltx_text" id="S2.T2.10.10.10.10.10.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.10"></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.10.10.10.10.10.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.10.10.10.10.10.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.10.10.10.10.10.12.1.1.1">Grid segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.10.10.10.10.10.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.10.10.10.10.10.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.10.10.10.10.10.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.10.10.10.10.10.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.10.10.10.10.10.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.10.10.10.10.10.14.1.1.1">Boundary evolution</td>
</tr>
<tr class="ltx_tr" id="S2.T2.10.10.10.10.10.14.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.10.10.10.10.10.14.1.2.1">clustering</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.15"></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.10.10.10.10.10.17"><span class="ltx_text" id="S2.T2.10.10.10.10.10.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.18"></td>
<td class="ltx_td" id="S2.T2.10.10.10.10.10.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.10.10.10.10.10.20"><span class="ltx_text" id="S2.T2.10.10.10.10.10.20.1" style="background-color:#F0F0F0;">WSBM <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib141" title="">2020</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.11.11.11.11.11">
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.2">LSC-Manhattan <cite class="ltx_cite ltx_citemacro_citep">(Qiao and Di, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib92" title="">2022</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.4">✓</td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.1">✓<sup class="ltx_sup" id="S2.T2.11.11.11.11.11.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.11.11.11.11.11.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.7">✓</td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.8"></td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.9"></td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.10"></td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.11.11.11.11.11.12.1">
<tr class="ltx_tr" id="S2.T2.11.11.11.11.11.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.11.11.11.11.11.12.1.1.1">Texture complexity</td>
</tr>
<tr class="ltx_tr" id="S2.T2.11.11.11.11.11.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.11.11.11.11.11.12.1.2.1">classification</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.11.11.11.11.11.13.1">
<tr class="ltx_tr" id="S2.T2.11.11.11.11.11.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.11.11.11.11.11.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.14"></td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.15"></td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.16"></td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.11.11.11.11.11.18">✓</td>
<td class="ltx_td" id="S2.T2.11.11.11.11.11.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.11.11.11.11.11.20">LSC <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib24" title="">2017a</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.73.11" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.1"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.1.1" style="background-color:#F0F0F0;">FLS <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib89" title="">2022</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.3"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.3.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.4"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.4.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.5"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.6"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.8"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.9"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.73.11.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.73.11.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.73.11.12.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.73.11.12.1.1.1.1">arch:</span> FCN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.73.11.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.73.11.12.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.73.11.12.1.2.1.1">out:</span> Affinity map</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.73.11.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.73.11.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.73.11.13.1.1.1">Boundary evolution</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.73.11.18"><span class="ltx_text" id="S2.T2.62.62.62.62.73.11.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.73.11.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.73.11.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.73.11.20.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.73.11.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.73.11.20.1.1.1">SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.73.11.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.73.11.20.1.2.1">SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib116" title="">2015</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.12.12.12.12.12">
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.2">SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.3"></td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.4"></td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.8">✓</td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.10">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.12.12.12.12.12.1.m1.1"><semantics id="S2.T2.12.12.12.12.12.1.m1.1a"><mrow id="S2.T2.12.12.12.12.12.1.m1.1.2" xref="S2.T2.12.12.12.12.12.1.m1.1.2.cmml"><mi id="S2.T2.12.12.12.12.12.1.m1.1.2.2" xref="S2.T2.12.12.12.12.12.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.12.12.12.12.12.1.m1.1.2.1" xref="S2.T2.12.12.12.12.12.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.12.12.12.12.12.1.m1.1.2.3.2" xref="S2.T2.12.12.12.12.12.1.m1.1.2.cmml"><mo id="S2.T2.12.12.12.12.12.1.m1.1.2.3.2.1" stretchy="false" xref="S2.T2.12.12.12.12.12.1.m1.1.2.cmml">(</mo><mi id="S2.T2.12.12.12.12.12.1.m1.1.1" xref="S2.T2.12.12.12.12.12.1.m1.1.1.cmml">n</mi><mo id="S2.T2.12.12.12.12.12.1.m1.1.2.3.2.2" stretchy="false" xref="S2.T2.12.12.12.12.12.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.12.12.12.12.12.1.m1.1b"><apply id="S2.T2.12.12.12.12.12.1.m1.1.2.cmml" xref="S2.T2.12.12.12.12.12.1.m1.1.2"><times id="S2.T2.12.12.12.12.12.1.m1.1.2.1.cmml" xref="S2.T2.12.12.12.12.12.1.m1.1.2.1"></times><ci id="S2.T2.12.12.12.12.12.1.m1.1.2.2.cmml" xref="S2.T2.12.12.12.12.12.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.12.12.12.12.12.1.m1.1.1.cmml" xref="S2.T2.12.12.12.12.12.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.12.12.12.12.12.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.12.12.12.12.12.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.12.12.12.12.12.12.1">
<tr class="ltx_tr" id="S2.T2.12.12.12.12.12.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.12.12.12.12.12.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.12.12.12.12.12.13.1">
<tr class="ltx_tr" id="S2.T2.12.12.12.12.12.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.12.12.12.12.12.13.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.14"></td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.12.12.12.12.12.16">✓</td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.17"></td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.18"></td>
<td class="ltx_td" id="S2.T2.12.12.12.12.12.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.12.12.12.12.12.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.13.13.13.13.13" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.2"><span class="ltx_text" id="S2.T2.13.13.13.13.13.2.1" style="background-color:#F0F0F0;">CONIC <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib44" title="">2021</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.3"></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.4"></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.6"><span class="ltx_text" id="S2.T2.13.13.13.13.13.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.7"><span class="ltx_text" id="S2.T2.13.13.13.13.13.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.8"><span class="ltx_text" id="S2.T2.13.13.13.13.13.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.10"><span class="ltx_text" id="S2.T2.13.13.13.13.13.10.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.13.13.13.13.13.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.13.13.13.13.13.1.m1.1a"><mrow id="S2.T2.13.13.13.13.13.1.m1.1.2" xref="S2.T2.13.13.13.13.13.1.m1.1.2.cmml"><mi id="S2.T2.13.13.13.13.13.1.m1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.13.13.13.13.13.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.13.13.13.13.13.1.m1.1.2.1" xref="S2.T2.13.13.13.13.13.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.13.13.13.13.13.1.m1.1.2.3.2" xref="S2.T2.13.13.13.13.13.1.m1.1.2.cmml"><mo id="S2.T2.13.13.13.13.13.1.m1.1.2.3.2.1" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.13.13.13.13.13.1.m1.1.2.cmml">(</mo><mi id="S2.T2.13.13.13.13.13.1.m1.1.1" mathbackground="#F0F0F0" xref="S2.T2.13.13.13.13.13.1.m1.1.1.cmml">n</mi><mo id="S2.T2.13.13.13.13.13.1.m1.1.2.3.2.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.13.13.13.13.13.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.13.13.13.13.13.1.m1.1b"><apply id="S2.T2.13.13.13.13.13.1.m1.1.2.cmml" xref="S2.T2.13.13.13.13.13.1.m1.1.2"><times id="S2.T2.13.13.13.13.13.1.m1.1.2.1.cmml" xref="S2.T2.13.13.13.13.13.1.m1.1.2.1"></times><ci id="S2.T2.13.13.13.13.13.1.m1.1.2.2.cmml" xref="S2.T2.13.13.13.13.13.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.13.13.13.13.13.1.m1.1.1.cmml" xref="S2.T2.13.13.13.13.13.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.13.13.13.13.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.13.13.13.13.13.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.13.13.13.13.13.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.13.13.13.13.13.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.13.13.13.13.13.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.13.13.13.13.13.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.13.13.13.13.13.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.13.13.13.13.13.13.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.14"></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.15"></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.13.13.13.13.13.17"><span class="ltx_text" id="S2.T2.13.13.13.13.13.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.18"></td>
<td class="ltx_td" id="S2.T2.13.13.13.13.13.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.13.13.13.13.13.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.13.13.13.13.13.20.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.13.13.13.13.13.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.13.13.13.13.13.20.1.1.1">SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.13.13.13.13.13.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.13.13.13.13.13.20.1.2.1">SCALP <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.14.14.14.14.14">
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.2">DRW <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib58" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.3"></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.4"></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.7">✓</td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.8"></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.9"></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.14.14.14.14.14.1.m1.1"><semantics id="S2.T2.14.14.14.14.14.1.m1.1a"><mrow id="S2.T2.14.14.14.14.14.1.m1.1.2" xref="S2.T2.14.14.14.14.14.1.m1.1.2.cmml"><mi id="S2.T2.14.14.14.14.14.1.m1.1.2.2" xref="S2.T2.14.14.14.14.14.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.14.14.14.14.14.1.m1.1.2.1" xref="S2.T2.14.14.14.14.14.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.14.14.14.14.14.1.m1.1.2.3.2" xref="S2.T2.14.14.14.14.14.1.m1.1.2.cmml"><mo id="S2.T2.14.14.14.14.14.1.m1.1.2.3.2.1" stretchy="false" xref="S2.T2.14.14.14.14.14.1.m1.1.2.cmml">(</mo><mi id="S2.T2.14.14.14.14.14.1.m1.1.1" xref="S2.T2.14.14.14.14.14.1.m1.1.1.cmml">n</mi><mo id="S2.T2.14.14.14.14.14.1.m1.1.2.3.2.2" stretchy="false" xref="S2.T2.14.14.14.14.14.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.14.14.14.14.14.1.m1.1b"><apply id="S2.T2.14.14.14.14.14.1.m1.1.2.cmml" xref="S2.T2.14.14.14.14.14.1.m1.1.2"><times id="S2.T2.14.14.14.14.14.1.m1.1.2.1.cmml" xref="S2.T2.14.14.14.14.14.1.m1.1.2.1"></times><ci id="S2.T2.14.14.14.14.14.1.m1.1.2.2.cmml" xref="S2.T2.14.14.14.14.14.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.14.14.14.14.14.1.m1.1.1.cmml" xref="S2.T2.14.14.14.14.14.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.14.14.14.14.14.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.14.14.14.14.14.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.14.14.14.14.14.12.1">
<tr class="ltx_tr" id="S2.T2.14.14.14.14.14.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.14.14.14.14.14.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.14.14.14.14.14.13.1">
<tr class="ltx_tr" id="S2.T2.14.14.14.14.14.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.14.14.14.14.14.13.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.14.14.14.14.14.14.1">
<tr class="ltx_tr" id="S2.T2.14.14.14.14.14.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.14.14.14.14.14.14.1.1.1">Label propagation</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.15"></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.14.14.14.14.14.17">✓</td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.18"></td>
<td class="ltx_td" id="S2.T2.14.14.14.14.14.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.14.14.14.14.14.20">RW <cite class="ltx_cite ltx_citemacro_citep">(Grady, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib46" title="">2006</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.18.18.18.18.18" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.5"><span class="ltx_text" id="S2.T2.18.18.18.18.18.5.1" style="background-color:#F0F0F0;">FCSS <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib67" title="">2021</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.7"><span class="ltx_text" id="S2.T2.18.18.18.18.18.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.15.15.15.15.15.1"><span class="ltx_text" id="S2.T2.15.15.15.15.15.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.15.15.15.15.15.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.15.15.15.15.15.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.8"><span class="ltx_text" id="S2.T2.18.18.18.18.18.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.16.16.16.16.16.2"><span class="ltx_text" id="S2.T2.16.16.16.16.16.2.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.16.16.16.16.16.2.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.16.16.16.16.16.2.1.1.1" style="background-color:#F0F0F0;">a</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.9"><span class="ltx_text" id="S2.T2.18.18.18.18.18.9.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.11"><span class="ltx_text" id="S2.T2.18.18.18.18.18.11.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.4">
<math alttext="O(n+nt)" class="ltx_Math" display="inline" id="S2.T2.17.17.17.17.17.3.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.17.17.17.17.17.3.m1.1a"><mrow id="S2.T2.17.17.17.17.17.3.m1.1.1" xref="S2.T2.17.17.17.17.17.3.m1.1.1.cmml"><mi id="S2.T2.17.17.17.17.17.3.m1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.17.17.17.17.17.3.m1.1.1.3.cmml">O</mi><mo id="S2.T2.17.17.17.17.17.3.m1.1.1.2" xref="S2.T2.17.17.17.17.17.3.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.cmml"><mo id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.cmml"><mi id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.2" mathbackground="#F0F0F0" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.1" mathbackground="#F0F0F0" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.2" mathbackground="#F0F0F0" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.2.cmml">n</mi><mo id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.1" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.3" mathbackground="#F0F0F0" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.3.cmml">t</mi></mrow></mrow><mo id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.3" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.17.17.17.17.17.3.m1.1b"><apply id="S2.T2.17.17.17.17.17.3.m1.1.1.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1"><times id="S2.T2.17.17.17.17.17.3.m1.1.1.2.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.2"></times><ci id="S2.T2.17.17.17.17.17.3.m1.1.1.3.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.3">𝑂</ci><apply id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1"><plus id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.1.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.1"></plus><ci id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.2.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.2">𝑛</ci><apply id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3"><times id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.1"></times><ci id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.2">𝑛</ci><ci id="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.3.cmml" xref="S2.T2.17.17.17.17.17.3.m1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.17.17.17.17.17.3.m1.1c">O(n+nt)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.17.17.17.17.17.3.m1.1d">italic_O ( italic_n + italic_n italic_t )</annotation></semantics></math><span class="ltx_text" id="S2.T2.18.18.18.18.18.4.1" style="background-color:#F0F0F0;"> <sup class="ltx_sup" id="S2.T2.18.18.18.18.18.4.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.18.18.18.18.18.4.1.1.1" style="background-color:#F0F0F0;">d</span></sup></span>
</td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.13"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.18.18.18.18.18.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.18.18.18.18.18.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.18.18.18.18.18.14.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.15"></td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.18.18.18.18.18.17"><span class="ltx_text" id="S2.T2.18.18.18.18.18.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.18"></td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.19"></td>
<td class="ltx_td" id="S2.T2.18.18.18.18.18.20"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.18.18.18.18.18.21"><span class="ltx_text" id="S2.T2.18.18.18.18.18.21.1" style="background-color:#F0F0F0;">SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.19.19.19.19.19">
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.2">F-DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Loke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib75" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.3"></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.4"></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.7">✓</td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.8"></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.10">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.19.19.19.19.19.1.m1.1"><semantics id="S2.T2.19.19.19.19.19.1.m1.1a"><mrow id="S2.T2.19.19.19.19.19.1.m1.1.2" xref="S2.T2.19.19.19.19.19.1.m1.1.2.cmml"><mi id="S2.T2.19.19.19.19.19.1.m1.1.2.2" xref="S2.T2.19.19.19.19.19.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.19.19.19.19.19.1.m1.1.2.1" xref="S2.T2.19.19.19.19.19.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.19.19.19.19.19.1.m1.1.2.3.2" xref="S2.T2.19.19.19.19.19.1.m1.1.2.cmml"><mo id="S2.T2.19.19.19.19.19.1.m1.1.2.3.2.1" stretchy="false" xref="S2.T2.19.19.19.19.19.1.m1.1.2.cmml">(</mo><mi id="S2.T2.19.19.19.19.19.1.m1.1.1" xref="S2.T2.19.19.19.19.19.1.m1.1.1.cmml">n</mi><mo id="S2.T2.19.19.19.19.19.1.m1.1.2.3.2.2" stretchy="false" xref="S2.T2.19.19.19.19.19.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.19.19.19.19.19.1.m1.1b"><apply id="S2.T2.19.19.19.19.19.1.m1.1.2.cmml" xref="S2.T2.19.19.19.19.19.1.m1.1.2"><times id="S2.T2.19.19.19.19.19.1.m1.1.2.1.cmml" xref="S2.T2.19.19.19.19.19.1.m1.1.2.1"></times><ci id="S2.T2.19.19.19.19.19.1.m1.1.2.2.cmml" xref="S2.T2.19.19.19.19.19.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.19.19.19.19.19.1.m1.1.1.cmml" xref="S2.T2.19.19.19.19.19.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.19.19.19.19.19.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.19.19.19.19.19.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.19.19.19.19.19.13.1">
<tr class="ltx_tr" id="S2.T2.19.19.19.19.19.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.19.19.19.19.19.13.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.14"></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.19.19.19.19.19.16">✓</td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.17"></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.18"></td>
<td class="ltx_td" id="S2.T2.19.19.19.19.19.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.19.19.19.19.19.20">RT-DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib45" title="">2018</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.20.20.20.20.20" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.2"><span class="ltx_text" id="S2.T2.20.20.20.20.20.2.1" style="background-color:#F0F0F0;">SCBP <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib145" title="">2021b</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.3"></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.4"></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.6"><span class="ltx_text" id="S2.T2.20.20.20.20.20.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.7"><span class="ltx_text" id="S2.T2.20.20.20.20.20.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.8"><span class="ltx_text" id="S2.T2.20.20.20.20.20.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.10"><span class="ltx_text" id="S2.T2.20.20.20.20.20.10.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.20.20.20.20.20.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.20.20.20.20.20.1.m1.1a"><mrow id="S2.T2.20.20.20.20.20.1.m1.1.2" xref="S2.T2.20.20.20.20.20.1.m1.1.2.cmml"><mi id="S2.T2.20.20.20.20.20.1.m1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.20.20.20.20.20.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.20.20.20.20.20.1.m1.1.2.1" xref="S2.T2.20.20.20.20.20.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.20.20.20.20.20.1.m1.1.2.3.2" xref="S2.T2.20.20.20.20.20.1.m1.1.2.cmml"><mo id="S2.T2.20.20.20.20.20.1.m1.1.2.3.2.1" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.20.20.20.20.20.1.m1.1.2.cmml">(</mo><mi id="S2.T2.20.20.20.20.20.1.m1.1.1" mathbackground="#F0F0F0" xref="S2.T2.20.20.20.20.20.1.m1.1.1.cmml">n</mi><mo id="S2.T2.20.20.20.20.20.1.m1.1.2.3.2.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.20.20.20.20.20.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.20.20.20.20.20.1.m1.1b"><apply id="S2.T2.20.20.20.20.20.1.m1.1.2.cmml" xref="S2.T2.20.20.20.20.20.1.m1.1.2"><times id="S2.T2.20.20.20.20.20.1.m1.1.2.1.cmml" xref="S2.T2.20.20.20.20.20.1.m1.1.2.1"></times><ci id="S2.T2.20.20.20.20.20.1.m1.1.2.2.cmml" xref="S2.T2.20.20.20.20.20.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.20.20.20.20.20.1.m1.1.1.cmml" xref="S2.T2.20.20.20.20.20.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.20.20.20.20.20.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.20.20.20.20.20.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.20.20.20.20.20.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.20.20.20.20.20.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.20.20.20.20.20.13.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.20.20.20.20.20.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.20.20.20.20.20.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.20.20.20.20.20.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.15"></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.20.20.20.20.20.17"><span class="ltx_text" id="S2.T2.20.20.20.20.20.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.18"></td>
<td class="ltx_td" id="S2.T2.20.20.20.20.20.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.20.20.20.20.20.20"><span class="ltx_text" id="S2.T2.20.20.20.20.20.20.1" style="background-color:#F0F0F0;">DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib104" title="">2016</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.21.21.21.21.21">
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.2">A-DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Wang and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib124" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.3"></td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.4"></td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.8">✓</td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.10">RGB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.21.21.21.21.21.1.m1.1"><semantics id="S2.T2.21.21.21.21.21.1.m1.1a"><mrow id="S2.T2.21.21.21.21.21.1.m1.1.2" xref="S2.T2.21.21.21.21.21.1.m1.1.2.cmml"><mi id="S2.T2.21.21.21.21.21.1.m1.1.2.2" xref="S2.T2.21.21.21.21.21.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.21.21.21.21.21.1.m1.1.2.1" xref="S2.T2.21.21.21.21.21.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.21.21.21.21.21.1.m1.1.2.3.2" xref="S2.T2.21.21.21.21.21.1.m1.1.2.cmml"><mo id="S2.T2.21.21.21.21.21.1.m1.1.2.3.2.1" stretchy="false" xref="S2.T2.21.21.21.21.21.1.m1.1.2.cmml">(</mo><mi id="S2.T2.21.21.21.21.21.1.m1.1.1" xref="S2.T2.21.21.21.21.21.1.m1.1.1.cmml">n</mi><mo id="S2.T2.21.21.21.21.21.1.m1.1.2.3.2.2" stretchy="false" xref="S2.T2.21.21.21.21.21.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.21.21.21.21.21.1.m1.1b"><apply id="S2.T2.21.21.21.21.21.1.m1.1.2.cmml" xref="S2.T2.21.21.21.21.21.1.m1.1.2"><times id="S2.T2.21.21.21.21.21.1.m1.1.2.1.cmml" xref="S2.T2.21.21.21.21.21.1.m1.1.2.1"></times><ci id="S2.T2.21.21.21.21.21.1.m1.1.2.2.cmml" xref="S2.T2.21.21.21.21.21.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.21.21.21.21.21.1.m1.1.1.cmml" xref="S2.T2.21.21.21.21.21.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.21.21.21.21.21.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.21.21.21.21.21.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.21.21.21.21.21.12.1">
<tr class="ltx_tr" id="S2.T2.21.21.21.21.21.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.21.21.21.21.21.12.1.1.1">Texture computation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.21.21.21.21.21.13.1">
<tr class="ltx_tr" id="S2.T2.21.21.21.21.21.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.21.21.21.21.21.13.1.1.1">Dynamic-center-update</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.21.21.21.21.21.14.1">
<tr class="ltx_tr" id="S2.T2.21.21.21.21.21.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.21.21.21.21.21.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.15"></td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.16"></td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.21.21.21.21.21.18">✓</td>
<td class="ltx_td" id="S2.T2.21.21.21.21.21.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.21.21.21.21.21.20">DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib104" title="">2016</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.74.12" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.1"><span class="ltx_text" id="S2.T2.62.62.62.62.74.12.1.1" style="background-color:#F0F0F0;">ERGC <cite class="ltx_cite ltx_citemacro_citep">(Buyssens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib21" title="">2014</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.2"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.5"><span class="ltx_text" id="S2.T2.62.62.62.62.74.12.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.6"><span class="ltx_text" id="S2.T2.62.62.62.62.74.12.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.7"><span class="ltx_text" id="S2.T2.62.62.62.62.74.12.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.9"><span class="ltx_text" id="S2.T2.62.62.62.62.74.12.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.74.12.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.74.12.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.74.12.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.74.12.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.74.12.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.74.12.13.1.1.1">Path-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.74.12.17"><span class="ltx_text" id="S2.T2.62.62.62.62.74.12.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.74.12.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.74.12.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.22.22.22.22.22">
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.2">ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.8">✓</td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.10">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.1"><math alttext="O(n\log n)" class="ltx_Math" display="inline" id="S2.T2.22.22.22.22.22.1.m1.1"><semantics id="S2.T2.22.22.22.22.22.1.m1.1a"><mrow id="S2.T2.22.22.22.22.22.1.m1.1.1" xref="S2.T2.22.22.22.22.22.1.m1.1.1.cmml"><mi id="S2.T2.22.22.22.22.22.1.m1.1.1.3" xref="S2.T2.22.22.22.22.22.1.m1.1.1.3.cmml">O</mi><mo id="S2.T2.22.22.22.22.22.1.m1.1.1.2" xref="S2.T2.22.22.22.22.22.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.cmml"><mo id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.2" stretchy="false" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.cmml"><mi id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.2" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.1" lspace="0.167em" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.1" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.2" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.3" stretchy="false" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.22.22.22.22.22.1.m1.1b"><apply id="S2.T2.22.22.22.22.22.1.m1.1.1.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1"><times id="S2.T2.22.22.22.22.22.1.m1.1.1.2.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.2"></times><ci id="S2.T2.22.22.22.22.22.1.m1.1.1.3.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.3">𝑂</ci><apply id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1"><times id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.1.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.1"></times><ci id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.2.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3"><log id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.1"></log><ci id="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.22.22.22.22.22.1.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.22.22.22.22.22.1.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.22.22.22.22.22.1.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.22.22.22.22.22.12.1">
<tr class="ltx_tr" id="S2.T2.22.22.22.22.22.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.22.22.22.22.22.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.22.22.22.22.22.13.1">
<tr class="ltx_tr" id="S2.T2.22.22.22.22.22.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.22.22.22.22.22.13.1.1.1">Path-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.14"></td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.15"></td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.22.22.22.22.22.17">✓</td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.18"></td>
<td class="ltx_td" id="S2.T2.22.22.22.22.22.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.22.22.22.22.22.20">IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.23.23.23.23.23" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.2"><span class="ltx_text" id="S2.T2.23.23.23.23.23.2.1" style="background-color:#F0F0F0;">RSS <cite class="ltx_cite ltx_citemacro_citep">(Chai, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib22" title="">2020</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.3"></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.4"></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.6"><span class="ltx_text" id="S2.T2.23.23.23.23.23.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.7"><span class="ltx_text" id="S2.T2.23.23.23.23.23.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.8"><span class="ltx_text" id="S2.T2.23.23.23.23.23.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.9"></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.23.23.23.23.23.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.23.23.23.23.23.1.m1.1a"><mrow id="S2.T2.23.23.23.23.23.1.m1.1.2" xref="S2.T2.23.23.23.23.23.1.m1.1.2.cmml"><mi id="S2.T2.23.23.23.23.23.1.m1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.23.23.23.23.23.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.23.23.23.23.23.1.m1.1.2.1" xref="S2.T2.23.23.23.23.23.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.23.23.23.23.23.1.m1.1.2.3.2" xref="S2.T2.23.23.23.23.23.1.m1.1.2.cmml"><mo id="S2.T2.23.23.23.23.23.1.m1.1.2.3.2.1" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.23.23.23.23.23.1.m1.1.2.cmml">(</mo><mi id="S2.T2.23.23.23.23.23.1.m1.1.1" mathbackground="#F0F0F0" xref="S2.T2.23.23.23.23.23.1.m1.1.1.cmml">n</mi><mo id="S2.T2.23.23.23.23.23.1.m1.1.2.3.2.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.23.23.23.23.23.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.23.23.23.23.23.1.m1.1b"><apply id="S2.T2.23.23.23.23.23.1.m1.1.2.cmml" xref="S2.T2.23.23.23.23.23.1.m1.1.2"><times id="S2.T2.23.23.23.23.23.1.m1.1.2.1.cmml" xref="S2.T2.23.23.23.23.23.1.m1.1.2.1"></times><ci id="S2.T2.23.23.23.23.23.1.m1.1.2.2.cmml" xref="S2.T2.23.23.23.23.23.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.23.23.23.23.23.1.m1.1.1.cmml" xref="S2.T2.23.23.23.23.23.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.23.23.23.23.23.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.23.23.23.23.23.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.23.23.23.23.23.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.23.23.23.23.23.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.23.23.23.23.23.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.23.23.23.23.23.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.23.23.23.23.23.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.23.23.23.23.23.13.1.1.1">Path-based</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.14"></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.15"></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.23.23.23.23.23.17"><span class="ltx_text" id="S2.T2.23.23.23.23.23.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.18"></td>
<td class="ltx_td" id="S2.T2.23.23.23.23.23.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.23.23.23.23.23.20"><span class="ltx_text" id="S2.T2.23.23.23.23.23.20.1" style="background-color:#F0F0F0;">IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.24.24.24.24.24">
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.2">DISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.3"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.4">✓</td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.7">✓</td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.8"></td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.10">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.1"><math alttext="O(n\log n)" class="ltx_Math" display="inline" id="S2.T2.24.24.24.24.24.1.m1.1"><semantics id="S2.T2.24.24.24.24.24.1.m1.1a"><mrow id="S2.T2.24.24.24.24.24.1.m1.1.1" xref="S2.T2.24.24.24.24.24.1.m1.1.1.cmml"><mi id="S2.T2.24.24.24.24.24.1.m1.1.1.3" xref="S2.T2.24.24.24.24.24.1.m1.1.1.3.cmml">O</mi><mo id="S2.T2.24.24.24.24.24.1.m1.1.1.2" xref="S2.T2.24.24.24.24.24.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.cmml"><mo id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.2" stretchy="false" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.cmml"><mi id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.2" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.1" lspace="0.167em" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.1" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.2" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.3" stretchy="false" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.24.24.24.24.24.1.m1.1b"><apply id="S2.T2.24.24.24.24.24.1.m1.1.1.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1"><times id="S2.T2.24.24.24.24.24.1.m1.1.1.2.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.2"></times><ci id="S2.T2.24.24.24.24.24.1.m1.1.1.3.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.3">𝑂</ci><apply id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1"><times id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.1.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.1"></times><ci id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.2.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3"><log id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.1"></log><ci id="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.24.24.24.24.24.1.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.24.24.24.24.24.1.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.24.24.24.24.24.1.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.24.24.24.24.24.12.1">
<tr class="ltx_tr" id="S2.T2.24.24.24.24.24.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.24.24.24.24.24.12.1.1.1">Seed oversampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.24.24.24.24.24.13.1">
<tr class="ltx_tr" id="S2.T2.24.24.24.24.24.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.24.24.24.24.24.13.1.1.1">Path-based</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.14"></td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.15"></td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.24.24.24.24.24.17">✓</td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.18"></td>
<td class="ltx_td" id="S2.T2.24.24.24.24.24.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.24.24.24.24.24.20">ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.3"><span class="ltx_text" id="S2.T2.26.26.26.26.26.3.1" style="background-color:#F0F0F0;">ODISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.5"><span class="ltx_text" id="S2.T2.26.26.26.26.26.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.7"><span class="ltx_text" id="S2.T2.26.26.26.26.26.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.8"><span class="ltx_text" id="S2.T2.26.26.26.26.26.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.10"><span class="ltx_text" id="S2.T2.26.26.26.26.26.10.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.11"><span class="ltx_text" id="S2.T2.26.26.26.26.26.11.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.2">
<math alttext="O(n\log n)" class="ltx_Math" display="inline" id="S2.T2.25.25.25.25.25.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.25.25.25.25.25.1.m1.1a"><mrow id="S2.T2.25.25.25.25.25.1.m1.1.1" xref="S2.T2.25.25.25.25.25.1.m1.1.1.cmml"><mi id="S2.T2.25.25.25.25.25.1.m1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.25.25.25.25.25.1.m1.1.1.3.cmml">O</mi><mo id="S2.T2.25.25.25.25.25.1.m1.1.1.2" xref="S2.T2.25.25.25.25.25.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.cmml"><mo id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.cmml"><mi id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.2" mathbackground="#F0F0F0" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.1" lspace="0.167em" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.1" mathbackground="#F0F0F0" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.2" mathbackground="#F0F0F0" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.3" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.25.25.25.25.25.1.m1.1b"><apply id="S2.T2.25.25.25.25.25.1.m1.1.1.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1"><times id="S2.T2.25.25.25.25.25.1.m1.1.1.2.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.2"></times><ci id="S2.T2.25.25.25.25.25.1.m1.1.1.3.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.3">𝑂</ci><apply id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1"><times id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.1.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.1"></times><ci id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.2.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3"><log id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.1"></log><ci id="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.25.25.25.25.25.1.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.25.25.25.25.25.1.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.25.25.25.25.25.1.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math><span class="ltx_text" id="S2.T2.26.26.26.26.26.2.1" style="background-color:#F0F0F0;"> <sup class="ltx_sup" id="S2.T2.26.26.26.26.26.2.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.26.26.26.26.26.2.1.1.1" style="background-color:#F0F0F0;">e</span></sup></span>
</td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.26.26.26.26.26.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.26.26.26.26.26.13.1.1.1.1">arch:</span> Encoder-decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.26.26.26.26.26.13.1.2.1.1">out:</span> Saliency map</td>
</tr>
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.13.1.3.1">and Seed oversampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.26.26.26.26.26.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.14.1.1.1">Path-based</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.15"></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.16"></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.17"></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.18"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.26.26.26.26.26.19"><span class="ltx_text" id="S2.T2.26.26.26.26.26.19.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.26.26.26.26.26.20"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.21">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.26.26.26.26.26.21.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26.21.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.21.1.1.1">DISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.26.26.26.26.26.21.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.26.26.26.26.26.21.1.2.1">OISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib13" title="">2018</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.29.29.29.29.29">
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.4">SICLE <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib14" title="">2022b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib12" title="">a</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.27.27.27.27.27.1">✓ <sup class="ltx_sup" id="S2.T2.27.27.27.27.27.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.27.27.27.27.27.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.8">✓</td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.10">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.11">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.3">
<math alttext="O(n\log n)" class="ltx_Math" display="inline" id="S2.T2.28.28.28.28.28.2.m1.1"><semantics id="S2.T2.28.28.28.28.28.2.m1.1a"><mrow id="S2.T2.28.28.28.28.28.2.m1.1.1" xref="S2.T2.28.28.28.28.28.2.m1.1.1.cmml"><mi id="S2.T2.28.28.28.28.28.2.m1.1.1.3" xref="S2.T2.28.28.28.28.28.2.m1.1.1.3.cmml">O</mi><mo id="S2.T2.28.28.28.28.28.2.m1.1.1.2" xref="S2.T2.28.28.28.28.28.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.cmml"><mo id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.2" stretchy="false" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.cmml"><mi id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.2" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.1" lspace="0.167em" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.1" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.2" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.3" stretchy="false" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.28.28.28.28.28.2.m1.1b"><apply id="S2.T2.28.28.28.28.28.2.m1.1.1.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1"><times id="S2.T2.28.28.28.28.28.2.m1.1.1.2.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.2"></times><ci id="S2.T2.28.28.28.28.28.2.m1.1.1.3.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.3">𝑂</ci><apply id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1"><times id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.1.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.1"></times><ci id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.2.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.2">𝑛</ci><apply id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3"><log id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.1"></log><ci id="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.28.28.28.28.28.2.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.28.28.28.28.28.2.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.28.28.28.28.28.2.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math> <sup class="ltx_sup" id="S2.T2.29.29.29.29.29.3.1"><span class="ltx_text ltx_font_italic" id="S2.T2.29.29.29.29.29.3.1.1">e</span></sup>
</td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.29.29.29.29.29.13.1">
<tr class="ltx_tr" id="S2.T2.29.29.29.29.29.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.29.29.29.29.29.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.29.29.29.29.29.13.1.1.1.1">arch:</span> Encoder-decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.29.29.29.29.29.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.29.29.29.29.29.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.29.29.29.29.29.13.1.2.1.1">out:</span> Saliency map</td>
</tr>
<tr class="ltx_tr" id="S2.T2.29.29.29.29.29.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.29.29.29.29.29.13.1.3.1">and Seed oversampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.29.29.29.29.29.14.1">
<tr class="ltx_tr" id="S2.T2.29.29.29.29.29.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.29.29.29.29.29.14.1.1.1">Path-based</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.15"></td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.16"></td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.17"></td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.18"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.29.29.29.29.29.19">✓</td>
<td class="ltx_td" id="S2.T2.29.29.29.29.29.20"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.29.29.29.29.29.21">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.29.29.29.29.29.21.1">
<tr class="ltx_tr" id="S2.T2.29.29.29.29.29.21.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.29.29.29.29.29.21.1.1.1">ODISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.30.30.30.30.30" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.2"><span class="ltx_text" id="S2.T2.30.30.30.30.30.2.1" style="background-color:#F0F0F0;">SH <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib128" title="">2018</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.3"></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.4"></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.6"><span class="ltx_text" id="S2.T2.30.30.30.30.30.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.7"><span class="ltx_text" id="S2.T2.30.30.30.30.30.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.8"></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.10"><span class="ltx_text" id="S2.T2.30.30.30.30.30.10.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.1"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.30.30.30.30.30.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.30.30.30.30.30.1.m1.1a"><mrow id="S2.T2.30.30.30.30.30.1.m1.1.2" xref="S2.T2.30.30.30.30.30.1.m1.1.2.cmml"><mi id="S2.T2.30.30.30.30.30.1.m1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.30.30.30.30.30.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.30.30.30.30.30.1.m1.1.2.1" xref="S2.T2.30.30.30.30.30.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.30.30.30.30.30.1.m1.1.2.3.2" xref="S2.T2.30.30.30.30.30.1.m1.1.2.cmml"><mo id="S2.T2.30.30.30.30.30.1.m1.1.2.3.2.1" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.30.30.30.30.30.1.m1.1.2.cmml">(</mo><mi id="S2.T2.30.30.30.30.30.1.m1.1.1" mathbackground="#F0F0F0" xref="S2.T2.30.30.30.30.30.1.m1.1.1.cmml">n</mi><mo id="S2.T2.30.30.30.30.30.1.m1.1.2.3.2.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.30.30.30.30.30.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.30.30.30.30.30.1.m1.1b"><apply id="S2.T2.30.30.30.30.30.1.m1.1.2.cmml" xref="S2.T2.30.30.30.30.30.1.m1.1.2"><times id="S2.T2.30.30.30.30.30.1.m1.1.2.1.cmml" xref="S2.T2.30.30.30.30.30.1.m1.1.2.1"></times><ci id="S2.T2.30.30.30.30.30.1.m1.1.2.2.cmml" xref="S2.T2.30.30.30.30.30.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.30.30.30.30.30.1.m1.1.1.cmml" xref="S2.T2.30.30.30.30.30.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.30.30.30.30.30.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.30.30.30.30.30.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.30.30.30.30.30.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.30.30.30.30.30.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.30.30.30.30.30.13.1.1.1">Hierarchical</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.14"></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.15"></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.30.30.30.30.30.17"><span class="ltx_text" id="S2.T2.30.30.30.30.30.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.18"></td>
<td class="ltx_td" id="S2.T2.30.30.30.30.30.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.30.30.30.30.30.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.75.13">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.1">UOIFT <cite class="ltx_cite ltx_citemacro_citep">(Bejar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib10" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.2"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.6">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.7"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.75.13.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.75.13.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.75.13.12.1.1.1">Clustering method</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.75.13.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.75.13.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.75.13.13.1.1.1">Hierarchical</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.75.13.17">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.75.13.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.75.13.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.75.13.20.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.75.13.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.75.13.20.1.1.1">IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.75.13.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.75.13.20.1.2.1">OIFT <cite class="ltx_cite ltx_citemacro_citep">(Mansilla and Miranda, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib80" title="">2013</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.33.33.33.33.33" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.4"><span class="ltx_text" id="S2.T2.33.33.33.33.33.4.1" style="background-color:#F0F0F0;">HMLI-SLIC <cite class="ltx_cite ltx_citemacro_citep">(Di et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib31" title="">2021</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.6"><span class="ltx_text" id="S2.T2.33.33.33.33.33.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.7"><span class="ltx_text" id="S2.T2.33.33.33.33.33.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.31.31.31.31.31.1"><span class="ltx_text" id="S2.T2.31.31.31.31.31.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.31.31.31.31.31.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.31.31.31.31.31.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.8"><span class="ltx_text" id="S2.T2.33.33.33.33.33.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.9"><span class="ltx_text" id="S2.T2.33.33.33.33.33.9.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.11"><span class="ltx_text" id="S2.T2.33.33.33.33.33.11.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.3">
<math alttext="O(nd)" class="ltx_Math" display="inline" id="S2.T2.32.32.32.32.32.2.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.32.32.32.32.32.2.m1.1a"><mrow id="S2.T2.32.32.32.32.32.2.m1.1.1" xref="S2.T2.32.32.32.32.32.2.m1.1.1.cmml"><mi id="S2.T2.32.32.32.32.32.2.m1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.32.32.32.32.32.2.m1.1.1.3.cmml">O</mi><mo id="S2.T2.32.32.32.32.32.2.m1.1.1.2" xref="S2.T2.32.32.32.32.32.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.cmml"><mo id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.cmml"><mi id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.2" mathbackground="#F0F0F0" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.1" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.3.cmml">d</mi></mrow><mo id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.3" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.32.32.32.32.32.2.m1.1b"><apply id="S2.T2.32.32.32.32.32.2.m1.1.1.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1"><times id="S2.T2.32.32.32.32.32.2.m1.1.1.2.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1.2"></times><ci id="S2.T2.32.32.32.32.32.2.m1.1.1.3.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1.3">𝑂</ci><apply id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1"><times id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.1.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.1"></times><ci id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.2.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.2">𝑛</ci><ci id="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.3.cmml" xref="S2.T2.32.32.32.32.32.2.m1.1.1.1.1.1.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.32.32.32.32.32.2.m1.1c">O(nd)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.32.32.32.32.32.2.m1.1d">italic_O ( italic_n italic_d )</annotation></semantics></math><span class="ltx_text" id="S2.T2.33.33.33.33.33.3.1" style="background-color:#F0F0F0;"> <sup class="ltx_sup" id="S2.T2.33.33.33.33.33.3.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.33.33.33.33.33.3.1.1.1" style="background-color:#F0F0F0;">f</span></sup></span>
</td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.33.33.33.33.33.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.33.33.33.33.33.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.33.33.33.33.33.13.1.1.1">Clustering method</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.33.33.33.33.33.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.33.33.33.33.33.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.33.33.33.33.33.14.1.1.1">Hierarchical</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.15">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.33.33.33.33.33.15.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.33.33.33.33.33.15.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.33.33.33.33.33.15.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.33.33.33.33.33.17"><span class="ltx_text" id="S2.T2.33.33.33.33.33.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.18"></td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.19"></td>
<td class="ltx_td" id="S2.T2.33.33.33.33.33.20"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.33.33.33.33.33.21"><span class="ltx_text" id="S2.T2.33.33.33.33.33.21.1" style="background-color:#F0F0F0;">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.76.14">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.1">RISF <cite class="ltx_cite ltx_citemacro_citep">(Galvão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib38" title="">2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib39" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.7">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.11"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.76.14.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.76.14.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.76.14.13.1.1.1">Hierarchical</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.76.14.14.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.76.14.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.76.14.14.1.1.1">Hierarchical</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.76.14.14.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.76.14.14.1.2.1">region merging</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.76.14.17">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.76.14.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.76.14.20">ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.3"><span class="ltx_text" id="S2.T2.35.35.35.35.35.3.1" style="background-color:#F0F0F0;">DAL-HERS <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib90" title="">2022</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.4"></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.5"></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.7"><span class="ltx_text" id="S2.T2.35.35.35.35.35.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.8"><span class="ltx_text" id="S2.T2.35.35.35.35.35.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.10"><span class="ltx_text" id="S2.T2.35.35.35.35.35.10.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.11"><span class="ltx_text" id="S2.T2.35.35.35.35.35.11.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.2">
<math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.34.34.34.34.34.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.34.34.34.34.34.1.m1.1a"><mrow id="S2.T2.34.34.34.34.34.1.m1.1.2" xref="S2.T2.34.34.34.34.34.1.m1.1.2.cmml"><mi id="S2.T2.34.34.34.34.34.1.m1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.34.34.34.34.34.1.m1.1.2.2.cmml">O</mi><mo id="S2.T2.34.34.34.34.34.1.m1.1.2.1" xref="S2.T2.34.34.34.34.34.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.34.34.34.34.34.1.m1.1.2.3.2" xref="S2.T2.34.34.34.34.34.1.m1.1.2.cmml"><mo id="S2.T2.34.34.34.34.34.1.m1.1.2.3.2.1" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.34.34.34.34.34.1.m1.1.2.cmml">(</mo><mi id="S2.T2.34.34.34.34.34.1.m1.1.1" mathbackground="#F0F0F0" xref="S2.T2.34.34.34.34.34.1.m1.1.1.cmml">n</mi><mo id="S2.T2.34.34.34.34.34.1.m1.1.2.3.2.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.34.34.34.34.34.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.34.34.34.34.34.1.m1.1b"><apply id="S2.T2.34.34.34.34.34.1.m1.1.2.cmml" xref="S2.T2.34.34.34.34.34.1.m1.1.2"><times id="S2.T2.34.34.34.34.34.1.m1.1.2.1.cmml" xref="S2.T2.34.34.34.34.34.1.m1.1.2.1"></times><ci id="S2.T2.34.34.34.34.34.1.m1.1.2.2.cmml" xref="S2.T2.34.34.34.34.34.1.m1.1.2.2">𝑂</ci><ci id="S2.T2.34.34.34.34.34.1.m1.1.1.cmml" xref="S2.T2.34.34.34.34.34.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.34.34.34.34.34.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.34.34.34.34.34.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math><span class="ltx_text" id="S2.T2.35.35.35.35.35.2.1" style="background-color:#F0F0F0;"> <sup class="ltx_sup" id="S2.T2.35.35.35.35.35.2.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.35.35.35.35.35.2.1.1.1" style="background-color:#F0F0F0;">g</span></sup></span>
</td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.35.35.35.35.35.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.35.35.35.35.35.13.1.1.1.1">arch:</span> Multi-scale</td>
</tr>
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.13.1.2.1">Residual CNN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.13.1.3.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.35.35.35.35.35.13.1.3.1.1">out:</span> Affinity map</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.35.35.35.35.35.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.14.1.1.1">Hierarchical</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.15"></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.16"></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.17"></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.18"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.35.35.35.35.35.19"><span class="ltx_text" id="S2.T2.35.35.35.35.35.19.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.35.35.35.35.35.20"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.21">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.35.35.35.35.35.21.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35.21.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.21.1.1.1">SEAL <cite class="ltx_cite ltx_citemacro_citep">(Tu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib113" title="">2018</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.35.35.35.35.35.21.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.35.35.35.35.35.21.1.2.1">ERS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib72" title="">2011</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.36.36.36.36.36">
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.2">PGDPC <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib48" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.3"></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.4"></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.7">✓</td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.8"></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.10">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.1"><math alttext="O(n\log n)" class="ltx_Math" display="inline" id="S2.T2.36.36.36.36.36.1.m1.1"><semantics id="S2.T2.36.36.36.36.36.1.m1.1a"><mrow id="S2.T2.36.36.36.36.36.1.m1.1.1" xref="S2.T2.36.36.36.36.36.1.m1.1.1.cmml"><mi id="S2.T2.36.36.36.36.36.1.m1.1.1.3" xref="S2.T2.36.36.36.36.36.1.m1.1.1.3.cmml">O</mi><mo id="S2.T2.36.36.36.36.36.1.m1.1.1.2" xref="S2.T2.36.36.36.36.36.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.cmml"><mo id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.2" stretchy="false" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.cmml"><mi id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.2" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.1" lspace="0.167em" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.cmml"><mi id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.1" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.2" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.3" stretchy="false" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.36.36.36.36.36.1.m1.1b"><apply id="S2.T2.36.36.36.36.36.1.m1.1.1.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1"><times id="S2.T2.36.36.36.36.36.1.m1.1.1.2.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.2"></times><ci id="S2.T2.36.36.36.36.36.1.m1.1.1.3.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.3">𝑂</ci><apply id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1"><times id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.1.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.1"></times><ci id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.2.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3"><log id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.1.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.1"></log><ci id="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.2.cmml" xref="S2.T2.36.36.36.36.36.1.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.36.36.36.36.36.1.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.36.36.36.36.36.1.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.36.36.36.36.36.12.1">
<tr class="ltx_tr" id="S2.T2.36.36.36.36.36.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.36.36.36.36.36.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.36.36.36.36.36.13.1">
<tr class="ltx_tr" id="S2.T2.36.36.36.36.36.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.36.36.36.36.36.13.1.1.1">Density-based</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.14"></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.15"></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.36.36.36.36.36.17">✓</td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.18"></td>
<td class="ltx_td" id="S2.T2.36.36.36.36.36.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.36.36.36.36.36.20">DPC <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib118" title="">2018</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.37.37.37.37.37" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.2"><span class="ltx_text" id="S2.T2.37.37.37.37.37.2.1" style="background-color:#F0F0F0;">DPS <cite class="ltx_cite ltx_citemacro_citep">(Shah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib103" title="">2021</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.3"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.4"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.1"><span class="ltx_text" id="S2.T2.37.37.37.37.37.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.37.37.37.37.37.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.37.37.37.37.37.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.6"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.7"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.9"><span class="ltx_text" id="S2.T2.37.37.37.37.37.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.10"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.37.37.37.37.37.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.37.37.37.37.37.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.37.37.37.37.37.12.1.1.1">Compute features</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.37.37.37.37.37.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.37.37.37.37.37.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.37.37.37.37.37.13.1.1.1">Density-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.37.37.37.37.37.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.37.37.37.37.37.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.37.37.37.37.37.14.1.1.1">Clustering method</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.15"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.37.37.37.37.37.17"><span class="ltx_text" id="S2.T2.37.37.37.37.37.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.18"></td>
<td class="ltx_td" id="S2.T2.37.37.37.37.37.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.37.37.37.37.37.20"><span class="ltx_text" id="S2.T2.37.37.37.37.37.20.1" style="background-color:#F0F0F0;">DP <cite class="ltx_cite ltx_citemacro_citep">(Rodriguez and Laio, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib96" title="">2014</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.38.38.38.38.38">
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.2">ANRW <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib119" title="">2020b</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.3"></td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.4"></td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.7">✓</td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.8"></td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.10">YCbCr</td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.1"><math alttext="O(n^{2})" class="ltx_Math" display="inline" id="S2.T2.38.38.38.38.38.1.m1.1"><semantics id="S2.T2.38.38.38.38.38.1.m1.1a"><mrow id="S2.T2.38.38.38.38.38.1.m1.1.1" xref="S2.T2.38.38.38.38.38.1.m1.1.1.cmml"><mi id="S2.T2.38.38.38.38.38.1.m1.1.1.3" xref="S2.T2.38.38.38.38.38.1.m1.1.1.3.cmml">O</mi><mo id="S2.T2.38.38.38.38.38.1.m1.1.1.2" xref="S2.T2.38.38.38.38.38.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.cmml"><mo id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.2" stretchy="false" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.cmml"><mi id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.2" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.3" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.3" stretchy="false" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.38.38.38.38.38.1.m1.1b"><apply id="S2.T2.38.38.38.38.38.1.m1.1.1.cmml" xref="S2.T2.38.38.38.38.38.1.m1.1.1"><times id="S2.T2.38.38.38.38.38.1.m1.1.1.2.cmml" xref="S2.T2.38.38.38.38.38.1.m1.1.1.2"></times><ci id="S2.T2.38.38.38.38.38.1.m1.1.1.3.cmml" xref="S2.T2.38.38.38.38.38.1.m1.1.1.3">𝑂</ci><apply id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.cmml" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.1.cmml" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1">superscript</csymbol><ci id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.2.cmml" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.2">𝑛</ci><cn id="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.T2.38.38.38.38.38.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.38.38.38.38.38.1.m1.1c">O(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S2.T2.38.38.38.38.38.1.m1.1d">italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.38.38.38.38.38.12.1">
<tr class="ltx_tr" id="S2.T2.38.38.38.38.38.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.38.38.38.38.38.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.38.38.38.38.38.13.1">
<tr class="ltx_tr" id="S2.T2.38.38.38.38.38.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.38.38.38.38.38.13.1.1.1">Sparse linear</td>
</tr>
<tr class="ltx_tr" id="S2.T2.38.38.38.38.38.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.38.38.38.38.38.13.1.2.1">system</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.14">Merging Step</td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.15"></td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.16"></td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.38.38.38.38.38.18">✓</td>
<td class="ltx_td" id="S2.T2.38.38.38.38.38.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.38.38.38.38.38.20">NRW <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib138" title="">2015</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.39.39.39.39.39" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.1"><span class="ltx_text" id="S2.T2.39.39.39.39.39.1.1" style="background-color:#F0F0F0;">GL<math alttext="l_{1/2}" class="ltx_Math" display="inline" id="S2.T2.39.39.39.39.39.1.1.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.39.39.39.39.39.1.1.m1.1a"><msub id="S2.T2.39.39.39.39.39.1.1.m1.1.1" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.cmml"><mi id="S2.T2.39.39.39.39.39.1.1.m1.1.1.2" mathbackground="#F0F0F0" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.2.cmml">l</mi><mrow id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.cmml"><mn id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.2" mathbackground="#F0F0F0" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.1" mathbackground="#F0F0F0" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.1.cmml">/</mo><mn id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.3" mathbackground="#F0F0F0" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.T2.39.39.39.39.39.1.1.m1.1b"><apply id="S2.T2.39.39.39.39.39.1.1.m1.1.1.cmml" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.39.39.39.39.39.1.1.m1.1.1.1.cmml" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1">subscript</csymbol><ci id="S2.T2.39.39.39.39.39.1.1.m1.1.1.2.cmml" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.2">𝑙</ci><apply id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.cmml" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3"><divide id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.1.cmml" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.1"></divide><cn id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.2">1</cn><cn id="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.3.cmml" type="integer" xref="S2.T2.39.39.39.39.39.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.39.39.39.39.39.1.1.m1.1c">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="S2.T2.39.39.39.39.39.1.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math>RSC <cite class="ltx_cite ltx_citemacro_citep">(Francis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib37" title="">2022</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.3"><span class="ltx_text" id="S2.T2.39.39.39.39.39.3.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.5"><span class="ltx_text" id="S2.T2.39.39.39.39.39.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.6"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.7"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.8"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.9"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.10"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.39.39.39.39.39.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.39.39.39.39.39.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.39.39.39.39.39.12.1.1.1">Clustering method</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.39.39.39.39.39.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.39.39.39.39.39.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.39.39.39.39.39.13.1.1.1">Sparse linear</td>
</tr>
<tr class="ltx_tr" id="S2.T2.39.39.39.39.39.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.39.39.39.39.39.13.1.2.1">system</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.39.39.39.39.39.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.39.39.39.39.39.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.39.39.39.39.39.14.1.1.1">Encoding procedure</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.15"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.39.39.39.39.39.17"><span class="ltx_text" id="S2.T2.39.39.39.39.39.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.18"></td>
<td class="ltx_td" id="S2.T2.39.39.39.39.39.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.39.39.39.39.39.20"><span class="ltx_text" id="S2.T2.39.39.39.39.39.20.1" style="background-color:#F0F0F0;">CAWR <cite class="ltx_cite ltx_citemacro_citep">(Wang and Wu, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib125" title="">2017</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.77.15">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.1">SCSC <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib66" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.5">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.6"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.7"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.9">RGB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.77.15.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.77.15.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.77.15.12.1.1.1">Clustering method</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.77.15.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.77.15.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.77.15.13.1.1.1">Sparse linear</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.77.15.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.77.15.13.1.2.1">system</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.77.15.14.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.77.15.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.77.15.14.1.1.1">Clustering method</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.77.15.18">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.77.15.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.77.15.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.41.41.41.41.41" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.3"><span class="ltx_text" id="S2.T2.41.41.41.41.41.3.1" style="background-color:#F0F0F0;">EAM <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib5" title="">2020</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.4"></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.5"></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.40.40.40.40.40.1"><span class="ltx_text" id="S2.T2.40.40.40.40.40.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.40.40.40.40.40.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.40.40.40.40.40.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.7"><span class="ltx_text" id="S2.T2.41.41.41.41.41.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.8"></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.10"><span class="ltx_text" id="S2.T2.41.41.41.41.41.10.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.2"><math alttext="O(\log^{2}n)" class="ltx_Math" display="inline" id="S2.T2.41.41.41.41.41.2.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.41.41.41.41.41.2.m1.1a"><mrow id="S2.T2.41.41.41.41.41.2.m1.1.1" xref="S2.T2.41.41.41.41.41.2.m1.1.1.cmml"><mi id="S2.T2.41.41.41.41.41.2.m1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.41.41.41.41.41.2.m1.1.1.3.cmml">O</mi><mo id="S2.T2.41.41.41.41.41.2.m1.1.1.2" xref="S2.T2.41.41.41.41.41.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.cmml"><mo id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.cmml"><msup id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.cmml"><mi id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.2" mathbackground="#F0F0F0" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.2.cmml">log</mi><mn id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.3" mathbackground="#F0F0F0" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1a" lspace="0.167em" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.cmml">⁡</mo><mi id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.2" mathbackground="#F0F0F0" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.2.cmml">n</mi></mrow><mo id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.3" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.41.41.41.41.41.2.m1.1b"><apply id="S2.T2.41.41.41.41.41.2.m1.1.1.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1"><times id="S2.T2.41.41.41.41.41.2.m1.1.1.2.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.2"></times><ci id="S2.T2.41.41.41.41.41.2.m1.1.1.3.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.3">𝑂</ci><apply id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1"><apply id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.1.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1">superscript</csymbol><log id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.2.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.2"></log><cn id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.1.3">2</cn></apply><ci id="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.2.cmml" xref="S2.T2.41.41.41.41.41.2.m1.1.1.1.1.1.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.41.41.41.41.41.2.m1.1c">O(\log^{2}n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.41.41.41.41.41.2.m1.1d">italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.41.41.41.41.41.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.41.41.41.41.41.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.41.41.41.41.41.12.1.1.1">Noise remotion and</td>
</tr>
<tr class="ltx_tr" id="S2.T2.41.41.41.41.41.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.41.41.41.41.41.12.1.2.1">Boundary map computation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.41.41.41.41.41.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.41.41.41.41.41.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.41.41.41.41.41.13.1.1.1">Regional attributes</td>
</tr>
<tr class="ltx_tr" id="S2.T2.41.41.41.41.41.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.41.41.41.41.41.13.1.2.1">extraction</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.41.41.41.41.41.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.41.41.41.41.41.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.41.41.41.41.41.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.15"></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.41.41.41.41.41.17"><span class="ltx_text" id="S2.T2.41.41.41.41.41.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.18"></td>
<td class="ltx_td" id="S2.T2.41.41.41.41.41.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.41.41.41.41.41.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.1">ECCPD <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib77" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.5">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.6">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.9">RGB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.78.16.12.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.78.16.12.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.78.16.12.1.1.1.1">arch:</span> Multi-scale CNN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16.12.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.78.16.12.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.78.16.12.1.2.1.1">out:</span> Boundary map</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16.12.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.78.16.12.1.3.1">and Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.78.16.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.78.16.13.1.1.1">Polygonal decomposition</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.78.16.14.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.78.16.14.1.1.1">Boundary evolution</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.78.16.14.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.78.16.14.1.2.1">clustering</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.78.16.18">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.78.16.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.78.16.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.44.44.44.44.44" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.4">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.44.44.44.44.44.4.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.44.44.44.44.44.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.44.44.44.44.44.4.1.1.1">GMMSP <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.6"><span class="ltx_text" id="S2.T2.44.44.44.44.44.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.7"><span class="ltx_text" id="S2.T2.44.44.44.44.44.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.42.42.42.42.42.1"><span class="ltx_text" id="S2.T2.42.42.42.42.42.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.42.42.42.42.42.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.42.42.42.42.42.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.43.43.43.43.43.2"><span class="ltx_text" id="S2.T2.43.43.43.43.43.2.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.43.43.43.43.43.2.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.43.43.43.43.43.2.1.1.1" style="background-color:#F0F0F0;">a</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.8"><span class="ltx_text" id="S2.T2.44.44.44.44.44.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.10"><span class="ltx_text" id="S2.T2.44.44.44.44.44.10.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.3"><math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.44.44.44.44.44.3.m1.1" style="background-color:#F0F0F0;"><semantics id="S2.T2.44.44.44.44.44.3.m1.1a"><mrow id="S2.T2.44.44.44.44.44.3.m1.1.2" xref="S2.T2.44.44.44.44.44.3.m1.1.2.cmml"><mi id="S2.T2.44.44.44.44.44.3.m1.1.2.2" mathbackground="#F0F0F0" xref="S2.T2.44.44.44.44.44.3.m1.1.2.2.cmml">O</mi><mo id="S2.T2.44.44.44.44.44.3.m1.1.2.1" xref="S2.T2.44.44.44.44.44.3.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.44.44.44.44.44.3.m1.1.2.3.2" xref="S2.T2.44.44.44.44.44.3.m1.1.2.cmml"><mo id="S2.T2.44.44.44.44.44.3.m1.1.2.3.2.1" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.44.44.44.44.44.3.m1.1.2.cmml">(</mo><mi id="S2.T2.44.44.44.44.44.3.m1.1.1" mathbackground="#F0F0F0" xref="S2.T2.44.44.44.44.44.3.m1.1.1.cmml">n</mi><mo id="S2.T2.44.44.44.44.44.3.m1.1.2.3.2.2" mathbackground="#F0F0F0" stretchy="false" xref="S2.T2.44.44.44.44.44.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.44.44.44.44.44.3.m1.1b"><apply id="S2.T2.44.44.44.44.44.3.m1.1.2.cmml" xref="S2.T2.44.44.44.44.44.3.m1.1.2"><times id="S2.T2.44.44.44.44.44.3.m1.1.2.1.cmml" xref="S2.T2.44.44.44.44.44.3.m1.1.2.1"></times><ci id="S2.T2.44.44.44.44.44.3.m1.1.2.2.cmml" xref="S2.T2.44.44.44.44.44.3.m1.1.2.2">𝑂</ci><ci id="S2.T2.44.44.44.44.44.3.m1.1.1.cmml" xref="S2.T2.44.44.44.44.44.3.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.44.44.44.44.44.3.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.44.44.44.44.44.3.m1.1d">italic_O ( italic_n )</annotation></semantics></math></td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.44.44.44.44.44.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.44.44.44.44.44.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.44.44.44.44.44.13.1.1.1">Data distribution-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.44.44.44.44.44.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.44.44.44.44.44.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.44.44.44.44.44.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.15"></td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.16"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.44.44.44.44.44.17"><span class="ltx_text" id="S2.T2.44.44.44.44.44.17.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.18"></td>
<td class="ltx_td" id="S2.T2.44.44.44.44.44.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.44.44.44.44.44.20"><span class="ltx_text" id="S2.T2.44.44.44.44.44.20.1" style="background-color:#F0F0F0;">SCGAGMM <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib57" title="">2016</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.48.48.48.48.48">
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.5">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.48.48.48.48.48.5.1">
<tr class="ltx_tr" id="S2.T2.48.48.48.48.48.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.48.48.48.48.48.5.1.1.1">gGMMSP <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib8" title="">2020</a>)</cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.45.45.45.45.45.1">✓<sup class="ltx_sup" id="S2.T2.45.45.45.45.45.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.45.45.45.45.45.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.46.46.46.46.46.2">✓<sup class="ltx_sup" id="S2.T2.46.46.46.46.46.2.1"><span class="ltx_text ltx_font_italic" id="S2.T2.46.46.46.46.46.2.1.1">a</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.9">✓</td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.11">CIELAB</td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.4">
<math alttext="O(n)" class="ltx_Math" display="inline" id="S2.T2.47.47.47.47.47.3.m1.1"><semantics id="S2.T2.47.47.47.47.47.3.m1.1a"><mrow id="S2.T2.47.47.47.47.47.3.m1.1.2" xref="S2.T2.47.47.47.47.47.3.m1.1.2.cmml"><mi id="S2.T2.47.47.47.47.47.3.m1.1.2.2" xref="S2.T2.47.47.47.47.47.3.m1.1.2.2.cmml">O</mi><mo id="S2.T2.47.47.47.47.47.3.m1.1.2.1" xref="S2.T2.47.47.47.47.47.3.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T2.47.47.47.47.47.3.m1.1.2.3.2" xref="S2.T2.47.47.47.47.47.3.m1.1.2.cmml"><mo id="S2.T2.47.47.47.47.47.3.m1.1.2.3.2.1" stretchy="false" xref="S2.T2.47.47.47.47.47.3.m1.1.2.cmml">(</mo><mi id="S2.T2.47.47.47.47.47.3.m1.1.1" xref="S2.T2.47.47.47.47.47.3.m1.1.1.cmml">n</mi><mo id="S2.T2.47.47.47.47.47.3.m1.1.2.3.2.2" stretchy="false" xref="S2.T2.47.47.47.47.47.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.47.47.47.47.47.3.m1.1b"><apply id="S2.T2.47.47.47.47.47.3.m1.1.2.cmml" xref="S2.T2.47.47.47.47.47.3.m1.1.2"><times id="S2.T2.47.47.47.47.47.3.m1.1.2.1.cmml" xref="S2.T2.47.47.47.47.47.3.m1.1.2.1"></times><ci id="S2.T2.47.47.47.47.47.3.m1.1.2.2.cmml" xref="S2.T2.47.47.47.47.47.3.m1.1.2.2">𝑂</ci><ci id="S2.T2.47.47.47.47.47.3.m1.1.1.cmml" xref="S2.T2.47.47.47.47.47.3.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.47.47.47.47.47.3.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.T2.47.47.47.47.47.3.m1.1d">italic_O ( italic_n )</annotation></semantics></math> <sup class="ltx_sup" id="S2.T2.48.48.48.48.48.4.1"><span class="ltx_text ltx_font_italic" id="S2.T2.48.48.48.48.48.4.1.1">h</span></sup>
</td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.13"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.48.48.48.48.48.14.1">
<tr class="ltx_tr" id="S2.T2.48.48.48.48.48.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.48.48.48.48.48.14.1.1.1">Data distribution-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.15">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.48.48.48.48.48.15.1">
<tr class="ltx_tr" id="S2.T2.48.48.48.48.48.15.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.48.48.48.48.48.15.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.16"></td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.48.48.48.48.48.18">✓</td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.19"></td>
<td class="ltx_td" id="S2.T2.48.48.48.48.48.20"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.48.48.48.48.48.21">GMMSP <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.79.17" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.1"><span class="ltx_text" id="S2.T2.62.62.62.62.79.17.1.1" style="background-color:#F0F0F0;">ERS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib72" title="">2011</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.2"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.5"><span class="ltx_text" id="S2.T2.62.62.62.62.79.17.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.6"><span class="ltx_text" id="S2.T2.62.62.62.62.79.17.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.7"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.9"><span class="ltx_text" id="S2.T2.62.62.62.62.79.17.9.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.79.17.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.79.17.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.79.17.13.1.1.1">Graph-based</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.15"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.79.17.16"><span class="ltx_text" id="S2.T2.62.62.62.62.79.17.16.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.17"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.18"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.79.17.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.79.17.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.50.50.50.50.50">
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.3">SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.4"></td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.5"></td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.49.49.49.49.49.1">✓<sup class="ltx_sup" id="S2.T2.49.49.49.49.49.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.49.49.49.49.49.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.2">✓<sup class="ltx_sup" id="S2.T2.50.50.50.50.50.2.1"><span class="ltx_text ltx_font_italic" id="S2.T2.50.50.50.50.50.2.1.1">a</span></sup>
</td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.10"></td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.50.50.50.50.50.13.1">
<tr class="ltx_tr" id="S2.T2.50.50.50.50.50.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.50.50.50.50.50.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.50.50.50.50.50.13.1.1.1.1">arch:</span> FCN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.50.50.50.50.50.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.50.50.50.50.50.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.50.50.50.50.50.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.50.50.50.50.50.14.1">
<tr class="ltx_tr" id="S2.T2.50.50.50.50.50.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.50.50.50.50.50.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.15"></td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.16"></td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.50.50.50.50.50.18">✓</td>
<td class="ltx_td" id="S2.T2.50.50.50.50.50.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.50.50.50.50.50.20">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.2"><span class="ltx_text" id="S2.T2.51.51.51.51.51.2.1" style="background-color:#F0F0F0;">E2E-SIS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib121" title="">2020a</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.3"></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.4"></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.6"><span class="ltx_text" id="S2.T2.51.51.51.51.51.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.1"><span class="ltx_text" id="S2.T2.51.51.51.51.51.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.51.51.51.51.51.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.51.51.51.51.51.1.1.1.1" style="background-color:#F0F0F0;">a</span></sup></span></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.8"><span class="ltx_text" id="S2.T2.51.51.51.51.51.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.9"><span class="ltx_text" id="S2.T2.51.51.51.51.51.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.10"></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.11"></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.51.51.51.51.51.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.51.51.51.51.51.13.1.1.1.1">arch:</span> FCN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.51.51.51.51.51.13.1.2.1.1">out:</span> Superpixels and</td>
</tr>
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.13.1.3.1">image segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.51.51.51.51.51.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.15"></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.16"></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.51.51.51.51.51.18"><span class="ltx_text" id="S2.T2.51.51.51.51.51.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.51.51.51.51.51.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.51.51.51.51.51.20.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.20.1.1.1">DEL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib74" title="">2018</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.51.51.51.51.51.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.51.51.51.51.51.20.1.2.1">SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.80.18">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.1">LNS-net <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib148" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.2"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.5">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.7">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.9">LAB/RGB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.80.18.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.80.18.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.80.18.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.80.18.13.1.1.1.1">arch:</span> FCN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.80.18.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.80.18.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.80.18.13.1.2.1.1">out:</span> Image reconstruction</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.80.18.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.80.18.13.1.3.1">and Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.80.18.14.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.80.18.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.80.18.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.80.18.18">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.80.18.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.80.18.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.52.52.52.52.52" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.52.52.52.52.52.2"><span class="ltx_text" id="S2.T2.52.52.52.52.52.2.1" style="background-color:#F0F0F0;">ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.3"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.4"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.52.52.52.52.52.1"><span class="ltx_text" id="S2.T2.52.52.52.52.52.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.52.52.52.52.52.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.52.52.52.52.52.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.6"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.7"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.52.52.52.52.52.9"><span class="ltx_text" id="S2.T2.52.52.52.52.52.9.1" style="background-color:#F0F0F0;">RGB</span></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.10"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.52.52.52.52.52.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.52.52.52.52.52.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.52.52.52.52.52.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.52.52.52.52.52.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.52.52.52.52.52.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.52.52.52.52.52.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.52.52.52.52.52.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.52.52.52.52.52.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.52.52.52.52.52.13.1.2.1.1">out:</span> Image reconstruction</td>
</tr>
<tr class="ltx_tr" id="S2.T2.52.52.52.52.52.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.52.52.52.52.52.13.1.3.1">and Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.14"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.15"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.16"></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.52.52.52.52.52.18"><span class="ltx_text" id="S2.T2.52.52.52.52.52.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.52.52.52.52.52.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.52.52.52.52.52.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.52.52.52.52.52.20.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.52.52.52.52.52.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.52.52.52.52.52.20.1.1.1">DIP <cite class="ltx_cite ltx_citemacro_citep">(Lempitsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib64" title="">2018</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.52.52.52.52.52.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.52.52.52.52.52.20.1.2.1">RIM <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib61" title="">2010</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.53.53.53.53.53">
<td class="ltx_td ltx_align_center" id="S2.T2.53.53.53.53.53.2">EW-RIM <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.3"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.4"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.53.53.53.53.53.1">✓<sup class="ltx_sup" id="S2.T2.53.53.53.53.53.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.53.53.53.53.53.1.1.1">c</span></sup>
</td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.6"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.7"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.53.53.53.53.53.9">RBG</td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.10"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.53.53.53.53.53.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.53.53.53.53.53.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.53.53.53.53.53.13.1">
<tr class="ltx_tr" id="S2.T2.53.53.53.53.53.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.53.53.53.53.53.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.53.53.53.53.53.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.53.53.53.53.53.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.53.53.53.53.53.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.53.53.53.53.53.13.1.2.1.1">out:</span> Image reconstruction</td>
</tr>
<tr class="ltx_tr" id="S2.T2.53.53.53.53.53.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.53.53.53.53.53.13.1.3.1">and Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.14"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.15"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.16"></td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.53.53.53.53.53.18">✓</td>
<td class="ltx_td" id="S2.T2.53.53.53.53.53.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.53.53.53.53.53.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.53.53.53.53.53.20.1">
<tr class="ltx_tr" id="S2.T2.53.53.53.53.53.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.53.53.53.53.53.20.1.1.1">ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.53.53.53.53.53.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.53.53.53.53.53.20.1.2.1">DIP <cite class="ltx_cite ltx_citemacro_citep">(Lempitsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib64" title="">2018</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.54.54.54.54.54.2"><span class="ltx_text" id="S2.T2.54.54.54.54.54.2.1" style="background-color:#F0F0F0;">ML-RIM <cite class="ltx_cite ltx_citemacro_citep">(Eliasof et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib33" title="">2022</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.3"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.4"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.54.54.54.54.54.1"><span class="ltx_text" id="S2.T2.54.54.54.54.54.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.54.54.54.54.54.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.54.54.54.54.54.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.6"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.7"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.54.54.54.54.54.9"><span class="ltx_text" id="S2.T2.54.54.54.54.54.9.1" style="background-color:#F0F0F0;">RBG</span></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.10"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.54.54.54.54.54.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.54.54.54.54.54.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.54.54.54.54.54.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.54.54.54.54.54.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.13.1.2.1">multi-scale module</td>
</tr>
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.13.1.3.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.54.54.54.54.54.13.1.3.1.1">out:</span> Image reconstruction</td>
</tr>
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54.13.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.13.1.4.1">and Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.14"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.15"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.16"></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.54.54.54.54.54.18"><span class="ltx_text" id="S2.T2.54.54.54.54.54.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.54.54.54.54.54.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.20">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.54.54.54.54.54.20.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54.20.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.20.1.1.1">ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>,</td>
</tr>
<tr class="ltx_tr" id="S2.T2.54.54.54.54.54.20.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.54.54.54.54.54.20.1.2.1">EW-RIM <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.81.19">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.81.19.1">SEN <cite class="ltx_cite ltx_citemacro_citep">(Gaur and Manjunath, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib40" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.2"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.81.19.5">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.6"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.7"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.81.19.9">RGB</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.11"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.81.19.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.81.19.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.81.19.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.81.19.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.81.19.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.81.19.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.81.19.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.81.19.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.81.19.18">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.81.19.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.81.19.20">RPEIG <cite class="ltx_cite ltx_citemacro_citep">(Kong and Fowlkes, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib60" title="">2018</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.56.56.56.56.56" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.56.56.56.56.56.3"><span class="ltx_text" id="S2.T2.56.56.56.56.56.3.1" style="background-color:#F0F0F0;">ML-SGN <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib71" title="">2022</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.4"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.5"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.55.55.55.55.55.1"><span class="ltx_text" id="S2.T2.55.55.55.55.55.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.55.55.55.55.55.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.55.55.55.55.55.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.56.56.56.56.56.2"><span class="ltx_text" id="S2.T2.56.56.56.56.56.2.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.56.56.56.56.56.2.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.56.56.56.56.56.2.1.1.1" style="background-color:#F0F0F0;">a</span></sup></span></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.7"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.8"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.9"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.10"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.56.56.56.56.56.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.56.56.56.56.56.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.56.56.56.56.56.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.56.56.56.56.56.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.56.56.56.56.56.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.56.56.56.56.56.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.56.56.56.56.56.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.56.56.56.56.56.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.56.56.56.56.56.13.1.2.1.1">out:</span> Superpixels and</td>
</tr>
<tr class="ltx_tr" id="S2.T2.56.56.56.56.56.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.56.56.56.56.56.13.1.3.1">image segmentation</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.56.56.56.56.56.14"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.15"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.16"></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.56.56.56.56.56.18"><span class="ltx_text" id="S2.T2.56.56.56.56.56.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.56.56.56.56.56.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.56.56.56.56.56.20"><span class="ltx_text" id="S2.T2.56.56.56.56.56.20.1" style="background-color:#F0F0F0;">SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.58.58.58.58.58">
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.3">SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.4"></td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.5"></td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.57.57.57.57.57.1">✓<sup class="ltx_sup" id="S2.T2.57.57.57.57.57.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.57.57.57.57.57.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.2">✓<sup class="ltx_sup" id="S2.T2.58.58.58.58.58.2.1"><span class="ltx_text ltx_font_italic" id="S2.T2.58.58.58.58.58.2.1.1">a</span></sup>
</td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.9">CIELAB</td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.10"></td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.58.58.58.58.58.13.1">
<tr class="ltx_tr" id="S2.T2.58.58.58.58.58.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.58.58.58.58.58.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.58.58.58.58.58.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.58.58.58.58.58.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.58.58.58.58.58.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.58.58.58.58.58.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.58.58.58.58.58.14.1">
<tr class="ltx_tr" id="S2.T2.58.58.58.58.58.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.58.58.58.58.58.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.15"></td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.16"></td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.58.58.58.58.58.18">✓</td>
<td class="ltx_td" id="S2.T2.58.58.58.58.58.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.58.58.58.58.58.20">SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.59.59.59.59.59" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.2"><span class="ltx_text" id="S2.T2.59.59.59.59.59.2.1" style="background-color:#F0F0F0;">SENSS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib120" title="">2022</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.3"></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.4"></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.1"><span class="ltx_text" id="S2.T2.59.59.59.59.59.1.1" style="background-color:#F0F0F0;">✓<sup class="ltx_sup" id="S2.T2.59.59.59.59.59.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.59.59.59.59.59.1.1.1.1" style="background-color:#F0F0F0;">c</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.6"><span class="ltx_text" id="S2.T2.59.59.59.59.59.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.7"><span class="ltx_text" id="S2.T2.59.59.59.59.59.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.8"><span class="ltx_text" id="S2.T2.59.59.59.59.59.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.9"><span class="ltx_text" id="S2.T2.59.59.59.59.59.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.10"></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.59.59.59.59.59.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.59.59.59.59.59.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.59.59.59.59.59.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.59.59.59.59.59.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.59.59.59.59.59.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.59.59.59.59.59.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.59.59.59.59.59.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.14"></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.15"></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.16"></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.59.59.59.59.59.18"><span class="ltx_text" id="S2.T2.59.59.59.59.59.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.59.59.59.59.59.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.59.59.59.59.59.20"><span class="ltx_text" id="S2.T2.59.59.59.59.59.20.1" style="background-color:#F0F0F0;">SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.61.61.61.61.61">
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.3">AINET <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib127" title="">2021a</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.4"></td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.5"></td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.60.60.60.60.60.1">✓<sup class="ltx_sup" id="S2.T2.60.60.60.60.60.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.60.60.60.60.60.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.2">✓<sup class="ltx_sup" id="S2.T2.61.61.61.61.61.2.1"><span class="ltx_text ltx_font_italic" id="S2.T2.61.61.61.61.61.2.1.1">a</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.7">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.8">✓</td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.9"></td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.10"></td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.61.61.61.61.61.13.1">
<tr class="ltx_tr" id="S2.T2.61.61.61.61.61.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.61.61.61.61.61.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.61.61.61.61.61.13.1.1.1.1">arch:</span> Encoder-Decoder</td>
</tr>
<tr class="ltx_tr" id="S2.T2.61.61.61.61.61.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.61.61.61.61.61.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.61.61.61.61.61.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.61.61.61.61.61.14.1">
<tr class="ltx_tr" id="S2.T2.61.61.61.61.61.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.61.61.61.61.61.14.1.1.1">Merging sStep</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.15"></td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.16"></td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.61.61.61.61.61.18">✓</td>
<td class="ltx_td" id="S2.T2.61.61.61.61.61.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.61.61.61.61.61.20">SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.82.20" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.1"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.1.1" style="background-color:#F0F0F0;">DAFnet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib131" title="">2021a</a>)</cite></span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.2"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.5"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.6"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.8"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.9"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.9.1" style="background-color:#F0F0F0;">CIELAB</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.82.20.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.82.20.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.82.20.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.82.20.13.1.1.1.1">arch:</span> Weight-shared CNN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.82.20.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.82.20.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.82.20.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.82.20.18"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.82.20.19"></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.82.20.20"><span class="ltx_text" id="S2.T2.62.62.62.62.82.20.20.1" style="background-color:#F0F0F0;">SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.62">
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.2">SIN <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib139" title="">2021a</a>)</cite>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.3"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.4"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.5"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.1">✓<sup class="ltx_sup" id="S2.T2.62.62.62.62.62.1.1"><span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.62.1.1.1">c</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.6">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.7"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.8">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.9"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.10"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.11"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.12"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.62.13.1">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.62.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.62.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.62.13.1.1.1.1">arch:</span> Interpolation Network</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.62.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.62.13.1.2.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.62.13.1.2.1.1">out:</span> Superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.14"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.15"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.16"></td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.17"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.62.62.62.62.62.18">✓</td>
<td class="ltx_td" id="S2.T2.62.62.62.62.62.19"></td>
<td class="ltx_td ltx_nopad_r" id="S2.T2.62.62.62.62.62.20"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.1"><span class="ltx_text" id="S2.T2.62.62.62.62.83.21.1.1" style="background-color:#F0F0F0;">BP-net <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib142" title="">2021c</a>)</cite></span></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.2"></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.3"></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.5"><span class="ltx_text" id="S2.T2.62.62.62.62.83.21.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.6"></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.7"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.8"><span class="ltx_text" id="S2.T2.62.62.62.62.83.21.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.9"><span class="ltx_text" id="S2.T2.62.62.62.62.83.21.9.1" style="background-color:#F0F0F0;">RGB-D</span></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.10"></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.11"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.12">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.83.21.12.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21.12.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.83.21.12.1.1.1">Seed sampling</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.13">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.83.21.13.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.83.21.13.1.1.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.83.21.13.1.1.1.1">arch:</span> Multi-scale CNN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.83.21.13.1.2.1">and FCN</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21.13.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.83.21.13.1.3.1">
<span class="ltx_text ltx_font_italic" id="S2.T2.62.62.62.62.83.21.13.1.3.1.1">out:</span> Boundary map</td>
</tr>
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21.13.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.83.21.13.1.4.1">and superpixels</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.14">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.62.62.62.62.83.21.14.1" style="background-color:#F0F0F0;">
<tr class="ltx_tr" id="S2.T2.62.62.62.62.83.21.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.62.62.62.62.83.21.14.1.1.1">Merging step</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.15"></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.16"></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.17"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.62.62.62.62.83.21.18"><span class="ltx_text" id="S2.T2.62.62.62.62.83.21.18.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_border_bb" id="S2.T2.62.62.62.62.83.21.19"></td>
<td class="ltx_td ltx_nopad_r ltx_border_bb" id="S2.T2.62.62.62.62.83.21.20"></td>
</tr>
</tbody>
</table>
</span></div>
<p class="ltx_p" id="S2.T2.74.74"><sup class="ltx_sup" id="S2.T2.74.74.12"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.12.1" style="font-size:80%;">a</span></sup><span class="ltx_text" id="S2.T2.74.74.11" style="font-size:80%;"> With post-processing. <sup class="ltx_sup" id="S2.T2.74.74.11.1"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.1.1">b</span></sup> <math alttext="k" class="ltx_Math" display="inline" id="S2.T2.65.65.2.m2.1"><semantics id="S2.T2.65.65.2.m2.1a"><mi id="S2.T2.65.65.2.m2.1.1" xref="S2.T2.65.65.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.T2.65.65.2.m2.1b"><ci id="S2.T2.65.65.2.m2.1.1.cmml" xref="S2.T2.65.65.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.65.65.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.T2.65.65.2.m2.1d">italic_k</annotation></semantics></math> is the number of iterations and <math alttext="z" class="ltx_Math" display="inline" id="S2.T2.66.66.3.m3.1"><semantics id="S2.T2.66.66.3.m3.1a"><mi id="S2.T2.66.66.3.m3.1.1" xref="S2.T2.66.66.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.T2.66.66.3.m3.1b"><ci id="S2.T2.66.66.3.m3.1.1.cmml" xref="S2.T2.66.66.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.66.66.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S2.T2.66.66.3.m3.1d">italic_z</annotation></semantics></math> represents the number of small isolated superpixels to be merged. <sup class="ltx_sup" id="S2.T2.74.74.11.2"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.2.1">c</span></sup> Partially. <sup class="ltx_sup" id="S2.T2.74.74.11.3"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.3.1">d</span></sup> <math alttext="t" class="ltx_Math" display="inline" id="S2.T2.69.69.6.m6.1"><semantics id="S2.T2.69.69.6.m6.1a"><mi id="S2.T2.69.69.6.m6.1.1" xref="S2.T2.69.69.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.T2.69.69.6.m6.1b"><ci id="S2.T2.69.69.6.m6.1.1.cmml" xref="S2.T2.69.69.6.m6.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.69.69.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.T2.69.69.6.m6.1d">italic_t</annotation></semantics></math> is the number of relocations. <sup class="ltx_sup" id="S2.T2.74.74.11.4"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.4.1">e</span></sup> Without the saliency map computation. <sup class="ltx_sup" id="S2.T2.74.74.11.5"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.5.1">f</span></sup> <math alttext="d" class="ltx_Math" display="inline" id="S2.T2.72.72.9.m9.1"><semantics id="S2.T2.72.72.9.m9.1a"><mi id="S2.T2.72.72.9.m9.1.1" xref="S2.T2.72.72.9.m9.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.T2.72.72.9.m9.1b"><ci id="S2.T2.72.72.9.m9.1.1.cmml" xref="S2.T2.72.72.9.m9.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.72.72.9.m9.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.T2.72.72.9.m9.1d">italic_d</annotation></semantics></math> is the number of hierarchy levels. <sup class="ltx_sup" id="S2.T2.74.74.11.6"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.6.1">g</span></sup> Time complexity in HERS module. <sup class="ltx_sup" id="S2.T2.74.74.11.7"><span class="ltx_text ltx_font_italic" id="S2.T2.74.74.11.7.1">h</span></sup> Without parallelization.</span></p>
</div>
</figure>
<div class="ltx_para" id="S2.SS3.p8">
<p class="ltx_p" id="S2.SS3.p8.2">The deep learning-based approaches are all end-to-end trainable. However, the aforementioned deep-based methods train soft pixel-superpixel assignments, requiring a post-processing step to compute hard associations. The interpolation network in SIN <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib139" title="">2021a</a>)</cite> overcomes it by extracting features with convolutional operations followed by multiple interpolations to expand the pixel-superpixel association matrix while enforcing spatial connectivity.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.T2" title="Table 2 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> presents superpixel methods according to the proposed taxonomy, their superpixel properties (second to third columns), color space, time complexity (when available), and inspiration method (if any).
<span class="ltx_text" id="S2.SS3.p8.2.1" style="color:#000000;"> However, instead of indicating the pixel-superpixel assignment category in the <span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.1.1">Main Processing</span> of deep learning methods (as in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.F3" title="Figure 3 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>), we complement it by informing, along with the network output (<span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.1.2">out</span>), its architecture (<span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.1.3">arch</span>) for methods with CNN in any processing step. As far as we know, there is no categorization for deep convolutional networks. Therefore, we classify each architecture according to its most important aspect.</span>
In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.T2" title="Table 2 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, the superpixel properties are whether a method is iterative (Iterative), its control over the number of iterations (<math alttext="\#" class="ltx_Math" display="inline" id="S2.SS3.p8.1.m1.1"><semantics id="S2.SS3.p8.1.m1.1a"><mi id="S2.SS3.p8.1.m1.1.1" mathvariant="normal" xref="S2.SS3.p8.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p8.1.m1.1b"><ci id="S2.SS3.p8.1.m1.1.1.cmml" xref="S2.SS3.p8.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p8.1.m1.1c">\#</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p8.1.m1.1d">#</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.2">Iter.</span>) and the number of superpixels (<math alttext="\#" class="ltx_Math" display="inline" id="S2.SS3.p8.2.m2.1"><semantics id="S2.SS3.p8.2.m2.1a"><mi id="S2.SS3.p8.2.m2.1.1" mathvariant="normal" xref="S2.SS3.p8.2.m2.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p8.2.m2.1b"><ci id="S2.SS3.p8.2.m2.1.1.cmml" xref="S2.SS3.p8.2.m2.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p8.2.m2.1c">\#</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p8.2.m2.1d">#</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.3">Superp.</span>), whether its superpixels are connected (<span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.4">Connec.</span>) and compact (<span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.5">Compact.</span>), and if the network training (if any) is supervised (<span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.6">Superv.</span>).
One may note that a method may perform several procedures in a processing step, implying that more than one category may appear. For instance, DSR, Semasuperpixel, ODISF, SICLE, EAM, and ECCPD have two categories each in the initial processing, since each one performs two distinct processes before the main processing (<span class="ltx_text ltx_font_italic" id="S2.SS3.p8.2.7">i.e.</span>, before the clustering strategy). The reader should refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A1" title="Appendix A Superpixel segmentation methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">A</span></a> for a detailed description of each method in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S2.T2" title="Table 2 ‣ 2.3. The proposed taxonomy in superpixel literature ‣ 2. Taxonomy of superpixel methods ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Benchmark</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Superpixel methods</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.4">In this work, we identified <math alttext="17" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn id="S3.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">17</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">17</annotation></semantics></math> open source codes from the recent superpixel literature: <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.1">AINET</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib127" title="">2021a</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.2">SIN</span> <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib139" title="">2021a</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.3">SSFCN</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.4">DISF</span> <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.5">RSS</span> <cite class="ltx_cite ltx_citemacro_citep">(Chai, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib22" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.6">ODISF</span> <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.7">IBIS</span> <cite class="ltx_cite ltx_citemacro_citep">(Bobbia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib18" title="">2021</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.8">DRW</span> <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib58" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.9">DAL-HERS</span> <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib90" title="">2022</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.10">ISF</span> <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.11">GMMSP</span> <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.12">SCALP</span> <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.13">SNIC</span> <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.14">SH</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib128" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.15">LNSNet</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib148" title="">2021</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.16">SICLE</span> <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib12" title="">2022a</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.17">LSC</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib24" title="">2017a</a>; Li and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib68" title="">2015</a>)</cite>. In addition, we include the <math alttext="6" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">6</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">6</annotation></semantics></math> methods recommended as state-of-the-art in <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>: <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.18">SLIC</span> <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.19">SEEDS</span> <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.20">ERS</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib72" title="">2011</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.21">ETPS</span> <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.22">CRS</span> <cite class="ltx_cite ltx_citemacro_citep">(Conrad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib29" title="">2013</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.23">ERGC</span> <cite class="ltx_cite ltx_citemacro_citep">(Buyssens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib21" title="">2014</a>)</cite>. Finally, a grid segmentation (<span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.24">GRID</span>) was used as a baseline.
Regarding implementation, we used the <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.25">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.26">CRS</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.27">ERGC</span> code available in the benchmark of Stutz et al <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>. Also, we implemented grid segmentation. For the other methods, we use the original authors’ code.
Concerning the method’s parameters, we use those recommended by the original works, since fine-tuning them may result in a worse parameter setting than the original ones, and tuning them for each dataset does not assess the methods’ generalization ability.
All evaluated methods allow some control over the number of superpixels generated. In our experiments, we assess segmentations with <math alttext="K\approx\{25,50,75,100,200,300,400,500,600,700,800,900,1000\}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.13"><semantics id="S3.SS1.p1.3.m3.13a"><mrow id="S3.SS1.p1.3.m3.13.14" xref="S3.SS1.p1.3.m3.13.14.cmml"><mi id="S3.SS1.p1.3.m3.13.14.2" xref="S3.SS1.p1.3.m3.13.14.2.cmml">K</mi><mo id="S3.SS1.p1.3.m3.13.14.1" xref="S3.SS1.p1.3.m3.13.14.1.cmml">≈</mo><mrow id="S3.SS1.p1.3.m3.13.14.3.2" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml"><mo id="S3.SS1.p1.3.m3.13.14.3.2.1" stretchy="false" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">{</mo><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">25</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.2" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">50</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.3" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.3.3" xref="S3.SS1.p1.3.m3.3.3.cmml">75</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.4" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.4.4" xref="S3.SS1.p1.3.m3.4.4.cmml">100</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.5" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.5.5" xref="S3.SS1.p1.3.m3.5.5.cmml">200</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.6" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.6.6" xref="S3.SS1.p1.3.m3.6.6.cmml">300</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.7" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.7.7" xref="S3.SS1.p1.3.m3.7.7.cmml">400</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.8" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.8.8" xref="S3.SS1.p1.3.m3.8.8.cmml">500</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.9" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.9.9" xref="S3.SS1.p1.3.m3.9.9.cmml">600</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.10" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.10.10" xref="S3.SS1.p1.3.m3.10.10.cmml">700</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.11" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.11.11" xref="S3.SS1.p1.3.m3.11.11.cmml">800</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.12" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.12.12" xref="S3.SS1.p1.3.m3.12.12.cmml">900</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.13" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.13.13" xref="S3.SS1.p1.3.m3.13.13.cmml">1000</mn><mo id="S3.SS1.p1.3.m3.13.14.3.2.14" stretchy="false" xref="S3.SS1.p1.3.m3.13.14.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.13b"><apply id="S3.SS1.p1.3.m3.13.14.cmml" xref="S3.SS1.p1.3.m3.13.14"><approx id="S3.SS1.p1.3.m3.13.14.1.cmml" xref="S3.SS1.p1.3.m3.13.14.1"></approx><ci id="S3.SS1.p1.3.m3.13.14.2.cmml" xref="S3.SS1.p1.3.m3.13.14.2">𝐾</ci><set id="S3.SS1.p1.3.m3.13.14.3.1.cmml" xref="S3.SS1.p1.3.m3.13.14.3.2"><cn id="S3.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1">25</cn><cn id="S3.SS1.p1.3.m3.2.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2">50</cn><cn id="S3.SS1.p1.3.m3.3.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.3.3">75</cn><cn id="S3.SS1.p1.3.m3.4.4.cmml" type="integer" xref="S3.SS1.p1.3.m3.4.4">100</cn><cn id="S3.SS1.p1.3.m3.5.5.cmml" type="integer" xref="S3.SS1.p1.3.m3.5.5">200</cn><cn id="S3.SS1.p1.3.m3.6.6.cmml" type="integer" xref="S3.SS1.p1.3.m3.6.6">300</cn><cn id="S3.SS1.p1.3.m3.7.7.cmml" type="integer" xref="S3.SS1.p1.3.m3.7.7">400</cn><cn id="S3.SS1.p1.3.m3.8.8.cmml" type="integer" xref="S3.SS1.p1.3.m3.8.8">500</cn><cn id="S3.SS1.p1.3.m3.9.9.cmml" type="integer" xref="S3.SS1.p1.3.m3.9.9">600</cn><cn id="S3.SS1.p1.3.m3.10.10.cmml" type="integer" xref="S3.SS1.p1.3.m3.10.10">700</cn><cn id="S3.SS1.p1.3.m3.11.11.cmml" type="integer" xref="S3.SS1.p1.3.m3.11.11">800</cn><cn id="S3.SS1.p1.3.m3.12.12.cmml" type="integer" xref="S3.SS1.p1.3.m3.12.12">900</cn><cn id="S3.SS1.p1.3.m3.13.13.cmml" type="integer" xref="S3.SS1.p1.3.m3.13.13">1000</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.13c">K\approx\{25,50,75,100,200,300,400,500,600,700,800,900,1000\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.13d">italic_K ≈ { 25 , 50 , 75 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 }</annotation></semantics></math> desired superpixels, except for the robustness evaluation, with only <math alttext="K\approx 400" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">K</mi><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">≈</mo><mn id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><approx id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></approx><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝐾</ci><cn id="S3.SS1.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">K\approx 400</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_K ≈ 400</annotation></semantics></math> superpixels.
In the following, we briefly present the superpixel methods used in our benchmark.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">SLIC</span> <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.2">SCALP</span> <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.3">LSC</span> <cite class="ltx_cite ltx_citemacro_citep">(Li and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib68" title="">2015</a>)</cite> perform <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">neighborhood-based</span> clustering, in which the clustering strategy comprises a distance function limited to a region concerning a reference point in the image. In these three methods, the reference point consists of the center of each cluster, and the search region size depends on the expected superpixel size.
<span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.5">SLIC</span> is an iterative method based on k-means whose superpixel centers start with a simple grid sampling and its distance measurement, which is based only on color, spatial position, and superpixel area, gives better control over the size and compactness of the superpixels. In <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.6">SCALP</span>, the distance function from a center to a pixel is weighted according to the linear path between these two points using a boundary map. On the other hand, <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.7">LSC</span> explores features at the pixel level, mapping them into 10-dimensional points. Similarly, <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.8">SNIC</span> <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.9">DRW</span> <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib58" title="">2020</a>)</cite> use a <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.10">dynamic-center-update</span> clustering strategy. Based on <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.11">SLIC</span>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.12">SNIC</span> guarantees the connectivity of its superpixels during clustering and does not require multiple iterations. Conversely, <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.13">DRW</span> formulates the clustering problem based on the Random Walk algorithm <cite class="ltx_cite ltx_citemacro_citep">(Grady, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib46" title="">2006</a>)</cite> and adds dynamic nodes to the graph to reduce redundant computation and capture features at the region level.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">CRS</span> <cite class="ltx_cite ltx_citemacro_citep">(Conrad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib29" title="">2013</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.2">SEEDS</span> <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.3">ETPS</span> <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.4">IBIS</span> <cite class="ltx_cite ltx_citemacro_citep">(Bobbia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib18" title="">2021</a>)</cite> perform a <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.5">boundary evolution</span> clustering, which begins with a grid segmentation and updates the superpixel contours according to an energy function. <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.6">CRS</span> updates the pixel-superpixel assignment based on the image content and the <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.7">Gibbs-Markov random field</span> model, improving the segmentation through the iterations. In contrast, <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.8">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.9">ETPS</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.10">IBIS</span> evaluate the superpixel boundaries with a coarse-to-fine strategy, explicitly dividing the image blocks. <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.11">SEEDS</span> uses an approach based on the <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.12">Hill-Climbing</span> algorithm and an optimization function with characteristics based on the color histogram. On the other hand, <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.13">ETPS</span> orders the superpixel boundaries evaluation using a priority queue. Its optimization function uses features at the pixel level to optimize homogeneity, compactness, size, and smoothness. Conversely, <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.14">IBIS</span> focuses on efficiency by employing a parallel coarse-to-fine strategy to optimize a SLIC-based distance function.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">In contrast, the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">path-based</span> clustering methods <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.2">ERGC</span> <cite class="ltx_cite ltx_citemacro_citep">(Buyssens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib21" title="">2014</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.3">RSS</span> <cite class="ltx_cite ltx_citemacro_citep">(Chai, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib22" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.4">ISF</span> <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.5">DISF</span> <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.6">ODISF</span> <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.7">SICLE</span> <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib14" title="">2022b</a>)</cite> usually focus on boundary adherence instead of compactness. While <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.8">RSS</span> provides a non-iterative method that guarantees the optimality of the generated forest, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.9">ISF</span>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.10">DISF</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.11">ODISF</span> use iterative strategies. The <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.12">ISF</span> recalculates the position of the seeds at the end of each iteration. At the same time, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.13">DISF</span>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.14">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.15">SICLE</span> perform an initial oversampling and further remove the less relevant seeds at the end of each iteration. While <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.16">DISF</span> only uses pixel and path-based characteristics, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.17">ODISF</span> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.18">SICLE</span> include saliency information in their removal step, but only <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.19">SICLE</span> allows to control the saliency importance. Unlike the others, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.20">ERGC</span> formulates the segmentation with the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.21">Eikonal</span> equation, solving it with the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.22">Fast Marching Algorithm</span> <cite class="ltx_cite ltx_citemacro_citep">(Sethian, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib102" title="">1999</a>)</cite>, which calculates the minimum geodesic paths of the graph. Similar to the path-based methods, the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.23">hierarchical</span> ones usually prioritize boundary adherence. However, most of them do not require several iterations to generate superpixels, and their hierarchical structure provides several segmentation levels (also called scales) with a unique execution. The <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.24">SH</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib128" title="">2018</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.25">DAL-HERS</span> <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib90" title="">2022</a>)</cite> produce a superpixel <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.26">hierarchy</span> according to locality and causality criteria <cite class="ltx_cite ltx_citemacro_citep">(Guigues et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib51" title="">2006</a>)</cite>. While <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.27">SH</span> relies on <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.28">Boruvka’s algorithm</span> <cite class="ltx_cite ltx_citemacro_citep">(West et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib129" title="">2001</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.29">DAL-HERS</span> generates affinity maps with a residual convolutional network and uses these maps to create a superpixel hierarchy.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Concerning methods that generate superpixels using <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.1">deep neural architecture</span>, <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.2">SSFCN</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.3">AINET</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib127" title="">2021a</a>)</cite>, they employ u-shaped networks to extract soft pixel-superpixel assignment using a supervised strategy. The former directly outputs a soft pixel-superpixel association map, while the latter employs a new <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.4">Association Implantation</span> module to compute the soft assignment. <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.5">AINET</span> also uses a boundary-perceiving loss to improve boundary delineation. Conversely, <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.6">LNSNet</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib148" title="">2021</a>)</cite> relies on a non-iterative clustering module and employs a lifelong learning strategy that does not require ground truth labels. Unlike previous deep-based architectures, <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.7">SIN</span> <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib139" title="">2021a</a>)</cite> uses an interpolation network composed of fully convolutional layers to extract multi-layer features used in interpolation layers as association scores. These scores are used in multiple vertical and horizontal interpolation steps to expand the pixel-superpixel association matrix while enforcing spatial connectivity.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Finally, <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">GMMSP</span> <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite> models the segmentation task as a weighted sum of Gaussians, each associated with a superpixel. On the other hand, <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.2">ERS</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib72" title="">2011</a>)</cite> models it using <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.3">Random Walk</span> <cite class="ltx_cite ltx_citemacro_citep">(Grady, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib46" title="">2006</a>)</cite> and generates superpixels from the cut in the image graph that optimizes its function.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Datasets</h3>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Characteristics of the five datasets used in this work to evaluate superpixels.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.10">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.10.11.1">
<td class="ltx_td ltx_border_tt" id="S3.T3.10.11.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.10.11.1.2">Birds</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.10.11.1.3">Insects</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.10.11.1.4">Sky</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.10.11.1.5">ECSSD</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.10.11.1.6">NYUV2</td>
</tr>
<tr class="ltx_tr" id="S3.T3.10.12.2">
<td class="ltx_td ltx_align_center" id="S3.T3.10.12.2.1">Image content</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.10.12.2.2">Natural</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.10.12.2.3">Natural</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.10.12.2.4">Natural</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.10.12.2.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.10.12.2.5.1">
<tr class="ltx_tr" id="S3.T3.10.12.2.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.10.12.2.5.1.1.1">Natural and urban</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.10.12.2.6">Indoor</td>
</tr>
<tr class="ltx_tr" id="S3.T3.10.13.3">
<td class="ltx_td ltx_align_center" id="S3.T3.10.13.3.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.10.13.3.1.1">
<tr class="ltx_tr" id="S3.T3.10.13.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.10.13.3.1.1.1.1">Number of images</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.13.3.2">150</td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.13.3.3">130</td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.13.3.4">60</td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.13.3.5">1,000</td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.13.3.6">1,449</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5">
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.6">Minimum image size</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1"><math alttext="300\times 300" class="ltx_Math" display="inline" id="S3.T3.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mn id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2.cmml">300</mn><mo id="S3.T3.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.m1.1.1.3.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><times id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1.1"></times><cn id="S3.T3.1.1.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.1.1.1.m1.1.1.2">300</cn><cn id="S3.T3.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.1.1.1.m1.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">300\times 300</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.m1.1d">300 × 300</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2"><math alttext="640\times 359" class="ltx_Math" display="inline" id="S3.T3.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.m1.1a"><mrow id="S3.T3.2.2.2.m1.1.1" xref="S3.T3.2.2.2.m1.1.1.cmml"><mn id="S3.T3.2.2.2.m1.1.1.2" xref="S3.T3.2.2.2.m1.1.1.2.cmml">640</mn><mo id="S3.T3.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S3.T3.2.2.2.m1.1.1.3" xref="S3.T3.2.2.2.m1.1.1.3.cmml">359</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><apply id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1"><times id="S3.T3.2.2.2.m1.1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1.1"></times><cn id="S3.T3.2.2.2.m1.1.1.2.cmml" type="integer" xref="S3.T3.2.2.2.m1.1.1.2">640</cn><cn id="S3.T3.2.2.2.m1.1.1.3.cmml" type="integer" xref="S3.T3.2.2.2.m1.1.1.3">359</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">640\times 359</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.m1.1d">640 × 359</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3"><math alttext="599\times 399" class="ltx_Math" display="inline" id="S3.T3.3.3.3.m1.1"><semantics id="S3.T3.3.3.3.m1.1a"><mrow id="S3.T3.3.3.3.m1.1.1" xref="S3.T3.3.3.3.m1.1.1.cmml"><mn id="S3.T3.3.3.3.m1.1.1.2" xref="S3.T3.3.3.3.m1.1.1.2.cmml">599</mn><mo id="S3.T3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.3.3.3.m1.1.1.1.cmml">×</mo><mn id="S3.T3.3.3.3.m1.1.1.3" xref="S3.T3.3.3.3.m1.1.1.3.cmml">399</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.m1.1b"><apply id="S3.T3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1"><times id="S3.T3.3.3.3.m1.1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1.1"></times><cn id="S3.T3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S3.T3.3.3.3.m1.1.1.2">599</cn><cn id="S3.T3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S3.T3.3.3.3.m1.1.1.3">399</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.m1.1c">599\times 399</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.m1.1d">599 × 399</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4"><math alttext="400\times 139" class="ltx_Math" display="inline" id="S3.T3.4.4.4.m1.1"><semantics id="S3.T3.4.4.4.m1.1a"><mrow id="S3.T3.4.4.4.m1.1.1" xref="S3.T3.4.4.4.m1.1.1.cmml"><mn id="S3.T3.4.4.4.m1.1.1.2" xref="S3.T3.4.4.4.m1.1.1.2.cmml">400</mn><mo id="S3.T3.4.4.4.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.4.4.4.m1.1.1.1.cmml">×</mo><mn id="S3.T3.4.4.4.m1.1.1.3" xref="S3.T3.4.4.4.m1.1.1.3.cmml">139</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.m1.1b"><apply id="S3.T3.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.m1.1.1"><times id="S3.T3.4.4.4.m1.1.1.1.cmml" xref="S3.T3.4.4.4.m1.1.1.1"></times><cn id="S3.T3.4.4.4.m1.1.1.2.cmml" type="integer" xref="S3.T3.4.4.4.m1.1.1.2">400</cn><cn id="S3.T3.4.4.4.m1.1.1.3.cmml" type="integer" xref="S3.T3.4.4.4.m1.1.1.3">139</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.m1.1c">400\times 139</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.m1.1d">400 × 139</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5"><math alttext="608\times 448" class="ltx_Math" display="inline" id="S3.T3.5.5.5.m1.1"><semantics id="S3.T3.5.5.5.m1.1a"><mrow id="S3.T3.5.5.5.m1.1.1" xref="S3.T3.5.5.5.m1.1.1.cmml"><mn id="S3.T3.5.5.5.m1.1.1.2" xref="S3.T3.5.5.5.m1.1.1.2.cmml">608</mn><mo id="S3.T3.5.5.5.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.5.5.5.m1.1.1.1.cmml">×</mo><mn id="S3.T3.5.5.5.m1.1.1.3" xref="S3.T3.5.5.5.m1.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.m1.1b"><apply id="S3.T3.5.5.5.m1.1.1.cmml" xref="S3.T3.5.5.5.m1.1.1"><times id="S3.T3.5.5.5.m1.1.1.1.cmml" xref="S3.T3.5.5.5.m1.1.1.1"></times><cn id="S3.T3.5.5.5.m1.1.1.2.cmml" type="integer" xref="S3.T3.5.5.5.m1.1.1.2">608</cn><cn id="S3.T3.5.5.5.m1.1.1.3.cmml" type="integer" xref="S3.T3.5.5.5.m1.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.m1.1c">608\times 448</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.5.m1.1d">608 × 448</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T3.10.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.10.10.6">Maximum image size</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.1"><math alttext="640\times 640" class="ltx_Math" display="inline" id="S3.T3.6.6.1.m1.1"><semantics id="S3.T3.6.6.1.m1.1a"><mrow id="S3.T3.6.6.1.m1.1.1" xref="S3.T3.6.6.1.m1.1.1.cmml"><mn id="S3.T3.6.6.1.m1.1.1.2" xref="S3.T3.6.6.1.m1.1.1.2.cmml">640</mn><mo id="S3.T3.6.6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.6.6.1.m1.1.1.1.cmml">×</mo><mn id="S3.T3.6.6.1.m1.1.1.3" xref="S3.T3.6.6.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.m1.1b"><apply id="S3.T3.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1"><times id="S3.T3.6.6.1.m1.1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1.1"></times><cn id="S3.T3.6.6.1.m1.1.1.2.cmml" type="integer" xref="S3.T3.6.6.1.m1.1.1.2">640</cn><cn id="S3.T3.6.6.1.m1.1.1.3.cmml" type="integer" xref="S3.T3.6.6.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.m1.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.1.m1.1d">640 × 640</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.7.7.2"><math alttext="640\times 640" class="ltx_Math" display="inline" id="S3.T3.7.7.2.m1.1"><semantics id="S3.T3.7.7.2.m1.1a"><mrow id="S3.T3.7.7.2.m1.1.1" xref="S3.T3.7.7.2.m1.1.1.cmml"><mn id="S3.T3.7.7.2.m1.1.1.2" xref="S3.T3.7.7.2.m1.1.1.2.cmml">640</mn><mo id="S3.T3.7.7.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.7.7.2.m1.1.1.1.cmml">×</mo><mn id="S3.T3.7.7.2.m1.1.1.3" xref="S3.T3.7.7.2.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.2.m1.1b"><apply id="S3.T3.7.7.2.m1.1.1.cmml" xref="S3.T3.7.7.2.m1.1.1"><times id="S3.T3.7.7.2.m1.1.1.1.cmml" xref="S3.T3.7.7.2.m1.1.1.1"></times><cn id="S3.T3.7.7.2.m1.1.1.2.cmml" type="integer" xref="S3.T3.7.7.2.m1.1.1.2">640</cn><cn id="S3.T3.7.7.2.m1.1.1.3.cmml" type="integer" xref="S3.T3.7.7.2.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.2.m1.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.7.2.m1.1d">640 × 640</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.8.8.3"><math alttext="825\times 600" class="ltx_Math" display="inline" id="S3.T3.8.8.3.m1.1"><semantics id="S3.T3.8.8.3.m1.1a"><mrow id="S3.T3.8.8.3.m1.1.1" xref="S3.T3.8.8.3.m1.1.1.cmml"><mn id="S3.T3.8.8.3.m1.1.1.2" xref="S3.T3.8.8.3.m1.1.1.2.cmml">825</mn><mo id="S3.T3.8.8.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.8.8.3.m1.1.1.1.cmml">×</mo><mn id="S3.T3.8.8.3.m1.1.1.3" xref="S3.T3.8.8.3.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.3.m1.1b"><apply id="S3.T3.8.8.3.m1.1.1.cmml" xref="S3.T3.8.8.3.m1.1.1"><times id="S3.T3.8.8.3.m1.1.1.1.cmml" xref="S3.T3.8.8.3.m1.1.1.1"></times><cn id="S3.T3.8.8.3.m1.1.1.2.cmml" type="integer" xref="S3.T3.8.8.3.m1.1.1.2">825</cn><cn id="S3.T3.8.8.3.m1.1.1.3.cmml" type="integer" xref="S3.T3.8.8.3.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.3.m1.1c">825\times 600</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.8.3.m1.1d">825 × 600</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.9.9.4"><math alttext="400\times 400" class="ltx_Math" display="inline" id="S3.T3.9.9.4.m1.1"><semantics id="S3.T3.9.9.4.m1.1a"><mrow id="S3.T3.9.9.4.m1.1.1" xref="S3.T3.9.9.4.m1.1.1.cmml"><mn id="S3.T3.9.9.4.m1.1.1.2" xref="S3.T3.9.9.4.m1.1.1.2.cmml">400</mn><mo id="S3.T3.9.9.4.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.9.9.4.m1.1.1.1.cmml">×</mo><mn id="S3.T3.9.9.4.m1.1.1.3" xref="S3.T3.9.9.4.m1.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.4.m1.1b"><apply id="S3.T3.9.9.4.m1.1.1.cmml" xref="S3.T3.9.9.4.m1.1.1"><times id="S3.T3.9.9.4.m1.1.1.1.cmml" xref="S3.T3.9.9.4.m1.1.1.1"></times><cn id="S3.T3.9.9.4.m1.1.1.2.cmml" type="integer" xref="S3.T3.9.9.4.m1.1.1.2">400</cn><cn id="S3.T3.9.9.4.m1.1.1.3.cmml" type="integer" xref="S3.T3.9.9.4.m1.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.4.m1.1c">400\times 400</annotation><annotation encoding="application/x-llamapun" id="S3.T3.9.9.4.m1.1d">400 × 400</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.10.10.5"><math alttext="608\times 448" class="ltx_Math" display="inline" id="S3.T3.10.10.5.m1.1"><semantics id="S3.T3.10.10.5.m1.1a"><mrow id="S3.T3.10.10.5.m1.1.1" xref="S3.T3.10.10.5.m1.1.1.cmml"><mn id="S3.T3.10.10.5.m1.1.1.2" xref="S3.T3.10.10.5.m1.1.1.2.cmml">608</mn><mo id="S3.T3.10.10.5.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T3.10.10.5.m1.1.1.1.cmml">×</mo><mn id="S3.T3.10.10.5.m1.1.1.3" xref="S3.T3.10.10.5.m1.1.1.3.cmml">448</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.5.m1.1b"><apply id="S3.T3.10.10.5.m1.1.1.cmml" xref="S3.T3.10.10.5.m1.1.1"><times id="S3.T3.10.10.5.m1.1.1.1.cmml" xref="S3.T3.10.10.5.m1.1.1.1"></times><cn id="S3.T3.10.10.5.m1.1.1.2.cmml" type="integer" xref="S3.T3.10.10.5.m1.1.1.2">608</cn><cn id="S3.T3.10.10.5.m1.1.1.3.cmml" type="integer" xref="S3.T3.10.10.5.m1.1.1.3">448</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.5.m1.1c">608\times 448</annotation><annotation encoding="application/x-llamapun" id="S3.T3.10.10.5.m1.1d">608 × 448</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We selected five datasets which impose different challenges for superpixel segmentation: <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Birds</span> <cite class="ltx_cite ltx_citemacro_citep">(Mansilla and Miranda, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib81" title="">2016</a>)</cite>; <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Insects</span> <cite class="ltx_cite ltx_citemacro_citep">(Mansilla and Miranda, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib81" title="">2016</a>)</cite>; <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">Sky</span> <cite class="ltx_cite ltx_citemacro_citep">(Alexandre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib4" title="">2015</a>)</cite>; <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4">ECSSD</span> <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib107" title="">2015</a>)</cite>; and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.5">NYUV2</span> <cite class="ltx_cite ltx_citemacro_citep">(Silberman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib108" title="">2012</a>)</cite>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S3.T3" title="Table 3 ‣ 3.2. Datasets ‣ 3. Benchmark ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes their main characteristics.
Birds, Insects, and Sky have natural images. The former contains images of birds, which have thin and elongated parts, hard to delineate with compact superpixels. When there are more birds, they often overlap, making it difficult to delineate them separately. In the Birds dataset, the background does not have a specific pattern and may (or may not) be blurred, colored, and textured. Similarly, the Insects dataset has images containing one or more insects with thin and elongated parts. Compared to the Birds dataset, it has more blurred and less textured backgrounds, and their objects (the insects) have thinner parts. Therefore, it has more challenging objects but less difficult backgrounds than the Birds dataset. In contrast, the Sky dataset has images with one plane each. Most images in the Sky dataset have large regions with low color and subtle luminosity variations. The ground truth in Birds, Insects, and Sky datasets are binary masks with only one connected object per image. The objects in Birds, Insects, and Sky, are a bird, an insect, and the sky, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In contrast with the aforementioned datasets, ECSSD and NYUV2 datasets have urban scenes. Specifically, the ECSSD dataset has images in both natural and urban environments, while NYUV2 is composed of video sequences from several indoor scenes recorded by Microsoft Kinect. The images in the ECSSD dataset have complex scenes, most with non-uniform regions and backgrounds composed of several parts. In the ECSSD dataset, the images may have more objects, many without well-defined boundaries. Furthermore, some objects have transparency, which makes them difficult to identify. Conversely, the RGBD images in the NYUV2 dataset have rich geometric structures with large planar surfaces, such as the floor, walls, and table tops. Its images also have small objects and occlusion, accentuated by the mess and disorder common in inhabited environments. In the ECSSD dataset the ground truth images are binary masks, each one with at least one connected object. In the NYUV2 dataset, the ground truth images have dense multi-class labels. However, for those in the NYUV2 dataset, we remove unlabeled pixels similar to <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Evaluation criteria</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Connectivity</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Connectivity is one of the most fundamental properties in superpixel segmentation. Superpixels are connected when their pixels form a connected component considering the X and Y axes. Also, some superpixel methods may consider pixels in diagonal on the X and Y axes as connected. However, several superpixel approaches fail to meet this property. Specifically, most methods with deep networks in their Main Processing step still struggle to produce superpixels, since they cannot provide a hard pixel-superpixel assignment. This work evaluates connectivity considering the eight neighbors on the X and Y axes — <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.1.1">i.e.</span>, including diagonals. We also perform a simple post-processing to ensure connectivity that gives a unique label to each connected component. Then, to maintain the generated number of superpixel labels, we merge the superpixel with fewer pixels and its most similar adjacent superpixel, considering the mean color. The merging step continues until the number of superpixels achieves the number of labels. For example, considering a method that generates 105 superpixel labels but has 200 connected components, the aforementioned post-processing step ensures 105 connected superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Control over the number of superpixels</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Controlling the number of superpixels is also of utmost importance. In image segmentation, one may perform different segmentation to achieve distinct goals. For instance, object segmentation aims to divide images into object and background regions, while semantic segmentation densely labels pixels to each label associated with a semantic meaning. Conversely, in superpixel segmentation, each labeled region must have pixels with similar characteristics (usually color). Furthermore, one must delineate image objects by merging superpixels, and the number of superpixels may vary, typically being parameterized. Such control is important when using superpixel segmentation as preprocessing in other tasks. For instance, a very high number of superpixels may not effectively reduce redundant information or image primitives. Likewise, very few superpixels may result in lost important information due to non-homogeneous regions. In this work, we evaluate the number of superpixels generated considering the number of distinct labels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span>Boundary delineation</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Boundary adherence concerns the ability to superpixel borders to adhere to the object borders. Most superpixel methods evaluate their boundary adherence with quantitative and qualitative evaluation. In superpixel segmentation, the quantitative evaluation involves using ground truth images with binary masks or dense multi-class labels. However, such images may consider only some of the objects. Therefore, a quantitative analysis may be insufficient. Several measures evaluate superpixel boundary adherence, but some of them have important drawbacks, while others have a high correlation <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>. Following Stutz et al. <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, we use <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS3.p1.1.1">Boundary Recall</span> <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib82" title="">2004</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS3.p1.1.2">Undersegmentation Error</span> <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite> to evaluate boundary adherence.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4. </span>Color homogeneity</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">Color homogeneity relates to the inner color similarity in superpixels. As the union of superpixels must be able to delineate image objects, more homogeneous superpixels tend to contain information from a single object. In superpixel literature, this property is quantitatively evaluated and is independent of the image ground truth. One may initially define color homogeneity as the simple color variation in superpixels concerning the image color variation. However, regions with textures such as grass to water are visually homogeneous, increasing the difficulty in homogeneity assessment. In this work, we quantitatively assess color homogeneity with <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS4.p1.1.1">Explained Variation</span> <cite class="ltx_cite ltx_citemacro_citep">(Moore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib87" title="">2008</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS4.p1.1.2">Similarity between Image and Reconstruction from Superpixels</span> <cite class="ltx_cite ltx_citemacro_citep">(Barcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib9" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.5. </span>Compactness</h4>
<div class="ltx_para" id="S3.SS3.SSS5.p1">
<p class="ltx_p" id="S3.SS3.SSS5.p1.1">Compact superpixels have convex and regular shapes. Previous works demonstrate that highly adherent superpixels do not necessarily produce better segmentations <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>; Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>; Stutz, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib109" title="">2015</a>; Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>. Specifically, Schick et al. <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib99" title="">2012</a>)</cite> demonstrated an inverse and non-linear association between compactness and boundary adherence, and they argue that highly adherent superpixels are similar to overfitting the image boundaries, not necessarily capturing the most important ones. Compactness is usually quantitative and qualitatively assessed. In this work, we evaluate compactness using the <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS5.p1.1.1">Compactness index</span> <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib99" title="">2012</a>)</cite> and include its qualitative analysis in our visual quality criteria.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.6. </span>Stability</h4>
<div class="ltx_para" id="S3.SS3.SSS6.p1">
<p class="ltx_p" id="S3.SS3.SSS6.p1.1">Stability is not a common evaluation criterion in superpixel segmentation, being first conducted in <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite> to evaluate the superpixel stability of affine transformations. In <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, stability is redefined to consider stable the segmentation whose performance monotonically increases with the number of superpixels. In this sense, the minimum and maximum performances are considered the lower and upper bounds, while the standard deviation gives how much the overall performance varies. Following <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, we assess superpixel stability considering the minimum, maximum, and standard deviation of boundary adherence and color homogeneity measures.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.7. </span>Robustness</h4>
<div class="ltx_para" id="S3.SS3.SSS7.p1">
<p class="ltx_p" id="S3.SS3.SSS7.p1.1">Similar to stability, evaluating robustness is uncommon in superpixel segmentation, being mostly performed in benchmark papers. In <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite>, robustness and stability are considered the same, and they refer to evaluating how much superpixels change when applying affine transformations. In <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, the concept of robustness was extended to the ability to maintain performance while increasing image blur and noise. Following <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, we evaluate robustness against noise by considering salt and pepper and average blur.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.8. </span>Runtime</h4>
<div class="ltx_para" id="S3.SS3.SSS8.p1">
<p class="ltx_p" id="S3.SS3.SSS8.p1.1">Superpixels are widely used as pre-processing to reduce the workload and extract higher-level features. However, such benefits may be diminished with time-consuming superpixel algorithms. This problem is especially significant in tasks that require real-time execution. Therefore, the execution time is a crucial factor to consider in superpixel segmentation papers. In this work, we assess the execution time of CPU and GPU-based methods.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS9">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.9. </span>Visual quality</h4>
<div class="ltx_para" id="S3.SS3.SSS9.p1">
<p class="ltx_p" id="S3.SS3.SSS9.p1.1">Superpixel papers often qualitatively assess to determine whether the quantitative results reflect segmentation quality. When evaluating superpixels, qualitative assessment is crucial as it captures aspects not covered by quantitative analysis due to the absence of a specific ground truth. Our visual quality assessment focuses on four key aspects: boundary adherence, compactness, smoothness, and regularity. <span class="ltx_text" id="S3.SS3.SSS9.p1.1.1" style="color:#000000;"> Boundary adherence</span> refers to the superpixels’ ability to accurately delineate important image boundaries, regardless of the ground truth image. Conversely, compact superpixels have a regular and convex shape, whereas smoothness (or smooth boundaries) relates to the boundary length of superpixels.
Moreover, regularity refers to their shape, size, and arrangement. A regular superpixel segmentation contains compact superpixels of similar size and approximately the same number of adjacent superpixels in both the X and Y image axes.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this work, we evaluate <math alttext="23" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn id="S4.p1.1.m1.1.1.cmml" type="integer" xref="S4.p1.1.m1.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">23</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">23</annotation></semantics></math> superpixel methods and a grid segmentation baseline according to different aspects. In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we quantitatively evaluate object delineation, color homogeneity, and compactness, summarizing these results with a distribution analysis using boxplots. Then, we evaluate the runtime in CPU and GPU in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS2" title="4.2. Runtime ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.2</span></a>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3" title="4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we perform a qualitative evaluation concerning superpixel contour smoothness, compactness, and adherence to the object borders.
The reader should refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3" title="Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C</span></a> for more experiments. In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.1</span></a>, we analyze the number of generated superpixels and their connectivity. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.2</span></a> presents the stability assessment using the minimum (min), maximum (max), and standard deviation (std) of the measures BR, UE, EV, and SIRS. In addition, Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS3" title="C.3. Robustness ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.3</span></a> presents the robustness assessment against salt and pepper noise and average blur.
According to the connectivity analysis in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.1</span></a>, we include a merging step as post-processing on methods that do not guarantee connectivity to perform the experiments in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.2</span></a>. Finally, in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS4" title="C.4. Overall performance ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.4</span></a>, we discuss the overall performance of superpixel methods concerning their clustering category.
<span class="ltx_text" id="S4.p1.1.1" style="color:#000000;"> In quantitative (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a>), connectivity (Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.1</span></a>), and stability (Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.2</span></a>) results, we report the experiments on Birds, Insects, Sky, and ECSSD on the same plot since their results are similar.</span>
The superpixel methods and evaluation codes used in this work are available in our benchmark at https://github.com/IMScience-PPGINF-PucMinas/superpixel-benchmark.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Quantitative evaluation</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Object delineation</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">As shown in <span class="ltx_text" id="S4.SS1.SSS1.p1.1.1" style="color:#000000;"> Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a></span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.2">GRID</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.3">CRS</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.4">SEEDS</span> reach the worst results in most datasets. According to the evaluation with UE, most methods have low leakage in <span class="ltx_text" id="S4.SS1.SSS1.p1.1.5" style="color:#000000;"> Birds+Insects+Sky+ECSSD datasets (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>),</span> while the indoor images in NYUV2 are more challenging. Similarly, the delineation measured by BR is generally high, except in NYUV2. In <span class="ltx_text" id="S4.SS1.SSS1.p1.1.6" style="color:#000000;"> Birds+Insects+Sky+ECSSD</span> datasets, the best scores in both UE and BR were achieved by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.7">SICLE</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.8">ODISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.9">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.10">LSC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.11">ERS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.12">GMMSP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.13">ISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.14">SH</span>. Furthermore, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.15">SH</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.16">ISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.17">RSS</span> achieved similar BR, but <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.18">RSS</span> has worse UE. The distribution of these results can also be observed in the boxplots (at the bottom of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="684" id="S4.F5.1.g1" src="x7.png" width="606"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="419" id="S4.F5.2.g1" src="x8.png" width="688"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span><span class="ltx_text" id="S4.F5.4.1" style="color:#000000;"> Results for BR and UE on Birds+Insects+Sky+ECSSD and NYUV2 datasets.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text" id="S4.SS1.SSS1.p2.1.1" style="color:#000000;">In Birds+Insects+Sky+ECSSD datasets, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1.1">SICLE</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1.2">ODISF</span> have the best BR and UE, followed by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1.3">DISF</span>, while <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1.4">SH</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1.5">RSS</span> have the best BR in NYUV2. Also, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1.6">ETPS</span> has the best UE in NYUV2</span>.
<span class="ltx_text" id="S4.SS1.SSS1.p2.1.2" style="color:#000000;"> In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.2.1">SICLE</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.2.2">ODISF</span> have poor performance in Sky and NYUV2 datasets, while the same occurs for <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.2.3">ETPS</span> in Birds+Insects+Sky+ECSSD datasets.</span>
In the Sky dataset, the poor delineation of <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.3">SICLE</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.4">ODISF</span> corresponds to a poor saliency map guiding the segmentation, whose importance can only be reduced in <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.5">SICLE</span>. In contrast, their poor performance in the NYUV2 dataset is due to these methods producing more superpixels in the image region identified as salient. This strategy is interesting when the salient region corresponds to the object of interest or when this region has more complex information, requiring more superpixels to obtain a better delineation. However, the ground truth in the NYUV2 dataset has multi-class labeling. As one may note in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, the leakage in the NYUV2 dataset is too high compared to the other datasets, resulting in similar UE results for most methods.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">In most datasets, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">RSS</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.2">SH</span> have competitive and similar BR results but worse UE. This observation is more evident in the boxplots <span class="ltx_text" id="S4.SS1.SSS1.p3.1.3" style="color:#000000;"> (at the bottom of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>)</span>. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.4">DAL-HERS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.5">ETPS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.6">IBIS</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.7">SLIC</span> obtained a low delineation, only superior to <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.8">GRID</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.9">SEEDS</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.10">CRS</span>. Their results are followed by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.11">SNIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.12">SCALP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.13">DRW</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.14">LNSNet</span>. <span class="ltx_text" id="S4.SS1.SSS1.p3.1.15" style="color:#000000;"> Also, according to the boxplot results (at the bottom of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>), <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.15.1">LNSNet</span> seems to have the best UE in the NYUV2 dataset, but it achieves low leakage when the number of superpixels is too high (see the control over the number of superpixels in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.1</span></a>).</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Color homogeneity</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">When evaluating the color homogeneity <span class="ltx_text" id="S4.SS1.SSS2.p1.1.1" style="color:#000000;"> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F6" title="Figure 6 ‣ 4.1.2. Color homogeneity ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>)</span> with EV and SIRS, the results of the first measure are generally higher and closer to each other compared to the second one. However, their results show some similarities. <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.2">GRID</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.3">CRS</span> have the worst results in all datasets in both measures, followed by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.4">ODISF</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.5">SICLE</span>. Among these methods, only <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.6">ODISF</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.7">SICLE</span> have an accurate delineation, and their low color homogeneity results from fewer superpixels in the non-salient image regions. Conversely, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.8">DISF</span> has the best results in most datasets, followed by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.9">SH</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.10">LSC</span>. Although they have different clustering approaches, all these methods have high boundary adherence. <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.11">ISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.12">RSS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.13">SCALP</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.14">GMMSP</span> also achieve competitive color homogeneity results. The distribution of these results can also be observed in the boxplots <span class="ltx_text" id="S4.SS1.SSS2.p1.1.15" style="color:#000000;"> (at the bottom of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F6" title="Figure 6 ‣ 4.1.2. Color homogeneity ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>)</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="695" id="S4.F6.1.g1" src="x9.png" width="664"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="419" id="S4.F6.2.g1" src="x10.png" width="688"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span><span class="ltx_text" id="S4.F6.4.1" style="color:#000000;"> Results for EV and SIRS on Birds+Insects+Sky+ECSSD and NYUV2 datasets.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="421" id="S4.F7.1.g1" src="x11.png" width="689"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="275" id="S4.F7.2.g1" src="x12.png" width="747"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span><span class="ltx_text" id="S4.F7.4.1" style="color:#000000;"> Results for CO on Birds+Insects+Sky+ECSSD and NYUV2 datasets.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Compactness</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F7" title="Figure 7 ‣ 4.1.2. Color homogeneity ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">7</span></a> shows the compactness evaluation. As expected, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.1">GRID</span> obtains the most compact segmentations.
Aside from <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.2">GRID</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.3">CRS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.4">SIN</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.5">ETPS</span> have the highest compactness across the datasets, followed by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.6">SCALP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.7">SNIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.8">AINET</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.9">SSFCN</span>. Also, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.10">SLIC</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.11">IBIS</span> achieve similar compactness, usually lower than <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.12">AINET</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.13">SSFCN</span>.
The label propagating strategy in <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.14">AINET</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.15">SSFCN</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.16">SIN</span> favors compactness. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.17">CRS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.18">SCALP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.19">SNIC</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.20">SLIC</span> have a parameter to control compactness, while the initial grid segmentation in <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.21">ETPS</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.22">IBIS</span> provides initial compact superpixels.
While <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.23">CRS</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.24">ETPS</span> produce superpixels by optimizing the contours of a grid segmentation, the others use different approaches based on <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.25">SLIC</span>. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.26">LSC</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.27">GMMSP</span> present similar and moderate compactness. Among the evaluated methods, only <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.28">SEEDS</span> have high variability in compactness. More delineation-focused methods, such as <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.29">SICLE</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.30">ODISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.31">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.32">SH</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p1.1.33">DAL-HERS</span>, produced less compact segmentations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Overall</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1"><span class="ltx_text" id="S4.SS1.SSS4.p1.1.1" style="color:#000000;">As one may see in Figures <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F5" title="Figure 5 ‣ 4.1.1. Object delineation ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F6" title="Figure 6 ‣ 4.1.2. Color homogeneity ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F7" title="Figure 7 ‣ 4.1.2. Color homogeneity ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">7</span></a>, most methods with <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.1.1">path-based</span> clustering (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.2">ERGC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.3">ISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.4">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.5">RSS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.6">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.7">SICLE</span>) have similar performance in object delineation, compactness, and homogeneity. They usually achieve high delineation but low compactness.
Similarly, the method with <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.1.8">graph-based</span> clustering (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.9">ERS</span>) performs clustering based on graphs, having a competitive (but not the best) delineation on all datasets with more compactness than <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.1.10">path-based</span> methods.
On the other hand, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.1.11">neighborhood-based</span> clustering approaches (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.12">SLIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.13">LSC</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.14">SCALP</span>) have more varied results. While <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.15">LSC</span> achieves excellent delineation and more homogeneous superpixels, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.16">SLIC</span> has moderate compactness and worse delineation. Conversely, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.17">SCALP</span> has better delineation, color homogeneity, and compactness than <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.18">SLIC</span> but lower delineation and color homogeneity than <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.19">LSC</span>.
Methods with <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.1.20">boundary evolution</span> clustering (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.21">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.22">IBIS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.23">CRS</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.24">ETPS</span>) perform clustering based on contour optimization, achieving different results due to the distinction between their optimization functions. These methods produce the worst results in object delineation and color homogeneity but with higher compactness. Among them, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.25">IBIS</span> achieves similar object delineation and color homogeneity to <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.26">SLIC</span>, whereas <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.27">CRS</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1.28">SEEDS</span> have the worst delineation and homogeneity but greater compactness among all methods.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1">Methods with a <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.1">dynamic-center-update</span> clustering (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.2">DRW</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.3">SNIC</span>) use strategies to adapt the number of generated superpixels to the image content. Despite their similarities, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.4">DRW</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.5">SNIC</span> use different features and optimization functions, which explains the contrast in their results. While <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.6">DRW</span> has better delineation and fewer superpixels, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.7">SNIC</span> generates more compact and homogeneous superpixels. Also, the lower color homogeneity of <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.8">DRW</span> compared to <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.9">SNIC</span> is due to the smaller number of superpixels produced by <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.10">DRW</span>.
Concerning <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.11">hierarchical</span> approaches (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.12">SH</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.13">DAL-HERS</span>), they have low compacity and high color homogeneity. However, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.14">SH</span> has competitive delineation and color homogeneity, while <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.15">DAL-HERS</span> has worse results.
On the other hand, the method with <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.16">data distribution-based</span> clustering (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.17">GMMSP</span>) has a performance similar to <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.18">LSC</span>. <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.19">GMMSP</span> have moderate compactness, producing more compact superpixels than <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.20">path-based</span> clustering methods but less than most <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.21">neighborhood-based</span> ones.
Finally, methods that perform clustering with <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.22">deep architectures</span> have varied performances. Using u-shaped architectures, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.23">SSFCN</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.24">AINET</span> have similar and moderate results in all metrics. On the other hand, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.25">LNSNet</span> has excellent BR, but its UE indicates more leakage. Finally, although the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.26">interpolation network</span> in <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p2.1.27">SIN</span> guarantees connected superpixels, they have poor delineation and color homogeneity but high compactness.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="578" id="S4.F8.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span><span class="ltx_text" id="S4.F8.2.1" style="color:#000000;"> Runtime in seconds on Birds+Insects+Sky, ECSSD, and NYUV2 datasets.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Runtime</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Execution time may be critical in superpixel methods, especially for real-time applications. <span class="ltx_text" id="S4.SS2.p1.1.1" style="color:#000000;"> Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F8" title="Figure 8 ‣ 4.1.4. Overall ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">8</span></a></span> shows the CPU<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>CPU Intel® Core<math alttext="{}^{\text{TM}}" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><msup id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml"><mi id="footnote1.m1.1.1b" xref="footnote1.m1.1.1.cmml"></mi><mtext id="footnote1.m1.1.1.1" xref="footnote1.m1.1.1.1a.cmml">TM</mtext></msup><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"><ci id="footnote1.m1.1.1.1a.cmml" xref="footnote1.m1.1.1.1"><mtext id="footnote1.m1.1.1.1.cmml" mathsize="70%" xref="footnote1.m1.1.1.1">TM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">{}^{\text{TM}}</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">start_FLOATSUPERSCRIPT TM end_FLOATSUPERSCRIPT</annotation></semantics></math> i5-7200U @ 2.5GHz x 4, 64bit with 24GB RAM.</span></span></span> and GPU<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>CPU Intel® Core<math alttext="{}^{\text{TM}}" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><msup id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml"><mi id="footnote2.m1.1.1b" xref="footnote2.m1.1.1.cmml"></mi><mtext id="footnote2.m1.1.1.1" xref="footnote2.m1.1.1.1a.cmml">TM</mtext></msup><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><apply id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"><ci id="footnote2.m1.1.1.1a.cmml" xref="footnote2.m1.1.1.1"><mtext id="footnote2.m1.1.1.1.cmml" mathsize="70%" xref="footnote2.m1.1.1.1">TM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">{}^{\text{TM}}</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">start_FLOATSUPERSCRIPT TM end_FLOATSUPERSCRIPT</annotation></semantics></math> i7-8700 @ 3.20GHz x 12, 64bit with 31GB RAM and a GPU Nvidia GeForce GTX 1080 with 8GB RAM.</span></span></span> time in seconds without the post-processing of Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.1</span></a>. For <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">SCALP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.4">SICLE</span>, we do not include the edge maps and saliency maps computation. <span class="ltx_text" id="S4.SS2.p1.1.5" style="color:#000000;"> As one may see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F8" title="Figure 8 ‣ 4.1.4. Overall ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>, due to the images of the ECSSD dataset being generally smaller than the others, the runtime in this dataset is usually shorter. Therefore, we merged the results of the datasets Birds, Insects, and Sky since they were similar.</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text" id="S4.SS2.p2.1.1" style="color:#000000;">According to the CPU runtime (on the first row in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F8" title="Figure 8 ‣ 4.1.4. Overall ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>), methods with <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1.1">boundary evolution</span> clustering (except <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.2">CRS</span>) achieve the lowest execution time (around 0.05 seconds), followed by <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.3">SH</span> (which performs <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1.4">hierarchical</span> clustering) with around 0.09 seconds.
<span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.5">SLIC</span> has similar efficiency, requiring around 0.14 seconds per image, while the remaining <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1.6">neighborhood-based</span> clustering methods (<span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.7">LSC</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.8">SCALP</span>) vary in efficiency. <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.9">LSC</span> requires around 0.4 seconds per image on Birds+Insects+Sky and NYUV2 datasets, while <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.10">SCALP</span> needs approximately twice as long. <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1.11">Path-based</span> clustering methods (<span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.12">ERGC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.13">ISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.14">RSS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.15">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.16">SICLE</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.17">ODISF</span>) also show varied efficiency. <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.18">SICLE</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.19">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.20">DISF</span> achieve higher runtimes, while <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.21">ISF</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.22">ERGC</span> require less than 1 second per image. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1.23">RSS</span> has a competitive execution time (less than 0.1 seconds). </span></p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Methods with a <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">dynamic center update</span> clustering (<span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.2">DRW</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.3">SNIC</span>) also have distinct runtimes. While <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.4">DRW</span> requires around 1 second per image, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.5">SNIC</span> is the most time-consuming on a CPU. Considering <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.6">graph-based</span> clustering, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.7">ERS</span> requires a high execution time, similar to <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.8">ODISF</span>. <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.9">GMMSP</span>, the only one with clustering based on <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.10">data distribution</span>, achieves similar efficiency to <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.11">SCALP</span>.
As one may see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F8" title="Figure 8 ‣ 4.1.4. Overall ‣ 4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.12">DAL-HERS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.13">SSFCN</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.14">AINET</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.15">LNSNet</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.16">SIN</span> were executed on a GPU. <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.17">LNSNet</span> has the worst execution time of all evaluated methods, and it increases according to the number of superpixels. <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.18">SSFCN</span>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.19">AINET</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.20">SIN</span> have similar behavior, but their running times are much lower. Among these, <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.21">SIN</span> is usually faster, except in the Birds dataset. One may argue that the Birds dataset has more stretched images than the other datasets, which may hinder the interpolation operations. In contrast, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.22">DAL-HERS</span> has an excellent execution time, requiring less than 0.3 seconds per image.
<span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.23">SH</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.24">DAL-HERS</span> are the only ones whose execution time is constant since they produce a <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.25">hierarchy</span> of superpixels in a single execution. From cuts on the hierarchy, they produce different numbers of superpixels.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Qualitative evaluation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this section, we evaluate the segmentations’ visual quality regardless of their ground truth since the image object may vary according to the application. We assess visual quality based on the superpixels’ adherence to the image boundaries, smoothness, compactness, and regularity. The smoothness is inversely related to the superpixel’s boundary length. On the other hand, the superpixels’ compactness relates to their area. Moreover, regularity refers to their shape, size, and arrangement. Figures <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F9" title="Figure 9 ‣ 4.3.4. Boundary evolution clustering ‣ 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">9</span></a>,  <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F10" title="Figure 10 ‣ 4.3.4. Boundary evolution clustering ‣ 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">10</span></a>, and  <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F11" title="Figure 11 ‣ 4.3.4. Boundary evolution clustering ‣ 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">11</span></a> present segmentations with approximately 100 and 700 superpixels on Birds, Insects, Sky, ECSSD, and NYUV2 datasets.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Path-based clustering</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1"><span class="ltx_text" id="S4.SS3.SSS1.p1.1.1" style="color:#000000;">Relative to path-based clustering methods, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.1">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.2">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.3">SICLE</span> produce superpixels with no compactness but competitive delineation. <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.4">ODISF</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.5">SICLE</span> produce more superpixels on the salient area identified by the saliency map, which can improve the delineation of this region. Due to this, there is a low number of superpixels in regions with low saliency, leading to a worse delineation in these regions but a superior delineation in the salient ones. Also, by fine-tuning the saliency maps, their results can improve. Comparing <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.6">SICLE</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.7">ODISF</span>, one can observe more accurate delineation in <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.8">SICLE</span> segmentations. On the other hand, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.9">RSS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.10">ISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.11">ERGC</span> have some compactness and smooth contours. However, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.12">RSS</span> cannot produce compact superpixels in very homogeneous regions, and its superpixels have a high variance in size. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.13">ISF</span> produces regular superpixels in homogeneous regions, but it has a high sensitivity to color variations, leading to non-smooth superpixels, highly variable in size, in more complex regions.
Similarly, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1.14">ERGC</span> has good adherence to the object boundaries, and its superpixels have some regularity, without significant variations in size. </span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Neighborhood-based clustering</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1"><span class="ltx_text" id="S4.SS3.SSS2.p1.1.1" style="color:#000000;">Regarding the neighborhood-based methods, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.1.1">SLIC</span> produces very compact superpixels with good adherence to the image boundaries and more regularity in homogeneous regions.
Although both delineation and compactness reduce for a lower number of superpixels, the delineation of SLIC is more compromised</span>.
In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.2">SCALP</span> produces superpixels with excellent delineation that are more compact, smooth, and regular than <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.3">SLIC</span>. Although it produces less compact superpixels when the number of them reduces, the superpixels’ contours of <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.4">SCALP</span> remain smooth.
<span class="ltx_text" id="S4.SS3.SSS2.p1.1.5" style="color:#000000;"> Unlike <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.5.1">SLIC</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.5.2">SCALP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.5.3">LSC</span> only produces smooth superpixels in more homogeneous regions. However, it is sensitive to minor color variations, reducing its smoothness and compactness in regions with simpler textures.
Furthermore, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.5.4">LSC</span> may generate more elongated and thin superpixels at the strong image boundaries, obtaining an excellent delineation but with no compactness.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Dynamic center update clustering</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">With visually very similar segmentation to <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p1.1.1">SLIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p1.1.2">SNIC</span> also produces superpixels with high compactness and better delineation. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p1.1.3">DRW</span> does not generate compact superpixels and produces noticeably fewer superpixels than expected, especially in homogeneous regions. Despite this, it generates superpixels with good adherence.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4. </span>Boundary evolution clustering</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1"><span class="ltx_text" id="S4.SS3.SSS4.p1.1.1" style="color:#000000;">Similarly to <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1.1">DRW</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1.2">SEEDS</span> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.F10" title="Figure 10 ‣ 4.3.4. Boundary evolution clustering ‣ 4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">10</span></a>) creates superpixels with poor compactness and non-smooth boundaries. They have moderate delineation with small leakage regions, which significantly reduces when the number of superpixels reduces. In contrast to <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1.3">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1.4">CRS</span> generates highly compact and regular superpixels but with low boundary adherence.
Similarly, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1.5">ETPS</span> produces very regular, smooth, and compact superpixels.
However, its compactness, smoothness, and regularity slightly reduce for lower superpixels, while the delineation suffers drastically.
<span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1.6">IBIS</span> also generates significantly compact superpixels, whose compactness and smoothness vary depending on the region’s homogeneity. Also, it produces regular superpixels at the homogeneous image regions.
However, its sensitivity to color variations reduces compactness and smoothness in less homogeneous areas. Also, for lower superpixels, its adherence to contours is significantly reduced. </span></p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="830" id="S4.F9.g1" src="x14.png" width="514"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Segmentation comparison with images from Birds, and Insects with <math alttext="100" class="ltx_Math" display="inline" id="S4.F9.3.m1.1"><semantics id="S4.F9.3.m1.1b"><mn id="S4.F9.3.m1.1.1" xref="S4.F9.3.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.F9.3.m1.1c"><cn id="S4.F9.3.m1.1.1.cmml" type="integer" xref="S4.F9.3.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.3.m1.1d">100</annotation><annotation encoding="application/x-llamapun" id="S4.F9.3.m1.1e">100</annotation></semantics></math> and <math alttext="700" class="ltx_Math" display="inline" id="S4.F9.4.m2.1"><semantics id="S4.F9.4.m2.1b"><mn id="S4.F9.4.m2.1.1" xref="S4.F9.4.m2.1.1.cmml">700</mn><annotation-xml encoding="MathML-Content" id="S4.F9.4.m2.1c"><cn id="S4.F9.4.m2.1.1.cmml" type="integer" xref="S4.F9.4.m2.1.1">700</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.4.m2.1d">700</annotation><annotation encoding="application/x-llamapun" id="S4.F9.4.m2.1e">700</annotation></semantics></math> superpixels.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="638" id="S4.F10.g1" src="x15.png" width="508"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Segmentation comparison with images from Sky with <math alttext="100" class="ltx_Math" display="inline" id="S4.F10.3.m1.1"><semantics id="S4.F10.3.m1.1b"><mn id="S4.F10.3.m1.1.1" xref="S4.F10.3.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.F10.3.m1.1c"><cn id="S4.F10.3.m1.1.1.cmml" type="integer" xref="S4.F10.3.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.3.m1.1d">100</annotation><annotation encoding="application/x-llamapun" id="S4.F10.3.m1.1e">100</annotation></semantics></math> and <math alttext="700" class="ltx_Math" display="inline" id="S4.F10.4.m2.1"><semantics id="S4.F10.4.m2.1b"><mn id="S4.F10.4.m2.1.1" xref="S4.F10.4.m2.1.1.cmml">700</mn><annotation-xml encoding="MathML-Content" id="S4.F10.4.m2.1c"><cn id="S4.F10.4.m2.1.1.cmml" type="integer" xref="S4.F10.4.m2.1.1">700</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.4.m2.1d">700</annotation><annotation encoding="application/x-llamapun" id="S4.F10.4.m2.1e">700</annotation></semantics></math> superpixels.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="830" id="S4.F11.g1" src="x16.png" width="517"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Segmentation comparison with images from ECSSD and NYUV2 with <math alttext="100" class="ltx_Math" display="inline" id="S4.F11.3.m1.1"><semantics id="S4.F11.3.m1.1b"><mn id="S4.F11.3.m1.1.1" xref="S4.F11.3.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.F11.3.m1.1c"><cn id="S4.F11.3.m1.1.1.cmml" type="integer" xref="S4.F11.3.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F11.3.m1.1d">100</annotation><annotation encoding="application/x-llamapun" id="S4.F11.3.m1.1e">100</annotation></semantics></math> and <math alttext="700" class="ltx_Math" display="inline" id="S4.F11.4.m2.1"><semantics id="S4.F11.4.m2.1b"><mn id="S4.F11.4.m2.1.1" xref="S4.F11.4.m2.1.1.cmml">700</mn><annotation-xml encoding="MathML-Content" id="S4.F11.4.m2.1c"><cn id="S4.F11.4.m2.1.1.cmml" type="integer" xref="S4.F11.4.m2.1.1">700</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F11.4.m2.1d">700</annotation><annotation encoding="application/x-llamapun" id="S4.F11.4.m2.1e">700</annotation></semantics></math> superpixels.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5. </span>Hierarchical clustering</h4>
<div class="ltx_para" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">Regarding the hierarchical methods, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS5.p1.1.1">SH</span> has an excellent delineation, but its superpixels are not regular nor compact and have non-smooth contours in more complex regions. In addition, it generates elongated and thin superpixels at some of the strong image boundaries. <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS5.p1.1.2">DAL-HERS</span> also has superb delineation but generates rough superpixels and some tiny ones, resulting in visibly poor segmentation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.6. </span>Deep-based clustering</h4>
<div class="ltx_para" id="S4.SS3.SSS6.p1">
<p class="ltx_p" id="S4.SS3.SSS6.p1.1">Methods that perform clustering using deep learning usually have moderate delineation and high compactness. <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.1">AINET</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.2">SSFCN</span> produce smooth superpixels with some regularity. Also, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.3">AINET</span> produces less smooth superpixels than <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.4">SSFCN</span>, indicating more sensitivity to low color variation. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.5">LNSNet</span> generates a significantly higher number of superpixels than desired. Similarly to <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.6">ISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.7">LNSNet</span> produces compact superpixels in homogeneous regions, but its sensitivity to color variations implies very rough superpixels. It has good delineation when the number of superpixels is higher. However, their non-smooth contours do not have a high delineation even in regions with a more prominent boundary, causing small leaks. On the other hand, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS6.p1.1.8">SIN</span> has much more compact and regular superpixels than the other deep learning strategies, but its delineation often misses strong boundaries.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.7. </span>Others</h4>
<div class="ltx_para" id="S4.SS3.SSS7.p1">
<p class="ltx_p" id="S4.SS3.SSS7.p1.1"><span class="ltx_text" id="S4.SS3.SSS7.p1.1.1" style="color:#000000;">The superpixels in <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS7.p1.1.1.1">ERS</span> have good boundary adherence and do not vary much in size, but they have low smoothness. </span>
In comparison, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS7.p1.1.2">GMMSP</span> produces significantly more compact superpixels in homogeneous regions. In less homogeneous ones, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS7.p1.1.3">GMMSP</span> produces fewer compact superpixels but usually with smoother contours. By reducing the number of superpixels, the compactness of the less homogeneous image region barely changes. However, the compactness and smoothness drastically reduce in less homogeneous regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.8. </span>Overall</h4>
<div class="ltx_para" id="S4.SS3.SSS8.p1">
<p class="ltx_p" id="S4.SS3.SSS8.p1.1">As one may see, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.1">CRS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.2">ERGC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.3">ETPS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.4">SCALP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.5">SLIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.6">SNIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.7">IBIS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.8">GMMSP</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.9">SSFCN</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.10">AINET</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.11">SIN</span> produce visibly smooth, compact, and regular superpixels. These properties are most noticeable on <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.12">CRS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.13">SCALP</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.14">ETPS</span>. Nevertheless, high compactness may lead to a worse delineation, as in <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.15">CRS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.16">SIN</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.17">ETPS</span>. Conversely, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.18">ERGC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.19">IBIS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.20">SNIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.21">SLIC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.22">AINET</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.23">SSFCN</span> had moderate boundary adherence, with worse results on smooth image boundaries. Among these methods, only <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.24">SCALP</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.25">GMMSP</span> achieved excellent boundary delineation.
<span class="ltx_text" id="S4.SS3.SSS8.p1.1.26" style="color:#000000;"> Concerning methods with less (or no) compactness and smoothness, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.1">LNSNet</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.2">SEEDS</span> have the worst delineations. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.3">DRW</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.4">ERS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.5">RSS</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.6">LSC</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.7">ISF</span> have some compactness, smoothness, and good boundary adherence. One may observe the best delineation in <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.8">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.9">LSC</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.10">ODISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.11">SICLE</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.12">ISF</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.13">SH</span>, although most do not have compactness or regularity. In particular, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.14">DISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.15">ODISF</span>, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.16">SICLE</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.17">GMMSP</span> have visually better boundary adherence. Also, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.18">ODISF</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.19">SICLE</span> use a saliency map to guide the segmentation, generating more superpixels in salient regions and fewer superpixels in non-salient ones, reducing the superpixel color homogeneity. As observed in the quantitative evaluation, when the saliency map corresponds to the desired object, the <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.20">ODISF</span>’s and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.21">SICLE</span>’s delineations outperform the other methods, indicating that decreasing the saliency map importance may improve such results, which is only possible in <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p1.1.26.22">SICLE</span>.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.SSS8.p2">
<p class="ltx_p" id="S4.SS3.SSS8.p2.1">Among the main processing categories, methods with contour evolution-based clustering usually produce the most compact and regular superpixels, although they have low boundary adherence. Conversely, those with neighborhood-based clustering usually have good delineation with high compactness and regularity. Similarly, dynamic-center-update clustering methods also achieve good boundary adherence. However, only <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.1">SNIC</span> shows high compactness and regularity, whereas <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.2">DRW</span> only has smooth superpixel contours. Finally, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.3">GMMSP</span> has great compactness and competitive delineation.
Hierarchical methods, path-based methods, and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.4">ERS</span> produce superpixels with low compactness. Also, the superpixels on hierarchical methods are neither compact nor smooth. Conversely, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.5">ERS</span> and most path-based methods generate superpixels with low compactness and smoothness but with excellent boundary adherence.
Methods with deep-based clustering have variate results in all criteria. Like neighborhood-based methods, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.6">SSFCN</span> and <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.7">AINET</span>, both with u-shaped architectures but distinct loss functions and clustering layers, have moderate delineation and good compactness, with similar results across all criteria. In contrast, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.8">SIN</span> produces much more compact and regular superpixels, but with low delineation. The regularity in <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.9">SIN</span> may be the result of its interpolation operations, which propagate superpixel labels to neighbor pixels considering the image lattice. On the other hand, <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS8.p2.1.10">LNSNet</span> produces much more superpixels than desired, resulting in superpixels with no compactness or smoothness.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">In this work, we present a taxonomy for superpixel methods, which categorizes them according to their processing steps and the level of abstraction of the features used. Our taxonomy separates each superpixel approach into up to three processing steps and categorizes the task performed at each one. <span class="ltx_text" id="S5.p1.2.2" style="color:#000000;"> Along with our taxonomy, we inform other important properties of <math alttext="59" class="ltx_Math" display="inline" id="S5.p1.1.1.m1.1"><semantics id="S5.p1.1.1.m1.1a"><mn id="S5.p1.1.1.m1.1.1" mathcolor="#000000" xref="S5.p1.1.1.m1.1.1.cmml">59</mn><annotation-xml encoding="MathML-Content" id="S5.p1.1.1.m1.1b"><cn id="S5.p1.1.1.m1.1.1.cmml" type="integer" xref="S5.p1.1.1.m1.1.1">59</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.1.m1.1c">59</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.1.m1.1d">59</annotation></semantics></math> of the most recently and commonly used superpixel methods. We also provide a comprehensive literature review encompassing these methods. We present an extensive comparison among <math alttext="23" class="ltx_Math" display="inline" id="S5.p1.2.2.m2.1"><semantics id="S5.p1.2.2.m2.1a"><mn id="S5.p1.2.2.m2.1.1" mathcolor="#000000" xref="S5.p1.2.2.m2.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.2.m2.1b"><cn id="S5.p1.2.2.m2.1.1.cmml" type="integer" xref="S5.p1.2.2.m2.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.2.m2.1c">23</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.2.m2.1d">23</annotation></semantics></math> superpixel methods and a grid baseline considering: superpixel connectivity, control of the number of superpixels, compactness, adherence to object contours, color homogeneity, stability, robustness to noise and blur, execution time, and visual quality. </span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">According to our experiments, methods with clustering based on boundary evolution generally present greater efficiency, compactness, and regularity. Nevertheless, they have worse boundary adherence and color homogeneity.
In contrast, methods with dynamic-update-clustering are less efficient and generate slightly less compact and regular superpixels. Also, they have better delineation and homogeneity than those based on boundary evolution.
Conversely, methods with neighborhood-based clustering present more varied performances. For instance, <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">LSC</span> achieves good boundary adherence, compactness, and smoothness, while <span class="ltx_text ltx_font_bold" id="S5.p2.1.2">SLIC</span> and <span class="ltx_text ltx_font_bold" id="S5.p2.1.3">SCALP</span> have higher compactness but worse delineation.
<span class="ltx_text" id="S5.p2.1.4" style="color:#000000;"> <span class="ltx_text ltx_font_bold" id="S5.p2.1.4.1">GMMSP</span>, which performs clustering based on data distribution, obtains a competitive delineation, good compactness, and smooth superpixel contours, although no regularity. In addition, <span class="ltx_text ltx_font_bold" id="S5.p2.1.4.2">ERS</span>, the only evaluated method with graph-based clustering, has a similar delineation but worse efficiency, compactness, and color homogeneity.</span>
Hierarchical methods also produce superpixels with excellent boundary adherence. In addition, they have low execution time, but their superpixels are neither visually compact nor regular.
In contrast, deep learning-based methods achieve moderate or low delineation and varied compactness, but most are efficient, requiring around <math alttext="0.1" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn id="S5.p2.1.m1.1.1.cmml" type="float" xref="S5.p2.1.m1.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">0.1</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">0.1</annotation></semantics></math> seconds to process an image. In our evaluation, the path-based clustering methods generally have the best delineation and the most homogeneous superpixels. However, they have varied efficiency, low compactness, and low smoothness.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.3">Most evaluated methods ensure superpixel connectivity and generate superpixels in a similar number to the desired one. In particular, <span class="ltx_text ltx_font_bold" id="S5.p3.3.1">DISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.2">ODISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.3">SICLE</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.4">SH</span>, and <span class="ltx_text ltx_font_bold" id="S5.p3.3.5">ERS</span> generated the exact number of desired superpixels. <span class="ltx_text" id="S5.p3.3.6" style="color:#000000;"> In contrast, only <span class="ltx_text ltx_font_bold" id="S5.p3.3.6.1">LNSNet</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.6.2">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.6.3">CRS</span>, and <span class="ltx_text ltx_font_bold" id="S5.p3.3.6.4">DRW</span> may produce unconnected superpixels. <span class="ltx_text ltx_font_bold" id="S5.p3.3.6.5">LNSNet</span> creates almost twice the superpixels, many of these disconnected. On the other hand, the number of superpixels produced by <span class="ltx_text ltx_font_bold" id="S5.p3.3.6.6">DRW</span> is usually lower than desired.
Furthermore, in deep-based learning methods, the network usually cannot output connected superpixels but only soft pixel-superpixel assignments</span>. These methods rely on post-processing to compute the hard assignment. Only <span class="ltx_text ltx_font_bold" id="S5.p3.3.7">SIN</span> can directly output connected superpixels, but its label propagation mechanism cannot produce highly boundary-adherent superpixels.
When evaluating robustness, most methods achieve good robustness to noise and blur. The worst results were observed in <span class="ltx_text ltx_font_bold" id="S5.p3.3.8">DAL-HERS</span>, followed by <span class="ltx_text ltx_font_bold" id="S5.p3.3.9">SICLE</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.10">LNSNet</span>, and <span class="ltx_text ltx_font_bold" id="S5.p3.3.11">ERS</span>. In contrast, the most robust methods are <span class="ltx_text ltx_font_bold" id="S5.p3.3.12">DISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.13">ERGC</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.14">RSS</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.15">ISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.16">ODISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.17">SEEDS</span>, and <span class="ltx_text ltx_font_bold" id="S5.p3.3.18">SH</span>. We could also see that some methods produce a different number of superpixels according to the addition of noise or blur. For <math alttext="K\approx 400" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">K</mi><mo id="S5.p3.1.m1.1.1.1" xref="S5.p3.1.m1.1.1.1.cmml">≈</mo><mn id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><approx id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1"></approx><ci id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2">𝐾</ci><cn id="S5.p3.1.m1.1.1.3.cmml" type="integer" xref="S5.p3.1.m1.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">K\approx 400</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">italic_K ≈ 400</annotation></semantics></math>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.19">LNSNet</span> has more sensitivity in this criterion, creating more than <math alttext="30,000" class="ltx_Math" display="inline" id="S5.p3.2.m2.2"><semantics id="S5.p3.2.m2.2a"><mrow id="S5.p3.2.m2.2.3.2" xref="S5.p3.2.m2.2.3.1.cmml"><mn id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml">30</mn><mo id="S5.p3.2.m2.2.3.2.1" xref="S5.p3.2.m2.2.3.1.cmml">,</mo><mn id="S5.p3.2.m2.2.2" xref="S5.p3.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.2b"><list id="S5.p3.2.m2.2.3.1.cmml" xref="S5.p3.2.m2.2.3.2"><cn id="S5.p3.2.m2.1.1.cmml" type="integer" xref="S5.p3.2.m2.1.1">30</cn><cn id="S5.p3.2.m2.2.2.cmml" type="integer" xref="S5.p3.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.2c">30,000</annotation><annotation encoding="application/x-llamapun" id="S5.p3.2.m2.2d">30 , 000</annotation></semantics></math> superpixels. <span class="ltx_text ltx_font_bold" id="S5.p3.3.20">DAL-HERS</span> also produces significantly more superpixels, reaching almost <math alttext="1,500" class="ltx_Math" display="inline" id="S5.p3.3.m3.2"><semantics id="S5.p3.3.m3.2a"><mrow id="S5.p3.3.m3.2.3.2" xref="S5.p3.3.m3.2.3.1.cmml"><mn id="S5.p3.3.m3.1.1" xref="S5.p3.3.m3.1.1.cmml">1</mn><mo id="S5.p3.3.m3.2.3.2.1" xref="S5.p3.3.m3.2.3.1.cmml">,</mo><mn id="S5.p3.3.m3.2.2" xref="S5.p3.3.m3.2.2.cmml">500</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.3.m3.2b"><list id="S5.p3.3.m3.2.3.1.cmml" xref="S5.p3.3.m3.2.3.2"><cn id="S5.p3.3.m3.1.1.cmml" type="integer" xref="S5.p3.3.m3.1.1">1</cn><cn id="S5.p3.3.m3.2.2.cmml" type="integer" xref="S5.p3.3.m3.2.2">500</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m3.2c">1,500</annotation><annotation encoding="application/x-llamapun" id="S5.p3.3.m3.2d">1 , 500</annotation></semantics></math> when increasing noise. <span class="ltx_text" id="S5.p3.3.21" style="color:#000000;"> In addition, the number of superpixels produced by <span class="ltx_text ltx_font_bold" id="S5.p3.3.21.1">IBIS</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.21.2">DRW</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.21.3">SLIC</span>, <span class="ltx_text ltx_font_bold" id="S5.p3.3.21.4">GMMSP</span>, and <span class="ltx_text ltx_font_bold" id="S5.p3.3.21.5">SCALP</span> produce quantities of superpixels close to the desired ones when increasing noise or blur. </span></p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Due to the trade-off between delineation and compactness, it is hard to establish which method has the best performance. Considering object delineation and color homogeneity, <span class="ltx_text ltx_font_bold" id="S5.p4.1.1">DISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p4.1.2">ISF</span>, <span class="ltx_text ltx_font_bold" id="S5.p4.1.3">LSC</span>, <span class="ltx_text ltx_font_bold" id="S5.p4.1.4">GMMSP</span>, and <span class="ltx_text ltx_font_bold" id="S5.p4.1.5">SH</span> have the best average performance and stability. <span class="ltx_text ltx_font_bold" id="S5.p4.1.6">SH</span> has greater efficiency, followed by <span class="ltx_text ltx_font_bold" id="S5.p4.1.7">LSC</span> and <span class="ltx_text ltx_font_bold" id="S5.p4.1.8">ISF</span>. On the other hand, <span class="ltx_text ltx_font_bold" id="S5.p4.1.9">GMMSP</span> has more compact superpixels, followed by <span class="ltx_text ltx_font_bold" id="S5.p4.1.10">ISF</span>. When delineation and homogeneity are more critical than compactness, <span class="ltx_text ltx_font_bold" id="S5.p4.1.11">DISF</span> is the most recommended. We also recommend <span class="ltx_text ltx_font_bold" id="S5.p4.1.12">SICLE</span> and <span class="ltx_text ltx_font_bold" id="S5.p4.1.13">ODISF</span> when only object delineation is crucial. Despite not having good results when the saliency map does not find the desired object, their superior performance in other datasets may indicate that fine-tuning the saliency detector can improve the results. However, for greater compactness at the expense of delineation, both <span class="ltx_text ltx_font_bold" id="S5.p4.1.14">SCALP</span> and <span class="ltx_text ltx_font_bold" id="S5.p4.1.15">SLIC</span> are recommended. Between these, <span class="ltx_text ltx_font_bold" id="S5.p4.1.16">SLIC</span> has more compactness and low execution time but worse delineation and less color homogeneity. Considering real-time applications, only <span class="ltx_text ltx_font_bold" id="S5.p4.1.17">SIN</span> could achieve around 30fps on GPU in most datasets. In smaller images (from ECSSD), <span class="ltx_text ltx_font_bold" id="S5.p4.1.18">ETPS</span>, <span class="ltx_text ltx_font_bold" id="S5.p4.1.19">SH</span>, and <span class="ltx_text ltx_font_bold" id="S5.p4.1.20">RSS</span> have real-time results. Among these, <span class="ltx_text ltx_font_bold" id="S5.p4.1.21">ETPS</span> produces more compact superpixels but with low delineation, while <span class="ltx_text ltx_font_bold" id="S5.p4.1.22">SH</span> and <span class="ltx_text ltx_font_bold" id="S5.p4.1.23">RSS</span> performed similarly in all criteria with high boundary adherence but low compactness.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
The authors thank the Conselho Nacional de Desenvolvimento Científico e Tecnológico – CNPq – (Universal 407242/2021-0, PQ 303808/2018-7, 310075/2019-0), the Fundação de Amparo a Pesquisa do Estado de Minas Gerais – FAPEMIG – (PPM-00006-18), the Fundação de Amparo a Pesquisa do Estado de São Paulo – FAPESP – (2014/12236-1) and the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (COFECUB 88887.191730/2018-00 and Finance code 001) for the financial support.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achanta et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. 2012.

</span>
<span class="ltx_bibblock">SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 34, 11 (2012), 2274–2282.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2012.120" title="">https://doi.org/10.1109/TPAMI.2012.120</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achanta and Süsstrunk (2017)</span>
<span class="ltx_bibblock">
Radhakrishna Achanta and Sabine Süsstrunk. 2017.

</span>
<span class="ltx_bibblock">Superpixels and Polygons Using Simple Non-iterative Clustering. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 4895–4904.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2017.520" title="">https://doi.org/10.1109/CVPR.2017.520</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexandre et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Eduardo Barreto Alexandre, Ananda Shankar Chowdhury, Alexandre Xavier Falcao, and Paulo A. Vechiatto Miranda. 2015.

</span>
<span class="ltx_bibblock">IFT-SLIC: A General Framework for Superpixel Generation Based on Simple Linear Iterative Clustering and Image Foresting Transform. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">2015 28th SIBGRAPI Conference on Graphics, Patterns and Images</em>. 337–344.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/SIBGRAPI.2015.20" title="">https://doi.org/10.1109/SIBGRAPI.2015.20</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">An et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jianqiao An, Yucheng Shi, Yahong Han, Meijun Sun, and Qi Tian. 2020.

</span>
<span class="ltx_bibblock">Extract and Merge: Superpixel Segmentation with Regional Attributes. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">European Conference on Computer Vision</em>. Springer, 155–170.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aurenhammer (1987)</span>
<span class="ltx_bibblock">
F. Aurenhammer. 1987.

</span>
<span class="ltx_bibblock">Power Diagrams: Properties, Algorithms and Applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">SIAM J. Comput.</em> 16, 1 (1987), 78–96.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1137/0216006" title="">https://doi.org/10.1137/0216006</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ban et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhihua Ban, Jianguo Liu, and Li Cao. 2018.

</span>
<span class="ltx_bibblock">Superpixel Segmentation Using Gaussian Mixture Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">IEEE Transactions on Image Processing</em> 27, 8 (2018), 4105–4117.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2018.2836306" title="">https://doi.org/10.1109/TIP.2018.2836306</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ban et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhihua Ban, Jianguo Liu, and Jeremy Fouriaux. 2020.

</span>
<span class="ltx_bibblock">GMMSP on GPU.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Journal of Real-Time Image Processing</em> 17, 2 (2020), 245–257.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barcelos et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Isabela B Barcelos, Felipe De C Belém, Leonardo De M João, Alexandre X Falcão, and Guimarães Silvio JF. 2022.

</span>
<span class="ltx_bibblock">Improving color homogeneity measure in superpixel segmentation assessment. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</em>, Vol. 1. 79–84.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/SIBGRAPI55357.2022.9991772" title="">https://doi.org/10.1109/SIBGRAPI55357.2022.9991772</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bejar et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hans H.C. Bejar, Silvio Jamil Ferzoli Guimarães, and Paulo A.V. Miranda. 2020.

</span>
<span class="ltx_bibblock">Efficient hierarchical graph partitioning for image segmentation by optimum oriented cuts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Pattern Recognition Letters</em> 131 (2020), 185–192.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.patrec.2020.01.008" title="">https://doi.org/10.1016/j.patrec.2020.01.008</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bejar et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Hans HC Bejar, Lucy AC Mansilla, and Paulo AV Miranda. 2018.

</span>
<span class="ltx_bibblock">Efficient unsupervised image segmentation by optimum cuts in graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Iberoamerican Congress on Pattern Recognition</em>. Springer, 359–367.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belém et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Felipe Belém, Isabela Borlido, Leonardo João, Benjamin Perret, Jean Cousty, Silvio JF Guimarães, and Alexandre Falcão. 2022a.

</span>
<span class="ltx_bibblock">Fast and Effective Superpixel Segmentation Using Accurate Saliency Estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">International Conference on Discrete Geometry and Mathematical Morphology</em>. Springer, 261–273.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belém et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Felipe Belém, Silvio Jamil F Guimarães, and Alexandre Xavier Falcão. 2018.

</span>
<span class="ltx_bibblock">Superpixel segmentation by object-based iterative spanning forest. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Iberoamerican Congress on Pattern Recognition</em>. Springer, 334–341.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belém et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Felipe Belém, Benjamin Perret, Jean Cousty, Silvio JF Guimarães, and Alexandre Falcão. 2022b.

</span>
<span class="ltx_bibblock">Efficient Multiscale Object-based Superpixel Framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">arXiv preprint arXiv:2204.03533</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belém et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Felipe C. Belém, Silvio Jamil F. Guimarães, and Alexandre X. Falcão. 2020.

</span>
<span class="ltx_bibblock">Superpixel Segmentation Using Dynamic and Iterative Spanning Forest.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">IEEE Signal Processing Letters</em> 27 (2020), 1440–1444.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/LSP.2020.3015433" title="">https://doi.org/10.1109/LSP.2020.3015433</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belém et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Felipe C Belém, Benjamin Perret, Jean Cousty, Silvio JF Guimarães, and Alexandre X Falcão. 2021.

</span>
<span class="ltx_bibblock">Towards a Simple and Efficient Object-based Superpixel Delineation Framework. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</em>. IEEE, 346–353.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beucher (1992)</span>
<span class="ltx_bibblock">
Serge Beucher. 1992.

</span>
<span class="ltx_bibblock">The watershed transformation applied to image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Scanning Microscopy</em> 1992, 6 (1992), 28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bobbia et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Serge Bobbia, Richard Macwan, Yannick Benezeth, Keisuke Nakamura, Randy Gomez, and Julien Dubois. 2021.

</span>
<span class="ltx_bibblock">Iterative Boundaries implicit Identification for superpixels Segmentation: a real-time approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">IEEE Access</em> 9 (2021), 77250–77263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borlido Barcelos et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Isabela Borlido Barcelos, Felipe Belém, Paulo Miranda, Alexandre Xavier Falcão, Zenilton KG do Patrocínio, and Silvio Jamil F Guimarães. 2021.

</span>
<span class="ltx_bibblock">Towards interactive image segmentation by dynamic and iterative spanning forest. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">International Conference on Discrete Geometry and Mathematical Morphology</em>. Springer, 351–364.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boyd et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011.

</span>
<span class="ltx_bibblock">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Foundations and Trends® in Machine Learning</em> 3, 1 (2011), 1–122.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1561/2200000016" title="">https://doi.org/10.1561/2200000016</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buyssens et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Pierre Buyssens, Isabelle Gardin, and Su Ruan. 2014.

</span>
<span class="ltx_bibblock">Eikonal based region growing for superpixels generation: Application to semi-supervised real time organ segmentation in CT images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">IRBM</em> 35, 1 (2014), 20–26.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.irbm.2013.12.007" title="">https://doi.org/10.1016/j.irbm.2013.12.007</a>
</span>
<span class="ltx_bibblock">Biomedical image segmentation using variational and statistical approaches.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai (2020)</span>
<span class="ltx_bibblock">
Dengfeng Chai. 2020.

</span>
<span class="ltx_bibblock">Rooted Spanning Superpixels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">International Journal of Computer Vision</em> 128, 12 (2020), 2962–2978.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang and Chen (2018)</span>
<span class="ltx_bibblock">
Jia-Ren Chang and Yong-Sheng Chen. 2018.

</span>
<span class="ltx_bibblock">Pyramid stereo matching network. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 5410–5418.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2017a)</span>
<span class="ltx_bibblock">
Jiansheng Chen, Zhengqin Li, and Bo Huang. 2017a.

</span>
<span class="ltx_bibblock">Linear Spectral Clustering Superpixel.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">IEEE Transactions on Image Processing</em> 26, 7 (2017), 3317–3330.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2017.2651389" title="">https://doi.org/10.1109/TIP.2017.2651389</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2017b)</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. 2017b.

</span>
<span class="ltx_bibblock">Rethinking atrous convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:1706.05587</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. 2018.

</span>
<span class="ltx_bibblock">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the European Conference on Computer Vision (ECCV)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Condori et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Marcos A.T. Condori, Fábio A.M. Cappabianco, Alexandre X. Falcão, and Paulo A.V. Miranda. 2020.

</span>
<span class="ltx_bibblock">An extension of the differential image foresting transform and its application to superpixel generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Journal of Visual Communication and Image Representation</em> 71 (2020), 102748.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.jvcir.2019.102748" title="">https://doi.org/10.1016/j.jvcir.2019.102748</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Condori et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Marcos Ademir Tejada Condori, Fabio Augusto Menocci Cappabianco, Alexandre Xavier Falcão, and Paulo Andre Vechiatto De Miranda. 2017.

</span>
<span class="ltx_bibblock">Extending the differential image foresting transform to root-based path-cost functions with application to superpixel segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">2017 30th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</em>. Ieee, 7–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conrad et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Christian Conrad, Matthias Mertz, and Rudolf Mester. 2013.

</span>
<span class="ltx_bibblock">Contour-Relaxed Superpixels. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Energy Minimization Methods in Computer Vision and Pattern Recognition</em>. Springer Berlin Heidelberg, Berlin, Heidelberg, 280–293.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delong et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Andrew Delong, Anton Osokin, Hossam N Isack, and Yuri Boykov. 2012.

</span>
<span class="ltx_bibblock">Fast approximate energy minimization with label costs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">International journal of computer vision</em> 96, 1 (2012), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shuanhu Di, Miao Liao, Yuqian Zhao, Yang Li, and Yezhan Zeng. 2021.

</span>
<span class="ltx_bibblock">Image superpixel segmentation based on hierarchical multi-level LI-SLIC.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Optics &amp; Laser Technology</em> 135 (2021), 106703.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dijkstra et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (1959)</span>
<span class="ltx_bibblock">
Edsger W Dijkstra et al<span class="ltx_text" id="bib.bib32.3.1">.</span> 1959.

</span>
<span class="ltx_bibblock">A note on two problems in connexion with graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.4.1">Numerische mathematik</em> 1, 1 (1959), 269–271.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eliasof et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Moshe Eliasof, Nir Ben Zikri, and Eran Treister. 2022.

</span>
<span class="ltx_bibblock">Rethinking unsupervised neural superpixel segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">2022 IEEE International Conference on Image Processing (ICIP)</em>. IEEE, 3500–3504.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICIP46576.2022.9897484" title="">https://doi.org/10.1109/ICIP46576.2022.9897484</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falcão and Bergo (2004)</span>
<span class="ltx_bibblock">
Alexandre X Falcão and Felipe PG Bergo. 2004.

</span>
<span class="ltx_bibblock">Interactive volume segmentation with differential image foresting transforms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">IEEE Transactions on Medical Imaging</em> 23, 9 (2004), 1100–1108.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falcão et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Alexandre X Falcão, Jorge Stolfi, and Roberto de Alencar Lotufo. 2004.

</span>
<span class="ltx_bibblock">The image foresting transform: Theory, algorithms, and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 26, 1 (2004), 19–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Leyuan Fang, Shutao Li, Xudong Kang, and Jon Atli Benediktsson. 2015.

</span>
<span class="ltx_bibblock">Spectral–spatial classification of hyperspectral images with a superpixel-based discriminative sparse model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">IEEE Transactions on Geoscience and Remote Sensing</em> 53, 8 (2015), 4186–4201.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Francis et al<span class="ltx_text" id="bib.bib37.3.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jobin Francis, M Baburaj, and Sudhish N George. 2022.

</span>
<span class="ltx_bibblock">An <math alttext="l_{1/2}" class="ltx_Math" display="inline" id="bib.bib37.1.m1.1"><semantics id="bib.bib37.1.m1.1a"><msub id="bib.bib37.1.m1.1.1" xref="bib.bib37.1.m1.1.1.cmml"><mi id="bib.bib37.1.m1.1.1.2" xref="bib.bib37.1.m1.1.1.2.cmml">l</mi><mrow id="bib.bib37.1.m1.1.1.3" xref="bib.bib37.1.m1.1.1.3.cmml"><mn id="bib.bib37.1.m1.1.1.3.2" xref="bib.bib37.1.m1.1.1.3.2.cmml">1</mn><mo id="bib.bib37.1.m1.1.1.3.1" xref="bib.bib37.1.m1.1.1.3.1.cmml">/</mo><mn id="bib.bib37.1.m1.1.1.3.3" xref="bib.bib37.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="bib.bib37.1.m1.1b"><apply id="bib.bib37.1.m1.1.1.cmml" xref="bib.bib37.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib37.1.m1.1.1.1.cmml" xref="bib.bib37.1.m1.1.1">subscript</csymbol><ci id="bib.bib37.1.m1.1.1.2.cmml" xref="bib.bib37.1.m1.1.1.2">𝑙</ci><apply id="bib.bib37.1.m1.1.1.3.cmml" xref="bib.bib37.1.m1.1.1.3"><divide id="bib.bib37.1.m1.1.1.3.1.cmml" xref="bib.bib37.1.m1.1.1.3.1"></divide><cn id="bib.bib37.1.m1.1.1.3.2.cmml" type="integer" xref="bib.bib37.1.m1.1.1.3.2">1</cn><cn id="bib.bib37.1.m1.1.1.3.3.cmml" type="integer" xref="bib.bib37.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib37.1.m1.1c">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib37.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math> and Graph Regularized Subspace Clustering Method for Robust Image Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.4.1">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</em> 18, 2 (2022), 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galvão et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Felipe Lemes Galvão, Alexandre Xavier Falcão, and Ananda Shankar Chowdhury. 2018.

</span>
<span class="ltx_bibblock">RISF: recursive iterative spanning forest for superpixel segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</em>. IEEE, 408–415.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galvão et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Felipe Lemes Galvão, Silvio Jamil Ferzoli Guimarães, and Alexandre Xavier Falcão. 2020.

</span>
<span class="ltx_bibblock">Image segmentation using dense and sparse hierarchies of superpixels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Pattern Recognition</em> 108 (2020), 107532.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.patcog.2020.107532" title="">https://doi.org/10.1016/j.patcog.2020.107532</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaur and Manjunath (2020)</span>
<span class="ltx_bibblock">
Utkarsh Gaur and B. S. Manjunath. 2020.

</span>
<span class="ltx_bibblock">Superpixel Embedding Network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">IEEE Transactions on Image Processing</em> 29 (2020), 3199–3212.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2019.2957937" title="">https://doi.org/10.1109/TIP.2019.2957937</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giraud et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Rémi Giraud, Vinh-Thong Ta, and Nicolas Papadakis. 2016.

</span>
<span class="ltx_bibblock">SCALP: Superpixels with Contour Adherence using Linear Path. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">2016 23rd International Conference on Pattern Recognition (ICPR)</em>. 2374–2379.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICPR.2016.7899991" title="">https://doi.org/10.1109/ICPR.2016.7899991</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giraud et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rémi Giraud, Vinh-Thong Ta, and Nicolas Papadakis. 2018.

</span>
<span class="ltx_bibblock">Robust superpixels using color and contour features along linear path.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Computer Vision and Image Understanding</em> 170 (2018), 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Godard et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Clément Godard, Oisin Mac Aodha, and Gabriel J Brostow. 2017.

</span>
<span class="ltx_bibblock">Unsupervised monocular depth estimation with left-right consistency. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 270–279.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jianglei Gong, Nannan Liao, Cheng Li, Xiaojun Ma, Wangpeng He, and Baolong Guo. 2021.

</span>
<span class="ltx_bibblock">Superpixel Segmentation via Contour Optimized Non-Iterative Clustering. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">International Conference on Neural Computing for Advanced Applications</em>. Springer, 645–658.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yikai Gong, Richard O. Sinnott, and Paul Rimba. 2018.

</span>
<span class="ltx_bibblock">RT-DBSCAN: Real-Time Parallel Clustering of Spatio-Temporal Data Using Spark-Streaming. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Computational Science – ICCS 2018</em>, Yong Shi, Haohuan Fu, Yingjie Tian, Valeria V. Krzhizhanovskaya, Michael Harold Lees, Jack Dongarra, and Peter M. A. Sloot (Eds.). Springer International Publishing, Cham, 524–539.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grady (2006)</span>
<span class="ltx_bibblock">
Leo Grady. 2006.

</span>
<span class="ltx_bibblock">Random walks for image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">IEEE transactions on pattern analysis and machine intelligence</em> 28, 11 (2006), 1768–1783.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grewal et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Reaya Grewal, Singara Singh Kasana, and Geeta Kasana. 2023.

</span>
<span class="ltx_bibblock">Hyperspectral image segmentation: a comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Multimedia Tools and Applications</em> 82, 14 (2023), 20819–20872.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11042-022-13959-w" title="">https://doi.org/10.1007/s11042-022-13959-w</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Junyi Guan, Sheng Li, Xiongxiong He, and Jiajia Chen. 2021.

</span>
<span class="ltx_bibblock">Peak-Graph-Based Fast Density Peak Clustering for Image Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">IEEE Signal Processing Letters</em> 28 (2021), 897–901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guevara et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2011a)</span>
<span class="ltx_bibblock">
Alvaro Guevara, Christian Conrad, and Rudolf Mester. 2011a.

</span>
<span class="ltx_bibblock">Boosting segmentation results by contour relaxation. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">2011 18th IEEE International Conference on Image Processing</em>. IEEE, 1405–1408.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guevara et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2011b)</span>
<span class="ltx_bibblock">
Alvaro Guevara, Christian Conrad, and Rudolf Mester. 2011b.

</span>
<span class="ltx_bibblock">Boosting segmentation results by contour relaxation. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">2011 18th IEEE International Conference on Image Processing</em>. IEEE, 1405–1408.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICIP.2011.6115703" title="">https://doi.org/10.1109/ICIP.2011.6115703</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guigues et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Laurent Guigues, Jean Pierre Cocquerez, and Hervé Le Men. 2006.

</span>
<span class="ltx_bibblock">Scale-sets image analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">International Journal of Computer Vision</em> 68, 3 (2006), 289–317.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ashish Kumar Gupta, Ayan Seal, Pritee Khanna, Ondrej Krejcar, and Anis Yazidi. 2021.

</span>
<span class="ltx_bibblock">AWkS: adaptive, weighted k-means-based superpixels for improved saliency detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Pattern Analysis and Applications</em> 24, 2 (2021), 625–639.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou and Zhang (2007)</span>
<span class="ltx_bibblock">
Xiaodi Hou and Liqing Zhang. 2007.

</span>
<span class="ltx_bibblock">Saliency Detection: A Spectral Residual Approach. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">2007 IEEE Conference on Computer Vision and Pattern Recognition</em>. 1–8.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2007.383267" title="">https://doi.org/10.1109/CVPR.2007.383267</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jie Hu, Li Shen, and Gang Sun. 2018.

</span>
<span class="ltx_bibblock">Squeeze-and-Excitation Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 7132–7141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Joshua Zhexue Huang, Michael K Ng, Hongqiang Rong, and Zichen Li. 2005.

</span>
<span class="ltx_bibblock">Automated variable weighting in k-means type clustering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 27, 5 (2005), 657–668.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jampani et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Varun Jampani, Deqing Sun, Ming-Yu Liu, Ming-Hsuan Yang, and Jan Kautz. 2018.

</span>
<span class="ltx_bibblock">Superpixel sampling networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Proceedings of the European Conference on Computer Vision (ECCV)</em>. 352–368.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Zexuan Ji, Yubo Huang, Quansen Sun, and Guo Cao. 2016.

</span>
<span class="ltx_bibblock">A spatially constrained generative asymmetric Gaussian mixture model for image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Journal of Visual Communication and Image Representation</em> 40 (2016), 611–626.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xuejing Kang, Lei Zhu, and Anlong Ming. 2020.

</span>
<span class="ltx_bibblock">Dynamic Random Walk for Superpixel Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">IEEE Transactions on Image Processing</em> 29 (2020), 3871–3884.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2020.2967583" title="">https://doi.org/10.1109/TIP.2020.2967583</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke-Chen et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Song Ke-Chen, YAN Yun-Hui, CHEN Wen-Hui, and Xu Zhang. 2013.

</span>
<span class="ltx_bibblock">Research and perspective on local binary pattern.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Acta Automatica Sinica</em> 39, 6 (2013), 730–744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong and Fowlkes (2018)</span>
<span class="ltx_bibblock">
Shu Kong and Charless C Fowlkes. 2018.

</span>
<span class="ltx_bibblock">Recurrent pixel embedding for instance grouping. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 9018–9028.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krause et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Andreas Krause, Pietro Perona, and Ryan Gomes. 2010.

</span>
<span class="ltx_bibblock">Discriminative clustering by regularized information maximization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Advances in neural information processing systems</em> 23 (2010), 775–783.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar (2023)</span>
<span class="ltx_bibblock">
B Vinoth Kumar. 2023.

</span>
<span class="ltx_bibblock">An Extensive Survey on Superpixel Segmentation: A Research Perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Archives of Computational Methods in Engineering</em> (2023), 1–19.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11831-023-09919-8" title="">https://doi.org/10.1007/s11831-023-09919-8</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwak et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Suha Kwak, Seunghoon Hong, and Bohyung Han. 2017.

</span>
<span class="ltx_bibblock">Weakly supervised semantic segmentation using superpixel pooling network. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 31. 4111–4117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lempitsky et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Victor Lempitsky, Andrea Vedaldi, and Dmitry Ulyanov. 2018.

</span>
<span class="ltx_bibblock">Deep image prior. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE, 9446–9454.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levinshtein et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Alex Levinshtein, Adrian Stere, Kiriakos N Kutulakos, David J Fleet, Sven J Dickinson, and Kaleem Siddiqi. 2009.

</span>
<span class="ltx_bibblock">Turbopixels: Fast superpixels using geometric flows.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 31, 12 (2009), 2290–2297.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hua Li, Yuheng Jia, Runmin Cong, Wenhui Wu, Sam Tak Wu Kwong, and Chuanbo Chen. 2020.

</span>
<span class="ltx_bibblock">Superpixel segmentation based on spatially constrained subspace clustering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">IEEE Transactions on Industrial Informatics</em> 17, 11 (2020), 7501–7512.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiangjun Li, Yong Zhou, Xinping Zhang, Su Xu, and Peng Yu. 2021.

</span>
<span class="ltx_bibblock">Cluster-based fine-to-coarse superpixel segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Engineering Applications of Artificial Intelligence</em> 102 (2021), 104281.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Chen (2015)</span>
<span class="ltx_bibblock">
Zhengqin Li and Jiansheng Chen. 2015.

</span>
<span class="ltx_bibblock">Superpixel Segmentation Using Linear Spectral Clustering. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 1356–1363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yun Liang, Yuqing Zhang, Yihan Wu, Shuqin Tu, and Caixing Liu. 2020.

</span>
<span class="ltx_bibblock">Robust video object segmentation via propagating seams and matching superpixels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">IEEE Access</em> 8 (2020), 53766–53776.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Duan (2020)</span>
<span class="ltx_bibblock">
Guohua Liu and Jianchun Duan. 2020.

</span>
<span class="ltx_bibblock">RGB-D image segmentation using superpixel and multi-feature fusion graph theory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Signal, Image and Video Processing</em> 14, 6 (2020), 1171–1179.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiafei Liu, Qingsong Wang, Jianda Cheng, Deliang Xiang, and Wenbo Jing. 2022.

</span>
<span class="ltx_bibblock">Multitask Learning-Based for SAR Image Superpixel Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Remote Sensing</em> 14, 4 (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/rs14040899" title="">https://doi.org/10.3390/rs14040899</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Ming-Yu Liu, Oncel Tuzel, Srikumar Ramalingam, and Rama Chellappa. 2011.

</span>
<span class="ltx_bibblock">Entropy rate superpixel segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">CVPR 2011</em>. 2097–2104.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2011.5995323" title="">https://doi.org/10.1109/CVPR.2011.5995323</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, and Xiang Bai. 2017.

</span>
<span class="ltx_bibblock">Richer Convolutional Features for Edge Detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yun Liu, Peng-Tao Jiang, Vahan Petrosyan, Shi-Jie Li, Jiawang Bian, Le Zhang, and Ming-Ming Cheng. 2018.

</span>
<span class="ltx_bibblock">DEL: Deep Embedding Learning for Efficient Image Segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">IJCAI</em>. 864–870.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="10.24963/ijcai.2018/120" title="">10.24963/ijcai.2018/120</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loke et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Seng Cheong Loke, Bruce A MacDonald, Matthew Parsons, and Burkhard Claus Wünsche. 2021.

</span>
<span class="ltx_bibblock">Accelerated superpixel image segmentation with a parallelized DBSCAN algorithm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">Journal of Real-Time Image Processing</em> 18, 6 (2021), 2361–2376.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, and Shuicheng Yan. 2012.

</span>
<span class="ltx_bibblock">Robust and efficient subspace segmentation via least squares regression. In <em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">European conference on computer vision</em>. Springer, 347–360.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Dongyang Ma, Yuanfeng Zhou, Shiqing Xin, and Wenping Wang. 2020.

</span>
<span class="ltx_bibblock">Convex and compact superpixels by edge-constrained centroidal power diagram.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">IEEE Transactions on Image Processing</em> 30 (2020), 1825–1839.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Machairas et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Vaïa Machairas, Matthieu Faessel, David Cárdenas-Peña, Théodore Chabardes, Thomas Walter, and Etienne Decenciere. 2015.

</span>
<span class="ltx_bibblock">Waterpixels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">IEEE Transactions on Image Processing</em> 24, 11 (2015), 3707–3716.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maierhofer et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Georg Maierhofer, Daniel Heydecker, Angelica I. Aviles-Rivero, Samar M. Alsaleh, and Carola-Bibiane Schonlieb. 2018.

</span>
<span class="ltx_bibblock">Peekaboo-Where are the Objects? Structure Adjusting Superpixels. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">2018 25th IEEE International Conference on Image Processing (ICIP)</em>. 3693–3697.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICIP.2018.8451822" title="">https://doi.org/10.1109/ICIP.2018.8451822</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mansilla and Miranda (2013)</span>
<span class="ltx_bibblock">
Lucy AC Mansilla and Paulo AV Miranda. 2013.

</span>
<span class="ltx_bibblock">Image segmentation by oriented image foresting transform: Handling ties and colored images. In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">2013 18th International Conference on Digital Signal Processing (DSP)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mansilla and Miranda (2016)</span>
<span class="ltx_bibblock">
Lucy A. C. Mansilla and Paulo A. V. Miranda. 2016.

</span>
<span class="ltx_bibblock">Oriented Image Foresting Transform Segmentation: Connectivity Constraints with Adjustable Width. In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">2016 29th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</em>. 289–296.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/SIBGRAPI.2016.047" title="">https://doi.org/10.1109/SIBGRAPI.2016.047</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
David R Martin, Charless C Fowlkes, and Jitendra Malik. 2004.

</span>
<span class="ltx_bibblock">Learning to detect natural image boundaries using local brightness, color, and texture cues.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 26, 5 (2004), 530–549.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2004.1273918" title="">https://doi.org/10.1109/TPAMI.2004.1273918</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathieu et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Bérengère Mathieu, Alain Crouzil, and Jean Baptiste Puel. 2017.

</span>
<span class="ltx_bibblock">Oversegmentation methods: a new evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">Iberian Conference on Pattern Recognition and Image Analysis</em>. Springer, 185–193.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mester et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2011a)</span>
<span class="ltx_bibblock">
Rudolf Mester, Christian Conrad, and Alvaro Guevara. 2011a.

</span>
<span class="ltx_bibblock">Multichannel segmentation using contour relaxation: fast super-pixels and temporal propagation. In <em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">Scandinavian conference on image analysis</em>. Springer, 250–261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mester et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2011b)</span>
<span class="ltx_bibblock">
Rudolf Mester, Christian Conrad, and Alvaro Guevara. 2011b.

</span>
<span class="ltx_bibblock">Multichannel segmentation using contour relaxation: fast super-pixels and temporal propagation. In <em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Scandinavian conference on image analysis</em>, Anders Heyden and Fredrik Kahl (Eds.). Springer, Berlin, Heidelberg, 250–261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miranda and Mansilla (2013)</span>
<span class="ltx_bibblock">
Paulo AV Miranda and Lucy AC Mansilla. 2013.

</span>
<span class="ltx_bibblock">Oriented image foresting transform segmentation by seed competition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">IEEE Transactions on Image Processing</em> 23, 1 (2013), 389–398.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moore et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Alastair P Moore, Simon JD Prince, Jonathan Warrell, Umar Mohammed, and Graham Jones. 2008.

</span>
<span class="ltx_bibblock">Superpixel lattices. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">2008 IEEE conference on computer vision and pattern recognition</em>. IEEE, 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neubert and Protzel (2012)</span>
<span class="ltx_bibblock">
Peer Neubert and Peter Protzel. 2012.

</span>
<span class="ltx_bibblock">Superpixel benchmark and comparison. In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proc. Forum Bildverarbeitung</em>, Vol. 6. KIT Scientific Publishing, 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiao Pan, Yuanfeng Zhou, Yunfeng Zhang, and Caiming Zhang. 2022.

</span>
<span class="ltx_bibblock">Fast Generation of Superpixels With Lattice Topology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">IEEE Transactions on Image Processing</em> 31 (2022), 4828–4841.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2022.3188155" title="">https://doi.org/10.1109/TIP.2022.3188155</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hankui Peng, Angelica I Aviles-Rivero, and Carola-Bibiane Schönlieb. 2022.

</span>
<span class="ltx_bibblock">HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate Segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>. 217–226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polya (2020)</span>
<span class="ltx_bibblock">
George Polya. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Mathematics and plausible reasoning: Induction and analogy in mathematics</em>. Vol. 1.

</span>
<span class="ltx_bibblock">Princeton University Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao and Di (2022)</span>
<span class="ltx_bibblock">
Nianzu Qiao and Lamei Di. 2022.

</span>
<span class="ltx_bibblock">An improved method of linear spectral clustering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Multimedia Tools and Applications</em> 81, 1 (2022), 1287–1311.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. 2020.

</span>
<span class="ltx_bibblock">U2-Net: Going deeper with nested U-structure for salient object detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Pattern Recognition</em> 106 (2020), 107404.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Lang Ren, Liaoying Zhao, and Yulei Wang. 2019.

</span>
<span class="ltx_bibblock">A superpixel-based dual window RX for hyperspectral anomaly detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">IEEE Geoscience and Remote Sensing Letters</em> 17, 7 (2019), 1233–1237.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren and Malik (2003)</span>
<span class="ltx_bibblock">
Xiaofeng Ren and Jitendra Malik. 2003.

</span>
<span class="ltx_bibblock">Learning a classification model for segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Proceedings Ninth IEEE International Conference on Computer Vision</em>, Vol. 2. IEEE Computer Society, 10–17.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCV.2003.1238308" title="">https://doi.org/10.1109/ICCV.2003.1238308</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodriguez and Laio (2014)</span>
<span class="ltx_bibblock">
Alex Rodriguez and Alessandro Laio. 2014.

</span>
<span class="ltx_bibblock">Clustering by fast search and find of density peaks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Science</em> 344, 6191 (2014), 1492–1496.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1126/science.1242072" title="">https://doi.org/10.1126/science.1242072</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">International Conference on Medical image computing and computer-assisted intervention</em>. Springer, 234–241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sasmal and Dhal (2023)</span>
<span class="ltx_bibblock">
Buddhadev Sasmal and Krishna Gopal Dhal. 2023.

</span>
<span class="ltx_bibblock">A survey on the utilization of Superpixel image for clustering based image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Multimedia Tools and Applications</em> (2023), 1–63.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11042-023-14861-9" title="">https://doi.org/10.1007/s11042-023-14861-9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Alexander Schick, Mika Fischer, and Rainer Stiefelhagen. 2012.

</span>
<span class="ltx_bibblock">Measuring and evaluating the compactness of superpixels. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Proceedings of the 21st international conference on pattern recognition (ICPR2012)</em>. IEEE, 930–934.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Alexander Schick, Mika Fischer, and Rainer Stiefelhagen. 2014.

</span>
<span class="ltx_bibblock">An evaluation of the compactness of superpixels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">Pattern Recognition Letters</em> 43 (2014), 71–80.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellars et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Philip Sellars, Angelica I Aviles-Rivero, and Carola-Bibiane Schönlieb. 2020.

</span>
<span class="ltx_bibblock">Superpixel contracted graph-based learning for hyperspectral image classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">IEEE Transactions on Geoscience and Remote Sensing</em> 58, 6 (2020), 4180–4193.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sethian (1999)</span>
<span class="ltx_bibblock">
James Albert Sethian. 1999.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Level set methods and fast marching methods: evolving interfaces in computational geometry, fluid mechanics, computer vision, and materials science</em>. Vol. 3.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Sayed Asad Hussain Shah, Liang Li, Yajun Li, and Jiawan Zhang. 2021.

</span>
<span class="ltx_bibblock">Superpixel Segmentation via Density Peaks. In <em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">2021 4th International Conference on Information and Computer Technologies (ICICT)</em>. IEEE, 93–98.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Jianbing Shen, Xiaopeng Hao, Zhiyuan Liang, Yu Liu, Wenguan Wang, and Ling Shao. 2016.

</span>
<span class="ltx_bibblock">Real-Time Superpixel Segmentation by DBSCAN Clustering Algorithm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">IEEE Transactions on Image Processing</em> 25, 12 (2016), 5933–5942.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2016.2616302" title="">https://doi.org/10.1109/TIP.2016.2616302</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Bin Sheng, Ping Li, Shuangjia Mo, Huating Li, Xuhong Hou, Qiang Wu, Jing Qin, Ruogu Fang, and David Dagan Feng. 2018.

</span>
<span class="ltx_bibblock">Retinal vessel segmentation using minimum spanning superpixel tree detector.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">IEEE transactions on cybernetics</em> 49, 7 (2018), 2707–2719.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi and Malik (2000)</span>
<span class="ltx_bibblock">
Jianbo Shi and J. Malik. 2000.

</span>
<span class="ltx_bibblock">Normalized cuts and image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 22, 8 (2000), 888–905.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/34.868688" title="">https://doi.org/10.1109/34.868688</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. 2015.

</span>
<span class="ltx_bibblock">Hierarchical image saliency detection on extended CSSD.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">IEEE transactions on pattern analysis and machine intelligence</em> 38, 4 (2015), 717–729.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silberman et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. 2012.

</span>
<span class="ltx_bibblock">Indoor segmentation and support inference from RGB-D images. In <em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12</em>. Springer, 746–760.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stutz (2015)</span>
<span class="ltx_bibblock">
David Stutz. 2015.

</span>
<span class="ltx_bibblock">Superpixel segmentation: An evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">German conference on pattern recognition</em>. Springer, 555–562.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stutz et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
David Stutz, Alexander Hermans, and Bastian Leibe. 2018.

</span>
<span class="ltx_bibblock">Superpixels: An evaluation of the state-of-the-art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">Computer Vision and Image Understanding</em> 166 (2018), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzuki (2020)</span>
<span class="ltx_bibblock">
Teppei Suzuki. 2020.

</span>
<span class="ltx_bibblock">Superpixel Segmentation Via Convolutional Neural Networks with Regularized Information Maximization. In <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. 2573–2577.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICASSP40776.2020.9054140" title="">https://doi.org/10.1109/ICASSP40776.2020.9054140</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tomasi and Manduchi (1998)</span>
<span class="ltx_bibblock">
Carlo Tomasi and Roberto Manduchi. 1998.

</span>
<span class="ltx_bibblock">Bilateral filtering for gray and color images. In <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">Sixth international conference on computer vision (IEEE Cat. No. 98CH36271)</em>. IEEE, 839–846.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei-Chih Tu, Ming-Yu Liu, Varun Jampani, Deqing Sun, Shao-Yi Chien, Ming-Hsuan Yang, and Jan Kautz. 2018.

</span>
<span class="ltx_bibblock">Learning superpixels with segmentation-aware affinity loss. In <em class="ltx_emph ltx_font_italic" id="bib.bib113.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 568–576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ullah et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shakir Ullah, Naeem Bhatti, and Muhammad Zia. 2021.

</span>
<span class="ltx_bibblock">Adaptive tuning of SLIC parameter K.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.3.1">Multimedia Tools and Applications</em> 80, 17 (2021), 25649–25672.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van den Bergh et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Michael Van den Bergh, Xavier Boix, Gemma Roig, Benjamin de Capitani, and Luc Van Gool. 2012.

</span>
<span class="ltx_bibblock">SEEDS: Superpixels Extracted via Energy-Driven Sampling. In <em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">Computer Vision – ECCV 2012</em>, Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 13–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van den Bergh et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Michael Van den Bergh, Xavier Boix, Gemma Roig, and Luc Van Gool. 2015.

</span>
<span class="ltx_bibblock">Seeds: Superpixels extracted via energy-driven sampling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">International Journal of Computer Vision</em> 111, 3 (2015), 298–314.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11263-014-0744-2" title="">https://doi.org/10.1007/s11263-014-0744-2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vargas-Muñoz et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
John E Vargas-Muñoz, Ananda S Chowdhury, Eduardo B Alexandre, Felipe L Galvão, Paulo A Vechiatto Miranda, and Alexandre X Falcão. 2019.

</span>
<span class="ltx_bibblock">An iterative spanning forest framework for superpixel segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.3.1">IEEE Transactions on Image Processing</em> 28, 7 (2019), 3477–3489.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Gaochao Wang, Yiheng Wei, and Peter Tse. 2018.

</span>
<span class="ltx_bibblock">Clustering by defining and merging candidates of cluster centers via independence and affinity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">Neurocomputing</em> 315 (2018), 486–495.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neucom.2018.07.043" title="">https://doi.org/10.1016/j.neucom.2018.07.043</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Hui Wang, Jianbing Shen, Junbo Yin, Xingping Dong, Hanqiu Sun, and Ling Shao. 2020b.

</span>
<span class="ltx_bibblock">Adaptive Nonlocal Random Walks for Image Superpixel Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">IEEE Transactions on Circuits and Systems for Video Technology</em> 30, 3 (2020), 822–834.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TCSVT.2019.2896438" title="">https://doi.org/10.1109/TCSVT.2019.2896438</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jingjing Wang, Zhenye Luan, Zishu Yu, Jinwen Ren, Jun Gao, Kejiang Yuan, and Huaqiang Xu. 2022.

</span>
<span class="ltx_bibblock">Superpixel segmentation with squeeze-and-excitation networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">Signal, Image and Video Processing</em> (2022), 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Kai Wang, Liang Li, and Jiawan Zhang. 2020a.

</span>
<span class="ltx_bibblock">End-to-end trainable network for superpixel and image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">Pattern Recognition Letters</em> 140 (2020), 135–142.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.patrec.2020.09.016" title="">https://doi.org/10.1016/j.patrec.2020.09.016</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, and Yulan Guo. 2019.

</span>
<span class="ltx_bibblock">Learning Parallax Attention for Stereo Image Super-Resolution. In <em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Murong Wang, Xiabi Liu, Yixuan Gao, Xiao Ma, and Nouman Q Soomro. 2017.

</span>
<span class="ltx_bibblock">Superpixel segmentation: A benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">Signal Processing: Image Communication</em> 56 (2017), 28–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Zhang (2021)</span>
<span class="ltx_bibblock">
Nannan Wang and Yongxia Zhang. 2021.

</span>
<span class="ltx_bibblock">Adaptive and fast image superpixel segmentation approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">Image and Vision Computing</em> 116 (2021), 104315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Wu (2017)</span>
<span class="ltx_bibblock">
Weiwei Wang and Cuiling Wu. 2017.

</span>
<span class="ltx_bibblock">Image segmentation by correlation adaptive weighted regression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Neurocomputing</em> 267 (2017), 426–435.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neucom.2017.06.046" title="">https://doi.org/10.1016/j.neucom.2017.06.046</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Xuehui Wang, Qingyun Zhao, Lei Fan, Yuzhi Zhao, Tiantian Wang, Qiong Yan, and Long Chen. 2021b.

</span>
<span class="ltx_bibblock">Semasuperpixel: A Multi-Channel Probability-Driven Superpixel Segmentation Method. In <em class="ltx_emph ltx_font_italic" id="bib.bib126.3.1">2021 IEEE International Conference on Image Processing (ICIP)</em>. IEEE, 1859–1863.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang. 2021a.

</span>
<span class="ltx_bibblock">AINet: Association implantation for superpixel segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 7078–7087.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xing Wei, Qingxiong Yang, Yihong Gong, Narendra Ahuja, and Ming-Hsuan Yang. 2018.

</span>
<span class="ltx_bibblock">Superpixel hierarchy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">IEEE Transactions on Image Processing</em> 27, 10 (2018), 4838–4849.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">West et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Douglas Brent West et al<span class="ltx_text" id="bib.bib129.3.1">.</span> 2001.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.4.1">Introduction to graph theory</em>. Vol. 2.

</span>
<span class="ltx_bibblock">Prentice hall Upper Saddle River.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Jiang Wu, Chunxiao Liu, and Biao Li. 2021b.

</span>
<span class="ltx_bibblock">Texture-aware and structure-preserving superpixel segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">Computers &amp; Graphics</em> 94 (2021), 152–163.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Ruiqi Wu, Yajuan Du, Hua Li, and Yucong Dai. 2021a.

</span>
<span class="ltx_bibblock">Stereo Superpixel Segmentation Via Dual-Attention Fusion Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">2021 IEEE International Conference on Multimedia and Expo (ICME)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiang Wu, Yufei Chen, Xianhui Liu, Jianan Shen, Keqiang Zhuo, and Weidong Zhao. 2020.

</span>
<span class="ltx_bibblock">Superpixel via coarse-to-fine boundary shift.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.3.1">Applied Intelligence</em> 50, 7 (2020), 2079–2092.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Juanying Xie, Hongchao Gao, Weixin Xie, Xiaohui Liu, and Philip W. Grant. 2016.

</span>
<span class="ltx_bibblock">Robust clustering by detecting density peaks and assigning points based on fuzzy weighted K-nearest neighbors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">Information Sciences</em> 354 (2016), 19–40.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.ins.2016.03.011" title="">https://doi.org/10.1016/j.ins.2016.03.011</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamaguchi et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Koichiro Yamaguchi, David McAllester, and Raquel Urtasun. 2014.

</span>
<span class="ltx_bibblock">Efficient joint segmentation, occlusion labeling, stereo and flow estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib134.3.1">European Conference on Computer Vision</em>, Fleet David, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (Eds.), Vol. 8693. Springer, Cham, 756–771.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-319-10602-1_49" title="">https://doi.org/10.1007/978-3-319-10602-1_49</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Fengting Yang, Qian Sun, Hailin Jin, and Zihan Zhou. 2020.

</span>
<span class="ltx_bibblock">Superpixel Segmentation With Fully Convolutional Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jian Yao, Marko Boben, Sanja Fidler, and Raquel Urtasun. 2015.

</span>
<span class="ltx_bibblock">Real-time coarse-to-fine topologically preserving segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2947–2955.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yue Yu, Yang Yang, and Kezhao Liu. 2021.

</span>
<span class="ltx_bibblock">Edge-Aware Superpixel Segmentation with Unsupervised Convolutional Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib137.3.1">2021 IEEE International Conference on Image Processing (ICIP)</em>. IEEE, 1504–1508.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Hongxing Yuan, Shaoqun Wu, Peihong Cheng, Peng An, and Shudi Bao. 2015.

</span>
<span class="ltx_bibblock">Nonlocal Random Walks Algorithm for Semi-Automatic 2D-to-3D Image Conversion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.3.1">IEEE Signal Processing Letters</em> 22, 3 (2015), 371–374.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/LSP.2014.2359643" title="">https://doi.org/10.1109/LSP.2014.2359643</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Qing Yuan, Songfeng Lu, Yan Huang, and Wuxin Sha. 2021a.

</span>
<span class="ltx_bibblock">SIN: Superpixel Interpolation Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib139.3.1">Pacific Rim International Conference on Artificial Intelligence</em>. Springer, 293–307.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib140.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Ye Yuan, Wei Zhang, Hai Yu, and Zhiliang Zhu. 2021b.

</span>
<span class="ltx_bibblock">Superpixels With Content-Adaptive Criteria.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.3.1">IEEE Transactions on Image Processing</em> 30 (2021), 7702–7716.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ye Yuan, Zhiliang Zhu, Hai Yu, and Wei Zhang. 2020.

</span>
<span class="ltx_bibblock">Watershed-Based Superpixels With Global and Local Boundary Marching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">IEEE Transactions on Image Processing</em> 29 (2020), 7375–7388.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TIP.2020.3002078" title="">https://doi.org/10.1109/TIP.2020.3002078</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2021c)</span>
<span class="ltx_bibblock">
Bin Zhang, Xuejing Kang, and Anlong Ming. 2021c.

</span>
<span class="ltx_bibblock">BP-net: deep learning-based superpixel segmentation for RGB-D image. In <em class="ltx_emph ltx_font_italic" id="bib.bib142.3.1">2020 25th International Conference on Pattern Recognition (ICPR)</em>. IEEE, 7433–7438.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jianchao Zhang, Angelica I Aviles-Rivero, Daniel Heydecker, Xiaosheng Zhuang, Raymond Chan, and Carola-Bibiane Schönlieb. 2021a.

</span>
<span class="ltx_bibblock">Dynamic spectral residual superpixels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">Pattern Recognition</em> 112 (2021), 107705.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jianhua Zhang, Jingbo Chen, Qichao Wang, and Shengyong Chen. 2019.

</span>
<span class="ltx_bibblock">Spatiotemporal saliency detection based on maximum consistency superpixels merging for video analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.3.1">IEEE Transactions on Industrial Informatics</em> 16, 1 (2019), 606–614.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Yongxia Zhang, Qiang Guo, and Caiming Zhang. 2021b.

</span>
<span class="ltx_bibblock">Simple and fast image superpixels generation with color and boundary probability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">The Visual Computer</em> 37 (2021), 1061–1074.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00371-020-01852-2" title="">https://doi.org/10.1007/s00371-020-01852-2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei Zhao, Yi Fu, Xiaosong Wei, and Hai Wang. 2018.

</span>
<span class="ltx_bibblock">An improved image semantic segmentation method based on superpixels and conditional random fields.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">Applied Sciences</em> 8, 5 (2018), 837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xianen Zhou, Yaonan Wang, Qing Zhu, Changyan Xiao, and Xiao Lu. 2019.

</span>
<span class="ltx_bibblock">SSG: superpixel segmentation and grabcut-based salient object segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">The Visual Computer</em> 35, 3 (2019), 385–398.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Lei Zhu, Qi She, Bin Zhang, Yanye Lu, Zhilin Lu, Duo Li, and Jie Hu. 2021.

</span>
<span class="ltx_bibblock">Learning the Superpixel in a Non-iterative and Lifelong Manner. In <em class="ltx_emph ltx_font_italic" id="bib.bib148.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 1225–1234.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zohrizadeh et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Fariba Zohrizadeh, Mohsen Kheirandishfard, and Farhad Kamangar. 2018.

</span>
<span class="ltx_bibblock">Image Segmentation Using Sparse Subset Selection. In <em class="ltx_emph ltx_font_italic" id="bib.bib149.3.1">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>. 1470–1479.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/WACV.2018.00165" title="">https://doi.org/10.1109/WACV.2018.00165</a>
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Superpixel segmentation methods</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Superpixel segmentation has a vast literature covering several techniques. In <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, a benchmark for superpixels is provided with an extensive evaluation of methods. Nevertheless, due to the rapid progress in developing new strategies for superpixel segmentation, an analysis of the most recent proposals becomes essential. Therefore, this section reviews 59 methods in recent and commonly used literature on superpixel segmentation.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Neighborhood-based clustering</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Neighborhood-based methods for superpixel segmentation perform clustering of image pixels based on the similarity between pixels restricted to a maximum spatial distance from some reference point in the image. For example, several methods constrain the clustering region of a superpixel to a fixed-size image patch around this superpixel <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>; Ullah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib114" title="">2021</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib130" title="">2021b</a>; Liu and Duan, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib70" title="">2020</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1. </span>SLIC</h4>
<div class="ltx_para" id="A1.SS1.SSS1.p1">
<p class="ltx_p" id="A1.SS1.SSS1.p1.1">SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> starts with a grid sampling of superpixel centers and iteratively assigns at each superpixel the most similar pixels in a limited region around the superpixel center. As post-processing, SLIC ensures connectivity by assigning unconnected superpixels to their nearest neighbors. SLIC reduces the segmentation complexity to linear concerning the number of pixels. Also, its distance function gives better control over the superpixel size and compactness. Although SLIC presents fair delineation and efficiency, it does not consider the relationship between adjacent pixels, resulting in worse delineation in regions with complex textures.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2. </span>K-SLIC</h4>
<div class="ltx_para" id="A1.SS1.SSS2.p1">
<p class="ltx_p" id="A1.SS1.SSS2.p1.1">The authors <cite class="ltx_cite ltx_citemacro_citep">(Ullah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib114" title="">2021</a>)</cite> propose a granulometric approach and a quality metric method to allow controlling the number of desired superpixels in a SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> segmentation. The former represents the relative importance of the image components computed for each color channel and the second uses several metrics based on entropy, texture, and ground-truth independent quality metrics to choose by the majority vote. In bad-lighted conditions, the quality metric method is less affected and provides a large number of superpixels as compared to the granulometric process and performs better with different spatial resolutions. Despite its improved results, the quality metric method is computationally expensive, while the granulometric one has worse performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.3. </span>LSC</h4>
<div class="ltx_para" id="A1.SS1.SSS3.p1">
<p class="ltx_p" id="A1.SS1.SSS3.p1.1">The authors <cite class="ltx_cite ltx_citemacro_citep">(Li and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib68" title="">2015</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib24" title="">2017a</a>)</cite> investigated the relationship between the normalized cuts <cite class="ltx_cite ltx_citemacro_citep">(Ren and Malik, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib95" title="">2003</a>)</cite> and the weighted K-means to propose the LSC, which uses an NCut function that can obtain the same optimum result as the weighted kernel K-means. The LSC applies a kernel function to map pixels into a 10-dimensional feature space in a fixed limited region. LSC provides an efficient segmentation method and it obtains regular shapes. It also has linear time complexity with high memory efficiency. By considering a shape constraint, LSC achieves high boundary adherence without sacrificing spatial compactness. However, its fixed search range prevents LSC from ensuring connectivity, requiring post-processing.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.4. </span>SCALP</h4>
<div class="ltx_para" id="A1.SS1.SSS4.p1">
<p class="ltx_p" id="A1.SS1.SSS4.p1.1">SCALP <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib41" title="">2016</a>)</cite> considers image features and contour intensity on a linear path to the superpixel barycenter to improve SLIC’s <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> distance function with neighborhood information. It integrates the contour prior information as a soft constraint in the color distance to improve the adherence to the object boundaries and performs clustering in high-dimensional feature space <cite class="ltx_cite ltx_citemacro_citep">(Li and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib68" title="">2015</a>)</cite>. SCALP is efficient, robust to noise, and produces compact superpixels. The authors further improve SCALP <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>)</cite> with a hard constraint based on the contour prior to providing an initial segmentation. The hard constraint increases SCALP’s robustness and its boundary adherence, but it slightly reduces regularity and smoothness.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.5. </span>TASP</h4>
<div class="ltx_para" id="A1.SS1.SSS5.p1">
<p class="ltx_p" id="A1.SS1.SSS5.p1.1">TASP <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib130" title="">2021b</a>)</cite> intends to solve the problem of handling weak gradient structures and strong gradient textures. The proposal’s pipeline is based on SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> with an integrated structure-avoiding clustering distance based on a centroid-oriented quarter-circular mask and a hybrid gradient. <span class="ltx_text" id="A1.SS1.SSS5.p1.1.1" style="color:#000000;"> The proposed mask prevents inconsistent texture pixels from being sampled from the local image patch. TASP has an effective structure-preserving and texture-suppression procedure, especially in images with strong texture and weak boundary structures</span>. However, TASP is highly time-consuming and does not produce more superpixels in regions with finer details, missing some structure boundaries.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.6. </span>MFGS</h4>
<div class="ltx_para" id="A1.SS1.SSS6.p1">
<p class="ltx_p" id="A1.SS1.SSS6.p1.1">MFGS <cite class="ltx_cite ltx_citemacro_citep">(Liu and Duan, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib70" title="">2020</a>)</cite> is a two-stage method for superpixel segmentation in RGB-D images. In the first stage, MFGS uses color, and 2D and 3D spatial positions (with depth) to perform iterative clustering. Then, it performs a merging multi-feature step to estimate the similarity between superpixels. Also, it uses the label cost proposed in <cite class="ltx_cite ltx_citemacro_citep">(Delong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib30" title="">2012</a>)</cite> to remove redundant labels. MFGS is faster, produces compact and regular superpixels, and has a higher segmentation accuracy. However, the proposal’s merging stage does not allow control of the number of final superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.7. </span>DSR</h4>
<div class="ltx_para" id="A1.SS1.SSS7.p1">
<p class="ltx_p" id="A1.SS1.SSS7.p1.1">DSR <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib143" title="">2021a</a>)</cite> incorporates saliency information into the seed sampling and clustering stages. The proposed method computes the saliency map based on Fourier analysis <cite class="ltx_cite ltx_citemacro_citep">(Hou and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib53" title="">2007</a>)</cite> and uses a structure measure function to define the search range for clustering and seed sampling. DSR performs clustering similar to SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, but with a search range based on the structure measure to connect uniform regions, avoiding unnecessary small superpixels in large regions. DSR creates more seeds in heterogeneous areas but avoids creating redundant seeds. Also, it produces larger (and fewer) superpixels on homogenous regions, by connecting pixels in a range search based on saliency. Compared to SLIC, DSR provides a consistent performance improvement by increasing a low computational load, producing superpixels that capture more details, and reducing the redundancy of the represented information. However, it creates less regular and compact superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.8. </span>Semasuperpixel</h4>
<div class="ltx_para" id="A1.SS1.SSS8.p1">
<p class="ltx_p" id="A1.SS1.SSS8.p1.1">The proposal in <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib126" title="">2021b</a>)</cite> improves SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> clustering with a new distance measure function including semantic information. The proposal clusters pixels based on semantic information and uses color and spatial information as refinement factors. The authors use a DeepLab v3+ <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib26" title="">2018</a>)</cite> network to obtain semantic information. Semasuperpixel achieves excellent boundary adherence and substantially reduces leakage achieving improved performance compared to SLIC.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS9">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.9. </span>AWkS</h4>
<div class="ltx_para" id="A1.SS1.SSS9.p1">
<p class="ltx_p" id="A1.SS1.SSS9.p1.1">AWkS <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib52" title="">2021</a>)</cite> adopts dynamic weighted distances based on weighted k-means clustering (W-k-means) <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib55" title="">2005</a>)</cite> and proposes an adaptative term for each variable in its distance formulation. The proposed method extends SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> to explore the degree of feature relevances during objective function minimization, adopting a pipeline similar to SLIC AWks outperforms SLIC in boundary adherence and produces visually better segmentations, with more compact superpixels and fewer small ones close to the image boundaries. However, the proposal has a high running time compared to SLIC.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Boundary evolution clustering</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In boundary evolution clustering, the algorithm iteratively updates the superpixels’ boundaries to improve delineation, usually using a coarse-to-fine image block strategy. SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>)</cite> and ETPS <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>)</cite> are examples of superpixel methods using the boundary evolution strategy for clustering.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1. </span>SEEDS</h4>
<div class="ltx_para" id="A1.SS2.SSS1.p1">
<p class="ltx_p" id="A1.SS2.SSS1.p1.1">SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib116" title="">2015</a>)</cite> start from a regular grid partitioning and iteratively refine the superpixels’ boundaries. The iterative process follows a coarse-to-fine approach with a hill-climbing algorithm for optimization. SEEDS is an efficient method that performs optimization based on a hill-climbing algorithm. SEEDS introduces an energy function that encourages color homogeneity, shape regularity, and smooth boundary shapes. However, the compactness constraint degrades the results, and the number of superpixels is challenging to control.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2. </span>CRS</h4>
<div class="ltx_para" id="A1.SS2.SSS2.p1">
<p class="ltx_p" id="A1.SS2.SSS2.p1.1">CRS <cite class="ltx_cite ltx_citemacro_citep">(Conrad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib29" title="">2013</a>)</cite> formulates the segmentation problem as an estimation task and transforms the model in <cite class="ltx_cite ltx_citemacro_citep">(Mester et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib85" title="">2011b</a>; Guevara et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib50" title="">2011b</a>)</cite> into a superpixel approach. From an initial image partition, CRS generates superpixels under the constraint of maximum homogeneity inside each image patch and maximum accordance of the contours with both the image content and a Gibbs-Markov random field model. CRS explicitly models the superpixel’s shape and content as a statistical model, allowing it to handle an arbitrary number of feature channels. In addition, CRS allows direct control of the number of superpixels and their compacity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3. </span>ETPS</h4>
<div class="ltx_para" id="A1.SS2.SSS3.p1">
<p class="ltx_p" id="A1.SS2.SSS3.p1.1">Inspired by SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib115" title="">2012</a>)</cite>, ETPS <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib136" title="">2015</a>)</cite> performs a coarse-to-fine approach to superpixel segmentation, starting with grid partitioning. ETPS uses a priority list to optimize its energy function. Also, despite its energy function being at the pixel level, it measures shape regularization, color homogeneity, and smoothness of the contours. In addition, ETPS enforces connectivity and minimum size during the optimization process. The authors also presented a stereo version of the proposal and demonstrated that ETPS’ efficiency surpasses SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>. Compared to <cite class="ltx_cite ltx_citemacro_citep">(Yamaguchi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib134" title="">2014</a>)</cite>, ETPS achieves a better convergence value in a single iteration.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.4. </span>IBIS</h4>
<div class="ltx_para" id="A1.SS2.SSS4.p1">
<p class="ltx_p" id="A1.SS2.SSS4.p1.1">IBIS <cite class="ltx_cite ltx_citemacro_citep">(Bobbia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib18" title="">2021</a>)</cite> starts with a grid segmentation and, using the same distance measure in SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>compares the pixels located on the edge of the blocks, subdividing them into four blocks assigned to another superpixel. At each iteration, pixels in non-homogeneous blocks are assigned to the nearest superpixel according to the SLIC’s distance measure. After the clustering step, IBIS performs the same merging stage as SLIC. In <cite class="ltx_cite ltx_citemacro_citep">(Bobbia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib18" title="">2021</a>)</cite>, the authors also present a GPU variant aimed at real-time use cases, the IBIScuda. IBIS is faster than SLIC with similar boundary adherence. Also, its Cuda version can improve efficiency, reducing computational time.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.5. </span>CFBS</h4>
<div class="ltx_para" id="A1.SS2.SSS5.p1">
<p class="ltx_p" id="A1.SS2.SSS5.p1.1">CFBS <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib132" title="">2020</a>)</cite> aims to overcome the two main limitations of many methods based on k-means: redundancy and the need for post-processing. The proposal performs a coarse-to-fine pixel block optimization using an optimization function similar to SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>. The CFBS updates all pixel blocks in the superpixels’ boundary while the centers are updated dynamically. The number of iterations is defined by the maximum split operations of the initial block pixels. The authors demonstrated the proposal’s ability to increase the performance of k-means-based methods while reducing its running time for superpixel segmentation, along with different applications. However, the CFBS segmentation does not capture finer details in more complex image regions, leading to a worse adherence to the image borders in these regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.6. </span>SCAC</h4>
<div class="ltx_para" id="A1.SS2.SSS6.p1">
<p class="ltx_p" id="A1.SS2.SSS6.p1.1">From an initial grid segmentation, SCAC <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib140" title="">2021b</a>)</cite> performs an accuracy step followed by a compactness step. The former relabels the superpixel boundaries to maximize the adherence to the object contours according to balanced color weighted and spatial distances. Then, the compactness step performs a second relabeling based on color, gradient, and texture filters to detect regions with meaningless content. The gradient, color, and texture filters identify homogeneous, noised, and similar texture pattern regions. SCAC identifies meaningless-content regions, produces more compact superpixels, and prioritizes accuracy on regions with meaningful content. <span class="ltx_text" id="A1.SS2.SSS6.p1.1.1" style="color:#000000;"> The proposal can run in real-time, but its runtime increases with the number of superpixels. Also, SCAC provides limited control over the number of superpixels, producing a number similar to the desired. </span></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.7. </span>LSC-Manhattan</h4>
<div class="ltx_para" id="A1.SS2.SSS7.p1">
<p class="ltx_p" id="A1.SS2.SSS7.p1.1">LSC-Manhattan <cite class="ltx_cite ltx_citemacro_citep">(Qiao and Di, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib92" title="">2022</a>)</cite> improves LSC <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib24" title="">2017a</a>)</cite> performance with a distance measurement based on non-convex image features and Manhattan distance. The proposal classifies the input image according to its texture complexity for subsampling and performs a semantic segmentation using DeepLabV3+ <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib26" title="">2018</a>)</cite> to classify whether a pixel is part of some convex region. The subsampling strategy labels pixels according to texture complexity, applying different subsampling ratios according to the texture complexity level. The LSC-Manhattan produces better segmentation than LSC, with a reduced running time. However, the proposed distance measure is based on a specific dataset, which can lead to generalization issues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.8. </span>FLS</h4>
<div class="ltx_para" id="A1.SS2.SSS8.p1">
<p class="ltx_p" id="A1.SS2.SSS8.p1.1">In <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib89" title="">2022</a>)</cite> the authors proposed a superpixel approach focused on lattice topology consistency. The proposed <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS8.p1.1.1">Fast Lattice Superpixels</span> (FLS) formulates the superpixel generation problem as an energy function optimized through a hill-climbing optimization algorithm constrained to maintain lattice topology. Using a multilevel block strategy similar to SEEDS <cite class="ltx_cite ltx_citemacro_citep">(Van den Bergh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib116" title="">2015</a>)</cite>, FLS adjusts the superpixel affiliation of each block in the superpixel boundary, processing each block at least once per level. In the last level (the pixel level), several iterations of pixel updating are performed to improve boundary delineation. Furthermore, efficiency improves due to the parallelization of non-neighbor blocks. Instead of using hand-crafted features, FLS inputs features from a convolutional network based on SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite> that includes in its loss function the local similarity of pixels with their neighboring pixels. FLS maintains the lattice topology (<span class="ltx_text ltx_font_italic" id="A1.SS2.SSS8.p1.1.2">i.e.</span>., a fixed number of neighbor superpixels), and the local similarity loss function improves boundary delineation. <span class="ltx_text" id="A1.SS2.SSS8.p1.1.3" style="color:#000000;"> However, it has low compactness, and the proposed network, like SSN, is not able to produce the exact number of superpixels.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Dynamic-center-update clustering</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">The dynamic-center-update algorithms perform clustering with a distance function based on the features of the clusters, dynamically updating its centers. Unlike neighborhood-based clustering, this approach does not perform a limited regional search to calculate distances.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1. </span>SNIC</h4>
<div class="ltx_para" id="A1.SS3.SSS1.p1">
<p class="ltx_p" id="A1.SS3.SSS1.p1.1">SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite> intends to overcome SLIC’s limitations <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>. The proposal starts with a sampling grid and dynamically updates the centroids during the clustering process. Furthermore, instead of searching limited to an image patch, SNIC uses a priority queue to group neighboring pixels, similar to path-based approaches, but with a distance function based on the superpixel centroid. Due to its clustering process based on neighboring pixels, SNIC enforces connectivity without requiring post-processing. Furthermore, SNIC requires less memory and is computationally more efficient than SLIC. The authors also proposed an algorithm for polygonal segmentation called SNICPOLY, which starts with superpixels generated with SNIC.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.2. </span>FCSS</h4>
<div class="ltx_para" id="A1.SS3.SSS2.p1">
<p class="ltx_p" id="A1.SS3.SSS2.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib67" title="">2021</a>)</cite> uses an SNIC-based algorithm <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite> with depth information. FCSS controls the clustering process with a priority queue, a distance function, and a color threshold. When the queue is empty, FCSS performs a relocation process to solve the miss segmentation problem caused by the initial seed position. During relocation, FCSS pushes new cluster centers to the queue and updates the color threshold. Finally, the proposal merges unconnected pixels. The FCSS is relatively fast, even with the addition of time complexity due to the seed relocation processing. Also, it achieves a visually balanced segmentation between compactness and boundary adherence. However, the FCSS segmentation does not capture finer details in structure-rich regions, even reducing the compactness factor.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.3. </span>CONIC</h4>
<div class="ltx_para" id="A1.SS3.SSS3.p1">
<p class="ltx_p" id="A1.SS3.SSS3.p1.1">Based on SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite> and SCALP <cite class="ltx_cite ltx_citemacro_citep">(Giraud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib42" title="">2018</a>)</cite>, CONIC <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib44" title="">2021</a>)</cite> incorporates contour prior in a new distance measure, named joint color-spatial-contour measurement, which prevents the boundary pixels from being assigned prematurely. The proposal achieves competitive performance compared to SNIC and SCALP, with moderate compactness and an improved F-measure and boundary precision. CONIC’s superpixels have low sensitivity to the gradient variation in textured regions, leading to less boundary degradation. Compared to SNIC, CONIC avoids redundant feature distance computations and has faster execution. However, the contour prior fails to identify some weak image boundaries.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.4. </span>SCBP</h4>
<div class="ltx_para" id="A1.SS3.SSS4.p1">
<p class="ltx_p" id="A1.SS3.SSS4.p1.1">SCBP <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib145" title="">2021b</a>)</cite> is a two-stage and non-iterative method based on DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib104" title="">2016</a>)</cite>. In the first stage, SCBP clusters the pixels in the conventional image order with an adaptative distance measure, processing each pixel only once and dynamically updating the cluster centers. The adaptative distance measure weights the spatial and color distances, balanced by a boundary probability term computed with the Sobel operator. The second stage merges superpixels based on their combined size according to the expected superpixel size. The proposed method produces compact and regular superpixels in homogenous image regions and superpixels closer to the boundaries in complex regions. Therefore, SCBP has <math alttext="O(n)" class="ltx_Math" display="inline" id="A1.SS3.SSS4.p1.1.m1.1"><semantics id="A1.SS3.SSS4.p1.1.m1.1a"><mrow id="A1.SS3.SSS4.p1.1.m1.1.2" xref="A1.SS3.SSS4.p1.1.m1.1.2.cmml"><mi id="A1.SS3.SSS4.p1.1.m1.1.2.2" xref="A1.SS3.SSS4.p1.1.m1.1.2.2.cmml">O</mi><mo id="A1.SS3.SSS4.p1.1.m1.1.2.1" xref="A1.SS3.SSS4.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="A1.SS3.SSS4.p1.1.m1.1.2.3.2" xref="A1.SS3.SSS4.p1.1.m1.1.2.cmml"><mo id="A1.SS3.SSS4.p1.1.m1.1.2.3.2.1" stretchy="false" xref="A1.SS3.SSS4.p1.1.m1.1.2.cmml">(</mo><mi id="A1.SS3.SSS4.p1.1.m1.1.1" xref="A1.SS3.SSS4.p1.1.m1.1.1.cmml">n</mi><mo id="A1.SS3.SSS4.p1.1.m1.1.2.3.2.2" stretchy="false" xref="A1.SS3.SSS4.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS4.p1.1.m1.1b"><apply id="A1.SS3.SSS4.p1.1.m1.1.2.cmml" xref="A1.SS3.SSS4.p1.1.m1.1.2"><times id="A1.SS3.SSS4.p1.1.m1.1.2.1.cmml" xref="A1.SS3.SSS4.p1.1.m1.1.2.1"></times><ci id="A1.SS3.SSS4.p1.1.m1.1.2.2.cmml" xref="A1.SS3.SSS4.p1.1.m1.1.2.2">𝑂</ci><ci id="A1.SS3.SSS4.p1.1.m1.1.1.cmml" xref="A1.SS3.SSS4.p1.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS4.p1.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS4.p1.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math> time complexity, with a running time close to DBSCAN.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.5. </span>A-DBSCAN</h4>
<div class="ltx_para" id="A1.SS3.SSS5.p1">
<p class="ltx_p" id="A1.SS3.SSS5.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Wang and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib124" title="">2021</a>)</cite> adopts an adaptative threshold and uses a new distance measurement that constrains superpixel shapes based on the linear path from a pixel to a seed. The proposal also uses a local binary pattern (LBP) operator <cite class="ltx_cite ltx_citemacro_citep">(Ke-Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib59" title="">2013</a>)</cite> to compute texture and manage the regularity and boundary adherence tradeoff. After the clustering step, the A-DBSCAN performs a merging stage to produce ﬁnal superpixels with regular size. The proposed method is faster than DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib104" title="">2016</a>)</cite> and produces fewer regular superpixels in textured regions, even with weak edges, achieving a more accurate delineation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.6. </span>F-DBSCAN</h4>
<div class="ltx_para" id="A1.SS3.SSS6.p1">
<p class="ltx_p" id="A1.SS3.SSS6.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Loke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib75" title="">2021</a>)</cite> surpasses many drawbacks of the previous Real-Time DBSCAN (RT-DBSCAN) <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib45" title="">2018</a>)</cite> and parallelization issues. Instead of limiting the search range, the F-DBSCAN defines a limited number of points to assign for each superpixel, which minimizes the overlap and enables parallelization. The performance also maximizes the memory hints with large memory buffers, eliminating fragmentation. After the clustering step, the F-DBSCAN merges small clusters using a watershed transformation <cite class="ltx_cite ltx_citemacro_citep">(Beucher, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib17" title="">1992</a>)</cite>. The proposal’s segmentation presents similar qualitative results to RT-DBSCAN with much faster computation. The processing time for F-DBSCAN drops as the degree of parallelism increases without increasing leakage. However, F-DBSCAN presents a poor performance in images with blue-white boundaries and low contrast due to the CIELAB colorspace used. Also, F-DBSCAN presents much slower results in GPU due to its regional parallelization instead of parallelizing a whole image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.7. </span>DRW</h4>
<div class="ltx_para" id="A1.SS3.SSS7.p1">
<p class="ltx_p" id="A1.SS3.SSS7.p1.1">The DRW <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib58" title="">2020</a>)</cite> model uses dynamic nodes, which reduces the redundant calculation by limiting the walking range. The proposed algorithm performs a new seed initialization strategy that creates a seed set with regular distribution in both 2D and 3D. It also can combine boundary prior information, such as gradient information or boundary probability. <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib82" title="">2004</a>)</cite>. DRW computes superpixels in linear time and allows control of the distribution of superpixels in complex and homogenous image regions. The proposed segmentation method has competitive performance and it is faster than existing RW models. However, DRW segmentation does not produce compact superpixels.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4. </span>Path-based clustering</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">Path-based approaches generate superpixels by creating paths in the image graph based on some criteria. Usually, their clustering criteria are a path-based function to optimize during clustering. The ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite> is an example of a path-based method that calculates a forest of optimal paths based on a path cost function.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.1. </span>ERGC</h4>
<div class="ltx_para" id="A1.SS4.SSS1.p1">
<p class="ltx_p" id="A1.SS4.SSS1.p1.1">First, the proposed ERGC <cite class="ltx_cite ltx_citemacro_citep">(Buyssens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib21" title="">2014</a>)</cite> simplifies Computed Tomography (CT) images by computing superpixels based on the Eikonal algorithm. The superpixels start from seeds sampled in a regular grid and evolve according to the Fast Marching algorithm <cite class="ltx_cite ltx_citemacro_citep">(Sethian, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib102" title="">1999</a>)</cite>. ERGC creates homogeneous superpixels with a spatial constraint to enforce compactness. <span class="ltx_text" id="A1.SS4.SSS1.p1.1.1" style="color:#000000;"> The proposal allows control over the number of superpixels and compactness and is extensible to supervoxels.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.2. </span>ISF</h4>
<div class="ltx_para" id="A1.SS4.SSS2.p1">
<p class="ltx_p" id="A1.SS4.SSS2.p1.1">Based on IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite>, the ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite> framework combines a seed sampling strategy, a connectivity function, an adjacency relation, and a seed recomputation procedure. The proposal’s algorithm starts with (i) a seed sampling, followed by (ii) a spanning forest computed by the IFT algorithm, and (iii) a seed recomputation procedure. The ISF refines the segmentation by iteratively executing steps (ii) and (iii). The computational complexity of the ISF framework using a binary heap is linearithmic, independent of the number of superpixels. Also, the<span class="ltx_text ltx_font_italic" id="A1.SS4.SSS2.p1.1.1">Differential Image Foresting Transform</span> (DIFT) <cite class="ltx_cite ltx_citemacro_citep">(Falcão and Bergo, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib34" title="">2004</a>; Condori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib28" title="">2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib27" title="">2020</a>)</cite> can reduce the computational cost to compute the IFTs, although its effectiveness depends on the cost function used. In <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>, the authors combine different components to present five ISF-based methods. They also demonstrated that ISF produces effective and efficient methods independent of the dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.3. </span>RSS</h4>
<div class="ltx_para" id="A1.SS4.SSS3.p1">
<p class="ltx_p" id="A1.SS4.SSS3.p1.1">The RSS <cite class="ltx_cite ltx_citemacro_citep">(Chai, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib22" title="">2020</a>)</cite> follows the IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite> algorithm and can form a forest with optimal costs. To measure color similarity and spatial closeness, the authors proposed two path-based cost functions, which are more robust than the geodesic distance. Inspired by counting sort and bucket sort, the RSS computes optimal forest with buckets of queues and groups of seeds in an IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite>-based algorithm. Due to the sorting strategy, the proposal has <math alttext="O(n)" class="ltx_Math" display="inline" id="A1.SS4.SSS3.p1.1.m1.1"><semantics id="A1.SS4.SSS3.p1.1.m1.1a"><mrow id="A1.SS4.SSS3.p1.1.m1.1.2" xref="A1.SS4.SSS3.p1.1.m1.1.2.cmml"><mi id="A1.SS4.SSS3.p1.1.m1.1.2.2" xref="A1.SS4.SSS3.p1.1.m1.1.2.2.cmml">O</mi><mo id="A1.SS4.SSS3.p1.1.m1.1.2.1" xref="A1.SS4.SSS3.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="A1.SS4.SSS3.p1.1.m1.1.2.3.2" xref="A1.SS4.SSS3.p1.1.m1.1.2.cmml"><mo id="A1.SS4.SSS3.p1.1.m1.1.2.3.2.1" stretchy="false" xref="A1.SS4.SSS3.p1.1.m1.1.2.cmml">(</mo><mi id="A1.SS4.SSS3.p1.1.m1.1.1" xref="A1.SS4.SSS3.p1.1.m1.1.1.cmml">n</mi><mo id="A1.SS4.SSS3.p1.1.m1.1.2.3.2.2" stretchy="false" xref="A1.SS4.SSS3.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.SSS3.p1.1.m1.1b"><apply id="A1.SS4.SSS3.p1.1.m1.1.2.cmml" xref="A1.SS4.SSS3.p1.1.m1.1.2"><times id="A1.SS4.SSS3.p1.1.m1.1.2.1.cmml" xref="A1.SS4.SSS3.p1.1.m1.1.2.1"></times><ci id="A1.SS4.SSS3.p1.1.m1.1.2.2.cmml" xref="A1.SS4.SSS3.p1.1.m1.1.2.2">𝑂</ci><ci id="A1.SS4.SSS3.p1.1.m1.1.1.cmml" xref="A1.SS4.SSS3.p1.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.SSS3.p1.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.SSS3.p1.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math> complexity. The proposal is fast and has competitive performance. The main strengths of RSS are the low computational complexity, great boundary adherence with stable performance, and adjustable compactness. However, besides the proposal extends to supervoxel segmentation, it performs poorly compared with the evaluated methods. Also, due to the initial seed sampling in a regular grid <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, RSS generates more superpixels in homogenous regions, which leads to a degrading in boundary adherence in complex regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.4. </span>DISF</h4>
<div class="ltx_para" id="A1.SS4.SSS4.p1">
<p class="ltx_p" id="A1.SS4.SSS4.p1.1">Based on ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite>, DISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>)</cite> is a three-step superpixel framework that improves its delineation even for fewer superpixels. The proposal initializes with a grid oversampling <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>. Then, iteratively compute a forest rooted at the seeds with an IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite> execution followed by a seed set reduction by choosing the most relevant seeds. It repeats IFT computation and seed set reduction until having the desired number of superpixels. DISF has an optimal delineation, especially for a few numbers of superpixels. Therefore, the proposal’s segmentation is able to correctly select relevant seeds, reducing its boundary adherence degradation when decreasing the number of final superpixels. Despite its iterative process increasing the running time, DISF performs a reduced and limited number of iterations. However, the proposal does not produce compact superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.5. </span>ODISF</h4>
<div class="ltx_para" id="A1.SS4.SSS5.p1">
<p class="ltx_p" id="A1.SS4.SSS5.p1.1">Motivated by OISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib13" title="">2018</a>)</cite> performance, ODISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>)</cite> extends DISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib15" title="">2020</a>)</cite> for an object-based proposal to improve the superpixel performance using object saliency maps created using a U2-net <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib93" title="">2020</a>)</cite>. The proposal performs the same three-step pipeline in DISF. First, the ODISF performs a seed oversampling. Then, it iteratively computes a spanning forest rooted at the seed set with an IFT <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite> execution followed by an object-based seed removal. In the remotion step, the algorithm maintains seeds closer to the object saliency boundaries or with higher saliency. The proposed method demonstrates a generalization ability by performing an effective superpixel segmentation in datasets with different object properties. Also, it demonstrates robustness to saliency map errors in comparison with OISF. Despite the ODISF delineation step being saliency-independent, its object-based removal strategy can circumvent the saliency errors. On the other hand, the ODISF does not allow controlling the number of iterations. Also, despite its computational complexity, it has a high running time.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.6. </span>SICLE</h4>
<div class="ltx_para" id="A1.SS4.SSS6.p1">
<p class="ltx_p" id="A1.SS4.SSS6.p1.1">SICLE <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib14" title="">2022b</a>)</cite> generalizes ODISF <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib16" title="">2021</a>)</cite> to control the number of iterations and to improve efficiency and delineation for poorly estimated saliency maps. SICLE starts with (i) seed oversampling and iteratively generates superpixels by (ii) computing the minimum forest rooted at the seed set <cite class="ltx_cite ltx_citemacro_citep">(Falcão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib35" title="">2004</a>)</cite>, followed by (iii) removal of the less relevant seeds. Similar to ODISF, SICLE incorporates saliency information during the seed removal step, but it is robust to incorrect saliency estimations. However, SICLE’s seed removal strategy allows controlling the number of iterations and avoids unnecessary iterations, improving efficiency. Since SICLE uses object information only on the removal step, its delineation is robust to saliency errors. However, SICLE cannot improve delineation performance for more accurate saliency estimators. The authors overcome this drawback in <cite class="ltx_cite ltx_citemacro_citep">(Belém et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib12" title="">2022a</a>)</cite> by encompassing a path cost function and a seed removal strategy to control the impact of object saliency information using a binary parameter. The proposal maintains its robustness for low-quality estimators and exploits the accurate information of high-quality estimators, improving performance with only two iterations. Despite the robustness and efficiency of SICLE, errors in the saliency map can still affect its results.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5. </span>Hierarchical clustering</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1">Hierarchical segmentation methods are generally not mentioned in the literature as superpixel methods. However, they fit most definitions for superpixels. Although hierarchical methods do not obtain a compact or regular segmentation, the regions produced are generally homogeneous. Furthermore, the hierarchy enables control of the desired number of regions without increasing the execution time.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.1. </span>SH</h4>
<div class="ltx_para" id="A1.SS5.SSS1.p1">
<p class="ltx_p" id="A1.SS5.SSS1.p1.1">SH <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib128" title="">2018</a>)</cite> uses the Borůvka algorithm to efficiently compute a <span class="ltx_text ltx_font_italic" id="A1.SS5.SSS1.p1.1.1">Minimum Spanning Tree</span> in a bottom-up manner representing a hierarchy. It improves efficiency with edge contraction, contracting each tree to a vertex and recording the edge selection order. Also, to improve accuracy with local searching, SH incorporates edge information from an edge detector and combines it with color information. In experiments, SH achieved high accuracy and low computational time. The authors also demonstrate the SH’s effectiveness in saliency detection, semantic segmentation, and stereo-matching. However, SH does not produce regular superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.2. </span>HMLI-SLIC</h4>
<div class="ltx_para" id="A1.SS5.SSS2.p1">
<p class="ltx_p" id="A1.SS5.SSS2.p1.1">HMLI-SLIC <cite class="ltx_cite ltx_citemacro_citep">(Di et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib31" title="">2021</a>)</cite> consists of an (i) initial segmentation, a (ii) hierarchical multi-level segmentation, and a (iii) superpixel merging. In (i), HMLI-SLIC produces a controlled number of superpixels with SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> segmentation. Then, it performs coarse to fine segmentation to ensure that each superpixel does not contain multiple object regions, producing a hierarchical segmentation. Finally, HMLI-SLIC performs a merging step with the most similar superpixels. The proposal is robust to noise and can fit image boundaries since it produces more superpixels in heterogeneous regions and less in homogenous ones. Also, HMLI-SLIC does not perform under- or over-segmentation, automatically setting the number of seeds and superpixels. Therefore, it does not allow controlling the number of superpixels. However, the proposal is time-consuming and does not produce regular or compact superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.3. </span>RISF</h4>
<div class="ltx_para" id="A1.SS5.SSS3.p1">
<p class="ltx_p" id="A1.SS5.SSS3.p1.1">RISF <cite class="ltx_cite ltx_citemacro_citep">(Galvão et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib38" title="">2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib39" title="">2020</a>)</cite> produces a sparse hierarchy by computing a multi-scale superpixel segmentation using ISF <cite class="ltx_cite ltx_citemacro_citep">(Vargas-Muñoz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib117" title="">2019</a>)</cite> over the Region Adjacency Graph (RAG) resulting from the previous scale. The region merging algorithm produces a dense hierarchy from a mid-level superpixel segmentation for more accurate segmentation in coarser scales.
<span class="ltx_text" id="A1.SS5.SSS3.p1.1.1" style="color:#000000;"> RISF produces more irregular superpixels than ISF, although it can produce a hierarchy from any superpixel segmentation method. It is also efficient, with a low complexity of <math alttext="O(n\log n)" class="ltx_Math" display="inline" id="A1.SS5.SSS3.p1.1.1.m1.1"><semantics id="A1.SS5.SSS3.p1.1.1.m1.1a"><mrow id="A1.SS5.SSS3.p1.1.1.m1.1.1" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.cmml"><mi id="A1.SS5.SSS3.p1.1.1.m1.1.1.3" mathcolor="#000000" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.3.cmml">O</mi><mo id="A1.SS5.SSS3.p1.1.1.m1.1.1.2" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.cmml"><mo id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.2" mathcolor="#000000" stretchy="false" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.cmml"><mi id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.2" mathcolor="#000000" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.1" lspace="0.167em" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.cmml"><mi id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.1" mathcolor="#000000" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.2" mathcolor="#000000" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.3" mathcolor="#000000" stretchy="false" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS5.SSS3.p1.1.1.m1.1b"><apply id="A1.SS5.SSS3.p1.1.1.m1.1.1.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1"><times id="A1.SS5.SSS3.p1.1.1.m1.1.1.2.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.2"></times><ci id="A1.SS5.SSS3.p1.1.1.m1.1.1.3.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.3">𝑂</ci><apply id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1"><times id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.1"></times><ci id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.2.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3"><log id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.1.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.1"></log><ci id="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.2.cmml" xref="A1.SS5.SSS3.p1.1.1.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.SSS3.p1.1.1.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.SSS3.p1.1.1.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math> and its computation over RAGs. However, due to the hierarchy construction, errors in coarser scales are propagated to the finer ones.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.4. </span>UOIFT</h4>
<div class="ltx_para" id="A1.SS5.SSS4.p1">
<p class="ltx_p" id="A1.SS5.SSS4.p1.1">UOIFT <cite class="ltx_cite ltx_citemacro_citep">(Bejar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib10" title="">2020</a>)</cite> extends <cite class="ltx_cite ltx_citemacro_citep">(Bejar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib11" title="">2018</a>)</cite> to propose a hierarchical and unsupervised image segmentation method that exploits non-monotonic-incremental cost functions in directed graphs to incorporate high-level priors of the objects as boundary polarity. UOIFT computes an initial forest over the image pixels and partitions the graph with multiple executions of the OIFT <cite class="ltx_cite ltx_citemacro_citep">(Mansilla and Miranda, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib80" title="">2013</a>; Miranda and Mansilla, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib86" title="">2013</a>)</cite> computed over the Region Adjacency Graph of the previous forest.
UOIFT is fast and demonstrates its ability to accurately segment medical images and colored images with different lighting conditions. Although its boundary polarity allows for improving the segmentation for a specific color (or texture or local contrast) transition, setting this parameter can be challenging for more generic applications.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS5.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.5. </span>DAL-HERS</h4>
<div class="ltx_para" id="A1.SS5.SSS5.p1">
<p class="ltx_p" id="A1.SS5.SSS5.p1.1">DAL-HERS <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib90" title="">2022</a>)</cite> is a two-stage superpixel framework that consists of a <span class="ltx_text ltx_font_italic" id="A1.SS5.SSS5.p1.1.1">Deep Affinity Learning</span> (DAL) neural network architecture and a <span class="ltx_text ltx_font_italic" id="A1.SS5.SSS5.p1.1.2">Hierarchical Entropy Rate Segmentation</span> (HERS) method. The DAL network aggregates multi-scale information to learn pairwise pixel affinities, and the HERS method builds a hierarchical tree structure by maximizing the graph’s entropy rate. Using the DAL’s affinity map, the proposed HERS algorithm constructs a hierarchy with Borůvka’s algorithm <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib128" title="">2018</a>)</cite>.
The proposal preserves fine details on the objects by focusing on rich-structure parts rather than uniform regions, producing large superpixels in color-homogeneous regions and an over-segmentation in texture-rich regions. Also, compared with deep-based learning methods, DAL-HERS has a competitive running time is competitive and requires the same <math alttext="O(n)" class="ltx_Math" display="inline" id="A1.SS5.SSS5.p1.1.m1.1"><semantics id="A1.SS5.SSS5.p1.1.m1.1a"><mrow id="A1.SS5.SSS5.p1.1.m1.1.2" xref="A1.SS5.SSS5.p1.1.m1.1.2.cmml"><mi id="A1.SS5.SSS5.p1.1.m1.1.2.2" xref="A1.SS5.SSS5.p1.1.m1.1.2.2.cmml">O</mi><mo id="A1.SS5.SSS5.p1.1.m1.1.2.1" xref="A1.SS5.SSS5.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="A1.SS5.SSS5.p1.1.m1.1.2.3.2" xref="A1.SS5.SSS5.p1.1.m1.1.2.cmml"><mo id="A1.SS5.SSS5.p1.1.m1.1.2.3.2.1" stretchy="false" xref="A1.SS5.SSS5.p1.1.m1.1.2.cmml">(</mo><mi id="A1.SS5.SSS5.p1.1.m1.1.1" xref="A1.SS5.SSS5.p1.1.m1.1.1.cmml">n</mi><mo id="A1.SS5.SSS5.p1.1.m1.1.2.3.2.2" stretchy="false" xref="A1.SS5.SSS5.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS5.SSS5.p1.1.m1.1b"><apply id="A1.SS5.SSS5.p1.1.m1.1.2.cmml" xref="A1.SS5.SSS5.p1.1.m1.1.2"><times id="A1.SS5.SSS5.p1.1.m1.1.2.1.cmml" xref="A1.SS5.SSS5.p1.1.m1.1.2.1"></times><ci id="A1.SS5.SSS5.p1.1.m1.1.2.2.cmml" xref="A1.SS5.SSS5.p1.1.m1.1.2.2">𝑂</ci><ci id="A1.SS5.SSS5.p1.1.m1.1.1.cmml" xref="A1.SS5.SSS5.p1.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.SSS5.p1.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.SSS5.p1.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math> time complexity to produce any number of superpixels. Due to the highly adaptive nature of the produced superpixels, delineating finer details, their superpixels have no compactness.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6. </span>Density-based clustering</h3>
<div class="ltx_para" id="A1.SS6.p1">
<p class="ltx_p" id="A1.SS6.p1.1">In the density-based clustering approach, the superpixel methods rely on an optimization function to find the cluster centers, calling them density peaks. The clustering of the non-peak pixels is performed according to the centers, generally assigning a pixel to the superpixels with the spatially closest density peak. Therefore, such methods model the problem of finding superpixels into one of finding density peaks.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.1. </span>PGDPC</h4>
<div class="ltx_para" id="A1.SS6.SSS1.p1">
<p class="ltx_p" id="A1.SS6.SSS1.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib48" title="">2021</a>)</cite> performs a two-step strategy, firstly dividing data points into peaks and non-peaks and computing a graph using DPC-based <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib118" title="">2018</a>)</cite> allocation. Then, it classifies the peak candidates and non-peaks by computing the KNN density <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib133" title="">2016</a>)</cite> for each pixel. After, PGDPC computes a graph based on a DPC allocation and initializes a peak graph with peak candidates as roots. The non-peak nodes are assigned to the closest root cluster, forming trees. Finally, PGDPC selects the cluster centers as candidate peaks with higher density and geodesic distances.
<span class="ltx_text" id="A1.SS6.SSS1.p1.1.1" style="color:#000000;"> The proposal is computationally efficient, having an <math alttext="O(n\log n)" class="ltx_Math" display="inline" id="A1.SS6.SSS1.p1.1.1.m1.1"><semantics id="A1.SS6.SSS1.p1.1.1.m1.1a"><mrow id="A1.SS6.SSS1.p1.1.1.m1.1.1" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.cmml"><mi id="A1.SS6.SSS1.p1.1.1.m1.1.1.3" mathcolor="#000000" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.3.cmml">O</mi><mo id="A1.SS6.SSS1.p1.1.1.m1.1.1.2" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.cmml"><mo id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.2" mathcolor="#000000" stretchy="false" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.cmml"><mi id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.2" mathcolor="#000000" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.1" lspace="0.167em" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.cmml"><mi id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.1" mathcolor="#000000" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.2" mathcolor="#000000" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.2.cmml">n</mi></mrow></mrow><mo id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.3" mathcolor="#000000" stretchy="false" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.SSS1.p1.1.1.m1.1b"><apply id="A1.SS6.SSS1.p1.1.1.m1.1.1.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1"><times id="A1.SS6.SSS1.p1.1.1.m1.1.1.2.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.2"></times><ci id="A1.SS6.SSS1.p1.1.1.m1.1.1.3.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.3">𝑂</ci><apply id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1"><times id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.1"></times><ci id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.2.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3"><log id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.1.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.1"></log><ci id="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.2.cmml" xref="A1.SS6.SSS1.p1.1.1.m1.1.1.1.1.1.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.SSS1.p1.1.1.m1.1c">O(n\log n)</annotation><annotation encoding="application/x-llamapun" id="A1.SS6.SSS1.p1.1.1.m1.1d">italic_O ( italic_n roman_log italic_n )</annotation></semantics></math> time complexity. In synthetic datasets, PGDPC demonstrates its ability to cluster complex structures, achieving an improved performance compared with DPC. To evaluate the proposal, the authors combined PGDPC and SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> to reduce the computational overhead. PGDPC achieves great performance in natural and medical datasets. However, using SLIC as pre-processing, the SLIC errors can be propagated to PGDPC, reducing its performance. </span></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.2. </span>DPS</h4>
<div class="ltx_para" id="A1.SS6.SSS2.p1">
<p class="ltx_p" id="A1.SS6.SSS2.p1.2">DPS <cite class="ltx_cite ltx_citemacro_citep">(Shah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib103" title="">2021</a>)</cite> aims to perform an efficient non-iterative density peak segmentation in a limited search region. The DPS initializes computing the pixels’ density and finds the density of peaks, searching in a limited region. Then, it found superpixel centers based on the pixel density and the peak density thresholds. Finally, it assigns the remaining pixels to their nearest superpixel with a higher density.
Due to the regional search, its time complexity is <math alttext="O(m^{2})" class="ltx_Math" display="inline" id="A1.SS6.SSS2.p1.1.m1.1"><semantics id="A1.SS6.SSS2.p1.1.m1.1a"><mrow id="A1.SS6.SSS2.p1.1.m1.1.1" xref="A1.SS6.SSS2.p1.1.m1.1.1.cmml"><mi id="A1.SS6.SSS2.p1.1.m1.1.1.3" xref="A1.SS6.SSS2.p1.1.m1.1.1.3.cmml">O</mi><mo id="A1.SS6.SSS2.p1.1.m1.1.1.2" xref="A1.SS6.SSS2.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="A1.SS6.SSS2.p1.1.m1.1.1.1.1" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.cmml"><mo id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.2" stretchy="false" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.cmml"><mi id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.2" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.2.cmml">m</mi><mn id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.3" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.3" stretchy="false" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.SSS2.p1.1.m1.1b"><apply id="A1.SS6.SSS2.p1.1.m1.1.1.cmml" xref="A1.SS6.SSS2.p1.1.m1.1.1"><times id="A1.SS6.SSS2.p1.1.m1.1.1.2.cmml" xref="A1.SS6.SSS2.p1.1.m1.1.1.2"></times><ci id="A1.SS6.SSS2.p1.1.m1.1.1.3.cmml" xref="A1.SS6.SSS2.p1.1.m1.1.1.3">𝑂</ci><apply id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.cmml" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.2">𝑚</ci><cn id="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="A1.SS6.SSS2.p1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.SSS2.p1.1.m1.1c">O(m^{2})</annotation><annotation encoding="application/x-llamapun" id="A1.SS6.SSS2.p1.1.m1.1d">italic_O ( italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math alttext="m\times m" class="ltx_Math" display="inline" id="A1.SS6.SSS2.p1.2.m2.1"><semantics id="A1.SS6.SSS2.p1.2.m2.1a"><mrow id="A1.SS6.SSS2.p1.2.m2.1.1" xref="A1.SS6.SSS2.p1.2.m2.1.1.cmml"><mi id="A1.SS6.SSS2.p1.2.m2.1.1.2" xref="A1.SS6.SSS2.p1.2.m2.1.1.2.cmml">m</mi><mo id="A1.SS6.SSS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS6.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mi id="A1.SS6.SSS2.p1.2.m2.1.1.3" xref="A1.SS6.SSS2.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.SSS2.p1.2.m2.1b"><apply id="A1.SS6.SSS2.p1.2.m2.1.1.cmml" xref="A1.SS6.SSS2.p1.2.m2.1.1"><times id="A1.SS6.SSS2.p1.2.m2.1.1.1.cmml" xref="A1.SS6.SSS2.p1.2.m2.1.1.1"></times><ci id="A1.SS6.SSS2.p1.2.m2.1.1.2.cmml" xref="A1.SS6.SSS2.p1.2.m2.1.1.2">𝑚</ci><ci id="A1.SS6.SSS2.p1.2.m2.1.1.3.cmml" xref="A1.SS6.SSS2.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.SSS2.p1.2.m2.1c">m\times m</annotation><annotation encoding="application/x-llamapun" id="A1.SS6.SSS2.p1.2.m2.1d">italic_m × italic_m</annotation></semantics></math> is the region size. The proposed DPS is faster than Density Peak <cite class="ltx_cite ltx_citemacro_citep">(Rodriguez and Laio, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib96" title="">2014</a>)</cite> and has a competitive segmentation, even using only color and spatial distances in a single iteration. However, DPS does not produce regular and compact superpixels. Also, its control over the number of superpixels is indirect and based on two parameters.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7. </span>Sparse linear system clustering</h3>
<div class="ltx_para" id="A1.SS7.p1">
<p class="ltx_p" id="A1.SS7.p1.1">Sparse linear system clustering methods model the segmentation problem with a sparse matrix and extract features from linear relationships in the matrix.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.1. </span>ANRW</h4>
<div class="ltx_para" id="A1.SS7.SSS1.p1">
<p class="ltx_p" id="A1.SS7.SSS1.p1.1">Based on the Non-local random walk (NRW) <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib138" title="">2015</a>)</cite>, the ANRW <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib119" title="">2020b</a>)</cite> performs an initial seed sampling based on the regional minima with a trade-off between local contrast and spatial distance. ANRW computes the weight matrix from the seed set according to an adaptative Gaussian function and the KNN features. It computes a Laplacian matrix and solves the Dirichlet problem, assigning labels according to it. Finally, the proposal performs a coarse-to-fine merging strategy. ANRW can deal with textured images, outperforming the compared methods in boundary recall, under-segmentation error, and accuracy, but it has high computational complexity. Although the ANRW doesn’t produce compact superpixels in complex regions, it does in homogeneous ones.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.2. </span>GL<math alttext="l_{1/2}" class="ltx_Math" display="inline" id="A1.SS7.SSS2.1.m1.1"><semantics id="A1.SS7.SSS2.1.m1.1b"><msub id="A1.SS7.SSS2.1.m1.1.1" xref="A1.SS7.SSS2.1.m1.1.1.cmml"><mi id="A1.SS7.SSS2.1.m1.1.1.2" xref="A1.SS7.SSS2.1.m1.1.1.2.cmml">l</mi><mrow id="A1.SS7.SSS2.1.m1.1.1.3" xref="A1.SS7.SSS2.1.m1.1.1.3.cmml"><mn id="A1.SS7.SSS2.1.m1.1.1.3.2" xref="A1.SS7.SSS2.1.m1.1.1.3.2.cmml">1</mn><mo id="A1.SS7.SSS2.1.m1.1.1.3.1" xref="A1.SS7.SSS2.1.m1.1.1.3.1.cmml">/</mo><mn id="A1.SS7.SSS2.1.m1.1.1.3.3" xref="A1.SS7.SSS2.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS7.SSS2.1.m1.1c"><apply id="A1.SS7.SSS2.1.m1.1.1.cmml" xref="A1.SS7.SSS2.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS7.SSS2.1.m1.1.1.1.cmml" xref="A1.SS7.SSS2.1.m1.1.1">subscript</csymbol><ci id="A1.SS7.SSS2.1.m1.1.1.2.cmml" xref="A1.SS7.SSS2.1.m1.1.1.2">𝑙</ci><apply id="A1.SS7.SSS2.1.m1.1.1.3.cmml" xref="A1.SS7.SSS2.1.m1.1.1.3"><divide id="A1.SS7.SSS2.1.m1.1.1.3.1.cmml" xref="A1.SS7.SSS2.1.m1.1.1.3.1"></divide><cn id="A1.SS7.SSS2.1.m1.1.1.3.2.cmml" type="integer" xref="A1.SS7.SSS2.1.m1.1.1.3.2">1</cn><cn id="A1.SS7.SSS2.1.m1.1.1.3.3.cmml" type="integer" xref="A1.SS7.SSS2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS7.SSS2.1.m1.1d">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="A1.SS7.SSS2.1.m1.1e">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math>RSC</h4>
<div class="ltx_para" id="A1.SS7.SSS2.p1">
<p class="ltx_p" id="A1.SS7.SSS2.p1.3">The authors <cite class="ltx_cite ltx_citemacro_citep">(Francis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib37" title="">2022</a>)</cite> propose an algorithm based on subspace clustering with enhanced segmentation capability using Laplacian and <math alttext="l_{1/2}" class="ltx_Math" display="inline" id="A1.SS7.SSS2.p1.1.m1.1"><semantics id="A1.SS7.SSS2.p1.1.m1.1a"><msub id="A1.SS7.SSS2.p1.1.m1.1.1" xref="A1.SS7.SSS2.p1.1.m1.1.1.cmml"><mi id="A1.SS7.SSS2.p1.1.m1.1.1.2" xref="A1.SS7.SSS2.p1.1.m1.1.1.2.cmml">l</mi><mrow id="A1.SS7.SSS2.p1.1.m1.1.1.3" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.cmml"><mn id="A1.SS7.SSS2.p1.1.m1.1.1.3.2" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.2.cmml">1</mn><mo id="A1.SS7.SSS2.p1.1.m1.1.1.3.1" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.1.cmml">/</mo><mn id="A1.SS7.SSS2.p1.1.m1.1.1.3.3" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS7.SSS2.p1.1.m1.1b"><apply id="A1.SS7.SSS2.p1.1.m1.1.1.cmml" xref="A1.SS7.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS7.SSS2.p1.1.m1.1.1.1.cmml" xref="A1.SS7.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="A1.SS7.SSS2.p1.1.m1.1.1.2.cmml" xref="A1.SS7.SSS2.p1.1.m1.1.1.2">𝑙</ci><apply id="A1.SS7.SSS2.p1.1.m1.1.1.3.cmml" xref="A1.SS7.SSS2.p1.1.m1.1.1.3"><divide id="A1.SS7.SSS2.p1.1.m1.1.1.3.1.cmml" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.1"></divide><cn id="A1.SS7.SSS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.2">1</cn><cn id="A1.SS7.SSS2.p1.1.m1.1.1.3.3.cmml" type="integer" xref="A1.SS7.SSS2.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS7.SSS2.p1.1.m1.1c">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="A1.SS7.SSS2.p1.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math> regularization techniques. The GLl<sub class="ltx_sub" id="A1.SS7.SSS2.p1.3.1"><span class="ltx_text ltx_font_italic" id="A1.SS7.SSS2.p1.3.1.1">1/2</span></sub>RSC starts with an initial superpixel segmentation <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib76" title="">2012</a>)</cite> and computes its Local Spectral Histogram (LSH) features to obtain a feature data matrix. Then, perform a spectral clustering on the matrix to obtain clustered data points and execute an encoding procedure to map superpixels into optimal regions <cite class="ltx_cite ltx_citemacro_citep">(Wang and Wu, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib125" title="">2017</a>; Zohrizadeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib149" title="">2018</a>)</cite>.
The proposal addresses the challenge of obtaining an improved sparse solution or a sparse representation matrix under the circumstances of noise-corrupted feature data vectors. GLl<sub class="ltx_sub" id="A1.SS7.SSS2.p1.3.2"><span class="ltx_text ltx_font_italic" id="A1.SS7.SSS2.p1.3.2.1">1/2</span></sub>RSC preserves the image structures, producing better results for images with a large number of small dominant regions. However, similar to other sparse linear system clustering methods, the proposal has a high running time due to the LSH feature vector generation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS7.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.3. </span>SCSC</h4>
<div class="ltx_para" id="A1.SS7.SSS3.p1">
<p class="ltx_p" id="A1.SS7.SSS3.p1.1">SCSC <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib66" title="">2020</a>)</cite> formulates the superpixels problem as a subspace clustering problem. The proposed method first performs a K-means clustering. Then, it constructs a coding matrix using the superpixel-based feature vectors and solves the matrix with an algorithm based on the alternating direction method of multipliers (ADMM) <cite class="ltx_cite ltx_citemacro_citep">(Boyd et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib20" title="">2011</a>)</cite>. Finally, SCSC computes the affinity graph and performs an NCut segmentation <cite class="ltx_cite ltx_citemacro_citep">(Shi and Malik, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib106" title="">2000</a>)</cite> with a further merging step to guarantee connectivity.
SCSC is able to capture finer boundary details but with poor regularity and compactness. Also, it may require many seconds to generate hundreds of superpixels.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.8. </span>Regional feature extraction, Polygonal decomposition, and Graph-based clustering</h3>
<div class="ltx_para" id="A1.SS8.p1">
<p class="ltx_p" id="A1.SS8.p1.1">Regional feature extraction clustering methods iteratively extract features from image regions and use these features to perform clustering. In contrast, methods that perform Polygonal decomposition clustering decompose the image into non-overlapping polygons as superpixels. Finally, graph-based clustering methods perform superpixel segmentation based on graph topology.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS8.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.8.1. </span>EAM</h4>
<div class="ltx_para" id="A1.SS8.SSS1.p1">
<p class="ltx_p" id="A1.SS8.SSS1.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib5" title="">2020</a>)</cite> first removes noise with a bilateral ﬁltering <cite class="ltx_cite ltx_citemacro_citep">(Tomasi and Manduchi, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib112" title="">1998</a>)</cite>. Then, it extracts regional attributes using power-windows to determine whether it contain a single object. The power-windows with more than one object are iteratively split into four until achieving a minimum size. Next, EAM computes a Dijkstra <cite class="ltx_cite ltx_citemacro_citep">(Dijkstra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib32" title="">1959</a>)</cite> algorithm to merge similar power-windows, followed by a binary search to merge them with unreached windows. Finally, the proposal uses the cluster diameter threshold to control the degree of detail of segmentation.
<span class="ltx_text" id="A1.SS8.SSS1.p1.1.1" style="color:#000000;"> EAM is relatively fast, generates larger and fewer superpixels in homogenous regions, and captures more details in complex ones. However, the EAM’s superpixels were neither compact nor regular.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS8.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.8.2. </span>ECCPD</h4>
<div class="ltx_para" id="A1.SS8.SSS2.p1">
<p class="ltx_p" id="A1.SS8.SSS2.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib77" title="">2020</a>)</cite> formulates the superpixel problem into a Centroidal Power Diagram (CPD) <cite class="ltx_cite ltx_citemacro_citep">(Aurenhammer, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib6" title="">1987</a>)</cite> problem. ECCPD starts creating fixed cluster centers for CPD using a boundary probability map from Richer Convolutional Features <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib73" title="">2017</a>)</cite> and random centers equally spaced. Then, iteratively adapt the power cell sizes and update the power cell centers according to their centroid. After performing the maximum number of iterations or achieving the threshold, ECCPD performs post-processing to align some boundaries.
Compared with other polygonal superpixel methods, the ECCPD can capture better boundaries in more complex regions. Also, the proposal is faster than other strategies to compute the CPD with capacity constraints in geometry but is highly time-consuming.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS8.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.8.3. </span>ERS</h4>
<div class="ltx_para" id="A1.SS8.SSS3.p1">
<p class="ltx_p" id="A1.SS8.SSS3.p1.1">ERS <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib5" title="">2020</a>)</cite> is a greedy algorithm that efficiently computes the entropy rate of a random walk on the image graph. The ERS’s objective function is composed of an entropy rate term and a balancing term of the cluster distribution. While the entropy rate favors compact and homogeneous clusters, the balancing term encourages clusters with similar sizes.
The authors demonstrated that the balancing term in ERS produces superpixels with similar sizes and enforces control over the number of superpixels. However, they are irregular in shape.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.9. </span>Data distribution-based clustering</h3>
<div class="ltx_para" id="A1.SS9.p1">
<p class="ltx_p" id="A1.SS9.p1.1">In superpixel segmentation, we name data distribution-based methods the approaches that assume that the image pixels follow a specific distribution. From this initial conjecture, the clustering step is performed. As far as we know, the distribution-based methods that perform superpixel segmentation are based on the Gaussian mixture model and assume that the image pixels follow a Gaussian distribution.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS9.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.9.1. </span>GMMSP</h4>
<div class="ltx_para" id="A1.SS9.SSS1.p1">
<p class="ltx_p" id="A1.SS9.SSS1.p1.1">GMMSP <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite> models superpixel segmentation as a weighted sum of Gaussian functions, each one corresponding to a superpixel. The proposal produces superpixels of similar size by using a constant weight for the weighted sum of Gaussians. It also imposes two parameters during the expectation-maximization iterations to prevent singular covariance matrices and control superpixel regularity. GMMSP has a reduced computational complexity by using only a subset of pixels to estimate the parameters of a Gaussian function. The proposal has well-balanced accuracy and regularity but does not allow direct control over the number of superpixels. Also, GMMSP may produce irregular superpixels on strong gradient regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS9.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.9.2. </span>gGMMSP</h4>
<div class="ltx_para" id="A1.SS9.SSS2.p1">
<p class="ltx_p" id="A1.SS9.SSS2.p1.1">To explore the parallelism in GMMSP <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib7" title="">2018</a>)</cite>, a real-time solution without the loss of segmentation consistency is proposed in <cite class="ltx_cite ltx_citemacro_citep">(Ban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib8" title="">2020</a>)</cite>. The proposed gGMMSP is implemented on CUDA for GPU processing and gives very similar segmentation results as GMMSP with much faster computation. The proposal maintains the core of the GMMSP algorithm, adapting its data structures and arithmetic computations to perform GPU processing. gGMMSP requires post-processing to ensure connectivity. However, this step has data dependencies preventing parallel computing, reducing the proposal of the speedup. Even with post-processing, the gGMMSP is faster than the serial and openMP versions of GMMSP, achieving speedups of 92.6 and 27.5, respectively.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.10. </span>CNN-based methods</h3>
<div class="ltx_para" id="A1.SS10.p1">
<p class="ltx_p" id="A1.SS10.p1.1"><span class="ltx_text" id="A1.SS10.p1.1.1" style="color:#000000;">In CNN-based superpixel segmentation, different strategies try to circumvent the limitations imposed by the rigid structure of the convolutional layers. First, we introduce SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>, E2E-SIS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib121" title="">2020a</a>)</cite>, BP-net <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib142" title="">2021c</a>)</cite>, and DAFnet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib131" title="">2021a</a>)</cite>, which use a differential clustering module based on SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite> for a pixel-superpixel assignment. Then, we discuss SEN <cite class="ltx_cite ltx_citemacro_citep">(Gaur and Manjunath, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib40" title="">2020</a>)</cite> and LNSNet <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib148" title="">2021</a>)</cite>, which perform unsupervised superpixel segmentation with differential clustering modules. After, we present ML-SGN <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib71" title="">2022</a>)</cite>, SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>, SENSS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib120" title="">2022</a>)</cite>, and AINET <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib127" title="">2021a</a>)</cite>, which are u-shaped architectures. Next, we introduce ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>, EW-RIM <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite>, and ML-RIM <cite class="ltx_cite ltx_citemacro_citep">(Eliasof et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib33" title="">2022</a>)</cite>, which integrate the soft pixel-superpixel assignment into the convolutional process. Finally, we present SIN <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib139" title="">2021a</a>)</cite>, which employs an interpolation network to enforce spatial connectivity. </span></p>
</div>
<section class="ltx_subsubsection" id="A1.SS10.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.1. </span>SSN</h4>
<div class="ltx_para" id="A1.SS10.SSS1.p1">
<p class="ltx_p" id="A1.SS10.SSS1.p1.1">The Superpixel Sampling Network (SSN) <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite> is the first deep-based approach for superpixel segmentation with an end-to-end trainable pipeline that provides superpixels instead of only extracting features. In <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>, the authors stated that previous superpixel algorithms are non-differentiable, making their backpropagation unfeasible. They overcome this by proposing a differentiable version of SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, where instead of a hard pixel-superpixel association, it provides a soft one. SSN has a fully convolutional network for feature extraction with seven convolutional layers interleaved with batch normalization and ReLU activations. After the second and fourth layers, a max pooling downsamples the input by a factor of two to increase the receptive field, and the input of the fourth and the sixth layers are upsampled and concatenated with the second layer’s output. The final layer output is concatenated with the XYLab of the given image and passed onto the differentiable SLIC that iteratively computes pixel-superpixel soft associations and superpixel centers. The authors demonstrated the proposal’s effectiveness for specific tasks. However, the SSN does not produce connected superpixels, making it necessary to make a non-differentiable post-processing. The number of superpixels is also based on the input image dimensions, thereby not providing the exact number of desired superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.2. </span>E2E-SIS</h4>
<div class="ltx_para" id="A1.SS10.SSS2.p1">
<p class="ltx_p" id="A1.SS10.SSS2.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib121" title="">2020a</a>)</cite> uses an end-to-end trainable CNN that learns deep features with two final layers, one for superpixels and the other for image segmentation. For superpixel segmentation, the deep features from the final CNN layer fed a differentiable clustering algorithm module <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>. The superpixel results and the deep features from the penultimate CNN layer are used by a superpixel pooling <cite class="ltx_cite ltx_citemacro_citep">(Kwak et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib63" title="">2017</a>)</cite> to learn semantic similarities. The ﬁnal segmentation is achieved by merging superpixels with high similarity.
The E2E-SIS has a high ability to perform image and superpixel segmentation with competitive results. Since the proposal is end-to-end trainable, it can be integrated into other deep learning-based methods.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.3. </span>BP-net</h4>
<div class="ltx_para" id="A1.SS10.SSS3.p1">
<p class="ltx_p" id="A1.SS10.SSS3.p1.1">BP-net <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib142" title="">2021c</a>)</cite> is a superpixel method for RGB-D images composed of a boundary detection network (B-net) and pixel labeling network (P-net). While the B-net learns boundaries in different scales to detect the geometry edges for depth information, the P-net extracts k-dimensional features from color information. The features extracted from P-net incorporate the geometry edge information from B-net by using a proposed boundary pass filter. The final feature map feds a differentiable SLIC <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite> to produce the final segmentation with a merging procedure, enforcing connectivity.
The BP-net generates visually regular superpixels, achieving a generally reasonable regularity and capturing structured-rich regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.4. </span>DAFnet</h4>
<div class="ltx_para" id="A1.SS10.SSS4.p1">
<p class="ltx_p" id="A1.SS10.SSS4.p1.1">To exploit stereo images, DAFnet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib131" title="">2021a</a>)</cite> integrates mutual information from both image views. The proposal first extracts deep features from both image views with a weight-shared convolution network. Then, the features are integrated with a Stereo Fusion Module (SFM), composed of a Parallax Attention Module (PAM) and a Stereo Channel Attention Module (SCAM). The PAM module models the relationship between the stereo image pair to capture its spatial level correspondence, generating an attention map through a parallax-attention mechanism <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib122" title="">2019</a>)</cite>. On the other hand, the SCAM module adaptively enhances the important information’s channel <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib54" title="">2018</a>)</cite>. Finally, inspired by SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>, a soft clustering module uses deep features and pixel-level information to generate the superpixels.
DAFnet is the first superpixel segmentation method that extracts deep features from stereo image pairs, and its proposed PAM and SCAM modules are demonstrated to improve the results.
However, it does not produce compact superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.5. </span>SEN</h4>
<div class="ltx_para" id="A1.SS10.SSS5.p1">
<p class="ltx_p" id="A1.SS10.SSS5.p1.1">SEN <cite class="ltx_cite ltx_citemacro_citep">(Gaur and Manjunath, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib40" title="">2020</a>)</cite> is an unsupervised method that learns deep embeddings using a U-net <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib97" title="">2015</a>)</cite> architecture with a differentiable Mean-Shift clustering based on <cite class="ltx_cite ltx_citemacro_citep">(Kong and Fowlkes, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib60" title="">2018</a>)</cite> for density estimation. The differentiable clustering module considers the global context, preventing embeddings from being labeled to optimize local distances. The proposal is end-to-end trainable and uses superpixel segmentation maps generated with SNIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta and Süsstrunk, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib3" title="">2017</a>)</cite> as a pseudo-ground-truth label to learn a new manifold whose feature distances act as a proxy for semantic similarity.
However, SEN produces more superpixels in homogenous image regions, missing some image boundaries in complex regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.6. </span>LNSNet</h4>
<div class="ltx_para" id="A1.SS10.SSS6.p1">
<p class="ltx_p" id="A1.SS10.SSS6.p1.1">LNSNet <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib148" title="">2021</a>)</cite> is an unsupervised CNN-based method that learns superpixels in a lifelong manner. It is composed of three major modules: a feature embedder module (FEM), a gradient rescaling module (GRM), and a non-iterative clustering module (NCM). FEM embeds the original feature into a cluster-friendly space. The NCM uses the embedded features to estimate the optimal cluster centers and assigns pixel labels based on similarity. Finally, the GRM solves the forgetting caused by lifelong learning during the backpropagation step using a Gradient Adaptive Layer (GAL) and a Gradient Bi-direction Layer (GBL).
LNSNet demonstrates a high generalization capacity and generates competitive superpixels using less complex and computationally faster architecture. However, the proposal has some drawbacks. First, due to the sequential training strategy, LNSNet cannot reach a complete convergence, requiring post-processing to remove trivial regions. Also, GBL’s boundary map may contain noises and lead to irregular superpixels when facing a background with a complex texture. Finally, the clustering step requires a distance matrix, which is inefficient when calculated by a CPU with a higher number of superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.7. </span>ML-SGN</h4>
<div class="ltx_para" id="A1.SS10.SSS7.p1">
<p class="ltx_p" id="A1.SS10.SSS7.p1.1">The authors in <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib71" title="">2022</a>)</cite> propose a multitask learning method for superpixel segmentation in SAR images. Along with superpixel segmentation, the proposed multitask learning-based superpixel generation network (ML-SGN) performs image segmentation as an auxiliary task to overcome the lack of labeled data. Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>, the ML-SGN uses a U-shaped architecture to extract features and a differential clustering strategy to produce a soft pixel-superpixel assignment. The authors employ a new distance metric based on the high-level feature space and propose a clustering module that considers high-dimensional features based on deep semantic, intensity, and spatial information. Its high-dimensional features can capture important image information, which results in highly adherent superpixels even in low-quality images. The ML-SGN is end-to-end trainable and can produce highly compact superpixels. However, it cannot directly produce superpixels.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.8. </span>SSFCN</h4>
<div class="ltx_para" id="A1.SS10.SSS8.p1">
<p class="ltx_p" id="A1.SS10.SSS8.p1.1">The proposal <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite> uses a standard encoder-decoder design with skip connections to predict association scores between pixels and regular grid cells and replace the hard pixel-superpixel assignment with a soft association map. In <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>, the authors proposed two loss functions: one, similar to SLIC <cite class="ltx_cite ltx_citemacro_citep">(Achanta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib2" title="">2012</a>)</cite>, uses an <math alttext="L_{2}" class="ltx_Math" display="inline" id="A1.SS10.SSS8.p1.1.m1.1"><semantics id="A1.SS10.SSS8.p1.1.m1.1a"><msub id="A1.SS10.SSS8.p1.1.m1.1.1" xref="A1.SS10.SSS8.p1.1.m1.1.1.cmml"><mi id="A1.SS10.SSS8.p1.1.m1.1.1.2" xref="A1.SS10.SSS8.p1.1.m1.1.1.2.cmml">L</mi><mn id="A1.SS10.SSS8.p1.1.m1.1.1.3" xref="A1.SS10.SSS8.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.SS10.SSS8.p1.1.m1.1b"><apply id="A1.SS10.SSS8.p1.1.m1.1.1.cmml" xref="A1.SS10.SSS8.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS10.SSS8.p1.1.m1.1.1.1.cmml" xref="A1.SS10.SSS8.p1.1.m1.1.1">subscript</csymbol><ci id="A1.SS10.SSS8.p1.1.m1.1.1.2.cmml" xref="A1.SS10.SSS8.p1.1.m1.1.1.2">𝐿</ci><cn id="A1.SS10.SSS8.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS10.SSS8.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS10.SSS8.p1.1.m1.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="A1.SS10.SSS8.p1.1.m1.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> norm as feature distance, and the other follows SSN <cite class="ltx_cite ltx_citemacro_citep">(Jampani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib56" title="">2018</a>)</cite>. Using the predicted pixel-superpixel association, SSFCN computes superpixels by assigning each pixel to the grid cell with the highest probability. The SSFCN generates compact superpixels on homogeneous image regions. The authors also demonstrate the proposal’s efficacy by modifying a network architecture for stereo matching <cite class="ltx_cite ltx_citemacro_citep">(Chang and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib23" title="">2018</a>)</cite> to predict simultaneously superpixels and disparities. As the main drawback, the number of superpixels is controlled based on the image size and requires a post-processing step to enforce connectivity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS9">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.9. </span>SENSS</h4>
<div class="ltx_para" id="A1.SS10.SSS9.p1">
<p class="ltx_p" id="A1.SS10.SSS9.p1.1">SENSS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib120" title="">2022</a>)</cite> incorporates Squeeze-and-Excitation (SE) modules <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib54" title="">2018</a>)</cite> into an SSFCN architecture <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>. The SE block explicitly models the inter-dependencies between channels, improving the representation power of the network. Therefore, the proposal has an encoder-decoder architecture with an attention module at each decoder block. The encoder produces high-level feature maps, and the decoder gradually upsamples the feature maps while modeling the channel-wise relationship. For training, the proposal uses the SSFCN’s differentiable loss function.
The proposed network outperforms the SSFCN performance, improving its learning ability with the SE blocks and achieving competitive results. However, the SE blocks have an additional computational cost. Also, SENSS has the same drawbacks as SSFCN, with limited control of the superpixels’ number, and needs post-processing to guarantee connectivity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS10">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.10. </span>AINET</h4>
<div class="ltx_para" id="A1.SS10.SSS10.p1">
<p class="ltx_p" id="A1.SS10.SSS10.p1.1">Most deep-based superpixel models identify pixel-pixel affinities in the grid image pattern to compute superpixels instead of directly associating pixels to superpixels. In <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib127" title="">2021a</a>)</cite>, an Association Implantation (AI) module is proposed to associate each pixel with its surrounding superpixels in a grid shape. They also employ a boundary-perceiving loss based on the distance between the pixel label and its reconstructed label to improve boundary delineation. The AINET is composed of a U-net architecture <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib97" title="">2015</a>)</cite> with skip connections based on SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>, followed by an AI module. The encoder outputs a superpixel embedding in which features are propagated with a convolution and the decoder outputs a pixel embedding. The AI module computes a pixel-superpixel association based on both embeddings. Using a loss function similar to <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite>, the model improves boundary precision by incorporating a boundary-perceiving loss.
The AINET produces highly boundary-adherent superpixels with no compacity in textured regions, but it produces fewer thin superpixels near the strong image boundaries. Following SSFCN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib135" title="">2020</a>)</cite> strategy, the association map produced by AINET is based on a fixed grid sampling. Therefore, it allows partial control over the number of superpixels with image resizing, which may reduce boundary precision for images whose original size is not a multiple of the sampling spacing.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS11">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.11. </span>ss-RIM</h4>
<div class="ltx_para" id="A1.SS10.SSS11.p1">
<p class="ltx_p" id="A1.SS10.SSS11.p1.1">Based on the idea that low-level features are insufficient to improve segmentation with few superpixels, the authors <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite> induce non-local properties into an unsupervised CNN-based method. The proposal uses the <span class="ltx_text ltx_font_italic" id="A1.SS10.SSS11.p1.1.1">Deep Image Prior</span> (DIP) <cite class="ltx_cite ltx_citemacro_citep">(Lempitsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib64" title="">2018</a>)</cite> procedure to generate task-agnostic superpixels with a new loss function based on clustering, spatial smoothness, and reconstruction. The clustering term is similar to the mutual information term of RIM <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib61" title="">2010</a>)</cite>, and the spatial smoothness cost is the same as proposed in <cite class="ltx_cite ltx_citemacro_citep">(Godard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib43" title="">2017</a>)</cite>. Finally, the reconstruction cost helps the loss function fit the superpixels at the components’ boundaries.
The proposed method is able to generate superpixels more attached to the image boundaries, especially in heterogeneous regions. However, the ss-RIM only allows control of the upper bound number of superpixels and does not ensure connectivity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS12">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.12. </span>EW-RIM</h4>
<div class="ltx_para" id="A1.SS10.SSS12.p1">
<p class="ltx_p" id="A1.SS10.SSS12.p1.1">Based on ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>, the proposal in <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite> encompasses a loss function composed of four terms based on clustering <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib61" title="">2010</a>)</cite>, smooth <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>, reconstruction <cite class="ltx_cite ltx_citemacro_citep">(Godard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib43" title="">2017</a>)</cite>, and edge-aware. The edge awareness accomplishes a differential approximation to the distribution of image gradients. Using RGB color and spatial information as input, the EW-RIM extracts feature information from a CNN architecture with a feature merging step to obtain the association probability maps.
The proposed edge-aware term improves the boundary adherence of the proposed EW-RIM compared with ss-RIM, but its compactness is less.
Also, since the proposal’s segmentation generates more similar superpixels in size, it does not preserve finer details in complex regions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS13">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.13. </span>ML-RIM</h4>
<div class="ltx_para" id="A1.SS10.SSS13.p1">
<p class="ltx_p" id="A1.SS10.SSS13.p1.1">Based on ss-RIM <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite> and EW-RIM <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite>, the authors in <cite class="ltx_cite ltx_citemacro_citep">(Eliasof et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib33" title="">2022</a>)</cite> proposed an unsupervised network for superpixel segmentation. Similar to EW-RIM, the proposed <span class="ltx_text ltx_font_italic" id="A1.SS10.SSS13.p1.1.1">Multi-Scale RIM</span> (ML-RIM) encompasses a loss function composed of four terms based on clustering <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib61" title="">2010</a>)</cite>, smooth <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>)</cite>, reconstruction, and edge-aware. Compared to other RIM-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Suzuki, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib111" title="">2020</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib137" title="">2021</a>)</cite>, the reconstruction loss term in ML-RIM considers the mean squared error of the differentiable superpixel assignment learned by the network, and the edge-aware term employs the Kull-back-Leibler (KL) divergence loss to match between the edge distributions. In ML-RIM, edge-maps are computed using a laplacian kernel followed by a softmax. In ML-RIM, each convolutional layer is followed by an instance normalization and ReLU activation. The network is composed of a feature extractor with four convolutional layers, an ASPP module <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib25" title="">2017b</a>)</cite> to combine multi-scale features, and a final convolutional layer to transform the output features to the desired shape followed by a softmax function. ML-RIM has improved accuracy and boundary recall compared to ss-RIM and EW-RIM, and similar compactness to ss-RIM.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS10.SSS14">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.10.14. </span>SIN</h4>
<div class="ltx_para" id="A1.SS10.SSS14.p1">
<p class="ltx_p" id="A1.SS10.SSS14.p1.1">The SIN’s <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib139" title="">2021a</a>)</cite> architecture utilizes multi-layer outputs to predict association scores using interpolations. The proposed architecture reduces the feature channels by half to extract multi-layer features with outputs to convolutional operations. Then, the convolutional operations transform the multi-layer features into 2-dimensional association scores. Finally, a pixel-superpixel map procedure uses multiple interpolations with the association scores to expand the pixel-superpixel association matrix while enforcing spatial connectivity. The initial pixel-superpixel map has a reduced size and initializes with regular sampling.
The proposed method produces connected components without post-processing, being able to integrate them into downstream tasks in an end-to-end way. SIN is faster than other deep learning-based superpixel methods, and it produces more compact and regular superpixels.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation measures</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In general, the measures for superpixel evaluation can be divided into measures that evaluate: (i) superpixel delineation; (ii) its shape; or (iii) its color homogeneity. The delineation measures evaluate the overlap of the superpixel boundaries with the image object. The delineation-based evaluation is widespread in superpixel segmentation since the oversegmentation of the object and background regions is not penalized. On the other hand, the quality of the superpixels inside these regions is also not evaluated <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>. In this work, we evaluated boundary delineation using <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">Boundary Recall</span> (BR) <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib82" title="">2004</a>)</cite> and <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">Undersegmentation Error</span> (UE) <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite>. For color homogeneity assessment, we used the <span class="ltx_text ltx_font_italic" id="A2.p1.1.3">Similarity between Image and Reconstruction from Superpixels</span> (SIRS) <cite class="ltx_cite ltx_citemacro_citep">(Barcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib9" title="">2022</a>)</cite> and <span class="ltx_text ltx_font_italic" id="A2.p1.1.4">Explained Variation</span> (EV) <cite class="ltx_cite ltx_citemacro_citep">(Moore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib87" title="">2008</a>)</cite>. Finally, we assess superpixels’ compactness using the <span class="ltx_text ltx_font_italic" id="A2.p1.1.5">Compactness index</span> (CO) <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib99" title="">2012</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.7"><span class="ltx_text ltx_font_italic" id="A2.p2.7.1">Boundary Recall</span> (BR) <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib82" title="">2004</a>)</cite> is a widely used measure for superpixel evaluation. It measures the fraction of ground-truth boundary pixels correctly detected, as presented in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E1" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, in which TP is the number of boundary pixels in a segmentation <math alttext="S" class="ltx_Math" display="inline" id="A2.p2.1.m1.1"><semantics id="A2.p2.1.m1.1a"><mi id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><ci id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="A2.p2.1.m1.1d">italic_S</annotation></semantics></math> that match the ground truth <math alttext="G" class="ltx_Math" display="inline" id="A2.p2.2.m2.1"><semantics id="A2.p2.2.m2.1a"><mi id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><ci id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p2.2.m2.1d">italic_G</annotation></semantics></math>, and FN is the number of boundary pixels in <math alttext="G" class="ltx_Math" display="inline" id="A2.p2.3.m3.1"><semantics id="A2.p2.3.m3.1a"><mi id="A2.p2.3.m3.1.1" xref="A2.p2.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p2.3.m3.1b"><ci id="A2.p2.3.m3.1.1.cmml" xref="A2.p2.3.m3.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.3.m3.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p2.3.m3.1d">italic_G</annotation></semantics></math> that does not match with <math alttext="S" class="ltx_Math" display="inline" id="A2.p2.4.m4.1"><semantics id="A2.p2.4.m4.1a"><mi id="A2.p2.4.m4.1.1" xref="A2.p2.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A2.p2.4.m4.1b"><ci id="A2.p2.4.m4.1.1.cmml" xref="A2.p2.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="A2.p2.4.m4.1d">italic_S</annotation></semantics></math>. The boundary pixels are matched within a local neighborhood of size <math alttext="(2r+1)^{2}" class="ltx_Math" display="inline" id="A2.p2.5.m5.1"><semantics id="A2.p2.5.m5.1a"><msup id="A2.p2.5.m5.1.1" xref="A2.p2.5.m5.1.1.cmml"><mrow id="A2.p2.5.m5.1.1.1.1" xref="A2.p2.5.m5.1.1.1.1.1.cmml"><mo id="A2.p2.5.m5.1.1.1.1.2" stretchy="false" xref="A2.p2.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="A2.p2.5.m5.1.1.1.1.1" xref="A2.p2.5.m5.1.1.1.1.1.cmml"><mrow id="A2.p2.5.m5.1.1.1.1.1.2" xref="A2.p2.5.m5.1.1.1.1.1.2.cmml"><mn id="A2.p2.5.m5.1.1.1.1.1.2.2" xref="A2.p2.5.m5.1.1.1.1.1.2.2.cmml">2</mn><mo id="A2.p2.5.m5.1.1.1.1.1.2.1" xref="A2.p2.5.m5.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="A2.p2.5.m5.1.1.1.1.1.2.3" xref="A2.p2.5.m5.1.1.1.1.1.2.3.cmml">r</mi></mrow><mo id="A2.p2.5.m5.1.1.1.1.1.1" xref="A2.p2.5.m5.1.1.1.1.1.1.cmml">+</mo><mn id="A2.p2.5.m5.1.1.1.1.1.3" xref="A2.p2.5.m5.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A2.p2.5.m5.1.1.1.1.3" stretchy="false" xref="A2.p2.5.m5.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.p2.5.m5.1.1.3" xref="A2.p2.5.m5.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="A2.p2.5.m5.1b"><apply id="A2.p2.5.m5.1.1.cmml" xref="A2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A2.p2.5.m5.1.1.2.cmml" xref="A2.p2.5.m5.1.1">superscript</csymbol><apply id="A2.p2.5.m5.1.1.1.1.1.cmml" xref="A2.p2.5.m5.1.1.1.1"><plus id="A2.p2.5.m5.1.1.1.1.1.1.cmml" xref="A2.p2.5.m5.1.1.1.1.1.1"></plus><apply id="A2.p2.5.m5.1.1.1.1.1.2.cmml" xref="A2.p2.5.m5.1.1.1.1.1.2"><times id="A2.p2.5.m5.1.1.1.1.1.2.1.cmml" xref="A2.p2.5.m5.1.1.1.1.1.2.1"></times><cn id="A2.p2.5.m5.1.1.1.1.1.2.2.cmml" type="integer" xref="A2.p2.5.m5.1.1.1.1.1.2.2">2</cn><ci id="A2.p2.5.m5.1.1.1.1.1.2.3.cmml" xref="A2.p2.5.m5.1.1.1.1.1.2.3">𝑟</ci></apply><cn id="A2.p2.5.m5.1.1.1.1.1.3.cmml" type="integer" xref="A2.p2.5.m5.1.1.1.1.1.3">1</cn></apply><cn id="A2.p2.5.m5.1.1.3.cmml" type="integer" xref="A2.p2.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.5.m5.1c">(2r+1)^{2}</annotation><annotation encoding="application/x-llamapun" id="A2.p2.5.m5.1d">( 2 italic_r + 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, in which <math alttext="r" class="ltx_Math" display="inline" id="A2.p2.6.m6.1"><semantics id="A2.p2.6.m6.1a"><mi id="A2.p2.6.m6.1.1" xref="A2.p2.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A2.p2.6.m6.1b"><ci id="A2.p2.6.m6.1.1.cmml" xref="A2.p2.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.6.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="A2.p2.6.m6.1d">italic_r</annotation></semantics></math> is <math alttext="0.0025" class="ltx_Math" display="inline" id="A2.p2.7.m7.1"><semantics id="A2.p2.7.m7.1a"><mn id="A2.p2.7.m7.1.1" xref="A2.p2.7.m7.1.1.cmml">0.0025</mn><annotation-xml encoding="MathML-Content" id="A2.p2.7.m7.1b"><cn id="A2.p2.7.m7.1.1.cmml" type="float" xref="A2.p2.7.m7.1.1">0.0025</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.7.m7.1c">0.0025</annotation><annotation encoding="application/x-llamapun" id="A2.p2.7.m7.1d">0.0025</annotation></semantics></math> times the image diagonal.</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{BR}(S,G)=\frac{\text{TP}(G,S)}{\text{TP}(G,S)+\text{FN}(G,S)}" class="ltx_Math" display="block" id="A2.E1.m1.8"><semantics id="A2.E1.m1.8a"><mrow id="A2.E1.m1.8.9" xref="A2.E1.m1.8.9.cmml"><mrow id="A2.E1.m1.8.9.2" xref="A2.E1.m1.8.9.2.cmml"><mtext id="A2.E1.m1.8.9.2.2" xref="A2.E1.m1.8.9.2.2a.cmml">BR</mtext><mo id="A2.E1.m1.8.9.2.1" xref="A2.E1.m1.8.9.2.1.cmml">⁢</mo><mrow id="A2.E1.m1.8.9.2.3.2" xref="A2.E1.m1.8.9.2.3.1.cmml"><mo id="A2.E1.m1.8.9.2.3.2.1" stretchy="false" xref="A2.E1.m1.8.9.2.3.1.cmml">(</mo><mi id="A2.E1.m1.7.7" xref="A2.E1.m1.7.7.cmml">S</mi><mo id="A2.E1.m1.8.9.2.3.2.2" xref="A2.E1.m1.8.9.2.3.1.cmml">,</mo><mi id="A2.E1.m1.8.8" xref="A2.E1.m1.8.8.cmml">G</mi><mo id="A2.E1.m1.8.9.2.3.2.3" stretchy="false" xref="A2.E1.m1.8.9.2.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E1.m1.8.9.1" xref="A2.E1.m1.8.9.1.cmml">=</mo><mfrac id="A2.E1.m1.6.6" xref="A2.E1.m1.6.6.cmml"><mrow id="A2.E1.m1.2.2.2" xref="A2.E1.m1.2.2.2.cmml"><mtext id="A2.E1.m1.2.2.2.4" xref="A2.E1.m1.2.2.2.4a.cmml">TP</mtext><mo id="A2.E1.m1.2.2.2.3" xref="A2.E1.m1.2.2.2.3.cmml">⁢</mo><mrow id="A2.E1.m1.2.2.2.5.2" xref="A2.E1.m1.2.2.2.5.1.cmml"><mo id="A2.E1.m1.2.2.2.5.2.1" stretchy="false" xref="A2.E1.m1.2.2.2.5.1.cmml">(</mo><mi id="A2.E1.m1.1.1.1.1" xref="A2.E1.m1.1.1.1.1.cmml">G</mi><mo id="A2.E1.m1.2.2.2.5.2.2" xref="A2.E1.m1.2.2.2.5.1.cmml">,</mo><mi id="A2.E1.m1.2.2.2.2" xref="A2.E1.m1.2.2.2.2.cmml">S</mi><mo id="A2.E1.m1.2.2.2.5.2.3" stretchy="false" xref="A2.E1.m1.2.2.2.5.1.cmml">)</mo></mrow></mrow><mrow id="A2.E1.m1.6.6.6" xref="A2.E1.m1.6.6.6.cmml"><mrow id="A2.E1.m1.6.6.6.6" xref="A2.E1.m1.6.6.6.6.cmml"><mtext id="A2.E1.m1.6.6.6.6.2" xref="A2.E1.m1.6.6.6.6.2a.cmml">TP</mtext><mo id="A2.E1.m1.6.6.6.6.1" xref="A2.E1.m1.6.6.6.6.1.cmml">⁢</mo><mrow id="A2.E1.m1.6.6.6.6.3.2" xref="A2.E1.m1.6.6.6.6.3.1.cmml"><mo id="A2.E1.m1.6.6.6.6.3.2.1" stretchy="false" xref="A2.E1.m1.6.6.6.6.3.1.cmml">(</mo><mi id="A2.E1.m1.3.3.3.1" xref="A2.E1.m1.3.3.3.1.cmml">G</mi><mo id="A2.E1.m1.6.6.6.6.3.2.2" xref="A2.E1.m1.6.6.6.6.3.1.cmml">,</mo><mi id="A2.E1.m1.4.4.4.2" xref="A2.E1.m1.4.4.4.2.cmml">S</mi><mo id="A2.E1.m1.6.6.6.6.3.2.3" stretchy="false" xref="A2.E1.m1.6.6.6.6.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E1.m1.6.6.6.5" xref="A2.E1.m1.6.6.6.5.cmml">+</mo><mrow id="A2.E1.m1.6.6.6.7" xref="A2.E1.m1.6.6.6.7.cmml"><mtext id="A2.E1.m1.6.6.6.7.2" xref="A2.E1.m1.6.6.6.7.2a.cmml">FN</mtext><mo id="A2.E1.m1.6.6.6.7.1" xref="A2.E1.m1.6.6.6.7.1.cmml">⁢</mo><mrow id="A2.E1.m1.6.6.6.7.3.2" xref="A2.E1.m1.6.6.6.7.3.1.cmml"><mo id="A2.E1.m1.6.6.6.7.3.2.1" stretchy="false" xref="A2.E1.m1.6.6.6.7.3.1.cmml">(</mo><mi id="A2.E1.m1.5.5.5.3" xref="A2.E1.m1.5.5.5.3.cmml">G</mi><mo id="A2.E1.m1.6.6.6.7.3.2.2" xref="A2.E1.m1.6.6.6.7.3.1.cmml">,</mo><mi id="A2.E1.m1.6.6.6.4" xref="A2.E1.m1.6.6.6.4.cmml">S</mi><mo id="A2.E1.m1.6.6.6.7.3.2.3" stretchy="false" xref="A2.E1.m1.6.6.6.7.3.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A2.E1.m1.8b"><apply id="A2.E1.m1.8.9.cmml" xref="A2.E1.m1.8.9"><eq id="A2.E1.m1.8.9.1.cmml" xref="A2.E1.m1.8.9.1"></eq><apply id="A2.E1.m1.8.9.2.cmml" xref="A2.E1.m1.8.9.2"><times id="A2.E1.m1.8.9.2.1.cmml" xref="A2.E1.m1.8.9.2.1"></times><ci id="A2.E1.m1.8.9.2.2a.cmml" xref="A2.E1.m1.8.9.2.2"><mtext id="A2.E1.m1.8.9.2.2.cmml" xref="A2.E1.m1.8.9.2.2">BR</mtext></ci><interval closure="open" id="A2.E1.m1.8.9.2.3.1.cmml" xref="A2.E1.m1.8.9.2.3.2"><ci id="A2.E1.m1.7.7.cmml" xref="A2.E1.m1.7.7">𝑆</ci><ci id="A2.E1.m1.8.8.cmml" xref="A2.E1.m1.8.8">𝐺</ci></interval></apply><apply id="A2.E1.m1.6.6.cmml" xref="A2.E1.m1.6.6"><divide id="A2.E1.m1.6.6.7.cmml" xref="A2.E1.m1.6.6"></divide><apply id="A2.E1.m1.2.2.2.cmml" xref="A2.E1.m1.2.2.2"><times id="A2.E1.m1.2.2.2.3.cmml" xref="A2.E1.m1.2.2.2.3"></times><ci id="A2.E1.m1.2.2.2.4a.cmml" xref="A2.E1.m1.2.2.2.4"><mtext id="A2.E1.m1.2.2.2.4.cmml" xref="A2.E1.m1.2.2.2.4">TP</mtext></ci><interval closure="open" id="A2.E1.m1.2.2.2.5.1.cmml" xref="A2.E1.m1.2.2.2.5.2"><ci id="A2.E1.m1.1.1.1.1.cmml" xref="A2.E1.m1.1.1.1.1">𝐺</ci><ci id="A2.E1.m1.2.2.2.2.cmml" xref="A2.E1.m1.2.2.2.2">𝑆</ci></interval></apply><apply id="A2.E1.m1.6.6.6.cmml" xref="A2.E1.m1.6.6.6"><plus id="A2.E1.m1.6.6.6.5.cmml" xref="A2.E1.m1.6.6.6.5"></plus><apply id="A2.E1.m1.6.6.6.6.cmml" xref="A2.E1.m1.6.6.6.6"><times id="A2.E1.m1.6.6.6.6.1.cmml" xref="A2.E1.m1.6.6.6.6.1"></times><ci id="A2.E1.m1.6.6.6.6.2a.cmml" xref="A2.E1.m1.6.6.6.6.2"><mtext id="A2.E1.m1.6.6.6.6.2.cmml" xref="A2.E1.m1.6.6.6.6.2">TP</mtext></ci><interval closure="open" id="A2.E1.m1.6.6.6.6.3.1.cmml" xref="A2.E1.m1.6.6.6.6.3.2"><ci id="A2.E1.m1.3.3.3.1.cmml" xref="A2.E1.m1.3.3.3.1">𝐺</ci><ci id="A2.E1.m1.4.4.4.2.cmml" xref="A2.E1.m1.4.4.4.2">𝑆</ci></interval></apply><apply id="A2.E1.m1.6.6.6.7.cmml" xref="A2.E1.m1.6.6.6.7"><times id="A2.E1.m1.6.6.6.7.1.cmml" xref="A2.E1.m1.6.6.6.7.1"></times><ci id="A2.E1.m1.6.6.6.7.2a.cmml" xref="A2.E1.m1.6.6.6.7.2"><mtext id="A2.E1.m1.6.6.6.7.2.cmml" xref="A2.E1.m1.6.6.6.7.2">FN</mtext></ci><interval closure="open" id="A2.E1.m1.6.6.6.7.3.1.cmml" xref="A2.E1.m1.6.6.6.7.3.2"><ci id="A2.E1.m1.5.5.5.3.cmml" xref="A2.E1.m1.5.5.5.3">𝐺</ci><ci id="A2.E1.m1.6.6.6.4.cmml" xref="A2.E1.m1.6.6.6.4">𝑆</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E1.m1.8c">\text{BR}(S,G)=\frac{\text{TP}(G,S)}{\text{TP}(G,S)+\text{FN}(G,S)}</annotation><annotation encoding="application/x-llamapun" id="A2.E1.m1.8d">BR ( italic_S , italic_G ) = divide start_ARG TP ( italic_G , italic_S ) end_ARG start_ARG TP ( italic_G , italic_S ) + FN ( italic_G , italic_S ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.10">Another widely used measure to assess the quality of superpixel segmentation delineation is the <span class="ltx_text ltx_font_italic" id="A2.p3.10.1">Undersegmentation Error</span> (UE). Introduced by <cite class="ltx_cite ltx_citemacro_citep">(Levinshtein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib65" title="">2009</a>)</cite>, the UE measures the adherence of the boundary pixels in a segmentation <math alttext="S" class="ltx_Math" display="inline" id="A2.p3.1.m1.1"><semantics id="A2.p3.1.m1.1a"><mi id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><ci id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="A2.p3.1.m1.1d">italic_S</annotation></semantics></math> to the ground truth <math alttext="G" class="ltx_Math" display="inline" id="A2.p3.2.m2.1"><semantics id="A2.p3.2.m2.1a"><mi id="A2.p3.2.m2.1.1" xref="A2.p3.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p3.2.m2.1b"><ci id="A2.p3.2.m2.1.1.cmml" xref="A2.p3.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.2.m2.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p3.2.m2.1d">italic_G</annotation></semantics></math> contours based on the area between <math alttext="S" class="ltx_Math" display="inline" id="A2.p3.3.m3.1"><semantics id="A2.p3.3.m3.1a"><mi id="A2.p3.3.m3.1.1" xref="A2.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A2.p3.3.m3.1b"><ci id="A2.p3.3.m3.1.1.cmml" xref="A2.p3.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="A2.p3.3.m3.1d">italic_S</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="A2.p3.4.m4.1"><semantics id="A2.p3.4.m4.1a"><mi id="A2.p3.4.m4.1.1" xref="A2.p3.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p3.4.m4.1b"><ci id="A2.p3.4.m4.1.1.cmml" xref="A2.p3.4.m4.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.4.m4.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p3.4.m4.1d">italic_G</annotation></semantics></math> regions. UE has different versions <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>. The most recommended was proposed by <cite class="ltx_cite ltx_citemacro_citep">(Neubert and Protzel, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib88" title="">2012</a>)</cite> that evaluated the adherence to contours based on the minimum area of overlap between <math alttext="S" class="ltx_Math" display="inline" id="A2.p3.5.m5.1"><semantics id="A2.p3.5.m5.1a"><mi id="A2.p3.5.m5.1.1" xref="A2.p3.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A2.p3.5.m5.1b"><ci id="A2.p3.5.m5.1.1.cmml" xref="A2.p3.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.5.m5.1c">S</annotation><annotation encoding="application/x-llamapun" id="A2.p3.5.m5.1d">italic_S</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="A2.p3.6.m6.1"><semantics id="A2.p3.6.m6.1a"><mi id="A2.p3.6.m6.1.1" xref="A2.p3.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p3.6.m6.1b"><ci id="A2.p3.6.m6.1.1.cmml" xref="A2.p3.6.m6.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.6.m6.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p3.6.m6.1d">italic_G</annotation></semantics></math>, as presented in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E2" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, in which <math alttext="N" class="ltx_Math" display="inline" id="A2.p3.7.m7.1"><semantics id="A2.p3.7.m7.1a"><mi id="A2.p3.7.m7.1.1" xref="A2.p3.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.p3.7.m7.1b"><ci id="A2.p3.7.m7.1.1.cmml" xref="A2.p3.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.7.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.p3.7.m7.1d">italic_N</annotation></semantics></math> is the number of pixels <math alttext="G" class="ltx_Math" display="inline" id="A2.p3.8.m8.1"><semantics id="A2.p3.8.m8.1a"><mi id="A2.p3.8.m8.1.1" xref="A2.p3.8.m8.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p3.8.m8.1b"><ci id="A2.p3.8.m8.1.1.cmml" xref="A2.p3.8.m8.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.8.m8.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p3.8.m8.1d">italic_G</annotation></semantics></math> and <math alttext="k" class="ltx_Math" display="inline" id="A2.p3.9.m9.1"><semantics id="A2.p3.9.m9.1a"><mi id="A2.p3.9.m9.1.1" xref="A2.p3.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.p3.9.m9.1b"><ci id="A2.p3.9.m9.1.1.cmml" xref="A2.p3.9.m9.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.9.m9.1c">k</annotation><annotation encoding="application/x-llamapun" id="A2.p3.9.m9.1d">italic_k</annotation></semantics></math> is the number of regions in <math alttext="G" class="ltx_Math" display="inline" id="A2.p3.10.m10.1"><semantics id="A2.p3.10.m10.1a"><mi id="A2.p3.10.m10.1.1" xref="A2.p3.10.m10.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A2.p3.10.m10.1b"><ci id="A2.p3.10.m10.1.1.cmml" xref="A2.p3.10.m10.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.10.m10.1c">G</annotation><annotation encoding="application/x-llamapun" id="A2.p3.10.m10.1d">italic_G</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{UE}(S,G)=\frac{1}{N}\sum_{i}^{k}\sum_{S_{j}\cap G_{i}\neq\varnothing}%
\min\{|S_{j}\cap G_{i}|,|S_{j}-G_{i}|\}" class="ltx_Math" display="block" id="A2.E2.m1.5"><semantics id="A2.E2.m1.5a"><mrow id="A2.E2.m1.5.5" xref="A2.E2.m1.5.5.cmml"><mrow id="A2.E2.m1.5.5.4" xref="A2.E2.m1.5.5.4.cmml"><mtext id="A2.E2.m1.5.5.4.2" xref="A2.E2.m1.5.5.4.2a.cmml">UE</mtext><mo id="A2.E2.m1.5.5.4.1" xref="A2.E2.m1.5.5.4.1.cmml">⁢</mo><mrow id="A2.E2.m1.5.5.4.3.2" xref="A2.E2.m1.5.5.4.3.1.cmml"><mo id="A2.E2.m1.5.5.4.3.2.1" stretchy="false" xref="A2.E2.m1.5.5.4.3.1.cmml">(</mo><mi id="A2.E2.m1.1.1" xref="A2.E2.m1.1.1.cmml">S</mi><mo id="A2.E2.m1.5.5.4.3.2.2" xref="A2.E2.m1.5.5.4.3.1.cmml">,</mo><mi id="A2.E2.m1.2.2" xref="A2.E2.m1.2.2.cmml">G</mi><mo id="A2.E2.m1.5.5.4.3.2.3" stretchy="false" xref="A2.E2.m1.5.5.4.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E2.m1.5.5.3" xref="A2.E2.m1.5.5.3.cmml">=</mo><mrow id="A2.E2.m1.5.5.2" xref="A2.E2.m1.5.5.2.cmml"><mfrac id="A2.E2.m1.5.5.2.4" xref="A2.E2.m1.5.5.2.4.cmml"><mn id="A2.E2.m1.5.5.2.4.2" xref="A2.E2.m1.5.5.2.4.2.cmml">1</mn><mi id="A2.E2.m1.5.5.2.4.3" xref="A2.E2.m1.5.5.2.4.3.cmml">N</mi></mfrac><mo id="A2.E2.m1.5.5.2.3" xref="A2.E2.m1.5.5.2.3.cmml">⁢</mo><mrow id="A2.E2.m1.5.5.2.2" xref="A2.E2.m1.5.5.2.2.cmml"><munderover id="A2.E2.m1.5.5.2.2.3" xref="A2.E2.m1.5.5.2.2.3.cmml"><mo id="A2.E2.m1.5.5.2.2.3.2.2" movablelimits="false" rspace="0em" xref="A2.E2.m1.5.5.2.2.3.2.2.cmml">∑</mo><mi id="A2.E2.m1.5.5.2.2.3.2.3" xref="A2.E2.m1.5.5.2.2.3.2.3.cmml">i</mi><mi id="A2.E2.m1.5.5.2.2.3.3" xref="A2.E2.m1.5.5.2.2.3.3.cmml">k</mi></munderover><mrow id="A2.E2.m1.5.5.2.2.2" xref="A2.E2.m1.5.5.2.2.2.cmml"><munder id="A2.E2.m1.5.5.2.2.2.3" xref="A2.E2.m1.5.5.2.2.2.3.cmml"><mo id="A2.E2.m1.5.5.2.2.2.3.2" movablelimits="false" xref="A2.E2.m1.5.5.2.2.2.3.2.cmml">∑</mo><mrow id="A2.E2.m1.5.5.2.2.2.3.3" xref="A2.E2.m1.5.5.2.2.2.3.3.cmml"><mrow id="A2.E2.m1.5.5.2.2.2.3.3.2" xref="A2.E2.m1.5.5.2.2.2.3.3.2.cmml"><msub id="A2.E2.m1.5.5.2.2.2.3.3.2.2" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2.cmml"><mi id="A2.E2.m1.5.5.2.2.2.3.3.2.2.2" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2.2.cmml">S</mi><mi id="A2.E2.m1.5.5.2.2.2.3.3.2.2.3" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2.3.cmml">j</mi></msub><mo id="A2.E2.m1.5.5.2.2.2.3.3.2.1" xref="A2.E2.m1.5.5.2.2.2.3.3.2.1.cmml">∩</mo><msub id="A2.E2.m1.5.5.2.2.2.3.3.2.3" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3.cmml"><mi id="A2.E2.m1.5.5.2.2.2.3.3.2.3.2" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3.2.cmml">G</mi><mi id="A2.E2.m1.5.5.2.2.2.3.3.2.3.3" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3.3.cmml">i</mi></msub></mrow><mo id="A2.E2.m1.5.5.2.2.2.3.3.1" xref="A2.E2.m1.5.5.2.2.2.3.3.1.cmml">≠</mo><mi id="A2.E2.m1.5.5.2.2.2.3.3.3" mathvariant="normal" xref="A2.E2.m1.5.5.2.2.2.3.3.3.cmml">∅</mi></mrow></munder><mrow id="A2.E2.m1.5.5.2.2.2.2.2" xref="A2.E2.m1.5.5.2.2.2.2.3.cmml"><mi id="A2.E2.m1.3.3" xref="A2.E2.m1.3.3.cmml">min</mi><mo id="A2.E2.m1.5.5.2.2.2.2.2a" xref="A2.E2.m1.5.5.2.2.2.2.3.cmml">⁡</mo><mrow id="A2.E2.m1.5.5.2.2.2.2.2.2" xref="A2.E2.m1.5.5.2.2.2.2.3.cmml"><mo id="A2.E2.m1.5.5.2.2.2.2.2.2.3" stretchy="false" xref="A2.E2.m1.5.5.2.2.2.2.3.cmml">{</mo><mrow id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><msub id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml">S</mi><mi id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">∩</mo><msub id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml">G</mi><mi id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="A2.E2.m1.5.5.2.2.2.2.2.2.4" xref="A2.E2.m1.5.5.2.2.2.2.3.cmml">,</mo><mrow id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.2.cmml"><mo id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.2" stretchy="false" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.2.1.cmml">|</mo><mrow id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.cmml"><msub id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.cmml"><mi id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.2" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.2.cmml">S</mi><mi id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.3" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.3.cmml">j</mi></msub><mo id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.1" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.1.cmml">−</mo><msub id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.cmml"><mi id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.2" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.2.cmml">G</mi><mi id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.3" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.3.cmml">i</mi></msub></mrow><mo id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.3" stretchy="false" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.2.1.cmml">|</mo></mrow><mo id="A2.E2.m1.5.5.2.2.2.2.2.2.5" stretchy="false" xref="A2.E2.m1.5.5.2.2.2.2.3.cmml">}</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.E2.m1.5b"><apply id="A2.E2.m1.5.5.cmml" xref="A2.E2.m1.5.5"><eq id="A2.E2.m1.5.5.3.cmml" xref="A2.E2.m1.5.5.3"></eq><apply id="A2.E2.m1.5.5.4.cmml" xref="A2.E2.m1.5.5.4"><times id="A2.E2.m1.5.5.4.1.cmml" xref="A2.E2.m1.5.5.4.1"></times><ci id="A2.E2.m1.5.5.4.2a.cmml" xref="A2.E2.m1.5.5.4.2"><mtext id="A2.E2.m1.5.5.4.2.cmml" xref="A2.E2.m1.5.5.4.2">UE</mtext></ci><interval closure="open" id="A2.E2.m1.5.5.4.3.1.cmml" xref="A2.E2.m1.5.5.4.3.2"><ci id="A2.E2.m1.1.1.cmml" xref="A2.E2.m1.1.1">𝑆</ci><ci id="A2.E2.m1.2.2.cmml" xref="A2.E2.m1.2.2">𝐺</ci></interval></apply><apply id="A2.E2.m1.5.5.2.cmml" xref="A2.E2.m1.5.5.2"><times id="A2.E2.m1.5.5.2.3.cmml" xref="A2.E2.m1.5.5.2.3"></times><apply id="A2.E2.m1.5.5.2.4.cmml" xref="A2.E2.m1.5.5.2.4"><divide id="A2.E2.m1.5.5.2.4.1.cmml" xref="A2.E2.m1.5.5.2.4"></divide><cn id="A2.E2.m1.5.5.2.4.2.cmml" type="integer" xref="A2.E2.m1.5.5.2.4.2">1</cn><ci id="A2.E2.m1.5.5.2.4.3.cmml" xref="A2.E2.m1.5.5.2.4.3">𝑁</ci></apply><apply id="A2.E2.m1.5.5.2.2.cmml" xref="A2.E2.m1.5.5.2.2"><apply id="A2.E2.m1.5.5.2.2.3.cmml" xref="A2.E2.m1.5.5.2.2.3"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.3.1.cmml" xref="A2.E2.m1.5.5.2.2.3">superscript</csymbol><apply id="A2.E2.m1.5.5.2.2.3.2.cmml" xref="A2.E2.m1.5.5.2.2.3"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.3.2.1.cmml" xref="A2.E2.m1.5.5.2.2.3">subscript</csymbol><sum id="A2.E2.m1.5.5.2.2.3.2.2.cmml" xref="A2.E2.m1.5.5.2.2.3.2.2"></sum><ci id="A2.E2.m1.5.5.2.2.3.2.3.cmml" xref="A2.E2.m1.5.5.2.2.3.2.3">𝑖</ci></apply><ci id="A2.E2.m1.5.5.2.2.3.3.cmml" xref="A2.E2.m1.5.5.2.2.3.3">𝑘</ci></apply><apply id="A2.E2.m1.5.5.2.2.2.cmml" xref="A2.E2.m1.5.5.2.2.2"><apply id="A2.E2.m1.5.5.2.2.2.3.cmml" xref="A2.E2.m1.5.5.2.2.2.3"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.2.3.1.cmml" xref="A2.E2.m1.5.5.2.2.2.3">subscript</csymbol><sum id="A2.E2.m1.5.5.2.2.2.3.2.cmml" xref="A2.E2.m1.5.5.2.2.2.3.2"></sum><apply id="A2.E2.m1.5.5.2.2.2.3.3.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3"><neq id="A2.E2.m1.5.5.2.2.2.3.3.1.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.1"></neq><apply id="A2.E2.m1.5.5.2.2.2.3.3.2.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2"><intersect id="A2.E2.m1.5.5.2.2.2.3.3.2.1.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.1"></intersect><apply id="A2.E2.m1.5.5.2.2.2.3.3.2.2.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.2.3.3.2.2.1.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2">subscript</csymbol><ci id="A2.E2.m1.5.5.2.2.2.3.3.2.2.2.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2.2">𝑆</ci><ci id="A2.E2.m1.5.5.2.2.2.3.3.2.2.3.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.2.3">𝑗</ci></apply><apply id="A2.E2.m1.5.5.2.2.2.3.3.2.3.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.2.3.3.2.3.1.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3">subscript</csymbol><ci id="A2.E2.m1.5.5.2.2.2.3.3.2.3.2.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3.2">𝐺</ci><ci id="A2.E2.m1.5.5.2.2.2.3.3.2.3.3.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.2.3.3">𝑖</ci></apply></apply><emptyset id="A2.E2.m1.5.5.2.2.2.3.3.3.cmml" xref="A2.E2.m1.5.5.2.2.2.3.3.3"></emptyset></apply></apply><apply id="A2.E2.m1.5.5.2.2.2.2.3.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2"><min id="A2.E2.m1.3.3.cmml" xref="A2.E2.m1.3.3"></min><apply id="A2.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1"><abs id="A2.E2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.2"></abs><apply id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1"><intersect id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1"></intersect><apply id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2">𝑆</ci><ci id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2">𝐺</ci><ci id="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="A2.E2.m1.5.5.2.2.2.2.2.2.2.2.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1"><abs id="A2.E2.m1.5.5.2.2.2.2.2.2.2.2.1.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.2"></abs><apply id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1"><minus id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.1.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.1"></minus><apply id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.1.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.2.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.2">𝑆</ci><ci id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.3.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.2.3">𝑗</ci></apply><apply id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.1.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.2.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.2">𝐺</ci><ci id="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.3.cmml" xref="A2.E2.m1.5.5.2.2.2.2.2.2.2.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E2.m1.5c">\text{UE}(S,G)=\frac{1}{N}\sum_{i}^{k}\sum_{S_{j}\cap G_{i}\neq\varnothing}%
\min\{|S_{j}\cap G_{i}|,|S_{j}-G_{i}|\}</annotation><annotation encoding="application/x-llamapun" id="A2.E2.m1.5d">UE ( italic_S , italic_G ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∩ italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≠ ∅ end_POSTSUBSCRIPT roman_min { | italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∩ italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | , | italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1">Shape-based evaluation metrics assess whether the superpixels have compact shapes with smooth contours and are arranged regularly — <span class="ltx_text ltx_font_italic" id="A2.p4.1.1">i.e.</span>, in a grid. Although these properties have an inverse relationship to the delineation, an improved boundary recall does not necessarily imply better segmentation <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib99" title="">2012</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib100" title="">2014</a>)</cite>. Due to this, the quality of the superpixel methods has been evaluated in previous benchmarks according to the trade-off between its shape quality and delineation <cite class="ltx_cite ltx_citemacro_citep">(Stutz, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib109" title="">2015</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib123" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A2.p5">
<p class="ltx_p" id="A2.p5.2">The <span class="ltx_text ltx_font_italic" id="A2.p5.2.1">Compactness index</span> (CO) <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib99" title="">2012</a>)</cite> measure uses the isoperimetric quotient to measure the similarity between the shape of a superpixel and a circle, which constitutes the most compact geometric shape. The CO measure is presented in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E3" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>, in which <math alttext="A(S_{j})" class="ltx_Math" display="inline" id="A2.p5.1.m1.1"><semantics id="A2.p5.1.m1.1a"><mrow id="A2.p5.1.m1.1.1" xref="A2.p5.1.m1.1.1.cmml"><mi id="A2.p5.1.m1.1.1.3" xref="A2.p5.1.m1.1.1.3.cmml">A</mi><mo id="A2.p5.1.m1.1.1.2" xref="A2.p5.1.m1.1.1.2.cmml">⁢</mo><mrow id="A2.p5.1.m1.1.1.1.1" xref="A2.p5.1.m1.1.1.1.1.1.cmml"><mo id="A2.p5.1.m1.1.1.1.1.2" stretchy="false" xref="A2.p5.1.m1.1.1.1.1.1.cmml">(</mo><msub id="A2.p5.1.m1.1.1.1.1.1" xref="A2.p5.1.m1.1.1.1.1.1.cmml"><mi id="A2.p5.1.m1.1.1.1.1.1.2" xref="A2.p5.1.m1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.p5.1.m1.1.1.1.1.1.3" xref="A2.p5.1.m1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="A2.p5.1.m1.1.1.1.1.3" stretchy="false" xref="A2.p5.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p5.1.m1.1b"><apply id="A2.p5.1.m1.1.1.cmml" xref="A2.p5.1.m1.1.1"><times id="A2.p5.1.m1.1.1.2.cmml" xref="A2.p5.1.m1.1.1.2"></times><ci id="A2.p5.1.m1.1.1.3.cmml" xref="A2.p5.1.m1.1.1.3">𝐴</ci><apply id="A2.p5.1.m1.1.1.1.1.1.cmml" xref="A2.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A2.p5.1.m1.1.1.1.1.1.1.cmml" xref="A2.p5.1.m1.1.1.1.1">subscript</csymbol><ci id="A2.p5.1.m1.1.1.1.1.1.2.cmml" xref="A2.p5.1.m1.1.1.1.1.1.2">𝑆</ci><ci id="A2.p5.1.m1.1.1.1.1.1.3.cmml" xref="A2.p5.1.m1.1.1.1.1.1.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.1.m1.1c">A(S_{j})</annotation><annotation encoding="application/x-llamapun" id="A2.p5.1.m1.1d">italic_A ( italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math> and <math alttext="P(S_{j})" class="ltx_Math" display="inline" id="A2.p5.2.m2.1"><semantics id="A2.p5.2.m2.1a"><mrow id="A2.p5.2.m2.1.1" xref="A2.p5.2.m2.1.1.cmml"><mi id="A2.p5.2.m2.1.1.3" xref="A2.p5.2.m2.1.1.3.cmml">P</mi><mo id="A2.p5.2.m2.1.1.2" xref="A2.p5.2.m2.1.1.2.cmml">⁢</mo><mrow id="A2.p5.2.m2.1.1.1.1" xref="A2.p5.2.m2.1.1.1.1.1.cmml"><mo id="A2.p5.2.m2.1.1.1.1.2" stretchy="false" xref="A2.p5.2.m2.1.1.1.1.1.cmml">(</mo><msub id="A2.p5.2.m2.1.1.1.1.1" xref="A2.p5.2.m2.1.1.1.1.1.cmml"><mi id="A2.p5.2.m2.1.1.1.1.1.2" xref="A2.p5.2.m2.1.1.1.1.1.2.cmml">S</mi><mi id="A2.p5.2.m2.1.1.1.1.1.3" xref="A2.p5.2.m2.1.1.1.1.1.3.cmml">j</mi></msub><mo id="A2.p5.2.m2.1.1.1.1.3" stretchy="false" xref="A2.p5.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p5.2.m2.1b"><apply id="A2.p5.2.m2.1.1.cmml" xref="A2.p5.2.m2.1.1"><times id="A2.p5.2.m2.1.1.2.cmml" xref="A2.p5.2.m2.1.1.2"></times><ci id="A2.p5.2.m2.1.1.3.cmml" xref="A2.p5.2.m2.1.1.3">𝑃</ci><apply id="A2.p5.2.m2.1.1.1.1.1.cmml" xref="A2.p5.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="A2.p5.2.m2.1.1.1.1.1.1.cmml" xref="A2.p5.2.m2.1.1.1.1">subscript</csymbol><ci id="A2.p5.2.m2.1.1.1.1.1.2.cmml" xref="A2.p5.2.m2.1.1.1.1.1.2">𝑆</ci><ci id="A2.p5.2.m2.1.1.1.1.1.3.cmml" xref="A2.p5.2.m2.1.1.1.1.1.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.2.m2.1c">P(S_{j})</annotation><annotation encoding="application/x-llamapun" id="A2.p5.2.m2.1d">italic_P ( italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math> are the superpixel area and perimeter, respectively.</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{CO}(S)=\frac{1}{N}\sum_{S_{j}}|S_{j}|\frac{4\pi A(S_{j})}{P(S_{j})}" class="ltx_Math" display="block" id="A2.E3.m1.4"><semantics id="A2.E3.m1.4a"><mrow id="A2.E3.m1.4.4" xref="A2.E3.m1.4.4.cmml"><mrow id="A2.E3.m1.4.4.3" xref="A2.E3.m1.4.4.3.cmml"><mtext id="A2.E3.m1.4.4.3.2" xref="A2.E3.m1.4.4.3.2a.cmml">CO</mtext><mo id="A2.E3.m1.4.4.3.1" xref="A2.E3.m1.4.4.3.1.cmml">⁢</mo><mrow id="A2.E3.m1.4.4.3.3.2" xref="A2.E3.m1.4.4.3.cmml"><mo id="A2.E3.m1.4.4.3.3.2.1" stretchy="false" xref="A2.E3.m1.4.4.3.cmml">(</mo><mi id="A2.E3.m1.3.3" xref="A2.E3.m1.3.3.cmml">S</mi><mo id="A2.E3.m1.4.4.3.3.2.2" stretchy="false" xref="A2.E3.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="A2.E3.m1.4.4.2" xref="A2.E3.m1.4.4.2.cmml">=</mo><mrow id="A2.E3.m1.4.4.1" xref="A2.E3.m1.4.4.1.cmml"><mfrac id="A2.E3.m1.4.4.1.3" xref="A2.E3.m1.4.4.1.3.cmml"><mn id="A2.E3.m1.4.4.1.3.2" xref="A2.E3.m1.4.4.1.3.2.cmml">1</mn><mi id="A2.E3.m1.4.4.1.3.3" xref="A2.E3.m1.4.4.1.3.3.cmml">N</mi></mfrac><mo id="A2.E3.m1.4.4.1.2" xref="A2.E3.m1.4.4.1.2.cmml">⁢</mo><mrow id="A2.E3.m1.4.4.1.1" xref="A2.E3.m1.4.4.1.1.cmml"><munder id="A2.E3.m1.4.4.1.1.2" xref="A2.E3.m1.4.4.1.1.2.cmml"><mo id="A2.E3.m1.4.4.1.1.2.2" movablelimits="false" rspace="0em" xref="A2.E3.m1.4.4.1.1.2.2.cmml">∑</mo><msub id="A2.E3.m1.4.4.1.1.2.3" xref="A2.E3.m1.4.4.1.1.2.3.cmml"><mi id="A2.E3.m1.4.4.1.1.2.3.2" xref="A2.E3.m1.4.4.1.1.2.3.2.cmml">S</mi><mi id="A2.E3.m1.4.4.1.1.2.3.3" xref="A2.E3.m1.4.4.1.1.2.3.3.cmml">j</mi></msub></munder><mrow id="A2.E3.m1.4.4.1.1.1" xref="A2.E3.m1.4.4.1.1.1.cmml"><mrow id="A2.E3.m1.4.4.1.1.1.1.1" xref="A2.E3.m1.4.4.1.1.1.1.2.cmml"><mo id="A2.E3.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="A2.E3.m1.4.4.1.1.1.1.2.1.cmml">|</mo><msub id="A2.E3.m1.4.4.1.1.1.1.1.1" xref="A2.E3.m1.4.4.1.1.1.1.1.1.cmml"><mi id="A2.E3.m1.4.4.1.1.1.1.1.1.2" xref="A2.E3.m1.4.4.1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.E3.m1.4.4.1.1.1.1.1.1.3" xref="A2.E3.m1.4.4.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="A2.E3.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="A2.E3.m1.4.4.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="A2.E3.m1.4.4.1.1.1.2" xref="A2.E3.m1.4.4.1.1.1.2.cmml">⁢</mo><mfrac id="A2.E3.m1.2.2" xref="A2.E3.m1.2.2.cmml"><mrow id="A2.E3.m1.1.1.1" xref="A2.E3.m1.1.1.1.cmml"><mn id="A2.E3.m1.1.1.1.3" xref="A2.E3.m1.1.1.1.3.cmml">4</mn><mo id="A2.E3.m1.1.1.1.2" xref="A2.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="A2.E3.m1.1.1.1.4" xref="A2.E3.m1.1.1.1.4.cmml">π</mi><mo id="A2.E3.m1.1.1.1.2a" xref="A2.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="A2.E3.m1.1.1.1.5" xref="A2.E3.m1.1.1.1.5.cmml">A</mi><mo id="A2.E3.m1.1.1.1.2b" xref="A2.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="A2.E3.m1.1.1.1.1.1" xref="A2.E3.m1.1.1.1.1.1.1.cmml"><mo id="A2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="A2.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="A2.E3.m1.1.1.1.1.1.1" xref="A2.E3.m1.1.1.1.1.1.1.cmml"><mi id="A2.E3.m1.1.1.1.1.1.1.2" xref="A2.E3.m1.1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.E3.m1.1.1.1.1.1.1.3" xref="A2.E3.m1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="A2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="A2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="A2.E3.m1.2.2.2" xref="A2.E3.m1.2.2.2.cmml"><mi id="A2.E3.m1.2.2.2.3" xref="A2.E3.m1.2.2.2.3.cmml">P</mi><mo id="A2.E3.m1.2.2.2.2" xref="A2.E3.m1.2.2.2.2.cmml">⁢</mo><mrow id="A2.E3.m1.2.2.2.1.1" xref="A2.E3.m1.2.2.2.1.1.1.cmml"><mo id="A2.E3.m1.2.2.2.1.1.2" stretchy="false" xref="A2.E3.m1.2.2.2.1.1.1.cmml">(</mo><msub id="A2.E3.m1.2.2.2.1.1.1" xref="A2.E3.m1.2.2.2.1.1.1.cmml"><mi id="A2.E3.m1.2.2.2.1.1.1.2" xref="A2.E3.m1.2.2.2.1.1.1.2.cmml">S</mi><mi id="A2.E3.m1.2.2.2.1.1.1.3" xref="A2.E3.m1.2.2.2.1.1.1.3.cmml">j</mi></msub><mo id="A2.E3.m1.2.2.2.1.1.3" stretchy="false" xref="A2.E3.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.E3.m1.4b"><apply id="A2.E3.m1.4.4.cmml" xref="A2.E3.m1.4.4"><eq id="A2.E3.m1.4.4.2.cmml" xref="A2.E3.m1.4.4.2"></eq><apply id="A2.E3.m1.4.4.3.cmml" xref="A2.E3.m1.4.4.3"><times id="A2.E3.m1.4.4.3.1.cmml" xref="A2.E3.m1.4.4.3.1"></times><ci id="A2.E3.m1.4.4.3.2a.cmml" xref="A2.E3.m1.4.4.3.2"><mtext id="A2.E3.m1.4.4.3.2.cmml" xref="A2.E3.m1.4.4.3.2">CO</mtext></ci><ci id="A2.E3.m1.3.3.cmml" xref="A2.E3.m1.3.3">𝑆</ci></apply><apply id="A2.E3.m1.4.4.1.cmml" xref="A2.E3.m1.4.4.1"><times id="A2.E3.m1.4.4.1.2.cmml" xref="A2.E3.m1.4.4.1.2"></times><apply id="A2.E3.m1.4.4.1.3.cmml" xref="A2.E3.m1.4.4.1.3"><divide id="A2.E3.m1.4.4.1.3.1.cmml" xref="A2.E3.m1.4.4.1.3"></divide><cn id="A2.E3.m1.4.4.1.3.2.cmml" type="integer" xref="A2.E3.m1.4.4.1.3.2">1</cn><ci id="A2.E3.m1.4.4.1.3.3.cmml" xref="A2.E3.m1.4.4.1.3.3">𝑁</ci></apply><apply id="A2.E3.m1.4.4.1.1.cmml" xref="A2.E3.m1.4.4.1.1"><apply id="A2.E3.m1.4.4.1.1.2.cmml" xref="A2.E3.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="A2.E3.m1.4.4.1.1.2.1.cmml" xref="A2.E3.m1.4.4.1.1.2">subscript</csymbol><sum id="A2.E3.m1.4.4.1.1.2.2.cmml" xref="A2.E3.m1.4.4.1.1.2.2"></sum><apply id="A2.E3.m1.4.4.1.1.2.3.cmml" xref="A2.E3.m1.4.4.1.1.2.3"><csymbol cd="ambiguous" id="A2.E3.m1.4.4.1.1.2.3.1.cmml" xref="A2.E3.m1.4.4.1.1.2.3">subscript</csymbol><ci id="A2.E3.m1.4.4.1.1.2.3.2.cmml" xref="A2.E3.m1.4.4.1.1.2.3.2">𝑆</ci><ci id="A2.E3.m1.4.4.1.1.2.3.3.cmml" xref="A2.E3.m1.4.4.1.1.2.3.3">𝑗</ci></apply></apply><apply id="A2.E3.m1.4.4.1.1.1.cmml" xref="A2.E3.m1.4.4.1.1.1"><times id="A2.E3.m1.4.4.1.1.1.2.cmml" xref="A2.E3.m1.4.4.1.1.1.2"></times><apply id="A2.E3.m1.4.4.1.1.1.1.2.cmml" xref="A2.E3.m1.4.4.1.1.1.1.1"><abs id="A2.E3.m1.4.4.1.1.1.1.2.1.cmml" xref="A2.E3.m1.4.4.1.1.1.1.1.2"></abs><apply id="A2.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="A2.E3.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E3.m1.4.4.1.1.1.1.1.1.1.cmml" xref="A2.E3.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="A2.E3.m1.4.4.1.1.1.1.1.1.2">𝑆</ci><ci id="A2.E3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="A2.E3.m1.4.4.1.1.1.1.1.1.3">𝑗</ci></apply></apply><apply id="A2.E3.m1.2.2.cmml" xref="A2.E3.m1.2.2"><divide id="A2.E3.m1.2.2.3.cmml" xref="A2.E3.m1.2.2"></divide><apply id="A2.E3.m1.1.1.1.cmml" xref="A2.E3.m1.1.1.1"><times id="A2.E3.m1.1.1.1.2.cmml" xref="A2.E3.m1.1.1.1.2"></times><cn id="A2.E3.m1.1.1.1.3.cmml" type="integer" xref="A2.E3.m1.1.1.1.3">4</cn><ci id="A2.E3.m1.1.1.1.4.cmml" xref="A2.E3.m1.1.1.1.4">𝜋</ci><ci id="A2.E3.m1.1.1.1.5.cmml" xref="A2.E3.m1.1.1.1.5">𝐴</ci><apply id="A2.E3.m1.1.1.1.1.1.1.cmml" xref="A2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E3.m1.1.1.1.1.1.1.1.cmml" xref="A2.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="A2.E3.m1.1.1.1.1.1.1.2.cmml" xref="A2.E3.m1.1.1.1.1.1.1.2">𝑆</ci><ci id="A2.E3.m1.1.1.1.1.1.1.3.cmml" xref="A2.E3.m1.1.1.1.1.1.1.3">𝑗</ci></apply></apply><apply id="A2.E3.m1.2.2.2.cmml" xref="A2.E3.m1.2.2.2"><times id="A2.E3.m1.2.2.2.2.cmml" xref="A2.E3.m1.2.2.2.2"></times><ci id="A2.E3.m1.2.2.2.3.cmml" xref="A2.E3.m1.2.2.2.3">𝑃</ci><apply id="A2.E3.m1.2.2.2.1.1.1.cmml" xref="A2.E3.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="A2.E3.m1.2.2.2.1.1.1.1.cmml" xref="A2.E3.m1.2.2.2.1.1">subscript</csymbol><ci id="A2.E3.m1.2.2.2.1.1.1.2.cmml" xref="A2.E3.m1.2.2.2.1.1.1.2">𝑆</ci><ci id="A2.E3.m1.2.2.2.1.1.1.3.cmml" xref="A2.E3.m1.2.2.2.1.1.1.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E3.m1.4c">\text{CO}(S)=\frac{1}{N}\sum_{S_{j}}|S_{j}|\frac{4\pi A(S_{j})}{P(S_{j})}</annotation><annotation encoding="application/x-llamapun" id="A2.E3.m1.4d">CO ( italic_S ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT | italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | divide start_ARG 4 italic_π italic_A ( italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG italic_P ( italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.p6">
<p class="ltx_p" id="A2.p6.7">Although the desired properties of superpixels are not a consensus in the literature, the inner color similarity usually underlies their methods. The <span class="ltx_text ltx_font_italic" id="A2.p6.7.1">Explained Variation</span> <cite class="ltx_cite ltx_citemacro_citep">(Moore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib87" title="">2008</a>)</cite> defines homogeneity by comparing the variance of the superpixels’ mean color <math alttext="\mu(S_{i})" class="ltx_Math" display="inline" id="A2.p6.1.m1.1"><semantics id="A2.p6.1.m1.1a"><mrow id="A2.p6.1.m1.1.1" xref="A2.p6.1.m1.1.1.cmml"><mi id="A2.p6.1.m1.1.1.3" xref="A2.p6.1.m1.1.1.3.cmml">μ</mi><mo id="A2.p6.1.m1.1.1.2" xref="A2.p6.1.m1.1.1.2.cmml">⁢</mo><mrow id="A2.p6.1.m1.1.1.1.1" xref="A2.p6.1.m1.1.1.1.1.1.cmml"><mo id="A2.p6.1.m1.1.1.1.1.2" stretchy="false" xref="A2.p6.1.m1.1.1.1.1.1.cmml">(</mo><msub id="A2.p6.1.m1.1.1.1.1.1" xref="A2.p6.1.m1.1.1.1.1.1.cmml"><mi id="A2.p6.1.m1.1.1.1.1.1.2" xref="A2.p6.1.m1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.p6.1.m1.1.1.1.1.1.3" xref="A2.p6.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="A2.p6.1.m1.1.1.1.1.3" stretchy="false" xref="A2.p6.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.1.m1.1b"><apply id="A2.p6.1.m1.1.1.cmml" xref="A2.p6.1.m1.1.1"><times id="A2.p6.1.m1.1.1.2.cmml" xref="A2.p6.1.m1.1.1.2"></times><ci id="A2.p6.1.m1.1.1.3.cmml" xref="A2.p6.1.m1.1.1.3">𝜇</ci><apply id="A2.p6.1.m1.1.1.1.1.1.cmml" xref="A2.p6.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A2.p6.1.m1.1.1.1.1.1.1.cmml" xref="A2.p6.1.m1.1.1.1.1">subscript</csymbol><ci id="A2.p6.1.m1.1.1.1.1.1.2.cmml" xref="A2.p6.1.m1.1.1.1.1.1.2">𝑆</ci><ci id="A2.p6.1.m1.1.1.1.1.1.3.cmml" xref="A2.p6.1.m1.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.1.m1.1c">\mu(S_{i})</annotation><annotation encoding="application/x-llamapun" id="A2.p6.1.m1.1d">italic_μ ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> and the variance of the pixels’ color <math alttext="I(p)" class="ltx_Math" display="inline" id="A2.p6.2.m2.1"><semantics id="A2.p6.2.m2.1a"><mrow id="A2.p6.2.m2.1.2" xref="A2.p6.2.m2.1.2.cmml"><mi id="A2.p6.2.m2.1.2.2" xref="A2.p6.2.m2.1.2.2.cmml">I</mi><mo id="A2.p6.2.m2.1.2.1" xref="A2.p6.2.m2.1.2.1.cmml">⁢</mo><mrow id="A2.p6.2.m2.1.2.3.2" xref="A2.p6.2.m2.1.2.cmml"><mo id="A2.p6.2.m2.1.2.3.2.1" stretchy="false" xref="A2.p6.2.m2.1.2.cmml">(</mo><mi id="A2.p6.2.m2.1.1" xref="A2.p6.2.m2.1.1.cmml">p</mi><mo id="A2.p6.2.m2.1.2.3.2.2" stretchy="false" xref="A2.p6.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.2.m2.1b"><apply id="A2.p6.2.m2.1.2.cmml" xref="A2.p6.2.m2.1.2"><times id="A2.p6.2.m2.1.2.1.cmml" xref="A2.p6.2.m2.1.2.1"></times><ci id="A2.p6.2.m2.1.2.2.cmml" xref="A2.p6.2.m2.1.2.2">𝐼</ci><ci id="A2.p6.2.m2.1.1.cmml" xref="A2.p6.2.m2.1.1">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.2.m2.1c">I(p)</annotation><annotation encoding="application/x-llamapun" id="A2.p6.2.m2.1d">italic_I ( italic_p )</annotation></semantics></math> towards the image’s mean color <math alttext="\mu(\mathbf{I})" class="ltx_Math" display="inline" id="A2.p6.3.m3.1"><semantics id="A2.p6.3.m3.1a"><mrow id="A2.p6.3.m3.1.2" xref="A2.p6.3.m3.1.2.cmml"><mi id="A2.p6.3.m3.1.2.2" xref="A2.p6.3.m3.1.2.2.cmml">μ</mi><mo id="A2.p6.3.m3.1.2.1" xref="A2.p6.3.m3.1.2.1.cmml">⁢</mo><mrow id="A2.p6.3.m3.1.2.3.2" xref="A2.p6.3.m3.1.2.cmml"><mo id="A2.p6.3.m3.1.2.3.2.1" stretchy="false" xref="A2.p6.3.m3.1.2.cmml">(</mo><mi id="A2.p6.3.m3.1.1" xref="A2.p6.3.m3.1.1.cmml">𝐈</mi><mo id="A2.p6.3.m3.1.2.3.2.2" stretchy="false" xref="A2.p6.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.3.m3.1b"><apply id="A2.p6.3.m3.1.2.cmml" xref="A2.p6.3.m3.1.2"><times id="A2.p6.3.m3.1.2.1.cmml" xref="A2.p6.3.m3.1.2.1"></times><ci id="A2.p6.3.m3.1.2.2.cmml" xref="A2.p6.3.m3.1.2.2">𝜇</ci><ci id="A2.p6.3.m3.1.1.cmml" xref="A2.p6.3.m3.1.1">𝐈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.3.m3.1c">\mu(\mathbf{I})</annotation><annotation encoding="application/x-llamapun" id="A2.p6.3.m3.1d">italic_μ ( bold_I )</annotation></semantics></math>, resulting in a normalized measure (Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E4" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>). This measure is maximum when <math alttext="\left|S\right|=\left|\mathbf{I}\right|" class="ltx_Math" display="inline" id="A2.p6.4.m4.2"><semantics id="A2.p6.4.m4.2a"><mrow id="A2.p6.4.m4.2.3" xref="A2.p6.4.m4.2.3.cmml"><mrow id="A2.p6.4.m4.2.3.2.2" xref="A2.p6.4.m4.2.3.2.1.cmml"><mo id="A2.p6.4.m4.2.3.2.2.1" xref="A2.p6.4.m4.2.3.2.1.1.cmml">|</mo><mi id="A2.p6.4.m4.1.1" xref="A2.p6.4.m4.1.1.cmml">S</mi><mo id="A2.p6.4.m4.2.3.2.2.2" xref="A2.p6.4.m4.2.3.2.1.1.cmml">|</mo></mrow><mo id="A2.p6.4.m4.2.3.1" xref="A2.p6.4.m4.2.3.1.cmml">=</mo><mrow id="A2.p6.4.m4.2.3.3.2" xref="A2.p6.4.m4.2.3.3.1.cmml"><mo id="A2.p6.4.m4.2.3.3.2.1" xref="A2.p6.4.m4.2.3.3.1.1.cmml">|</mo><mi id="A2.p6.4.m4.2.2" xref="A2.p6.4.m4.2.2.cmml">𝐈</mi><mo id="A2.p6.4.m4.2.3.3.2.2" xref="A2.p6.4.m4.2.3.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.4.m4.2b"><apply id="A2.p6.4.m4.2.3.cmml" xref="A2.p6.4.m4.2.3"><eq id="A2.p6.4.m4.2.3.1.cmml" xref="A2.p6.4.m4.2.3.1"></eq><apply id="A2.p6.4.m4.2.3.2.1.cmml" xref="A2.p6.4.m4.2.3.2.2"><abs id="A2.p6.4.m4.2.3.2.1.1.cmml" xref="A2.p6.4.m4.2.3.2.2.1"></abs><ci id="A2.p6.4.m4.1.1.cmml" xref="A2.p6.4.m4.1.1">𝑆</ci></apply><apply id="A2.p6.4.m4.2.3.3.1.cmml" xref="A2.p6.4.m4.2.3.3.2"><abs id="A2.p6.4.m4.2.3.3.1.1.cmml" xref="A2.p6.4.m4.2.3.3.2.1"></abs><ci id="A2.p6.4.m4.2.2.cmml" xref="A2.p6.4.m4.2.2">𝐈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.4.m4.2c">\left|S\right|=\left|\mathbf{I}\right|</annotation><annotation encoding="application/x-llamapun" id="A2.p6.4.m4.2d">| italic_S | = | bold_I |</annotation></semantics></math> or when <math alttext="I(p)=\mu(S_{i})" class="ltx_Math" display="inline" id="A2.p6.5.m5.2"><semantics id="A2.p6.5.m5.2a"><mrow id="A2.p6.5.m5.2.2" xref="A2.p6.5.m5.2.2.cmml"><mrow id="A2.p6.5.m5.2.2.3" xref="A2.p6.5.m5.2.2.3.cmml"><mi id="A2.p6.5.m5.2.2.3.2" xref="A2.p6.5.m5.2.2.3.2.cmml">I</mi><mo id="A2.p6.5.m5.2.2.3.1" xref="A2.p6.5.m5.2.2.3.1.cmml">⁢</mo><mrow id="A2.p6.5.m5.2.2.3.3.2" xref="A2.p6.5.m5.2.2.3.cmml"><mo id="A2.p6.5.m5.2.2.3.3.2.1" stretchy="false" xref="A2.p6.5.m5.2.2.3.cmml">(</mo><mi id="A2.p6.5.m5.1.1" xref="A2.p6.5.m5.1.1.cmml">p</mi><mo id="A2.p6.5.m5.2.2.3.3.2.2" stretchy="false" xref="A2.p6.5.m5.2.2.3.cmml">)</mo></mrow></mrow><mo id="A2.p6.5.m5.2.2.2" xref="A2.p6.5.m5.2.2.2.cmml">=</mo><mrow id="A2.p6.5.m5.2.2.1" xref="A2.p6.5.m5.2.2.1.cmml"><mi id="A2.p6.5.m5.2.2.1.3" xref="A2.p6.5.m5.2.2.1.3.cmml">μ</mi><mo id="A2.p6.5.m5.2.2.1.2" xref="A2.p6.5.m5.2.2.1.2.cmml">⁢</mo><mrow id="A2.p6.5.m5.2.2.1.1.1" xref="A2.p6.5.m5.2.2.1.1.1.1.cmml"><mo id="A2.p6.5.m5.2.2.1.1.1.2" stretchy="false" xref="A2.p6.5.m5.2.2.1.1.1.1.cmml">(</mo><msub id="A2.p6.5.m5.2.2.1.1.1.1" xref="A2.p6.5.m5.2.2.1.1.1.1.cmml"><mi id="A2.p6.5.m5.2.2.1.1.1.1.2" xref="A2.p6.5.m5.2.2.1.1.1.1.2.cmml">S</mi><mi id="A2.p6.5.m5.2.2.1.1.1.1.3" xref="A2.p6.5.m5.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="A2.p6.5.m5.2.2.1.1.1.3" stretchy="false" xref="A2.p6.5.m5.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.5.m5.2b"><apply id="A2.p6.5.m5.2.2.cmml" xref="A2.p6.5.m5.2.2"><eq id="A2.p6.5.m5.2.2.2.cmml" xref="A2.p6.5.m5.2.2.2"></eq><apply id="A2.p6.5.m5.2.2.3.cmml" xref="A2.p6.5.m5.2.2.3"><times id="A2.p6.5.m5.2.2.3.1.cmml" xref="A2.p6.5.m5.2.2.3.1"></times><ci id="A2.p6.5.m5.2.2.3.2.cmml" xref="A2.p6.5.m5.2.2.3.2">𝐼</ci><ci id="A2.p6.5.m5.1.1.cmml" xref="A2.p6.5.m5.1.1">𝑝</ci></apply><apply id="A2.p6.5.m5.2.2.1.cmml" xref="A2.p6.5.m5.2.2.1"><times id="A2.p6.5.m5.2.2.1.2.cmml" xref="A2.p6.5.m5.2.2.1.2"></times><ci id="A2.p6.5.m5.2.2.1.3.cmml" xref="A2.p6.5.m5.2.2.1.3">𝜇</ci><apply id="A2.p6.5.m5.2.2.1.1.1.1.cmml" xref="A2.p6.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p6.5.m5.2.2.1.1.1.1.1.cmml" xref="A2.p6.5.m5.2.2.1.1.1">subscript</csymbol><ci id="A2.p6.5.m5.2.2.1.1.1.1.2.cmml" xref="A2.p6.5.m5.2.2.1.1.1.1.2">𝑆</ci><ci id="A2.p6.5.m5.2.2.1.1.1.1.3.cmml" xref="A2.p6.5.m5.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.5.m5.2c">I(p)=\mu(S_{i})</annotation><annotation encoding="application/x-llamapun" id="A2.p6.5.m5.2d">italic_I ( italic_p ) = italic_μ ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> for all <math alttext="p\in S_{i}" class="ltx_Math" display="inline" id="A2.p6.6.m6.1"><semantics id="A2.p6.6.m6.1a"><mrow id="A2.p6.6.m6.1.1" xref="A2.p6.6.m6.1.1.cmml"><mi id="A2.p6.6.m6.1.1.2" xref="A2.p6.6.m6.1.1.2.cmml">p</mi><mo id="A2.p6.6.m6.1.1.1" xref="A2.p6.6.m6.1.1.1.cmml">∈</mo><msub id="A2.p6.6.m6.1.1.3" xref="A2.p6.6.m6.1.1.3.cmml"><mi id="A2.p6.6.m6.1.1.3.2" xref="A2.p6.6.m6.1.1.3.2.cmml">S</mi><mi id="A2.p6.6.m6.1.1.3.3" xref="A2.p6.6.m6.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.6.m6.1b"><apply id="A2.p6.6.m6.1.1.cmml" xref="A2.p6.6.m6.1.1"><in id="A2.p6.6.m6.1.1.1.cmml" xref="A2.p6.6.m6.1.1.1"></in><ci id="A2.p6.6.m6.1.1.2.cmml" xref="A2.p6.6.m6.1.1.2">𝑝</ci><apply id="A2.p6.6.m6.1.1.3.cmml" xref="A2.p6.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.p6.6.m6.1.1.3.1.cmml" xref="A2.p6.6.m6.1.1.3">subscript</csymbol><ci id="A2.p6.6.m6.1.1.3.2.cmml" xref="A2.p6.6.m6.1.1.3.2">𝑆</ci><ci id="A2.p6.6.m6.1.1.3.3.cmml" xref="A2.p6.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.6.m6.1c">p\in S_{i}</annotation><annotation encoding="application/x-llamapun" id="A2.p6.6.m6.1d">italic_p ∈ italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and for every <math alttext="S_{i}\in S" class="ltx_Math" display="inline" id="A2.p6.7.m7.1"><semantics id="A2.p6.7.m7.1a"><mrow id="A2.p6.7.m7.1.1" xref="A2.p6.7.m7.1.1.cmml"><msub id="A2.p6.7.m7.1.1.2" xref="A2.p6.7.m7.1.1.2.cmml"><mi id="A2.p6.7.m7.1.1.2.2" xref="A2.p6.7.m7.1.1.2.2.cmml">S</mi><mi id="A2.p6.7.m7.1.1.2.3" xref="A2.p6.7.m7.1.1.2.3.cmml">i</mi></msub><mo id="A2.p6.7.m7.1.1.1" xref="A2.p6.7.m7.1.1.1.cmml">∈</mo><mi id="A2.p6.7.m7.1.1.3" xref="A2.p6.7.m7.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.7.m7.1b"><apply id="A2.p6.7.m7.1.1.cmml" xref="A2.p6.7.m7.1.1"><in id="A2.p6.7.m7.1.1.1.cmml" xref="A2.p6.7.m7.1.1.1"></in><apply id="A2.p6.7.m7.1.1.2.cmml" xref="A2.p6.7.m7.1.1.2"><csymbol cd="ambiguous" id="A2.p6.7.m7.1.1.2.1.cmml" xref="A2.p6.7.m7.1.1.2">subscript</csymbol><ci id="A2.p6.7.m7.1.1.2.2.cmml" xref="A2.p6.7.m7.1.1.2.2">𝑆</ci><ci id="A2.p6.7.m7.1.1.2.3.cmml" xref="A2.p6.7.m7.1.1.2.3">𝑖</ci></apply><ci id="A2.p6.7.m7.1.1.3.cmml" xref="A2.p6.7.m7.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.7.m7.1c">S_{i}\in S</annotation><annotation encoding="application/x-llamapun" id="A2.p6.7.m7.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S</annotation></semantics></math>. However, EV considers the superpixels’ mean color, which is insufficient for describing perceptually homogeneous textures <cite class="ltx_cite ltx_citemacro_citep">(Moore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib87" title="">2008</a>)</cite>.</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="EV(S)=\frac{\sum_{S_{i}\in S}\left|S_{i}\right|\left\|\mu(S_{i})-\mu(\mathbf{I%
})\right\|_{1}^{2}}{\sum_{p\in\mathbf{I}}\left\|I(p)-\mu(\mathbf{I})\right\|_{%
1}^{2}}" class="ltx_Math" display="block" id="A2.E4.m1.7"><semantics id="A2.E4.m1.7a"><mrow id="A2.E4.m1.7.8" xref="A2.E4.m1.7.8.cmml"><mrow id="A2.E4.m1.7.8.2" xref="A2.E4.m1.7.8.2.cmml"><mi id="A2.E4.m1.7.8.2.2" xref="A2.E4.m1.7.8.2.2.cmml">E</mi><mo id="A2.E4.m1.7.8.2.1" xref="A2.E4.m1.7.8.2.1.cmml">⁢</mo><mi id="A2.E4.m1.7.8.2.3" xref="A2.E4.m1.7.8.2.3.cmml">V</mi><mo id="A2.E4.m1.7.8.2.1a" xref="A2.E4.m1.7.8.2.1.cmml">⁢</mo><mrow id="A2.E4.m1.7.8.2.4.2" xref="A2.E4.m1.7.8.2.cmml"><mo id="A2.E4.m1.7.8.2.4.2.1" stretchy="false" xref="A2.E4.m1.7.8.2.cmml">(</mo><mi id="A2.E4.m1.7.7" xref="A2.E4.m1.7.7.cmml">S</mi><mo id="A2.E4.m1.7.8.2.4.2.2" stretchy="false" xref="A2.E4.m1.7.8.2.cmml">)</mo></mrow></mrow><mo id="A2.E4.m1.7.8.1" xref="A2.E4.m1.7.8.1.cmml">=</mo><mfrac id="A2.E4.m1.6.6" xref="A2.E4.m1.6.6.cmml"><mrow id="A2.E4.m1.3.3.3" xref="A2.E4.m1.3.3.3.cmml"><msub id="A2.E4.m1.3.3.3.4" xref="A2.E4.m1.3.3.3.4.cmml"><mo id="A2.E4.m1.3.3.3.4.2" xref="A2.E4.m1.3.3.3.4.2.cmml">∑</mo><mrow id="A2.E4.m1.3.3.3.4.3" xref="A2.E4.m1.3.3.3.4.3.cmml"><msub id="A2.E4.m1.3.3.3.4.3.2" xref="A2.E4.m1.3.3.3.4.3.2.cmml"><mi id="A2.E4.m1.3.3.3.4.3.2.2" xref="A2.E4.m1.3.3.3.4.3.2.2.cmml">S</mi><mi id="A2.E4.m1.3.3.3.4.3.2.3" xref="A2.E4.m1.3.3.3.4.3.2.3.cmml">i</mi></msub><mo id="A2.E4.m1.3.3.3.4.3.1" xref="A2.E4.m1.3.3.3.4.3.1.cmml">∈</mo><mi id="A2.E4.m1.3.3.3.4.3.3" xref="A2.E4.m1.3.3.3.4.3.3.cmml">S</mi></mrow></msub><mrow id="A2.E4.m1.3.3.3.3" xref="A2.E4.m1.3.3.3.3.cmml"><mrow id="A2.E4.m1.2.2.2.2.1.1" xref="A2.E4.m1.2.2.2.2.1.2.cmml"><mo id="A2.E4.m1.2.2.2.2.1.1.2" lspace="0em" xref="A2.E4.m1.2.2.2.2.1.2.1.cmml">|</mo><msub id="A2.E4.m1.2.2.2.2.1.1.1" xref="A2.E4.m1.2.2.2.2.1.1.1.cmml"><mi id="A2.E4.m1.2.2.2.2.1.1.1.2" xref="A2.E4.m1.2.2.2.2.1.1.1.2.cmml">S</mi><mi id="A2.E4.m1.2.2.2.2.1.1.1.3" xref="A2.E4.m1.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="A2.E4.m1.2.2.2.2.1.1.3" xref="A2.E4.m1.2.2.2.2.1.2.1.cmml">|</mo></mrow><mo id="A2.E4.m1.3.3.3.3.3" xref="A2.E4.m1.3.3.3.3.3.cmml">⁢</mo><msubsup id="A2.E4.m1.3.3.3.3.2" xref="A2.E4.m1.3.3.3.3.2.cmml"><mrow id="A2.E4.m1.3.3.3.3.2.1.1.1" xref="A2.E4.m1.3.3.3.3.2.1.1.2.cmml"><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.2" xref="A2.E4.m1.3.3.3.3.2.1.1.2.1.cmml">‖</mo><mrow id="A2.E4.m1.3.3.3.3.2.1.1.1.1" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.cmml"><mrow id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.cmml"><mi id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.3" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.3.cmml">μ</mi><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.2" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.cmml"><mi id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.2" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.3" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.2" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.2.cmml">−</mo><mrow id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.cmml"><mi id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.2" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.2.cmml">μ</mi><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.1" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.1.cmml">⁢</mo><mrow id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.3.2" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.cmml"><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.3.2.1" stretchy="false" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.cmml">(</mo><mi id="A2.E4.m1.1.1.1.1" xref="A2.E4.m1.1.1.1.1.cmml">𝐈</mi><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.3.2.2" stretchy="false" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="A2.E4.m1.3.3.3.3.2.1.1.1.3" xref="A2.E4.m1.3.3.3.3.2.1.1.2.1.cmml">‖</mo></mrow><mn id="A2.E4.m1.3.3.3.3.2.1.3" xref="A2.E4.m1.3.3.3.3.2.1.3.cmml">1</mn><mn id="A2.E4.m1.3.3.3.3.2.3" xref="A2.E4.m1.3.3.3.3.2.3.cmml">2</mn></msubsup></mrow></mrow><mrow id="A2.E4.m1.6.6.6" xref="A2.E4.m1.6.6.6.cmml"><msub id="A2.E4.m1.6.6.6.4" xref="A2.E4.m1.6.6.6.4.cmml"><mo id="A2.E4.m1.6.6.6.4.2" xref="A2.E4.m1.6.6.6.4.2.cmml">∑</mo><mrow id="A2.E4.m1.6.6.6.4.3" xref="A2.E4.m1.6.6.6.4.3.cmml"><mi id="A2.E4.m1.6.6.6.4.3.2" xref="A2.E4.m1.6.6.6.4.3.2.cmml">p</mi><mo id="A2.E4.m1.6.6.6.4.3.1" xref="A2.E4.m1.6.6.6.4.3.1.cmml">∈</mo><mi id="A2.E4.m1.6.6.6.4.3.3" xref="A2.E4.m1.6.6.6.4.3.3.cmml">𝐈</mi></mrow></msub><msubsup id="A2.E4.m1.6.6.6.3" xref="A2.E4.m1.6.6.6.3.cmml"><mrow id="A2.E4.m1.6.6.6.3.1.1.1" xref="A2.E4.m1.6.6.6.3.1.1.2.cmml"><mo id="A2.E4.m1.6.6.6.3.1.1.1.2" lspace="0em" xref="A2.E4.m1.6.6.6.3.1.1.2.1.cmml">‖</mo><mrow id="A2.E4.m1.6.6.6.3.1.1.1.1" xref="A2.E4.m1.6.6.6.3.1.1.1.1.cmml"><mrow id="A2.E4.m1.6.6.6.3.1.1.1.1.2" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.cmml"><mi id="A2.E4.m1.6.6.6.3.1.1.1.1.2.2" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.2.cmml">I</mi><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.2.1" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.1.cmml">⁢</mo><mrow id="A2.E4.m1.6.6.6.3.1.1.1.1.2.3.2" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.cmml"><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.2.3.2.1" stretchy="false" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.cmml">(</mo><mi id="A2.E4.m1.4.4.4.1" xref="A2.E4.m1.4.4.4.1.cmml">p</mi><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.2.3.2.2" stretchy="false" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.1" xref="A2.E4.m1.6.6.6.3.1.1.1.1.1.cmml">−</mo><mrow id="A2.E4.m1.6.6.6.3.1.1.1.1.3" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.cmml"><mi id="A2.E4.m1.6.6.6.3.1.1.1.1.3.2" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.2.cmml">μ</mi><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.3.1" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.1.cmml">⁢</mo><mrow id="A2.E4.m1.6.6.6.3.1.1.1.1.3.3.2" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.cmml"><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.3.3.2.1" stretchy="false" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.cmml">(</mo><mi id="A2.E4.m1.5.5.5.2" xref="A2.E4.m1.5.5.5.2.cmml">𝐈</mi><mo id="A2.E4.m1.6.6.6.3.1.1.1.1.3.3.2.2" stretchy="false" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="A2.E4.m1.6.6.6.3.1.1.1.3" xref="A2.E4.m1.6.6.6.3.1.1.2.1.cmml">‖</mo></mrow><mn id="A2.E4.m1.6.6.6.3.1.3" xref="A2.E4.m1.6.6.6.3.1.3.cmml">1</mn><mn id="A2.E4.m1.6.6.6.3.3" xref="A2.E4.m1.6.6.6.3.3.cmml">2</mn></msubsup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A2.E4.m1.7b"><apply id="A2.E4.m1.7.8.cmml" xref="A2.E4.m1.7.8"><eq id="A2.E4.m1.7.8.1.cmml" xref="A2.E4.m1.7.8.1"></eq><apply id="A2.E4.m1.7.8.2.cmml" xref="A2.E4.m1.7.8.2"><times id="A2.E4.m1.7.8.2.1.cmml" xref="A2.E4.m1.7.8.2.1"></times><ci id="A2.E4.m1.7.8.2.2.cmml" xref="A2.E4.m1.7.8.2.2">𝐸</ci><ci id="A2.E4.m1.7.8.2.3.cmml" xref="A2.E4.m1.7.8.2.3">𝑉</ci><ci id="A2.E4.m1.7.7.cmml" xref="A2.E4.m1.7.7">𝑆</ci></apply><apply id="A2.E4.m1.6.6.cmml" xref="A2.E4.m1.6.6"><divide id="A2.E4.m1.6.6.7.cmml" xref="A2.E4.m1.6.6"></divide><apply id="A2.E4.m1.3.3.3.cmml" xref="A2.E4.m1.3.3.3"><apply id="A2.E4.m1.3.3.3.4.cmml" xref="A2.E4.m1.3.3.3.4"><csymbol cd="ambiguous" id="A2.E4.m1.3.3.3.4.1.cmml" xref="A2.E4.m1.3.3.3.4">subscript</csymbol><sum id="A2.E4.m1.3.3.3.4.2.cmml" xref="A2.E4.m1.3.3.3.4.2"></sum><apply id="A2.E4.m1.3.3.3.4.3.cmml" xref="A2.E4.m1.3.3.3.4.3"><in id="A2.E4.m1.3.3.3.4.3.1.cmml" xref="A2.E4.m1.3.3.3.4.3.1"></in><apply id="A2.E4.m1.3.3.3.4.3.2.cmml" xref="A2.E4.m1.3.3.3.4.3.2"><csymbol cd="ambiguous" id="A2.E4.m1.3.3.3.4.3.2.1.cmml" xref="A2.E4.m1.3.3.3.4.3.2">subscript</csymbol><ci id="A2.E4.m1.3.3.3.4.3.2.2.cmml" xref="A2.E4.m1.3.3.3.4.3.2.2">𝑆</ci><ci id="A2.E4.m1.3.3.3.4.3.2.3.cmml" xref="A2.E4.m1.3.3.3.4.3.2.3">𝑖</ci></apply><ci id="A2.E4.m1.3.3.3.4.3.3.cmml" xref="A2.E4.m1.3.3.3.4.3.3">𝑆</ci></apply></apply><apply id="A2.E4.m1.3.3.3.3.cmml" xref="A2.E4.m1.3.3.3.3"><times id="A2.E4.m1.3.3.3.3.3.cmml" xref="A2.E4.m1.3.3.3.3.3"></times><apply id="A2.E4.m1.2.2.2.2.1.2.cmml" xref="A2.E4.m1.2.2.2.2.1.1"><abs id="A2.E4.m1.2.2.2.2.1.2.1.cmml" xref="A2.E4.m1.2.2.2.2.1.1.2"></abs><apply id="A2.E4.m1.2.2.2.2.1.1.1.cmml" xref="A2.E4.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.E4.m1.2.2.2.2.1.1.1.1.cmml" xref="A2.E4.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="A2.E4.m1.2.2.2.2.1.1.1.2.cmml" xref="A2.E4.m1.2.2.2.2.1.1.1.2">𝑆</ci><ci id="A2.E4.m1.2.2.2.2.1.1.1.3.cmml" xref="A2.E4.m1.2.2.2.2.1.1.1.3">𝑖</ci></apply></apply><apply id="A2.E4.m1.3.3.3.3.2.cmml" xref="A2.E4.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="A2.E4.m1.3.3.3.3.2.2.cmml" xref="A2.E4.m1.3.3.3.3.2">superscript</csymbol><apply id="A2.E4.m1.3.3.3.3.2.1.cmml" xref="A2.E4.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="A2.E4.m1.3.3.3.3.2.1.2.cmml" xref="A2.E4.m1.3.3.3.3.2">subscript</csymbol><apply id="A2.E4.m1.3.3.3.3.2.1.1.2.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1"><csymbol cd="latexml" id="A2.E4.m1.3.3.3.3.2.1.1.2.1.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.2">norm</csymbol><apply id="A2.E4.m1.3.3.3.3.2.1.1.1.1.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1"><minus id="A2.E4.m1.3.3.3.3.2.1.1.1.1.2.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.2"></minus><apply id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1"><times id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.2.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.2"></times><ci id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.3.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.3">𝜇</ci><apply id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.2">𝑆</ci><ci id="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3"><times id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.1.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.1"></times><ci id="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.2.cmml" xref="A2.E4.m1.3.3.3.3.2.1.1.1.1.3.2">𝜇</ci><ci id="A2.E4.m1.1.1.1.1.cmml" xref="A2.E4.m1.1.1.1.1">𝐈</ci></apply></apply></apply><cn id="A2.E4.m1.3.3.3.3.2.1.3.cmml" type="integer" xref="A2.E4.m1.3.3.3.3.2.1.3">1</cn></apply><cn id="A2.E4.m1.3.3.3.3.2.3.cmml" type="integer" xref="A2.E4.m1.3.3.3.3.2.3">2</cn></apply></apply></apply><apply id="A2.E4.m1.6.6.6.cmml" xref="A2.E4.m1.6.6.6"><apply id="A2.E4.m1.6.6.6.4.cmml" xref="A2.E4.m1.6.6.6.4"><csymbol cd="ambiguous" id="A2.E4.m1.6.6.6.4.1.cmml" xref="A2.E4.m1.6.6.6.4">subscript</csymbol><sum id="A2.E4.m1.6.6.6.4.2.cmml" xref="A2.E4.m1.6.6.6.4.2"></sum><apply id="A2.E4.m1.6.6.6.4.3.cmml" xref="A2.E4.m1.6.6.6.4.3"><in id="A2.E4.m1.6.6.6.4.3.1.cmml" xref="A2.E4.m1.6.6.6.4.3.1"></in><ci id="A2.E4.m1.6.6.6.4.3.2.cmml" xref="A2.E4.m1.6.6.6.4.3.2">𝑝</ci><ci id="A2.E4.m1.6.6.6.4.3.3.cmml" xref="A2.E4.m1.6.6.6.4.3.3">𝐈</ci></apply></apply><apply id="A2.E4.m1.6.6.6.3.cmml" xref="A2.E4.m1.6.6.6.3"><csymbol cd="ambiguous" id="A2.E4.m1.6.6.6.3.2.cmml" xref="A2.E4.m1.6.6.6.3">superscript</csymbol><apply id="A2.E4.m1.6.6.6.3.1.cmml" xref="A2.E4.m1.6.6.6.3"><csymbol cd="ambiguous" id="A2.E4.m1.6.6.6.3.1.2.cmml" xref="A2.E4.m1.6.6.6.3">subscript</csymbol><apply id="A2.E4.m1.6.6.6.3.1.1.2.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1"><csymbol cd="latexml" id="A2.E4.m1.6.6.6.3.1.1.2.1.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.2">norm</csymbol><apply id="A2.E4.m1.6.6.6.3.1.1.1.1.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1"><minus id="A2.E4.m1.6.6.6.3.1.1.1.1.1.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.1"></minus><apply id="A2.E4.m1.6.6.6.3.1.1.1.1.2.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2"><times id="A2.E4.m1.6.6.6.3.1.1.1.1.2.1.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.1"></times><ci id="A2.E4.m1.6.6.6.3.1.1.1.1.2.2.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.2.2">𝐼</ci><ci id="A2.E4.m1.4.4.4.1.cmml" xref="A2.E4.m1.4.4.4.1">𝑝</ci></apply><apply id="A2.E4.m1.6.6.6.3.1.1.1.1.3.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3"><times id="A2.E4.m1.6.6.6.3.1.1.1.1.3.1.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.1"></times><ci id="A2.E4.m1.6.6.6.3.1.1.1.1.3.2.cmml" xref="A2.E4.m1.6.6.6.3.1.1.1.1.3.2">𝜇</ci><ci id="A2.E4.m1.5.5.5.2.cmml" xref="A2.E4.m1.5.5.5.2">𝐈</ci></apply></apply></apply><cn id="A2.E4.m1.6.6.6.3.1.3.cmml" type="integer" xref="A2.E4.m1.6.6.6.3.1.3">1</cn></apply><cn id="A2.E4.m1.6.6.6.3.3.cmml" type="integer" xref="A2.E4.m1.6.6.6.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E4.m1.7c">EV(S)=\frac{\sum_{S_{i}\in S}\left|S_{i}\right|\left\|\mu(S_{i})-\mu(\mathbf{I%
})\right\|_{1}^{2}}{\sum_{p\in\mathbf{I}}\left\|I(p)-\mu(\mathbf{I})\right\|_{%
1}^{2}}</annotation><annotation encoding="application/x-llamapun" id="A2.E4.m1.7d">italic_E italic_V ( italic_S ) = divide start_ARG ∑ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S end_POSTSUBSCRIPT | italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | ∥ italic_μ ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_μ ( bold_I ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_p ∈ bold_I end_POSTSUBSCRIPT ∥ italic_I ( italic_p ) - italic_μ ( bold_I ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.p7">
<p class="ltx_p" id="A2.p7.7">To overcome the mean color drawback, the <span class="ltx_text ltx_font_italic" id="A2.p7.7.1">Similarity between Image and Reconstruction from Superpixels</span> (SIRS) <cite class="ltx_cite ltx_citemacro_citep">(Barcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib9" title="">2022</a>)</cite> models the color homogeneity problem as an image reconstruction problem. The color descriptor <span class="ltx_text ltx_font_italic" id="A2.p7.7.2">RGB Bucket Descriptor</span> (RBD) represents each superpixel as a small set of its most relevant colors. Let <math alttext="G^{S_{i}}\in\mathbf{S}(S_{i},7)" class="ltx_Math" display="inline" id="A2.p7.1.m1.2"><semantics id="A2.p7.1.m1.2a"><mrow id="A2.p7.1.m1.2.2" xref="A2.p7.1.m1.2.2.cmml"><msup id="A2.p7.1.m1.2.2.3" xref="A2.p7.1.m1.2.2.3.cmml"><mi id="A2.p7.1.m1.2.2.3.2" xref="A2.p7.1.m1.2.2.3.2.cmml">G</mi><msub id="A2.p7.1.m1.2.2.3.3" xref="A2.p7.1.m1.2.2.3.3.cmml"><mi id="A2.p7.1.m1.2.2.3.3.2" xref="A2.p7.1.m1.2.2.3.3.2.cmml">S</mi><mi id="A2.p7.1.m1.2.2.3.3.3" xref="A2.p7.1.m1.2.2.3.3.3.cmml">i</mi></msub></msup><mo id="A2.p7.1.m1.2.2.2" xref="A2.p7.1.m1.2.2.2.cmml">∈</mo><mrow id="A2.p7.1.m1.2.2.1" xref="A2.p7.1.m1.2.2.1.cmml"><mi id="A2.p7.1.m1.2.2.1.3" xref="A2.p7.1.m1.2.2.1.3.cmml">𝐒</mi><mo id="A2.p7.1.m1.2.2.1.2" xref="A2.p7.1.m1.2.2.1.2.cmml">⁢</mo><mrow id="A2.p7.1.m1.2.2.1.1.1" xref="A2.p7.1.m1.2.2.1.1.2.cmml"><mo id="A2.p7.1.m1.2.2.1.1.1.2" stretchy="false" xref="A2.p7.1.m1.2.2.1.1.2.cmml">(</mo><msub id="A2.p7.1.m1.2.2.1.1.1.1" xref="A2.p7.1.m1.2.2.1.1.1.1.cmml"><mi id="A2.p7.1.m1.2.2.1.1.1.1.2" xref="A2.p7.1.m1.2.2.1.1.1.1.2.cmml">S</mi><mi id="A2.p7.1.m1.2.2.1.1.1.1.3" xref="A2.p7.1.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="A2.p7.1.m1.2.2.1.1.1.3" xref="A2.p7.1.m1.2.2.1.1.2.cmml">,</mo><mn id="A2.p7.1.m1.1.1" xref="A2.p7.1.m1.1.1.cmml">7</mn><mo id="A2.p7.1.m1.2.2.1.1.1.4" stretchy="false" xref="A2.p7.1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p7.1.m1.2b"><apply id="A2.p7.1.m1.2.2.cmml" xref="A2.p7.1.m1.2.2"><in id="A2.p7.1.m1.2.2.2.cmml" xref="A2.p7.1.m1.2.2.2"></in><apply id="A2.p7.1.m1.2.2.3.cmml" xref="A2.p7.1.m1.2.2.3"><csymbol cd="ambiguous" id="A2.p7.1.m1.2.2.3.1.cmml" xref="A2.p7.1.m1.2.2.3">superscript</csymbol><ci id="A2.p7.1.m1.2.2.3.2.cmml" xref="A2.p7.1.m1.2.2.3.2">𝐺</ci><apply id="A2.p7.1.m1.2.2.3.3.cmml" xref="A2.p7.1.m1.2.2.3.3"><csymbol cd="ambiguous" id="A2.p7.1.m1.2.2.3.3.1.cmml" xref="A2.p7.1.m1.2.2.3.3">subscript</csymbol><ci id="A2.p7.1.m1.2.2.3.3.2.cmml" xref="A2.p7.1.m1.2.2.3.3.2">𝑆</ci><ci id="A2.p7.1.m1.2.2.3.3.3.cmml" xref="A2.p7.1.m1.2.2.3.3.3">𝑖</ci></apply></apply><apply id="A2.p7.1.m1.2.2.1.cmml" xref="A2.p7.1.m1.2.2.1"><times id="A2.p7.1.m1.2.2.1.2.cmml" xref="A2.p7.1.m1.2.2.1.2"></times><ci id="A2.p7.1.m1.2.2.1.3.cmml" xref="A2.p7.1.m1.2.2.1.3">𝐒</ci><interval closure="open" id="A2.p7.1.m1.2.2.1.1.2.cmml" xref="A2.p7.1.m1.2.2.1.1.1"><apply id="A2.p7.1.m1.2.2.1.1.1.1.cmml" xref="A2.p7.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="A2.p7.1.m1.2.2.1.1.1.1.1.cmml" xref="A2.p7.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="A2.p7.1.m1.2.2.1.1.1.1.2.cmml" xref="A2.p7.1.m1.2.2.1.1.1.1.2">𝑆</ci><ci id="A2.p7.1.m1.2.2.1.1.1.1.3.cmml" xref="A2.p7.1.m1.2.2.1.1.1.1.3">𝑖</ci></apply><cn id="A2.p7.1.m1.1.1.cmml" type="integer" xref="A2.p7.1.m1.1.1">7</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.1.m1.2c">G^{S_{i}}\in\mathbf{S}(S_{i},7)</annotation><annotation encoding="application/x-llamapun" id="A2.p7.1.m1.2d">italic_G start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ∈ bold_S ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , 7 )</annotation></semantics></math> represent the set of <math alttext="7" class="ltx_Math" display="inline" id="A2.p7.2.m2.1"><semantics id="A2.p7.2.m2.1a"><mn id="A2.p7.2.m2.1.1" xref="A2.p7.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="A2.p7.2.m2.1b"><cn id="A2.p7.2.m2.1.1.cmml" type="integer" xref="A2.p7.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="A2.p7.2.m2.1d">7</annotation></semantics></math> disjoint groups related to each RGB cube vertices, whose colors are <math alttext="c_{l}\in\left[0,1\right]^{3}" class="ltx_Math" display="inline" id="A2.p7.3.m3.2"><semantics id="A2.p7.3.m3.2a"><mrow id="A2.p7.3.m3.2.3" xref="A2.p7.3.m3.2.3.cmml"><msub id="A2.p7.3.m3.2.3.2" xref="A2.p7.3.m3.2.3.2.cmml"><mi id="A2.p7.3.m3.2.3.2.2" xref="A2.p7.3.m3.2.3.2.2.cmml">c</mi><mi id="A2.p7.3.m3.2.3.2.3" xref="A2.p7.3.m3.2.3.2.3.cmml">l</mi></msub><mo id="A2.p7.3.m3.2.3.1" xref="A2.p7.3.m3.2.3.1.cmml">∈</mo><msup id="A2.p7.3.m3.2.3.3" xref="A2.p7.3.m3.2.3.3.cmml"><mrow id="A2.p7.3.m3.2.3.3.2.2" xref="A2.p7.3.m3.2.3.3.2.1.cmml"><mo id="A2.p7.3.m3.2.3.3.2.2.1" xref="A2.p7.3.m3.2.3.3.2.1.cmml">[</mo><mn id="A2.p7.3.m3.1.1" xref="A2.p7.3.m3.1.1.cmml">0</mn><mo id="A2.p7.3.m3.2.3.3.2.2.2" xref="A2.p7.3.m3.2.3.3.2.1.cmml">,</mo><mn id="A2.p7.3.m3.2.2" xref="A2.p7.3.m3.2.2.cmml">1</mn><mo id="A2.p7.3.m3.2.3.3.2.2.3" xref="A2.p7.3.m3.2.3.3.2.1.cmml">]</mo></mrow><mn id="A2.p7.3.m3.2.3.3.3" xref="A2.p7.3.m3.2.3.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.p7.3.m3.2b"><apply id="A2.p7.3.m3.2.3.cmml" xref="A2.p7.3.m3.2.3"><in id="A2.p7.3.m3.2.3.1.cmml" xref="A2.p7.3.m3.2.3.1"></in><apply id="A2.p7.3.m3.2.3.2.cmml" xref="A2.p7.3.m3.2.3.2"><csymbol cd="ambiguous" id="A2.p7.3.m3.2.3.2.1.cmml" xref="A2.p7.3.m3.2.3.2">subscript</csymbol><ci id="A2.p7.3.m3.2.3.2.2.cmml" xref="A2.p7.3.m3.2.3.2.2">𝑐</ci><ci id="A2.p7.3.m3.2.3.2.3.cmml" xref="A2.p7.3.m3.2.3.2.3">𝑙</ci></apply><apply id="A2.p7.3.m3.2.3.3.cmml" xref="A2.p7.3.m3.2.3.3"><csymbol cd="ambiguous" id="A2.p7.3.m3.2.3.3.1.cmml" xref="A2.p7.3.m3.2.3.3">superscript</csymbol><interval closure="closed" id="A2.p7.3.m3.2.3.3.2.1.cmml" xref="A2.p7.3.m3.2.3.3.2.2"><cn id="A2.p7.3.m3.1.1.cmml" type="integer" xref="A2.p7.3.m3.1.1">0</cn><cn id="A2.p7.3.m3.2.2.cmml" type="integer" xref="A2.p7.3.m3.2.2">1</cn></interval><cn id="A2.p7.3.m3.2.3.3.3.cmml" type="integer" xref="A2.p7.3.m3.2.3.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.3.m3.2c">c_{l}\in\left[0,1\right]^{3}</annotation><annotation encoding="application/x-llamapun" id="A2.p7.3.m3.2d">italic_c start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ [ 0 , 1 ] start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, in which <math alttext="1\leq l\leq 7" class="ltx_Math" display="inline" id="A2.p7.4.m4.1"><semantics id="A2.p7.4.m4.1a"><mrow id="A2.p7.4.m4.1.1" xref="A2.p7.4.m4.1.1.cmml"><mn id="A2.p7.4.m4.1.1.2" xref="A2.p7.4.m4.1.1.2.cmml">1</mn><mo id="A2.p7.4.m4.1.1.3" xref="A2.p7.4.m4.1.1.3.cmml">≤</mo><mi id="A2.p7.4.m4.1.1.4" xref="A2.p7.4.m4.1.1.4.cmml">l</mi><mo id="A2.p7.4.m4.1.1.5" xref="A2.p7.4.m4.1.1.5.cmml">≤</mo><mn id="A2.p7.4.m4.1.1.6" xref="A2.p7.4.m4.1.1.6.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p7.4.m4.1b"><apply id="A2.p7.4.m4.1.1.cmml" xref="A2.p7.4.m4.1.1"><and id="A2.p7.4.m4.1.1a.cmml" xref="A2.p7.4.m4.1.1"></and><apply id="A2.p7.4.m4.1.1b.cmml" xref="A2.p7.4.m4.1.1"><leq id="A2.p7.4.m4.1.1.3.cmml" xref="A2.p7.4.m4.1.1.3"></leq><cn id="A2.p7.4.m4.1.1.2.cmml" type="integer" xref="A2.p7.4.m4.1.1.2">1</cn><ci id="A2.p7.4.m4.1.1.4.cmml" xref="A2.p7.4.m4.1.1.4">𝑙</ci></apply><apply id="A2.p7.4.m4.1.1c.cmml" xref="A2.p7.4.m4.1.1"><leq id="A2.p7.4.m4.1.1.5.cmml" xref="A2.p7.4.m4.1.1.5"></leq><share href="https://arxiv.org/html/2409.19179v1#A2.p7.4.m4.1.1.4.cmml" id="A2.p7.4.m4.1.1d.cmml" xref="A2.p7.4.m4.1.1"></share><cn id="A2.p7.4.m4.1.1.6.cmml" type="integer" xref="A2.p7.4.m4.1.1.6">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.4.m4.1c">1\leq l\leq 7</annotation><annotation encoding="application/x-llamapun" id="A2.p7.4.m4.1d">1 ≤ italic_l ≤ 7</annotation></semantics></math>. Then, we populate each <math alttext="G^{S_{i}}_{l}\in G^{S_{i}}" class="ltx_Math" display="inline" id="A2.p7.5.m5.1"><semantics id="A2.p7.5.m5.1a"><mrow id="A2.p7.5.m5.1.1" xref="A2.p7.5.m5.1.1.cmml"><msubsup id="A2.p7.5.m5.1.1.2" xref="A2.p7.5.m5.1.1.2.cmml"><mi id="A2.p7.5.m5.1.1.2.2.2" xref="A2.p7.5.m5.1.1.2.2.2.cmml">G</mi><mi id="A2.p7.5.m5.1.1.2.3" xref="A2.p7.5.m5.1.1.2.3.cmml">l</mi><msub id="A2.p7.5.m5.1.1.2.2.3" xref="A2.p7.5.m5.1.1.2.2.3.cmml"><mi id="A2.p7.5.m5.1.1.2.2.3.2" xref="A2.p7.5.m5.1.1.2.2.3.2.cmml">S</mi><mi id="A2.p7.5.m5.1.1.2.2.3.3" xref="A2.p7.5.m5.1.1.2.2.3.3.cmml">i</mi></msub></msubsup><mo id="A2.p7.5.m5.1.1.1" xref="A2.p7.5.m5.1.1.1.cmml">∈</mo><msup id="A2.p7.5.m5.1.1.3" xref="A2.p7.5.m5.1.1.3.cmml"><mi id="A2.p7.5.m5.1.1.3.2" xref="A2.p7.5.m5.1.1.3.2.cmml">G</mi><msub id="A2.p7.5.m5.1.1.3.3" xref="A2.p7.5.m5.1.1.3.3.cmml"><mi id="A2.p7.5.m5.1.1.3.3.2" xref="A2.p7.5.m5.1.1.3.3.2.cmml">S</mi><mi id="A2.p7.5.m5.1.1.3.3.3" xref="A2.p7.5.m5.1.1.3.3.3.cmml">i</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.p7.5.m5.1b"><apply id="A2.p7.5.m5.1.1.cmml" xref="A2.p7.5.m5.1.1"><in id="A2.p7.5.m5.1.1.1.cmml" xref="A2.p7.5.m5.1.1.1"></in><apply id="A2.p7.5.m5.1.1.2.cmml" xref="A2.p7.5.m5.1.1.2"><csymbol cd="ambiguous" id="A2.p7.5.m5.1.1.2.1.cmml" xref="A2.p7.5.m5.1.1.2">subscript</csymbol><apply id="A2.p7.5.m5.1.1.2.2.cmml" xref="A2.p7.5.m5.1.1.2"><csymbol cd="ambiguous" id="A2.p7.5.m5.1.1.2.2.1.cmml" xref="A2.p7.5.m5.1.1.2">superscript</csymbol><ci id="A2.p7.5.m5.1.1.2.2.2.cmml" xref="A2.p7.5.m5.1.1.2.2.2">𝐺</ci><apply id="A2.p7.5.m5.1.1.2.2.3.cmml" xref="A2.p7.5.m5.1.1.2.2.3"><csymbol cd="ambiguous" id="A2.p7.5.m5.1.1.2.2.3.1.cmml" xref="A2.p7.5.m5.1.1.2.2.3">subscript</csymbol><ci id="A2.p7.5.m5.1.1.2.2.3.2.cmml" xref="A2.p7.5.m5.1.1.2.2.3.2">𝑆</ci><ci id="A2.p7.5.m5.1.1.2.2.3.3.cmml" xref="A2.p7.5.m5.1.1.2.2.3.3">𝑖</ci></apply></apply><ci id="A2.p7.5.m5.1.1.2.3.cmml" xref="A2.p7.5.m5.1.1.2.3">𝑙</ci></apply><apply id="A2.p7.5.m5.1.1.3.cmml" xref="A2.p7.5.m5.1.1.3"><csymbol cd="ambiguous" id="A2.p7.5.m5.1.1.3.1.cmml" xref="A2.p7.5.m5.1.1.3">superscript</csymbol><ci id="A2.p7.5.m5.1.1.3.2.cmml" xref="A2.p7.5.m5.1.1.3.2">𝐺</ci><apply id="A2.p7.5.m5.1.1.3.3.cmml" xref="A2.p7.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="A2.p7.5.m5.1.1.3.3.1.cmml" xref="A2.p7.5.m5.1.1.3.3">subscript</csymbol><ci id="A2.p7.5.m5.1.1.3.3.2.cmml" xref="A2.p7.5.m5.1.1.3.3.2">𝑆</ci><ci id="A2.p7.5.m5.1.1.3.3.3.cmml" xref="A2.p7.5.m5.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.5.m5.1c">G^{S_{i}}_{l}\in G^{S_{i}}</annotation><annotation encoding="application/x-llamapun" id="A2.p7.5.m5.1d">italic_G start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ italic_G start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> by assigning every <math alttext="p\in S_{i}" class="ltx_Math" display="inline" id="A2.p7.6.m6.1"><semantics id="A2.p7.6.m6.1a"><mrow id="A2.p7.6.m6.1.1" xref="A2.p7.6.m6.1.1.cmml"><mi id="A2.p7.6.m6.1.1.2" xref="A2.p7.6.m6.1.1.2.cmml">p</mi><mo id="A2.p7.6.m6.1.1.1" xref="A2.p7.6.m6.1.1.1.cmml">∈</mo><msub id="A2.p7.6.m6.1.1.3" xref="A2.p7.6.m6.1.1.3.cmml"><mi id="A2.p7.6.m6.1.1.3.2" xref="A2.p7.6.m6.1.1.3.2.cmml">S</mi><mi id="A2.p7.6.m6.1.1.3.3" xref="A2.p7.6.m6.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.p7.6.m6.1b"><apply id="A2.p7.6.m6.1.1.cmml" xref="A2.p7.6.m6.1.1"><in id="A2.p7.6.m6.1.1.1.cmml" xref="A2.p7.6.m6.1.1.1"></in><ci id="A2.p7.6.m6.1.1.2.cmml" xref="A2.p7.6.m6.1.1.2">𝑝</ci><apply id="A2.p7.6.m6.1.1.3.cmml" xref="A2.p7.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.p7.6.m6.1.1.3.1.cmml" xref="A2.p7.6.m6.1.1.3">subscript</csymbol><ci id="A2.p7.6.m6.1.1.3.2.cmml" xref="A2.p7.6.m6.1.1.3.2">𝑆</ci><ci id="A2.p7.6.m6.1.1.3.3.cmml" xref="A2.p7.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.6.m6.1c">p\in S_{i}</annotation><annotation encoding="application/x-llamapun" id="A2.p7.6.m6.1d">italic_p ∈ italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to its most similar group using a mapping function <math alttext="M(p)" class="ltx_Math" display="inline" id="A2.p7.7.m7.1"><semantics id="A2.p7.7.m7.1a"><mrow id="A2.p7.7.m7.1.2" xref="A2.p7.7.m7.1.2.cmml"><mi id="A2.p7.7.m7.1.2.2" xref="A2.p7.7.m7.1.2.2.cmml">M</mi><mo id="A2.p7.7.m7.1.2.1" xref="A2.p7.7.m7.1.2.1.cmml">⁢</mo><mrow id="A2.p7.7.m7.1.2.3.2" xref="A2.p7.7.m7.1.2.cmml"><mo id="A2.p7.7.m7.1.2.3.2.1" stretchy="false" xref="A2.p7.7.m7.1.2.cmml">(</mo><mi id="A2.p7.7.m7.1.1" xref="A2.p7.7.m7.1.1.cmml">p</mi><mo id="A2.p7.7.m7.1.2.3.2.2" stretchy="false" xref="A2.p7.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p7.7.m7.1b"><apply id="A2.p7.7.m7.1.2.cmml" xref="A2.p7.7.m7.1.2"><times id="A2.p7.7.m7.1.2.1.cmml" xref="A2.p7.7.m7.1.2.1"></times><ci id="A2.p7.7.m7.1.2.2.cmml" xref="A2.p7.7.m7.1.2.2">𝑀</ci><ci id="A2.p7.7.m7.1.1.cmml" xref="A2.p7.7.m7.1.1">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p7.7.m7.1c">M(p)</annotation><annotation encoding="application/x-llamapun" id="A2.p7.7.m7.1d">italic_M ( italic_p )</annotation></semantics></math> (Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E5" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="M(p)=\operatorname*{argmin}_{c_{i}\in V}\left\{\left\|x-c_{i}\right\|_{1}\right\}" class="ltx_Math" display="block" id="A2.E5.m1.3"><semantics id="A2.E5.m1.3a"><mrow id="A2.E5.m1.3.3" xref="A2.E5.m1.3.3.cmml"><mrow id="A2.E5.m1.3.3.4" xref="A2.E5.m1.3.3.4.cmml"><mi id="A2.E5.m1.3.3.4.2" xref="A2.E5.m1.3.3.4.2.cmml">M</mi><mo id="A2.E5.m1.3.3.4.1" xref="A2.E5.m1.3.3.4.1.cmml">⁢</mo><mrow id="A2.E5.m1.3.3.4.3.2" xref="A2.E5.m1.3.3.4.cmml"><mo id="A2.E5.m1.3.3.4.3.2.1" stretchy="false" xref="A2.E5.m1.3.3.4.cmml">(</mo><mi id="A2.E5.m1.1.1" xref="A2.E5.m1.1.1.cmml">p</mi><mo id="A2.E5.m1.3.3.4.3.2.2" stretchy="false" xref="A2.E5.m1.3.3.4.cmml">)</mo></mrow></mrow><mo id="A2.E5.m1.3.3.3" rspace="0.1389em" xref="A2.E5.m1.3.3.3.cmml">=</mo><mrow id="A2.E5.m1.3.3.2.2" xref="A2.E5.m1.3.3.2.3.cmml"><munder id="A2.E5.m1.2.2.1.1.1" xref="A2.E5.m1.2.2.1.1.1.cmml"><mo id="A2.E5.m1.2.2.1.1.1.2" lspace="0.1389em" rspace="0em" xref="A2.E5.m1.2.2.1.1.1.2.cmml">argmin</mo><mrow id="A2.E5.m1.2.2.1.1.1.3" xref="A2.E5.m1.2.2.1.1.1.3.cmml"><msub id="A2.E5.m1.2.2.1.1.1.3.2" xref="A2.E5.m1.2.2.1.1.1.3.2.cmml"><mi id="A2.E5.m1.2.2.1.1.1.3.2.2" xref="A2.E5.m1.2.2.1.1.1.3.2.2.cmml">c</mi><mi id="A2.E5.m1.2.2.1.1.1.3.2.3" xref="A2.E5.m1.2.2.1.1.1.3.2.3.cmml">i</mi></msub><mo id="A2.E5.m1.2.2.1.1.1.3.1" xref="A2.E5.m1.2.2.1.1.1.3.1.cmml">∈</mo><mi id="A2.E5.m1.2.2.1.1.1.3.3" xref="A2.E5.m1.2.2.1.1.1.3.3.cmml">V</mi></mrow></munder><mrow id="A2.E5.m1.3.3.2.2.2" xref="A2.E5.m1.3.3.2.3.cmml"><mo id="A2.E5.m1.3.3.2.2.2.2" xref="A2.E5.m1.3.3.2.3.cmml">{</mo><msub id="A2.E5.m1.3.3.2.2.2.1" xref="A2.E5.m1.3.3.2.2.2.1.cmml"><mrow id="A2.E5.m1.3.3.2.2.2.1.1.1" xref="A2.E5.m1.3.3.2.2.2.1.1.2.cmml"><mo id="A2.E5.m1.3.3.2.2.2.1.1.1.2" xref="A2.E5.m1.3.3.2.2.2.1.1.2.1.cmml">‖</mo><mrow id="A2.E5.m1.3.3.2.2.2.1.1.1.1" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.cmml"><mi id="A2.E5.m1.3.3.2.2.2.1.1.1.1.2" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.2.cmml">x</mi><mo id="A2.E5.m1.3.3.2.2.2.1.1.1.1.1" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.1.cmml">−</mo><msub id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.cmml"><mi id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.2" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.2.cmml">c</mi><mi id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.3" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="A2.E5.m1.3.3.2.2.2.1.1.1.3" xref="A2.E5.m1.3.3.2.2.2.1.1.2.1.cmml">‖</mo></mrow><mn id="A2.E5.m1.3.3.2.2.2.1.3" xref="A2.E5.m1.3.3.2.2.2.1.3.cmml">1</mn></msub><mo id="A2.E5.m1.3.3.2.2.2.3" xref="A2.E5.m1.3.3.2.3.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.E5.m1.3b"><apply id="A2.E5.m1.3.3.cmml" xref="A2.E5.m1.3.3"><eq id="A2.E5.m1.3.3.3.cmml" xref="A2.E5.m1.3.3.3"></eq><apply id="A2.E5.m1.3.3.4.cmml" xref="A2.E5.m1.3.3.4"><times id="A2.E5.m1.3.3.4.1.cmml" xref="A2.E5.m1.3.3.4.1"></times><ci id="A2.E5.m1.3.3.4.2.cmml" xref="A2.E5.m1.3.3.4.2">𝑀</ci><ci id="A2.E5.m1.1.1.cmml" xref="A2.E5.m1.1.1">𝑝</ci></apply><apply id="A2.E5.m1.3.3.2.3.cmml" xref="A2.E5.m1.3.3.2.2"><apply id="A2.E5.m1.2.2.1.1.1.cmml" xref="A2.E5.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.E5.m1.2.2.1.1.1.1.cmml" xref="A2.E5.m1.2.2.1.1.1">subscript</csymbol><ci id="A2.E5.m1.2.2.1.1.1.2.cmml" xref="A2.E5.m1.2.2.1.1.1.2">argmin</ci><apply id="A2.E5.m1.2.2.1.1.1.3.cmml" xref="A2.E5.m1.2.2.1.1.1.3"><in id="A2.E5.m1.2.2.1.1.1.3.1.cmml" xref="A2.E5.m1.2.2.1.1.1.3.1"></in><apply id="A2.E5.m1.2.2.1.1.1.3.2.cmml" xref="A2.E5.m1.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="A2.E5.m1.2.2.1.1.1.3.2.1.cmml" xref="A2.E5.m1.2.2.1.1.1.3.2">subscript</csymbol><ci id="A2.E5.m1.2.2.1.1.1.3.2.2.cmml" xref="A2.E5.m1.2.2.1.1.1.3.2.2">𝑐</ci><ci id="A2.E5.m1.2.2.1.1.1.3.2.3.cmml" xref="A2.E5.m1.2.2.1.1.1.3.2.3">𝑖</ci></apply><ci id="A2.E5.m1.2.2.1.1.1.3.3.cmml" xref="A2.E5.m1.2.2.1.1.1.3.3">𝑉</ci></apply></apply><apply id="A2.E5.m1.3.3.2.2.2.1.cmml" xref="A2.E5.m1.3.3.2.2.2.1"><csymbol cd="ambiguous" id="A2.E5.m1.3.3.2.2.2.1.2.cmml" xref="A2.E5.m1.3.3.2.2.2.1">subscript</csymbol><apply id="A2.E5.m1.3.3.2.2.2.1.1.2.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1"><csymbol cd="latexml" id="A2.E5.m1.3.3.2.2.2.1.1.2.1.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.2">norm</csymbol><apply id="A2.E5.m1.3.3.2.2.2.1.1.1.1.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1"><minus id="A2.E5.m1.3.3.2.2.2.1.1.1.1.1.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.1"></minus><ci id="A2.E5.m1.3.3.2.2.2.1.1.1.1.2.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.2">𝑥</ci><apply id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.1.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3">subscript</csymbol><ci id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.2.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.2">𝑐</ci><ci id="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.3.cmml" xref="A2.E5.m1.3.3.2.2.2.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn id="A2.E5.m1.3.3.2.2.2.1.3.cmml" type="integer" xref="A2.E5.m1.3.3.2.2.2.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E5.m1.3c">M(p)=\operatorname*{argmin}_{c_{i}\in V}\left\{\left\|x-c_{i}\right\|_{1}\right\}</annotation><annotation encoding="application/x-llamapun" id="A2.E5.m1.3d">italic_M ( italic_p ) = roman_argmin start_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_V end_POSTSUBSCRIPT { ∥ italic_x - italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.p8">
<p class="ltx_p" id="A2.p8.1">The colors in RBD are used to reconstruct the original image. The reconstruction error is measured by the <span class="ltx_text ltx_font_italic" id="A2.p8.1.1">Mean Exponential Error</span> (MEE) between the original and reconstructed image (Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E6" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>). The MEE increases the error weight of heterogeneous colors based on the maximum distance between the colors of the RBD. The MEE’s exponent interval varies between one and two (the absolute or the mean error). Finally, SIRS defines segmentation quality as the Gaussian weighted error of reconstruction using MEE (Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A2.E7" title="In Appendix B Evaluation measures ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{MEE}(S)=\frac{1}{\left|\mathbf{I}\right|}\sum\limits_{S_{i}\in S}\sum%
\limits_{p\in S_{i}}{\left\|R(p)-I(p)\right\|_{1}^{2-\psi}}" class="ltx_Math" display="block" id="A2.E6.m1.5"><semantics id="A2.E6.m1.5a"><mrow id="A2.E6.m1.5.5" xref="A2.E6.m1.5.5.cmml"><mrow id="A2.E6.m1.5.5.3" xref="A2.E6.m1.5.5.3.cmml"><mtext id="A2.E6.m1.5.5.3.2" xref="A2.E6.m1.5.5.3.2a.cmml">MEE</mtext><mo id="A2.E6.m1.5.5.3.1" xref="A2.E6.m1.5.5.3.1.cmml">⁢</mo><mrow id="A2.E6.m1.5.5.3.3.2" xref="A2.E6.m1.5.5.3.cmml"><mo id="A2.E6.m1.5.5.3.3.2.1" stretchy="false" xref="A2.E6.m1.5.5.3.cmml">(</mo><mi id="A2.E6.m1.2.2" xref="A2.E6.m1.2.2.cmml">S</mi><mo id="A2.E6.m1.5.5.3.3.2.2" stretchy="false" xref="A2.E6.m1.5.5.3.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.5.5.2" xref="A2.E6.m1.5.5.2.cmml">=</mo><mrow id="A2.E6.m1.5.5.1" xref="A2.E6.m1.5.5.1.cmml"><mfrac id="A2.E6.m1.1.1" xref="A2.E6.m1.1.1.cmml"><mn id="A2.E6.m1.1.1.3" xref="A2.E6.m1.1.1.3.cmml">1</mn><mrow id="A2.E6.m1.1.1.1.3" xref="A2.E6.m1.1.1.1.2.cmml"><mo id="A2.E6.m1.1.1.1.3.1" xref="A2.E6.m1.1.1.1.2.1.cmml">|</mo><mi id="A2.E6.m1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.cmml">𝐈</mi><mo id="A2.E6.m1.1.1.1.3.2" xref="A2.E6.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="A2.E6.m1.5.5.1.2" xref="A2.E6.m1.5.5.1.2.cmml">⁢</mo><mrow id="A2.E6.m1.5.5.1.1" xref="A2.E6.m1.5.5.1.1.cmml"><munder id="A2.E6.m1.5.5.1.1.2" xref="A2.E6.m1.5.5.1.1.2.cmml"><mo id="A2.E6.m1.5.5.1.1.2.2" movablelimits="false" rspace="0em" xref="A2.E6.m1.5.5.1.1.2.2.cmml">∑</mo><mrow id="A2.E6.m1.5.5.1.1.2.3" xref="A2.E6.m1.5.5.1.1.2.3.cmml"><msub id="A2.E6.m1.5.5.1.1.2.3.2" xref="A2.E6.m1.5.5.1.1.2.3.2.cmml"><mi id="A2.E6.m1.5.5.1.1.2.3.2.2" xref="A2.E6.m1.5.5.1.1.2.3.2.2.cmml">S</mi><mi id="A2.E6.m1.5.5.1.1.2.3.2.3" xref="A2.E6.m1.5.5.1.1.2.3.2.3.cmml">i</mi></msub><mo id="A2.E6.m1.5.5.1.1.2.3.1" xref="A2.E6.m1.5.5.1.1.2.3.1.cmml">∈</mo><mi id="A2.E6.m1.5.5.1.1.2.3.3" xref="A2.E6.m1.5.5.1.1.2.3.3.cmml">S</mi></mrow></munder><mrow id="A2.E6.m1.5.5.1.1.1" xref="A2.E6.m1.5.5.1.1.1.cmml"><munder id="A2.E6.m1.5.5.1.1.1.2" xref="A2.E6.m1.5.5.1.1.1.2.cmml"><mo id="A2.E6.m1.5.5.1.1.1.2.2" movablelimits="false" rspace="0em" xref="A2.E6.m1.5.5.1.1.1.2.2.cmml">∑</mo><mrow id="A2.E6.m1.5.5.1.1.1.2.3" xref="A2.E6.m1.5.5.1.1.1.2.3.cmml"><mi id="A2.E6.m1.5.5.1.1.1.2.3.2" xref="A2.E6.m1.5.5.1.1.1.2.3.2.cmml">p</mi><mo id="A2.E6.m1.5.5.1.1.1.2.3.1" xref="A2.E6.m1.5.5.1.1.1.2.3.1.cmml">∈</mo><msub id="A2.E6.m1.5.5.1.1.1.2.3.3" xref="A2.E6.m1.5.5.1.1.1.2.3.3.cmml"><mi id="A2.E6.m1.5.5.1.1.1.2.3.3.2" xref="A2.E6.m1.5.5.1.1.1.2.3.3.2.cmml">S</mi><mi id="A2.E6.m1.5.5.1.1.1.2.3.3.3" xref="A2.E6.m1.5.5.1.1.1.2.3.3.3.cmml">i</mi></msub></mrow></munder><msubsup id="A2.E6.m1.5.5.1.1.1.1" xref="A2.E6.m1.5.5.1.1.1.1.cmml"><mrow id="A2.E6.m1.5.5.1.1.1.1.1.1.1" xref="A2.E6.m1.5.5.1.1.1.1.1.1.2.cmml"><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.2" xref="A2.E6.m1.5.5.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.2" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml">R</mi><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.1" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mrow id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.3.2" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.cmml"><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.1" stretchy="false" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="A2.E6.m1.3.3" xref="A2.E6.m1.3.3.cmml">p</mi><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.2" stretchy="false" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml">I</mi><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.1" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mrow id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.3.2" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.3.2.1" stretchy="false" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="A2.E6.m1.4.4" xref="A2.E6.m1.4.4.cmml">p</mi><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.3.2.2" stretchy="false" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="A2.E6.m1.5.5.1.1.1.1.1.1.1.3" xref="A2.E6.m1.5.5.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="A2.E6.m1.5.5.1.1.1.1.1.3" xref="A2.E6.m1.5.5.1.1.1.1.1.3.cmml">1</mn><mrow id="A2.E6.m1.5.5.1.1.1.1.3" xref="A2.E6.m1.5.5.1.1.1.1.3.cmml"><mn id="A2.E6.m1.5.5.1.1.1.1.3.2" xref="A2.E6.m1.5.5.1.1.1.1.3.2.cmml">2</mn><mo id="A2.E6.m1.5.5.1.1.1.1.3.1" xref="A2.E6.m1.5.5.1.1.1.1.3.1.cmml">−</mo><mi id="A2.E6.m1.5.5.1.1.1.1.3.3" xref="A2.E6.m1.5.5.1.1.1.1.3.3.cmml">ψ</mi></mrow></msubsup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.5b"><apply id="A2.E6.m1.5.5.cmml" xref="A2.E6.m1.5.5"><eq id="A2.E6.m1.5.5.2.cmml" xref="A2.E6.m1.5.5.2"></eq><apply id="A2.E6.m1.5.5.3.cmml" xref="A2.E6.m1.5.5.3"><times id="A2.E6.m1.5.5.3.1.cmml" xref="A2.E6.m1.5.5.3.1"></times><ci id="A2.E6.m1.5.5.3.2a.cmml" xref="A2.E6.m1.5.5.3.2"><mtext id="A2.E6.m1.5.5.3.2.cmml" xref="A2.E6.m1.5.5.3.2">MEE</mtext></ci><ci id="A2.E6.m1.2.2.cmml" xref="A2.E6.m1.2.2">𝑆</ci></apply><apply id="A2.E6.m1.5.5.1.cmml" xref="A2.E6.m1.5.5.1"><times id="A2.E6.m1.5.5.1.2.cmml" xref="A2.E6.m1.5.5.1.2"></times><apply id="A2.E6.m1.1.1.cmml" xref="A2.E6.m1.1.1"><divide id="A2.E6.m1.1.1.2.cmml" xref="A2.E6.m1.1.1"></divide><cn id="A2.E6.m1.1.1.3.cmml" type="integer" xref="A2.E6.m1.1.1.3">1</cn><apply id="A2.E6.m1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.3"><abs id="A2.E6.m1.1.1.1.2.1.cmml" xref="A2.E6.m1.1.1.1.3.1"></abs><ci id="A2.E6.m1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1">𝐈</ci></apply></apply><apply id="A2.E6.m1.5.5.1.1.cmml" xref="A2.E6.m1.5.5.1.1"><apply id="A2.E6.m1.5.5.1.1.2.cmml" xref="A2.E6.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.5.5.1.1.2.1.cmml" xref="A2.E6.m1.5.5.1.1.2">subscript</csymbol><sum id="A2.E6.m1.5.5.1.1.2.2.cmml" xref="A2.E6.m1.5.5.1.1.2.2"></sum><apply id="A2.E6.m1.5.5.1.1.2.3.cmml" xref="A2.E6.m1.5.5.1.1.2.3"><in id="A2.E6.m1.5.5.1.1.2.3.1.cmml" xref="A2.E6.m1.5.5.1.1.2.3.1"></in><apply id="A2.E6.m1.5.5.1.1.2.3.2.cmml" xref="A2.E6.m1.5.5.1.1.2.3.2"><csymbol cd="ambiguous" id="A2.E6.m1.5.5.1.1.2.3.2.1.cmml" xref="A2.E6.m1.5.5.1.1.2.3.2">subscript</csymbol><ci id="A2.E6.m1.5.5.1.1.2.3.2.2.cmml" xref="A2.E6.m1.5.5.1.1.2.3.2.2">𝑆</ci><ci id="A2.E6.m1.5.5.1.1.2.3.2.3.cmml" xref="A2.E6.m1.5.5.1.1.2.3.2.3">𝑖</ci></apply><ci id="A2.E6.m1.5.5.1.1.2.3.3.cmml" xref="A2.E6.m1.5.5.1.1.2.3.3">𝑆</ci></apply></apply><apply id="A2.E6.m1.5.5.1.1.1.cmml" xref="A2.E6.m1.5.5.1.1.1"><apply id="A2.E6.m1.5.5.1.1.1.2.cmml" xref="A2.E6.m1.5.5.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.5.5.1.1.1.2.1.cmml" xref="A2.E6.m1.5.5.1.1.1.2">subscript</csymbol><sum id="A2.E6.m1.5.5.1.1.1.2.2.cmml" xref="A2.E6.m1.5.5.1.1.1.2.2"></sum><apply id="A2.E6.m1.5.5.1.1.1.2.3.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3"><in id="A2.E6.m1.5.5.1.1.1.2.3.1.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3.1"></in><ci id="A2.E6.m1.5.5.1.1.1.2.3.2.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3.2">𝑝</ci><apply id="A2.E6.m1.5.5.1.1.1.2.3.3.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3.3"><csymbol cd="ambiguous" id="A2.E6.m1.5.5.1.1.1.2.3.3.1.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3.3">subscript</csymbol><ci id="A2.E6.m1.5.5.1.1.1.2.3.3.2.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3.3.2">𝑆</ci><ci id="A2.E6.m1.5.5.1.1.1.2.3.3.3.cmml" xref="A2.E6.m1.5.5.1.1.1.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="A2.E6.m1.5.5.1.1.1.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.5.5.1.1.1.1.2.cmml" xref="A2.E6.m1.5.5.1.1.1.1">superscript</csymbol><apply id="A2.E6.m1.5.5.1.1.1.1.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.5.5.1.1.1.1.1.2.cmml" xref="A2.E6.m1.5.5.1.1.1.1">subscript</csymbol><apply id="A2.E6.m1.5.5.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="latexml" id="A2.E6.m1.5.5.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.2">norm</csymbol><apply id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1"><minus id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.1"></minus><apply id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2"><times id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.1"></times><ci id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.2.2">𝑅</ci><ci id="A2.E6.m1.3.3.cmml" xref="A2.E6.m1.3.3">𝑝</ci></apply><apply id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3"><times id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.1"></times><ci id="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.5.5.1.1.1.1.1.1.1.1.3.2">𝐼</ci><ci id="A2.E6.m1.4.4.cmml" xref="A2.E6.m1.4.4">𝑝</ci></apply></apply></apply><cn id="A2.E6.m1.5.5.1.1.1.1.1.3.cmml" type="integer" xref="A2.E6.m1.5.5.1.1.1.1.1.3">1</cn></apply><apply id="A2.E6.m1.5.5.1.1.1.1.3.cmml" xref="A2.E6.m1.5.5.1.1.1.1.3"><minus id="A2.E6.m1.5.5.1.1.1.1.3.1.cmml" xref="A2.E6.m1.5.5.1.1.1.1.3.1"></minus><cn id="A2.E6.m1.5.5.1.1.1.1.3.2.cmml" type="integer" xref="A2.E6.m1.5.5.1.1.1.1.3.2">2</cn><ci id="A2.E6.m1.5.5.1.1.1.1.3.3.cmml" xref="A2.E6.m1.5.5.1.1.1.1.3.3">𝜓</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.5c">\text{MEE}(S)=\frac{1}{\left|\mathbf{I}\right|}\sum\limits_{S_{i}\in S}\sum%
\limits_{p\in S_{i}}{\left\|R(p)-I(p)\right\|_{1}^{2-\psi}}</annotation><annotation encoding="application/x-llamapun" id="A2.E6.m1.5d">MEE ( italic_S ) = divide start_ARG 1 end_ARG start_ARG | bold_I | end_ARG ∑ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_p ∈ italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ italic_R ( italic_p ) - italic_I ( italic_p ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 - italic_ψ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{SIRS}(S)=\exp^{-\frac{\text{MEE}(S)}{\sigma^{2}}}" class="ltx_Math" display="block" id="A2.E7.m1.2"><semantics id="A2.E7.m1.2a"><mrow id="A2.E7.m1.2.3" xref="A2.E7.m1.2.3.cmml"><mrow id="A2.E7.m1.2.3.2" xref="A2.E7.m1.2.3.2.cmml"><mtext id="A2.E7.m1.2.3.2.2" xref="A2.E7.m1.2.3.2.2a.cmml">SIRS</mtext><mo id="A2.E7.m1.2.3.2.1" xref="A2.E7.m1.2.3.2.1.cmml">⁢</mo><mrow id="A2.E7.m1.2.3.2.3.2" xref="A2.E7.m1.2.3.2.cmml"><mo id="A2.E7.m1.2.3.2.3.2.1" stretchy="false" xref="A2.E7.m1.2.3.2.cmml">(</mo><mi id="A2.E7.m1.2.2" xref="A2.E7.m1.2.2.cmml">S</mi><mo id="A2.E7.m1.2.3.2.3.2.2" stretchy="false" xref="A2.E7.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="A2.E7.m1.2.3.1" xref="A2.E7.m1.2.3.1.cmml">=</mo><msup id="A2.E7.m1.2.3.3" xref="A2.E7.m1.2.3.3.cmml"><mi id="A2.E7.m1.2.3.3.2" xref="A2.E7.m1.2.3.3.2.cmml">exp</mi><mrow id="A2.E7.m1.1.1.1" xref="A2.E7.m1.1.1.1.cmml"><mo id="A2.E7.m1.1.1.1a" xref="A2.E7.m1.1.1.1.cmml">−</mo><mfrac id="A2.E7.m1.1.1.1.1" xref="A2.E7.m1.1.1.1.1.cmml"><mrow id="A2.E7.m1.1.1.1.1.1" xref="A2.E7.m1.1.1.1.1.1.cmml"><mtext id="A2.E7.m1.1.1.1.1.1.3" xref="A2.E7.m1.1.1.1.1.1.3a.cmml">MEE</mtext><mo id="A2.E7.m1.1.1.1.1.1.2" xref="A2.E7.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="A2.E7.m1.1.1.1.1.1.4.2" xref="A2.E7.m1.1.1.1.1.1.cmml"><mo id="A2.E7.m1.1.1.1.1.1.4.2.1" stretchy="false" xref="A2.E7.m1.1.1.1.1.1.cmml">(</mo><mi id="A2.E7.m1.1.1.1.1.1.1" xref="A2.E7.m1.1.1.1.1.1.1.cmml">S</mi><mo id="A2.E7.m1.1.1.1.1.1.4.2.2" stretchy="false" xref="A2.E7.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><msup id="A2.E7.m1.1.1.1.1.3" xref="A2.E7.m1.1.1.1.1.3.cmml"><mi id="A2.E7.m1.1.1.1.1.3.2" xref="A2.E7.m1.1.1.1.1.3.2.cmml">σ</mi><mn id="A2.E7.m1.1.1.1.1.3.3" xref="A2.E7.m1.1.1.1.1.3.3.cmml">2</mn></msup></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.E7.m1.2b"><apply id="A2.E7.m1.2.3.cmml" xref="A2.E7.m1.2.3"><eq id="A2.E7.m1.2.3.1.cmml" xref="A2.E7.m1.2.3.1"></eq><apply id="A2.E7.m1.2.3.2.cmml" xref="A2.E7.m1.2.3.2"><times id="A2.E7.m1.2.3.2.1.cmml" xref="A2.E7.m1.2.3.2.1"></times><ci id="A2.E7.m1.2.3.2.2a.cmml" xref="A2.E7.m1.2.3.2.2"><mtext id="A2.E7.m1.2.3.2.2.cmml" xref="A2.E7.m1.2.3.2.2">SIRS</mtext></ci><ci id="A2.E7.m1.2.2.cmml" xref="A2.E7.m1.2.2">𝑆</ci></apply><apply id="A2.E7.m1.2.3.3.cmml" xref="A2.E7.m1.2.3.3"><csymbol cd="ambiguous" id="A2.E7.m1.2.3.3.1.cmml" xref="A2.E7.m1.2.3.3">superscript</csymbol><exp id="A2.E7.m1.2.3.3.2.cmml" xref="A2.E7.m1.2.3.3.2"></exp><apply id="A2.E7.m1.1.1.1.cmml" xref="A2.E7.m1.1.1.1"><minus id="A2.E7.m1.1.1.1.2.cmml" xref="A2.E7.m1.1.1.1"></minus><apply id="A2.E7.m1.1.1.1.1.cmml" xref="A2.E7.m1.1.1.1.1"><divide id="A2.E7.m1.1.1.1.1.2.cmml" xref="A2.E7.m1.1.1.1.1"></divide><apply id="A2.E7.m1.1.1.1.1.1.cmml" xref="A2.E7.m1.1.1.1.1.1"><times id="A2.E7.m1.1.1.1.1.1.2.cmml" xref="A2.E7.m1.1.1.1.1.1.2"></times><ci id="A2.E7.m1.1.1.1.1.1.3a.cmml" xref="A2.E7.m1.1.1.1.1.1.3"><mtext id="A2.E7.m1.1.1.1.1.1.3.cmml" mathsize="50%" xref="A2.E7.m1.1.1.1.1.1.3">MEE</mtext></ci><ci id="A2.E7.m1.1.1.1.1.1.1.cmml" xref="A2.E7.m1.1.1.1.1.1.1">𝑆</ci></apply><apply id="A2.E7.m1.1.1.1.1.3.cmml" xref="A2.E7.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E7.m1.1.1.1.1.3.1.cmml" xref="A2.E7.m1.1.1.1.1.3">superscript</csymbol><ci id="A2.E7.m1.1.1.1.1.3.2.cmml" xref="A2.E7.m1.1.1.1.1.3.2">𝜎</ci><cn id="A2.E7.m1.1.1.1.1.3.3.cmml" type="integer" xref="A2.E7.m1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E7.m1.2c">\text{SIRS}(S)=\exp^{-\frac{\text{MEE}(S)}{\sigma^{2}}}</annotation><annotation encoding="application/x-llamapun" id="A2.E7.m1.2d">SIRS ( italic_S ) = roman_exp start_POSTSUPERSCRIPT - divide start_ARG MEE ( italic_S ) end_ARG start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Experimental Results</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">This Section presents additional experimental results. First, we assess the method’s ability to control the number of superpixels and to maintain connectivity. Based on the connectivity results, we perform a post-processing step to ensure connectivity for the experiments in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.2</span></a>. In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.2</span></a>, we evaluate the stability of superpixel methods considering the minimum, maximum, and standard deviation of Boundary Recall (BR), Undersegmentation Error (UE), Explained Variation (EV), and Similarity between Image and Reconstruction from Superpixels (SIRS). We consider a stable segmentation to have a monotonically increasing performance in those measures according to the number of superpixels. In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS3" title="C.3. Robustness ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.3</span></a>, we evaluate the robustness of superpixel methods against salt and pepper noise and average blur. Finally, Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS4" title="C.4. Overall performance ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.4</span></a> reviews the overall performance of superpixel methods concerning their clustering category.</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1. </span>Number of superpixels and connectivity</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1"><span class="ltx_text" id="A3.SS1.p1.1.1" style="color:#000000;">All superpixel methods evaluated in this work have a parameter for the desired number of superpixels, but most generate a different number than the desired one. Although control over the number of superpixels is desirable, some works reduce this control to produce a segmentation that better suits the image content. As one may note in the middle and right columns of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F12" title="Figure 12 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">12</span></a>, most superpixel methods generate superpixels in a number close to the desired one</span>. However, only <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.2">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.3">ODISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.4">SICLE</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.5">SH</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.6">ERS</span> generate precisely the desired number of superpixels. In contrast, <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.7">LNSNet</span> and <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.8">DRW</span> generate quantities farther from the desired ones. While <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.9">DRW</span> usually produces fewer superpixels, <span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.10">LNSNet</span> creates thousands more in the NYUV2 dataset.</p>
</div>
<div class="ltx_para" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.1">Superpixel connectivity is also an important property to consider. However, many methods in the literature do not guarantee it. As one may see in the left column of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F12" title="Figure 12 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">12</span></a>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.1">LNSNet</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.2">CRS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.3">SEEDS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.4">DRW</span> do not guarantee the connectivity of their superpixels.
<span class="ltx_text" id="A3.SS1.p2.1.5" style="color:#000000;"> While <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.5.1">CRS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.5.2">SEEDS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.5.3">DRW</span> produce fewer unconnected superpixels, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.5.4">LNSNet</span> generates much more</span>.
Only in the NYUV2 dataset, none of the methods produce unconnected superpixels. Therefore, we omit its chart.
For the quantitative and stability experiments (Sections <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS2" title="C.2. Superpixels stability ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.2</span></a>, respectively), we perform post-processing to enforce connectivity in <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.6">LNSNet</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.7">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.8">DRW</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.9">CRS</span>. Let the similarity between two superpixels as the <span class="ltx_text ltx_font_italic" id="A3.SS1.p2.1.10">Euclidean distance</span> between their average colors. The merging step combines the smaller-area superpixels with their most similar neighbor (in an 8-neighborhood) until the number of superpixels reaches the number of segmentation labels.</p>
</div>
<figure class="ltx_figure" id="A3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="324" id="A3.F12.g1" src="x17.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span><span class="ltx_text" id="A3.F12.2.1" style="color:#000000;"> Number of not connected superpixels concerning the number of generated superpixels (on the left) and the number of generated superpixels in relation to the desired number of superpixels (at the middle and right) on Birds+Insects+Sky+ECSSD and NYUV2 datasets. Note that methods without unconnected superpixels do not appear on the connectivity plot (on the left). Also, none of the methods produce unconnected superpixels in NYUV2.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="875" id="A3.F13.g1" src="x18.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span><span class="ltx_text" id="A3.F13.2.1" style="color:#000000;"> Results for the minimum BR, maximum UE, and standard deviation of BR and UE on Birds+Insects+Sky+ECSSD and NYUV2 datasets.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="874" id="A3.F14.g1" src="x19.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14. </span><span class="ltx_text" id="A3.F14.2.1" style="color:#000000;"> Results for the minimum and standard deviation of EV and SIRS on Birds+Insects+Sky+ECSSD and NYUV2 datasets.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1128" id="A3.F15.g1" src="x20.png" width="806"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>Influence of average blur (first and third rows) and salt and pepper noise (second and fourth rows) for <math alttext="K\approx 400" class="ltx_Math" display="inline" id="A3.F15.2.m1.1"><semantics id="A3.F15.2.m1.1b"><mrow id="A3.F15.2.m1.1.1" xref="A3.F15.2.m1.1.1.cmml"><mi id="A3.F15.2.m1.1.1.2" xref="A3.F15.2.m1.1.1.2.cmml">K</mi><mo id="A3.F15.2.m1.1.1.1" xref="A3.F15.2.m1.1.1.1.cmml">≈</mo><mn id="A3.F15.2.m1.1.1.3" xref="A3.F15.2.m1.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.F15.2.m1.1c"><apply id="A3.F15.2.m1.1.1.cmml" xref="A3.F15.2.m1.1.1"><approx id="A3.F15.2.m1.1.1.1.cmml" xref="A3.F15.2.m1.1.1.1"></approx><ci id="A3.F15.2.m1.1.1.2.cmml" xref="A3.F15.2.m1.1.1.2">𝐾</ci><cn id="A3.F15.2.m1.1.1.3.cmml" type="integer" xref="A3.F15.2.m1.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F15.2.m1.1d">K\approx 400</annotation><annotation encoding="application/x-llamapun" id="A3.F15.2.m1.1e">italic_K ≈ 400</annotation></semantics></math> on Birds dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2. </span>Superpixels stability</h3>
<section class="ltx_subsubsection" id="A3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.1. </span>Object delineation</h4>
<div class="ltx_para" id="A3.SS2.SSS1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.p1.1"><span class="ltx_text" id="A3.SS2.SSS1.p1.1.1" style="color:#000000;">As one may see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F13" title="Figure 13 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">13</span></a>, most methods present high stability on Birds+Insects+Sky+ECSSD datasets regarding object delineation since most of them present a performance that monotonically increases in BR and decreases in UE. However, most methods are unstable in the NYUV2 dataset. In contrast, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.1.1">DAL-HERS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.1.2">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.1.3">ETPS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.1.4">CRS</span> show lower BR stability on all datasets. Also, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.1.5">ODISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.1.6">SICLE</span> only present instability on Sky and NYUV2 datasets.</span>
The <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.2">ODISF</span>’s and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.3">ODISF</span>’s instability explains their inferior mean BR and UE (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS1" title="4.1. Quantitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a>) performance on those datasets. In comparison, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.4">DAL-HERS</span> presents greater instability due to its creation of tiny regions, as one may see in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#S4.SS3" title="4.3. Qualitative evaluation ‣ 4. Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F13" title="Figure 13 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">13</span></a>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.5">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.6">GMMSP</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.7">LSC</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.8">SH</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.9">ERS</span> show high stability, while <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.10">DRW</span> presents some instability across most datasets. <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.11">ISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.12">RSS</span> present stable and low std BR and UE but with some instability in max UE and min BR. <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.13">AINET</span> and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.14">SSFCN</span> also present stability, but less than <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.15">DRW</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.16">ISF</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.17">RSS</span>. A few instability was also observed on <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.18">SIN</span> and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.19">SLIC</span>. Concerning min BR, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.20">GRID</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.21">CRS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.22">SEEDS</span> have the worst results, while <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.23">SH</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.24">ISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.25">RSS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.26">GMMSP</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.27">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.28">LSC</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS1.p1.1.29">ERS</span> have the highest ones.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.2. </span>Color homogeneity</h4>
<div class="ltx_para" id="A3.SS2.SSS2.p1">
<p class="ltx_p" id="A3.SS2.SSS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F14" title="Figure 14 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">14</span></a> presents the color homogeneity stability evaluation. Concerning min EV and min SIRS, most of the methods with the former have increasing values, while the methods with the second show more rigorous minimum scores.
In both minimum measures, the methods with the highest minimum differ, except for <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.1">DISF</span>, which presents higher results in most datasets, followed by <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.2">SH</span>. Among the evaluations with min EV, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.3">ODISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.4">SICLE</span> have almost constant values and worse results than <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.5">GRID</span> in Sky and NYUV2 datasets. These results are due to the saliency map and the concentration of superpixels in the salient region, as aforementioned.
Furthermore, std SIRS and std EV also show distinct variations. While the std EV results present less stable values, the std SIRS evaluation presents more increasing results, indicating greater instability in some methods. For the std EV assessment, the methods <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.6">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.7">SH</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.8">LSC</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.9">SIN</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.10">AINET</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.11">SSFCN</span> show high stability on all datasets. In addition, the <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.12">ISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.13">RSS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.14">SCALP</span> also show high stability on at least one dataset. In std SIRS, the methods <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.15">LNSNet</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.16">GRID</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.17">IBIS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.18">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.19">SLIC</span> show less stability on Birds and Insects datasets. On the other hand, <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.20">DISF</span> shows high stability in SIRS, followed by <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.21">SH</span> and <span class="ltx_text ltx_font_bold" id="A3.SS2.SSS2.p1.1.22">ETPS</span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3. </span>Robustness</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.4">Noise and blur robustness evaluate, respectively, the susceptibility of the algorithm to strong and irrelevant edges and potentially relevant but soft edges. Similar to <cite class="ltx_cite ltx_citemacro_citep">(Stutz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#bib.bib110" title="">2018</a>)</cite>, we evaluated robustness against salt and pepper noise and average blur. In this experiment, we varied the average blur filter size by <math alttext="\{0,5,9,13,17\}" class="ltx_Math" display="inline" id="A3.SS3.p1.1.m1.5"><semantics id="A3.SS3.p1.1.m1.5a"><mrow id="A3.SS3.p1.1.m1.5.6.2" xref="A3.SS3.p1.1.m1.5.6.1.cmml"><mo id="A3.SS3.p1.1.m1.5.6.2.1" stretchy="false" xref="A3.SS3.p1.1.m1.5.6.1.cmml">{</mo><mn id="A3.SS3.p1.1.m1.1.1" xref="A3.SS3.p1.1.m1.1.1.cmml">0</mn><mo id="A3.SS3.p1.1.m1.5.6.2.2" xref="A3.SS3.p1.1.m1.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.1.m1.2.2" xref="A3.SS3.p1.1.m1.2.2.cmml">5</mn><mo id="A3.SS3.p1.1.m1.5.6.2.3" xref="A3.SS3.p1.1.m1.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.1.m1.3.3" xref="A3.SS3.p1.1.m1.3.3.cmml">9</mn><mo id="A3.SS3.p1.1.m1.5.6.2.4" xref="A3.SS3.p1.1.m1.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.1.m1.4.4" xref="A3.SS3.p1.1.m1.4.4.cmml">13</mn><mo id="A3.SS3.p1.1.m1.5.6.2.5" xref="A3.SS3.p1.1.m1.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.1.m1.5.5" xref="A3.SS3.p1.1.m1.5.5.cmml">17</mn><mo id="A3.SS3.p1.1.m1.5.6.2.6" stretchy="false" xref="A3.SS3.p1.1.m1.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.SS3.p1.1.m1.5b"><set id="A3.SS3.p1.1.m1.5.6.1.cmml" xref="A3.SS3.p1.1.m1.5.6.2"><cn id="A3.SS3.p1.1.m1.1.1.cmml" type="integer" xref="A3.SS3.p1.1.m1.1.1">0</cn><cn id="A3.SS3.p1.1.m1.2.2.cmml" type="integer" xref="A3.SS3.p1.1.m1.2.2">5</cn><cn id="A3.SS3.p1.1.m1.3.3.cmml" type="integer" xref="A3.SS3.p1.1.m1.3.3">9</cn><cn id="A3.SS3.p1.1.m1.4.4.cmml" type="integer" xref="A3.SS3.p1.1.m1.4.4">13</cn><cn id="A3.SS3.p1.1.m1.5.5.cmml" type="integer" xref="A3.SS3.p1.1.m1.5.5">17</cn></set></annotation-xml><annotation encoding="application/x-tex" id="A3.SS3.p1.1.m1.5c">\{0,5,9,13,17\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS3.p1.1.m1.5d">{ 0 , 5 , 9 , 13 , 17 }</annotation></semantics></math> and the noise probability by <math alttext="\{0,0.4,0.08,0.12,0.16\}" class="ltx_Math" display="inline" id="A3.SS3.p1.2.m2.5"><semantics id="A3.SS3.p1.2.m2.5a"><mrow id="A3.SS3.p1.2.m2.5.6.2" xref="A3.SS3.p1.2.m2.5.6.1.cmml"><mo id="A3.SS3.p1.2.m2.5.6.2.1" stretchy="false" xref="A3.SS3.p1.2.m2.5.6.1.cmml">{</mo><mn id="A3.SS3.p1.2.m2.1.1" xref="A3.SS3.p1.2.m2.1.1.cmml">0</mn><mo id="A3.SS3.p1.2.m2.5.6.2.2" xref="A3.SS3.p1.2.m2.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.2.m2.2.2" xref="A3.SS3.p1.2.m2.2.2.cmml">0.4</mn><mo id="A3.SS3.p1.2.m2.5.6.2.3" xref="A3.SS3.p1.2.m2.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.2.m2.3.3" xref="A3.SS3.p1.2.m2.3.3.cmml">0.08</mn><mo id="A3.SS3.p1.2.m2.5.6.2.4" xref="A3.SS3.p1.2.m2.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.2.m2.4.4" xref="A3.SS3.p1.2.m2.4.4.cmml">0.12</mn><mo id="A3.SS3.p1.2.m2.5.6.2.5" xref="A3.SS3.p1.2.m2.5.6.1.cmml">,</mo><mn id="A3.SS3.p1.2.m2.5.5" xref="A3.SS3.p1.2.m2.5.5.cmml">0.16</mn><mo id="A3.SS3.p1.2.m2.5.6.2.6" stretchy="false" xref="A3.SS3.p1.2.m2.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.SS3.p1.2.m2.5b"><set id="A3.SS3.p1.2.m2.5.6.1.cmml" xref="A3.SS3.p1.2.m2.5.6.2"><cn id="A3.SS3.p1.2.m2.1.1.cmml" type="integer" xref="A3.SS3.p1.2.m2.1.1">0</cn><cn id="A3.SS3.p1.2.m2.2.2.cmml" type="float" xref="A3.SS3.p1.2.m2.2.2">0.4</cn><cn id="A3.SS3.p1.2.m2.3.3.cmml" type="float" xref="A3.SS3.p1.2.m2.3.3">0.08</cn><cn id="A3.SS3.p1.2.m2.4.4.cmml" type="float" xref="A3.SS3.p1.2.m2.4.4">0.12</cn><cn id="A3.SS3.p1.2.m2.5.5.cmml" type="float" xref="A3.SS3.p1.2.m2.5.5">0.16</cn></set></annotation-xml><annotation encoding="application/x-tex" id="A3.SS3.p1.2.m2.5c">\{0,0.4,0.08,0.12,0.16\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS3.p1.2.m2.5d">{ 0 , 0.4 , 0.08 , 0.12 , 0.16 }</annotation></semantics></math> in the Birds dataset images with approximately <math alttext="400" class="ltx_Math" display="inline" id="A3.SS3.p1.3.m3.1"><semantics id="A3.SS3.p1.3.m3.1a"><mn id="A3.SS3.p1.3.m3.1.1" xref="A3.SS3.p1.3.m3.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="A3.SS3.p1.3.m3.1b"><cn id="A3.SS3.p1.3.m3.1.1.cmml" type="integer" xref="A3.SS3.p1.3.m3.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.SS3.p1.3.m3.1c">400</annotation><annotation encoding="application/x-llamapun" id="A3.SS3.p1.3.m3.1d">400</annotation></semantics></math> superpixels. The evaluation measures used were BR, UE, EV, SIRS, and the number of superpixels produced (<math alttext="K" class="ltx_Math" display="inline" id="A3.SS3.p1.4.m4.1"><semantics id="A3.SS3.p1.4.m4.1a"><mi id="A3.SS3.p1.4.m4.1.1" xref="A3.SS3.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A3.SS3.p1.4.m4.1b"><ci id="A3.SS3.p1.4.m4.1.1.cmml" xref="A3.SS3.p1.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS3.p1.4.m4.1c">K</annotation><annotation encoding="application/x-llamapun" id="A3.SS3.p1.4.m4.1d">italic_K</annotation></semantics></math>) in the segmentations.</p>
</div>
<div class="ltx_para" id="A3.SS3.p2">
<p class="ltx_p" id="A3.SS3.p2.1">As one may see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F15" title="Figure 15 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">15</span></a>, blur and noise generally tend to have a similar impact. <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.1">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.2">ERGC</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.3">RSS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.4">ISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.5">ODISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.6">SEEDS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.7">SH</span> are robust in blur and noise. On the other hand, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.8">DAL-HERS</span> shows the lowest noise robustness, followed by <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.9">LNSNet</span> and <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.10">ERS</span>. Despite being the least robust to noise, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.11">DAL-HERS</span> achieves considerable robustness to blur. A similar sensitivity to noise can be observed in <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.12">SICLE</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.13">AINET</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.14">SSFCN</span> regarding homogeneity. However, their homogeneity highly increased with blur. Also, they present high robustness to noise and blur concerning delineation. On the other hand, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.15">DRW</span> was the most influenced by blur. One can also see that some methods present a slightly better evaluation when adding blur or noise. That is the case for <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.16">LNSNet</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.17">IBIS</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.18">SLIC</span> with blur. The same occurs less perceptibly in <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.19">SEEDS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.20">DAL-HERS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.21">ERGC</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.22">ERS</span>.</p>
</div>
<div class="ltx_para" id="A3.SS3.p3">
<p class="ltx_p" id="A3.SS3.p3.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.F15" title="Figure 15 ‣ C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">15</span></a>, some methods try to compensate for noise and blur by producing more or fewer superpixels. Among the evaluated methods, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.1">LNSNet</span> is the most impacted in the number of superpixels generated, especially when adding noise. As seen in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19179v1#A3.SS1" title="C.1. Number of superpixels and connectivity ‣ Appendix C Additional Experimental Results ‣ A comprehensive review and new taxonomy on superpixel segmentation"><span class="ltx_text ltx_ref_tag">C.1</span></a>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.2">LNSNet</span> produces superpixels that are more discrepant in quantity, many of those disconnected. The second with the most influenced number of superpixels is <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.3">DAL-HERS</span> when adding noise. In addition to these, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.4">IBIS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.5">DRW</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.6">SLIC</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.7">GMMSP</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.8">SCALP</span> show a moderate susceptibility to the number of superpixels. Finally, the addition of noise or blur does not modify the number of superpixels generated in the <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.9">CRS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.10">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.11">ERGC</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.12">ERS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.13">ETPS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.14">SICLE</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.15">ODISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.16">RSS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.17">SH</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.18">SNIC</span> methods.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4. </span>Overall performance</h3>
<div class="ltx_para" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.1">This Section discusses the overall performance of superpixel methods within the same clustering category. In this work, the evaluated methods with boundary evolution clustering present higher compactness, regularity, and efficiency. They nearly achieve real-time computation in ECSSD (due to its smaller images) and around 0.1 seconds per image on the others, except by <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.1">CRS</span> which takes around 0.3 seconds per image in ECSSD. Nevertheless, they have worse boundary adherence and color homogeneity. <span class="ltx_text" id="A3.SS4.p1.1.2" style="color:#000000;"> <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.2.1">IBIS</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.2.2">ETPS</span> have the best delineation and color homogeneity in this category</span>. Conversely, <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.3">CRS</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.4">SEEDS</span> have the worst delineation, but the former produces the most compact superpixels. Although the superpixels in <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.5">ETPS</span> are not as compact as those in <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.6">CRS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.7">ETPS</span> produces more compact superpixels than the remaining methods with boundary evolution clustering. However, <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.8">ETPS</span> along with <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.9">CRS</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.10">SEEDS</span> present some instability. Also, <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.11">CRS</span> is more sensitive to noise since its delineation results greatly decrease when increasing the average blur.</p>
</div>
<div class="ltx_para" id="A3.SS4.p2">
<p class="ltx_p" id="A3.SS4.p2.1">In contrast, methods with dynamic-update-clustering are less efficient and generate slightly less compact and regular superpixels. Also, they have better delineation and homogeneity than those based on boundary evolution. Although <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.1">SNIC</span> requires more time per image than all other CPU-based methods (around 3 to 4 seconds in most datasets compared to 1 second in <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.2">DRW</span>), <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.3">DRW</span> offers less control over the number of superpixels, being the method that produces fewer superpixels. <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.4">DRW</span> also produces a few unconnected superpixels, requiring post-processing. In contrast, the delineation in <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.5">DRW</span> usually surpasses <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.6">SNIC</span>, which has more compact superpixels. However, <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.7">DRW</span> is very sensitive to noise. By increasing average blur, <span class="ltx_text ltx_font_bold" id="A3.SS4.p2.1.8">DRW</span> produces fewer superpixels, and they are less adherent to the objects’ borders and less homogeneous.</p>
</div>
<div class="ltx_para" id="A3.SS4.p3">
<p class="ltx_p" id="A3.SS4.p3.1">Methods with neighborhood-based clustering present more varied performances, but similar stability. Among these, the <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.1">LSC</span> has more boundary adherence and homogeneity, but less compactness and smoothness than <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.2">SLIC</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.3">SCALP</span>. On the other hand, <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.4">SLIC</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.5">SCALP</span> produce more compact superpixels, the latter more than the former. Also, <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.6">SLIC</span> is less robust to salt and pepper noise than <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.7">LSC</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.8">SCALP</span> since its boundary adherence decreases when the noise increases. Although <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.9">SCALP</span> is better at managing the tradeoff between boundary adherence and compactness, it is the less efficient method in its clustering category, requiring around one second to segment an image in most datasets, while <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.10">LSC</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.11">SLIC</span> require around 0.4 and 0.1 seconds, respectively.</p>
</div>
<div class="ltx_para" id="A3.SS4.p4">
<p class="ltx_p" id="A3.SS4.p4.1"><span class="ltx_text" id="A3.SS4.p4.1.1" style="color:#000000;">The path-based clustering methods are generally stable, robust to noise, and have the best delineation along with the most homogeneous superpixels. However, they have varied efficiency, low compactness, and low smoothness. <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.1">RSS</span>, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.2">ISF</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.3">ERGC</span> produce superpixels with smooth borders. <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.4">ERGC</span> has worse delineation but higher compactness than <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.5">RSS</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.6">ISF</span>. Unlike <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.7">ISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.8">ERGC</span> does not produce highly compact superpixels at homogeneous image regions. In addition, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.9">RSS</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.10">ISF</span> have similar good boundary adherence, high color homogeneity, and some compactness. However, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.11">ISF</span> usually has better delineation, color homogeneity, and compactness than <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.12">RSS</span>, which is much more efficient, nearly achieving real-time processing in the <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.13">ECSSD</span> dataset and 0.1 seconds per image on the others. On the other hand, in most cases, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.14">ISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.15">ERGC</span> take around 0.7 and 0.2 seconds, respectively. Conversely, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.16">DISF</span>, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.17">ODISF</span>, and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.18">SICLE</span> have excellent delineation and color homogeneity in most datasets and produce the exact number of desired superpixels. However, they are less efficient than <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.19">ISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.20">RSS</span>, especially <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.21">ODISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.22">SICLE</span>. While <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.23">DISF</span> requires around 1.8 seconds per image, <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.24">ODISF</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.25">SICLE</span> require 2.5 seconds. Although their delineation may degrade when the saliency map fails to identify the image object, such a problem may be surpassed in <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1.26">SICLE</span> by reducing its saliency map importance</span>.</p>
</div>
<div class="ltx_para" id="A3.SS4.p5">
<p class="ltx_p" id="A3.SS4.p5.1">Hierarchical methods also produce superpixels with excellent boundary adherence and they have low execution time, but their superpixels are neither visually compact nor smooth. Among these, <span class="ltx_text ltx_font_bold" id="A3.SS4.p5.1.1">DAL-HERS</span> has low delineation, is less stable, and is highly sensitive to salt and pepper noise, even producing much more superpixel when the noise increases. In contrast, <span class="ltx_text ltx_font_bold" id="A3.SS4.p5.1.2">SH</span> has a competitive delineation, is stable, is more robust to noise, and is one of the most efficient methods evaluated, with nearly real-time processing.
Regarding methods with clustering based on data distribution, <span class="ltx_text ltx_font_bold" id="A3.SS4.p5.1.3">GMMSP</span> is stable, robust to noise, and has a competitive delineation. In contrast to other methods with competitive delineation, the superpixels in <span class="ltx_text ltx_font_bold" id="A3.SS4.p5.1.4">GMMSP</span> have visually good compactness and smooth contours. However, its runtime is far from real-time, requiring around 1 second per image to produce superpixels. Similarly, <span class="ltx_text ltx_font_bold" id="A3.SS4.p5.1.5">ERS</span>, the only evaluated method that performs graph-based clustering is stable, robust, has a competitive delineation, visually compact subpixels, and an efficiency worse than <span class="ltx_text ltx_font_bold" id="A3.SS4.p5.1.6">GMMSP</span>, requiring around 2.5 seconds per image.</p>
</div>
<div class="ltx_para" id="A3.SS4.p6">
<p class="ltx_p" id="A3.SS4.p6.1">In contrast, methods that perform clustering with a deep network achieve moderate to low results. Among these, <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.1">LNSNet</span> presents a visually poor delineation and low compactness. It also has the worse control over the number of superpixels, since it may produce thousands more superpixels than desired. <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.2">LNSNet</span> has the worst efficiency and it is very sensitive to noise, generating much more superpixels when increasing average blur or salt and pepper noise. Conversely, <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.3">SSFCN</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.4">AINET</span> have similar results in all criteria. Both are sensitive to salt and pepper noise and average blur, have a moderate delineation, and have good compactness. However, they are stable and require around 0.1 seconds per image to generate superpixels. Although <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.5">AINET</span> slightly achieves better delineation than <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.6">SSFCN</span>, it is slightly less efficient. Conversely, <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.7">SIN</span> is a more efficient approach than the other deep-based clustering methods (except in the Birds dataset) and produces more compact superpixels, but has low boundary adherence. <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.8">SIN</span> is also less sensitive to salt and pepper noise than <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.9">AINET</span> and <span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.10">SSFCN</span>, but is more sensitive to average blur.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 27 22:50:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
